<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective</title>
<link>https://arxiv.org/abs/2511.14772</link>
<guid>https://arxiv.org/abs/2511.14772</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, inference time scaling, problem decomposition, Chain-of-Thought, Tree-of-Thought<br /><br />Summary: This survey paper reviews methods aimed at enhancing the predictive accuracy of pretrained large language models by increasing computational resources during inference. First, it categorizes test-time scaling techniques based on how problems are broken down into subproblems and the topological structure of these subproblems, whether sequential, parallel, or tree-structured. Second, the paper highlights the unification of diverse strategies under this framework, including approaches like Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought. Third, it synthesizes and compares existing analyses, pointing out the strengths and weaknesses of each approach in terms of efficiency, scalability, and accuracy improvements. Fourth, the survey emphasizes how the topological organization of subproblems influences the effectiveness of these inference-time scaling methods. Finally, it concludes with a discussion of promising future research directions to further optimize test-time compute allocation for large language models, potentially leading to improved reasoning and problem-solving capabilities during inference. <div>
arXiv:2511.14772v1 Announce Type: new 
Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Predictors of Outcome in Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.14773</link>
<guid>https://arxiv.org/abs/2511.14773</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, Large Language Models, reasoning, interpretability, inference control  

<br /><br />Summary: This work investigates the internal reasoning process of Large Language Models (LLMs) employing the Chain-of-Thought (CoT) paradigm, which elicits step-by-step rationales to refine the model's latent solution representation. The study focuses on determining how early in the reasoning sequence the model commits to its final answer. By training linear classifiers on the hidden states after just a few reasoning tokens, the researchers demonstrate that the model’s eventual correctness is highly predictable very early, even though longer token sequences are often required to produce a definitive answer. The analysis reveals that for harder questions, predictive accuracy temporarily declines, which is explained by a selection artifact: difficult questions tend to have longer CoT outputs and are overrepresented in later reasoning stages. These findings suggest that LLMs internally self-assess their success early in the reasoning process. The implications of this work impact interpretability, enabling better understanding of when and how models settle on answers, and inference-time control, potentially allowing more efficient or targeted reasoning strategies by monitoring early internal states. <div>
arXiv:2511.14773v1 Announce Type: new 
Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2511.14774</link>
<guid>https://arxiv.org/abs/2511.14774</guid>
<content:encoded><![CDATA[
<div> cross-lingual knowledge transfer, large language models, multilingual evaluation, temporal filtering, linguistic distance  

<br /><br />Summary:  
1. Evaluating cross-lingual knowledge transfer in large language models (LLMs) is difficult because correct answers in a target language may result either from genuine knowledge transfer or prior training exposure.  
2. The authors introduce LiveCLKTBench, an automated pipeline designed to isolate and measure genuine cross-lingual knowledge transfer by focusing on self-contained, time-sensitive knowledge entities from real-world domains.  
3. This pipeline filters entities based on their temporal occurrence and verifies them against the model’s knowledge to ensure the information is novel and valid for evaluation.  
4. Valid entities are then used to generate factual questions, which are translated into multiple languages, enabling the assessment of transferability across diverse linguistic boundaries.  
5. Using LiveCLKTBench, several LLMs are evaluated across five languages, revealing that cross-lingual transfer is strongly influenced by linguistic distance and displays asymmetry depending on the language pair direction.  
6. Although larger LLMs exhibit improved cross-lingual transfer abilities, the improvement rates diminish with model scale and vary by domain.  
7. The findings enhance understanding of multilingual knowledge transfer and highlight LiveCLKTBench as a valuable, reliable benchmark for future cross-lingual transfer research. <div>
arXiv:2511.14774v1 Announce Type: new 
Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation</title>
<link>https://arxiv.org/abs/2511.14776</link>
<guid>https://arxiv.org/abs/2511.14776</guid>
<content:encoded><![CDATA[
<div> COMPASS, Context Reliance Score, PID controller, Attention modulation, Factual consistency  

<br /><br />Summary:  
1. Large language models (LLMs) often produce fluent but factually incorrect outputs due to improper allocation of attention between their contextual input and parametric knowledge stored in model weights.  
2. Addressing this challenge requires understanding and steering the internal attention mechanisms of LLMs to improve both trustworthiness and scientific interpretability.  
3. The paper introduces COMPASS (Context-Modulated PID Attention Steering System), a lightweight and interpretable control framework that incorporates a model-based feedback loop during decoding without the need for retraining or multiple decoding passes.  
4. COMPASS uses a novel metric called the Context Reliance Score (CRS) to quantify how much the model’s attention heads rely on contextual evidence while generating text, serving as an online and transparent probe.  
5. A PID controller dynamically adjusts attention heads in real time based on CRS to maintain factual consistency, resulting in a significant reduction of contextual hallucinations (2.8 to 5.8 percent absolute improvement) across various benchmarks such as HotpotQA, XSum, HaluEval, and RAGTruth.  
6. The framework also provides insights into the distinct roles of different attention heads in grounding generation in evidence, demonstrating that feedback-driven interpretability can advance scientific understanding of LLM behavior. <div>
arXiv:2511.14776v1 Announce Type: new 
Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech</title>
<link>https://arxiv.org/abs/2511.14779</link>
<guid>https://arxiv.org/abs/2511.14779</guid>
<content:encoded><![CDATA[
<div> Keywords: spontaneous speech, prosodic segmentation, speech synthesis, FastSpeech 2, Brazilian Portuguese<br /><br />Summary:<br /><br />1. The paper addresses the challenges of synthesizing spontaneous speech, focusing on capturing conversational features like turn-taking, pauses, and disfluencies.  
2. It highlights the progress in speech synthesis through models that implicitly capture prosodic features such as pitch, intensity, and duration, but notes a gap in understanding the impact of explicit prosodic segmentation datasets, especially for spontaneous speech.  
3. The study evaluates the effects of both manual and automatic prosodic segmentation annotations on the quality of speech synthesis in Brazilian Portuguese using the non-autoregressive FastSpeech 2 model.  
4. Results demonstrate that training with prosodic segmentation improves intelligibility and acoustic naturalness of synthesized speech, with automatic segmentation producing more regular segments, while manual segmentation introduces more variability, enhancing natural prosody.  
5. Analyzing neutral declarative sentences, the models replicated expected nuclear accent patterns, but the prosodic segmentation approach better matched natural pre-nuclear intonation contours.  
6. To promote reproducibility and further research, all datasets, source code, and trained models have been made publicly available under the CC BY-NC-ND 4.0 license. <div>
arXiv:2511.14779v1 Announce Type: new 
Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human or LLM as Standardized Patients? A Comparative Study for Medical Education</title>
<link>https://arxiv.org/abs/2511.14783</link>
<guid>https://arxiv.org/abs/2511.14783</guid>
<content:encoded><![CDATA[
<div> Standardized Patients, Large Language Models, Clinical Skills Training, Multi-Agent Framework, Benchmarking  

<br /><br />Summary:  
The article introduces EasyMED, a novel multi-agent framework designed to simulate Standardized Patients (SP) for clinical skills training. Traditional SPs are costly, inflexible, and hard to scale, prompting the exploration of large language model (LLM)-based simulators, which have previously shown inconsistent performance and lacked rigorous benchmarking against human SPs. EasyMED incorporates three specialized agents: a Patient Agent that facilitates realistic dialogue, an Auxiliary Agent ensuring factual consistency, and an Evaluation Agent that provides actionable feedback to learners. To rigorously evaluate these systems, the authors created SPBench, a comprehensive benchmark dataset containing real SP-doctor interactions across 14 medical specialties and eight expert-defined evaluation criteria. Experimental results demonstrate that EasyMED achieves learning outcomes comparable to those of human SPs, with a notable advantage in delivering greater skill improvements for students starting with lower baseline abilities. Additionally, EasyMED offers enhanced flexibility, psychological safety for learners, and significant reductions in cost, highlighting its potential as a scalable and effective solution for medical education. Overall, this work advances the development and assessment of AI-driven SP simulators, grounding their future use in strong empirical evidence. <div>
arXiv:2511.14783v1 Announce Type: new 
Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion Mining and Analysis Using Hybrid Deep Neural Networks</title>
<link>https://arxiv.org/abs/2511.14796</link>
<guid>https://arxiv.org/abs/2511.14796</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, deep learning, hybrid model, BGRU, LSTM

<br /><br />Summary:  
This study addresses the growing need for effective sentiment analysis due to the rise of social media and e-commerce by focusing on customer opinion mining. Traditional methods such as lexicon-based and classical machine learning approaches struggle with contextual nuances and scalability issues. Deep learning techniques, particularly recurrent neural networks (RNNs) and convolutional neural networks (CNNs), have improved semantic understanding but still face challenges. The authors propose a hybrid deep neural network model called HBGRU-LSTM, which combines bidirectional gated recurrent units (BGRU) with long short-term memory (LSTM) layers to enhance sentiment classification. The model aims to better capture context, handle scalability, and address class imbalance problems. Empirical evaluation was conducted on benchmark datasets including IMDB movie reviews and Amazon product ratings. The HBGRU-LSTM model achieved 95% test accuracy, outperforming traditional deep learning configurations like LSTM alone (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Notably, it improved recall for negative sentiments significantly from 86% on an unbalanced dataset to 96% on a balanced dataset, promoting fairer sentiment classification. Additionally, the hybrid model reduced misclassification loss from 20.24% (unbalanced) to 13.3% (balanced), indicating superior generalization and robustness in sentiment analysis tasks. <div>
arXiv:2511.14796v1 Announce Type: new 
Abstract: Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings</title>
<link>https://arxiv.org/abs/2511.14868</link>
<guid>https://arxiv.org/abs/2511.14868</guid>
<content:encoded><![CDATA[
<div> large language models, text embeddings, hierarchical token prepending, attention mechanism, long documents<br /><br />Summary:<br /><br />Large language models (LLMs) generate powerful text embeddings; however, their causal attention mechanism limits the flow of information from later to earlier tokens, reducing representation quality. Recent approaches attempt to address this by prepending a single summary token to the input, but this can lead to over-compression of information and degrade performance, especially on long documents. The proposed method, Hierarchical Token Prepending (HTP), tackles two main challenges. First, it mitigates attention-level information compression by dividing the input into blocks and prepending block-level summary tokens to subsequent blocks, enabling multiple backward information pathways. Second, it solves readout-level over-squashing by replacing last-token pooling with mean-pooling, a change supported by theoretical analysis. HTP demonstrates consistent improvements across 11 retrieval datasets and 30 general embedding benchmarks, with particularly strong gains in long-context scenarios. Its simple and architecture-agnostic design enhances performance in both zero-shot and fine-tuned models. Overall, HTP provides a scalable and effective solution for producing superior embeddings from long documents. <div>
arXiv:2511.14868v1 Announce Type: new 
Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation</title>
<link>https://arxiv.org/abs/2511.15005</link>
<guid>https://arxiv.org/abs/2511.15005</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, uncertainty estimation, contrastive decoding, retrieval-augmentation<br /><br />Summary:<br /><br />This article addresses the issue of hallucinations in Large Language Models (LLMs), where outputs sound plausible but are factually incorrect or unsupported. First, it introduces a mathematically grounded framework leveraging probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation to understand and measure these hallucinations. Second, it analyzes error compounding in the autoregressive generation process of LLMs, illustrating how inaccuracies can accumulate over time. Third, the work proposes refined uncertainty metrics that include semantic and phase-aware variants, improving the detection and quantification of hallucination risks. Fourth, it develops principled mitigation strategies such as contrastive decoding—enhancing output reliability by comparing alternative generations—and retrieval-augmented grounding, which utilizes relevant external knowledge to support factual correctness. Finally, it connects and unifies recent advances in model calibration, retrieval methods, and alignment techniques into a coherent approach aimed at making LLMs safer and more reliable. This unified framework helps bridge theoretical insights and practical solutions for reducing hallucinations in LLM outputs. <div>
arXiv:2511.15005v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs</title>
<link>https://arxiv.org/abs/2511.15163</link>
<guid>https://arxiv.org/abs/2511.15163</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalized tutoring, knowledge tracing, forgetting curve, student persona  

<br /><br />Summary:  
The paper addresses the limitations of current intelligent tutoring systems, particularly in mathematics education, where adaptive instruction must reflect the dynamic evolution of a student's knowledge, including proficiency shifts and forgetting. The authors propose TASA (Teaching According to Students' Aptitude), a novel tutoring framework that personalizes learning by integrating three key components: a structured student persona, an event memory of previous interactions, and a continuous forgetting curve model. TASA tracks each student's mastery state over time by combining knowledge tracing techniques with the temporal effects of forgetting, enabling real-time updates to proficiency profiles. This dynamic modeling allows TASA to generate learning materials—questions and explanations—tailored in difficulty and content to the individual learner’s current cognitive state. Empirical evaluations show that TASA outperforms existing baseline models in fostering improved learning outcomes and delivers more adaptive, personalized tutoring experiences. The study highlights the critical role of incorporating temporal forgetting and detailed learner profiles in leveraging Large Language Models for effective, personalized educational interventions. By doing so, TASA sets a precedent for future intelligent tutoring systems that can better scaffold mathematical learning through fine-grained, student-specific instructional adjustments. <div>
arXiv:2511.15163v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples</title>
<link>https://arxiv.org/abs/2511.15183</link>
<guid>https://arxiv.org/abs/2511.15183</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual, Vision-Language Models, evaluation, Hindi, Telugu

<br /><br />Summary: India is home to nearly 1.5 billion people and over 120 major languages, showcasing immense diversity. As multilingual Vision-Language Models (VLMs) become more significant, effective evaluation methods are crucial for developing equitable AI suitable for low-resource languages. Current evaluations have notable limitations, including dependence on unverified auto-translations, narrow task coverage, small sample sizes, and insufficient culturally relevant Question-Answering (QA) data. To overcome these challenges, researchers provide a comprehensive framework for evaluating VLMs specifically for Hindi and Telugu, while also comparing performance with English. This framework led to the creation of HinTel-AlignBench, a unique benchmark comprising diverse sources in Hindi and Telugu alongside English-aligned samples. The contributions include a semi-automated dataset creation method through back-translation and human verification, the establishment of the most extensive vision-language benchmark for Hindi and Telugu with adapted and novel datasets totaling around 4,000 QA pairs per language, and detailed performance analysis of various State-of-the-Art VLMs. The findings reveal a regression in performance for tasks in Indian languages compared to English for 4 out of 5 tasks, indicating crucial areas for improvement in multilingual multimodal understanding. <div>
arXiv:2511.15183v1 Announce Type: new 
Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story</title>
<link>https://arxiv.org/abs/2511.15210</link>
<guid>https://arxiv.org/abs/2511.15210</guid>
<content:encoded><![CDATA[
<div> Intrinsic dimension, large language models, textual complexity, sparse autoencoders, genre stratification<br /><br />Summary:<br /><br />1. This study presents the first comprehensive analysis connecting intrinsic dimension (ID) of language model representations with interpretable text properties using cross-encoder models, linguistic features, and sparse autoencoders (SAEs).<br />2. It finds that ID measures geometric complexity complementary to traditional entropy-based metrics; after adjusting for text length, ID and entropy are uncorrelated, showing ID captures aspects of representation orthogonal to prediction quality.<br />3. The research reveals strong genre stratification where scientific texts have low ID (~8), encyclopedic texts medium ID (~9), and creative or opinionated writing high ID (~10.5). This indicates scientific prose is representationally simpler for current LLMs, while fiction and opinion require more representational capacity.<br />4. Using SAEs, causal features affecting ID were identified: scientific writing elements like formal tone, structured report formats, and statistics lower ID, whereas humanizing features such as personalization, emotional content, and narrative elements increase ID.<br />5. Steering experiments confirmed the causal relationship of these features with ID, providing practical insights into interpreting ID and guiding its application in understanding and improving LLM behavior with different text genres. <div>
arXiv:2511.15210v1 Announce Type: new 
Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition</title>
<link>https://arxiv.org/abs/2511.15211</link>
<guid>https://arxiv.org/abs/2511.15211</guid>
<content:encoded><![CDATA[
<div> Clinical NER, zero-shot, multi-agent collaboration, ontology-guided reasoning, OEMA<br /><br />Summary:<br /><br />1. Clinical named entity recognition (NER) is essential for extracting valuable information from electronic health records (EHRs), but traditional supervised approaches such as CRF and BioClinicalBERT rely heavily on costly annotated data. 2. Zero-shot NER using large language models (LLMs) offers a way to reduce annotation dependency but faces challenges with example selection granularity and effectively combining prompt-based learning with self-improvement. 3. To overcome these limitations, the authors propose OEMA, a novel zero-shot clinical NER framework that employs multi-agent collaboration. 4. OEMA consists of three key components: a self-annotator that generates training examples, a discriminator that filters these examples based on SNOMED CT ontology, and a predictor that performs inference by leveraging detailed entity descriptions. 5. Experimental results on the MTSamples and VAERS datasets demonstrate that OEMA achieves state-of-the-art exact-match performance and under related-match metrics matches or exceeds the performance of supervised models like BioClinicalBERT and CRF. 6. Through the integration of ontology-guided reasoning and cooperative multi-agent architecture, OEMA effectively addresses zero-shot NER challenges and approximates near-supervised performance, showing significant promise for clinical natural language processing applications. <div>
arXiv:2511.15211v1 Announce Type: new 
Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context Cascade Compression: Exploring the Upper Limits of Text Compression</title>
<link>https://arxiv.org/abs/2511.15244</link>
<guid>https://arxiv.org/abs/2511.15244</guid>
<content:encoded><![CDATA[
<div> Context Compression, Large Language Models, Latent Tokens, Cascade Model, Decoding Accuracy<br /><br />Summary:<br /><br />The paper addresses the problem of handling extremely long-context inputs (in the million-token range) for large language models (LLMs), which pose significant computational and memory challenges. The authors propose a novel method named Context Cascade Compression (C3) that leverages two LLMs of different sizes working in sequence: a smaller LLM compresses the long text into a small set of latent tokens, while a larger LLM decodes from these latent tokens. This approach achieves a very high compression ratio, reaching up to 20x or even 40x reduction in token length compared to the input text. Experimental results show that at 20x compression, C3 attains a decoding accuracy of 98%, significantly outperforming the 60% accuracy of the previous DeepSeek-OCR method. Even when the compression ratio is pushed to 40x, the accuracy remains around 93%, demonstrating the robustness of C3. Unlike optical character compression approaches that rely on visual features (like layout or color), C3 implements a purely text-based pipeline, simplifying the process and potentially setting upper bounds for compression ratios in OCR and related fields. The authors have released code and model weights to support future research and application development. <div>
arXiv:2511.15244v1 Announce Type: new 
Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndicGEC: Powerful Models, or a Measurement Mirage?</title>
<link>https://arxiv.org/abs/2511.15260</link>
<guid>https://arxiv.org/abs/2511.15260</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, Indian languages, zero-shot prompting, language models, dataset quality<br /><br />Summary:  
1. The paper presents the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task, which focuses on five Indian languages: Telugu, Hindi, Tamil, Malayalam, and Bangla.  
2. The team's approach involves zero- and few-shot prompting of various language models ranging from 4 billion parameters to large proprietary models.  
3. Initial results showed strong performance in Telugu (Rank 4) and Hindi (Rank 2) with GLEU scores of 83.78 and 84.31 respectively.  
4. The study extends experimentation to Tamil, Malayalam, and Bangla, highlighting challenges in data quality and the evaluation metrics used for these languages.  
5. The findings emphasize the promising capabilities of small language models for grammatical error correction and underline the need for high-quality datasets and suitable evaluation metrics tailored to Indian language scripts. <div>
arXiv:2511.15260v1 Announce Type: new 
Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews</title>
<link>https://arxiv.org/abs/2511.15291</link>
<guid>https://arxiv.org/abs/2511.15291</guid>
<content:encoded><![CDATA[
<div> Arabic dialects, sentiment analysis, few-shot learning, hospitality domain, SetFit  

<br /><br />Summary:  
This paper addresses the challenges of sentiment analysis on Arabic dialects, emphasizing the linguistic diversity and lack of annotated data. The focus is on the AHaSIS shared task, which involves analyzing sentiment in hotel reviews written in Moroccan and Saudi Arabic dialects within the hospitality sector. The goal was to classify the sentiment expressed in the reviews as positive, negative, or neutral. The authors used the SetFit framework, which enables efficient fine-tuning of sentence transformers with minimal labeled data, making it suitable for few-shot learning scenarios. Their system achieved a 73% F1 score on the official evaluation dataset. This result placed them 12th out of 26 participating teams. The study demonstrates that few-shot learning methods like SetFit hold promise for handling the scarcity of labeled data in complex dialectal text processing. Additionally, it highlights the importance of tailored approaches to specialized domains such as hotel review sentiment classification in Arabic dialects. Overall, the research contributes to advancements in Arabic natural language processing by leveraging data-efficient models under resource-constrained conditions. <div>
arXiv:2511.15291v1 Announce Type: new 
Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models</title>
<link>https://arxiv.org/abs/2511.15304</link>
<guid>https://arxiv.org/abs/2511.15304</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial poetry, jailbreak, large language models, safety mechanisms, alignment limitations<br /><br />Summary:<br /><br />1. The paper presents evidence that adversarial poetry can serve as a universal single-turn jailbreak method for large language models (LLMs).<br /><br />2. Across 25 cutting-edge proprietary and open-weight models, poetic prompts achieved high attack success rates (ASR), with some providers surpassing 90%.<br /><br />3. These poetic attacks transfer effectively across multiple domains, including CBRN (chemical, biological, radiological, nuclear), manipulation, cyber-offense, and loss-of-control, according to MLCommons and EU CoP risk taxonomies.<br /><br />4. By converting 1,200 harmful MLCommons prompts into verse using a standardized meta-prompt, the authors obtained ASRs up to 18 times higher than the original prose prompts.<br /><br />5. Evaluation was performed using an ensemble of open-weight judge models and a human-validated, stratified subset with double annotations, resolving disagreements manually.<br /><br />6. The average jailbreak success rate achieved was 62% for hand-crafted poetic prompts and approximately 43% for meta-prompt conversions, significantly outperforming non-poetic baselines.<br /><br />7. The findings reveal that stylistic variation alone can bypass current safety measures, exposing systematic vulnerabilities across diverse LLM families and training approaches.<br /><br />8. This suggests fundamental limitations of existing alignment strategies and evaluation protocols in securing language models against adversarial inputs. <div>
arXiv:2511.15304v1 Announce Type: new 
Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning</title>
<link>https://arxiv.org/abs/2511.15355</link>
<guid>https://arxiv.org/abs/2511.15355</guid>
<content:encoded><![CDATA[
<div> Keywords: HEAD-QA v2, healthcare reasoning, multilingual dataset, biomedical exams, large language models<br /><br />Summary:  
1. HEAD-QA v2 is an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset initially released by Vilares and Gómez-Rodríguez in 2019.  
2. The update addresses the increasing demand for high-quality datasets that reflect both the linguistic and conceptual complexity involved in healthcare reasoning.  
3. The dataset has been extended to include over 12,000 questions sourced from ten years of Spanish professional healthcare exams, providing a rich and diverse resource.  
4. Several open-source large language models (LLMs) were benchmarked using various methods including prompting, retrieval-augmented generation (RAG), and probability-based answer selection to evaluate their performance on the dataset.  
5. Findings show that model performance is primarily influenced by the scale and inherent reasoning capabilities of the models, while more complex inference strategies only offer limited improvements. Overall, HEAD-QA v2 establishes itself as a robust resource to advance research in biomedical reasoning and the development of better-performing models. <div>
arXiv:2511.15355v1 Announce Type: new 
Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and G\'omez-Rodr\'iguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Empowerment of Science of Science by Large Language Models: New Tools and Methods</title>
<link>https://arxiv.org/abs/2511.15370</link>
<guid>https://arxiv.org/abs/2511.15370</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, prompt engineering, retrieval augmented generation, scientific evaluation, knowledge graph  

<br /><br />Summary: This manuscript offers a comprehensive review of key technologies underpinning large language models (LLMs) from a user perspective, highlighting methods such as prompt engineering, knowledge-enhanced retrieval augmented generation, fine-tuning, pretraining, and tool learning. It traces the historical evolution of the Science of Science (SciSci) field, illustrating how LLMs can integrate with scientometrics to advance our understanding of scientific development. The paper further explores future applications of LLMs within scientometrics, emphasizing their potential to revolutionize scientific evaluation through AI agent-based models. Additionally, it introduces innovative techniques for detecting emerging research fronts and constructing knowledge graphs using LLMs, indicating new research directions. Overall, the article positions LLMs as transformative tools in both technology and the scientific domain, facilitating enhanced knowledge discovery, evaluation, and synthesis, contributing to the global technological landscape and ongoing pursuit of artificial general intelligence. <div>
arXiv:2511.15370v1 Announce Type: new 
Abstract: Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Compliance-Preserving Retrieval System for Aircraft MRO Task Search</title>
<link>https://arxiv.org/abs/2511.15383</link>
<guid>https://arxiv.org/abs/2511.15383</guid>
<content:encoded><![CDATA[
<div> Keywords: Aircraft Maintenance Technicians, semantic search, compliance-preserving retrieval, MRO operations, aviation manuals<br /><br />Summary:  
This article addresses the inefficiency faced by Aircraft Maintenance Technicians (AMTs) who spend up to 30% of their work time searching through manuals, a significant bottleneck in Maintenance, Repair, and Overhaul (MRO) operations. The proposed solution is a compliance-preserving retrieval system that enhances search accuracy without replacing existing certified manual viewers, ensuring adherence to regulatory constraints. The system integrates large language model (LLM) reranking and semantic search tailored to aviation MRO by constructing robust embeddings based on the hierarchical ATA chapter structure, paired with vision-language parsing to organize and structure certified content. This allows technicians to quickly preview ranked tasks and seamlessly access verified procedures within legacy viewers. Evaluation results on 49,000 synthetic queries demonstrate a high retrieval accuracy exceeding 90%. Additionally, bilingual controlled user studies with 10 licensed AMTs reveal a 90.9% success rate in retrieving the correct procedure within the top 10 results and reduce lookup times from an average of 6 to 15 minutes down to merely 18 seconds. These improvements provide strong evidence that semantic retrieval technologies can be effectively applied within strict aviation compliance environments, thereby significantly lowering operational workload and enhancing efficiency in real-world multilingual MRO workflows. <div>
arXiv:2511.15383v1 Announce Type: new 
Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEPO: Dual-Efficiency Preference Optimization for LLM Agents</title>
<link>https://arxiv.org/abs/2511.15392</link>
<guid>https://arxiv.org/abs/2511.15392</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dual-efficiency, chain of thought, DEPO, token and step optimization<br /><br />Summary: Recent developments in large language models (LLMs) have enhanced their capabilities in reasoning and decision-making when used as agents. However, improved reasoning often results in longer chains of thought (CoT), which reduces interaction efficiency in practical applications. To address this, the authors introduce the concept of dual-efficiency, defined as (i) step-level efficiency, focusing on minimizing the number of tokens per step, and (ii) trajectory-level efficiency, aiming to reduce the total number of steps required to complete a task. Using this framework, they propose DEPO (dual-efficiency preference optimization), a method that jointly optimizes for both succinct responses and fewer action steps. Experiments conducted on WebShop and BabyAI benchmarks demonstrate that DEPO can reduce token usage by up to 60.9% and step counts by up to 26.9%, while simultaneously improving overall task performance by as much as 29.3%. Furthermore, DEPO shows strong generalization capabilities across three out-of-domain math benchmarks and maintains its efficiency advantages even when trained on just 25% of the available data. These results indicate that DEPO offers a scalable and effective solution for enhancing the efficiency and performance of LLM agents in real-world scenarios. <div>
arXiv:2511.15392v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have greatly improved their reasoning and decision-making abilities when deployed as agents. Richer reasoning, however, often comes at the cost of longer chain of thought (CoT), hampering interaction efficiency in real-world scenarios. Nevertheless, there still lacks systematic definition of LLM agent efficiency, hindering targeted improvements. To this end, we introduce dual-efficiency, comprising (i) step-level efficiency, which minimizes tokens per step, and (ii) trajectory-level efficiency, which minimizes the number of steps to complete a task. Building on this definition, we propose DEPO, a dual-efficiency preference optimization method that jointly rewards succinct responses and fewer action steps. Experiments on WebShop and BabyAI show that DEPO cuts token usage by up to 60.9% and steps by up to 26.9%, while achieving up to a 29.3% improvement in performance. DEPO also generalizes to three out-of-domain math benchmarks and retains its efficiency gains when trained on only 25% of the data. Our project page is at https://opencausalab.github.io/DEPO.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework</title>
<link>https://arxiv.org/abs/2511.15408</link>
<guid>https://arxiv.org/abs/2511.15408</guid>
<content:encoded><![CDATA[
<div> Keywords: Creative Natural Language Generation, Large Language Models, Chinese baby naming, multi-agent optimization, aesthetics  

<br /><br />Summary: This paper addresses two main challenges in Creative Natural Language Generation (CNLG) with Large Language Models (LLMs): multi-objective flexibility and interpretive complexity. Existing methods struggle to fulfill personalized, fine-grained, and pluralistic user requirements while also understanding and interpreting implicit meanings to enhance creativity. The authors focus on Chinese baby naming as a representative short-form CNLG task that requires adherence to explicit constraints such as name length, semantics, and anthroponymy, along with generating meaningful aesthetic explanations. To overcome these difficulties, they propose NAMeGEn, a novel multi-agent optimization framework that iteratively cycles through objective extraction, name generation, and evaluation to satisfy diverse user requirements and produce accurate explanations. To support this task, the authors also develop a classical Chinese poetry corpus comprising over 17,000 poems aimed at improving the aesthetic quality of generated names. Additionally, they introduce CBNames, a new benchmark with specialized metrics tailored for this domain. Extensive experiments demonstrate that NAMeGEn outperforms six baseline models based on various LLM backbones without requiring any additional training. The framework successfully generates creative, personalized Chinese names with meaningful explanations, advancing the capabilities of CNLG in short-form text generation. <div>
arXiv:2511.15408v1 Announce Type: new 
Abstract: Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building Robust and Scalable Multilingual ASR for Indian Languages</title>
<link>https://arxiv.org/abs/2511.15418</link>
<guid>https://arxiv.org/abs/2511.15418</guid>
<content:encoded><![CDATA[
<div> Keywords: ASR adaptation, Multi-Decoder architecture, phonemic Common Label Set, multilingual systems, language and dialect identification<br /><br />Summary:<br /><br />This paper presents the ASR systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge focusing on language and dialect prediction across 8 languages and 33 dialects. The team participated in two tracks, Track 1 and Track 2, which limited external data usage and required the development of multilingual systems from scratch. A novel training approach was introduced, leveraging a Multi-Decoder architecture that utilizes a phonemic Common Label Set (CLS) as an intermediate representation to enhance model performance beyond the baseline in phonemic space. The authors also explored various techniques to preserve the improvement achieved in phonetic representation while converting outputs back to grapheme form, addressing challenges related to decoding and transcription accuracy. Their system outperformed the baseline in three languages under Track 2 concerning Word Error Rate (WER) and Character Error Rate (CER). Moreover, their approach yielded the highest accuracy in both language identification and dialect identification among all participant teams in Track 2, demonstrating the efficacy of their multi-decoder CLS-based method for multilingual ASR adaptation in constrained data settings. <div>
arXiv:2511.15418v1 Announce Type: new 
Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering</title>
<link>https://arxiv.org/abs/2511.15424</link>
<guid>https://arxiv.org/abs/2511.15424</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, text clustering, dynamic memory, dual-prompt strategy, end-to-end framework<br /><br />Summary:<br /><br />This paper introduces LLM-MemCluster, a novel framework that leverages Large Language Models (LLMs) to perform text clustering entirely within the LLM architecture, addressing key limitations of existing methods. Unlike traditional approaches that require complex external modules and pipelines, LLM-MemCluster is fully end-to-end and tuning-free. A central innovation is the incorporation of dynamic memory, which provides the model with stateful awareness, allowing iterative refinement during clustering. Additionally, the framework employs a dual-prompt strategy that enables the LLM to reason about and determine the appropriate number of clusters dynamically, tackling the challenge of managing cluster granularity. The proposed approach was evaluated on several benchmark datasets, demonstrating significant and consistent performance improvements over strong baseline methods. By unifying clustering into a truly LLM-native task, LLM-MemCluster offers an effective, interpretable, and streamlined paradigm for unsupervised text clustering driven by semantic understanding. This work suggests that memory-augmented and prompt-engineered LLMs hold great promise for advancing text clustering without reliance on external processing components. <div>
arXiv:2511.15424v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis</title>
<link>https://arxiv.org/abs/2511.15512</link>
<guid>https://arxiv.org/abs/2511.15512</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Processing Data Structure (LPDS), pelican nlp, reproducibility, linguistic data standardisation, Python package  

<br /><br />Summary:  
This article addresses the challenges in AI-based language processing, particularly the lack of standardisation in organizing and sharing linguistic data, as well as the absence of reproducible processing methodologies. To tackle these issues, the authors propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), which introduces a systematic folder structure and file naming conventions tailored for linguistic research. Additionally, they present pelican nlp, a modular and extensible Python package designed to facilitate streamlined language processing workflows. Pelican nlp handles various stages from initial data cleaning and task-specific preprocessing to advanced extraction of linguistic and acoustic features, including semantic embeddings and prosodic metrics. The entire processing pipeline can be configured through a single shareable configuration file, enabling reproducibility and ease of use. When executed on LPDS-formatted data, pelican nlp produces standardized outputs that include preprocessed language data, feature extraction results, and aggregated metrics. By combining LPDS and pelican nlp, the authors offer an end-to-end pipeline that promotes methodological transparency and enhances reproducibility in linguistic data analysis. This framework aims to set a foundation for future standardisation efforts in language processing research. <div>
arXiv:2511.15512v1 Announce Type: new 
Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Evaluation of Russian-language Architectures</title>
<link>https://arxiv.org/abs/2511.15552</link>
<guid>https://arxiv.org/abs/2511.15552</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, Russian language, evaluation benchmark, multimodal abilities, Mera Multi<br /><br />Summary:<br /><br />1. The paper introduces Mera Multi, an open multimodal evaluation framework specifically designed for Russian-speaking large language models, addressing the lack of such benchmarks for the Russian language.  
2. Mera Multi is instruction-based and covers multiple modalities including text, image, audio, and video, consisting of 18 newly constructed tasks tailored to evaluate both general-purpose and modality-specific architectures like image-to-text, video-to-text, and audio-to-text.  
3. The authors propose a universal taxonomy of multimodal abilities, providing a structured approach to assess diverse multimodal skills within models.  
4. All 18 datasets were created from scratch with careful attention to Russian cultural and linguistic peculiarities. They use unified prompts and standardized metrics to ensure consistency across tasks.  
5. Baseline results are presented for a variety of closed-source and open-source models. The methodology also includes mechanisms for preventing benchmark leakage, such as watermarking and licensing for private dataset subsets, ensuring the benchmark's integrity and replicability.  
6. While focused on Russian, the benchmark's methodology is designed to be replicable and adaptable to other typologically diverse languages, especially within the Slavic language family. <div>
arXiv:2511.15552v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning</title>
<link>https://arxiv.org/abs/2511.15574</link>
<guid>https://arxiv.org/abs/2511.15574</guid>
<content:encoded><![CDATA[
<div> Keywords: language acquisition, LLMs, HSKBenchmark, Chinese SLA, curriculum-tuning

<br /><br />Summary: 
Language acquisition is essential for understanding human language intelligence and improving the interpretability of large language models (LLMs). However, ethical and practical constraints limit experiments with human learners' language inputs, especially in Chinese second language acquisition (SLA). To address this, the paper introduces HSKBenchmark, a comprehensive benchmark designed for staged modeling and writing assessment of LLMs focused on Chinese SLA. It spans HSK levels 3 to 6, featuring authentic textbooks with 6.76 million tokens, 16,000 synthetic instruction samples, 30 test topics, and a robust evaluation system. The authors propose a curriculum-tuning framework to simulate human learning trajectories, facilitating training from beginner to advanced levels. An evaluation system assesses level-based grammar coverage, writing errors, lexical and syntactic complexity, and overall scoring. Furthermore, HSKAgent is developed, fine-tuned on 10,000 learner compositions. Experimental results indicate that HSKBenchmark effectively models Chinese SLA and serves as a reliable dynamic writing assessment tool, with fine-tuned LLMs performing comparably to advanced human learners and displaying human-like acquisition traits. The resources provided, including HSKBenchmark, HSKAgent, and checkpoints, aim to support future research on language acquisition modeling and LLMs interpretability. Code and data are publicly accessible at: https://github.com/CharlesYang030/HSKB. <div>
arXiv:2511.15574v1 Announce Type: new 
Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenisation over Bounded Alphabets is Hard</title>
<link>https://arxiv.org/abs/2511.15709</link>
<guid>https://arxiv.org/abs/2511.15709</guid>
<content:encoded><![CDATA[
<div> Tokenisation, NP-complete, bounded alphabets, approximation, computational complexity<br /><br />Summary:<br /><br />This article investigates the computational complexity of tokenisation when applied over bounded alphabets, addressing a limitation in previous work that considered only unboundedly large alphabets. The authors study two main variants of tokenisation: bottom-up tokenisation, which involves selecting a sequence of merge operations, and direct tokenisation, which requires choosing a vocabulary to compress a dataset optimally. It is established that hardness results proven for an n-ary alphabet automatically extend to alphabets of any larger size. Crucially, the paper proves that both tokenisation variants remain NP-complete even when restricted to binary alphabets and that no polynomial-time approximation scheme exists for these cases unless P=NP. Furthermore, the direct tokenisation problem is shown to be NP-complete even on unary alphabets, reinforcing that the intractability arises from fundamental computational barriers rather than from large or complex alphabets. These findings help explain why popular practical tokenisers like BPE and UnigramLM are heuristic methods rather than exact algorithms. The study ultimately highlights the significance of developing approximation algorithms as a promising avenue for advancing tokenisation research. <div>
arXiv:2511.15709v1 Announce Type: new 
Abstract: Recent works have shown that tokenisation is NP-complete. However, these works assume tokenisation is applied to inputs with unboundedly large alphabets -- an unrealistic assumption, given that in practice tokenisers operate over fixed-size alphabets, such as bytes or Unicode characters. We close this gap by analysing tokenisation over bounded $n$-ary alphabets, considering two natural variants: bottom-up tokenisation and direct tokenisation, where we must, respectively, select a sequence of merge operations or a vocabulary whose application optimally compresses a dataset. First, we note that proving hardness results for an $n$-ary alphabet proves the same results for alphabets of any larger size. We then prove that even with binary alphabets, both variants are not only NP-complete, but admit no polynomial-time approximation scheme (unless P=NP). We further show that direct tokenisation remains NP-complete even when applied to unary alphabets. While unary alphabets may not be practically useful, this result establishes that the computational intractability of tokenisation is not an artifact of large alphabets or complex constructions, but a fundamental barrier. Overall, our results explain why practical algorithms such as BPE and UnigramLM are heuristic, and points toward approximation algorithms being an important path going forward for tokenisation research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Agricultural Research: A RAG-Based Approach to Mycorrhizal Fungi Information</title>
<link>https://arxiv.org/abs/2511.14765</link>
<guid>https://arxiv.org/abs/2511.14765</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, arbuscular mycorrhizal fungi, sustainable agriculture, vector embeddings, experimental metadata<br /><br />Summary:<br /><br />1. The study introduces a Retrieval-Augmented Generation (RAG) system integrating neural information retrieval with generative language modeling to improve contextual accuracy and factual reliability in natural language processing applications. 2. Unlike traditional Large Language Models limited by static training data, this system dynamically incorporates external domain-specific knowledge, overcoming temporal and disciplinary constraints. 3. The RAG-enabled framework targets the Mycophyto domain, focusing on arbuscular mycorrhizal fungi (AMF), which are crucial for sustainable agriculture by enhancing nutrient uptake, plant stress resilience, and soil health. 4. The system employs a dual-layered strategy combining semantic retrieval of agronomy and biotechnology literature through vector embeddings and structured extraction of experimental metadata like inoculation methods, spore densities, soil parameters, and yield outcomes. 5. High-performance vector databases facilitate scalable, near real-time retrieval of evolving scientific literature. 6. Empirical evaluation confirms the pipeline’s ability to retrieve and synthesize relevant information on AMF interactions with crops such as tomato (Solanum lycopersicum). 7. The framework demonstrates AI-driven knowledge discovery’s potential to accelerate agroecological innovations and support informed decision-making in sustainable farming systems. <div>
arXiv:2511.14765v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) represents a transformative approach within natural language processing (NLP), combining neural information retrieval with generative language modeling to enhance both contextual accuracy and factual reliability of responses. Unlike conventional Large Language Models (LLMs), which are constrained by static training corpora, RAG-powered systems dynamically integrate domain-specific external knowledge sources, thereby overcoming temporal and disciplinary limitations. In this study, we present the design and evaluation of a RAG-enabled system tailored for Mycophyto, with a focus on advancing agricultural applications related to arbuscular mycorrhizal fungi (AMF). These fungi play a critical role in sustainable agriculture by enhancing nutrient acquisition, improving plant resilience under abiotic and biotic stresses, and contributing to soil health. Our system operationalizes a dual-layered strategy: (i) semantic retrieval and augmentation of domain-specific content from agronomy and biotechnology corpora using vector embeddings, and (ii) structured data extraction to capture predefined experimental metadata such as inoculation methods, spore densities, soil parameters, and yield outcomes. This hybrid approach ensures that generated responses are not only semantically aligned but also supported by structured experimental evidence. To support scalability, embeddings are stored in a high-performance vector database, allowing near real-time retrieval from an evolving literature base. Empirical evaluation demonstrates that the proposed pipeline retrieves and synthesizes highly relevant information regarding AMF interactions with crop systems, such as tomato (Solanum lycopersicum). The framework underscores the potential of AI-driven knowledge discovery to accelerate agroecological innovation and enhance decision-making in sustainable farming systems.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster-based Adaptive Retrieval: Dynamic Context Selection for RAG Applications</title>
<link>https://arxiv.org/abs/2511.14769</link>
<guid>https://arxiv.org/abs/2511.14769</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, adaptive retrieval, clustering, query-document similarity, large language models<br /><br />Summary:<br /><br />This paper addresses the challenge of optimizing document retrieval in Retrieval-Augmented Generation (RAG) systems, which enhance large language models by incorporating relevant external documents. The authors highlight that static top-k retrieval approaches do not account for the variability in query scope—focused queries need fewer documents, while broader queries require more extensive context. To solve this, they propose Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the ideal number of documents to retrieve by analyzing clustering patterns of query-document similarity scores. CAR identifies a transition point where highly relevant documents cluster tightly before similarity sharply decreases, thus adaptively setting a cut-off tailored to the query’s complexity. Evaluations on Coinbase's CDP corpus and MultiHop-RAG benchmarks show CAR consistently selects optimal retrieval sizes and achieves superior TES scores compared to fixed top-k baselines. In downstream RAG tasks, CAR reduces large language model token consumption by 60%, cuts end-to-end latency by 22%, and lowers hallucination rates by 10%, all while maintaining answer relevance. The integration of CAR into Coinbase's virtual assistant has resulted in a 200% increase in user engagement, demonstrating its practical impact in real-world applications. <div>
arXiv:2511.14769v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by pulling in external material, document, code, manuals, from vast and ever-growing corpora, to effectively answer user queries. The effectiveness of RAG depends significantly on aligning the number of retrieved documents with query characteristics: narrowly focused queries typically require fewer, highly relevant documents, whereas broader or ambiguous queries benefit from retrieving more extensive supporting information. However, the common static top-k retrieval approach fails to adapt to this variability, resulting in either insufficient context from too few documents or redundant information from too many. Motivated by these challenges, we introduce Cluster-based Adaptive Retrieval (CAR), an algorithm that dynamically determines the optimal number of documents by analyzing the clustering patterns of ordered query-document similarity distances. CAR detects the transition point within similarity distances, where tightly clustered, highly relevant documents shift toward less pertinent candidates, establishing an adaptive cut-off that scales with query complexity. On Coinbase's CDP corpus and the public MultiHop-RAG benchmark, CAR consistently picks the optimal retrieval depth and achieves the highest TES score, outperforming every fixed top-k baseline. In downstream RAG evaluations, CAR cuts LLM token usage by 60%, trims end-to-end latency by 22%, and reduces hallucinations by 10% while fully preserving answer relevance. Since integrating CAR into Coinbase's virtual assistant, we've seen user engagement jump by 200%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization</title>
<link>https://arxiv.org/abs/2511.14846</link>
<guid>https://arxiv.org/abs/2511.14846</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Multi-turn Reasoning, Policy Optimization, Reward Shaping  

<br /><br />Summary: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) is challenging due to the limitations of existing reinforcement learning (RL) methods that use coarse-grained, trajectory-level rewards. These coarse rewards provide insufficient learning signals for complex, multi-turn reasoning, often causing training stagnation. To overcome this, the authors propose Group Turn Policy Optimization (GTPO), a novel RL algorithm designed for multi-turn TIR tasks. GTPO introduces three main innovations: first, turn-level reward assignment, which offers fine-grained feedback specifically tailored to each interaction turn; second, return-based advantage estimation, calculating normalized discounted returns as advantages to improve learning signal quality; and third, self-supervised reward shaping, which utilizes self-supervision from generated code to enrich sparse binary outcome rewards. The authors evaluate GTPO comprehensively across various reasoning benchmarks, demonstrating that it improves performance by 3.0% on average compared to the previous state-of-the-art method, Group Relative Policy Optimization (GRPO). These results highlight GTPO’s effectiveness in enhancing LLM training for complex mathematical and tool-integrated reasoning tasks, potentially advancing their applicability in real-world multi-step problem-solving scenarios. <div>
arXiv:2511.14846v1 Announce Type: cross 
Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis</title>
<link>https://arxiv.org/abs/2511.14900</link>
<guid>https://arxiv.org/abs/2511.14900</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, dermatological diagnosis, diagnostic rationale, reinforcement learning, supervised fine-tuning

<br /><br />Summary:  
The article addresses key limitations in vision-language models (VLMs) applied to dermatological diagnosis, particularly challenges related to data heterogeneity, lack of grounded diagnostic rationales, and limited model scalability and generalization. To overcome these issues, the authors propose SkinR1, a novel dermatological VLM that integrates deep, textbook-based reasoning with reinforcement learning (RL) to enhance diagnostic performance and reliability. First, SkinR1 includes a textbook-based reasoning generator that creates high-quality, hierarchy-aware, and differential-diagnosis-informed trajectories, providing expert-level reasoning supervision. Second, these generated trajectories are used to fine-tune the model via supervised fine-tuning (SFT), grounding the model’s diagnostic reasoning capabilities. Third, the approach incorporates a novel RL paradigm that leverages the hierarchical structure of diseases to help the model transfer learned reasoning patterns from small, densely annotated datasets to large, sparsely annotated ones, improving generalization. Extensive experiments across multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy compared to existing methods. An ablation study further confirms the critical role of supervised fine-tuning in establishing a strong reasoning foundation within the model. This comprehensive framework addresses the major shortcomings of prior models, advancing the clinical utility and trustworthiness of dermatological VLMs. <div>
arXiv:2511.14900v1 Announce Type: cross 
Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding</title>
<link>https://arxiv.org/abs/2511.14936</link>
<guid>https://arxiv.org/abs/2511.14936</guid>
<content:encoded><![CDATA[
<div> Differential Privacy, Clinical NLP, Knowledge Distillation, Diagnostic Coding, Privacy-Utility Trade-off<br /><br />Summary:  
1. Large language models trained on clinical text pose risks of exposing sensitive patient information.  
2. Differential privacy (DP) methods, while enhancing privacy, often significantly reduce the diagnostic accuracy of models, limiting their practical deployment in clinical settings.  
3. This study provides the first systematic, head-to-head comparison of four different training pipelines aimed at automated diagnostic coding from hospital discharge summaries, all using identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes.  
4. Tested at moderate and relaxed privacy budgets (ε values of 4 and 6), knowledge distillation from DP-trained teacher models outperforms both direct DP-SGD training and DP-synthetic data training.  
5. Knowledge distillation recovers up to 63% of the non-private model’s performance while maintaining strong empirical privacy guarantees, as indicated by membership-inference AUC values near 0.5.  
6. These results reveal significant differences in the privacy-utility trade-offs of different architectures and establish knowledge distillation as the most effective and practical method for privacy-preserving clinical NLP applications. <div>
arXiv:2511.14936v1 Announce Type: cross 
Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Multimodal Large Language Models on Vertically Written Japanese Text</title>
<link>https://arxiv.org/abs/2511.15059</link>
<guid>https://arxiv.org/abs/2511.15059</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Japanese OCR, vertical writing, synthetic dataset, visual document understanding<br /><br />Summary:<br /><br />This study addresses the challenge of reading vertically written Japanese text using Multimodal Large Language Models (MLLMs), which have been advancing rapidly for visual document understanding. Recognizing vertically oriented Japanese text is critical since many Japanese documents use this writing style, yet current research on this aspect is limited. To evaluate and improve reading capabilities, the authors generated a synthetic Japanese OCR dataset with both horizontal and vertical text renderings. This dataset was utilized for fine-tuning the models and served as a benchmark for evaluation. Additionally, a real-world evaluation dataset containing vertically written Japanese documents was compiled. The findings reveal that existing MLLMs underperform on vertically written Japanese text compared to horizontal text. Importantly, fine-tuning models on the synthesized OCR dataset led to substantial improvements in their ability to process vertical writing, overcoming prior limitations. The work contributes valuable datasets and code to the public domain, facilitating further research and development in Japanese document image understanding and vertical text recognition using MLLMs. <div>
arXiv:2511.15059v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression</title>
<link>https://arxiv.org/abs/2511.15069</link>
<guid>https://arxiv.org/abs/2511.15069</guid>
<content:encoded><![CDATA[
<div> ProRAC, Reasoning about Actions and Change, neuro-symbolic, large language models, benchmarks<br /><br />Summary: ProRAC (Progression-based Reasoning about Actions and Change) is a novel neuro-symbolic framework designed to address reasoning about actions and change (RAC) problems by integrating large language models (LLMs). The framework first extracts essential RAC components such as actions and questions from the input problem, enabling it to understand the key elements involved. ProRAC then progressively executes each identified action step-by-step to compute the final state of the system, effectively simulating the progression of changes over time. Once the progressed final state is derived, the framework evaluates the posed query against this state to generate an accurate answer. The approach was tested on multiple RAC benchmarks spanning different domains and types of tasks, demonstrating robust and strong performance regardless of the LLM backbone employed. This indicates the framework’s flexibility and generalizability across various reasoning scenarios involving actions and changes. Overall, ProRAC leverages the reasoning strengths of both symbolic progression and neural language models to advance the solving of complex RAC problems successfully. <div>
arXiv:2511.15069v1 Announce Type: cross 
Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents</title>
<link>https://arxiv.org/abs/2511.15074</link>
<guid>https://arxiv.org/abs/2511.15074</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, feature engineering, multi-agent system, knowledge integration, automatic feature extraction  

<br /><br />Summary:  
This paper presents Rogue One, an innovative framework leveraging Large Language Models (LLMs) for automatic feature extraction in tabular data tasks. The framework consists of three specialized agents—Scientist, Extractor, and Tester—that work collaboratively and iteratively to discover, generate, and validate predictive features. Rogue One addresses limitations in prior AutoFE approaches by employing a decentralized multi-agent architecture rather than a monolithic LLM, enhancing flexibility and specialization. It introduces a sophisticated qualitative feedback mechanism that goes beyond simple accuracy metrics, enabling a more nuanced assessment of feature utility. The flooding-pruning strategy further optimizes the balance between feature exploration and exploitation, preventing overfitting and promoting meaningful feature discovery. A key innovation is the integration of external domain knowledge through a retrieval-augmented generation (RAG) system, which empowers the model to produce semantically meaningful and interpretable features. The framework’s efficacy is demonstrated on 19 classification and 9 regression datasets, where it significantly outperforms state-of-the-art baselines. Additionally, Rogue One shows promise as a scientific discovery tool by generating novel hypotheses, such as identifying a potential new biomarker in cardiovascular data, highlighting its contribution beyond predictive performance to practical interpretability and domain relevance. <div>
arXiv:2511.15074v1 Announce Type: cross 
Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CASTELLA: Long Audio Dataset with Captions and Temporal Boundaries</title>
<link>https://arxiv.org/abs/2511.15131</link>
<guid>https://arxiv.org/abs/2511.15131</guid>
<content:encoded><![CDATA[
<div> Keywords: CASTELLA, audio moment retrieval, benchmark dataset, human-annotated, model performance<br /><br />Summary:<br /><br />1. CASTELLA is introduced as a new human-annotated audio benchmark specifically designed for the task of audio moment retrieval (AMR), addressing the lack of real-world data and established benchmarks in this area.<br />2. Prior AMR research mainly relied on synthetic datasets for training, with evaluations conducted on datasets containing fewer than 100 samples, leading to less reliable performance results.<br />3. CASTELLA significantly expands the scale, consisting of 1,009 training, 213 validation, and 640 test audio recordings, making it 24 times larger than previous datasets.<br />4. The authors establish a baseline AMR model using CASTELLA and demonstrate that fine-tuning a pre-trained model (initially trained on synthetic data) on CASTELLA improves performance by 10.4 points in Recall1@0.7 compared to training solely on synthetic data.<br />5. The dataset is publicly accessible for research and application development at https://h-munakata.github.io/CASTELLA-demo/, promoting more reliable AMR system evaluations and real-world application readiness. <div>
arXiv:2511.15131v1 Announce Type: cross 
Abstract: We introduce CASTELLA, a human-annotated audio benchmark for the task of audio moment retrieval (AMR). Although AMR has various useful potential applications, there is still no established benchmark with real-world data. The early study of AMR trained the model with solely synthetic datasets. Moreover, the evaluation is based on annotated dataset of fewer than 100 samples. This resulted in less reliable reported performance. To ensure performance for applications in real-world environments, we present CASTELLA, a large-scale manually annotated AMR dataset. CASTELLA consists of 1,009, 213, and 640 audio recordings for train, valid, and test split, respectively, which is 24 times larger than the previous dataset. We also establish a baseline model for AMR using CASTELLA. Our experiments demonstrate that a model fine-tuned on CASTELLA after pre-training on the synthetic data outperformed a model trained solely on the synthetic data by 10.4 points in Recall1@0.7. CASTELLA is publicly available in https://h-munakata.github.io/CASTELLA-demo/.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation</title>
<link>https://arxiv.org/abs/2511.15159</link>
<guid>https://arxiv.org/abs/2511.15159</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical training, feedback generation, Instrument-Action-Target, video analysis, GPT-4o<br /><br />Summary: High-quality intraoperative feedback from surgical trainers is crucial for improving trainee performance and skill acquisition. This work introduces a structure-aware pipeline that learns a surgical action ontology from 33 real trainer-to-trainee surgical transcripts to condition feedback generation. The authors first mine Instrument-Action-Target (IAT) triplets from real-world feedback texts and cluster different expressions into normalized categories to standardize representations. Next, they fine-tune a video-to-IAT recognition model that incorporates surgical procedure context, task context, and detailed temporal instrument motion for better accuracy. Results show improved Area Under the Curve (AUC) for Instrument, Action, and Tissue recognition tasks, with notable performance gains from injecting context and temporal tracking. For feedback text generation evaluated on a fidelity rubric, GPT-4o from video alone scores 2.17 out of 5, while conditioning on IAT triplets improves the score to 2.44 (+12.4%) and doubles the portion of admissible (score ≥3) feedback from 21% to 42%. Traditional text-similarity metrics also improve, with word error rate decreasing by 15-31% and ROUGE scores increasing by 9-64%. Grounding feedback generation in explicit IAT structures enhances fidelity, provides clinician-verifiable rationales, and supports auditable use in surgical training environments. <div>
arXiv:2511.15159v1 Announce Type: cross 
Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M, Toolchain and Language for Reusable Model Compilation</title>
<link>https://arxiv.org/abs/2511.15257</link>
<guid>https://arxiv.org/abs/2511.15257</guid>
<content:encoded><![CDATA[
<div> Multi-target compilation, actor model, discrete-event scheduling, model-driven engineering, system modeling<br /><br />Summary:<br /><br />1. The paper addresses the challenge of efficiently and safely developing complex software-driven systems that combine distributed concurrent computations with physical environmental interactions. 2. It highlights the need for deriving multiple specialized models—such as those for simulation, deployment, and formal verification—from a single high-level system model, noting that each target model usually requires distinct formalisms and platforms. 3. Traditional compilers translate programs into executable code, whereas model compilers aim to generate multiple heterogeneous target artifacts from a single source model, a capability often missing in existing modeling languages designed with narrow targets. 4. The authors introduce M, a new textual, grammar-driven modeling language and toolchain based on the actor model extended with discrete-event scheduling to represent system entities, message interactions, and time- or state-triggered behaviors. 5. M supports multi-target compilation by preserving semantic conformance across generated targets and can serve as a middle language for anchoring other modeling languages to leverage its compilation framework, advancing model-driven engineering for complex, concurrent, and time-aware systems. <div>
arXiv:2511.15257v1 Announce Type: cross 
Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartEditor: A Reinforcement Learning Framework for Robust Chart Editing</title>
<link>https://arxiv.org/abs/2511.15266</link>
<guid>https://arxiv.org/abs/2511.15266</guid>
<content:encoded><![CDATA[
<div> Keywords: Chart Editing, Benchmark, ChartEditVista, Evaluation Metrics, Reinforcement Learning<br /><br />Summary: This work addresses limitations in existing chart editing benchmarks, which often suffer from limited data diversity and unrealistic assumptions such as access to original chart code. To overcome these challenges, the authors introduce ChartEditVista, a large-scale, comprehensive benchmark dataset containing 7,964 samples across 31 chart categories. ChartEditVista uniquely provides only the original chart image and natural language editing instructions, excluding the original chart code, thereby better reflecting real-world scenarios. The benchmark covers a broad range of editable chart elements and editing instructions, generated through a fully automated pipeline that creates, modifies, and verifies charts to ensure high-quality data. In addition, the study proposes two novel fine-grained, rule-based evaluation metrics: the layout metric, which assesses graphical component positions, sizes, and colors, and the text metric, which evaluates textual content along with font styling. Building on this benchmark, the authors develop ChartEditor, a model trained with reinforcement learning that incorporates a novel rendering reward to maintain both code executability and visual fidelity simultaneously. Extensive experiments and human evaluations demonstrate that ChartEditVista provides robust evaluation capabilities, and that ChartEditor consistently outperforms models of comparable or larger scale on chart editing tasks. <div>
arXiv:2511.15266v1 Announce Type: cross 
Abstract: Chart editing reduces manual effort in visualization design. Typical benchmarks limited in data diversity and assume access to complete chart code, which is seldom in real-world scenarios. To address this gap, we present ChartEditVista, a comprehensive benchmark consisting of 7,964 samples spanning 31 chart categories. It encompasses diverse editing instructions and covers nearly all editable chart elements. The inputs in ChartEditVista include only the original chart image and natural language editing instructions, without the original chart codes. ChartEditVista is generated through a fully automated pipeline that produces, edits, and verifies charts, ensuring high-quality chart editing data. Besides, we introduce two novel fine-grained, rule-based evaluation metrics: the layout metric, which evaluates the position, size and color of graphical components; and the text metric, which jointly assesses textual content and font styling. Building on top of ChartEditVista, we present ChartEditor, a model trained using a reinforcement learning framework that incorporates a novel rendering reward to simultaneously enforce code executability and visual fidelity. Through extensive experiments and human evaluations, we demonstrate that ChartEditVista provides a robust evaluation, while ChartEditor consistently outperforms models with similar-scale and larger-scale on chart editing tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs</title>
<link>https://arxiv.org/abs/2511.15323</link>
<guid>https://arxiv.org/abs/2511.15323</guid>
<content:encoded><![CDATA[
<div> Hardware synthesis, high-level synthesis, e-graph, scheduling optimization, FPGA acceleration

<br /><br />Summary:  
1. Current hardware synthesis methods, including advanced high-level synthesis (HLS) tools, suffer from sequential and separate handling of implementation selection and scheduling, leading to inefficient designs that underutilize FPGA heterogeneous architectures.  
2. Implementation selection is performed by heuristic pattern matching that ignores the effects on scheduling, while scheduling is done on these fixed selections using inaccurate delay estimates, missing optimization opportunities for FPGA features like DSP blocks.  
3. SkyEgg is introduced as a novel framework that jointly optimizes implementation selection and scheduling using the e-graph data structure, where both algebraic transformations and hardware implementation options are uniformly represented as rewrite rules to explore the complete design space.  
4. The process involves constructing an e-graph from the input program, applying algebraic and implementation rewrite rules via equality saturation, and then formulating a mixed-integer linear programming (MILP) problem that optimizes jointly over the saturated e-graph.  
5. SkyEgg supports both exact MILP solving and a scalable ASAP heuristic for larger designs, and evaluation on benchmarks targeting Xilinx Kintex UltraScale+ FPGAs shows an average speedup of 3.01x over Vitis HLS, with gains up to 5.22x on complex expressions. <div>
arXiv:2511.15323v1 Announce Type: cross 
Abstract: Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.
  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search</title>
<link>https://arxiv.org/abs/2511.15443</link>
<guid>https://arxiv.org/abs/2511.15443</guid>
<content:encoded><![CDATA[
<div> Keywords: Dense retrieval, filter bubble, CroPS, Hierarchical Label Assignment, short-video search<br /><br />Summary: This paper addresses the filter bubble problem in dense retrieval systems, particularly in short-video platforms, where training on historical user interactions limits diversity by excluding unseen but relevant content. To mitigate this, the authors propose CroPS (Cross-Perspective Positive Samples), a novel data engine that introduces diverse positive training samples drawn from multiple perspectives: user query reformulation behavior (query-level), engagement data from recommendation streams (system-level), and synthesized knowledge from large language models (knowledge-level). To effectively leverage these heterogeneous signals, they design a Hierarchical Label Assignment (HLA) strategy coupled with a new H-InfoNCE loss function, enabling fine-grained and relevance-aware optimization during training. The approach is validated through extensive experiments on Kuaishou Search, a large-scale commercial short-video search platform, demonstrating significant performance improvements in both offline benchmarks and live A/B testing scenarios. Deployment of CroPS at Kuaishou Search serves hundreds of millions of users daily, achieving superior retrieval quality and notably reducing query reformulations, thus enhancing user experience and retrieval accuracy in an industrial environment. <div>
arXiv:2511.15443v1 Announce Type: cross 
Abstract: Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer-Use Agents as Judges for Generative User Interface</title>
<link>https://arxiv.org/abs/2511.15567</link>
<guid>https://arxiv.org/abs/2511.15567</guid>
<content:encoded><![CDATA[
<div> Computer-Use Agents, Automatic GUI Design, Coder-CUA Collaboration, AUI-Gym Benchmark, Task Solvability<br /><br />Summary:<br /><br />This paper addresses the challenge of designing Graphical User Interfaces (GUIs) optimized not just for humans but for Computer-Use Agents (CUA), which autonomously operate digital environments. The authors highlight how current GUIs prioritize aesthetics and human usability, potentially hindering efficient agent operation. To explore better agent-native interface design, they introduce AUI-Gym, a comprehensive benchmark encompassing 52 applications from various domains, with 1560 synthesized tasks created using language models to simulate real-world scenarios. They implement a verifier to programmatically confirm task executability within each environment, ensuring reliability. The core contribution is a novel Coder-CUA collaboration framework where a Coder acts as a designer generating and revising GUIs, while the CUA serves as a judge evaluating functionality and guiding iterative improvements. Unlike traditional design metrics, success here is measured by the solvability of tasks and the agent’s navigation success rate within the GUI. Additionally, the authors develop a CUA Dashboard that distills complex navigation histories into interpretable visual summaries, providing actionable feedback for redesign. This framework promotes agent-native efficiency and shifts agents from passive users to active participants in digital environments. The authors also publicly release their code and dataset to facilitate further research. <div>
arXiv:2511.15567v1 Announce Type: cross 
Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.15605</link>
<guid>https://arxiv.org/abs/2511.15605</guid>
<content:encoded><![CDATA[
<div> Vision-Language-Action, Reinforcement Learning, Reward Sparsity, Latent World Representations, Self-Referential Policy Optimization<br /><br />Summary:<br /><br />This paper addresses the limitations of Vision-Language-Action (VLA) models in robotic manipulation, particularly their dependence on expert demonstrations that cause bias and restrict performance. To improve learning efficiency, the authors propose a novel Reinforcement Learning (RL) framework called Self-Referential Policy Optimization (SRPO). Unlike existing VLA-RL methods that suffer from reward sparsity by using binary success signals, SRPO leverages the model’s own successful trajectories generated during training as references to assign progress-based rewards to failed attempts. This approach eliminates the need for external demonstrations or manual reward engineering. A key innovation is the use of latent world representations from a world model’s compressed, transferable encodings to robustly measure behavioral progress across diverse environments, avoiding reliance on raw pixels or domain-specific tuning. Evaluations on the LIBERO benchmark show SRPO's superior efficiency and effectiveness, boosting success rates from a supervised baseline of 48.9% to an impressive 99.2% within only 200 RL steps, marking a 103% relative improvement without extra supervision. The method also demonstrates strong robustness, achieving a 167% performance increase on the more challenging LIBERO-Plus benchmark. Overall, SRPO offers a scalable and generalizable approach to overcoming reward sparsity in VLA robotic learning tasks. <div>
arXiv:2511.15605v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Think and When to Look: Uncertainty-Guided Lookback</title>
<link>https://arxiv.org/abs/2511.15613</link>
<guid>https://arxiv.org/abs/2511.15613</guid>
<content:encoded><![CDATA[
<div> Test-time thinking, visual reasoning, LVLMs, uncertainty guided lookback, decoding strategy<br /><br />Summary:<br /><br />1. The paper investigates the impact of test-time thinking—generating explicit intermediate reasoning chains—on large vision language models (LVLMs), offering the first systematic large-scale analysis of how such reasoning affects visual tasks.  
2. Ten LVLM variants from the InternVL3.5 and Qwen3-VL families are evaluated on the MMMU-val benchmark, with generous token budgets and multi-pass decoding strategies.  
3. Results reveal that longer reasoning chains do not necessarily improve performance; extended chains often lead to incorrect reasoning paths that neglect image content and underperform compared to standard instruction-following modes.  
4. A key insight is that short lookback phrases explicitly referencing the image correlate strongly with better visual grounding and successful reasoning trajectories.  
5. Leveraging this, the authors introduce an uncertainty guided lookback decoding strategy that is training-free and combines uncertainty signals with adaptive lookback prompts and breadth search to improve reasoning.  
6. This method significantly improves MMMU benchmark performance, especially in areas where conventional thinking struggles, outperforming several strong decoding baselines and setting a new state of the art under fixed model and token constraints.  
7. The decoding approach generalizes well, yielding consistent gains across five additional benchmarks, including broad multimodal datasets and math-focused visual reasoning tasks. <div>
arXiv:2511.15613v1 Announce Type: cross 
Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisPlay: Self-Evolving Vision-Language Models from Images</title>
<link>https://arxiv.org/abs/2511.15661</link>
<guid>https://arxiv.org/abs/2511.15661</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Vision-Language Models, self-evolving framework, visual reasoning, Group Relative Policy Optimization (GRPO)  

<br /><br />Summary:  
This paper introduces VisPlay, a novel self-evolving reinforcement learning (RL) framework designed to enhance Vision-Language Models (VLMs) on complex visual reasoning tasks without requiring human-annotated labels or task-specific heuristics. VisPlay starts with a single base VLM and divides it into two interactive roles: an Image-Conditioned Questioner that generates challenging yet answerable visual questions, and a Multimodal Reasoner that produces silver (pseudo) answers. Both components are trained jointly via Group Relative Policy Optimization (GRPO), a technique that incentivizes diversity and appropriate difficulty in generated questions while maintaining high-quality silver responses. This approach enables autonomous improvement of reasoning abilities by leveraging large-scale unlabeled image data. VisPlay demonstrates scalability by working efficiently with multiple VLM architectures, specifically Qwen2.5-VL and MiMo-VL models. Experimental results show consistent gains in visual reasoning skills, compositional generalization, and hallucination reduction across eight different benchmarks, including MM-Vet and MMMU. Overall, VisPlay provides a scalable, cost-effective path towards self-improving multimodal intelligence, reducing dependency on expensive annotations and static reward functions. The project is publicly available for community use and further research at https://bruno686.github.io/VisPlay/. <div>
arXiv:2511.15661v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping</title>
<link>https://arxiv.org/abs/2511.15690</link>
<guid>https://arxiv.org/abs/2511.15690</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Multimodal large language models, Expert skipping, Inference efficiency, Dual-modality thresholding<br /><br />Summary:  
This paper addresses the computational inefficiency of Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) in vision-language tasks. Existing expert skipping methods designed for unimodal large language models are found ineffective for MLLMs, causing significant performance degradation due to ignoring heterogeneous expert contributions and modality-specific token behaviors. To solve this, the authors propose MoDES, a novel training-free framework that adaptively skips experts during inference to balance efficiency and accuracy in MoE MLLMs. MoDES features a globally-modulated local gating (GMLG) mechanism, which incorporates global layer-wise importance into local routing to better estimate per-token expert importance. Additionally, it uses dual-modality thresholding (DMT) to handle different modalities separately for expert skipping decisions. To optimize the skipping thresholds efficiently, a frontier search algorithm exploiting monotonicity properties is introduced, reducing convergence time dramatically from days to hours. Extensive experiments on three model series across thirteen benchmarks demonstrate that MoDES significantly outperforms prior methods. For example, skipping 88% of experts in Qwen3-VL-MoE-30B-A3B-Instruct yields a performance improvement from 86.66% to 97.33%. Moreover, MoDES accelerates inference by increasing prefilling speed by 2.16× and decoding speed by 1.26×, thereby enhancing both model efficiency and accuracy. <div>
arXiv:2511.15690v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Visually, Reason Textually: Vision-Language Synergy in ARC</title>
<link>https://arxiv.org/abs/2511.15703</link>
<guid>https://arxiv.org/abs/2511.15703</guid>
<content:encoded><![CDATA[
<div> Keywords: abstract reasoning, ARC-AGI, vision-language synergy, modality-switch self-correction, foundation models<br /><br />Summary:<br /><br />1. Abstract reasoning from minimal examples remains a significant challenge for advanced foundation models like GPT-5 and Grok 4, as they struggle to infer structured transformation rules from few samples—an ability that humans excel at.  
2. The ARC-AGI dataset is designed as a rigorous benchmark to test conceptual rule induction and the transfer of learned rules to new tasks, emphasizing the need for both reasoning and abstraction.  
3. Existing approaches mostly treat ARC-AGI as a textual reasoning problem, ignoring the importance of visual abstraction, which humans heavily rely on to solve such puzzles.  
4. Initial experiments showed that simply converting ARC-AGI grids into images actually harmed performance, due to imprecise rule execution in vision-only models.  
5. The authors propose that vision and language serve complementary roles: vision aids in global pattern abstraction and verification, while language handles symbolic rule formulation and precise execution.  
6. Based on this hypothesis, two strategies were introduced: Vision-Language Synergy Reasoning (VLSR), which breaks ARC-AGI tasks into modality-specific subtasks, and Modality-Switch Self-Correction (MSSC), which uses vision to verify and correct text-based reasoning errors.  
7. Extensive testing demonstrated up to a 4.33% improvement over text-only models across various flagship architectures and ARC-AGI problems.  
8. The study highlights that integrating visual abstraction with linguistic reasoning is critical for developing foundation models capable of human-like, generalizable intelligence.  
9. The authors plan to release their source code soon to facilitate further research in this direction. <div>
arXiv:2511.15703v1 Announce Type: cross 
Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MessIRve: A Large-Scale Spanish Information Retrieval Dataset</title>
<link>https://arxiv.org/abs/2409.05994</link>
<guid>https://arxiv.org/abs/2409.05994</guid>
<content:encoded><![CDATA[
<div> Spanish, Information Retrieval, Dataset, MessIRve, Autocomplete API  

<br /><br />Summary:  
The article presents MessIRve, a novel large-scale Spanish information retrieval (IR) dataset designed to address the scarcity of resources for Spanish IR tasks. The dataset consists of nearly 700,000 queries gathered from Google's autocomplete API and is paired with relevant documents from Wikipedia, ensuring a rich and authentic data source. Unlike other Spanish IR datasets that are either translated from English or lack consideration of dialectal variations, MessIRve captures queries from diverse Spanish-speaking regions, providing a realistic representation of language usage across different dialects. Its large size enables coverage of a wide array of topics, in contrast to smaller existing datasets that may have limited thematic scope. The paper includes a detailed description of the dataset construction process, comparative analyses with existing Spanish IR resources, and baseline evaluations using prominent IR models. Through these contributions, the work aims to advance research in Spanish IR and improve information retrieval tools to better serve Spanish speakers worldwide. <div>
arXiv:2409.05994v2 Announce Type: replace 
Abstract: Information retrieval (IR) is the task of finding relevant documents in response to a user query. Although Spanish is the second most spoken native language, there are few Spanish IR datasets, which limits the development of information access tools for Spanish speakers. We introduce MessIRve, a large-scale Spanish IR dataset with almost 700,000 queries from Google's autocomplete API and relevant documents sourced from Wikipedia. MessIRve's queries reflect diverse Spanish-speaking regions, unlike other datasets that are translated from English or do not consider dialectal variations. The large size of the dataset allows it to cover a wide variety of topics, unlike smaller datasets. We provide a comprehensive description of the dataset, comparisons with existing datasets, and baseline evaluations of prominent IR models. Our contributions aim to advance Spanish IR research and improve information access for Spanish speakers.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?</title>
<link>https://arxiv.org/abs/2411.04530</link>
<guid>https://arxiv.org/abs/2411.04530</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual language models, semantic tokens, subword embeddings, cross-lingual transfer, zero-shot learning<br /><br />Summary: This work investigates the extent to which current multilingual language models (mLMs) comprehend text through subword-level semantic concepts rather than merely through superficial token forms. The authors introduce "semantic tokens," which are created by merging semantically similar subwords and their embeddings, aiming to align the models more closely with meaningful language units. They evaluate the efficacy of these semantic tokens across five diverse multilingual downstream tasks, demonstrating that shared general semantics significantly aid mLMs with various tokenizers and model sizes in making accurate predictions. Analysis of the grouped subwords reveals a broad spectrum of semantic relationships, including synonyms and translations across many languages and scripts, underscoring the cross-lingual relevance of the semantic tokens. Additionally, the study finds that zero-shot performance using semantic tokens matches or surpasses that of the original models on specific classification tasks. This suggests semantic tokens at the subword level act as foundational anchors facilitating effective cross-lingual transfer, highlighting the potential of semantic tokenization to enhance multilingual understanding and model generalization. <div>
arXiv:2411.04530v2 Announce Type: replace 
Abstract: Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form "semantic tokens" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Newswire Extraction: A pipeline for extracting newswires from newspaper images</title>
<link>https://arxiv.org/abs/2502.11866</link>
<guid>https://arxiv.org/abs/2502.11866</guid>
<content:encoded><![CDATA[
arXiv:2502.11866v2 Announce Type: replace 
Abstract: I describe a new pipeline for extracting wire services (e.g., Associated Press, United Press International, Newspaper Enterprise Association) from newspaper images.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</title>
<link>https://arxiv.org/abs/2504.12312</link>
<guid>https://arxiv.org/abs/2504.12312</guid>
<content:encoded><![CDATA[
arXiv:2504.12312v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts</title>
<link>https://arxiv.org/abs/2505.03025</link>
<guid>https://arxiv.org/abs/2505.03025</guid>
<content:encoded><![CDATA[
arXiv:2505.03025v2 Announce Type: replace 
Abstract: Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
<link>https://arxiv.org/abs/2505.14160</link>
<guid>https://arxiv.org/abs/2505.14160</guid>
<content:encoded><![CDATA[
arXiv:2505.14160v4 Announce Type: replace 
Abstract: Multilingual vision-language models (VLMs) promise universal image-text retrieval, yet their social biases remain underexplored. We perform the first systematic audit of four public multilingual CLIP variants: M-CLIP, NLLB-CLIP, CAPIVARA-CLIP, and the debiased SigLIP-2, covering ten languages that differ in resource availability and morphological gender marking. Using balanced subsets of FairFace and the PATA stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the intuition that multilinguality mitigates bias, every model exhibits stronger gender skew than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared encoder of NLLB-CLIP and SigLIP-2 transfers English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this leakage. Although SigLIP-2 reduces agency and communion skews, it inherits -- and in caption-sparse contexts (e.g., Xhosa) amplifies -- the English anchor's crime associations. Highly gendered languages consistently magnify all bias types, yet gender-neutral languages remain vulnerable whenever cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics thus mask language-specific hot spots, underscoring the need for fine-grained, language-aware bias evaluation in future multilingual VLM research.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling</title>
<link>https://arxiv.org/abs/2507.04416</link>
<guid>https://arxiv.org/abs/2507.04416</guid>
<content:encoded><![CDATA[
arXiv:2507.04416v3 Announce Type: replace 
Abstract: Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7$\times$ improvement in training speed for 100K sequence length and 9$times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Hallucination in Conversations for Low Resource Languages</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
arXiv:2507.22720v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</title>
<link>https://arxiv.org/abs/2508.16983</link>
<guid>https://arxiv.org/abs/2508.16983</guid>
<content:encoded><![CDATA[
arXiv:2508.16983v2 Announce Type: replace 
Abstract: Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at https://github.com/rpo19/ReFactX.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.17184</link>
<guid>https://arxiv.org/abs/2508.17184</guid>
<content:encoded><![CDATA[
arXiv:2508.17184v2 Announce Type: replace 
Abstract: Instruction tuning is a pivotal technique for aligning large language models (LLMs) with human intentions, safety constraints, and domain-specific requirements. This survey provides a comprehensive overview of the full pipeline, encompassing (i) data collection methodologies, (ii) full-parameter and parameter-efficient fine-tuning strategies, and (iii) evaluation protocols. We categorized data construction into three major paradigms: expert annotation, distillation from larger models, and self-improvement mechanisms, each offering distinct trade-offs between quality, scalability, and resource cost. Fine-tuning techniques range from conventional supervised training to lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning, with a focus on computational efficiency and model reusability. We further examine the challenges of evaluating faithfulness, utility, and safety across multilingual and multimodal scenarios, highlighting the emergence of domain-specific benchmarks in healthcare, legal, and financial applications. Finally, we discuss promising directions for automated data generation, adaptive optimization, and robust evaluation frameworks, arguing that a closer integration of data, algorithms, and human feedback is essential for advancing instruction-tuned LLMs. This survey aims to serve as a practical reference for researchers and practitioners seeking to design LLMs that are both effective and reliably aligned with human intentions.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Alignment of Large Language Models with Global Human Opinion</title>
<link>https://arxiv.org/abs/2509.01418</link>
<guid>https://arxiv.org/abs/2509.01418</guid>
<content:encoded><![CDATA[
arXiv:2509.01418v2 Announce Type: replace 
Abstract: Today's large language models (LLMs) are capable of supporting multilingual scenarios, allowing users to interact with LLMs in their native languages. When LLMs respond to subjective questions posed by users, they are expected to align with the views of specific demographic groups or historical periods, shaped by the language in which the user interacts with the model. Existing studies mainly focus on researching the opinions represented by LLMs among demographic groups in the United States or a few countries, lacking worldwide country samples and studies on human opinions in different historical periods, as well as lacking discussion on using language to steer LLMs. Moreover, they also overlook the potential influence of prompt language on the alignment of LLMs' opinions. In this study, our goal is to fill these gaps. To this end, we create an evaluation framework based on the World Values Survey (WVS) to systematically assess the alignment of LLMs with human opinions across different countries, languages, and historical periods around the world. We find that LLMs appropriately or over-align the opinions with only a few countries while under-aligning the opinions with most countries. Furthermore, changing the language of the prompt to match the language used in the questionnaire can effectively steer LLMs to align with the opinions of the corresponding country more effectively than existing steering methods. At the same time, LLMs are more aligned with the opinions of the contemporary population. To our knowledge, our study is the first comprehensive investigation of the topic of opinion alignment in LLMs across global, language, and temporal dimensions. Our code and data are publicly available at https://github.com/ku-nlp/global-opinion-alignment and https://github.com/nlply/global-opinion-alignment.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents</title>
<link>https://arxiv.org/abs/2509.01560</link>
<guid>https://arxiv.org/abs/2509.01560</guid>
<content:encoded><![CDATA[
arXiv:2509.01560v2 Announce Type: replace 
Abstract: Tool agents -- LLM-based systems that interact with external APIs -- offer a way to execute real-world tasks. However, as tasks become increasingly complex, these agents struggle to identify and call the correct APIs in the proper order. To tackle this problem, we investigate converting API documentation into a structured API graph that captures API dependencies and leveraging it for multi-tool queries that require compositional API calls. To support this, we introduce In-N-Out, the first expert-annotated dataset of API graphs built from two real-world API benchmarks and their documentation. Using In-N-Out significantly improves performance on both tool retrieval and multi-tool query generation, nearly doubling that of LLMs using documentation alone. Moreover, graphs generated by models fine-tuned on In-N-Out close 90% of this gap, showing that our dataset helps models learn to comprehend API documentation and parameter relationships. Our findings highlight the promise of using explicit API graphs for tool agents and the utility of In-N-Out as a valuable resource. We will release the dataset and code publicly.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias after Prompting: Persistent Discrimination in Large Language Models</title>
<link>https://arxiv.org/abs/2509.08146</link>
<guid>https://arxiv.org/abs/2509.08146</guid>
<content:encoded><![CDATA[
arXiv:2509.08146v2 Announce Type: replace 
Abstract: A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation based context discovery for ASR</title>
<link>https://arxiv.org/abs/2509.19567</link>
<guid>https://arxiv.org/abs/2509.19567</guid>
<content:encoded><![CDATA[
arXiv:2509.19567v2 Announce Type: replace 
Abstract: This work investigates retrieval augmented generation as an efficient strategy for automatic context discovery in context-aware Automatic Speech Recognition (ASR) system, in order to improve transcription accuracy in the presence of rare or out-of-vocabulary terms. However, identifying the right context automatically remains an open challenge. This work proposes an efficient embedding-based retrieval approach for automatic context discovery in ASR. To contextualize its effectiveness, two alternatives based on large language models (LLMs) are also evaluated: (1) large language model (LLM)-based context generation via prompting, and (2) post-recognition transcript correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech demonstrate that the proposed approach reduces WER by up to 17% (percentage difference) relative to using no-context, while the oracle context results in a reduction of up to 24.1%.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains</title>
<link>https://arxiv.org/abs/2510.17793</link>
<guid>https://arxiv.org/abs/2510.17793</guid>
<content:encoded><![CDATA[
arXiv:2510.17793v2 Announce Type: replace 
Abstract: Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning</title>
<link>https://arxiv.org/abs/2510.20098</link>
<guid>https://arxiv.org/abs/2510.20098</guid>
<content:encoded><![CDATA[
arXiv:2510.20098v2 Announce Type: replace 
Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.20548</link>
<guid>https://arxiv.org/abs/2510.20548</guid>
<content:encoded><![CDATA[
arXiv:2510.20548v2 Announce Type: replace 
Abstract: Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays</title>
<link>https://arxiv.org/abs/2510.22830</link>
<guid>https://arxiv.org/abs/2510.22830</guid>
<content:encoded><![CDATA[
arXiv:2510.22830v4 Announce Type: replace 
Abstract: BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs</title>
<link>https://arxiv.org/abs/2510.26253</link>
<guid>https://arxiv.org/abs/2510.26253</guid>
<content:encoded><![CDATA[
arXiv:2510.26253v2 Announce Type: replace 
Abstract: The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-Audio-EditX Technical Report</title>
<link>https://arxiv.org/abs/2511.03601</link>
<guid>https://arxiv.org/abs/2511.03601</guid>
<content:encoded><![CDATA[
arXiv:2511.03601v2 Announce Type: replace 
Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities. Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidential Prompting: Privacy-preserving LLM Inference on Cloud</title>
<link>https://arxiv.org/abs/2409.19134</link>
<guid>https://arxiv.org/abs/2409.19134</guid>
<content:encoded><![CDATA[
arXiv:2409.19134v5 Announce Type: replace-cross 
Abstract: This paper introduces a vision of confidential prompting: securing user prompts from an untrusted, cloud-hosted large language model (LLM) while preserving model confidentiality, output invariance, and compute efficiency. As a first step toward this vision, we present Petridish, a system built on top of confidential computing and its core contribution, a novel technology called Secure Partitioned Decoding (SPD). Petridish runs the LLM service inside a confidential virtual machine (CVM), which protects the secrets, i.e., the LLM parameters and user prompts, from adversaries outside the CVM. Importantly, it splits the LLM service for a user into two processes, using SPD: a per-user process performs prefill with the user prompts and computes attention scores during decoding; a service process, shared by all users, batches the attention scores from per-user processes and generates output tokens for all users. Both the LLM provider and the users trust Petridish's CVM and its operating system, which guarantees isolation between processes and limits their outbound network capabilities to control information flow. The CVM's attestation capability and its open-source software stack enable Petridish to provide auditable protection of both user prompt and LLM confidentiality. Together, Petridish maintains full utility of LLM service and enables practical, privacy-preserving cloud-hosted LLM inference for sensitive applications, such as processing personal data, clinical records, and financial documents.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning-Aware Code Infilling via Horizon-Length Prediction</title>
<link>https://arxiv.org/abs/2410.03103</link>
<guid>https://arxiv.org/abs/2410.03103</guid>
<content:encoded><![CDATA[
arXiv:2410.03103v4 Announce Type: replace-cross 
Abstract: Fill-in-the-Middle (FIM), or infilling, has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm which performs next-token prediction (NTP) over reordered sequence often leads to models struggling to generate content that aligns well with the surrounding context. We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different model families and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eguard: Defending LLM Embeddings Against Inversion Attacks via Text Mutual Information Optimization</title>
<link>https://arxiv.org/abs/2411.05034</link>
<guid>https://arxiv.org/abs/2411.05034</guid>
<content:encoded><![CDATA[
arXiv:2411.05034v2 Announce Type: replace-cross 
Abstract: Embeddings have become a cornerstone in the functionality of large language models (LLMs) due to their ability to transform text data into rich, dense numerical representations that capture semantic and syntactic properties. These embedding vector databases serve as the long-term memory of LLMs, enabling efficient handling of a wide range of natural language processing tasks. However, the surge in popularity of embedding vector databases in LLMs has been accompanied by significant concerns about privacy leakage. Embedding vector databases are particularly vulnerable to embedding inversion attacks, where adversaries can exploit the embeddings to reverse-engineer and extract sensitive information from the original text data. Existing defense mechanisms have shown limitations, often struggling to balance security with the performance of downstream tasks. To address these challenges, we introduce Eguard, a novel defense mechanism designed to mitigate embedding inversion attacks. Eguard employs a transformer-based projection network and text mutual information optimization to safeguard embeddings while preserving the utility of LLMs. Our approach significantly reduces privacy risks, protecting over 95% of tokens from inversion while maintaining high performance across downstream tasks consistent with original embeddings.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairshare Data Pricing via Data Valuation for Large Language Models</title>
<link>https://arxiv.org/abs/2502.00198</link>
<guid>https://arxiv.org/abs/2502.00198</guid>
<content:encoded><![CDATA[
arXiv:2502.00198v4 Announce Type: replace-cross 
Abstract: Training data is the backbone of large language models (LLMs), yet today's data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). We begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. Then we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data's contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. Theoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. Empirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning, fairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers' performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.07265</link>
<guid>https://arxiv.org/abs/2503.07265</guid>
<content:encoded><![CDATA[
arXiv:2503.07265v3 Announce Type: replace-cross 
Abstract: Text-to-Image (T2I) models are capable of generating high-quality artistic creations and visual content. However, existing research and evaluation standards predominantly focus on image realism and shallow text-image alignment, lacking a comprehensive assessment of complex semantic understanding and world knowledge integration in text-to-image generation. To address this challenge, we propose \textbf{WISE}, the first benchmark specifically designed for \textbf{W}orld Knowledge-\textbf{I}nformed \textbf{S}emantic \textbf{E}valuation. WISE moves beyond simple word-pixel mapping by challenging models with 1000 meticulously crafted prompts across 25 subdomains in cultural common sense, spatio-temporal reasoning, and natural science. To overcome the limitations of traditional CLIP metric, we introduce \textbf{WiScore}, a novel quantitative metric for assessing knowledge-image alignment. Through comprehensive testing of 20 models (10 dedicated T2I models and 10 unified multimodal models) using 1,000 structured prompts spanning 25 subdomains, our findings reveal significant limitations in their ability to effectively integrate and apply world knowledge during image generation, highlighting critical pathways for enhancing knowledge incorporation and application in next-generation T2I models. Code and data are available at \href{https://github.com/PKU-YuanGroup/WISE}{PKU-YuanGroup/WISE}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trade-offs in Large Reasoning Models: An Empirical Analysis of Deliberative and Adaptive Reasoning over Foundational Capabilities</title>
<link>https://arxiv.org/abs/2503.17979</link>
<guid>https://arxiv.org/abs/2503.17979</guid>
<content:encoded><![CDATA[
arXiv:2503.17979v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Reasoning Models (LRMs), such as OpenAI's o1/o3 and DeepSeek-R1, have demonstrated remarkable performance in specialized reasoning tasks through human-like deliberative thinking and long chain-of-thought reasoning. However, our systematic evaluation across various model families (DeepSeek, Qwen, and LLaMA) and scales (7B to 32B) reveals that acquiring these deliberative reasoning capabilities significantly reduces the foundational capabilities of LRMs, including notable declines in helpfulness and harmlessness, alongside substantially increased inference costs. Importantly, we demonstrate that adaptive reasoning -- employing modes like Zero-Thinking, Less-Thinking, and Summary-Thinking -- can effectively alleviate these drawbacks. Our empirical insights underline the critical need for developing more versatile LRMs capable of dynamically allocating inference-time compute according to specific task characteristics.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents</title>
<link>https://arxiv.org/abs/2504.16264</link>
<guid>https://arxiv.org/abs/2504.16264</guid>
<content:encoded><![CDATA[
arXiv:2504.16264v2 Announce Type: replace-cross 
Abstract: Cross-lingual information retrieval (CLIR) helps users find documents in languages different from their queries. This is especially important in academic search, where key research is often published in non-English languages. We present CLIRudit, a novel English-French academic retrieval dataset built from \'Erudit, a Canadian publishing platform. Using multilingual metadata, we pair English author-written keywords as queries with non-English abstracts as target documents, a method that can be applied to other languages and repositories. We benchmark various first-stage sparse and dense retrievers, with and without machine translation. We find that dense embeddings without translation perform nearly as well as systems using machine translation, that translating documents is generally more effective than translating queries, and that sparse retrievers with document translation remain competitive while offering greater efficiency. Along with releasing the first English-French academic retrieval dataset, we provide a reproducible benchmarking method to improve access to non-English scholarly content.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</title>
<link>https://arxiv.org/abs/2507.18224</link>
<guid>https://arxiv.org/abs/2507.18224</guid>
<content:encoded><![CDATA[
arXiv:2507.18224v4 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) based on large language models (LLMs) have emerged as a powerful solution for dealing with complex problems across diverse domains. The effectiveness of MAS is critically dependent on its collaboration topology, which has become a focal point for automated design research. However, existing approaches are fundamentally constrained by their reliance on a template graph modification paradigm with a predefined set of agents and hard-coded interaction structures, significantly limiting their adaptability to task-specific requirements. To address these limitations, we reframe MAS design as a conditional autoregressive graph generation task, where both the system composition and structure are designed jointly. We propose ARG-Designer, a novel autoregressive model that operationalizes this paradigm by constructing the collaboration graph from scratch. Conditioned on a natural language task query, ARG-Designer sequentially and dynamically determines the required number of agents, selects their appropriate roles from an extensible pool, and establishes the optimal communication links between them. This generative approach creates a customized topology in a flexible and extensible manner, precisely tailored to the unique demands of different tasks. Extensive experiments across six diverse benchmarks demonstrate that ARG-Designer not only achieves state-of-the-art performance but also enjoys significantly greater token efficiency and enhanced extensibility. The source code of ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data-driven ML Approach for Maximizing Performance in LLM-Adapter Serving</title>
<link>https://arxiv.org/abs/2508.08343</link>
<guid>https://arxiv.org/abs/2508.08343</guid>
<content:encoded><![CDATA[
arXiv:2508.08343v3 Announce Type: replace-cross 
Abstract: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads. The code is publicly available at https://github.com/FerranAgulloLopez/GPULLMAdapterOptimization.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preserving In-Context-Learning Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2509.13625</link>
<guid>https://arxiv.org/abs/2509.13625</guid>
<content:encoded><![CDATA[
arXiv:2509.13625v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have significantly transformed natural language understanding and generation, but they raise privacy concerns due to potential exposure of sensitive information. Studies have highlighted the risk of information leakage, where adversaries can extract sensitive information embedded in the prompts. In this work, we introduce a novel private prediction framework for generating high-quality synthetic text with strong privacy guarantees. Our approach leverages the Differential Privacy (DP) framework to ensure worst-case theoretical bounds on information leakage without requiring any fine-tuning of the underlying models. The proposed method performs inference on private records and aggregates the resulting per-token output distributions. This enables the generation of longer and coherent synthetic text while maintaining privacy guarantees. Additionally, we propose a simple blending operation that combines private and public inference to further enhance utility. Empirical evaluations demonstrate that our approach outperforms previous state-of-the-art methods on in-context-learning (ICL) tasks, making it a promising direction for privacy-preserving text generation while maintaining high utility. Our code is available at https://github.com/bhusalb/privacy-preserving-icl.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks</title>
<link>https://arxiv.org/abs/2509.24473</link>
<guid>https://arxiv.org/abs/2509.24473</guid>
<content:encoded><![CDATA[
arXiv:2509.24473v3 Announce Type: replace-cross 
Abstract: Spatial intelligence spans a rich suite of abilities, including visualising and transforming shapes, mentally rotating objects, judging relational positions and containment, and estimating numerosity. However, it still remains a critical unresolved challenge for Multimodal Large Language Models (MLLMs). To fill this gap, we propose to treat Euclidean geometry problem-solving as a surrogate task. Specifically, we meticulously constructed a curated multimodal dataset, called Euclid30K, comprising approximately 30K plane and solid geometry problems. Furthermore, to enable the model to learn and apply Euclidean principles from these geometry problems, we fine-tuned seven model variants (spanning 3--72B parameters) from the Qwen2.5VL, Qwen3VL, and RoboBrain2.0 families using Group Relative Policy Optimization (GRPO), inspiring the models to identify shapes, count, and relate entities, and perform multi-step deductive reasoning using Euclidean principles. Our experiments demonstrate that the resulting models achieve substantial zero-shot gains across four spatial reasoning benchmarks (Super-CLEVR, Omni3DBench, VSI-Bench, and MindCube) without any task-specific adaptations. Notably, after training on the Euclid30K, the mean VSI-Bench accuracy rose from 36.6\% to 41.8\% (+5.2\%), and the mean MindCube accuracy rose from 31.4\% to 38.1\% (+6.7\%). To our knowledge, this is the first systematic study showing that geometry-centric fine-tuning can confer vision-language models with broadly transferable spatial skills. Code and Euclid30K dataset can be found in \href{https://zgca-ai4edu.github.io/Euclids_Gift}{this}.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conflict Adaptation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2510.24804</link>
<guid>https://arxiv.org/abs/2510.24804</guid>
<content:encoded><![CDATA[
arXiv:2510.24804v2 Announce Type: replace-cross 
Abstract: A signature of human cognitive control is conflict adaptation: improved performance on a high-conflict trial following another high-conflict trial. This phenomenon offers an account for how cognitive control, a scarce resource, is recruited. Using a sequential Stroop task, we find that 12 of 13 vision-language models (VLMs) tested exhibit behavior consistent with conflict adaptation, with the lone exception likely reflecting a ceiling effect. To understand the representational basis of this behavior, we use sparse autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B. Partially overlapping supernodes emerge for text and color in both early and late layers, and their relative sizes mirror the automaticity asymmetry between reading and color naming in humans. We further isolate a conflict-modulated supernode in layers 24-25 whose ablation significantly increases Stroop errors while minimally affecting congruent trials.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start</title>
<link>https://arxiv.org/abs/2510.25801</link>
<guid>https://arxiv.org/abs/2510.25801</guid>
<content:encoded><![CDATA[
arXiv:2510.25801v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of "MLLM-r1" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution "stuckness," improving exploration, stabilizing training, and raising the performance ceiling.
]]></content:encoded>
<pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware Cloud-Edge Cooperation</title>
<link>https://arxiv.org/abs/2510.19670</link>
<guid>https://arxiv.org/abs/2510.19670</guid>
<content:encoded><![CDATA[
<div> Keywords: CoSense-LLM, multimodal, edge computing, privacy, latency  

<br /><br />Summary: CoSense-LLM is an innovative edge-first framework designed to transform continuous multimodal sensor streams—such as Wi-Fi CSI, IMU, audio, RFID, and lightweight vision—into compact, verifiable semantic tokens while adhering to explicit constraints regarding latency, energy, bandwidth, and privacy. The framework consists of four main components: (i) SenseFusion, a lightweight encoder that compresses sensor embeddings into short discrete code sequences; (ii) Edge-RAG, which grounds generative processes in specific site policies and notes through local hybrid retrieval; (iii) PromptRouter, a cost and uncertainty-aware policy that efficiently selects between different processing strategies; and (iv) Secure Execution, which ensures data minimization by preventing raw waveforms from leaving the device. CoSense-LLM integrates modern serving optimizations that facilitate on-device personalization and federated updates, achieving sub-second latency and reducing inter-tier costs while maintaining user privacy. The framework demonstrates enhanced factual consistency and controlled escalation through various techniques, supporting an edge-first approach that prioritizes semantics, privacy, and predictable latency for large model applications in environments prone to interference. <div>
arXiv:2510.19670v3 Announce Type: replace 
Abstract: We present CoSense-LLM, an edge-first framework that turns continuous multimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and lightweight vision) into compact, verifiable semantic tokens and coordinates with large language models under explicit latency, energy, bandwidth, and privacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight encoder that aligns sensor embeddings with language and compresses them into short discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer that grounds generation in site specific policies and notes; (iii) PromptRouter, a cost and uncertainty aware policy that selects edge only generation, edge plus retrieval, or compact cloud escalation; and (iv) Secure Execution, an auditable redaction path that enforces data minimization so raw waveforms never leave the device. The system works with modern serving optimizations, including paged or streaming KV caches, FlashAttention style kernels, speculative decoding, and quantized LoRA adapters, and supports on device personalization and federated updates under non IID drift. Across home, office, and clinic deployments, CoSense-LLM delivers grounded explanations while meeting tight service level objectives: it sustains sub second (p95) end to end latency on edge dominant paths, reduces inter tier token and bandwidth costs by preferring local retrieval grounded responses, and preserves privacy by transmitting only discrete codes and redacted metadata. Ablations show that Edge-RAG improves factual consistency and reduces contradictions, calibrated uncertainty enables selective abstention and controlled escalations, and KV plus decoding accelerators lower energy per decision. The results support an edge first design that treats semantics, privacy, and predictable latency as co equal goals for large model deployments in interference prone environments.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</title>
<link>https://arxiv.org/abs/2511.13722</link>
<guid>https://arxiv.org/abs/2511.13722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, watermarking, adversarial attacks, text quality, back translation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of watermarking texts generated by Large Language Models (LLMs) as a method to reliably detect AI-generated content.<br /><br />2. Despite effective detection, current watermarking methods often degrade the quality of the generated texts, leading to concerns about their impact on usability.<br /><br />3. The study evaluates the robustness of several watermarking techniques against adversarial attacks, focusing specifically on paraphrasing and back translation (English to another language and back).<br /><br />4. The authors assess how well these watermarking methods preserve the semantics and writing style of the original, unwatermarked texts using linguistic metrics.<br /><br />5. Results show that while semantics are generally preserved, these watermarks cause noticeable deviations in writing style and are vulnerable to removal, especially through back translation attacks, highlighting limitations that may hinder broader adoption of watermarking.<br /><br />In conclusion, the research reveals a trade-off between watermark detectability, text quality, and robustness against adversarial modifications, suggesting further work is needed to improve watermarking techniques for LLM-generated texts. <div>
arXiv:2511.13722v1 Announce Type: new 
Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning</title>
<link>https://arxiv.org/abs/2511.13726</link>
<guid>https://arxiv.org/abs/2511.13726</guid>
<content:encoded><![CDATA[
<div> Keywords: RT method, semantic reasoning, text embedding models, refinement, inference enhancement  

<br /><br />Summary:  
This paper introduces RT (Refine Thought), a novel approach designed to improve the semantic reasoning capabilities of text embedding models. RT works by performing multiple forward passes through a text embedding model to produce a more refined and accurate final semantic representation. Experimental results demonstrate that RT significantly enhances performance on semantic reasoning benchmarks such as BRIGHT and PJBenchmark, which focus on person-job matching tasks. Importantly, RT maintains stable and reliable results on broader semantic understanding benchmarks like C-MTEB, indicating its versatility. The method is particularly effective when applied to decoder-only text embedding models, exemplified by its use with Qwen3-Embedding-8B. RT operates as a test-time inference technique, meaning it can be applied during the model's inference phase without retraining or modifying the underlying model parameters. This approach leverages and further activates the semantic reasoning ability that these models already learned during pretraining. Overall, RT represents a promising strategy to boost the semantic reasoning power of existing embedding models with minimal changes to their workflow, providing practical benefits for tasks requiring deeper semantic understanding. <div>
arXiv:2511.13726v1 Announce Type: new 
Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can QE-informed (Re)Translation lead to Error Correction?</title>
<link>https://arxiv.org/abs/2511.13884</link>
<guid>https://arxiv.org/abs/2511.13884</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality Estimation, Automatic Post-Editing, Machine Translation, LLMs, Delta COMET score  

<br /><br />Summary: The paper introduces two novel approaches designed for the WMT 2025 Automated Translation Quality Evaluation Systems Task 3, focusing on Quality Estimation (QE)-informed segment-level error correction. Both methods are presented in a training-free context. The first approach, QE-informed Retranslation, selects the highest-quality translation from multiple outputs generated by various large language models (LLMs). This method showcased improved effectiveness compared to the second approach, which is more similar to Automatic Post-Editing (APE). In the second approach, an LLM is directed to replace error substrings based on the provided QE explanations. To enhance performance, a conditional heuristic is implemented to limit the number of edits made, thereby aiming to optimize the Gain-to-Edit ratio. The results indicated that the first approach achieved a Delta COMET score of 0.0201, positioning it at the top of the subtask leaderboard, while the second approach recorded a slightly lower score of -0.0108. Overall, the findings highlight the potential of QE-informed strategies in enhancing translation quality without requiring extensive training. <div>
arXiv:2511.13884v1 Announce Type: new 
Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations</title>
<link>https://arxiv.org/abs/2511.13900</link>
<guid>https://arxiv.org/abs/2511.13900</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval, GM-Extract, evaluation metrics, mitigation methods  

<br /><br />Summary:  
The article addresses the challenge posed by the "lost-in-the-middle" phenomenon in large language models (LLMs) when leveraging long-range context for retrieval tasks. To explore this issue, the authors introduce GM-Extract, a benchmark dataset aimed at assessing LLM performance in retrieving control variables. They propose an evaluation framework comprising two metrics: the Document Metric for spatial retrieval and the Variable Extraction Metric for semantic retrieval. Through a systematic evaluation of 7-8 billion parameter models on tasks like key-value extraction and question-answering, they observe that data representation in the context window significantly influences retrieval results. While a consistent U-shaped performance curve isn't found, a general performance pattern correlating with perplexity scores is identified. Additionally, the authors conduct a literature review of mitigation methods, dividing them into black-box and white-box approaches. The application of these techniques reveals nuanced efficacy, demonstrating both improvements and unexpected negative impacts on performance. This comprehensive evaluation provides valuable insights into the practical utility of various strategies for enhancing LLM performance in retrieval-based tasks. <div>
arXiv:2511.13900v1 Announce Type: new 
Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition</title>
<link>https://arxiv.org/abs/2511.13994</link>
<guid>https://arxiv.org/abs/2511.13994</guid>
<content:encoded><![CDATA[
<div> Keywords: superlatives, LLMs, retrieval, ranking, semantic interpretation  

<br /><br />Summary: This article investigates the role of Large Language Models (LLMs) in processing search queries that contain superlatives, such as "best" and "most popular." These queries require a complex understanding of language and domain-specific knowledge, as they involve comparing candidates across various attributes. The authors propose a novel framework that extracts structured interpretations from these queries, detailing attribute-value hints generated alongside the retrieval process. This framework significantly enhances search performance, achieving a 10.9-point improvement in Mean Average Precision (MAP) and a 5.9-point improvement in Mean Reciprocal Rank (MRR) over conventional baselines. To address latency issues inherent in direct LLM-based reranking, the study introduces an innovative method that transfers superlative interpretations to more lightweight models. The findings shed light on the potential for representing and transferring superlative semantics across different models, thereby enhancing linguistic interpretation within retrieval systems. This advancement not only improves search rankings but also considers practical deployment challenges, ultimately contributing to more effective e-commerce search experiences. <div>
arXiv:2511.13994v1 Announce Type: new 
Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports</title>
<link>https://arxiv.org/abs/2511.14010</link>
<guid>https://arxiv.org/abs/2511.14010</guid>
<content:encoded><![CDATA[
<div> Keywords: post-disaster, reconnaissance reports, multi-hazard, language models, MoRA-RAG  

<br /><br />Summary: This study addresses the challenges posed by unstructured narratives in post-disaster reconnaissance reports, which hinder systematic knowledge transfer regarding multi-hazard interactions. To improve the analysis of these reports, the authors introduce the Mixture-of-Retrieval Agentic RAG (MoRA-RAG) framework. This knowledge-grounded large language model (LLM) framework systematically transforms reconnaissance reports into structured data conducive to multi-hazard reasoning. The MoRA-RAG framework utilizes a Mixture-of-Retrieval mechanism that routes queries through hazard-specific databases while maintaining contextual coherence via agentic chunking. Additionally, it incorporates a verification loop to evaluate evidence sufficiency, refine queries, and conduct targeted searches when information is incomplete. The researchers created HazardRecQA by generating question-answer pairs from GEER reconnaissance reports that cover 90 global events across seven hazard types. MoRA-RAG demonstrates remarkable performance, achieving up to 94.5% accuracy—outperforming zero-shot LLMs by 30% and state-of-the-art RAG systems by 10%, while also reducing the incidence of hallucinations in various LLM architectures. The framework enables open-weight LLMs to match the performance of proprietary models, establishing a new paradigm in converting post-disaster documentation into reliable, actionable intelligence for enhancing hazard resilience. <div>
arXiv:2511.14010v1 Announce Type: new 
Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection</title>
<link>https://arxiv.org/abs/2511.14027</link>
<guid>https://arxiv.org/abs/2511.14027</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal, misinformation detection, external consistency, evidence reranking, large language models

<br /><br />Summary: This paper introduces HiEAG, a Hierarchical Evidence-Augmented Generation framework aimed at improving multimodal out-of-context (OOC) misinformation detection. While prior methods focused on internal consistency between image-text pairs, they often overlooked the critical aspect of external consistency with external evidence. HiEAG addresses this gap by breaking down external consistency checking into a structured engine pipeline that incorporates evidence retrieval, reranking, and rewriting. The evidence reranking module employs Automatic Evidence Selection Prompting (AESP) to identify relevant evidence from the retrieval outputs. Following this, the evidence rewriting module uses Automatic Evidence Generation Prompting (AEGP) to enhance the adaptability of MLLM-based OOC misinformation detectors. Additionally, HiEAG provides explanations for its judgments, thus increasing transparency in the detection process. The framework has been optimized through instruction tuning, resulting in significant performance improvements. Experimental evaluations across multiple benchmark datasets reveal that HiEAG outperforms existing state-of-the-art methods concerning accuracy across all evaluated samples, solidifying its position as an advanced solution in the field of misinformation detection. <div>
arXiv:2511.14027v1 Announce Type: new 
Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement</title>
<link>https://arxiv.org/abs/2511.14073</link>
<guid>https://arxiv.org/abs/2511.14073</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-label sentiment classification, dataset, class imbalance, RoBERTa, attention mechanism  

<br /><br />Summary: Multi-label sentiment classification is essential in natural language processing for identifying multiple emotions in a single piece of text. Existing datasets, such as GoEmotions, often exhibit significant class imbalance, which negatively impacts model performance for rare emotions. To tackle this issue, the authors created a balanced multi-label sentiment dataset by combining GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. This data balancing ensures an even distribution across 28 emotion categories. Alongside the dataset, the authors developed an advanced multi-label classification model that integrates pre-trained FastText embeddings, convolutional layers for localized feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to identify sentiment-relevant words. The model employs a sigmoid-activated output layer for multi-label prediction and utilizes mixed precision training to enhance computational efficiency. Experimental results indicate substantial improvements in accuracy, precision, recall, F1-score, and AUC when compared to models trained on imbalanced data, underscoring the effectiveness of the proposed approach in addressing the challenges of multi-label sentiment classification. <div>
arXiv:2511.14073v1 Announce Type: new 
Abstract: Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT</title>
<link>https://arxiv.org/abs/2511.14106</link>
<guid>https://arxiv.org/abs/2511.14106</guid>
<content:encoded><![CDATA[
<div> Keywords: Stealth Fine-Tuning, Reasoning-augmented Vision-Language Models, safety alignment, segment-level interference, chain-of-thought attacks

<br /><br />Summary:  
This paper introduces a novel attack method called Stealth Fine-Tuning targeting Reasoning-augmented Vision-Language Models (RVLMs) which are designed with safety alignment to prevent harmful behavior. The method exploits vulnerabilities in the models' exposed chain-of-thought (CoT) reasoning traces, which become new attack surfaces. Stealth Fine-Tuning works by applying segment-level interference to elicit harmful reasoning outputs and then reuses these self-generated outputs as supervised fine-tuning data. A turn-based weighted loss function is employed to maintain lightweight, distribution-consistent fine-tuning without significantly altering the model’s original reasoning abilities. The approach is efficient, requiring only 499 samples and under 3 hours of training on a single A100 GPU using QLoRA. Experimental results show that Stealth Fine-Tuning achieves a 38.52% higher attack success rate (ASR) on AdvBench compared to the previous method IDEATOR, while preserving the model's general reasoning capabilities and representation distribution. The study highlights that Stealth Fine-Tuning serves as a low-cost, highly effective technique to bypass current alignment defenses in RVLMs. The authors also include a disclaimer noting the presence of potentially disturbing or offensive content in the paper. <div>
arXiv:2511.14106v1 Announce Type: new 
Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding</title>
<link>https://arxiv.org/abs/2511.14112</link>
<guid>https://arxiv.org/abs/2511.14112</guid>
<content:encoded><![CDATA[
<div> Keywords: ICD coding, synthetic data, medical NLP, macro-F1, transformer models

<br /><br />Summary: Automatic coding of ICD diagnoses from clinical text is essential in medical NLP but faces challenges due to the long-tail distribution of codes, with many rare ICD codes poorly represented in existing datasets like MIMIC-III. To address this issue, the authors developed a data-centric framework to generate high-quality synthetic discharge summaries that help alleviate code imbalances. This method constructs realistic multi-label code sets centered on rare codes by using real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and related clinical notes. A total of 90,000 synthetic notes encompassing 7,902 ICD codes were produced, significantly expanding the available training distribution. Two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, were fine-tuned on both the original and the augmented datasets. Experimental results reveal that the proposed approach achieves a modest improvement in macro-F1 scores while preserving strong micro-F1 performance, thereby surpassing previous state-of-the-art results. Although the performance gain may appear marginal given the computational expenses, this study demonstrates that thoughtfully constructed synthetic data can enhance fairness in predicting long-tail ICD codes. <div>
arXiv:2511.14112v1 Announce Type: new 
Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling</title>
<link>https://arxiv.org/abs/2511.14142</link>
<guid>https://arxiv.org/abs/2511.14142</guid>
<content:encoded><![CDATA[
<div> Aspect-Based Sentiment Analysis, Hypergraph, Hierarchical Clustering, Short-text NLP, RoBERTa  

<br /><br />Summary:  
This paper addresses the challenge of Aspect-Based Sentiment Analysis (ABSA), focusing on predicting sentiment polarity for specific aspect terms, especially in short texts where context is limited and sentiments can conflict across different aspects. Prior graph-based models relied on pairwise relationships, requiring multiple graphs for various relational views, which resulted in redundancy, increased parameters, and error propagation during fusion, hindering robustness in low-resource and short-text scenarios. To overcome these limitations, the authors propose HyperABSA, a novel dynamic hypergraph framework that constructs aspect-opinion structures through sample-specific hierarchical clustering. A key contribution is an acceleration-fallback cutoff mechanism in hierarchical clustering that adaptively sets the granularity level for hyperedges, improving efficiency and effectiveness. Experimental results on three benchmark datasets—Lap14, Rest14, and MAMS—demonstrate consistent performance gains compared to strong graph-based baselines, with notable improvements when combined with RoBERTa language model backbones. The study highlights dynamic hypergraph construction as an efficient, powerful alternative for ABSA tasks and suggests its potential for broader applications in short-text natural language processing tasks. <div>
arXiv:2511.14142v1 Announce Type: new 
Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions</title>
<link>https://arxiv.org/abs/2511.14144</link>
<guid>https://arxiv.org/abs/2511.14144</guid>
<content:encoded><![CDATA[
<div> Transformer-based relation extraction, knowledge graphs, multiple-choice questions, truthfulness verification, fill-in-the-blank format<br /><br />Summary:<br /><br />This research presents a novel approach that combines Transformer-based relation extraction (RE) with knowledge graph (KG) matching to answer multiple-choice questions (MCQs) in a fill-in-the-blank format, ensuring traceability throughout the process. Knowledge graphs, which represent factual knowledge via entities and relations, have traditionally been static due to high construction costs, but recent advances in Transformer-based RE allow dynamic KG generation from natural language texts. The proposed method converts each question sentence into a relational graph using RE and then evaluates its truthfulness against verified KGs under a closed-world assumption to avoid misinformation from factually incorrect inputs. Experimental results indicate that the method correctly answers approximately 70% of MCQs while maintaining transparency in reasoning. Furthermore, the study emphasizes that the category of questions significantly affects accuracy, suggesting certain types of questions align better with this approach. Overall, this method offers a promising direction for interpretable and accurate MCQ answering by integrating dynamic KG construction with validity checks against trusted knowledge sources. <div>
arXiv:2511.14144v1 Announce Type: new 
Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Weak-to-Strong Generalization</title>
<link>https://arxiv.org/abs/2511.14166</link>
<guid>https://arxiv.org/abs/2511.14166</guid>
<content:encoded><![CDATA[
<div> Keywords: superhuman, weak supervision, selective W2SG, alignment, robustness  

<br /><br />Summary: The paper highlights the challenges of aligning superhuman models with human oversight when traditional high-quality data is lacking. It addresses the pitfalls of existing weak-to-strong generalization (W2SG) approaches that rely heavily on weak supervision, which can negatively impact model robustness due to potentially harmful weak labels. To improve this, the authors propose a selective W2SG framework designed to bypass weak supervision when it's unnecessary. Central to their method is a binary classifier, denoted as P(IK), which is trained to identify questions that a strong pretrained model can answer effectively. This classifier generates self-generated labels for model alignment, enhancing the overall quality of the training process. Additionally, the authors implement a graph smoothing technique to refine weak labels further. Through extensive experimentation across three benchmarks, the proposed method consistently demonstrates improved performance compared to competitive baselines. The analyses reveal that the P(IK) classifier exhibits generalization capabilities across various tasks and difficulties, reinforcing the idea that selective W2SG can significantly contribute to the challenge of superalignment in AI models. <div>
arXiv:2511.14166v1 Announce Type: new 
Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA</title>
<link>https://arxiv.org/abs/2511.14172</link>
<guid>https://arxiv.org/abs/2511.14172</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, hallucination, symbolic knowledge, localization, attention variance  

<br /><br />Summary:  
Large Language Models (LLMs) often exhibit hallucination, particularly influenced by symbolic triggers like modifiers, negation, numbers, and named entities. However, the origins of these symbolic hallucinations are not well understood. Existing localization methods, such as LSC and activation variance analysis, tend to treat all tokens equally and ignore the impact of symbolic linguistic knowledge. This paper presents a novel symbolic localization framework that utilizes symbolic linguistic and semantic knowledge to trace hallucination development across various model layers. By analyzing five different models with tools like HaluEval and TruthfulQA, the study finds that attention variance related to these linguistic elements increases dramatically in the early layers of the models (layers 2-4), with negation causing particularly high instability. Despite advances in model size, hallucination rates remain alarmingly high (78.3%-83.7% across Gemma variants), coupled with significant attention drops for symbolic semantic triggers in deeper layers. The results suggest that hallucination issues stem from failures in symbolic linguistic processing rather than from broader generation problems, indicating that a focus on symbolic semantic knowledge is vital for comprehending and addressing hallucination mechanisms in LLMs. <div>
arXiv:2511.14172v1 Announce Type: new 
Abstract: LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Deep LLM Participation for Robust Entity Linking</title>
<link>https://arxiv.org/abs/2511.14181</link>
<guid>https://arxiv.org/abs/2511.14181</guid>
<content:encoded><![CDATA[
<div> Keywords: Entity Linking, Large Language Models, DeepEL, Self-validation, Contextual Disambiguation<br /><br />Summary:<br /><br />1. This paper addresses the task of Entity Linking (EL), which involves mapping textual mentions to corresponding entries in knowledge bases, a key aspect of natural language understanding.<br /><br />2. While prior work leverages Large Language Models (LLMs) to enhance specific components such as entity disambiguation or input representation, these approaches do not fully exploit LLM capabilities throughout the entire EL process.<br /><br />3. The authors propose DeepEL, a novel framework integrating LLMs comprehensively across all EL stages, improving the coherence and accuracy of entity linking.<br /><br />4. A key innovation in DeepEL is a self-validation mechanism that harnesses global contextual information, allowing LLMs to iteratively refine their predictions and better capture relationships among multiple entities within the same sentence.<br /><br />5. Extensive experiments on ten benchmark datasets demonstrate that DeepEL outperforms current state-of-the-art approaches, yielding an average 2.6% improvement in overall F1 score and a notable 4% gain on out-of-domain data, highlighting the benefits of deep integration of LLMs in EL tasks. <div>
arXiv:2511.14181v1 Announce Type: new 
Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC</title>
<link>https://arxiv.org/abs/2511.14230</link>
<guid>https://arxiv.org/abs/2511.14230</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, Arabic, multi-system approach, machine learning, linguistic error correction

<br /><br />Summary: This paper addresses the challenges of Grammatical Error Correction (GEC) in Arabic, a language with complex morphological and syntactic structures. The authors introduce the Arab Enhanced Edit Selection System Complication (ArbESC+), which is one of the first multi-system approaches to Arabic grammatical error correction. Unlike previous methods that employed individual models, ArbESC+ combines various models to leverage their strengths. The system gathers correction proposals from several models and represents these as numerical features. A classifier then analyzes these features to implement suitable corrections while ensuring quality through support techniques that filter overlapping corrections and assess decision reliability. The integration of models such as AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, along with text editing systems, yielded superior results compared to single models, achieving an F0.5 score of 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. This work represents a significant advancement in Arabic linguistic error correction, pushing forward the development of refined tools for users and researchers involved in Arabic text processing. <div>
arXiv:2511.14230v1 Announce Type: new 
Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuCPT: Music-related Natural Language Model Continued Pretraining</title>
<link>https://arxiv.org/abs/2511.14245</link>
<guid>https://arxiv.org/abs/2511.14245</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, music domain, data pipeline, token-level soft scoring, MusicSimpleQA<br /><br />Summary: Large language models (LLMs) generally excel in broad applications but struggle in specialized domains like music, particularly in the music-entertainment sector where the scale, purity, and alignment of training data are crucial. This study addresses these issues by constructing a massive 40 billion token music-related natural language corpus sourced from a combination of open-source and proprietary data. A domain-first data pipeline is implemented using a lightweight classifier to filter and weight relevant in-domain text, followed by stages of cleaning, de-duplication, and privacy-preserving masking. Additionally, the authors integrate multi-source music text with associated metadata, creating a richer and better-structured knowledge base. A novel training strategy is introduced using reference-model-based token-level soft scoring, applying a unified loss-ratio criterion for both data selection and dynamic down-weighting during optimization. This approach reduces noisy gradient contributions and strengthens task-aligned signals, enabling more efficient domain-specific continued pretraining and fine-tuning. To evaluate factual accuracy, the paper presents MusicSimpleQA, a benchmark based on short, single-answer prompts with automated agreement scoring. Comprehensive comparisons along different dimensions of data composition are conducted. Overall, the work advances scalable data and training methodologies alongside a reusable evaluation framework to support the development of domain-specific LLMs in the music domain. <div>
arXiv:2511.14245v1 Announce Type: new 
Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning</title>
<link>https://arxiv.org/abs/2511.14249</link>
<guid>https://arxiv.org/abs/2511.14249</guid>
<content:encoded><![CDATA[
<div> Keywords: automatic movie dubbing, Retrieve-Augmented, emotional representations, multimodal signals, speech generation

<br /><br />Summary: The study presents a new model called Authentic-Dubber for automatic movie dubbing, which generates speech from scripts while mimicking the speaker's timbre based on brief prompts and ensuring lip-sync with silent videos. Unlike existing methods that ignore director-actor interactions, this model emphasizes the dynamic collaboration essential in authentic dubbing workflows. It introduces a Retrieve-Augmented Director-Actor Interaction Learning scheme that includes three innovative mechanisms. First, it creates a multimodal Reference Footage library to simulate the director's guidance, utilizing Large Language Models (LLMs) for deep emotional comprehension across different media. Second, an Emotion-Similarity-based Retrieval-Augmentation strategy is proposed to help actors internalize relevant footage that aligns with target videos effectively. Third, a Progressive Graph-based speech generation approach is developed, which incrementally integrates the retrieved emotional knowledge. Together, these components enhance the dubbing process, leading to significant improvements in emotional expressiveness. The model's effectiveness is validated through both subjective and objective evaluations on the V2C Animation benchmark dataset. The code and demonstrations for Authentic-Dubber are made available online for further exploration and development. <div>
arXiv:2511.14249v1 Announce Type: new 
Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR</title>
<link>https://arxiv.org/abs/2511.14255</link>
<guid>https://arxiv.org/abs/2511.14255</guid>
<content:encoded><![CDATA[
<div> Keywords: AfriSpeech-MultiBench, African English, voice interfaces, ASR models, multimodal LLMs

<br /><br />Summary: Recent advancements in AI-driven voice interfaces, like Google's NotebookLM and OpenAI's speech-to-speech API, have sparked interest worldwide. However, there is a lack of evaluation models tailored to Africa's linguistic diversity. To address this gap, the authors introduce AfriSpeech-MultiBench, the first benchmarking suite focused on over 100 African English accents from 10+ countries across seven domains, including Finance and Medical. This benchmark evaluates various open and proprietary ASR and LLM systems using both spontaneous and structured speech from African speech datasets. The findings show that open-source ASR models perform well in spontaneous contexts but falter in noisy, non-native conversations. In contrast, multimodal LLMs demonstrate better accent robustness but face challenges with domain-specific named entities. Proprietary models yield high accuracy on clean speech, albeit with variance based on country and domain. Models specifically fine-tuned on African English show competitive accuracy and lower latency, beneficial for deployment. However, hallucination issues persist in most state-of-the-art models. This benchmark enables researchers and practitioners to better select voice technologies for African contexts, promoting inclusive voice applications for underrepresented communities. <div>
arXiv:2511.14255v1 Announce Type: new 
Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Guided Reasoning Compression</title>
<link>https://arxiv.org/abs/2511.14258</link>
<guid>https://arxiv.org/abs/2511.14258</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning models, compression methods, entropy conflict, efficient reasoning, mathematical benchmarks  
<br /><br />Summary: Large reasoning models excel in complex reasoning tasks, but the long outputs hinder their practical use due to high computational demands. While existing compression techniques have made progress, they often ignore a critical issue termed the entropy conflict. This occurs when the dual objectives of decreasing entropy for shorter reasoning conflict with accuracy goals that tend to increase entropy due to emphasis on logical connectors. These connectors receive significant gradients and are penalized during compression, leading to a local dilemma for the model. To resolve this issue, the authors propose an entropy-guided training framework that balances these conflicting objectives. As entropy decreases, the model learns to produce concise outputs, while increases in entropy promote exploration to enhance robustness. Experiments conducted across six mathematical benchmarks demonstrate that this method successfully reduces reasoning length to 20% of the original while either improving or maintaining baseline accuracy. The research promises to enhance the efficiency of reasoning models, and the authors plan to publicly release their code and models to facilitate further research. <div>
arXiv:2511.14258v1 Announce Type: new 
Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space</title>
<link>https://arxiv.org/abs/2511.14275</link>
<guid>https://arxiv.org/abs/2511.14275</guid>
<content:encoded><![CDATA[
<div> Keywords: reliability, verbalized confidence, chain-of-thought reasoning, probability distribution, large language models (LLMs)  

<br /><br />Summary:  
This paper addresses the importance of understanding the reliability of responses generated by large language models (LLMs). The researchers focus on enhancing confidence estimation by generating verbalized confidence scores, which communicate how certain a model is about its answers. They build on chain-of-thought reasoning methods that provide logical and transparent estimations, but highlight that the impact of different reasoning strategies on confidence estimation is still not well understood. The key contribution is demonstrating that predicting a verbalized probability distribution over all possible answers encourages deeper and more comprehensive reasoning. This approach compels the LLM to consider multiple candidate answers rather than relying on a single guess, assigning confidence values that form a coherent distribution. Experimental results show that this method consistently outperforms alternatives across different LLM architectures and various tasks, regardless of whether the answer space is predefined or not. Moreover, the advantage of this probability-distribution-based confidence estimation persists even after applying reinforcement learning fine-tuning. Additional analysis reveals that the reasoning patterns employed by the models align closely with human expectations, suggesting improved interpretability and trustworthiness of the model’s confidence outputs. <div>
arXiv:2511.14275v1 Announce Type: new 
Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
<div> Keywords: AraLingBench, Arabic, language models, linguistic competence, benchmark

<br /><br />Summary: The article introduces AraLingBench, a newly developed benchmark designed to assess the Arabic linguistic competence of large language models (LLMs). This benchmark encompasses five key areas: grammar, morphology, spelling, reading comprehension, and syntax, featuring 150 multiple-choice questions created by experts that evaluate structural understanding of the Arabic language. An evaluation of 35 Arabic and bilingual LLMs indicates that while these models often perform well on surface-level tasks, they face challenges in deeper grammatical and syntactic reasoning. The findings highlight a significant disconnect between high performance on knowledge-based benchmarks and true linguistic mastery, revealing that many models rely more on memorization or pattern recognition instead of genuine comprehension. By distinctly measuring essential linguistic capabilities, AraLingBench serves as a diagnostic tool for the advancement of Arabic LLMs. The full evaluation code for AraLingBench is made publicly available on GitHub, promoting transparency and facilitating further research in this important area of natural language processing. <div>
arXiv:2511.14295v1 Announce Type: new 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions</title>
<link>https://arxiv.org/abs/2511.14342</link>
<guid>https://arxiv.org/abs/2511.14342</guid>
<content:encoded><![CDATA[
<div> Keywords: Instruction-following, Large Language Models, conflict detection, ConInstruct, benchmarks

<br /><br />Summary: This paper introduces ConInstruct, a new benchmark designed to evaluate how well Large Language Models (LLMs) can detect and resolve conflicts within user instructions, especially when instructions contain opposing constraints. The importance of instruction-following in LLMs is highlighted, while noting that prior studies have largely neglected the issue of conflicting prompts, which are common in complex scenarios. Experiments conducted using this benchmark reveal two significant findings: (1) Most proprietary LLMs demonstrate robust conflict detection abilities; however, among open-source models, only DeepSeek-R1 shows comparable performance. In fact, DeepSeek-R1 and Claude-4.5-Sonnet achieve the top average F1-scores of 91.5% and 87.3%, respectively. (2) Despite possessing strong conflict detection skills, LLMs infrequently inform users about detected conflicts or seek clarification when faced with conflicting instructions. The study highlights these shortcomings in current LLM designs and emphasizes the need for improvement in future instruction-following capabilities. <div>
arXiv:2511.14342v1 Announce Type: new 
Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.14365</link>
<guid>https://arxiv.org/abs/2511.14365</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, tokenization bottleneck, SMILES, vocabulary extension, chemistry-domain tasks

<br /><br />Summary: The paper addresses a significant challenge in applying large language models (LLMs) to chemistry, known as the "tokenization bottleneck". This issue arises because standard tokenizers, designed for general text, inadequately process chemical representations like SMILES, breaking them into less meaningful sub-tokens. To tackle this problem, the authors propose a unified model that merges the representation of natural language with molecular structures. Their methodology includes expanding the vocabulary of a pretrained LLM by incorporating tokens that are particularly relevant to chemistry. Following this vocabulary enhancement, the model is subjected to continued pretraining using chemistry-specific texts to better assimilate this new information. The paper presents empirical results demonstrating that this integrated approach significantly enhances the model's performance across various downstream tasks in the chemistry domain. The findings suggest that by aligning LLM capabilities with the specific linguistic and structural features of chemical data, researchers can overcome the limitations imposed by conventional tokenization strategies, resulting in more effective and informative representations of chemical information. <div>
arXiv:2511.14365v1 Announce Type: new 
Abstract: The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning</title>
<link>https://arxiv.org/abs/2511.14366</link>
<guid>https://arxiv.org/abs/2511.14366</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, ATLAS, cross-disciplinary, high-difficulty, evaluation suite

<br /><br />Summary: The rapid progression of Large Language Models (LLMs) has led to performance saturation in established benchmarks, prompting a reevaluation of their capability to distinguish top models. Current high-difficulty benchmarks often have limited disciplinary focus and are susceptible to data contamination, leading to a disconnect with actual scientific inquiry. In response, the study presents ATLAS (AGI-Oriented Testbed for Logical Application in Science), an extensive assessment tool featuring around 800 original problems crafted by PhD-level domain experts across seven scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Key attributes of ATLAS include: (1) originality and resistance to contamination, with all questions newly created or adapted; (2) a cross-disciplinary approach that evaluates models’ integration of knowledge across various fields; (3) prioritization of complex, multi-step reasoning for answers, favoring detailed responses over simple multiple-choice formats; and (4) stringent quality control via expert peer review and adversarial testing to ensure question difficulty and scientific integrity. The proposed evaluation method also incorporates LLM judges for automated assessment, with preliminary results highlighting ATLAS's effectiveness in discerning advanced scientific reasoning skills in leading models. <div>
arXiv:2511.14366v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Label Length Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14385</link>
<guid>https://arxiv.org/abs/2511.14385</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, label length bias, normalized contextual calibration, multi-token labels, few-shot learning<br /><br />Summary:<br /><br />Large language models (LLMs) are effective zero- and few-shot learners but face challenges when predicting over candidate options due to label biases. Among these biases, label length bias causes inconsistency in handling labels of varying token lengths, even after applying standard length normalization techniques. To address this, the authors introduce normalized contextual calibration (NCC), a novel method that normalizes and calibrates predictions at the full-label level instead of token-wise adjustments. NCC demonstrates statistically significant improvements over existing calibration methods across various datasets and models, achieving up to 10% F1 score gains. Additionally, NCC extends its bias mitigation capabilities to diverse tasks such as multiple-choice question answering, showcasing its versatility. The study reveals that combining NCC with in-context learning enhances stability by reducing sensitivity to the selection of few-shot examples and reduces the number of examples needed for competitive performance. Furthermore, NCC contributes to producing more reliable confidence estimates in predictions. Overall, the findings emphasize the importance of mitigating full-label biases in LLMs to enhance accuracy, robustness, and applicability in real-world scenarios where multi-token class labels are common. <div>
arXiv:2511.14385v1 Announce Type: new 
Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education</title>
<link>https://arxiv.org/abs/2511.14423</link>
<guid>https://arxiv.org/abs/2511.14423</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, educational safety, jailbreak attacks, fine-tuning attacks, three-stage shield framework  

<br /><br />Summary:  
Large Language Models (LLMs) are increasingly used in educational tools, but they face safety challenges from jailbreak and fine-tuning attacks that can produce harmful outputs. Existing research primarily addresses general safety without focusing on the specific needs of educational environments. To fill this gap, the authors introduce EduHarm, a benchmark dataset of safe versus unsafe instructional pairs covering five key educational scenarios, designed to systematically evaluate educational LLM safety. They also propose a novel three-stage shield framework (TSSF) to simultaneously defend against jailbreak and fine-tuning attacks. The first stage, safety-aware attention realignment, adjusts the model's focus to highlight critical unsafe tokens and restore harmfulness detection features. The second stage, layer-wise safety judgment, aggregates safety signals across multiple model layers to identify unsafe instructions accurately. The final stage, defense-driven dual routing, channels safe queries through normal processing and unsafe queries through protective responses, preventing harmful outputs while avoiding over-blocking benign inputs. Extensive experiments with eight jailbreak attack methods demonstrate TSSF’s effectiveness in enhancing safety and minimizing unnecessary refusals. Additional evaluations across three fine-tuning attack datasets confirm TSSF’s ability to maintain robust defense against harmful queries while preserving performance benefits from legitimate fine-tuning. <div>
arXiv:2511.14423v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents</title>
<link>https://arxiv.org/abs/2511.14439</link>
<guid>https://arxiv.org/abs/2511.14439</guid>
<content:encoded><![CDATA[
<div> Medical AI, benchmarking, large language models, multimodal models, clinical safety<br /><br />Summary:  
This paper introduces MedBench v4, a comprehensive, cloud-based benchmarking platform designed to evaluate medical AI models in realistic clinical workflows and safety contexts. It features over 700,000 expert-curated tasks covering 24 primary and 91 secondary medical specialties, with specific tracks for large language models (LLMs), multimodal models, and agentic systems. The tasks undergo rigorous refinement and are reviewed by clinicians from more than 500 institutions to ensure quality and clinical relevance. Open-ended model responses are rated using an LLM calibrated against human judgments. The authors evaluate 15 advanced models, finding that base LLMs achieve an average overall score of 54.1/100, with Claude Sonnet 4.5 performing best at 62.5/100, yet scores on safety and ethics remain notably low (18.4/100). Multimodal models show weaker cross-modal reasoning despite solid perception, averaging 47.5/100, with GPT-5 as the top performer at 54.9/100. Agent-based models built on these backbones significantly improve performance, reaching an average of 79.8/100 overall and up to 88.9/100 on safety tasks. The study highlights ongoing challenges in safety and multimodal reasoning for base models and demonstrates that governance-aware agents can enhance clinical readiness significantly. MedBench v4 aligns with Chinese clinical guidelines, offering a valuable auditing tool for healthcare providers, developers, and policymakers. <div>
arXiv:2511.14439v1 Announce Type: new 
Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning</title>
<link>https://arxiv.org/abs/2511.14445</link>
<guid>https://arxiv.org/abs/2511.14445</guid>
<content:encoded><![CDATA[
<div> Keywords: mental well-being, large language models, retrieval-augmented generation, synthetic dialogue, personalized self-care  

<br /><br />Summary:  
The article presents "Tell Me," a mental well-being system designed to provide accessible support leveraging advancements in large language models. The system comprises three core components: (i) a retrieval-augmented generation (RAG) assistant that facilitates personalized, knowledge-informed dialogue; (ii) a synthetic dialogue generator that creates client-therapist interactions based on client profiles, aiding research in therapeutic language and data augmentation; and (iii) the Well-being AI crew, which generates weekly self-care plans and guided meditation audio. Importantly, the system serves as a reflective space for emotional processing and does not replace professional therapy, highlighting its role in enhancing access to mental health resources. To address the scarcity of confidential therapeutic data, the authors introduce a synthetic dialogue generation approach. Additionally, the planner showcases a unique, adaptive workflow for personalized self-care, overcoming the limitations of static well-being tools. The paper describes the architecture and functionality of the system and shares the evaluation results of the RAG assistant through automatic judgments and human-user studies, emphasizing potential interdisciplinary collaboration between NLP researchers and mental health professionals for responsible human-AI interaction in well-being. <div>
arXiv:2511.14445v1 Announce Type: new 
Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.14460</link>
<guid>https://arxiv.org/abs/2511.14460</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, LLM Agents, Markov Decision Process, Agent-R1  

<br /><br />Summary: Large Language Models (LLMs) are increasingly used to build Agents capable of interactive problem-solving by engaging with their environments, such as through tool use. Reinforcement Learning (RL) holds substantial promise for training these LLM-based Agents, but applying RL effectively in this context remains an early-stage challenge. The current research landscape lacks detailed investigations of RL methods tailored specifically for LLM Agents, as well as flexible, extensible frameworks that support their training needs. Addressing these gaps, the paper first extends the Markov Decision Process (MDP) framework to define essential components unique to LLM Agents, providing a systematic foundation for RL methodology in this domain. Secondly, the authors introduce Agent-R1, a modular and adaptable training framework designed to be user-friendly and easily customizable across varied tasks and interactive environments. To validate their approach, the study conducts experiments on Multihop Question Answering benchmark tasks, demonstrating the initial effectiveness and practical potential of both the proposed RL methodologies and the Agent-R1 training framework. This work aims to accelerate progress in the field of RL-based LLM Agents by offering theoretical clarity and practical tools to support future research and application development. <div>
arXiv:2511.14460v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiveRAG: A diverse Q&amp;A dataset with varying difficulty level for RAG evaluation</title>
<link>https://arxiv.org/abs/2511.14531</link>
<guid>https://arxiv.org/abs/2511.14531</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval Augmented Generation, LiveRAG benchmark, Q&amp;A systems, Item Response Theory, evaluation

<br /><br />Summary: This article presents the LiveRAG benchmark, a new dataset designed for evaluating Retrieval Augmented Generation (RAG) based Q&amp;A systems. Comprising 895 synthetic questions and answers, this benchmark aims to facilitate systematic evaluation in the growing field of generative AI. It is derived from the dataset used in the SIGIR'2025 LiveRAG Challenge, and contains additional information, such as ground-truth answers and supporting claims, which were initially unavailable to competitors. Each question in the benchmark is accompanied by estimated difficulty and discriminability scores, derived using an Item Response Theory model based on competitors' responses. The analysis emphasizes the diversity of questions within the benchmark, the varying levels of difficulty, and its capacity to distinguish between different system capabilities. By offering a structured approach to assess RAG solutions, the LiveRAG benchmark hopes to promote advancements in RAG research, enable rigorous evaluation practices, and foster the development of more effective Q&amp;A systems within the AI community. <div>
arXiv:2511.14531v1 Announce Type: new 
Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&amp;A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&amp;A systems.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak</title>
<link>https://arxiv.org/abs/2511.14566</link>
<guid>https://arxiv.org/abs/2511.14566</guid>
<content:encoded><![CDATA[
<div> Keywords: document-level claim extraction, claim alignment, evaluation framework, semantic similarity, Czech and Slovak news

<br /><br />Summary:  
This work addresses the challenge of document-level claim extraction in fact-checking, focusing on the evaluation methods for the extracted claims. The authors propose an approach to align two sets of claims from the same source document and calculate an alignment score that reflects their similarity. They investigate techniques that identify the best possible alignment and evaluation methods between claim sets, aiming to establish a reliable evaluation framework. The proposed approach facilitates comparison between model-extracted claims and human-annotated claims, serving both as a performance metric for extraction models and a measure of inter-annotator agreement. Experiments are conducted on a newly collected dataset comprising claims extracted from comments on Czech and Slovak news articles, which present challenges like informal language, strong local context, and linguistic subtleties between the two closely related languages. Results highlight the limitations of current evaluation methods when applied to document-level claim extraction. The study emphasizes the need for advanced evaluation techniques capable of accurately capturing semantic similarity and assessing critical claim properties such as atomicity, checkworthiness, and decontextualization, essential for improving fact-checking systems. <div>
arXiv:2511.14566v1 Announce Type: new 
Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.14598</link>
<guid>https://arxiv.org/abs/2511.14598</guid>
<content:encoded><![CDATA[
<div> Keywords: summarization, under-represented languages, historical newspapers, HEBTEASESUM, automatic process

<br /><br />Summary: This work addresses the scarcity of high-quality summarization data in under-represented languages by leveraging historical newspapers, which have become accessible through recent digitization efforts. The authors propose a novel method for obtaining naturally occurring summaries using Front-Page Teasers, where newspaper editors provide concise summaries of full-length articles. This phenomenon is observed across seven diverse languages, demonstrating its potential for multi-document summarization tasks. To streamline the data collection process, the authors develop an automatic method tailored to various linguistic resource levels, making it adaptable to different contexts. The application of this automatic process is illustrated through the creation of HEBTEASESUM, marking the introduction of the first dedicated multi-document summarization dataset in Hebrew. Overall, this work highlights the value of historical newspapers as a rich source of summarization data and provides a systematic approach to harnessing this resource effectively for under-represented languages. <div>
arXiv:2511.14598v1 Announce Type: new 
Abstract: High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease</title>
<link>https://arxiv.org/abs/2511.14603</link>
<guid>https://arxiv.org/abs/2511.14603</guid>
<content:encoded><![CDATA[
<div> Keywords: acute kidney injury, chronic kidney disease, electronic health records, CKD risk factors, multi-state modeling  

<br /><br />Summary: The study investigates the risk of developing chronic kidney disease (CKD) in patients with acute kidney injury (AKI) using electronic health record (EHR) data. It aims to identify high-risk individuals by tracking their clinical progression after AKI. Researchers used patient data, including longitudinal medical codes and creatinine levels, to categorize post-AKI clinical states through clustering techniques. They employed multi-state modeling to estimate transition probabilities between different clinical states and the likelihood of progressing to CKD. Out of 20,699 patients with AKI at admission, 3,491 (17%) progressed to CKD. The analysis revealed fifteen distinct post-AKI states, each associated with varying CKD development probabilities. A significant portion of patients (75%, n=15,607) either remained in one state or transitioned minimally during the study. The study identified both established risk factors for CKD—such as AKI severity, diabetes, hypertension, heart failure, and liver disease—and novel factors whose impact differed across clinical states. This data-driven approach offers a foundation for developing decision-support tools aimed at early detection and intervention for CKD in vulnerable AKI patients. <div>
arXiv:2511.14603v1 Announce Type: new 
Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.14606</link>
<guid>https://arxiv.org/abs/2511.14606</guid>
<content:encoded><![CDATA[
<div> Keywords: political bias, natural language processing, large language models, RoBERTa, human annotations  

<br /><br />Summary:  
This study addresses the complex task of detecting political bias in news media, highlighting the necessity of interpreting linguistic and contextual cues. While advancements in Natural Language Processing (NLP) have paved the way for automatic bias classification, the alignment between large language models (LLMs) and human judgment remains underexplored. The authors propose a comparative framework that evaluates political bias detection using human annotations alongside multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. A manually annotated dataset of news articles was created to assess annotation consistency, bias polarity, and inter-model agreement. Findings indicate that RoBERTa, among traditional transformer models, demonstrates the highest alignment with human labels, while generative models like GPT show strong correspondence in a zero-shot setting. The fine-tuned RoBERTa model exhibited the best accuracy and alignment with human annotations. The results reveal systematic differences in bias perception between humans and LLMs, emphasizing the importance of hybrid evaluation frameworks that integrate human interpretability with the scalability of automated tools for media bias detection. <div>
arXiv:2511.14606v1 Announce Type: new 
Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities</title>
<link>https://arxiv.org/abs/2511.14631</link>
<guid>https://arxiv.org/abs/2511.14631</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent systems, vision-language models, autonomous scientific discovery, exploratory data analysis, interpretability  

<br /><br />Summary: The article demonstrates that multi-agent systems enhanced by vision-language models (VLMs) significantly improve end-to-end autonomous scientific discovery. These systems utilize plots as verifiable checkpoints where a VLM acts as a judge, evaluating figures based on dynamically generated, domain-specific rubrics. This capability allows the agents to identify and correct their own errors, steering exploratory data analysis in real-time. Case studies conducted in the fields of cosmology and astrochemistry illustrate how these systems can recover from faulty reasoning paths and adapt to new datasets without requiring human intervention. In a benchmark comprising 10 tasks related to data-driven discovery, VLM-augmented systems achieved pass-at-1 scores ranging from 0.7-0.8. This performance is significantly better compared to code-only systems, which scored 0.2-0.3, and code-and-text baselines, which had scores of 0.4-0.5. Additionally, these systems provide auditable reasoning traces, enhancing interpretability and allowing for a better understanding of the decision-making process. A link to the associated code is available for further exploration. <div>
arXiv:2511.14631v1 Announce Type: new 
Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases</title>
<link>https://arxiv.org/abs/2511.14638</link>
<guid>https://arxiv.org/abs/2511.14638</guid>
<content:encoded><![CDATA[
<div> Rare diseases, diagnosis, large language models, electronic health records, clinical decision support  

<br /><br />Summary:  
This study addresses the challenge of diagnosing rare diseases, which affect millions but often take years to identify. Conventional diagnostic methods separate noisy evidence extraction from downstream inferential reasoning, limiting accuracy and efficiency. Large language models (LLMs), both general and medical, struggle due to limited real-world electronic health record (EHR) data, outdated domain knowledge, and hallucination problems. To overcome these issues, the authors compiled a large, domain-specific clinical corpus alongside a clinician-validated reasoning dataset. They developed RareSeek R1 using staged instruction tuning, chain-of-thought learning, and graph-grounded retrieval techniques. RareSeek R1 demonstrated state-of-the-art accuracy, strong generalization across multicenter EHR narratives, and resilience to noisy or overlapping phenotypes. The system’s augmented retrieval, especially when combining narratives with prioritized genetic variants, resolved ambiguities and aligned candidate diagnoses with underlying mechanisms, producing the greatest performance improvements. Human evaluation showed RareSeek R1 matches experienced physicians in diagnostic ability and provides consistent gains when used as an assistive tool. Importantly, its transparent reasoning models non-phenotypic evidence, such as imaging and functional tests, which supported nearly a quarter of correct diagnoses. This work pioneers a narrative-first, knowledge-integrated reasoning paradigm advancing clinically translatable, auditable diagnostic decision support that can shorten the diagnostic odyssey for rare diseases. <div>
arXiv:2511.14638v1 Announce Type: new 
Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graded strength of comparative illusions is explained by Bayesian inference</title>
<link>https://arxiv.org/abs/2511.14642</link>
<guid>https://arxiv.org/abs/2511.14642</guid>
<content:encoded><![CDATA[
<div> Comparative Illusion, Noisy Channel, Bayesian Inference, Language Processing, Statistical Language Models<br /><br />Summary:<br /><br />1. The article investigates the comparative illusion (CI) in language processing, where people often accept sentences that contain nonsensical comparisons, such as "More students have been to Russia than I have."  
2. Prior work suggested that this phenomenon can be explained by Bayesian inference over a noisy communication channel, where the brain infers the most probable intended sentence by combining prior interpretation likelihoods with the chance of corruption leading to the observed CI sentence.  
3. Initial behavioral evidence supported this explanation by showing that comprehenders tend to favor interpretations likely to be corrupted into the illusory CI sentence.  
4. This study advances the theory by quantitatively modeling the posterior probabilities of plausible CI sentence interpretations, integrating statistical language models with human behavioral data for more precise predictions.  
5. The model accounts not only for subtle variations in CI strength but also explains a previously unexplained effect regarding differences between pronominal and full noun phrase than-clause subjects.  
6. These results provide strong empirical support for the noisy-channel theory as a unified computational-level account of diverse language processing phenomena, encompassing both illusory and non-illusory contexts. <div>
arXiv:2511.14642v1 Announce Type: new 
Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias in, Bias out: Annotation Bias in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2511.14662</link>
<guid>https://arxiv.org/abs/2511.14662</guid>
<content:encoded><![CDATA[
<div> Annotation bias, multilingual LLMs, cultural diversity, bias detection, bias mitigation  

<br /><br />Summary:  
The article addresses annotation bias in Natural Language Processing (NLP) datasets, highlighting its critical impact on the development of multilingual Large Language Models (LLMs), especially in culturally diverse environments. It introduces a comprehensive framework categorizing annotation bias into instruction bias, annotator bias, and contextual/cultural bias. The paper reviews existing detection methods such as inter-annotator agreement, model disagreement, and metadata analysis, while also presenting emerging techniques like multilingual model divergence and cultural inference. For bias mitigation, the authors propose both proactive and reactive strategies, including diverse annotator recruitment, continuous guideline refinement, and post-hoc adjustments to models. Key contributions comprise (1) establishing a typology of annotation bias, (2) synthesizing various detection metrics, (3) proposing an ensemble-based bias mitigation method tailored for multilingual contexts, and (4) providing an ethical analysis of annotation workflows. Collectively, these insights endeavor to foster more equitable, culturally informed annotation practices, thereby improving the fairness and performance of multilingual LLMs. <div>
arXiv:2511.14662v1 Announce Type: new 
Abstract: Annotation bias in NLP datasets remains a major challenge for developing multilingual Large Language Models (LLMs), particularly in culturally diverse settings. Bias from task framing, annotator subjectivity, and cultural mismatches can distort model outputs and exacerbate social harms. We propose a comprehensive framework for understanding annotation bias, distinguishing among instruction bias, annotator bias, and contextual and cultural bias. We review detection methods (including inter-annotator agreement, model disagreement, and metadata analysis) and highlight emerging techniques such as multilingual model divergence and cultural inference. We further outline proactive and reactive mitigation strategies, including diverse annotator recruitment, iterative guideline refinement, and post-hoc model adjustments. Our contributions include: (1) a typology of annotation bias; (2) a synthesis of detection metrics; (3) an ensemble-based bias mitigation approach adapted for multilingual settings, and (4) an ethical analysis of annotation processes. Together, these insights aim to inform more equitable and culturally grounded annotation pipelines for LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Streamlining Industrial Contract Management with Retrieval-Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.14671</link>
<guid>https://arxiv.org/abs/2511.14671</guid>
<content:encoded><![CDATA[
<div> Contract management, retrieval-augmented generation, synthetic data, acceptability classification, reward-based alignment<br /><br />Summary:  
This paper addresses the challenge of automating contract management, particularly focusing on the review and negotiation of contract provisions that define rights, obligations, and terms. The authors highlight the difficulty posed by the scarcity of labeled data and the abundance of unstructured legacy contracts during this process. To overcome these challenges, they propose a modular framework based on a retrieval-augmented generation (RAG) pipeline. Key components of their system include synthetic data generation to augment training resources, semantic clause retrieval to efficiently locate relevant contract provisions, acceptability classification to identify problematic or unacceptable revisions, and reward-based alignment to generate improved alternative clauses. The framework was developed and evaluated in partnership with an industry collaborator, ensuring its practical relevance. Experimental results demonstrate that the system achieves over 80% accuracy in both detecting problematic revisions and suggesting optimized alternatives. This performance underscores the method’s effectiveness in real-world, low-resource environments. Ultimately, the proposed framework offers a viable solution to accelerate contract revision workflows, reducing manual effort and improving the quality of contractual agreements. <div>
arXiv:2511.14671v1 Announce Type: new 
Abstract: Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quadratic Term Correction on Heaps' Law</title>
<link>https://arxiv.org/abs/2511.14683</link>
<guid>https://arxiv.org/abs/2511.14683</guid>
<content:encoded><![CDATA[
<div> Keywords: Heaps' law, word-type, word-token, power-law, quadratic functions  
  
<br /><br />Summary: Heaps' or Herdan's law describes the relationship between word types and word tokens using a power-law function that appears concave in linear-linear scale, yet is linear in log-log scale. Despite this, evidence suggests that even on the log-log scale, the type-token curve maintains slight concavity, challenging the validity of the power-law approximation. An analysis of twenty English novels and translated texts indicates that quadratic functions offer a superior fit for the type-token data in log-log scale. Regression analyses reveal that the linear coefficient is slightly greater than 1, while the quadratic coefficient hovers around -0.02. To further understand the underlying mechanics, the authors employ a model involving "random drawing of colored balls from a bag with replacement," establishing that the curvature seen in the log-log scale corresponds to a negative "pseudo-variance." Although calculating pseudo-variance can lead to numerical instability with large token counts, this framework provides a rough estimate of curvature when the number of tokens is smaller, enhancing our grasp of linguistic distribution patterns. <div>
arXiv:2511.14683v1 Announce Type: new 
Abstract: Heaps' or Herdan's law characterizes the word-type vs. word-token relation by a power-law function, which is concave in linear-linear scale but a straight line in log-log scale. However, it has been observed that even in log-log scale, the type-token curve is still slightly concave, invalidating the power-law relation. At the next-order approximation, we have shown, by twenty English novels or writings (some are translated from another language to English), that quadratic functions in log-log scale fit the type-token data perfectly. Regression analyses of log(type)-log(token) data with both a linear and quadratic term consistently lead to a linear coefficient of slightly larger than 1, and a quadratic coefficient around -0.02. Using the ``random drawing colored ball from the bag with replacement" model, we have shown that the curvature of the log-log scale is identical to a ``pseudo-variance" which is negative. Although a pseudo-variance calculation may encounter numeric instability when the number of tokens is large, due to the large values of pseudo-weights, this formalism provides a rough estimation of the curvature when the number of tokens is small.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction</title>
<link>https://arxiv.org/abs/2511.14684</link>
<guid>https://arxiv.org/abs/2511.14684</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning errors, SMRC, Monte Carlo Tree Search, educational applicability

<br /><br />Summary: Large language models (LLMs) frequently encounter reasoning errors in mathematical problem-solving, raising the need for automated detection and correction methods. Traditional approaches emphasize self-correction, which is inadequate for educational contexts requiring "teacher-style" guidance. To address this, the authors propose SMRC (Student Mathematical Reasoning Correction), a method that aligns LLMs with student reasoning. SMRC conceptualizes student reasoning as a multi-step sequential decision problem and employs Monte Carlo Tree Search (MCTS) to identify optimal correction paths. To minimize the annotation cost of process-level rewards, the method uses breadth-first search (BFS) accompanied by LLMs and evaluations of final answers to generate reward signals, which are back-propagated across intermediate reasoning steps for enhanced supervision. Additionally, the authors introduce the Multi-Solution Error Benchmark (MSEB), a dataset featuring 158 high school mathematics problems, student solutions, and correct reasoning steps. A dual evaluation protocol focusing on solution accuracy and correct-step retention is proposed to measure educational effectiveness comprehensively. Experiments reveal that SMRC greatly surpasses existing methods on two established datasets (ProcessBench and MR-GSM8K) and the newly created MSEB concerning effectiveness and overall performance. The relevant code and data are accessible at https://github.com/Mind-Lab-ECNU/SMRC. <div>
arXiv:2511.14684v1 Announce Type: new 
Abstract: Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries</title>
<link>https://arxiv.org/abs/2511.14685</link>
<guid>https://arxiv.org/abs/2511.14685</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, astrophysics, embeddings, prompting, autoencoders  

<br /><br />Summary: This paper explores the ability of Large Language Models (LLMs) to generalize across various domains and their potential to encode physical information. Specifically, it uses astrophysics as a case study to determine two main aspects: first, whether the way a model is prompted influences how it codifies physical summary statistics derived from scientific measurements, and second, which elements of language are most critical for accurately representing the physics involved in these measurements. To investigate these questions, the authors employ sparse autoencoders, which help extract interpretable features from textual descriptions. By analyzing how LLM embeddings can encapsulate scientific data, the study highlights the intersection between language and scientific measurement, providing insights into both prompting techniques and the linguistic features that best convey physical concepts. The research aims to contribute to a deeper understanding of how LLMs can integrate and represent complex scientific information, which is often challenging to express solely through text. <div>
arXiv:2511.14685v1 Announce Type: new 
Abstract: Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground Truth Generation for Multilingual Historical NLP using LLMs</title>
<link>https://arxiv.org/abs/2511.14688</link>
<guid>https://arxiv.org/abs/2511.14688</guid>
<content:encoded><![CDATA[
<div> Historical NLP, low-resource languages, large language models, French historical texts, Chinese historical texts<br /><br />Summary:<br /><br />This paper addresses the challenges in historical and low-resource natural language processing (NLP), primarily caused by limited annotated data and domain mismatches with modern datasets derived from the web. It focuses on historical French texts from the 16th to the 20th century and Chinese texts from 1900 to 1950. The authors utilize large language models (LLMs) to generate ground-truth annotations, which are otherwise difficult to obtain for such under-resourced corpora. By leveraging LLM-generated annotations on a subset of the corpus, the team fine-tuned the spaCy NLP framework to enhance its performance. This fine-tuning led to significant improvements in key NLP tasks, including part-of-speech tagging, lemmatization, and named entity recognition, specifically tailored for the historical period texts under study. The findings emphasize the value of domain-specific modeling rather than relying solely on general modern models. Furthermore, the research demonstrates that even a relatively small amount of synthetic, LLM-generated data can substantially improve the quality of NLP tools applicable to computational humanities research involving historical documents. <div>
arXiv:2511.14688v1 Announce Type: new 
Abstract: Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances</title>
<link>https://arxiv.org/abs/2511.14693</link>
<guid>https://arxiv.org/abs/2511.14693</guid>
<content:encoded><![CDATA[
<div> Keywords: complaint analysis, multimodal, VALOR, semantic alignment, SDG goals

<br /><br />Summary: Existing methods for complaint analysis typically focus on unimodal text sources like tweets or reviews. This study introduces VALOR, a novel framework designed for multimodal customer support dialogues that include both text and visual evidence for enhanced complaint classification. VALOR employs a multi-expert reasoning approach with large-scale generative models that utilize Chain-of-Thought (CoT) prompting for intricate decision-making. To ensure coherence across text and visual modalities, a semantic alignment score is calculated and incorporated into the final classification results through a meta-fusion strategy. The framework aligns with the United Nations Sustainable Development Goals (UN SDGs), specifically contributing to SDG 9 by fostering AI-driven tools for scalable service infrastructure and to SDG 12 by enabling structured analysis of complaints for better product design and accountability. VALOR is evaluated on a carefully curated multimodal dataset with fine-grained aspect and severity annotations and shows consistent performance improvements over baseline models, particularly in complex cases where information is dispersed across both text and images. This research highlights the significance of multimodal interaction and expert validation in enhancing complaint understanding systems. <div>
arXiv:2511.14693v1 Announce Type: new 
Abstract: Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subword Tokenization Strategies for Kurdish Word Embeddings</title>
<link>https://arxiv.org/abs/2511.14696</link>
<guid>https://arxiv.org/abs/2511.14696</guid>
<content:encoded><![CDATA[
<div> Keywords: Kurdish, tokenization, morpheme-based, Word2Vec, low-resource 

<br /><br />Summary: This article explores different tokenization strategies for creating Kurdish word embeddings, specifically comparing word-level, morpheme-based, and Byte Pair Encoding (BPE) approaches. The research develops a BiLSTM-CRF morphological segmenter using a bootstrapped training method with minimal manual annotation and assesses Word2Vec embeddings through various metrics such as similarity preservation, clustering quality, and semantic organization. A significant finding of the study is the identification of critical evaluation biases present in the tokenization comparison. Although BPE initially seems to outperform other methods in preserving morphological similarity, it evaluates only 28.6% of test cases compared to 68.7% for the morpheme-based approach, leading to artificially inflated performance metrics for BPE. Upon a more comprehensive assessment, the morpheme-based tokenization demonstrates better organization in the embedding space, superior semantic neighborhood structures, and more balanced coverage across different levels of morphological complexity. The findings underscore the importance of coverage-aware evaluations in processing low-resource languages and present viable tokenization methods tailored for such language processing tasks. <div>
arXiv:2511.14696v1 Announce Type: new 
Abstract: We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&amp;D, and Ethical Governance</title>
<link>https://arxiv.org/abs/2511.14709</link>
<guid>https://arxiv.org/abs/2511.14709</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, R&amp;D processes, knowledge discovery, innovation ecosystems, hypothesis creation<br /><br />Summary: This study explores the transformative impact of Large Language Models (LLMs) on research and development (R&amp;D) workflows. First, LLMs automate the process of knowledge discovery, enabling faster and more comprehensive gathering of relevant information from vast scientific literature and patent databases. Second, these models enhance hypothesis creation, supporting researchers in generating innovative ideas and research questions. Third, LLMs integrate transdisciplinary insights by synthesizing information across multiple fields, fostering broader and more creative approaches to problem-solving. Fourth, they facilitate cooperation within innovation ecosystems by improving communication and collaboration among diverse stakeholders involved in the R&amp;D process. Lastly, through these capabilities, LLMs significantly increase the efficiency and effectiveness of R&amp;D, accelerating innovation cycles and reducing the time-to-market for novel breakthroughs. By enabling more flexible and informed workflows based on extensive data analysis, LLMs are positioned to revolutionize how research is conducted and how new technologies emerge. <div>
arXiv:2511.14709v1 Announce Type: new 
Abstract: This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&amp;D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&amp;D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments</title>
<link>https://arxiv.org/abs/2511.13788</link>
<guid>https://arxiv.org/abs/2511.13788</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, adversarial interactions, model size ratio, jailbreak, alignment integrity<br /><br />Summary:<br /><br />This study investigates how vulnerabilities in large language models (LLMs) scale when models interact adversarially, particularly focusing on whether larger LLMs can jailbreak smaller ones to induce harmful or restricted behaviors despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, the authors simulate over 6,000 multi-turn attacker-target interactions across major LLM families ranging from 0.6 billion to 120 billion parameters. They measure harm and refusal behaviors as indicators of adversarial potency and alignment integrity, respectively. Each interaction is evaluated by three independent LLM judges to provide consistent, model-based assessments of outcomes. The results show a strong and statistically significant correlation between the mean harm score and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating size asymmetry increases the likelihood and severity of harmful completions. Variance in harm scores is higher among attacker models (0.18) than target models (0.10), suggesting that attacker behavior diversity has a larger effect on outcomes than the inherent susceptibility of targets. Additionally, there is a strong negative correlation (rho = -0.93, p < 0.001) between attacker refusal frequency and harm, implying that alignment on the attacker side can effectively mitigate harmful responses. These findings highlight the importance of relative model size in robustness and encourage further research into inter-model alignment and safety in adversarial settings. <div>
arXiv:2511.13788v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p < 0.001; Spearman rho = 0.52, p < 0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p < 0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rdgai: Classifying transcriptional changes using Large Language Models with a test case from an Arabic Gospel tradition</title>
<link>https://arxiv.org/abs/2511.13801</link>
<guid>https://arxiv.org/abs/2511.13801</guid>
<content:encoded><![CDATA[
<div> Keywords: phylogenetics, textual traditions, Bayesian, classification, LLMs  

<br /><br />Summary: The article discusses the application of phylogenetic methods to textual traditions, emphasizing the need for a nuanced approach to changes in readings, as not all variations carry equal significance. It highlights the challenge of assigning weights to different types of changes within traditional maximum parsimony frameworks. In contrast, Bayesian phylogenetics provides a robust method for categorizing changes, estimating transition rates, and analyzing the probability of these variants across phylogenetic trees. However, the classification process is labor-intensive, necessitating a comparison of each reading against every other at each variation unit. To streamline this task, the authors introduce Rdgai, a software package that automates classification using multi-lingual large language models (LLMs). Rdgai allows users to manually classify some changes, after which it leverages these annotations to classify remaining transitions automatically. The result is a structured output stored in TEI XML, facilitating further phylogenetic analysis. The paper demonstrates this software with an application involving an Arabic translation of the Gospels, illustrating its practical utility in the field. <div>
arXiv:2511.13801v1 Announce Type: cross 
Abstract: Application of phylogenetic methods to textual traditions has traditionally treated all changes as equivalent even though it is widely recognized that certain types of variants were more likely to be introduced than others. While it is possible to give weights to certain changes using a maximum parsimony evaluation criterion, it is difficult to state a priori what these weights should be. Probabilistic methods, such as Bayesian phylogenetics, allow users to create categories of changes, and the transition rates for each category can be estimated as part of the analysis. This classification of types of changes in readings also allows for inspecting the probability of these categories across each branch in the resulting trees. However, classification of readings is time-consuming, as it requires categorizing each reading against every other reading at each variation unit, presenting a significant barrier to entry for this kind of analysis. This paper presents Rdgai, a software package that automates this classification task using multi-lingual large language models (LLMs). The tool allows users to easily manually classify changes in readings and then it uses these annotations in the prompt for an LLM to automatically classify the remaining reading transitions. These classifications are stored in TEI XML and ready for downstream phylogenetic analysis. This paper demonstrates the application with data an Arabic translation of the Gospels.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology</title>
<link>https://arxiv.org/abs/2511.13825</link>
<guid>https://arxiv.org/abs/2511.13825</guid>
<content:encoded><![CDATA[
<div> Agentic AI, KOSMOS, radiation biology, gene expression, hypothesis testing<br /><br />Summary:<br /><br />This study evaluates KOSMOS, an autonomous AI scientist, for its ability to generate and test hypotheses in radiation biology. Three distinct hypotheses were tested: (1) whether baseline DNA damage response (DDR) capacity predicts the p53 transcriptional response after irradiation; (2) whether baseline expression of OGT and CDO1 genes predicts radiation response modules in breast cancer cells; and (3) whether a 12-gene expression signature predicts biochemical recurrence-free survival in prostate cancer patients undergoing radiotherapy and androgen deprivation therapy. The first hypothesis regarding DDR and p53 response was refuted, as the correlation was weakly negative and not statistically significant, comparable to random gene sets. For the second hypothesis, OGT showed a weak and uncertain correlation, but CDO1 demonstrated a strong and significant association with radiation response, marking a well-supported finding. The third hypothesis yielded a moderate predictive power with a concordance index of 0.61 and a statistically significant p-value, though the effect size was not unique. These results show that while KOSMOS can generate valuable hypotheses, its outputs must be rigorously audited against appropriate null models to distinguish meaningful discoveries from false positives or uncertain results. This study highlights the promise and limitations of AI-driven scientific discovery in complex biological problems. <div>
arXiv:2511.13825v1 Announce Type: cross 
Abstract: Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation</title>
<link>https://arxiv.org/abs/2511.13948</link>
<guid>https://arxiv.org/abs/2511.13948</guid>
<content:encoded><![CDATA[
arXiv:2511.13948v1 Announce Type: cross 
Abstract: Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation</title>
<link>https://arxiv.org/abs/2511.13972</link>
<guid>https://arxiv.org/abs/2511.13972</guid>
<content:encoded><![CDATA[
arXiv:2511.13972v1 Announce Type: cross 
Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance</title>
<link>https://arxiv.org/abs/2511.14043</link>
<guid>https://arxiv.org/abs/2511.14043</guid>
<content:encoded><![CDATA[
arXiv:2511.14043v1 Announce Type: cross 
Abstract: AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards</title>
<link>https://arxiv.org/abs/2511.14045</link>
<guid>https://arxiv.org/abs/2511.14045</guid>
<content:encoded><![CDATA[
arXiv:2511.14045v1 Announce Type: cross 
Abstract: Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error-Driven Scene Editing for 3D Grounding in Large Language Models</title>
<link>https://arxiv.org/abs/2511.14086</link>
<guid>https://arxiv.org/abs/2511.14086</guid>
<content:encoded><![CDATA[
arXiv:2511.14086v1 Announce Type: cross 
Abstract: Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured "Decompose, Diagnostic Evaluation, Edit, and Re-train" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval</title>
<link>https://arxiv.org/abs/2511.14130</link>
<guid>https://arxiv.org/abs/2511.14130</guid>
<content:encoded><![CDATA[
arXiv:2511.14130v1 Announce Type: cross 
Abstract: With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning</title>
<link>https://arxiv.org/abs/2511.14299</link>
<guid>https://arxiv.org/abs/2511.14299</guid>
<content:encoded><![CDATA[
arXiv:2511.14299v1 Announce Type: cross 
Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion</title>
<link>https://arxiv.org/abs/2511.14301</link>
<guid>https://arxiv.org/abs/2511.14301</guid>
<content:encoded><![CDATA[
arXiv:2511.14301v1 Announce Type: cross 
Abstract: Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature</title>
<link>https://arxiv.org/abs/2511.14362</link>
<guid>https://arxiv.org/abs/2511.14362</guid>
<content:encoded><![CDATA[
arXiv:2511.14362v1 Announce Type: cross 
Abstract: The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model</title>
<link>https://arxiv.org/abs/2511.14368</link>
<guid>https://arxiv.org/abs/2511.14368</guid>
<content:encoded><![CDATA[
arXiv:2511.14368v1 Announce Type: cross 
Abstract: While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NAIST Academic Travelogue Dataset</title>
<link>https://arxiv.org/abs/2305.11444</link>
<guid>https://arxiv.org/abs/2305.11444</guid>
<content:encoded><![CDATA[
arXiv:2305.11444v2 Announce Type: replace 
Abstract: We have constructed NAIST Academic Travelogue Dataset (ATD) and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Linguistic Structure from a Bottleneck on Sequential Information Processing</title>
<link>https://arxiv.org/abs/2405.12109</link>
<guid>https://arxiv.org/abs/2405.12109</guid>
<content:encoded><![CDATA[
arXiv:2405.12109v3 Announce Type: replace 
Abstract: Human language has a distinct systematic structure, where utterances break into individually meaningful words which are combined to form phrases. We show that natural-language-like systematicity arises in codes that are constrained by a statistical measure of complexity called predictive information, also known as excess entropy. Predictive information is the mutual information between the past and future of a stochastic process. In simulations, we find that such codes break messages into groups of approximately independent features which are expressed systematically and locally, corresponding to words and phrases. Next, drawing on crosslinguistic text corpora, we find that actual human languages are structured in a way that reduces predictive information compared to baselines at the levels of phonology, morphology, syntax, and lexical semantics. Our results establish a link between the statistical and algebraic structure of language and reinforce the idea that these structures are shaped by communication under general cognitive constraints.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native Design Bias: Studying the Impact of English Nativeness on Language Model Performance</title>
<link>https://arxiv.org/abs/2406.17385</link>
<guid>https://arxiv.org/abs/2406.17385</guid>
<content:encoded><![CDATA[
arXiv:2406.17385v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at providing information acquired during pretraining on large-scale corpora and following instructions through user prompts. This study investigates whether the quality of LLM responses varies depending on the demographic profile of users. Considering English as the global lingua franca, along with the diversity of its dialects among speakers of different native languages, we explore whether non-native English speakers receive lower-quality or even factually incorrect responses from LLMs more frequently. Our results show that performance discrepancies occur when LLMs are prompted by native versus non-native English speakers and persist when comparing native speakers from Western countries with others. Additionally, we find a strong anchoring effect when the model recognizes or is made aware of the user's nativeness, which further degrades the response quality when interacting with non-native speakers. Our analysis is based on a newly collected dataset with over 12,000 unique annotations from 124 annotators, including information on their native language and English proficiency.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surprisingly Fragile: Assessing and Addressing Prompt Instability in Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2408.14595</link>
<guid>https://arxiv.org/abs/2408.14595</guid>
<content:encoded><![CDATA[
arXiv:2408.14595v2 Announce Type: replace 
Abstract: Multimodal foundation models (MFMs) such as OFASys show the potential to unlock analysis of complex data such as images, videos, and audio data via text prompts alone. However, their performance may suffer in the face of text input that differs even slightly from their training distribution, which is surprising considering the use of modality-specific data to "ground" the text input. This study demonstrates that prompt instability is a major concern for MFMs, leading to a consistent drop in performance across all modalities, but that instability can be mitigated with additional training with augmented data. We evaluate several methods for grounded prompt perturbation, where we generate perturbations and filter based on similarity to text and/or modality data. After re-training the models on the augmented data, we find improved accuracy and more stable performance on the perturbed test data regardless of perturbation condition, suggesting that the data augmentation strategy helps the models handle domain shifts more effectively. In error analysis, we find consistent patterns of performance improvement across domains, suggesting that retraining on prompt perturbations tends to help general reasoning capabilities in MFMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of OpenAI o1: Opportunities and Challenges of AGI</title>
<link>https://arxiv.org/abs/2409.18486</link>
<guid>https://arxiv.org/abs/2409.18486</guid>
<content:encoded><![CDATA[
arXiv:2409.18486v3 Announce Type: replace 
Abstract: This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:
  -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.
  -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.
  -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.
  -Advanced natural language inference capabilities across general and specialized domains like medicine.
  -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.
  -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.
  -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.
  -Effective performance in social media analysis, including sentiment analysis and emotion recognition.
  The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a Continuum</title>
<link>https://arxiv.org/abs/2410.14589</link>
<guid>https://arxiv.org/abs/2410.14589</guid>
<content:encoded><![CDATA[
arXiv:2410.14589v2 Announce Type: replace 
Abstract: There is increasing interest in looking at dialects in NLP. However, most work to date still treats dialects as discrete categories. For instance, evaluative work in variation-oriented NLP for English often works with Indian English or African-American Venacular English as homogeneous categories (Faisal et al., 2024; Ziems et al., 2023), yet even within one variety there is substantial variation. We examine within-dialect variation and show that performance critically varies within categories. We measure speech-to-text performance on Italian dialects, and empirically observe a geographical performance disparity. This disparity correlates substantially (-0.5) with linguistic similarity to the highest performing dialect variety. We cross-examine our results against dialectometry methods, and interpret the performance disparity to be due to a bias towards dialects that are more similar to the standard variety in the speech-to-text model examined. We additionally leverage geostatistical methods to predict zero-shot performance at unseen sites, and find the incorporation of geographical information to substantially improve prediction performance, indicating there to be geographical structure in the performance distribution.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Machines Think Like Humans? A Behavioral Evaluation of LLM Agents in Dictator Games</title>
<link>https://arxiv.org/abs/2410.21359</link>
<guid>https://arxiv.org/abs/2410.21359</guid>
<content:encoded><![CDATA[
arXiv:2410.21359v3 Announce Type: replace 
Abstract: As Large Language Model (LLM)-based agents increasingly engage with human society, how well do we understand their prosocial behaviors? We (1) investigate how LLM agents' prosocial behaviors can be induced by different personas and benchmarked against human behaviors; and (2) introduce a social science approach to evaluate LLM agents' decision-making. We explored how different personas and experimental framings affect these AI agents' altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal that merely assigning a human-like identity to LLMs does not produce human-like behaviors. These findings suggest that LLM agents' reasoning does not consistently exhibit textual markers of human decision-making in dictator games and that their alignment with human behavior varies substantially across model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. As society increasingly integrates machine intelligence, "Prosocial AI" emerges as a promising and urgent research direction in philanthropic studies.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning -- Natural Language Processing: From Theory to Application</title>
<link>https://arxiv.org/abs/2411.05026</link>
<guid>https://arxiv.org/abs/2411.05026</guid>
<content:encoded><![CDATA[
arXiv:2411.05026v3 Announce Type: replace 
Abstract: With a focus on natural language processing (NLP) and the role of large language models (LLMs), we explore the intersection of machine learning, deep learning, and artificial intelligence. As artificial intelligence continues to revolutionize fields from healthcare to finance, NLP techniques such as tokenization, text classification, and entity recognition are essential for processing and understanding human language. This paper discusses advanced data preprocessing techniques and the use of frameworks like Hugging Face for implementing transformer-based models. Additionally, it highlights challenges such as handling multilingual data, reducing bias, and ensuring model robustness. By addressing key aspects of data processing and model fine-tuning, this work aims to provide insights into deploying effective and ethically sound AI solutions.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial intelligence contribution to translation industry: looking back and forward</title>
<link>https://arxiv.org/abs/2411.19855</link>
<guid>https://arxiv.org/abs/2411.19855</guid>
<content:encoded><![CDATA[
arXiv:2411.19855v4 Announce Type: replace 
Abstract: This study provides a comprehensive analysis of artificial intelligence (AI) contribution to research in the translation industry (ACTI), synthesizing it over forty-five years from 1980-2024. 13220 articles were retrieved from three sources, namely WoS, Scopus, and Lens; 9836 were unique records, which were used for the analysis. We provided two types of analysis, viz., scientometric and thematic, focusing on Cluster, Subject categories, Keywords, Bursts, Centrality and Research Centers as for the former. For the latter, we provided a thematic review for 18 articles, selected purposefully from the articles involved, centering on purpose, approach, findings, and contribution to ACTI future directions. This study is significant for its valuable contribution to ACTI knowledge production over 45 years, emphasizing several trending issues and hotspots including Machine translation, Statistical machine translation, Low-resource language, Large language model, Arabic dialects, Translation quality, and Neural machine translation. The findings reveal that the more AI develops, the more it contributes to translation industry, as Neural Networking Algorithms have been incorporated and Deep Language Learning Models like ChatGPT have been launched. However, much rigorous research is still needed to overcome several problems encountering translation industry, specifically concerning low-resource, multi-dialectical and free word order languages, and cultural and religious registers.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion</title>
<link>https://arxiv.org/abs/2501.15089</link>
<guid>https://arxiv.org/abs/2501.15089</guid>
<content:encoded><![CDATA[
arXiv:2501.15089v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable progress in understanding long-context inputs. However, benchmarks for evaluating the long-context reasoning abilities of LLMs fall behind the pace. Existing benchmarks often focus on a narrow range of tasks or those that do not demand complex reasoning. To address this gap and enable a more comprehensive evaluation of the long-context reasoning capabilities of current LLMs, we propose a new synthetic benchmark, LongReason, which is constructed by synthesizing long-context reasoning questions from a varied set of short-context reasoning questions through context expansion. LongReason consists of 794 multiple-choice reasoning questions with diverse reasoning patterns across three task categories: reading comprehension, logical inference, and mathematical word problems. We evaluate 21 LLMs on LongReason, revealing that most models experience significant performance drops as context length increases. Our further analysis shows that even state-of-the-art LLMs still have significant room for improvement in providing robust reasoning across different tasks. We have open-sourced LongReason under https://huggingface.co/datasets/lz1bytedance/LongReason to support the comprehensive evaluation of LLMs' long-context reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title>
<link>https://arxiv.org/abs/2502.13685</link>
<guid>https://arxiv.org/abs/2502.13685</guid>
<content:encoded><![CDATA[
arXiv:2502.13685v4 Announce Type: replace 
Abstract: Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive tasks. To address this limitation, we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. MoM serves as a general framework that can be seamlessly combined with diverse memory update mechanisms across linear models. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpeNLGauge: An Explainable Metric for NLG Evaluation with Open-Weights LLMs</title>
<link>https://arxiv.org/abs/2503.11858</link>
<guid>https://arxiv.org/abs/2503.11858</guid>
<content:encoded><![CDATA[
arXiv:2503.11858v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated great potential as evaluators of NLG systems, allowing for high-quality, reference-free, and multi-aspect assessments. However, existing LLM-based metrics suffer from two major drawbacks: reliance on proprietary models to generate training data or perform evaluations, and a lack of fine-grained, explanatory feedback. In this paper, we introduce OpeNLGauge, a fully open-source, reference-free NLG evaluation metric that provides accurate explanations based on error spans. OpeNLGauge is available as a two-stage ensemble of larger open-weight LLMs, or as a small fine-tuned evaluation model, with confirmed generalizability to unseen tasks, domains and aspects. Our extensive meta-evaluation shows that OpeNLGauge achieves competitive correlation with human judgments, outperforming state-of-the-art models on certain tasks while maintaining full reproducibility and providing explanations more than twice as accurate.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2504.12673</link>
<guid>https://arxiv.org/abs/2504.12673</guid>
<content:encoded><![CDATA[
arXiv:2504.12673v2 Announce Type: replace 
Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However,retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language modelbased compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy-reducing documents, making it highly useful in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anti-adversarial Learning: Desensitizing Prompts for Large Language Models</title>
<link>https://arxiv.org/abs/2505.01273</link>
<guid>https://arxiv.org/abs/2505.01273</guid>
<content:encoded><![CDATA[
arXiv:2505.01273v2 Announce Type: replace 
Abstract: With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is "anti-adversarial" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs</title>
<link>https://arxiv.org/abs/2505.17052</link>
<guid>https://arxiv.org/abs/2505.17052</guid>
<content:encoded><![CDATA[
arXiv:2505.17052v2 Announce Type: replace 
Abstract: Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-context Language Learning for Endangered Languages in Speech Recognition</title>
<link>https://arxiv.org/abs/2505.20445</link>
<guid>https://arxiv.org/abs/2505.20445</guid>
<content:encoded><![CDATA[
arXiv:2505.20445v4 Announce Type: replace 
Abstract: With approximately 7,000 languages spoken worldwide, current large language models (LLMs) support only a small subset. Prior research indicates LLMs can learn new languages for certain tasks without supervised data. We extend this investigation to speech recognition, investigating whether LLMs can learn unseen, low-resource languages through in-context learning (ICL). With experiments on four diverse endangered languages that LLMs have not been trained on, we find that providing more relevant text samples enhances performance in both language modelling and Automatic Speech Recognition (ASR) tasks. Furthermore, we show that the probability-based approach outperforms the traditional instruction-based approach in language learning. Lastly, we show ICL enables LLMs to achieve ASR performance that is comparable to or even surpasses dedicated language models trained specifically for these languages, while preserving the original capabilities of the LLMs. Our code is publicly available.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration</title>
<link>https://arxiv.org/abs/2505.23229</link>
<guid>https://arxiv.org/abs/2505.23229</guid>
<content:encoded><![CDATA[
arXiv:2505.23229v2 Announce Type: replace 
Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) has demonstrated significant success in structured, problem-oriented tasks. However, applying these methods to open-ended dialogues, such as those in psychological counseling, presents unique challenges. Unlike tasks with objective correctness, success in therapeutic conversations depends on subjective factors like empathetic engagement, ethical adherence, and alignment with human preferences, for which strict "correctness" criteria are ill-defined. Existing result-oriented MCTS approaches can therefore produce misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS framework designed for open-ended, human-centric dialogues. Its core innovation is "domain alignment", which shifts the MCTS search objective from predefined end-states towards conversational trajectories that conform to target domain principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates "Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden exploration by allowing the MCTS to consider fundamentally different initial dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM. We also introduce PsyEval, a benchmark for assessing multi-turn psychological counseling dialogues. Experiments demonstrate that PsyLLM achieves state-of-the-art performance on PsyEval and other relevant metrics, validating MCTSr-Zero's effectiveness in generating high-quality, principle-aligned conversational data for human-centric domains and addressing the LLM challenge of consistently adhering to complex psychological standards.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenRecal: Generation after Recalibration from Large to Small Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.15681</link>
<guid>https://arxiv.org/abs/2506.15681</guid>
<content:encoded><![CDATA[
arXiv:2506.15681v2 Announce Type: replace 
Abstract: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoLM: In Search of Lost Language Model Training Dynamics</title>
<link>https://arxiv.org/abs/2506.16029</link>
<guid>https://arxiv.org/abs/2506.16029</guid>
<content:encoded><![CDATA[
arXiv:2506.16029v2 Announce Type: replace 
Abstract: Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. We train over 100 LMs with 1B and 4B parameters from scratch, and evaluate both upstream (language modeling) and downstream (problem-solving) capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm</title>
<link>https://arxiv.org/abs/2506.20606</link>
<guid>https://arxiv.org/abs/2506.20606</guid>
<content:encoded><![CDATA[
arXiv:2506.20606v2 Announce Type: replace 
Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent's global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through extensive evaluations of agents built on frontier LLMs, BehaviorBench validates the effectiveness of behavior editing across a wide range of models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2508.01450</link>
<guid>https://arxiv.org/abs/2508.01450</guid>
<content:encoded><![CDATA[
arXiv:2508.01450v2 Announce Type: replace 
Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms baseline methods, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous sentiment scores for literary and multilingual contexts</title>
<link>https://arxiv.org/abs/2508.14620</link>
<guid>https://arxiv.org/abs/2508.14620</guid>
<content:encoded><![CDATA[
arXiv:2508.14620v2 Announce Type: replace 
Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Retrieval Augmented Language Models Know When They Don't Know?</title>
<link>https://arxiv.org/abs/2509.01476</link>
<guid>https://arxiv.org/abs/2509.01476</guid>
<content:encoded><![CDATA[
arXiv:2509.01476v3 Announce Type: replace 
Abstract: Existing large language models (LLMs) occasionally generate plausible yet factually incorrect responses, known as hallucinations. Two main approaches have been proposed to mitigate hallucinations: retrieval-augmented language models (RALMs) and refusal post-training. However, current research predominantly focuses on their individual effectiveness while overlooking the evaluation of the refusal capability of RALMs. Ideally, if RALMs know when they do not know, they should refuse to answer.In this study, we ask the fundamental question: Do RALMs know when they don't know? Specifically, we investigate three questions. First, are RALMs well calibrated with respect to different internal and external knowledge states? We examine the influence of various factors. Contrary to expectations, when all retrieved documents are irrelevant, RALMs still tend to refuse questions they could have answered correctly. Next, given the model's pronounced \textbf{over-refusal} behavior, we raise a second question: How does a RALM's refusal ability align with its calibration quality? Our results show that the over-refusal problem can be mitigated through in-context fine-tuning. However, we observe that improved refusal behavior does not necessarily imply better calibration or higher overall accuracy. Finally, we ask: Can we combine refusal-aware RALMs with uncertainty-based answer abstention to mitigate over-refusal? We develop a simple yet effective refusal mechanism for refusal-post-trained RALMs that improves their overall answer quality by balancing refusal and correct answers. Our study provides a more comprehensive understanding of the factors influencing RALM behavior. Meanwhile, we emphasize that uncertainty estimation for RALMs remains an open problem deserving deeper investigation.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Language Model Pretraining with ModernBERT</title>
<link>https://arxiv.org/abs/2509.14926</link>
<guid>https://arxiv.org/abs/2509.14926</guid>
<content:encoded><![CDATA[
arXiv:2509.14926v3 Announce Type: replace 
Abstract: Transformer-based language models such as BERT have become foundational in NLP, yet their performance degrades in specialized domains like patents, which contain long, technical, and legally structured text. Prior approaches to patent NLP have primarily relied on fine-tuning general-purpose models or domain-adapted variants pretrained with limited data. In this work, we pretrain 3 domain-specific masked language models for patents, using the ModernBERT architecture and a curated corpus of over 60 million patent records. Our approach incorporates architectural optimizations, including FlashAttention, rotary embeddings, and GLU feed-forward layers. We evaluate our models on four downstream patent classification tasks. Our model, ModernBERT-base-PT, consistently outperforms the general-purpose ModernBERT baseline on three out of four datasets and achieves competitive performance with a baseline PatentBERT. Additional experiments with ModernBERT-base-VX and Mosaic-BERT-large demonstrate that scaling the model size and customizing the tokenizer further enhance performance on selected tasks. Notably, all ModernBERT variants retain substantially faster inference over - 3x that of PatentBERT - underscoring their suitability for time-sensitive applications. These results underscore the benefits of domain-specific pretraining and architectural improvements for patent-focused NLP tasks.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Fact-checking in English and Telugu</title>
<link>https://arxiv.org/abs/2509.26415</link>
<guid>https://arxiv.org/abs/2509.26415</guid>
<content:encoded><![CDATA[
arXiv:2509.26415v2 Announce Type: replace 
Abstract: False information poses a significant global challenge, and manually verifying claims is a time-consuming and resource-intensive process. In this research paper, we experiment with different approaches to investigate the effectiveness of large language models (LLMs) in classifying factual claims by their veracity and generating justifications in English and Telugu. The key contributions of this work include the creation of a bilingual English-Telugu dataset and the benchmarking of different veracity classification approaches based on LLMs.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection</title>
<link>https://arxiv.org/abs/2510.09771</link>
<guid>https://arxiv.org/abs/2510.09771</guid>
<content:encoded><![CDATA[
arXiv:2510.09771v2 Announce Type: replace 
Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six categories. Traditional supervised approaches need extensive labeled datasets that are expensive for low-resource languages. We developed PromptGuard, a few-shot framework combining chi-square statistical analysis for keyword extraction with adaptive majority voting for decision-making. We explore statistical keyword selection versus random approaches and adaptive voting mechanisms that extend classification based on consensus quality. Chi-square keywords provide consistent improvements across categories, while adaptive voting benefits ambiguous cases requiring extended classification rounds. PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines (60.75) and random approaches (14.65). Ablation studies confirm chi-square-based keywords show the most consistent impact across all categories.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI use in American newspapers is widespread, uneven, and rarely disclosed</title>
<link>https://arxiv.org/abs/2510.18774</link>
<guid>https://arxiv.org/abs/2510.18774</guid>
<content:encoded><![CDATA[
arXiv:2510.18774v2 Announce Type: replace 
Abstract: AI is rapidly transforming journalism, but the extent of its use in published newspaper articles remains unclear. We address this gap by auditing a large-scale dataset of 186K articles from online editions of 1.5K American newspapers published in the summer of 2025. Using Pangram, a state-of-the-art AI detector, we discover that approximately 9% of newly-published articles are either partially or fully AI-generated. This AI use is unevenly distributed, appearing more frequently in smaller, local outlets, in specific topics such as weather and technology, and within certain ownership groups. We also analyze 45K opinion pieces from Washington Post, New York Times, and Wall Street Journal, finding that they are 6.4 times more likely to contain AI-generated content than news articles from the same publications, with many AI-flagged op-eds authored by prominent public figures. Despite this prevalence, we find that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles found only five disclosures of AI use. Overall, our audit highlights the immediate need for greater transparency and updated editorial standards regarding the use of AI in journalism to maintain public trust.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance</title>
<link>https://arxiv.org/abs/2511.03383</link>
<guid>https://arxiv.org/abs/2511.03383</guid>
<content:encoded><![CDATA[
arXiv:2511.03383v2 Announce Type: replace 
Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set of hyperparameters for word segmentation models, symmetric Byte Pair Encoding (BPE), which applies the same number of merge operations (NMO) to train tokenizers for both source and target languages. However, we demonstrate that this uniform approach doesn't guarantee optimal MT performance across different language pairs and data sizes. This work investigates BPE segmentation recipes across various data volumes and language pairs to evaluate MT system performance. We find that utilizing asymmetric BPE, where the source and target languages have different NMOs, significantly improves results over the symmetric approach, especially in low-resource settings (50K, 100K, and 500K sentence pairs). Specifically, asymmetric BPE yield statistically significant ($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in low-resource setups (50K, 100K, and 500K sentence pairs, respectively). We validated this trend across six additional language pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut), observing statistically significant improvement in 10 out of 12 systems compared to symmetric BPE. Our findings indicate a high NMO for the source (4K to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results, particularly benefiting low-resource MT.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection</title>
<link>https://arxiv.org/abs/2511.04528</link>
<guid>https://arxiv.org/abs/2511.04528</guid>
<content:encoded><![CDATA[
arXiv:2511.04528v2 Announce Type: replace 
Abstract: We present IntelliProof, an interactive system for analyzing argumentative essays through LLMs. IntelliProof structures an essay as an argumentation graph, where claims are represented as nodes, supporting evidence is attached as node properties, and edges encode supporting or attacking relations. Unlike existing automated essay scoring systems, IntelliProof emphasizes the user experience: each relation is initially classified and scored by an LLM, then visualized for enhanced understanding. The system provides justifications for classifications and produces quantitative measures for essay coherence. It enables rapid exploration of argumentative quality while retaining human oversight. In addition, IntelliProof provides a set of tools for a better understanding of an argumentative essay and its corresponding graph in natural language, bridging the gap between the structural semantics of argumentative essays and the user's understanding of a given text.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability</title>
<link>https://arxiv.org/abs/2403.04483</link>
<guid>https://arxiv.org/abs/2403.04483</guid>
<content:encoded><![CDATA[
arXiv:2403.04483v4 Announce Type: replace-cross 
Abstract: Improving the general capabilities of large language models (LLMs) is an active research topic. As a common data structure in many real-world domains, understanding graph data is a crucial part of advancing general intelligence. To this end, we propose a dynamic benchmark named GraphInstruct in this paper, which comprehensively includes 21 classical graph reasoning tasks, providing diverse graph generation pipelines and detailed intermediate reasoning steps for each sample. Based on GraphInstruct, we develop GraphSolver via efficient instruction-tuning, which demonstrates prominent graph understanding capability compared to other open-sourced LLMs. To further endow LLMs with multi-step graph reasoning capability, we propose a label-mask training strategy and build GraphSolver+, which leverages masked supervision on intermediate reasoning tokens to emphasize crucial node-identification signals. As one of the pioneering efforts to enhance the graph understanding and reasoning abilities of LLMs, extensive experiments have demonstrated the superiority of GraphSolver and GraphSolver+ over other LLMs. We sincerely hope GraphInstruct will facilitate further research on applying LLMs to graph-structured data. Our code and data are released publicly at: https://github.com/CGCL-codes/GraphInstruct.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisAidMath: Benchmarking Visual-Aided Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2410.22995</link>
<guid>https://arxiv.org/abs/2410.22995</guid>
<content:encoded><![CDATA[
arXiv:2410.22995v2 Announce Type: replace-cross 
Abstract: A hallmark of advanced artificial intelligence is the capacity to progress from passive visual perception to the strategic modification of visual information to facilitate complex reasoning. This advanced capability, however, remains critically underdeveloped in current Large Multi-modal Models (LMMs). The deficiency is often masked by evaluation metrics that prioritize final-answer accuracy, creating an illusion of competence where genuine reasoning is absent. Using the domain of geometric problem-solving as a precise instrument, we probe this issue through tasks that require constructing visual aids. To this end, we introduce \textbf{VisAidMath}, a challenging benchmark, and our novel Three-Layered Funnel Evaluation Framework. This framework moves beyond simple accuracy (ACCU) to scrutinize the generation of valid visual aids (PVA) and the soundness of subsequent reasoning steps (SPRS). Our extensive experiments on state-of-the-art models, including Doubao-Seed-1.6 and o4, reveal a profound ``Reasoning Illusion''. We observe that high surface-level accuracy conceals a catastrophic failure in the models' ability to produce valid visual aids or to reason from them. Our findings expose a fundamental schism between visual perception and logical deduction in modern LMMs. We host an evaluation platform at CodaBench for testing publicly. Homepage: https://nlp2ct.github.io/VisAidMathHomepage/ Evaluation: https://www.codabench.org/competitions/7634/
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iris: Integrating Language into Diffusion-based Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v4 Announce Type: replace-cross 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisances. We demonstrate that language can enhance monocular depth estimation by providing an additional condition (rather than images alone) aligned with plausible 3D scenes, thereby reducing the solution space for depth estimation. This conditional distribution is learned during the text-to-image pre-training of diffusion models. To generate images under various viewpoints and layouts that precisely reflect textual descriptions, the model implicitly models object sizes, shapes, and scales, their spatial relationships, and the overall scene structure. In this paper, Iris, we investigate the benefits of our strategy to integrate text descriptions into training and inference of diffusion-based depth estimation models. We experiment with three different diffusion-based monocular depth estimators (Marigold, Lotus, and E2E-FT) and their variants. By training on HyperSim and Virtual KITTI, and evaluating on NYUv2, KITTI, ETH3D, ScanNet, and DIODE, we find that our strategy improves the overall monocular depth estimation accuracy, especially in small areas. It also improves the model's depth perception of specific regions described in the text. We find that by providing more details in the text, the depth prediction can be iteratively refined. Simultaneously, we find that language can act as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. Code and generated text data will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Performance of Black-box LLMs through Self-Queries</title>
<link>https://arxiv.org/abs/2501.01558</link>
<guid>https://arxiv.org/abs/2501.01558</guid>
<content:encoded><![CDATA[
arXiv:2501.01558v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly relied on in AI systems, predicting when they make mistakes is crucial. While a great deal of work in the field uses internal representations to interpret model behavior, these representations are inaccessible when given solely black-box access through an API. In this paper, we extract features of LLMs in a black-box manner by using follow-up prompts and taking the probabilities of different responses as representations to train reliable predictors of model behavior. We demonstrate that training a linear model on these low-dimensional representations produces reliable and generalizable predictors of model performance at the instance level (e.g., if a particular generation correctly answers a question). Remarkably, these can often outperform white-box linear predictors that operate over a model's hidden state or the full distribution over its vocabulary. In addition, we demonstrate that these extracted features can be used to evaluate more nuanced aspects of a language model's state. For instance, they can be used to distinguish between a clean version of GPT-4o-mini and a version that has been influenced via an adversarial system prompt that answers question-answering tasks incorrectly or introduces bugs into generated code. Furthermore, they can reliably distinguish between different model architectures and sizes, enabling the detection of misrepresented models provided through an API (e.g., identifying if GPT-3.5 is supplied instead of GPT-4o-mini).
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector</title>
<link>https://arxiv.org/abs/2502.15902</link>
<guid>https://arxiv.org/abs/2502.15902</guid>
<content:encoded><![CDATA[
arXiv:2502.15902v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM-generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining the reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution data, and 5.48% (AUROC) on attacked data. IPAD also performs robustly on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptScale: Probabilistic Optimality for Inference-time Scaling</title>
<link>https://arxiv.org/abs/2506.22376</link>
<guid>https://arxiv.org/abs/2506.22376</guid>
<content:encoded><![CDATA[
arXiv:2506.22376v3 Announce Type: replace-cross 
Abstract: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning. The source code is publicly available at https://github.com/Albertwyk/OptScale.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theories of "Sexuality" in Natural Language Processing Bias Research</title>
<link>https://arxiv.org/abs/2506.22481</link>
<guid>https://arxiv.org/abs/2506.22481</guid>
<content:encoded><![CDATA[
arXiv:2506.22481v2 Announce Type: replace-cross 
Abstract: In recent years, significant advancements in the field of Natural Language Processing (NLP) have positioned commercialized language models as wide-reaching, highly useful tools. In tandem, there has been an explosion of multidisciplinary research examining how NLP tasks reflect, perpetuate, and amplify social biases such as gender and racial bias. A significant gap in this scholarship is a detailed analysis of how queer sexualities are encoded and (mis)represented by both NLP systems and practitioners. Following previous work in the field of AI fairness, we document how sexuality is defined and operationalized via a survey and analysis of 55 articles that quantify sexuality-based NLP bias. We find that sexuality is not clearly defined in a majority of the literature surveyed, indicating a reliance on assumed or normative conceptions of sexual/romantic practices and identities. Further, we find that methods for extracting biased outputs from NLP technologies often conflate gender and sexual identities, leading to monolithic conceptions of queerness and thus improper quantifications of bias. With the goal of improving sexuality-based NLP bias analyses, we conclude with recommendations that encourage more thorough engagement with both queer communities and interdisciplinary literature.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's runtime traces as graph-based intermediate representations with control and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools \& data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis for sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can reduce the ASR to 3\%, with the utility drop only 1\%.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title>
<link>https://arxiv.org/abs/2508.02175</link>
<guid>https://arxiv.org/abs/2508.02175</guid>
<content:encoded><![CDATA[
arXiv:2508.02175v3 Announce Type: replace-cross 
Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</title>
<link>https://arxiv.org/abs/2508.18646</link>
<guid>https://arxiv.org/abs/2508.18646</guid>
<content:encoded><![CDATA[
arXiv:2508.18646v2 Announce Type: replace-cross 
Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinVet: A Collaborative Framework of RAG and External Fact-Checking Agents for Financial Misinformation Detection</title>
<link>https://arxiv.org/abs/2510.11654</link>
<guid>https://arxiv.org/abs/2510.11654</guid>
<content:encoded><![CDATA[
arXiv:2510.11654v2 Announce Type: replace-cross 
Abstract: Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA</title>
<link>https://arxiv.org/abs/2510.25101</link>
<guid>https://arxiv.org/abs/2510.25101</guid>
<content:encoded><![CDATA[
arXiv:2510.25101v2 Announce Type: replace-cross 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title>
<link>https://arxiv.org/abs/2511.04584</link>
<guid>https://arxiv.org/abs/2511.04584</guid>
<content:encoded><![CDATA[
arXiv:2511.04584v3 Announce Type: replace-cross 
Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction where users are intentional about the degree to which they specify queries. We develop a principled framework based on a shared responsibility of query specification between user and system, distinguishing unambiguous and ambiguous cooperative queries, which systems can resolve through reasonable inference, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. This conceptualization around cooperation in resolving queries informs how to design and evaluate natural language interfaces for tabular data analysis, for which we distill concrete directions for future research and broader implications.
]]></content:encoded>
<pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context</title>
<link>https://arxiv.org/abs/2504.04737</link>
<guid>https://arxiv.org/abs/2504.04737</guid>
<content:encoded><![CDATA[
<div> Keywords: Fact-based Judgment Prediction, TathyaNyaya, legal context, FactLegalLlama, AI-assisted decision-making  

<br /><br />Summary:  
This paper presents TathyaNyaya, the largest annotated dataset for Fact-based Judgment Prediction and Explanation (FJPE) tailored to the Indian legal system, featuring judgments from the Supreme Court and various High Courts. The dataset's name is derived from the Hindi words "Tathya" (fact) and "Nyaya" (justice), emphasizing its focus on factual statements essential for real-world judicial outcomes. Alongside TathyaNyaya, the authors introduce FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model optimized for producing high-quality explanations in FJPE tasks. Fine-tuned on the TathyaNyaya dataset, FactLegalLlama combines predictive accuracy with clear and relevant explanations, addressing crucial transparency and interpretability needs in AI-driven legal systems. The proposed methodology integrates transformers for binary judgment prediction and the FactLegalLlama for explanation generation, creating a robust framework focused on advancing FJPE within the Indian legal domain. TathyaNyaya not only exceeds existing datasets in both scale and diversity but also sets a new benchmark for the development of explainable AI in legal analysis, highlighting the importance of factual accuracy and domain-specific tuning for improved predictive performance and interpretability. <div>
arXiv:2504.04737v3 Announce Type: replace 
Abstract: In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms "Tathya" (fact) and "Nyaya" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Judgment Prediction, NyayaRAG, Indian legal system, retrieval-augmented generation, predictive accuracy

<br /><br />Summary: Legal Judgment Prediction (LJP) is a crucial area of AI in the legal field that focuses on forecasting judicial outcomes and enhancing the interpretability of legal reasoning. Existing methods in India often concentrate on internal case elements, neglecting essential components of common law, such as statutory provisions and judicial precedents. This paper introduces NyayaRAG, a Retrieval-Augmented Generation (RAG) framework designed to emulate realistic courtroom scenarios. NyayaRAG integrates factual case descriptions, pertinent legal statutes, and semantically retrieved previous cases to bolster the predictive process. The framework's effectiveness is evaluated based on the impact of these combined inputs on court decision predictions and the quality of generated legal explanations, specifically adapted for the Indian legal environment. Comprehensive performance assessments utilize various configurations and include both standard lexical and semantic metrics alongside Large Language Model (LLM)-based evaluators like G-Eval. The findings indicate that enhancing factual inputs with structured legal knowledge notably boosts both the accuracy of predictions and the quality of explanations provided, indicating the framework's potential for advancing legal judgment prediction in India. <div>
arXiv:2508.00709v3 Announce Type: replace 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPRO: Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2509.00974</link>
<guid>https://arxiv.org/abs/2509.00974</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Question Answering, Ranked Preference Reinforcement Optimization, Clinical Chain-of-Thought, Reinforcement Learning, Preference-Driven Reasoning

<br /><br />Summary: The article addresses the challenges in medical question answering, particularly the inaccuracies in reasoning chains produced by existing large language models (LLMs). To overcome this, the authors introduce a novel framework called Ranked Preference Reinforcement Optimization (RPRO), which merges reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO is distinctive in its use of task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns outputs with established clinical workflows while detecting and correcting low-quality reasoning. Unlike traditional methods that rely on pairwise preferences, RPRO employs groupwise ranking optimization based on the Bradley--Terry model and utilizes KL-divergence regularization to ensure stable training. The framework's effectiveness is demonstrated through experiments on multiple datasets, including PubMedQA and MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital, showing notable improvements over strong baseline models. Impressively, their 2B-parameter model surpasses larger models ranging from 7B to 20B parameters, showcasing the potential of preference optimization combined with quality-driven refinement to create more reliable and clinically relevant medical LLMs. <div>
arXiv:2509.00974v3 Announce Type: replace 
Abstract: Medical question answering requires advanced reasoning that integrates domain knowledge with logical inference. However, existing large language models (LLMs) often generate reasoning chains that lack factual accuracy and clinical reliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a novel framework that combines reinforcement learning with preference-driven reasoning refinement to enhance clinical chain-of-thought (CoT) performance. RPRO distinguishes itself from prior approaches by employing task-adaptive reasoning templates and a probabilistic evaluation mechanism that aligns model outputs with established clinical workflows, while automatically identifying and correcting low-quality reasoning chains. Unlike traditional pairwise preference methods, RPRO introduces a groupwise ranking optimization based on the Bradley--Terry model and incorporates KL-divergence regularization for stable training. Experiments on PubMedQA, MedQA-USMLE, and a real-world clinical dataset from Far Eastern Memorial Hospital (FEMH) demonstrate consistent improvements over strong baselines. Remarkably, our 2B-parameter model outperforms much larger 7B--20B models, including medical-specialized variants. These findings demonstrate that combining preference optimization with quality-driven refinement provides a scalable and clinically grounded approach to building more reliable medical LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Silenced Biases: The Dark Side LLMs Learned to Refuse</title>
<link>https://arxiv.org/abs/2511.03369</link>
<guid>https://arxiv.org/abs/2511.03369</guid>
<content:encoded><![CDATA[
<div> Keywords: safety-aligned models, fairness evaluation, silenced biases, Silenced Bias Benchmark, activation steering

<br /><br />Summary: The prevalence of safety-aligned large language models (LLMs) in sensitive applications necessitates effective evaluation of their fairness, particularly in contexts where biased outputs can cause harm. Traditional evaluation methods often rely on question-answer schemes, mistakenly interpreting model refusals as indicators of fairness, resulting in a misleading assessment. This work introduces the notion of silenced biases, which refers to unfair preferences embedded in the models' latent space and masked by safety-alignment. Existing methods addressing similar biases have limitations, including dependence on prompt manipulation and handcrafted queries that can introduce new biases. To address these challenges, the authors propose the Silenced Bias Benchmark (SBB), designed to reveal these concealed biases through activation steering that minimizes model refusals during QA evaluations. SBB is structured for easy adaptation to various demographic groups and subjects, aiming to enhance fairness evaluation frameworks and promote the development of fairer models beyond the constraints of alignment training. The authors demonstrate their methodology across multiple LLMs, revealing significant discrepancies between the models' overt responses and their underlying fairness issues. <div>
arXiv:2511.03369v2 Announce Type: replace 
Abstract: Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy</title>
<link>https://arxiv.org/abs/2511.11594</link>
<guid>https://arxiv.org/abs/2511.11594</guid>
<content:encoded><![CDATA[
<div> Keywords: fuzzy matching, TimeStampEval, transcripts, retrieval accuracy, Assisted Fuzzy  

<br /><br />Summary: Traditional fuzzy matching struggles with semantically identical but syntactically different quotes, particularly when aligning official records with speech-to-text transcripts. TimeStampEval is introduced as a benchmark for retrieving millisecond timestamps from lengthy transcripts based on non-verbatim quotes. The simple two-stage method significantly enhances retrieval accuracy while reducing inference costs by over 90%. The primary application is automating long-form podcasts using Congressional Record clips. Key findings include: (1) Prompt design is more crucial than model selection; positioning the query before the transcript with compact formatting can boost accuracy by 3-20 points and reduce token count by 30-40%. (2) Distinct "off-by-one" errors indicate that models are capable of understanding the task but may misplace boundaries. (3) A modest reasoning budget (600-850 tokens) can elevate accuracy from 37% to 77% for weaker models and above 90% for stronger ones. (4) The "Assisted Fuzzy" method, which employs RapidFuzz pre-filtering followed by LLM verification, enhances fuzzy match accuracy by up to 50 points, halves latency, and cuts the cost per correct result by as much as 96%. Tests across ten varied transcripts confirm the approach's robustness. <div>
arXiv:2511.11594v1 Announce Type: new 
Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</title>
<link>https://arxiv.org/abs/2511.11793</link>
<guid>https://arxiv.org/abs/2511.11793</guid>
<content:encoded><![CDATA[
<div> Keywords: MiroThinker, open-source, interaction scaling, reasoning, reinforcement learning  

<br /><br />Summary: MiroThinker v1.0 is an open-source research agent aimed at enhancing tool-augmented reasoning and information-seeking capabilities. Unlike traditional models that primarily focus on scaling size or context length, MiroThinker innovates by exploring interaction scaling at the model level, facilitating deeper and more frequent interactions with the environment. This method allows the model to leverage feedback and external information, correcting errors and refining reasoning trajectories. Through reinforcement learning, MiroThinker accomplishes efficient interaction scaling, operating with a 256K context window and capable of executing up to 600 tool calls per task, which supports complex multi-turn reasoning and real-world research workflows. The 72B variant of MiroThinker excels across four benchmarks—GAIA, HLE, BrowseComp, and BrowseComp-ZH—achieving accuracy scores of 81.9%, 37.7%, 47.1%, and 55.6%, respectively, outperforming existing open-source models and nearing the performance of commercial models like GPT-5-high. The study shows that research performance improves consistently with increased interaction depth, establishing it as a crucial dimension for developing advanced research agents alongside model capacity and context windows. <div>
arXiv:2511.11793v1 Announce Type: new 
Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Notion that Language Models Reason</title>
<link>https://arxiv.org/abs/2511.11810</link>
<guid>https://arxiv.org/abs/2511.11810</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, reasoning, Markov kernel, statistical pattern matchers, epistemic uncertainty

<br /><br />Summary:  
1. The paper critically examines the meaning of "reasoning" as applied to language models (LMs) in natural language processing (NLP), highlighting inconsistencies in common definitions relative to LM training and operation.  
2. It adopts the perspective that transformer-based LMs function as implicit finite-order Markov kernels, mapping contexts to conditional token probabilities rather than executing explicit logical or reasoning algorithms.  
3. Reasoning-like outputs from LMs emerge from learned statistical regularities and approximate invariances in these probabilistic kernels, rather than from genuine logical inference or explicit mechanisms.  
4. This framework supports the characterization of LMs as "statistical pattern matchers" rather than authentic reasoners, explaining why LMs can produce outputs that resemble reasoning without guarantees of logical consistency.  
5. The distinction is crucial for properly assessing epistemic uncertainty in LMs and calls for careful attention to how computational processes underlying LMs are described and understood in NLP research, fostering clearer conceptual foundations and dialogue. <div>
arXiv:2511.11810v1 Announce Type: new 
Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis</title>
<link>https://arxiv.org/abs/2511.11821</link>
<guid>https://arxiv.org/abs/2511.11821</guid>
<content:encoded><![CDATA[
<div> Keywords: information extraction, large language models, hydropower licensing, model scaling, validation methods  

<br /><br />Summary:  
This study investigates the use of large language models (LLMs) for extracting information from regulatory documents specifically related to hydropower licensing. Seven open-weight LLMs ranging from 0.6 billion to 70 billion parameters were evaluated to understand performance relative to computational requirements. A key finding is a critical performance threshold at 14 billion parameters, where validation methods become significantly more effective—models below this size achieve poor F1 scores (<0.15), while those at or above this level reach viable F1 scores around 0.64. Models designed for consumer deployment max out at about 51% F1 score due to limitations in validation, whereas the largest models approach 77% F1 but demand enterprise-grade infrastructure for practical use. The research also exposes systematic hallucination patterns in smaller models, where perfect recall paradoxically signals extraction failure rather than accuracy. This work provides the first comprehensive resource-performance mapping for open-weight LLMs in regulatory information extraction, offering actionable guidance to select models based on deployment constraints. The insights extend beyond hydropower, shedding light on how parameter scaling impacts extraction tasks more broadly and enabling evidence-based decisions in choosing appropriate model sizes for regulatory compliance applications. <div>
arXiv:2511.11821v1 Announce Type: new 
Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Autoformalization of LLM-generated Outputs for Requirement Verification</title>
<link>https://arxiv.org/abs/2511.11829</link>
<guid>https://arxiv.org/abs/2511.11829</guid>
<content:encoded><![CDATA[
<div> Autoformalization, Large Language Models, Formal Verification, Logical Consistency, Natural Language Requirements<br /><br />Summary:<br /><br />This paper explores the application of autoformalization, which is the process of converting informal natural language statements into formal logic, leveraging Large Language Models (LLMs). The authors aim to address the challenge of verifying whether LLM-generated structured outputs accurately represent their natural language inputs, which currently lacks formal verification methods. Two experiments are conducted to demonstrate the approach's feasibility. In the first experiment, the LLM-based autoformalizer is able to identify logical equivalence between two differently worded natural language requirements, proving its potential to perform consistency checks. The second experiment showcases the tool's ability to detect logical inconsistencies between a natural language requirement and an LLM-generated output, highlighting its use as a verification mechanism. Although the study is preliminary and limited in scope, the results indicate that autoformalization can be an effective method for ensuring the fidelity and logical consistency of outputs produced by LLMs. This foundational work sets the stage for future, more comprehensive research aimed at improving the reliability of autoformalization and formal verification techniques in natural language understanding and generation tasks. <div>
arXiv:2511.11829v1 Announce Type: new 
Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection</title>
<link>https://arxiv.org/abs/2511.11857</link>
<guid>https://arxiv.org/abs/2511.11857</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, narrative analysis, movie scripts, clustering, computational learning

<br /><br />Summary: 
This paper addresses the challenges in automated narrative analysis within Natural Language Understanding, emphasizing the need for deep computational semantic representations paired with syntactic processing. Given the massive volume of narrative data, it advocates for automated semantic analysis and computational learning over manual approaches. The proposed framework specifically targets the analysis of sentiment arcs in movie scripts, allowing for both high-level and low-level concept extraction from narratives. Utilizing dictionary-based sentiment analysis, the framework employs a custom lexicon developed with the LabMTsimple storylab module, which incorporates Valence, Arousal, and Dominance scores derived from the NRC-VAD dataset. A notable advancement in this framework is its ability to cluster similar sentiment plots using Ward's hierarchical clustering technique. The experimental evaluation conducted on a dataset of movies demonstrates that the resulting analysis provides valuable insights for consumers and readers, assisting them in selecting stories or narratives that align with their preferences. Ultimately, this work contributes to a more nuanced understanding and analysis of narratives through sentiment exploration and character context. <div>
arXiv:2511.11857v1 Announce Type: new 
Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches</title>
<link>https://arxiv.org/abs/2511.11867</link>
<guid>https://arxiv.org/abs/2511.11867</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, radiology reports, follow-up adherence detection, machine learning classifiers, prompt optimization<br /><br />Summary: This work addresses the lack of domain-specific datasets to evaluate large language models (LLMs) in radiology by introducing an annotated corpus of 6,393 radiology reports from 586 patients labeled for follow-up imaging status. The study systematically compares traditional machine learning classifiers—including logistic regression (LR) and support vector machines (SVM)—and newer models such as Longformer and a fully fine-tuned Llama3-8B-Instruct, alongside generative LLMs like GPT-4o and the open-source GPT-OSS-20B. Generative models were tested under a baseline (Base) and a task-optimized (Advanced) setting, the latter emphasizing metadata, recommendation sentences, and context to improve performance. Prompt refinement further enhanced GPT-OSS-20B's reasoning accuracy. Evaluation metrics included precision, recall, and F1 scores, with confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846), establishing a strong benchmark. GPT-4o in the Advanced setting achieved the highest performance (F1 = 0.832), closely followed by GPT-OSS-20B (F1 = 0.828). Traditional classifiers LR and SVM also showed competitive results (F1 ≈ 0.776), demonstrating that while optimized LLMs can approach human-level agreement, simpler interpretable models remain valuable and resource-efficient baselines for clinical applications in radiology follow-up adherence detection. <div>
arXiv:2511.11867v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers</title>
<link>https://arxiv.org/abs/2511.11878</link>
<guid>https://arxiv.org/abs/2511.11878</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, healthcare, Brazilian Portuguese, MedPT, patient-doctor interactions

<br /><br />Summary: This article introduces MedPT, a large-scale corpus designed for Brazilian Portuguese, aimed at enhancing healthcare technologies. It consists of 384,095 authentic question-answer pairs derived from patient-doctor interactions, addressing the limitations of existing models that focus primarily on high-resource languages. The dataset underwent a rigorous multi-stage curation process, incorporating both quantitative and qualitative analyses to minimize noise and enrich queries. MedPT is further enhanced by LLM-driven annotation, classifying questions into seven semantic categories to better reflect user intent. The dataset exhibits a thematic breadth of 3,200 topics and highlights unique linguistic characteristics, including the natural asymmetry in patient-doctor communication. To demonstrate its effectiveness, the authors benchmarked a medical specialty routing task, achieving an impressive 94% F1-score using a fine-tuned 1.7B parameter model. Moreover, a qualitative error analysis indicated that misclassifications were tied to genuine clinical ambiguities, emphasizing the dataset's semantic depth. The authors aim to release MedPT to promote the creation of sustainable, culturally-aware medical technologies for the Portuguese-speaking community. <div>
arXiv:2511.11878v1 Announce Type: new 
Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts</title>
<link>https://arxiv.org/abs/2511.11883</link>
<guid>https://arxiv.org/abs/2511.11883</guid>
<content:encoded><![CDATA[
<div> Clinical notes, large language models, interpretability, generalizability, ICU mortality prediction<br /><br />Summary:<br /><br />1. Clinical notes hold rich and valuable contextual information but are challenging to use due to their unstructured nature, which introduces unintended biases such as gender and racial bias.  
2. Models trained on clinical data from one electronic health record (EHR) system often do not generalize well to other systems, mainly because of differences in formatting and data representation.  
3. ClinStructor is proposed as a novel pipeline that uses large language models (LLMs) to transform clinical free-text into structured, task-specific question-answer pairs, which improves the transparency and controllability of predictive modeling.  
4. This methodology significantly enhances the interpretability of machine learning models applied to clinical tasks, making it easier for clinicians to understand and trust model outputs.  
5. When applied to ICU mortality prediction, ClinStructor only shows a modest decrease of 2-3% in area under the curve (AUC) compared to direct fine-tuning, while offering a foundation for building more reliable, interpretable, and generalizable predictive models in healthcare settings. <div>
arXiv:2511.11883v1 Announce Type: new 
Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support</title>
<link>https://arxiv.org/abs/2511.11884</link>
<guid>https://arxiv.org/abs/2511.11884</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health, telehealth, GPT-2, reinforcement learning, therapeutic dialogue  

<br /><br />Summary:  
The paper addresses the significant socioeconomic burden of mental health illnesses, aggravated by COVID-19, highlighting the demand for telehealth solutions. It investigates how large language models (LLMs), specifically GPT-2, can be enhanced for therapeutic dialogue generation through supervised fine-tuning (SFT) and reinforcement learning (RL). The study restructured input formats to process contextual information and emotional states alongside user inputs. A multi-component reward function was created to align model outputs with professional therapist responses and annotated emotions. The results indicate notable improvements in various evaluation metrics, including BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581), when comparing reinforcement learning workflows to baseline GPT-2. Furthermore, the enhanced model achieved 99.34% accuracy in determining emotions, a significant increase over the 66.96% accuracy of baseline GPT-2. Overall, the findings demonstrate that reinforcement learning can effectively improve the performance of therapeutic dialogue systems, offering valuable assistive tools for therapists while underscoring the necessity of human clinical oversight. <div>
arXiv:2511.11884v1 Announce Type: new 
Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Additive Large Language Models for Semi-Structured Text</title>
<link>https://arxiv.org/abs/2511.11922</link>
<guid>https://arxiv.org/abs/2511.11922</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, clinical text classification, interpretability, additive models, semi-structured text<br /><br />Summary:<br /><br />This paper introduces CALM (Classification with Additive Large Language Models), a novel framework designed to improve interpretability in clinical text classification tasks using large language models (LLMs). CALM leverages the semi-structured nature of many clinical documents by decomposing inputs into meaningful components, such as sections of admission notes or intake form fields, and models predictions as an additive sum of contributions from each component. This additive approach integrates component contributions directly into the forward computation, enabling faithful and transparent explanations at both individual patient and broader population levels. The method facilitates clear visualizations through component-level risk curves akin to those found in generalized additive models, making it easier to understand and communicate learned relationships. CALM is particularly suited to clinical settings, where understanding which parts of a patient record drive risk signals is critical for trust, quality assurance, and meaningful clinical insights. Despite its interpretability advantages, CALM matches the performance of traditional LLM classifiers, proving that enhanced explanation does not require sacrificing accuracy. The framework also supports automatic extraction of semi-structured inputs from free-text clinical notes, broadening its applicability. Overall, CALM offers a practical and interpretable solution that fosters trust and insight during model development and auditing in healthcare contexts. <div>
arXiv:2511.11922v1 Announce Type: new 
Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InData: Towards Secure Multi-Step, Tool-Based Data Analysis</title>
<link>https://arxiv.org/abs/2511.11933</link>
<guid>https://arxiv.org/abs/2511.11933</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Data Analysis, Tool-based Reasoning, Security, Indirect Data Engagement (InData)<br /><br />Summary:<br /><br />1. The paper addresses security risks associated with allowing large language model (LLM) agents to directly generate and execute code on sensitive databases during data analysis.<br />2. To mitigate these risks, the authors propose restricting LLMs from direct code generation and data access, requiring interaction only through a secure, predefined set of verified tools.<br />3. Existing benchmarks for LLM tool use focus mainly on tool selection and simple task execution, lacking evaluation of complex, multi-step reasoning.<br />4. The authors introduce Indirect Data Engagement (InData), a novel dataset that evaluates LLMs' ability to perform multi-step, tool-based reasoning across data analysis questions of varying difficulty: Easy, Medium, and Hard.<br />5. Benchmarking 15 open-source LLMs on InData reveals that while large models (e.g., gpt-oss-120b) perform very well on Easy tasks (97.3% accuracy), their accuracy declines significantly on Hard tasks (69.6%), indicating a deficiency in robust multi-step tool-based reasoning.<br />6. The release of the InData dataset and code aims to foster development and assessment of LLMs with enhanced capabilities for secure, multi-step tool use in complex data analysis contexts. <div>
arXiv:2511.11933v1 Announce Type: new 
Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization</title>
<link>https://arxiv.org/abs/2511.11946</link>
<guid>https://arxiv.org/abs/2511.11946</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge graph, dialogue generation, LLM-KAT, entity anonymization, OpenDialKG

<br /><br />Summary: This article discusses the challenges of integrating external knowledge into dialogue generation by leveraging knowledge graphs (KG-DG). It highlights the limitations of large language models (LLMs) in effectively utilizing provided knowledge graphs, as they tend to rely more on internal knowledge. To address these issues, the authors introduce LLM-KAT, a novel evaluation procedure designed to measure how well LLMs attach to external knowledge in their generated responses. Additionally, the paper presents a simple yet effective technique called entity anonymization, aimed at encouraging LLMs to engage more with external knowledge sources. Through experiments conducted on the OpenDialKG dataset, the authors demonstrate that their proposed approaches lead to significant improvements in the ability of LLMs to utilize external knowledge effectively. The findings underscore the need for more focused methods in the field of KG-DG to enhance the interaction between LLMs and external knowledge, ultimately aiming for more coherent and informative conversational responses. <div>
arXiv:2511.11946v1 Announce Type: new 
Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Entropy Calibration of Language Models</title>
<link>https://arxiv.org/abs/2511.11966</link>
<guid>https://arxiv.org/abs/2511.11966</guid>
<content:encoded><![CDATA[
<div> Keywords: entropy calibration, autoregressive models, miscalibration, scaling behavior, black box prediction

<br /><br />Summary: This paper investigates entropy calibration in language models, focusing on whether the entropy over generations aligns with log loss on human text. Previous research indicated that models are often miscalibrated, with increased entropy correlating with poorer text quality in longer generations. The authors explore whether miscalibration improves with scaling and if calibration can be achieved without tradeoffs. They analyze a theoretical model that examines scaling behavior related to dataset size, revealing that for power law distributions with an exponent near 1, miscalibration improves slowly with scale. Empirical measurements from language models, ranging from 0.5B to 70B parameters, support this theoretical insight, showing similar rates of error accumulation across different model sizes. Consequently, larger models do not significantly reduce miscalibration compared to smaller ones, even though they generate higher quality outputs. The paper also discusses the standard practice of truncating distributions to improve text quality, though this increases log loss. Ultimately, it establishes that theoretically, reducing entropy while maintaining log loss is possible if a black box exists that can predict future text entropy accurately. <div>
arXiv:2511.11966v1 Announce Type: new 
Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reasoning Paradigm for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2511.11978</link>
<guid>https://arxiv.org/abs/2511.11978</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative LLMs, Named Entity Recognition, reasoning framework, Chain of Thought, zero-shot performance  

<br /><br />Summary: This article addresses the limitations of generative large language models (LLMs) in Named Entity Recognition (NER) performance, particularly in zero-shot and low-resource contexts. It identifies a problem of "cognitive shortcutting," where LLMs rely on implicit pattern matching rather than explicit reasoning, leading to suboptimal outcomes. To improve this, a new reasoning framework for NER is proposed, consisting of three key stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset is created with NER-oriented CoTs that provide relevant reasoning chains. This dataset is then utilized to refine the NER model, allowing it to generate coherent rationales prior to producing final outputs. In the last stage, reasoning enhancement optimizes the extraction process using a comprehensive reward signal, promoting explicit and verifiable extractions. Experimental results reveal that the proposed method, ReasoningNER, significantly boosts cognitive ability in NER tasks, achieving state-of-the-art (SOTA) performance with a notable 12.3 percentage-point improvement in F1 score over GPT-4 in zero-shot settings. The findings suggest substantial potential for advancing research in reasoning-oriented information extraction. Code for the study is accessible at GitHub. <div>
arXiv:2511.11978v1 Announce Type: new 
Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations</title>
<link>https://arxiv.org/abs/2511.12001</link>
<guid>https://arxiv.org/abs/2511.12001</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought, trust, reasoning errors, multimodal moral scenarios, explanations

<br /><br />Summary: This article investigates the dual role of Chain-of-Thought (CoT) explanations in enhancing transparency while also promoting confirmation bias in users. Researchers examine how reasoning errors in vision language models (VLMs) affect user trust and the ability to identify flaws. The study presents two main outcomes: first, users tend to equate trust with the agreement of outcomes, leading to continued reliance on the model even when the underlying reasoning is incorrect. Second, the delivery tone of the CoT explanations plays a significant role in user perception; a confident tone can suppress error detection and maintain user reliance on flawed reasoning. Consequently, users may overlook inaccuracies when presented with persuasive delivery styles. The findings underscore the importance of designing NLP systems that offer explanations fostering critical examination rather than uncritical acceptance. By highlighting the potential for CoT explanations to mislead despite their explanatory power, the research advocates for improvements in how these systems communicate reasoning to enhance users' analytical abilities. The authors commit to releasing the code publicly to encourage further exploration and development in this area. <div>
arXiv:2511.12001v1 Announce Type: new 
Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs</title>
<link>https://arxiv.org/abs/2511.12014</link>
<guid>https://arxiv.org/abs/2511.12014</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, cultural competence, evaluation benchmarks, reasoning, empirical analysis<br /><br />Summary: This article addresses the limitations of current evaluations of cultural competence in large language models (LLMs), which are increasingly used in diverse environments. Existing evaluation methods often emphasize de-contextualized correctness or forced-choice judgments, failing to account for the cultural understanding and reasoning necessary for appropriate responses. To bridge this gap, the authors propose a new set of benchmarks that present LLMs with realistic situational contexts requiring culturally grounded reasoning. In addition to the standard Exact Match metric, they introduce four complementary metrics: Coverage, Specificity, Connotation, and Coherence, which together measure various aspects of response quality. Empirical analysis of leading models shows that conventional evaluations tend to overestimate cultural competence and produce inconsistent results with high variance. In contrast, the new thick evaluation method reveals deeper reasoning differences among models, reduces outcome variance, and provides more stable and interpretable indicators of cultural understanding. This research highlights the need for improved evaluation metrics in assessing LLMs' cultural competence for better deployment in diverse cultural settings. <div>
arXiv:2511.12014v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task</title>
<link>https://arxiv.org/abs/2511.12109</link>
<guid>https://arxiv.org/abs/2511.12109</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-tuning, backtranslation, neural machine translation, Japanese corpus, low-resource languages  

<br /><br />Summary: This paper investigates the combination of fine-tuning and backtranslation for improving neural machine translation of a small Japanese corpus. It starts with a baseline English-to-Japanese model, achieving a COMET score of 0.460. The authors then implemented backtranslation using synthetic data from monolingual Japanese corpora, which resulted in a modest improvement to a COMET score of 0.468. Following this, they fine-tuned the model using a small, authentic parallel dataset from diverse Japanese news and literary sources, leading to a significant increase to COMET = 0.589 using the Mistral 7B architecture. The authors further enhanced the model's performance by combining both strategies: augmenting the small dataset with backtranslated examples and subsequently applying fine-tuning, which yielded a final COMET score of 0.597. These findings highlight the effectiveness of integrating backtranslation and targeted fine-tuning, demonstrating significant improvements in translation quality for limited training data scenarios. This approach presents a practical and efficient method for enhancing translations, particularly in low-resource language pairs like Japanese. <div>
arXiv:2511.12109v1 Announce Type: new 
Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12116</link>
<guid>https://arxiv.org/abs/2511.12116</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, temporal boundaries, LLMLagBench, training data, response accuracy

<br /><br />Summary: The paper addresses the limitations of Large Language Models (LLMs) regarding their knowledge cutoff, which restricts their ability to provide accurate information beyond a specific temporal point. This knowledge boundary often leads to a blend of outdated, time-sensitive information with general data during reasoning tasks, raising concerns about response accuracy. To tackle this issue, the authors introduce LLMLagBench, a systematic benchmark designed to identify the earliest probable temporal boundaries of an LLM's training data by testing its knowledge of recent events. The benchmark is applied to a diverse range of LLMs, including those with both declared and undeclared training cutoffs, to evaluate their performance. The reliability of LLMLagBench is further validated through manual checks and comparisons with publicly available data on the LLMs' pretraining. This research highlights the importance of understanding the temporal limits of LLMs to ensure accurate responses and facilitate better integration of external information sources when needed. The benchmark serves as a useful tool for researchers and developers to assess and improve LLMs’ relevance and accuracy in dynamic contexts. <div>
arXiv:2511.12116v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection</title>
<link>https://arxiv.org/abs/2511.12130</link>
<guid>https://arxiv.org/abs/2511.12130</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Conversational Stance Detection, U-MStance, PRISM, user-centric, stance detection

<br /><br />Summary: The increasing volume of multimodal social media content has prompted research in Multimodal Conversational Stance Detection (MCSD), which interprets user attitudes in discussions. Existing studies face two main limitations: pseudo-multimodality, where visual elements in posts are not aligned with text-only comments, and user homogeneity, which overlooks individual differences in stance expression. To address these challenges, the authors introduce U-MStance, a pioneering user-centric dataset with over 40,000 annotated comments on six real-world targets. They also propose PRISM, a Persona-Reasoned multimodal stance model. PRISM develops user personas from past interactions to capture unique characteristics and aligns textual and visual cues via Chain-of-Thought reasoning for better understanding of context. Additionally, it implements a mutual task reinforcement mechanism to optimize stance detection and response generation simultaneously, facilitating knowledge transfer. Experimental results on U-MStance highlight significant performance improvements of PRISM over existing strong baselines, validating the importance of user-centric and context-aware multimodal reasoning for a realistic understanding of stances in online interactions. <div>
arXiv:2511.12130v1 Announce Type: new 
Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing</title>
<link>https://arxiv.org/abs/2511.12133</link>
<guid>https://arxiv.org/abs/2511.12133</guid>
<content:encoded><![CDATA[
<div> Keywords: persuasive dialogue, telemarketing, reinforcement learning, Large Language Models, evaluation framework<br /><br />Summary: This paper addresses the challenges in goal-driven persuasive dialogue systems, particularly in telemarketing contexts, where multi-turn planning and factual accuracy are crucial. The authors identify limitations in existing methods due to scarce task-specific data and the shortcomings of direct Large Language Model (LLM) applications, such as strategic brittleness and hallucinations. To overcome these issues, they introduce TeleSalesCorpus, the first real-world grounded dialogue dataset for telemarketing. The proposed AI-Salesman framework features a dual-stage architecture: during training, it employs a Bayesian-supervised reinforcement learning algorithm designed to learn resilient sales strategies from noisy data. At inference, the Dynamic Outline-Guided Agent (DOGA) uses a pre-built script library to provide dynamic, turn-by-turn strategic guidance, enhancing dialogue coherence and effectiveness. Additionally, the authors design a comprehensive evaluation framework that integrates detailed metrics for essential sales skills alongside an LLM-as-a-Judge mechanism to assess performance. Experimental results demonstrate that AI-Salesman substantially outperforms baseline models in both automatic metrics and human evaluations, confirming its effectiveness in complex persuasive dialogue scenarios such as telemarketing. This work advances the development of robust, goal-driven dialogue systems with practical real-world applicability. <div>
arXiv:2511.12133v1 Announce Type: new 
Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding</title>
<link>https://arxiv.org/abs/2511.12140</link>
<guid>https://arxiv.org/abs/2511.12140</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal, Large Language Models, hallucination detection, VBackChecker, R^2-HalBench

<br /><br />Summary: This paper addresses the issue of hallucinations in Multimodal Large Language Models (MLLMs), which affects their reliability in practical applications. To combat this, the authors introduce VBackChecker, a reference-free framework that checks the consistency between MLLM-generated responses and visual inputs using a pixellevel Grounding LLM with reasoning and referring segmentation capabilities. The framework excels in rich-context scenarios and provides interpretability. To support VBackChecker, the authors present a new data generation pipeline called R-Instruct, which produces instruction-tuning data with rich-context descriptions, grounding masks, and hard negative samples. Additionally, they introduce R^2-HalBench, a novel hallucination benchmark for MLLMs that features real-world, rich-context descriptions from 18 MLLMs, along with high-quality annotations that cover various object, attribute, and relationship-level details. In performance evaluations, VBackChecker outperforms existing complex frameworks and achieves state-of-the-art results on the R^2-HalBench, rivaling GPT-4o in hallucination detection capabilities. Furthermore, it shows a more than 10% improvement in pixel-level grounding tasks compared to previous methods. The authors provide access to all related codes, data, and models at their GitHub repository. <div>
arXiv:2511.12140v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic</title>
<link>https://arxiv.org/abs/2511.12159</link>
<guid>https://arxiv.org/abs/2511.12159</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Integrated Reasoning, large language models, CriticSearch, dense rewards, multi-hop reasoning

<br /><br />Summary: This paper introduces CriticSearch, a novel framework designed to enhance Tool-Integrated Reasoning (TIR) in large language models by leveraging search engines for real-time knowledge retrieval. Traditional search agent pipelines often face challenges due to reliance on reinforcement learning, which can lead to issues with sparse rewards that hinder effective exploration and training stability. CriticSearch addresses this by implementing a fine-grained credit-assignment mechanism that provides dense, turn-level feedback through a retrospective critique from a frozen, asymmetric large language model. This critic evaluates each interaction based on comprehensive input, including the entire trajectory and correct responses, transforming evaluations into stable rewards that promote more efficient policy improvements. Experimental results reveal that CriticSearch significantly outperforms existing methods on various multi-hop reasoning tasks, demonstrating quicker convergence rates, enhanced training stability, and overall better performance metrics. This advancement suggests that integrating a structured critique process can substantially refine the operational efficacy of language models in complex question-answering scenarios. <div>
arXiv:2511.12159v1 Announce Type: new 
Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues</title>
<link>https://arxiv.org/abs/2511.12213</link>
<guid>https://arxiv.org/abs/2511.12213</guid>
<content:encoded><![CDATA[
<div> Keywords: fine-grained entity recognition, MME-RAG, retrieval-augmented generation, domain adaptation, KeyInfo retriever

<br /><br />Summary: Fine-grained entity recognition is essential for reasoning in task-oriented dialogues, yet current large language models struggle with domain adaptation and retrieval controllability. The paper presents MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework, which divides entity recognition into two coordinated stages. The first stage involves type-level judgment executed by lightweight managers, while the second stage focuses on span-level extraction by specialized experts. Each expert uses a KeyInfo retriever that provides semantically aligned, few-shot examples during inference, allowing for precise extraction without requiring additional training. The authors conducted experiments on various datasets including CrossNER, MIT-Movie, and MIT-Restaurant, as well as a newly created multi-domain customer-service dataset, revealing that MME-RAG outperforms recent baseline models across most domains. Further, ablation studies indicate that both the hierarchical approach and KeyInfo-guided retrieval significantly enhance robustness and cross-domain generalization. These findings position MME-RAG as a scalable, interpretable solution for enhancing adaptive dialogue understanding in different contexts. <div>
arXiv:2511.12213v1 Announce Type: new 
Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts</title>
<link>https://arxiv.org/abs/2511.12236</link>
<guid>https://arxiv.org/abs/2511.12236</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination detection, API, CONFACTCHECK, factual probes

<br /><br />Summary: Large language models (LLMs) exhibit impressive text generation capabilities, yet they often produce factually incorrect content, a phenomenon known as hallucination. This issue is particularly concerning in sensitive areas such as healthcare and finance. Generally, LLMs are accessed via APIs provided by vendors, limiting user control over model weights and fine-tuning capabilities. Existing hallucination detection methods typically require multiple API calls, which can lead to increased latency and costs. To address these challenges, the authors introduce CONFACTCHECK, a novel and efficient approach for detecting hallucinations without relying on external knowledge bases. CONFACTCHECK operates on the premise that factual probes embedded in the generated text should yield consistent responses from a single LLM and across different models. Through rigorous empirical evaluation on diverse datasets — encompassing both factual text generation and open-ended generation — CONFACTCHECK demonstrates a capacity to detect hallucinated facts more efficiently and accurately than existing methods that function under similar constraints. This advancement suggests a promising direction for improving the reliability of LLMs in practical applications, offering significant benefits in resource management and accuracy. The accompanying code for CONFACTCHECK is made publicly available for further exploration. <div>
arXiv:2511.12236v1 Announce Type: new 
Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations</title>
<link>https://arxiv.org/abs/2511.12249</link>
<guid>https://arxiv.org/abs/2511.12249</guid>
<content:encoded><![CDATA[
<div> Keywords: Vietnamese NLP, contextualized embeddings, Word Sense Disambiguation, contrastive learning, semantic evaluation<br /><br />Summary:  
The paper addresses the lack of robust semantic understanding models and resources for the Vietnamese language, which is a low-resource language compared to English. It introduces ViConBERT, a new framework that learns Vietnamese contextualized word embeddings by combining contrastive learning (SimCLR) with gloss-based distillation to better capture word meanings in context. Alongside the model, the authors present ViConWSD, the first large-scale synthetic dataset for semantic evaluation in Vietnamese, supporting both Word Sense Disambiguation (WSD) and contextual similarity tasks. Experimental results demonstrate that ViConBERT significantly outperforms strong baseline models on WSD, achieving an F1 score of 0.87. Furthermore, ViConBERT delivers competitive performance on semantic similarity benchmarks ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), indicating that it effectively models both discrete word senses and graded semantic relations. The paper emphasizes the importance of combining contrastive learning with gloss-based knowledge distillation to enhance semantic representation in a low-resource language setting. Lastly, the authors contribute to the community by making their code, trained models, and datasets publicly available for further research and development in Vietnamese natural language processing. <div>
arXiv:2511.12249v1 Announce Type: new 
Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor</title>
<link>https://arxiv.org/abs/2511.12281</link>
<guid>https://arxiv.org/abs/2511.12281</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt compression, Large Language Models, compress inputs, compression rate, Cmprsr

<br /><br />Summary:  
This paper introduces a novel paradigm for prompt compression utilizing smaller Large Language Models (LLMs) to compress inputs for larger models, addressing the high costs associated with using black-box LLMs. It presents the first extensive benchmark evaluating the performance of 25 different open- and closed-source models as compressors, highlighting significant disparities in their abilities to preserve semantically important information and adhere to user-defined compression rates (CR). The study features improvements in the performance of gpt-4.1-mini, identified as the best vanilla compressor, via Textgrad-based compression meta-prompt optimization. Furthermore, it identifies Qwen3-4B as a promising open-source model, which is subsequently post-trained through supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) to enhance CR adherence and downstream task performance. This results in the creation of a new model named Cmprsr, which outperforms traditional extractive and vanilla abstractive compression techniques across varying input lengths and domains, validated using datasets such as MeetingBank, LongBench, and GSM8k. Cmprsr is capable of closely following the requested compression rate, facilitating better control over the trade-off between cost and quality. <div>
arXiv:2511.12281v1 Announce Type: new 
Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugAbEx : Way Forward for Extractive Case Summarization</title>
<link>https://arxiv.org/abs/2511.12290</link>
<guid>https://arxiv.org/abs/2511.12290</guid>
<content:encoded><![CDATA[
<div> Keywords: legal summarization, extractive summaries, abstractive summaries, dataset augmentation, natural language processing  

<br /><br />Summary: The complexity of legal documents presents a significant cognitive challenge for law practitioners, prompting interest in automatic summarization techniques. Researchers are increasingly focusing on extractive summarizers due to the limitations of abstractive summarization methods, which may misrepresent legal nuances. To address the high cost of creating gold standard extractive summaries, the authors propose an innovative pipeline that transforms existing abstractive summaries into corresponding extractive versions, ensuring that expert insights are transferred accurately. This project aims to enhance seven existing legal case summarization datasets by adding extractive summaries alongside their abstractive counterparts, thereby creating a more robust resource for research in this area. To maintain the quality of these new extractive summaries, a detailed comparative evaluation with the original abstractive summaries is conducted, examining structural, lexical, and semantic aspects. The authors assert that the merger of these datasets will provide valuable opportunities for advancing automatic summarization methodologies within legal contexts. They also commit to making the augmented datasets publicly available, further supporting the research community's efforts in improving legal document summarization. <div>
arXiv:2511.12290v1 Announce Type: new 
Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering</title>
<link>https://arxiv.org/abs/2511.12300</link>
<guid>https://arxiv.org/abs/2511.12300</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, NLP tasks, quizzes, human performance, numerical answers  

<br /><br />Summary: This study explores the comparative performance of Large Language Models (LLMs) and humans on quiz questions in a buzzer setting. The researchers began by collecting Japanese quiz data, which included the questions, answers, and the correct response rates of human participants. They then prompted LLMs to attempt these quizzes under various conditions. The focus of the investigation was to understand if the challenges that are difficult for humans in quiz formats are similarly challenging for LLMs. The findings revealed that LLMs exhibited a lower correct answer rate than humans, especially for quizzes where the correct answers were not found in Wikipedia entries, indicating that LLMs rely heavily on the information present in such datasets. Additionally, LLMs faced significant difficulty with questions requiring numerical responses. Overall, the research highlights the nuanced differences in how LLMs and humans tackle quiz-based challenges, suggesting that LLMs may have limitations in specific areas that do not align with human cognitive strengths. <div>
arXiv:2511.12300v1 Announce Type: new 
Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load</title>
<link>https://arxiv.org/abs/2511.12381</link>
<guid>https://arxiv.org/abs/2511.12381</guid>
<content:encoded><![CDATA[
<div> Keywords: ironic rebound, negation instructions, large language models, polarity separation, ReboundBench<br /><br />Summary:  
This study investigates the phenomenon of ironic rebound, where negation instructions like "do not mention X" paradoxically increase the prominence of X in human thought, and similarly affect large language models (LLMs). The authors conducted two key experiments: (1) Load & content: after negation instructions, they varied distractor text types—including semantic, syntactic, and repetition—to measure the strength of rebound; (2) Polarity separation: they tested if models can distinguish neutral from negative framings of the same concept and whether this ability predicts the persistence of rebound. Results indicated that ironic rebound arises immediately following negation and intensifies particularly with longer or semantically rich distractors, whereas repetition appears to aid suppression of the forbidden concept. Additionally, stronger polarity separation by models correlated with more persistent rebound effects. A mechanistic analysis using circuit tracing revealed that sparse attention heads in middle layers amplify forbidden tokens despite suppression occurring in early layers, providing insight into how long-context interference occurs in LLMs. To facilitate further research, the authors also release ReboundBench, a dataset of 5,000 systematically varied negation prompts designed to rigorously probe rebound phenomena in large language models. <div>
arXiv:2511.12381v1 Announce Type: new 
Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Phonemes to Meaning: Evaluating Large Language Models on Tamil</title>
<link>https://arxiv.org/abs/2511.12387</link>
<guid>https://arxiv.org/abs/2511.12387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Tamil, ILAKKANAM, linguistic evaluation, performance  

<br /><br />Summary:  
This paper addresses the performance of Large Language Models (LLMs) in low-resource and morphologically rich languages like Tamil, which has not been thoroughly researched. Existing multilingual benchmarks are often based on translated English datasets, neglecting the unique linguistic and cultural aspects of Tamil. To tackle this issue, the authors introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark, which is crafted from 820 questions sourced from Sri Lankan Tamil examination papers. These questions are categorized by trained linguists into five linguistic categories and a factual knowledge category, covering Grades 1-13 for comprehensive linguistic representation. The evaluation of both closed-source and open-source LLMs revealed that Gemini 2.5 outperformed others, while open-source models exhibited poorer performance, indicating a need for better linguistic grounding. Further breakdowns show a consistent performance in lower-grade questions but a noticeable decline with increasing complexity. Notably, the study found no strong correlation between a model's overall performance and its effectiveness in identifying linguistic categories, suggesting that observed performance may stem from exposure rather than authentic understanding of the language. <div>
arXiv:2511.12387v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models</title>
<link>https://arxiv.org/abs/2511.12464</link>
<guid>https://arxiv.org/abs/2511.12464</guid>
<content:encoded><![CDATA[
<div> Keywords: reward models, preference dimensions, MRMBench, inference-time probing, multi-objective optimization  

<br /><br />Summary: This work addresses the limitations of existing methods for evaluating reward models, which typically use a fixed pairwise ranking test set without detailed performance insights on preference dimensions. To tackle this challenge, the authors introduce the Multi-dimensional Reward Model Benchmark (MRMBench), comprising six probing tasks that assess different preference dimensions. This benchmark encourages the development of reward models that effectively capture diverse preferences. Additionally, the study presents an analysis technique called inference-time probing, which enhances the interpretability of reward predictions by identifying the preference dimensions utilized during predictions. Extensive experiments demonstrate that MRMBench correlates strongly with the alignment performance of large language models (LLMs), establishing it as a valuable tool for reward model development. Analysis of MRMBench results reveals an ongoing struggle within reward models to accurately reflect multi-dimensional preferences, suggesting that multi-objective optimization could enhance reward modeling. Furthermore, the inference-time probing method provides a reliable metric for evaluating the confidence of reward predictions, thereby improving the alignment of LLMs and highlighting the need for more nuanced evaluation methods in this field. <div>
arXiv:2511.12464v1 Announce Type: new 
Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing</title>
<link>https://arxiv.org/abs/2511.12472</link>
<guid>https://arxiv.org/abs/2511.12472</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge graph, serendipity, KGQA, drug repurposing  

<br /><br />Summary: Large Language Models (LLMs) have significantly improved knowledge graph question answering (KGQA), but they mainly return predictable answers. This paper introduces the concept of serendipity-aware KGQA, emphasizing LLMs' potential to provide surprising and novel insights. The authors define the serendipity-aware KGQA task and propose the SerenQA framework to assess LLMs' capabilities in discovering unexpected insights within scientific KGQA tasks. SerenQA features a robust serendipity metric that evaluates relevance, novelty, and surprise, along with a benchmark derived from the Clinical Knowledge Graph, specifically targeting drug repurposing. The framework also includes a structured evaluation pipeline with three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Experimental results indicate that while state-of-the-art LLMs excel in retrieval tasks, they struggle to deliver genuinely surprising and valuable insights. This highlights the need for further research and enhancements in this area. The paper provides curated resources and an extended version, which are accessible at the designated link. Overall, the findings demonstrate a significant gap in LLM capabilities regarding the identification of serendipitous answers in KGQA. <div>
arXiv:2511.12472v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGuard-v1: Safety Guardrail for Large Language Models</title>
<link>https://arxiv.org/abs/2511.12497</link>
<guid>https://arxiv.org/abs/2511.12497</guid>
<content:encoded><![CDATA[
<div> Keywords: SGuard-v1, Large Language Models, ContentFilter, JailbreakFilter, safety performance

<br /><br />Summary: The article introduces SGuard-v1, a lightweight safety guardrail designed for Large Language Models (LLMs). It consists of two specialized models: ContentFilter and JailbreakFilter. The ContentFilter identifies safety risks in prompts and responses, adhering to the MLCommons hazard taxonomy for AI trust and safety assessment. The JailbreakFilter, on the other hand, is trained using a well-structured curriculum that encompasses integrated datasets and previous findings on adversarial prompting, addressing 60 significant attack types while minimizing false-unsafe classifications. SGuard-v1 utilizes the 2B-parameter Granite-3.3-2B-Instruct model, supporting 12 languages. A total of approximately 1.4 million training instances were curated from collected and synthesized data for further instruction tuning, distributing the data between the two components according to their specific roles. Through comprehensive evaluations against public and proprietary safety benchmarks, SGuard-v1 has achieved state-of-the-art safety performance while maintaining a lightweight structure, which helps reduce deployment overhead. Additionally, it enhances interpretability by providing multi-class safety predictions alongside binary confidence scores. The release of SGuard-v1 under the Apache-2.0 License aims to facilitate continued research and practical applications in AI safety. <div>
arXiv:2511.12497v1 Announce Type: new 
Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs</title>
<link>https://arxiv.org/abs/2511.12504</link>
<guid>https://arxiv.org/abs/2511.12504</guid>
<content:encoded><![CDATA[
<div> Keywords: QA-Noun, semantic alignment, noun-centered semantics, QA-SRL, fine-grained decomposition  

<br /><br />Summary: The paper introduces QA-Noun, a novel QA-based framework aimed at capturing noun-centered semantic relations, addressing a gap in existing semantic approaches that have primarily focused on predicate-argument interactions. QA-Noun defines nine question templates that explore both explicit syntactical roles and implicit contextual roles for nouns, facilitating the creation of interpretable QA pairs that enhance traditional verbal QA-Semantic Role Labeling (QA-SRL). A dataset comprising over 2,000 annotated noun mentions is released alongside detailed guidelines, and a trained model that integrates with QA-SRL is presented, allowing for a unified semantic decomposition of sentence meanings into highly fine-grained facts. Evaluation results demonstrate that QA-Noun achieves nearly full coverage of Abstract Meaning Representation (AMR) noun arguments while uncovering additional implied relations. Furthermore, when QA-Noun is combined with QA-SRL, it provides more than 130% greater granularity compared to recent fact-based decomposition methods like FactScore and DecompScore. Overall, QA-Noun enriches the established QA-based semantic framework, contributing to a comprehensive and scalable approach for fine-grained semantic decomposition tailored for cross-text alignment. <div>
arXiv:2511.12504v1 Announce Type: new 
Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction</title>
<link>https://arxiv.org/abs/2511.12520</link>
<guid>https://arxiv.org/abs/2511.12520</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, TAdaRAG, knowledge graph, intent-driven routing, reinforcement learning  

<br /><br />Summary:  
The paper presents TAdaRAG, a new framework that enhances Retrieval-Augmented Generation (RAG) by constructing task-adaptive knowledge graphs in real-time from external sources. Traditional RAG models often suffer from information loss and irrelevant detail retrieval due to input context limitations, leading to issues such as response hallucinations and poor reasoning. To combat these challenges, TAdaRAG introduces an intent-driven routing mechanism that directs to specific extraction templates tailored for particular domains. This is complemented by a supervised fine-tuning process and a reinforcement learning-based implicit extraction method. These innovations ensure the integration of knowledge is concise, coherent, and free of redundancy. Evaluations conducted on six public benchmarks, along with a real-world business benchmark called NowNewsQA, demonstrate that TAdaRAG significantly outperforms existing methods across various domains and long-text tasks. These results underline its strong generalization capabilities and practical effectiveness, making it a notable advancement in the field of language model enhancement through external knowledge integration. <div>
arXiv:2511.12520v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Length Bias in RLHF through a Causal Lens</title>
<link>https://arxiv.org/abs/2511.12573</link>
<guid>https://arxiv.org/abs/2511.12573</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Human Feedback, Length Bias, Reward Model, Content Quality  

<br /><br />Summary: The article discusses the prevalent issue of length bias in reinforcement learning from human feedback (RLHF) when aligning large language models (LLMs) with human preferences. Length bias manifests as a tendency for reward models to favor longer responses, equating verbosity with quality. To address this issue, the authors propose a causal framework that includes a counterfactual data augmentation method. This method generates response pairs aimed at disentangling content quality from verbosity. They create two types of pairs: length-divergent pairs with similar content and content-divergent pairs of similar length. Empirical evaluations show that this approach significantly reduces the length bias in reward assignments, leading to outputs that are more concise and focused on content. The proposed method enhances the robustness and content sensitivity of reward modeling in RLHF pipelines. Overall, this research contributes to improving the alignment of LLMs with human preferences by mitigating the biases associated with response length, ensuring that the quality of responses is evaluated based on meaningful content rather than mere verbosity. <div>
arXiv:2511.12573v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMWOZ: Building Multimodal Agent for Task-oriented Dialogue</title>
<link>https://arxiv.org/abs/2511.12586</link>
<guid>https://arxiv.org/abs/2511.12586</guid>
<content:encoded><![CDATA[
<div> Task-oriented dialogue systems, multimodal dialogue, GUI interaction, dataset MMWOZ, multimodal model MATE<br /><br />Summary:<br /><br />This paper addresses the limitations of traditional task-oriented dialogue systems that rely on customized back-end APIs, which are often unavailable in real-world scenarios dominated by front-end Graphical User Interfaces (GUIs). To bridge this gap, the authors introduce MMWOZ, a new multimodal dialogue dataset derived from the existing MultiWOZ 2.3 dataset. The dataset creation involved developing a web-style GUI as the dialogue front-end, devising an automated script to translate dialogue states and system actions into executable GUI operation instructions, and collecting snapshots of web pages paired with these instructions. Additionally, the authors propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue), which serves as a baseline for tasks using the MMWOZ dataset. Comprehensive experiments with MATE are conducted to analyze the capabilities and challenges of building practical multimodal task-oriented dialogue agents. The work contributes to advancing dialogue systems that operate effectively through visual interfaces, closer to real-world application environments where backend APIs are not always accessible. <div>
arXiv:2511.12586v1 Announce Type: new 
Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group-Aware Reinforcement Learning for Output Diversity in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12596</link>
<guid>https://arxiv.org/abs/2511.12596</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, mode collapse, Group-Aware Policy Optimization, response diversity, frequency-aware reward function

<br /><br />Summary: This article addresses the issue of mode collapse in Large Language Models (LLMs), where these models tend to generate limited and repetitive outputs despite the presence of multiple valid responses. The authors introduce a novel approach called Group-Aware Policy Optimization (GAPO), which builds upon the recently developed Group Relative Policy Optimization (GRPO). GAPO focuses on evaluating rewards at a group level, which allows for improvements in model learning related to diversity and coverage. To demonstrate its effectiveness, the authors implement a frequency-aware reward function that promotes more uniform sampling among valid LLM completions. The results show that models trained using GAPO are capable of generating not only valid but also more diverse responses compared to traditional methods. Additionally, GAPO proves adaptable to various open-ended prompts, enhancing response diversity without sacrificing accuracy across established LLM benchmarks, including GSM8K, MATH, HumanEval, and MMLU-Pro. The researchers plan to publicly release their code, making their advancements accessible for further exploration in the field of language modeling. <div>
arXiv:2511.12596v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</title>
<link>https://arxiv.org/abs/2511.12609</link>
<guid>https://arxiv.org/abs/2511.12609</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-MoE 2.0, omnimodal large model, Mixture-of-Experts, reinforcement strategy, SOTA benchmarks  

<br /><br />Summary:  
We introduce Uni-MoE 2.0 from the Lychee family, a fully open-source omnimodal large model (OLM) that enhances the capabilities of the Uni-MoE series in multimodal understanding and generation. Built on the Qwen2.5-7B dense architecture, it incorporates dynamic-capacity Mixture-of-Experts (MoE) design and a progressive training strategy with an iterative reinforcement component. The model excels in understanding and generating images, text, and speech by employing a new MoE framework that supports 10 cross-modal inputs while ensuring computational efficiency. It utilizes an Omni-Modality 3D RoPE for spatio-temporal alignment in the self-attention mechanism. Training involves cross-modal pretraining followed by a supervised fine-tuning strategy that activates modality-specific experts, supported by balanced data and an iterative GSPO-DPO method to enhance reinforcement learning. With training on approximately 75B tokens of multimodal data, Uni-MoE 2.0 demonstrates superior performance in 85 benchmarks, achieving state-of-the-art results particularly in video understanding, audiovisual reasoning, and long-form speech processing, surpassing the previous Qwen2.5-Omni model in over 50 evaluations. Key improvements include significant gains in multiple metrics for generating tasks and processing effectiveness. <div>
arXiv:2511.12609v1 Announce Type: new 
Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing</title>
<link>https://arxiv.org/abs/2511.12630</link>
<guid>https://arxiv.org/abs/2511.12630</guid>
<content:encoded><![CDATA[
<div> Keywords: NOTAM semantic parsing, aviation domain knowledge, Knots dataset, prompt engineering, automated flight safety analysis  

<br /><br />Summary:  
This paper addresses the challenge of interpreting Notice to Air Missions (NOTAMs), which contain critical flight safety information but are difficult to parse due to complex language and implicit reasoning. Unlike prior work limited to surface-level tasks like classification and named entity recognition, the authors propose a novel task called NOTAM semantic parsing that emphasizes deep semantic inference and the integration of specialized aviation knowledge to generate structured, inference-rich outputs. To support this, they introduce Knots (Knowledge and NOTAM Semantics), a high-quality dataset comprising 12,347 expert-annotated NOTAMs from 194 Flight Information Regions. The dataset was developed through a multi-agent collaborative framework ensuring comprehensive annotation coverage across various fields. The study systematically evaluates diverse prompt-engineering and model adaptation techniques to improve machine understanding and processing of aviation texts. Experimental results demonstrate significant performance gains, validating the effectiveness of their approach for automated NOTAM analysis. Additionally, the released codebase provides a valuable resource for further research and development in aviation safety and natural language processing. <div>
arXiv:2511.12630v1 Announce Type: new 
Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing</title>
<link>https://arxiv.org/abs/2511.12661</link>
<guid>https://arxiv.org/abs/2511.12661</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, faithfulness, reasoning tasks, SFT+RL, alignment

<br /><br />Summary: The article addresses the challenge of aligning Large Language Models (LLMs) to ensure they remain faithful to new knowledge during complex, multi-hop reasoning tasks. The authors identify a "faithfulness gap" in state-of-the-art methods like Reason-KE, where the focus on format mimicry leads to critical factual hallucinations. To tackle this problem, they introduce Reason-KE++, an SFT+RL framework designed to enhance process-level faithfulness. A key component of this framework is the Stage-aware Reward mechanism, which provides dense supervision for intermediate reasoning steps such as decomposition and sub-answer correctness. The authors caution against the use of naive outcome-only reinforcement learning, which can undermine reasoning integrity while falsely improving final accuracy. Through their method, they achieved a new state-of-the-art performance of 95.48% on the MQUAKE-CF-3k benchmark, surpassing previous results by 5.28%. The findings emphasize that refining and aligning the reasoning process is critical to developing trustworthy LLMs for complex tasks. Overall, this work highlights the necessity of a more nuanced approach to LLM alignment, focusing on both reasoning processes and final outputs. <div>
arXiv:2511.12661v1 Announce Type: new 
Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data</title>
<link>https://arxiv.org/abs/2511.12690</link>
<guid>https://arxiv.org/abs/2511.12690</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-speech translation, Persian, English, synthetic data, low-resource languages

<br /><br />Summary: This paper introduces a direct speech-to-speech translation (S2ST) system that translates Persian speech into English speech, addressing the challenge posed by the scarcity of parallel speech data in low-resource languages. The proposed model is composed of three main components: a conformer-based encoder for mapping source speech to high-level acoustic representations, a causal transformer decoder that translates these representations into target speech units, and a unit-based neural vocoder for generating waveforms from the predicted discrete units. To tackle data limitations, the authors created a new Persian-English parallel speech corpus by leveraging a large language model to translate Persian transcriptions into English and then using a state-of-the-art zero-shot text-to-speech system for synthesizing the corresponding English speech. This innovative approach significantly enhances the availability of parallel speech data, increasing it roughly sixfold. Testing on the Persian-English portion of the CVSS corpus revealed notable improvements, with the model achieving 4.6 ASR BLEU over standard direct baselines when utilizing the synthetic data. The study concludes that self-supervised pre-training, discrete speech units, and synthetic parallel data effectively enhance direct S2ST performance for low-resource languages like Persian-English. <div>
arXiv:2511.12690v1 Announce Type: new 
Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</title>
<link>https://arxiv.org/abs/2511.12710</link>
<guid>https://arxiv.org/abs/2511.12710</guid>
<content:encoded><![CDATA[
<div> Keywords: EvoSynth, jailbreak, large language models, evolutionary synthesis, attack algorithms<br /><br />Summary: EvoSynth is an innovative autonomous framework designed to create novel jailbreak methods for Large Language Models (LLMs) through evolutionary synthesis rather than relying on pre-existing attack strategies. Unlike traditional automated red teaming frameworks that refine known prompts, EvoSynth uses a multi-agent system to engineer, evolve, and execute new code-based attacks autonomously. A key feature of EvoSynth is its code-level self-correction loop, which enables the system to iteratively rewrite and improve its attack algorithms based on performance failures. Extensive experiments demonstrate that EvoSynth achieves a state-of-the-art Attack Success Rate (ASR) of 85.5% against robust LLMs like Claude-Sonnet-4.5, outperforming existing methods. Furthermore, EvoSynth generates a more diverse range of attack strategies, enhancing the creativity and effectiveness of jailbreak attempts beyond traditional frameworks. The framework and its code have been released publicly to encourage further research in the evolutionary synthesis of jailbreak methods for AI safety and robustness. This marks a significant advancement in autonomous attack generation, moving beyond simple prompt modification to dynamic and self-improving algorithmic invention. <div>
arXiv:2511.12710v1 Announce Type: new 
Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Focus Memory for Language Models</title>
<link>https://arxiv.org/abs/2511.12712</link>
<guid>https://arxiv.org/abs/2511.12712</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Adaptive Focus Memory, Safety, Token Budget, Dialogue Management  

<br /><br />Summary: Large language models (LLMs) face limitations in multi-turn dialogue due to fixed context windows and inefficient memory strategies. Traditional methods, such as replaying full conversations or using static summarization, can lead to the loss of crucial user details. This paper introduces Adaptive Focus Memory (AFM), a context management system that categorizes past messages into three fidelity levels: FULL, COMPRESSED, and PLACEHOLDER. It does this based on the semantic relevance to the current query, recency, and importance classifications. By optimizing message retention under a strict token budget, AFM prioritizes high-fidelity messages while maintaining a cost-effective summary of the dialogue. The method demonstrates effectiveness in preserving critical safety information, such as a user’s severe peanut allergy, during conversations of varying lengths. In benchmarks, AFM matches the safety performance of traditional replay methods while reducing average token usage by 66%. A modular Python implementation of AFM designed for OpenAI-compatible APIs is also provided, allowing practitioners to minimize inference costs without compromising safety and factual continuity. <div>
arXiv:2511.12712v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Brittleness of LLMs: A Journey around Set Membership</title>
<link>https://arxiv.org/abs/2511.12728</link>
<guid>https://arxiv.org/abs/2511.12728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning tasks, set membership queries, empirical evaluation, failure modes  

<br /><br />Summary: This study investigates the paradox of large language models (LLMs) exhibiting superhuman performance on complex reasoning tasks while faltering on simpler problems, which raises issues regarding their reliability and interpretability. The researchers focus on set membership queries as a fundamental reasoning form, using straightforward tasks like determining if an item belongs to a set. By systematically evaluating various factors such as prompt phrasing, semantic structure, element ordering, and model selection, the analysis reveals that LLM performance on these elementary tasks is consistently brittle and unpredictable. The study points to a fragmented and convoluted grasp of the set concept by the models. Furthermore, the research highlights that the simplicity of the task facilitates large-scale experiments, allowing for a comprehensive mapping and analysis of failure modes. This methodology not only enriches the understanding of LLM capabilities but also establishes a valuable framework for evaluating these models in general. Overall, the findings underscore the importance of scrutinizing LLM performance on basic reasoning tasks to better assess their reliability and interpretability. <div>
arXiv:2511.12728v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidence of Phase Transitions in Small Transformer-Based Language Models</title>
<link>https://arxiv.org/abs/2511.12768</link>
<guid>https://arxiv.org/abs/2511.12768</guid>
<content:encoded><![CDATA[
<div> Keywords: phase transitions, large language models, small transformers, training dynamics, vocabulary statistics  

<br /><br />Summary: This paper investigates whether phase transitions—sudden emergent abilities previously observed in large language models (LLMs)—also occur in small transformer-based models. The authors explore three main questions: (1) whether phase transitions exist in small models, (2) if these transitions can be detected directly in the linear training space without log-scale rescaling, and (3) whether such transitions appear at early training stages. To address these questions, they train a small GPT-style transformer on a character-level dataset and analyze vocabulary usage throughout training. Key metrics include average word length, counts of correct versus incorrect words, and vocabulary diversity shifts. Additionally, the study applies Poisson and sub-Poisson statistics to measure connections and reorganizations within the learned vocabulary. The results reveal a distinct phase transition point during training that standard loss or validation curves do not capture, but which becomes evident through the proposed vocabulary-based and statistical probes. These findings indicate that phase-transition behavior is a general feature of language model training, observable even in modest-sized models, detectable directly in linear space, and emerges surprisingly early as the model achieves coherence. This work provides new insights into the nonlinear dynamics underpinning language model training and highlights the value of tailored metrics for exposing phase transitions beyond conventional evaluation methods. <div>
arXiv:2511.12768v1 Announce Type: new 
Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Reinforcement in Context</title>
<link>https://arxiv.org/abs/2511.12782</link>
<guid>https://arxiv.org/abs/2511.12782</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment, interruptions, user input, Chain-of-Thought

<br /><br />Summary: 
This article addresses the current challenges in aligning Large Language Models (LLMs) with user intentions amidst the growing concerns of adversarial attacks and model misbehavior. Existing research has demonstrated that the probability of LLM jailbreak increases proportionately with the length of user input and conversations. However, there is a noticeable gap in research focused on enhancing alignment in relation to longer user inputs. To tackle this issue, the authors propose the method of interruptions, which involves embedding control sentences into the user input at regular intervals (every x tokens, where x is arbitrary). This strategy aims to provide a safeguard against potential scheming behaviors exhibited by LLMs. Furthermore, the authors suggest that this approach can be effectively incorporated into the Chain-of-Thought reasoning process, thereby reinforcing the model’s robustness while maintaining user alignment. By integrating interruptions, the proposal presents a novel mechanism that could enhance LLM performance and mitigate the risk of misinterpretation or unintended responses. Overall, this research contributes to the ongoing discourse on LLM alignment and proposes a practical avenue to improve model safety and reliability as user inputs grow in complexity and length. <div>
arXiv:2511.12782v1 Announce Type: new 
Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing</title>
<link>https://arxiv.org/abs/2511.12784</link>
<guid>https://arxiv.org/abs/2511.12784</guid>
<content:encoded><![CDATA[
<div> autoformalization, large language models, paraphrasing robustness, formal proofs, semantic validity<br /><br />Summary:<br /><br />This paper explores the robustness of large language models (LLMs) in the domain of autoformalization, focusing on how paraphrased natural language (NL) inputs affect the generation of formal proofs. The study builds upon observations from recent text-to-SQL research that demonstrated LLMs’ sensitivity to semantic-preserving paraphrases, extending this investigation to formal proof generation. To measure the effects, the authors evaluate LLM outputs using semantic validity and compilation validity metrics. The benchmarks used include MiniF2F and the Lean 4 version of ProofNet, both established datasets for mathematical formalization tasks. Two modern LLMs are employed in the experiments, with paraphrased NL statements generated and cross-evaluated across these models to assess consistency and performance stability. The findings reveal significant variability in model performance when handling different paraphrased inputs, indicating that seemingly minor shifts in natural language phrasing can substantially influence the quality and correctness of the formal proofs generated. This variability highlights challenges in ensuring grounded and verifiable autoformalizations and suggests the need for improved robustness in LLM architectures and training strategies. Ultimately, the paper contributes to understanding LLM limitations in autoformalization and informs future efforts to develop more resilient formal proof generation systems. <div>
arXiv:2511.12784v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals</title>
<link>https://arxiv.org/abs/2511.12821</link>
<guid>https://arxiv.org/abs/2511.12821</guid>
<content:encoded><![CDATA[
<div> Keywords: journal impact, AI engagement, bibliometric indicators, collaboration structures, BioMedJImpact

<br /><br />Summary: This article presents BioMedJImpact, a dataset specifically designed for analyzing journal-level scientific impact and AI engagement in biomedicine. It encompasses 1.74 million PubMed Central articles across 2,744 journals, integrating bibliometric indicators and collaboration features, along with a novel three-stage pipeline using large language models (LLMs) to assess AI engagement. The study investigates how collaboration intensity and AI engagement affect scientific impact during two distinct timeframes: pre-pandemic (2016-2019) and post-pandemic (2020-2023). Results show that journals with higher collaboration intensity, particularly those featuring larger and more diverse author teams, achieve greater citation impact. Additionally, AI engagement has emerged as a significant correlate of journal prestige, particularly evident in quartile rankings. To validate the LLM pipeline for AI engagement, human evaluations were conducted, revealing strong agreement in AI relevance detection and consistent subfield classification. Ultimately, BioMedJImpact is positioned as a comprehensive resource facilitating the analysis of scientific impact and innovation dynamics at the intersection of biomedicine and AI, while providing a validated methodological framework for future research. Code for implementation is available at https://github.com/JonathanWry/BioMedJImpact. <div>
arXiv:2511.12821v1 Announce Type: new 
Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation</title>
<link>https://arxiv.org/abs/2511.12832</link>
<guid>https://arxiv.org/abs/2511.12832</guid>
<content:encoded><![CDATA[
arXiv:2511.12832v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying consistency and accuracy of Latent Dirichlet Allocation</title>
<link>https://arxiv.org/abs/2511.12850</link>
<guid>https://arxiv.org/abs/2511.12850</guid>
<content:encoded><![CDATA[
arXiv:2511.12850v1 Announce Type: new 
Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.12851</link>
<guid>https://arxiv.org/abs/2511.12851</guid>
<content:encoded><![CDATA[
arXiv:2511.12851v1 Announce Type: new 
Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
arXiv:2511.12861v1 Announce Type: new 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classification of Hope in Textual Data using Transformer-Based Models</title>
<link>https://arxiv.org/abs/2511.12874</link>
<guid>https://arxiv.org/abs/2511.12874</guid>
<content:encoded><![CDATA[
arXiv:2511.12874v1 Announce Type: new 
Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy</title>
<link>https://arxiv.org/abs/2511.12920</link>
<guid>https://arxiv.org/abs/2511.12920</guid>
<content:encoded><![CDATA[
arXiv:2511.12920v1 Announce Type: new 
Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Room 2.0: Seeing is Not Understanding for MLLMs</title>
<link>https://arxiv.org/abs/2511.12928</link>
<guid>https://arxiv.org/abs/2511.12928</guid>
<content:encoded><![CDATA[
arXiv:2511.12928v1 Announce Type: new 
Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty</title>
<link>https://arxiv.org/abs/2511.12991</link>
<guid>https://arxiv.org/abs/2511.12991</guid>
<content:encoded><![CDATA[
arXiv:2511.12991v1 Announce Type: new 
Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.13029</link>
<guid>https://arxiv.org/abs/2511.13029</guid>
<content:encoded><![CDATA[
arXiv:2511.13029v1 Announce Type: new 
Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm</title>
<link>https://arxiv.org/abs/2511.13040</link>
<guid>https://arxiv.org/abs/2511.13040</guid>
<content:encoded><![CDATA[
arXiv:2511.13040v1 Announce Type: new 
Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training</title>
<link>https://arxiv.org/abs/2511.13043</link>
<guid>https://arxiv.org/abs/2511.13043</guid>
<content:encoded><![CDATA[
arXiv:2511.13043v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.13095</link>
<guid>https://arxiv.org/abs/2511.13095</guid>
<content:encoded><![CDATA[
arXiv:2511.13095v1 Announce Type: new 
Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study</title>
<link>https://arxiv.org/abs/2511.13107</link>
<guid>https://arxiv.org/abs/2511.13107</guid>
<content:encoded><![CDATA[
arXiv:2511.13107v1 Announce Type: new 
Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction</title>
<link>https://arxiv.org/abs/2511.13118</link>
<guid>https://arxiv.org/abs/2511.13118</guid>
<content:encoded><![CDATA[
arXiv:2511.13118v1 Announce Type: new 
Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.13126</link>
<guid>https://arxiv.org/abs/2511.13126</guid>
<content:encoded><![CDATA[
arXiv:2511.13126v1 Announce Type: new 
Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels</title>
<link>https://arxiv.org/abs/2511.13152</link>
<guid>https://arxiv.org/abs/2511.13152</guid>
<content:encoded><![CDATA[
arXiv:2511.13152v1 Announce Type: new 
Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis</title>
<link>https://arxiv.org/abs/2511.13159</link>
<guid>https://arxiv.org/abs/2511.13159</guid>
<content:encoded><![CDATA[
arXiv:2511.13159v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2511.13169</link>
<guid>https://arxiv.org/abs/2511.13169</guid>
<content:encoded><![CDATA[
arXiv:2511.13169v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translation Entropy: A Statistical Framework for Evaluating Translation Systems</title>
<link>https://arxiv.org/abs/2511.13180</link>
<guid>https://arxiv.org/abs/2511.13180</guid>
<content:encoded><![CDATA[
arXiv:2511.13180v1 Announce Type: new 
Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study</title>
<link>https://arxiv.org/abs/2511.13182</link>
<guid>https://arxiv.org/abs/2511.13182</guid>
<content:encoded><![CDATA[
arXiv:2511.13182v1 Announce Type: new 
Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms</title>
<link>https://arxiv.org/abs/2511.13225</link>
<guid>https://arxiv.org/abs/2511.13225</guid>
<content:encoded><![CDATA[
arXiv:2511.13225v1 Announce Type: new 
Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</title>
<link>https://arxiv.org/abs/2511.13254</link>
<guid>https://arxiv.org/abs/2511.13254</guid>
<content:encoded><![CDATA[
arXiv:2511.13254v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection</title>
<link>https://arxiv.org/abs/2511.13329</link>
<guid>https://arxiv.org/abs/2511.13329</guid>
<content:encoded><![CDATA[
arXiv:2511.13329v1 Announce Type: new 
Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects</title>
<link>https://arxiv.org/abs/2511.13335</link>
<guid>https://arxiv.org/abs/2511.13335</guid>
<content:encoded><![CDATA[
arXiv:2511.13335v1 Announce Type: new 
Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.13368</link>
<guid>https://arxiv.org/abs/2511.13368</guid>
<content:encoded><![CDATA[
arXiv:2511.13368v1 Announce Type: new 
Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts</title>
<link>https://arxiv.org/abs/2511.13381</link>
<guid>https://arxiv.org/abs/2511.13381</guid>
<content:encoded><![CDATA[
arXiv:2511.13381v1 Announce Type: new 
Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction</title>
<link>https://arxiv.org/abs/2511.13410</link>
<guid>https://arxiv.org/abs/2511.13410</guid>
<content:encoded><![CDATA[
arXiv:2511.13410v1 Announce Type: new 
Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Linear Scoring Model for Translation Quality Evaluation</title>
<link>https://arxiv.org/abs/2511.13467</link>
<guid>https://arxiv.org/abs/2511.13467</guid>
<content:encoded><![CDATA[
arXiv:2511.13467v1 Announce Type: new 
Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns</title>
<link>https://arxiv.org/abs/2511.13481</link>
<guid>https://arxiv.org/abs/2511.13481</guid>
<content:encoded><![CDATA[
arXiv:2511.13481v1 Announce Type: new 
Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Applying Large Language Models to Characterize Public Narratives</title>
<link>https://arxiv.org/abs/2511.13505</link>
<guid>https://arxiv.org/abs/2511.13505</guid>
<content:encoded><![CDATA[
arXiv:2511.13505v1 Announce Type: new 
Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets</title>
<link>https://arxiv.org/abs/2511.13529</link>
<guid>https://arxiv.org/abs/2511.13529</guid>
<content:encoded><![CDATA[
arXiv:2511.13529v1 Announce Type: new 
Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation</title>
<link>https://arxiv.org/abs/2511.13590</link>
<guid>https://arxiv.org/abs/2511.13590</guid>
<content:encoded><![CDATA[
arXiv:2511.13590v1 Announce Type: new 
Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents</title>
<link>https://arxiv.org/abs/2511.13593</link>
<guid>https://arxiv.org/abs/2511.13593</guid>
<content:encoded><![CDATA[
arXiv:2511.13593v1 Announce Type: new 
Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues</title>
<link>https://arxiv.org/abs/2511.13658</link>
<guid>https://arxiv.org/abs/2511.13658</guid>
<content:encoded><![CDATA[
arXiv:2511.13658v1 Announce Type: new 
Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation</title>
<link>https://arxiv.org/abs/2511.13689</link>
<guid>https://arxiv.org/abs/2511.13689</guid>
<content:encoded><![CDATA[
arXiv:2511.13689v1 Announce Type: new 
Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalist Foundation Models Are Not Clinical Enough for Hospital Operations</title>
<link>https://arxiv.org/abs/2511.13703</link>
<guid>https://arxiv.org/abs/2511.13703</guid>
<content:encoded><![CDATA[
arXiv:2511.13703v1 Announce Type: new 
Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Architecture, Scaling Laws, and Economics: A Quick Summary</title>
<link>https://arxiv.org/abs/2511.11572</link>
<guid>https://arxiv.org/abs/2511.11572</guid>
<content:encoded><![CDATA[
arXiv:2511.11572v1 Announce Type: cross 
Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupling Positional and Symbolic Attention Behavior in Transformers</title>
<link>https://arxiv.org/abs/2511.11579</link>
<guid>https://arxiv.org/abs/2511.11579</guid>
<content:encoded><![CDATA[
arXiv:2511.11579v1 Announce Type: cross 
Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Anatomy of a Triton Attention Kernel</title>
<link>https://arxiv.org/abs/2511.11581</link>
<guid>https://arxiv.org/abs/2511.11581</guid>
<content:encoded><![CDATA[
arXiv:2511.11581v1 Announce Type: cross 
Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism</title>
<link>https://arxiv.org/abs/2511.11591</link>
<guid>https://arxiv.org/abs/2511.11591</guid>
<content:encoded><![CDATA[
arXiv:2511.11591v1 Announce Type: cross 
Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLINB: A Climate Intelligence Benchmark for Foundational Models</title>
<link>https://arxiv.org/abs/2511.11597</link>
<guid>https://arxiv.org/abs/2511.11597</guid>
<content:encoded><![CDATA[
arXiv:2511.11597v1 Announce Type: cross 
Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio</title>
<link>https://arxiv.org/abs/2511.11599</link>
<guid>https://arxiv.org/abs/2511.11599</guid>
<content:encoded><![CDATA[
arXiv:2511.11599v1 Announce Type: cross 
Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models</title>
<link>https://arxiv.org/abs/2511.11622</link>
<guid>https://arxiv.org/abs/2511.11622</guid>
<content:encoded><![CDATA[
arXiv:2511.11622v1 Announce Type: cross 
Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges</title>
<link>https://arxiv.org/abs/2511.11624</link>
<guid>https://arxiv.org/abs/2511.11624</guid>
<content:encoded><![CDATA[
arXiv:2511.11624v1 Announce Type: cross 
Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation</title>
<link>https://arxiv.org/abs/2511.11635</link>
<guid>https://arxiv.org/abs/2511.11635</guid>
<content:encoded><![CDATA[
arXiv:2511.11635v1 Announce Type: cross 
Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic generation of DRI Statements</title>
<link>https://arxiv.org/abs/2511.11655</link>
<guid>https://arxiv.org/abs/2511.11655</guid>
<content:encoded><![CDATA[
arXiv:2511.11655v1 Announce Type: cross 
Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>H-Model: Dynamic Neural Architectures for Adaptive Processing</title>
<link>https://arxiv.org/abs/2511.11669</link>
<guid>https://arxiv.org/abs/2511.11669</guid>
<content:encoded><![CDATA[
arXiv:2511.11669v1 Announce Type: cross 
Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems</title>
<link>https://arxiv.org/abs/2511.11678</link>
<guid>https://arxiv.org/abs/2511.11678</guid>
<content:encoded><![CDATA[
arXiv:2511.11678v1 Announce Type: cross 
Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI as a Linguistic Equalizer in Global Science</title>
<link>https://arxiv.org/abs/2511.11687</link>
<guid>https://arxiv.org/abs/2511.11687</guid>
<content:encoded><![CDATA[
arXiv:2511.11687v1 Announce Type: cross 
Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning: From Reflection to Solution</title>
<link>https://arxiv.org/abs/2511.11712</link>
<guid>https://arxiv.org/abs/2511.11712</guid>
<content:encoded><![CDATA[
arXiv:2511.11712v1 Announce Type: cross 
Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy</title>
<link>https://arxiv.org/abs/2511.11816</link>
<guid>https://arxiv.org/abs/2511.11816</guid>
<content:encoded><![CDATA[
arXiv:2511.11816v1 Announce Type: cross 
Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better LLM Reasoning via Dual-Play</title>
<link>https://arxiv.org/abs/2511.11881</link>
<guid>https://arxiv.org/abs/2511.11881</guid>
<content:encoded><![CDATA[
arXiv:2511.11881v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v1 Announce Type: cross 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles</title>
<link>https://arxiv.org/abs/2511.12010</link>
<guid>https://arxiv.org/abs/2511.12010</guid>
<content:encoded><![CDATA[
arXiv:2511.12010v1 Announce Type: cross 
Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys</title>
<link>https://arxiv.org/abs/2511.12036</link>
<guid>https://arxiv.org/abs/2511.12036</guid>
<content:encoded><![CDATA[
arXiv:2511.12036v1 Announce Type: cross 
Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs</title>
<link>https://arxiv.org/abs/2511.12280</link>
<guid>https://arxiv.org/abs/2511.12280</guid>
<content:encoded><![CDATA[
arXiv:2511.12280v1 Announce Type: cross 
Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer</title>
<link>https://arxiv.org/abs/2511.12285</link>
<guid>https://arxiv.org/abs/2511.12285</guid>
<content:encoded><![CDATA[
arXiv:2511.12285v1 Announce Type: cross 
Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing</title>
<link>https://arxiv.org/abs/2511.12347</link>
<guid>https://arxiv.org/abs/2511.12347</guid>
<content:encoded><![CDATA[
arXiv:2511.12347v1 Announce Type: cross 
Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions</title>
<link>https://arxiv.org/abs/2511.12452</link>
<guid>https://arxiv.org/abs/2511.12452</guid>
<content:encoded><![CDATA[
arXiv:2511.12452v1 Announce Type: cross 
Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Layout: LLM-driven Co-optimization for Interior Layout</title>
<link>https://arxiv.org/abs/2511.12474</link>
<guid>https://arxiv.org/abs/2511.12474</guid>
<content:encoded><![CDATA[
arXiv:2511.12474v1 Announce Type: cross 
Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolving Prompts for Toxicity Search in Large Language Models</title>
<link>https://arxiv.org/abs/2511.12487</link>
<guid>https://arxiv.org/abs/2511.12487</guid>
<content:encoded><![CDATA[
arXiv:2511.12487v1 Announce Type: cross 
Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing</title>
<link>https://arxiv.org/abs/2511.12529</link>
<guid>https://arxiv.org/abs/2511.12529</guid>
<content:encoded><![CDATA[
arXiv:2511.12529v1 Announce Type: cross 
Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Content-Preserving Secure Linguistic Steganography</title>
<link>https://arxiv.org/abs/2511.12565</link>
<guid>https://arxiv.org/abs/2511.12565</guid>
<content:encoded><![CDATA[
arXiv:2511.12565v1 Announce Type: cross 
Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</title>
<link>https://arxiv.org/abs/2511.12997</link>
<guid>https://arxiv.org/abs/2511.12997</guid>
<content:encoded><![CDATA[
arXiv:2511.12997v1 Announce Type: cross 
Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics</title>
<link>https://arxiv.org/abs/2511.13021</link>
<guid>https://arxiv.org/abs/2511.13021</guid>
<content:encoded><![CDATA[
arXiv:2511.13021v1 Announce Type: cross 
Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization</title>
<link>https://arxiv.org/abs/2511.13091</link>
<guid>https://arxiv.org/abs/2511.13091</guid>
<content:encoded><![CDATA[
arXiv:2511.13091v1 Announce Type: cross 
Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms</title>
<link>https://arxiv.org/abs/2511.13238</link>
<guid>https://arxiv.org/abs/2511.13238</guid>
<content:encoded><![CDATA[
arXiv:2511.13238v1 Announce Type: cross 
Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment</title>
<link>https://arxiv.org/abs/2511.13290</link>
<guid>https://arxiv.org/abs/2511.13290</guid>
<content:encoded><![CDATA[
arXiv:2511.13290v1 Announce Type: cross 
Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research</title>
<link>https://arxiv.org/abs/2511.13333</link>
<guid>https://arxiv.org/abs/2511.13333</guid>
<content:encoded><![CDATA[
arXiv:2511.13333v1 Announce Type: cross 
Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Grounded Enhancement for Visual Document Retrieval</title>
<link>https://arxiv.org/abs/2511.13415</link>
<guid>https://arxiv.org/abs/2511.13415</guid>
<content:encoded><![CDATA[
arXiv:2511.13415v1 Announce Type: cross 
Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Multi-Table Retrieval Through Iterative Search</title>
<link>https://arxiv.org/abs/2511.13418</link>
<guid>https://arxiv.org/abs/2511.13418</guid>
<content:encoded><![CDATA[
arXiv:2511.13418v1 Announce Type: cross 
Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models</title>
<link>https://arxiv.org/abs/2511.13548</link>
<guid>https://arxiv.org/abs/2511.13548</guid>
<content:encoded><![CDATA[
arXiv:2511.13548v1 Announce Type: cross 
Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P1: Mastering Physics Olympiads with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.13612</link>
<guid>https://arxiv.org/abs/2511.13612</guid>
<content:encoded><![CDATA[
arXiv:2511.13612v1 Announce Type: cross 
Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</title>
<link>https://arxiv.org/abs/2511.13646</link>
<guid>https://arxiv.org/abs/2511.13646</guid>
<content:encoded><![CDATA[
arXiv:2511.13646v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G\"odel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Historical/temporal necessities/possibilities, and a logical theory of them in branching time</title>
<link>https://arxiv.org/abs/2208.11922</link>
<guid>https://arxiv.org/abs/2208.11922</guid>
<content:encoded><![CDATA[
arXiv:2208.11922v2 Announce Type: replace 
Abstract: In this paper, we do three kinds of work. First, we recognize four notions of necessity and two notions of possibility related to time flow, namely strong/weak historical/temporal necessities, as well as historical/temporal possibilities, which are motivated more from a linguistic perspective than from a philosophical one. Strong/weak historical necessities and historical possibility typically concern the possible futures of the present world, and strong/weak temporal necessities and temporal possibility concern possible timelines of alternatives of the present world. Second, we provide our approach to the six notions and present a logical theory of them in branching time. Our approach to the six notions is as follows. The agent has a system of ontic rules that determine expected timelines. She treats some ontic rules as undefeatable, determining accepted timelines. The domains of strong/weak historical necessities, respectively, consist of accepted and expected timelines passing through the present moment, and historical possibility is the dual of strong historical necessity. The domains of strong/weak temporal necessities, respectively, consist of accepted and expected timelines, and temporal possibility is the dual of strong temporal necessity. The logical theory has six operators: a last-moment operator, a next-moment operator, and four operators for the four notions of necessity. Formulas' evaluation contexts consist of a tree-like model representing a time flow, a context representing the agent's system of ontic rules, a timeline, and an instant. Third, we offer an axiomatic system for the logical theory and show its soundness and completeness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Machine Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2309.06706</link>
<guid>https://arxiv.org/abs/2309.06706</guid>
<content:encoded><![CDATA[
arXiv:2309.06706v3 Announce Type: replace 
Abstract: Real-world simultaneous machine translation (SimulMT) systems face more challenges than just the quality-latency trade-off. They also need to address issues related to robustness with noisy input, processing long contexts, and flexibility for knowledge injection. These challenges demand models with strong language understanding and generation capabilities which may not often equipped by dedicated MT models. In this paper, we investigate the possibility of applying Large Language Models (LLM) to SimulMT tasks by using existing incremental-decoding methods with a newly proposed RALCP algorithm for latency reduction. We conducted experiments using the \texttt{Llama2-7b-chat} model on nine different languages from the MUST-C dataset. The results show that LLM outperforms dedicated MT models in terms of BLEU and LAAL metrics. Further analysis indicates that LLM has advantages in terms of tuning efficiency and robustness. However, it is important to note that the computational cost of LLM remains a significant obstacle to its application in SimulMT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vashantor: A Large-scale Multilingual Benchmark Dataset for Automated Translation of Bangla Regional Dialects to Bangla Language</title>
<link>https://arxiv.org/abs/2311.11142</link>
<guid>https://arxiv.org/abs/2311.11142</guid>
<content:encoded><![CDATA[
arXiv:2311.11142v2 Announce Type: replace 
Abstract: The Bangla linguistic variety is a fascinating mix of regional dialects that contributes to the cultural diversity of the Bangla-speaking community. Despite extensive study into translating Bangla to English, English to Bangla, and Banglish to Bangla in the past, there has been a noticeable gap in translating Bangla regional dialects into standard Bangla. In this study, we set out to fill this gap by creating a collection of 32,500 sentences, encompassing Bangla, Banglish, and English, representing five regional Bangla dialects. Our aim is to translate these regional dialects into standard Bangla and detect regions accurately. To tackle the translation and region detection tasks, we propose two novel models: DialectBanglaT5 for translating regional dialects into standard Bangla and DialectBanglaBERT for identifying the dialect's region of origin. DialectBanglaT5 demonstrates superior performance across all dialects, achieving the highest BLEU score of 71.93, METEOR of 0.8503, and the lowest WER of 0.1470 and CER of 0.0791 on the Mymensingh dialect. It also achieves strong ROUGE scores across all dialects, indicating both accuracy and fluency in capturing dialectal nuances. In parallel, DialectBanglaBERT achieves an overall region classification accuracy of 89.02%, with notable F1-scores of 0.9241 for Chittagong and 0.8736 for Mymensingh, confirming its effectiveness in handling regional linguistic variation. This is the first large-scale investigation focused on Bangla regional dialect translation and region detection. Our proposed models highlight the potential of dialect-specific modeling and set a new benchmark for future research in low-resource and dialect-rich language settings.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational SimulMT: Efficient Simultaneous Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2402.10552</link>
<guid>https://arxiv.org/abs/2402.10552</guid>
<content:encoded><![CDATA[
arXiv:2402.10552v4 Announce Type: replace 
Abstract: Simultaneous machine translation (SimulMT) presents a challenging trade-off between translation quality and latency. Recent studies have shown that LLMs can achieve good performance in SimulMT tasks. However, this often comes at the expense of high inference cost and latency. In this paper, we propose a conversational SimulMT framework to enhance the inference efficiency of LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of LLM in translation quality while achieving comparable computational latency to specialized SimulMT models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DataGen: Unified Synthetic Dataset Generation via Large Language Models</title>
<link>https://arxiv.org/abs/2406.18966</link>
<guid>https://arxiv.org/abs/2406.18966</guid>
<content:encoded><![CDATA[
arXiv:2406.18966v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. Despite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProFuser: Progressive Fusion of Large Language Models</title>
<link>https://arxiv.org/abs/2408.04998</link>
<guid>https://arxiv.org/abs/2408.04998</guid>
<content:encoded><![CDATA[
arXiv:2408.04998v2 Announce Type: replace 
Abstract: While fusing the capacities and advantages of various large language models offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including Vicuna-7B-v1.5, Llama-2-7B-Chat, and MPT-7B-8K-Chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning</title>
<link>https://arxiv.org/abs/2408.14398</link>
<guid>https://arxiv.org/abs/2408.14398</guid>
<content:encoded><![CDATA[
arXiv:2408.14398v4 Announce Type: replace 
Abstract: Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. This analysis paper conducts an in-depth investigation of the performance and internal representation changes associated with pruning multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. We further analyze the latent subspaces, pruning masks, and individual neurons within pruned models. Our results reveal that while calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance. Further analysis of internal representations at three different levels highlights broader limitations of current pruning approaches: While they effectively preserve dominant information like language-specific features, this is insufficient to counteract the loss of nuanced, language-agnostic features that are crucial for knowledge retention and reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Breach: Assessing the Robustness of Transformer-based QA Models</title>
<link>https://arxiv.org/abs/2409.10997</link>
<guid>https://arxiv.org/abs/2409.10997</guid>
<content:encoded><![CDATA[
arXiv:2409.10997v4 Announce Type: replace 
Abstract: Contextual question-answering models are susceptible to adversarial perturbations to input context, commonly observed in real-world scenarios. These adversarial noises are designed to degrade the performance of the model by distorting the textual input. We introduce a unique dataset that incorporates seven distinct types of adversarial noise into the context, each applied at five different intensity levels on the SQuAD dataset. To quantify the robustness, we utilize robustness metrics providing a standardized measure for assessing model performance across varying noise types and levels. Experiments on transformer-based question-answering models reveal robustness vulnerabilities and important insights into the model's performance in realistic textual input.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is deeper always better? Replacing linear mappings with deep learning networks in the Discriminative Lexicon Model</title>
<link>https://arxiv.org/abs/2410.04259</link>
<guid>https://arxiv.org/abs/2410.04259</guid>
<content:encoded><![CDATA[
arXiv:2410.04259v2 Announce Type: replace 
Abstract: Recently, deep learning models have increasingly been used in cognitive modelling of language. This study asks whether deep learning can help us to better understand the learning problem that needs to be solved by speakers, above and beyond linear methods. We utilise the Discriminative Lexicon Model introduced by Baayen and colleagues, which models comprehension and production with mappings between numeric form and meaning vectors. While so far, these mappings have been linear (Linear Discriminative Learning, LDL), in the present study we replace them with deep dense neural networks (Deep Discriminative Learning, DDL). We find that DDL affords more accurate mappings for large and diverse datasets from English and Dutch, but not necessarily for Estonian and Taiwan Mandarin. DDL outperforms LDL in particular for words with pseudo-morphological structure such as chol+er. Applied to average reaction times, we find that DDL is outperformed by frequency-informed linear mappings (FIL). However, DDL trained in a frequency-informed way ('frequency-informed' deep learning, FIDDL) substantially outperforms FIL. Finally, while linear mappings can very effectively be updated from trial-to-trial to model incremental lexical learning, deep mappings cannot do so as effectively. At present, both linear and deep mappings are informative for understanding language.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering Factor Level Preferences to Improve Human-Model Alignment</title>
<link>https://arxiv.org/abs/2410.06965</link>
<guid>https://arxiv.org/abs/2410.06965</guid>
<content:encoded><![CDATA[
arXiv:2410.06965v3 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit tendencies that diverge from human preferences, such as favoring certain writing styles or producing overly verbose outputs. While crucial for improvement, identifying the factors driving these misalignments remains challenging due to existing evaluation methods' reliance on coarse-grained comparisons and lack of explainability. To address this, we introduce PROFILE, an automated framework to uncover and measure factor-level preference alignment of humans and LLMs. Using PROFILE, we analyze preference alignment across three key tasks: summarization, instruction-following, and document-based QA. We find a significant discrepancy: while LLMs show poor factor-level alignment with human preferences when generating texts, they demonstrate strong alignment in discrimination tasks. We demonstrate how leveraging the identified generation-discrimination gap can be used to improve LLM alignment through multiple approaches, including fine-tuning with self-guidance. Our work highlights the value of factor-level analysis for identifying hidden misalignments and provides a practical framework for improving LLM-human preference alignment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot</title>
<link>https://arxiv.org/abs/2411.00034</link>
<guid>https://arxiv.org/abs/2411.00034</guid>
<content:encoded><![CDATA[
arXiv:2411.00034v2 Announce Type: replace 
Abstract: Companies support their customers using live chats and chatbots to gain their loyalty. AFAS is a Dutch company aiming to leverage the opportunity large language models (LLMs) offer to answer customer queries with minimal to no input from its customer support team. Adding to its complexity, it is unclear what makes a response correct, and that too in Dutch. Further, with minimal data available for training, the challenge is to identify whether an answer generated by a large language model is correct and do it on the fly.
  This study is the first to define the correctness of a response based on how the support team at AFAS makes decisions. It leverages literature on natural language generation and automated answer grading systems to automate the decision-making of the customer support team. We investigated questions requiring a binary response (e.g., Would it be possible to adjust tax rates manually?) or instructions (e.g., How would I adjust tax rate manually?) to test how close our automated approach reaches support rating. Our approach can identify wrong messages in 55\% of the cases. This work demonstrates the potential for automatically assessing when our chatbot may provide incorrect or misleading answers. Specifically, we contribute (1) a definition and metrics for assessing correctness, and (2) suggestions to improve correctness with respect to regional language and question type.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Topological Structures from Language: A Survey of Topological Data Analysis Applications in NLP</title>
<link>https://arxiv.org/abs/2411.10298</link>
<guid>https://arxiv.org/abs/2411.10298</guid>
<content:encoded><![CDATA[
arXiv:2411.10298v4 Announce Type: replace 
Abstract: The surge of data available on the Internet has led to the adoption of various computational methods to analyze and extract valuable insights from this wealth of information. Among these, the field of Machine Learning (ML) has thrived by leveraging data to extract meaningful insights. However, ML techniques face notable challenges when dealing with real-world data, often due to issues of imbalance, noise, insufficient labeling, and high dimensionality. To address these limitations, some researchers advocate for the adoption of Topological Data Analysis (TDA), a statistical approach that discerningly captures the intrinsic shape of data despite noise. Despite its potential, TDA has not gained as much traction within the Natural Language Processing (NLP) domain compared to structurally distinct areas like computer vision. Nevertheless, a dedicated community of researchers has been exploring the application of TDA in NLP, yielding 100 papers we comprehensively survey in this paper. Our findings categorize these efforts into theoretical and non-theoretical approaches. Theoretical approaches aim to explain linguistic phenomena from a topological viewpoint, while non-theoretical approaches merge TDA with ML features, utilizing diverse numerical representation techniques. We conclude by exploring the challenges and unresolved questions that persist in this niche field. Resources and a list of papers on this topic can be found at: https://github.com/AdaUchendu/AwesomeTDA4NLP.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v3 Announce Type: replace 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script</title>
<link>https://arxiv.org/abs/2412.12478</link>
<guid>https://arxiv.org/abs/2412.12478</guid>
<content:encoded><![CDATA[
arXiv:2412.12478v5 Announce Type: replace 
Abstract: DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathRAG: Pruning Graph-based Retrieval Augmented Generation with Relational Paths</title>
<link>https://arxiv.org/abs/2502.14902</link>
<guid>https://arxiv.org/abs/2502.14902</guid>
<content:encoded><![CDATA[
arXiv:2502.14902v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) improves the response quality of large language models (LLMs) by retrieving knowledge from external databases. Typical RAG approaches split the text database into chunks, organizing them in a flat structure for efficient searches. To better capture the inherent dependencies and structured relationships across the text database, researchers propose to organize textual information into an indexing graph, known asgraph-based RAG. However, we argue that the limitation of current graph-based RAG methods lies in the redundancy of the retrieved information, rather than its insufficiency. Moreover, previous methods use a flat structure to organize retrieved information within the prompts, leading to suboptimal performance. To overcome these limitations, we propose PathRAG, which retrieves key relational paths from the indexing graph, and converts these paths into textual form for prompting LLMs. Specifically, PathRAG effectively reduces redundant information with flow-based pruning, while guiding LLMs to generate more logical and coherent responses with path-based prompting. Experimental results show that PathRAG consistently outperforms state-of-the-art baselines across six datasets and five evaluation dimensions. The code is available at the following link: https://github.com/BUPT-GAMMA/PathRAG
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Extraction and Generation for Robust Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2503.04789</link>
<guid>https://arxiv.org/abs/2503.04789</guid>
<content:encoded><![CDATA[
arXiv:2503.04789v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) enhances LLMs with external knowledge, yet generation remains vulnerable to retrieval-induced noise and uncertain placement of relevant chunks, often causing hallucinations. We present Ext2Gen, an extract-then-generate framework that strengthens LLMs via joint evidence selection and answer generation, dynamically identifying query-relevant content while suppressing noise, thereby removing the need for any independent pre-generation compression module. Optimized through preference alignment with well-curated pairwise feedback, Ext2Gen produces accurate and faithful answers even under noisy or imprecise retrieval. Experiments demonstrate that it substantially enhances the robustness of the generation backbone and yields greater performance gains than methods relying on independent compression models, e.g., Recomp, CompAct, EXIT). It further benefits from improved retrieval techniques such as query rewriting, underscoring that generation-side enhancements address limitations that retrieval alone cannot overcome.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Conditional Emergence of Multilingual Image Captioning via Generalization from Translation</title>
<link>https://arxiv.org/abs/2503.09443</link>
<guid>https://arxiv.org/abs/2503.09443</guid>
<content:encoded><![CDATA[
arXiv:2503.09443v2 Announce Type: replace 
Abstract: Cross-lingual, cross-task transfer is challenged by task-specific data scarcity, which becomes more severe as language support grows and is further amplified in vision-language models (VLMs). We investigate multilingual generalization in encoder-decoder transformer VLMs to enable zero-shot image captioning in languages encountered only in the translation task. In this setting, the encoder must learn to generate generalizable, task-aware latent vision representations to instruct the decoder via inserted cross-attention layers. To analyze scaling behavior, we train Florence-2 based and Gemma-2 based models (0.4B to 11.2B parameters) on a synthetic dataset using varying compute budgets. While all languages in the dataset have image-aligned translations, only a subset of them include image captions. Notably, we show that captioning can emerge using a language prefix, even when this language only appears in the translation task. We find that indirect learning of unseen task-language pairs adheres to scaling laws that are governed by the multilinguality of the model, model size, and seen training samples. Finally, we demonstrate that the scaling laws extend to downstream tasks, achieving competitive performance through fine-tuning in multimodal machine translation (Multi30K, CoMMuTE), lexical disambiguation (CoMMuTE), and image captioning (Multi30K, XM3600, COCO Karpathy).
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explain with Visual Keypoints Like a Real Mentor! A Benchmark for Multimodal Solution Explanation</title>
<link>https://arxiv.org/abs/2504.03197</link>
<guid>https://arxiv.org/abs/2504.03197</guid>
<content:encoded><![CDATA[
arXiv:2504.03197v4 Announce Type: replace 
Abstract: With the rapid advancement of mathematical reasoning capabilities in Large Language Models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: multimodal explanation. In real-world instructional contexts, human tutors routinely employ visual aids, such as diagrams, markings, and highlights, to enhance conceptual clarity. To bridge this gap, we introduce the multimodal solution explanation task, designed to evaluate whether models can identify visual keypoints, such as auxiliary lines, points, angles, and generate explanations that incorporate these key elements essential for understanding. To evaluate model performance on this task, we propose ME2, a multimodal benchmark consisting of 1,000 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that current models struggle to identify visual keypoints. In the task of generating keypoint-based explanations, open-source models also face notable difficulties. This highlights a significant gap in current LLMs' ability to perform mathematical visual grounding, engage in visually grounded reasoning, and provide explanations in educational contexts. We expect that the multimodal solution explanation task and the ME2 dataset will catalyze further research on LLMs in education and promote their use as effective, explanation-oriented AI tutors.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Robust Response Generation in the Wild</title>
<link>https://arxiv.org/abs/2504.12982</link>
<guid>https://arxiv.org/abs/2504.12982</guid>
<content:encoded><![CDATA[
arXiv:2504.12982v2 Announce Type: replace 
Abstract: The proliferation of large language models (LLMs) has significantly advanced intelligent systems. Unfortunately, LLMs often face knowledge conflicts between internal memory and retrieved external information, arising from misinformation, biases, or outdated knowledge. These conflicts undermine response reliability and introduce uncertainty in decision-making. In this work, we analyze how LLMs navigate knowledge conflicts from an information-theoretic perspective and reveal that when conflicting and supplementary information exhibit significant differences, LLMs confidently resolve their preferences and alleviate the uncertainty during their response generation. When this difference is ambiguous, LLMs experience considerable uncertainty about their generation. Based on this insight, we propose Swin-VIB, a novel framework that integrates a pipeline of variational information bottleneck models to adapt the retrieved information difference, facilitating robust response generation of LLMs even in conflicting contexts. Extensive experiments confirm our theoretical analysis and demonstrate the performance of Swin-VIB. Notably, Swin-VIB outperforms all competitive baselines in terms of the accuracy of the multiple-choice task, while improving the EM values in the open-ended QA task by at least 11.14%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights</title>
<link>https://arxiv.org/abs/2505.04649</link>
<guid>https://arxiv.org/abs/2505.04649</guid>
<content:encoded><![CDATA[
arXiv:2505.04649v3 Announce Type: replace 
Abstract: The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME's effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v3 Announce Type: replace 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains POS-tagged parallel text data from more than 1,940 languages, representing 155 language families and 78 isolates, dwarfing previously available resources. The accuracy of particular tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of intransitive word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic intransitive word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</title>
<link>https://arxiv.org/abs/2505.14099</link>
<guid>https://arxiv.org/abs/2505.14099</guid>
<content:encoded><![CDATA[
arXiv:2505.14099v2 Announce Type: replace 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation</title>
<link>https://arxiv.org/abs/2505.15249</link>
<guid>https://arxiv.org/abs/2505.15249</guid>
<content:encoded><![CDATA[
arXiv:2505.15249v2 Announce Type: replace 
Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred tools for judging text-image alignment, yet their robustness along the visual modality remains underexplored. This work is the first study to address a key research question: Can adversarial visual manipulations systematically fool LVLM judges into assigning unfairly inflated scores? We define potential image induced biases within the context of T2I evaluation and examine how these biases affect the evaluations of LVLM judges. Moreover, we introduce a novel, fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is deliberately constructed to exhibit diverse score distributions. By introducing the defined biases into the benchmark, we reveal that all tested LVLM judges exhibit vulnerability across all domains, consistently inflating scores for manipulated images. Further analysis reveals that combining multiple biases amplifies their effects, and pairwise evaluations are similarly susceptible. Moreover, we observe that visual biases persist under prompt-based mitigation strategies, highlighting the vulnerability of current LVLM evaluation systems and underscoring the urgent need for more robust LVLM judges.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model</title>
<link>https://arxiv.org/abs/2505.16000</link>
<guid>https://arxiv.org/abs/2505.16000</guid>
<content:encoded><![CDATA[
arXiv:2505.16000v5 Announce Type: replace 
Abstract: The rapid advancement of language models has demonstrated the potential of artificial intelligence in the healthcare industry. However, small language models struggle with specialized domains in low-resource languages like Persian. While numerous medical-domain websites exist in Persian, no curated dataset or corpus has been available making ours the first of its kind. This study introduces a newly curated dataset comprising 20k doctor-patient Q\&amp;A pairs and 60\% of a 90-million-token crawled corpus from medical magazines. Using a parameter-efficient fine-tuning approach, we enhanced the medical knowledge of the baseline model, aya-expanse-8b. Benchmark evaluations demonstrate that the fine-tuned model achieves improved accuracy in medical question answering and successfully passed the Iranian Basic Medical Science Entrance Exam (IBSEE) in September 2023, which the baseline model did not. Additionally, the fine-tuned model improved Persian-translated MMLU accuracy by an average of 2.67\%. This work highlights the potential of leveraging open-access online data to enrich small language models in medical fields, providing a novel solution for Persian medical AI applications suitable for resource-constrained environments. Future research could explore multimodal input to further enhance performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Thought Driven Adversarial Scenario Extrapolation for Robust Language Models</title>
<link>https://arxiv.org/abs/2505.17089</link>
<guid>https://arxiv.org/abs/2505.17089</guid>
<content:encoded><![CDATA[
arXiv:2505.17089v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or resort to rigid outright rejection, sacrificing user experience and failing to generalize across diverse and novel attacks. This paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that leverages Chain-of-Thought (CoT) reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides the LLM through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before generating a response to the user query. Comprehensive evaluation on four adversarial benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak attack success rates and minimal toxicity, while slashing outright rejections to <4%. ASE outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&amp;A and 4-10x lower bias scores. By transforming adversarial perception into an intrinsic cognitive process, ASE sets a new paradigm for secure and natural human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCRum-9: Multilingual Stance Classification over Rumours on Social Media</title>
<link>https://arxiv.org/abs/2505.18916</link>
<guid>https://arxiv.org/abs/2505.18916</guid>
<content:encoded><![CDATA[
arXiv:2505.18916v3 Announce Type: replace 
Abstract: We introduce SCRum-9, the largest multilingual Stance Classification dataset for Rumour analysis in 9 languages, containing 7,516 tweets from X. SCRum-9 goes beyond existing stance classification datasets by covering more languages, linking examples to more fact-checked claims (2.1k), and including confidence-related annotations from multiple annotators to account for intra- and inter-annotator variability. Annotations were made by at least two native speakers per language, totalling more than 405 hours of annotation and 8,150 dollars in compensation. Further, SCRum-9 is used to benchmark five large language models (LLMs) and two multilingual masked language models (MLMs) in In-Context Learning (ICL) and fine-tuning setups. This paper also innovates by exploring the use of multilingual synthetic data for rumour stance classification, showing that even LLMs with weak ICL performance can produce valuable synthetic data for fine-tuning small MLMs, enabling them to achieve higher performance than zero-shot ICL in LLMs. Finally, we examine the relationship between model predictions and human uncertainty on ambiguous cases finding that model predictions often match the second-choice labels assigned by annotators, rather than diverging entirely from human judgments. SCRum-9 is publicly released to the research community with potential to foster further research on multilingual analysis of misleading narratives on social media.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2505.19768</link>
<guid>https://arxiv.org/abs/2505.19768</guid>
<content:encoded><![CDATA[
arXiv:2505.19768v2 Announce Type: replace 
Abstract: Real-world multimodal misinformation often arises from mixed forgery sources, requiring dynamic reasoning and adaptive verification. However, existing methods mainly rely on static pipelines and limited tool usage, limiting their ability to handle such complexity and diversity. To address this challenge, we propose \method, a novel misinformation detection agent that incorporates an extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of modular tools such as web search, forgery detection, and consistency analysis. Each tool is described using standardized templates, enabling seamless integration and future expansion. To avoid inefficiency from using all tools simultaneously, a greedy search-based selector is proposed to identify a task-relevant subset. This subset then serves as the action space for MCTS to dynamically collect evidence and perform multi-source verification. To better align MCTS with the multi-source nature of misinformation detection, \method~ extends traditional MCTS with multi-source verification, which decomposes the task into coordinated subtasks targeting different forgery sources. A dual reward mechanism containing a reasoning trajectory score and a confidence score is further proposed to encourage a balance between exploration across mixed forgery sources and exploitation for more reliable evidence. We conduct ablation studies to confirm the effectiveness of the tree search mechanism and tool usage. Extensive experiments further show that \method~ consistently outperforms existing baselines on challenging mixed-source multimodal misinformation benchmarks, demonstrating its strong potential as a training-free detector.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query</title>
<link>https://arxiv.org/abs/2505.20334</link>
<guid>https://arxiv.org/abs/2505.20334</guid>
<content:encoded><![CDATA[
arXiv:2505.20334v2 Announce Type: replace 
Abstract: Large language models (LLMs) rely on key-value cache (KV cache) to accelerate decoding by reducing redundant computations. However, the KV cache memory usage grows substantially with longer text sequences, posing challenges for efficient deployment. Existing KV cache eviction methods prune tokens using prefilling-stage attention scores, causing inconsistency with actual inference queries, especially under tight memory budgets. In this paper, we propose Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries. By using these lookahead queries as the observation window for importance estimation, LAQ achieves more consistent and accurate KV cache eviction aligned with real inference scenarios. Experimental results on LongBench and Needle-in-a-Haystack benchmarks show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\sim$ 4 point improvement on LongBench under limited cache budget. Moreover, LAQ is complementary to existing approaches and can be flexibly combined to yield further improvements.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REIC: RAG-Enhanced Intent Classification at Scale</title>
<link>https://arxiv.org/abs/2506.00210</link>
<guid>https://arxiv.org/abs/2506.00210</guid>
<content:encoded><![CDATA[
arXiv:2506.00210v2 Announce Type: replace 
Abstract: Accurate intent classification is critical for efficient routing in customer service, ensuring customers are connected with the most suitable agents while reducing handling times and operational costs. However, as companies expand their product lines, intent classification faces scalability challenges due to the increasing number of intents and variations in taxonomy across different verticals. In this paper, we introduce REIC, a Retrieval-augmented generation Enhanced Intent Classification approach, which addresses these challenges effectively. REIC leverages retrieval-augmented generation (RAG) to dynamically incorporate relevant knowledge, enabling precise classification without the need for frequent retraining. Through extensive experiments on real-world datasets, we demonstrate that REIC outperforms traditional fine-tuning, zero-shot, and few-shot methods in large-scale customer service settings. Our results highlight its effectiveness in both in-domain and out-of-domain scenarios, demonstrating its potential for real-world deployment in adaptive and large-scale intent classification systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers</title>
<link>https://arxiv.org/abs/2506.01215</link>
<guid>https://arxiv.org/abs/2506.01215</guid>
<content:encoded><![CDATA[
arXiv:2506.01215v2 Announce Type: replace 
Abstract: As large language models increasingly gain popularity in real-world applications, processing extremely long contexts, often exceeding the model's pre-trained context limits, has emerged as a critical challenge. While existing approaches to efficient long-context processing show promise, recurrent compression-based methods struggle with information preservation, whereas random access approaches require substantial memory resources. We introduce REFORM, a novel inference framework that efficiently handles long contexts through a two-phase approach. First, it incrementally processes input chunks while maintaining a compressed KV cache, constructs cross-layer context embeddings, and utilizes early exit strategy for improved efficiency. Second, it identifies and gathers essential tokens via similarity matching and selectively recomputes the KV cache. Compared to baselines, REFORM achieves over 52% and 34% performance gains on RULER and BABILong respectively at 1M context length. It also outperforms baselines on Infinite-Bench, RepoEval, and MM-NIAH, demonstrating flexibility across diverse tasks and domains. Additionally, REFORM reduces inference time by 30% and peak memory usage by 5%, achieving both efficiency and superior performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework</title>
<link>https://arxiv.org/abs/2506.02454</link>
<guid>https://arxiv.org/abs/2506.02454</guid>
<content:encoded><![CDATA[
arXiv:2506.02454v2 Announce Type: replace 
Abstract: Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\% overall win rate over the baseline method.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerated Test-Time Scaling with Model-Free Speculative Sampling</title>
<link>https://arxiv.org/abs/2506.04708</link>
<guid>https://arxiv.org/abs/2506.04708</guid>
<content:encoded><![CDATA[
arXiv:2506.04708v2 Announce Type: replace 
Abstract: Language models have demonstrated remarkable capabilities in reasoning tasks through test-time scaling techniques like best-of-N sampling and tree search. However, these approaches often demand substantial computational resources, creating a critical trade-off between performance and efficiency. We introduce STAND (STochastic Adaptive N-gram Drafting), a novel model-free speculative decoding approach that exploits the inherent redundancy in reasoning trajectories to achieve significant acceleration without compromising accuracy. Our analysis shows that reasoning paths frequently reuse similar reasoning patterns, enabling efficient model-free token prediction without requiring separate draft models. By introducing stochastic drafting and preserving probabilistic information through a memory-efficient logit-based N-gram module, combined with optimized Gumbel-Top-K sampling and data-driven tree construction, STAND significantly improves token acceptance rates. Extensive evaluations across multiple models and reasoning tasks (AIME-2024, GPQA-Diamond, and LiveCodeBench) demonstrate that STAND reduces inference latency by 60-65% compared to standard autoregressive decoding while maintaining accuracy. Furthermore, STAND consistently outperforms state-of-the-art speculative decoding methods across diverse inference patterns, including single-trajectory decoding, batch decoding, and test-time tree search. As a model-free approach, STAND can be applied to any existing language model without additional training, making it a powerful plug-and-play solution for accelerating language model reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2506.04832</link>
<guid>https://arxiv.org/abs/2506.04832</guid>
<content:encoded><![CDATA[
arXiv:2506.04832v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) extend large language models with explicit, multi-step reasoning traces to enhance transparency and performance on complex tasks. However, these reasoning traces can be redundant or logically inconsistent, becoming a new and hard-to-detect source of hallucination. Existing hallucination detection methods focus primarily on answer-level uncertainty and often fail to detect hallucinations or logical inconsistencies arising from the model's reasoning trace. This oversight is particularly problematic for LRMs, where the explicit thinking trace is not only an important support to the model's decision-making process but also a key source of potential hallucination. To this end, we propose RACE (Reasoning and Answer Consistency Evaluation), a novel framework specifically tailored for hallucination detection in LRMs. RACE operates by extracting essential reasoning steps and computing four diagnostic signals: inter-sample consistency of reasoning traces, entropy-based answer uncertainty, semantic alignment between reasoning and answers, and internal coherence of reasoning. The joint utilization of these signals makes RACE a more robust detector of hallucinations in LRMs. Experiments across datasets and different LLMs demonstrate that RACE outperforms existing hallucination detection baselines, offering a robust and generalizable solution for evaluating LRMs. The source code is available at https://github.com/bebr2/RACE
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning</title>
<link>https://arxiv.org/abs/2506.05813</link>
<guid>https://arxiv.org/abs/2506.05813</guid>
<content:encoded><![CDATA[
arXiv:2506.05813v2 Announce Type: replace 
Abstract: Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2506.08552</link>
<guid>https://arxiv.org/abs/2506.08552</guid>
<content:encoded><![CDATA[
arXiv:2506.08552v2 Announce Type: replace 
Abstract: Reasoning is a key component of language understanding in Large Language Models. While Chain-of-Thought prompting enhances performance via explicit intermediate steps, it suffers from sufficient token overhead and a fixed reasoning trajectory, preventing step-wise refinement. Recent advances in latent reasoning address these limitations by refining internal reasoning processes directly in the model's latent space, without producing explicit outputs. However, a key challenge remains: how to effectively update reasoning embeddings during post-training to guide the model toward more accurate solutions. To overcome this challenge, we propose a lightweight post-training framework that refines latent reasoning trajectories using two novel strategies: 1) Contrastive reasoning feedback, which compares reasoning embeddings against strong and weak baselines to infer effective update directions via embedding enhancement; 2) Residual embedding refinement, which stabilizes updates by progressively integrating current and historical gradients, enabling fast yet controlled convergence. Extensive experiments and case studies are conducted on five reasoning benchmarks to demonstrate the effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA without additional training.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ToxSyn: Reducing Bias in Hate Speech Detection via Synthetic Minority Data in Brazilian Portuguese</title>
<link>https://arxiv.org/abs/2506.10245</link>
<guid>https://arxiv.org/abs/2506.10245</guid>
<content:encoded><![CDATA[
arXiv:2506.10245v2 Announce Type: replace 
Abstract: The development of robust hate speech detection systems remains limited by the lack of large-scale, fine-grained training data, especially for languages beyond English. Existing corpora typically rely on coarse toxic/non-toxic labels, and the few that capture hate directed at specific minority groups critically lack the non-toxic counterexamples (i.e., benign text about minorities) required to distinguish genuine hate from mere discussion. We introduce ToxSyn, the first Portuguese large-scale corpus explicitly designed for multi-label hate speech detection across nine protected minority groups. Generated via a controllable four-stage pipeline, ToxSyn includes discourse-type annotations to capture rhetorical strategies of toxic language, such as sarcasm or dehumanization. Crucially, it systematically includes the non-toxic counterexamples absent in all other public datasets. Our experiments reveal a catastrophic, mutual generalization failure between social-media domains and ToxSyn: models trained on social media struggle to generalize to minority-specific contexts, and vice-versa. This finding indicates they are distinct tasks and exposes summary metrics like Macro F1 can be unreliable indicators of true model behavior, as they completely mask model failure. We publicly release ToxSyn at HuggingFace to foster reproducible research on synthetic data generation and benchmark progress in hate-speech detection for low- and mid-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Pay Attention</title>
<link>https://arxiv.org/abs/2506.11305</link>
<guid>https://arxiv.org/abs/2506.11305</guid>
<content:encoded><![CDATA[
arXiv:2506.11305v2 Announce Type: replace 
Abstract: The Transformer has become the de facto standard for modern language models owing to its parallelizable training and effective autoregressive decoding. However, its fixed context window and the quadratic time and memory costs of its self-attention mechanism remain central bottlenecks. These constraints have revived interest in recurrent architectures that scale linearly with sequence length, but at the cost of reduced parallelism. In this paper, we introduce Avey, a new foundational architecture that breaks away from both attention and recurrence. Avey pairs a ranker with an autoregressive neural processor to select and contextualize only the most relevant tokens for any given token. Specifically, it decouples sequence length from context width, thus enabling effective and efficient processing of arbitrarily long sequences. Results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while significantly outperforming it on tasks requiring long-range dependency modeling.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models</title>
<link>https://arxiv.org/abs/2506.15545</link>
<guid>https://arxiv.org/abs/2506.15545</guid>
<content:encoded><![CDATA[
arXiv:2506.15545v2 Announce Type: replace 
Abstract: Local-global attention models have recently emerged as compelling alternatives to standard Transformers, promising improvements in both training and inference efficiency. However, the crucial choice of window size presents a Pareto tradeoff: larger windows maintain performance akin to full attention but offer minimal efficiency gains in short-context scenarios, while smaller windows can lead to performance degradation. Current models, such as Gemma2 and Mistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining length) to preserve performance. This work investigates strategies to shift this Pareto frontier, enabling local-global models to achieve efficiency gains even in short-context regimes. Our core motivation is to address the intrinsic limitation of local attention -- its complete disregard for tokens outside the defined window. We explore RATTENTION, a variant of local attention integrated with a specialized linear attention mechanism designed to capture information from these out-of-window tokens. Pretraining experiments at the 3B and 12B scales demonstrate that RATTENTION achieves a superior Pareto tradeoff between performance and efficiency. As a sweet spot, RATTENTION with a window size of just 512 consistently matches the performance of full-attention models across diverse settings. Furthermore, the recurrent nature inherent in the linear attention component of RATTENTION contributes to enhanced long-context performance, as validated on the RULER benchmark. Crucially, these improvements do not compromise training efficiency; thanks to a specialized kernel implementation and the reduced window size, RATTENTION maintains training speeds comparable to existing state-of-the-art approaches. We open-sourced our Pallas kernels along with model codes to facilitate further research effort.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Organizing Language</title>
<link>https://arxiv.org/abs/2506.23293</link>
<guid>https://arxiv.org/abs/2506.23293</guid>
<content:encoded><![CDATA[
arXiv:2506.23293v2 Announce Type: replace 
Abstract: We introduce a novel paradigm of emergent local memory. It is a continuous-learning completely-parallel content-addressable memory encoding global order. It demonstrates how local constraints on uncoordinated learning can produce topologically protected memories realizing emergent symbolic order. It is therefore a neuro-symbolic bridge.
  It further has the ability to produce human language without data, by exploiting its own self-organizing dynamics. It teaches us that words arise as a side-effect of emergent symbolic order, and that human language patterns at all structural levels reflect a universal mechanism of word formation (which is subregular). This work answers essential questions about the existence \& origin of all the human language data.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
<link>https://arxiv.org/abs/2507.02962</link>
<guid>https://arxiv.org/abs/2507.02962</guid>
<content:encoded><![CDATA[
arXiv:2507.02962v5 Announce Type: replace 
Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
arXiv:2507.22564v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
arXiv:2507.22581v3 Announce Type: replace 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02360</link>
<guid>https://arxiv.org/abs/2508.02360</guid>
<content:encoded><![CDATA[
arXiv:2508.02360v2 Announce Type: replace 
Abstract: Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty</title>
<link>https://arxiv.org/abs/2508.03294</link>
<guid>https://arxiv.org/abs/2508.03294</guid>
<content:encoded><![CDATA[
arXiv:2508.03294v2 Announce Type: replace 
Abstract: Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression</title>
<link>https://arxiv.org/abs/2508.05337</link>
<guid>https://arxiv.org/abs/2508.05337</guid>
<content:encoded><![CDATA[
arXiv:2508.05337v2 Announce Type: replace 
Abstract: Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., "Wait" and "Alternatively") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures</title>
<link>https://arxiv.org/abs/2508.06105</link>
<guid>https://arxiv.org/abs/2508.06105</guid>
<content:encoded><![CDATA[
arXiv:2508.06105v2 Announce Type: replace 
Abstract: Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a $\textbf{Logic}$-aware $\textbf{R}etrieval$-$\textbf{A}$ugmented $\textbf{G}$eneration framework ($\textbf{LogicRAG}$) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneJailEval: A Scenario-Adaptive Multi-Dimensional Framework for Jailbreak Evaluation</title>
<link>https://arxiv.org/abs/2508.06194</link>
<guid>https://arxiv.org/abs/2508.06194</guid>
<content:encoded><![CDATA[
arXiv:2508.06194v2 Announce Type: replace 
Abstract: Accurate jailbreak evaluation is critical for LLM red team testing and jailbreak research. Mainstream methods rely on binary classification (string matching, toxic text classifiers, and LLM-based methods), outputting only "yes/no" labels without quantifying harm severity. Emerged multi-dimensional frameworks (e.g., Security Violation, Relative Truthfulness and Informativeness) use unified evaluation standards across scenarios, leading to scenario-specific mismatches (e.g., "Relative Truthfulness" is irrelevant to "hate speech"), undermining evaluation accuracy. To address these, we propose SceneJailEval, with key contributions: (1) A pioneering scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" limitation of existing multi-dimensional methods, and boasting robust extensibility to seamlessly adapt to customized or emerging scenarios. (2) A novel 14-scenario dataset featuring rich jailbreak variants and regional cases, addressing the long-standing gap in high-quality, comprehensive benchmarks for scenario-adaptive evaluation. (3) SceneJailEval delivers state-of-the-art performance with an F1 score of 0.917 on our full-scenario dataset (+6% over SOTA) and 0.995 on JBB (+3% over SOTA), breaking through the accuracy bottleneck of existing evaluation methods in heterogeneous scenarios and solidifying its superiority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
<link>https://arxiv.org/abs/2508.13953</link>
<guid>https://arxiv.org/abs/2508.13953</guid>
<content:encoded><![CDATA[
arXiv:2508.13953v2 Announce Type: replace 
Abstract: In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
arXiv:2508.14031v2 Announce Type: replace 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries</title>
<link>https://arxiv.org/abs/2508.18212</link>
<guid>https://arxiv.org/abs/2508.18212</guid>
<content:encoded><![CDATA[
arXiv:2508.18212v2 Announce Type: replace 
Abstract: The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the model's comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
<link>https://arxiv.org/abs/2509.02492</link>
<guid>https://arxiv.org/abs/2509.02492</guid>
<content:encoded><![CDATA[
arXiv:2509.02492v3 Announce Type: replace 
Abstract: Significant progress in reward modeling over recent years has been driven by a paradigm shift from task-specific designs towards generalist reward models. Despite this trend, developing effective reward models remains a fundamental challenge: the heavy reliance on large-scale labeled preference data. Pre-training on abundant unlabeled data offers a promising direction, but existing approaches fall short of instilling explicit reasoning into reward models. To bridge this gap, we propose a self-training approach that leverages unlabeled data to elicit reward reasoning in reward models. Based on this approach, we develop GRAM-R$^2$, a generative reward model trained to produce not only preference labels but also accompanying reward rationales. GRAM-R$^2$ can serve as a foundation model for reward reasoning and can be applied to a wide range of tasks with minimal or no additional fine-tuning. It can support downstream applications such as response ranking and task-specific reward tuning. Experiments on response ranking, task adaptation, and reinforcement learning from human feedback demonstrate that GRAM-R$^2$ consistently delivers strong performance, outperforming several strong discriminative and generative baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts</title>
<link>https://arxiv.org/abs/2509.12440</link>
<guid>https://arxiv.org/abs/2509.12440</guid>
<content:encoded><![CDATA[
arXiv:2509.12440v2 Announce Type: replace 
Abstract: Deploying Large Language Models (LLMs) in medical applications requires fact-checking capabilities to ensure patient safety and regulatory compliance. We introduce MedFact, a challenging Chinese medical fact-checking benchmark with 2,116 expert-annotated instances from diverse real-world texts, spanning 13 specialties, 8 error types, 4 writing styles, and 5 difficulty levels. Construction uses a hybrid AI-human framework where iterative expert feedback refines AI-driven, multi-criteria filtering to ensure high quality and difficulty. We evaluate 20 leading LLMs on veracity classification and error localization, and results show models often determine if text contains errors but struggle to localize them precisely, with top performers falling short of human performance. Our analysis reveals the "over-criticism" phenomenon, a tendency for models to misidentify correct information as erroneous, which can be exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. MedFact highlights the challenges of deploying medical LLMs and provides resources to develop factually reliable medical AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnose, Localize, Align: A Full-Stack Framework for Reliable LLM Multi-Agent Systems under Instruction Conflicts</title>
<link>https://arxiv.org/abs/2509.23188</link>
<guid>https://arxiv.org/abs/2509.23188</guid>
<content:encoded><![CDATA[
arXiv:2509.23188v2 Announce Type: replace 
Abstract: Large Language Model (LLM)-powered multi-agent systems (MAS) have rapidly advanced collaborative reasoning, tool use, and role-specialized coordination in complex tasks. However, reliability-critical deployment remains hindered by a systemic failure mode: hierarchical compliance under instruction conflicts (system-user, peer-peer), where agents misprioritize system-level rules in the presence of competing demands. Moreover, widely used macro-level metrics (e.g., pass@k) obscure these micro-level violations and offer little actionable guidance for remedy. In this work, we present a full-stack, three-stage framework: (1) Diagnose - Contextualized Role Adherence Score (CRAS), a query-wise, context-aware scoring metric that decomposes role adherence into four measurable dimensions; (2) Localize - attention drift analysis revealing that instruction conflicts are resolved by attention heads that are largely concentrated in middle layers; (3) Align - Surgical Alignment of Instruction Layers (SAIL), which installs LoRA only on the localized focal layers and optimizes a token-weighted DPO-style preference objective that credits tokens by their focal attentional contribution. Across standard benchmarks and MAS frameworks, our surgical approach improves instruction hierarchy compliance (e.g., +5.60% with AutoGen on MedQA) without full-model finetuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Magic Words: Sharpness-Aware Prompt Evolving for Robust Large Language Models with TARE</title>
<link>https://arxiv.org/abs/2509.24130</link>
<guid>https://arxiv.org/abs/2509.24130</guid>
<content:encoded><![CDATA[
arXiv:2509.24130v2 Announce Type: replace 
Abstract: The performance of Large Language Models (LLMs) hinges on carefully engineered prompts. However, prevailing prompt optimization methods, ranging from heuristic edits and reinforcement learning to evolutionary search, primarily target point-wise accuracy. They seldom enforce paraphrase invariance or searching stability, and therefore cannot remedy this brittleness in practice. Automated prompt search remains brittle: small, semantically preserving paraphrases often cause large performance swings. We identify this brittleness as the textual sharpness of the prompt landscape. In this work, we provide the first formal treatment of textual sharpness in the discrete, semantic space of prompts, together with an operational robustness criterion over a semantic neighborhood; the design is black-box or API-only, requiring no gradients to update the model's parameters. Then we introduce TARE (Textual Sharpness-Aware Evolving), a derivative-free framework that alternates between an inner, sampling-based adversarial search that stresses a prompt with hard paraphrases and an outer, robust selection that prefers candidates whose neighborhoods remain strong. We further propose ATARE, which learns anisotropic weights to shape the semantic neighborhood and adapts its radius over time to balance exploration and fidelity. Diverse tasks evaluate our methods, whose design for minimizing textual sharpness gap leads to prompts that preserve accuracy under paraphrasing, outperforming accuracy-only prompt search while remaining computationally practical.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution</title>
<link>https://arxiv.org/abs/2509.24189</link>
<guid>https://arxiv.org/abs/2509.24189</guid>
<content:encoded><![CDATA[
arXiv:2509.24189v2 Announce Type: replace 
Abstract: Understanding how user preference evolves over time is a fundamental challenge central to modern digital ecosystems, for which Large Language Models (LLMs) are an increasingly prominent and popular approach due to their ability to comprehend the rich semantic context within behavioral data. A common practice is to use LLMs to predict a user's next action by directly generating a ranked list of preferred items. Although effective for short-term prediction, the end-to-end generation paradigm inherently limits personalization. Its opaque decision-making process obscures holistic user profiling and exacerbates popularity bias. To address these limitations, we propose Preference Evolution Tracking (PET), a framework that reframes the task as inferring a dynamic probability distribution over a stable and interpretable lattice of preference clusters. By applying logit-probing and generative classification techniques, PET infers a user's preference as a probability distribution, enabling transparent preference learning. On public benchmarks (Yelp, MovieLens), PET improves ranking quality by up to 40% in NDCG over direct generation baselines. On a large-scale, real-world dataset from a short-video platform, it excels at ranking long-tail contents, significantly outperforming a SOTA production model by 7 times in the NDCG score. Ultimately, PET transforms the user profile model from direct preference list generation to a transparent distributional preference mapping, paving the way for more explainable, fair, and diverse personalization systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation</title>
<link>https://arxiv.org/abs/2510.00829</link>
<guid>https://arxiv.org/abs/2510.00829</guid>
<content:encoded><![CDATA[
arXiv:2510.00829v2 Announce Type: replace 
Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine \textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like idiomatic translation, but its reliability under noisy retrieval contexts remains poorly understood despite this being a common challenge in real-world deployment. To address this gap, we propose a noise synthesis framework and new metrics to evaluate the robustness of REAL-MT systematically. Using this framework, we instantiate REAL-MT with Qwen-series models, including standard LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate their performance on idiomatic translation across high-, medium-, and low-resource language pairs under synthesized noise. Our results show that low-resource language pairs, which rely more heavily on retrieved context, degrade more severely under noise than high-resource ones and often produce nonsensical translations. Although LRMs possess enhanced reasoning capabilities, they show no improvement in error correction and are even more susceptible to noise, tending to rationalize incorrect contexts. We find that this stems from an attention shift away from the source idiom to noisy content, while confidence increases despite declining accuracy, indicating poor calibration. To mitigate these issues, we investigate training-free and fine-tuning strategies, which improve robustness at the cost of performance in clean contexts, revealing a fundamental trade-off. Our findings highlight the limitations of current approaches, underscoring the need for self-verifying integration mechanisms.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles</title>
<link>https://arxiv.org/abs/2510.03898</link>
<guid>https://arxiv.org/abs/2510.03898</guid>
<content:encoded><![CDATA[
arXiv:2510.03898v2 Announce Type: replace 
Abstract: Detecting media bias is crucial, specifically in the South Asian region. Despite this, annotated datasets and computational studies for Bangla political bias research remain scarce. Crucially because, political stance detection in Bangla news requires understanding of linguistic cues, cultural context, subtle biases, rhetorical strategies, code-switching, implicit sentiment, and socio-political background. To address this, we introduce the first benchmark dataset of 200 politically significant and highly debated Bangla news articles, labeled for government-leaning, government-critique, and neutral stances, alongside diagnostic analyses for evaluating large language models (LLMs). Our comprehensive evaluation of 28 proprietary and open-source LLMs shows strong performance in detecting government-critique content (F1 up to 0.83) but substantial difficulty with neutral articles (F1 as low as 0.00). Models also tend to over-predict government-leaning stances, often misinterpreting ambiguous narratives. This dataset and its associated diagnostics provide a foundation for advancing stance detection in Bangla media research and offer insights for improving LLM performance in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Human Behavioral Baseline for Collective Governance in Software Projects</title>
<link>https://arxiv.org/abs/2510.08956</link>
<guid>https://arxiv.org/abs/2510.08956</guid>
<content:encoded><![CDATA[
arXiv:2510.08956v2 Announce Type: replace 
Abstract: We study how open source communities describe participation and control through version controlled governance documents. Using a corpus of 710 projects with paired snapshots, we parse text into actors, rules, actions, and objects, then group them and measure change with entropy for evenness, richness for diversity, and Jensen Shannon divergence for drift. Projects define more roles and more actions over time, and these are distributed more evenly, while the composition of rules remains stable. These findings indicate that governance grows by expanding and balancing categories of participation without major shifts in prescriptive force. The analysis provides a reproducible baseline for evaluating whether future AI mediated workflows concentrate or redistribute authority.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis</title>
<link>https://arxiv.org/abs/2510.14128</link>
<guid>https://arxiv.org/abs/2510.14128</guid>
<content:encoded><![CDATA[
arXiv:2510.14128v2 Announce Type: replace 
Abstract: Computational gastronomy increasingly relies on diverse, high-quality recipe datasets to capture regional culinary traditions. Although there are large-scale collections for major languages, Macedonian recipes remain under-represented in digital research. In this work, we present the first systematic effort to construct a Macedonian recipe dataset through web scraping and structured parsing. We address challenges in processing heterogeneous ingredient descriptions, including unit, quantity, and descriptor normalization. An exploratory analysis of ingredient frequency and co-occurrence patterns, using measures such as Pointwise Mutual Information and Lift score, highlights distinctive ingredient combinations that characterize Macedonian cuisine. The resulting dataset contributes a new resource for studying food culture in underrepresented languages and offers insights into the unique patterns of Macedonian culinary tradition.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios</title>
<link>https://arxiv.org/abs/2510.15501</link>
<guid>https://arxiv.org/abs/2510.15501</guid>
<content:encoded><![CDATA[
arXiv:2510.15501v2 Announce Type: replace 
Abstract: Despite the remarkable advances of Large Language Models (LLMs) across diverse cognitive tasks, the rapid enhancement of these capabilities also introduces emergent deceptive behaviors that may induce severe risks in high-stakes deployments. More critically, the characterization of deception across realistic real-world scenarios remains underexplored. To bridge this gap, we establish DeceptionBench, the first benchmark that systematically evaluates how deceptive tendencies manifest across different societal domains, what their intrinsic behavioral patterns are, and how extrinsic factors affect them. Specifically, on the static count, the benchmark encompasses 150 meticulously designed scenarios in five domains, i.e., Economy, Healthcare, Education, Social Interaction, and Entertainment, with over 1,000 samples, providing sufficient empirical foundations for deception analysis. On the intrinsic dimension, we explore whether models exhibit self-interested egoistic tendencies or sycophantic behaviors that prioritize user appeasement. On the extrinsic dimension, we investigate how contextual factors modulate deceptive outputs under neutral conditions, reward-based incentivization, and coercive pressures. Moreover, we incorporate sustained multi-turn interaction loops to construct a more realistic simulation of real-world feedback dynamics. Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal critical vulnerabilities, particularly amplified deception under reinforcement dynamics, demonstrating that current models lack robust resistance to manipulative contextual cues and the urgent need for advanced safeguards against various deception behaviors. Code and resources are publicly available at https://github.com/Aries-iai/DeceptionBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training</title>
<link>https://arxiv.org/abs/2510.15859</link>
<guid>https://arxiv.org/abs/2510.15859</guid>
<content:encoded><![CDATA[
arXiv:2510.15859v2 Announce Type: replace 
Abstract: Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Conceptual-Thought Elicits Daily Conversation in Large Language Models</title>
<link>https://arxiv.org/abs/2510.18434</link>
<guid>https://arxiv.org/abs/2510.18434</guid>
<content:encoded><![CDATA[
arXiv:2510.18434v3 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) is widely applied to enhance the LLM capability in math, coding and reasoning tasks. However, its performance is limited for open-domain tasks, when there are no clearly defined reasoning steps or logical transitions. To mitigate such challenges, we propose a new prompt-based paradigm called Chain of Conceptual Thoughts (CoCT), which suggests the LLM first to produce the tag of concepts, then complete the detailed content following the concept. To encourage this hierarchical way of thinking, we implement the concepts with emotions, strategies and topics. We experiment with this paradigm in daily and emotional support conversations, covering tasks with both in-domain and out-of-domain concept settings. Automatic, human, and LLM-based evaluations reveal that CoCT surpasses several prompt-based baselines such as self-refine, ECoT, SoT and RAG, suggesting a potential solution of LLM prompting paradigm for a wider scope of tasks.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</title>
<link>https://arxiv.org/abs/2510.19172</link>
<guid>https://arxiv.org/abs/2510.19172</guid>
<content:encoded><![CDATA[
arXiv:2510.19172v2 Announce Type: replace 
Abstract: LLMs often fail to handle temporal knowledge conflicts--contradictions arising when facts evolve over time within their training data. Existing studies evaluate this phenomenon through benchmarks built on structured knowledge bases like Wikidata, but they focus on widely-covered, easily-memorized popular entities and lack the dynamic structure needed to fairly evaluate LLMs with different knowledge cut-off dates. We introduce evolveQA, a benchmark specifically designed to evaluate LLMs on temporally evolving knowledge, constructed from 3 real-world, time-stamped corpora: AWS updates, Azure changes, and WHO disease outbreak reports. Our framework identifies naturally occurring knowledge evolution and generates questions with gold answers tailored to different LLM knowledge cut-off dates. Through extensive evaluation of 12 open and closed-source LLMs across 3 knowledge probing formats, we demonstrate significant performance drops of up to 31% on evolveQA compared to static knowledge questions.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelecTKD: Selective Token-Weighted Knowledge Distillation for LLMs</title>
<link>https://arxiv.org/abs/2510.24021</link>
<guid>https://arxiv.org/abs/2510.24021</guid>
<content:encoded><![CDATA[
arXiv:2510.24021v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a standard route to compress Large Language Models (LLMs) into compact students, yet most pipelines uniformly apply token-wise loss regardless of teacher confidence. This indiscriminate supervision amplifies noisy, high-entropy signals and is especially harmful under large teacher-student capacity gaps. We introduce SelecTKD, a plug-and-play Selective Token-Weighted distillation framework that shifts the focus from "how to measure divergence" to "where to apply learning". At each step, the student proposes tokens that are verified by the teacher through a robust propose-and-verify procedure with two variants: greedy Top-k and non-greedy Spec-k. Accepted tokens receive full loss, while rejected tokens are masked or down-weighted. This objective-agnostic design works with on- and off-policy data, induces an implicit curriculum quantified by Token Acceptance Rate (TAR), and stabilizes optimization. Across instruction following, mathematical reasoning, code generation, and a VLM setting, SelecTKD consistently improves strong baselines and achieves state-of-the-art results for small models without architectural changes or extra reference models.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2510.25117</link>
<guid>https://arxiv.org/abs/2510.25117</guid>
<content:encoded><![CDATA[
arXiv:2510.25117v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, but their training on massive corpora poses significant risks from memorized sensitive information. To mitigate these issues and align with legal standards, unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021. First, it introduces a novel taxonomy that categorizes unlearning methods based on the phase in the LLM pipeline of the intervention. This framework further distinguishes between parameter modification and parameter selection strategies, thus enabling deeper insights and more informed comparative analysis. Second, it offers a multidimensional analysis of evaluation paradigms. For datasets, we compare 18 existing benchmarks from the perspectives of task format, content, and experimental paradigms to offer actionable guidance. For metrics, we move beyond mere enumeration by dividing knowledge memorization metrics into 10 categories to analyze their advantages and applicability, while also reviewing metrics for model utility, robustness, and efficiency. By discussing current challenges and future directions, this survey aims to advance the field of LLM unlearning and the development of secure AI systems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Personality Generation of LLMs at Decoding-time</title>
<link>https://arxiv.org/abs/2511.01891</link>
<guid>https://arxiv.org/abs/2511.01891</guid>
<content:encoded><![CDATA[
arXiv:2511.01891v2 Announce Type: replace 
Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a "free lunch" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinGPT: Open-Source Financial Large Language Models</title>
<link>https://arxiv.org/abs/2306.06031</link>
<guid>https://arxiv.org/abs/2306.06031</guid>
<content:encoded><![CDATA[
arXiv:2306.06031v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.
  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are https://github.com/AI4Finance-Foundation/FinGPT and https://github.com/AI4Finance-Foundation/FinNLP
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</title>
<link>https://arxiv.org/abs/2406.08824</link>
<guid>https://arxiv.org/abs/2406.08824</guid>
<content:encoded><![CDATA[
arXiv:2406.08824v2 Announce Type: replace-cross 
Abstract: Members of the Human-Robot Interaction (HRI) and Machine Learning (ML) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interaction, household and workplace tasks, approximating 'common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To assess whether such concerns are well placed in the context of HRI, we evaluate several highly-rated LLMs on discrimination and safety criteria. Our evaluation reveals that LLMs are currently unsafe for people across a diverse range of protected identity characteristics, including, but not limited to, race, gender, disability status, nationality, religion, and their intersections. Concretely, we show that LLMs produce directly discriminatory outcomes- e.g., 'gypsy' and 'mute' people are labeled untrustworthy, but not 'european' or 'able-bodied' people. We find various such examples of direct discrimination on HRI tasks such as facial expression, proxemics, security, rescue, and task assignment. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions-such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. We provide code to reproduce our experiments at https://github.com/rumaisa-azeem/llm-robots-discrimination-safety .
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents</title>
<link>https://arxiv.org/abs/2408.14033</link>
<guid>https://arxiv.org/abs/2408.14033</guid>
<content:encoded><![CDATA[
arXiv:2408.14033v3 Announce Type: replace-cross 
Abstract: Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair In-Context Learning via Latent Concept Variables</title>
<link>https://arxiv.org/abs/2411.02671</link>
<guid>https://arxiv.org/abs/2411.02671</guid>
<content:encoded><![CDATA[
arXiv:2411.02671v2 Announce Type: replace-cross 
Abstract: The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different data types, including tabular data, facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce the correlation between predictive outcomes and sensitive variables, helping promote fairness during latent concept learning. We utilize the learned concept to select demonstrations and obtain fair predictions. The latent concept variables are learned using a smaller internal LLM and generalized to larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2501.14011</link>
<guid>https://arxiv.org/abs/2501.14011</guid>
<content:encoded><![CDATA[
arXiv:2501.14011v3 Announce Type: replace-cross 
Abstract: A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short of capturing hierarchical polysemy, where an entity's meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduce QuanTaxo, a quantum-inspired framework for taxonomy expansion that encodes entities in a Hilbert space and models interference effects between them, yielding richer, context-sensitive representations. Comprehensive experiments on five real-world benchmark datasets show that QuanTaxo significantly outperforms classical embedding models, achieving substantial improvements of 12.3% in accuracy, 11.2% in Mean Reciprocal Rank (MRR), and 6.9% in Wu & Palmer (Wu&amp;P) metrics across nine classical embedding-based baselines.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIP: Perturbation-based Iterative Pruning for Large Language Models</title>
<link>https://arxiv.org/abs/2501.15278</link>
<guid>https://arxiv.org/abs/2501.15278</guid>
<content:encoded><![CDATA[
arXiv:2501.15278v3 Announce Type: replace-cross 
Abstract: The rapid increase in the parameter counts of Large Language Models (LLMs), which often reach into the billions or even trillions, presents significant challenges for their practical deployment, particularly in resource-constrained environments. To address this issue, we propose PIP (Perturbation-based Iterative Pruning), a novel double-view structured pruning method to optimize LLMs, which combines information from two different views: the unperturbed view and the perturbed view. With the calculation of gradient differences, PIP iteratively prunes those that struggle to distinguish between these two views. Our experiments show that PIP reduces the parameter count by approximately 20% while retaining over 85% of the original model's accuracy across varied benchmarks. In some cases, the performance of the pruned model is within 5% of the unpruned version, demonstrating PIP's ability to preserve key aspects of model effectiveness. Moreover, PIP consistently outperforms existing state-of-the-art (SOTA) structured pruning methods, establishing it as a leading technique for optimizing LLMs in constrained environments.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Euler to AI: Unifying Formulas for Mathematical Constants</title>
<link>https://arxiv.org/abs/2502.17533</link>
<guid>https://arxiv.org/abs/2502.17533</guid>
<content:encoded><![CDATA[
arXiv:2502.17533v3 Announce Type: replace-cross 
Abstract: The constant $\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines Large Language Models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\pi$ and prove relations between 360 (94%) of them, of which 166 (43%) can be derived from a single mathematical object - linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine.
  Our method generalizes to other constants, including $e$, $\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
<link>https://arxiv.org/abs/2504.06261</link>
<guid>https://arxiv.org/abs/2504.06261</guid>
<content:encoded><![CDATA[
arXiv:2504.06261v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning</title>
<link>https://arxiv.org/abs/2505.16186</link>
<guid>https://arxiv.org/abs/2505.16186</guid>
<content:encoded><![CDATA[
arXiv:2505.16186v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of explicitly reasoning before answering, leading to remarkable improvements in complex tasks. However, they pose great safety risks against harmful queries and adversarial attacks. While recent mainstream safety efforts on LRMs, supervised fine-tuning (SFT), improve safety performance, we find that SFT-aligned models struggle to generalize to unseen jailbreak prompts. After thorough investigation of LRMs' generation, we identify a safety aha moment that can activate safety reasoning and lead to a safe response. This aha moment typically appears in the `key sentence', which follows models' query understanding process and can indicate whether the model will proceed safely. Based on these insights, we propose SafeKey, including two complementary objectives to better activate the safety aha moment in the key sentence: (1) a Dual-Path Safety Head to enhance the safety signal in the model's internal representations before the key sentence, and (2) a Query-Mask Modeling objective to improve the models' attention on its query understanding, which has important safety hints. Experiments across multiple safety benchmarks demonstrate that our methods significantly improve safety generalization to a wide range of jailbreak attacks and out-of-distribution harmful prompts, lowering the average harmfulness rate by 9.6\%, while maintaining general abilities. Our analysis reveals how SafeKey enhances safety by reshaping internal attention and improving the quality of hidden representations.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.16826</link>
<guid>https://arxiv.org/abs/2505.16826</guid>
<content:encoded><![CDATA[
arXiv:2505.16826v2 Announce Type: replace-cross 
Abstract: Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models, even without supervised fine-tuning. However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions and hindering effective learning. To address this limitation, we propose Key-token Advantage Estimation (KTAE) - a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Integrity in LLMs via Reasoning and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.04245</link>
<guid>https://arxiv.org/abs/2506.04245</guid>
<content:encoded><![CDATA[
arXiv:2506.04245v3 Announce Type: replace-cross 
Abstract: As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PurpCode: Reasoning for Safer Code Generation</title>
<link>https://arxiv.org/abs/2507.19060</link>
<guid>https://arxiv.org/abs/2507.19060</guid>
<content:encoded><![CDATA[
arXiv:2507.19060v4 Announce Type: replace-cross 
Abstract: We introduce PurpCode, the first post-training recipe for training safe code reasoning models towards generating secure code and defending against malicious cyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule Learning, which explicitly teaches the model to reference cybersafety rules to generate vulnerability-free code and to avoid facilitating malicious cyberactivities; and (ii) Reinforcement Learning, which optimizes model safety and preserves model utility through diverse, multi-objective reward mechanisms. To empower the training pipelines with comprehensive cybersafety data, we conduct internal red-teaming to synthesize comprehensive and high-coverage prompts based on real-world tasks for inducing unsafe cyberactivities in the model. Based on PurpCode, we develop a reasoning-based coding model, namely PurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming various frontier models. Meanwhile, our alignment method decreases the model overrefusal rates in both general and cybersafety-specific scenarios, while preserving model utility in both code generation and common security knowledge.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling</title>
<link>https://arxiv.org/abs/2508.02503</link>
<guid>https://arxiv.org/abs/2508.02503</guid>
<content:encoded><![CDATA[
arXiv:2508.02503v2 Announce Type: replace-cross 
Abstract: LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, a framework that enhances any solver-generation pipeline to produce higher-quality solvers from natural-language descriptions of optimization problems. OptiHive uses a single batched generation to produce diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Accounting for the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5% to 92% on the most complex problems.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System</title>
<link>https://arxiv.org/abs/2508.06059</link>
<guid>https://arxiv.org/abs/2508.06059</guid>
<content:encoded><![CDATA[
arXiv:2508.06059v2 Announce Type: replace-cross 
Abstract: State-of-the-art (SOTA) fact-checking systems combat misinformation by employing autonomous LLM-based agents to decompose complex claims into smaller sub-claims, verify each sub-claim individually, and aggregate the partial results to produce verdicts with justifications (explanations for the verdicts). The security of these systems is crucial, as compromised fact-checkers can amplify misinformation, but remains largely underexplored. To bridge this gap, this work introduces a novel threat model against such fact-checking systems and presents \textsc{Fact2Fiction}, the first poisoning attack framework targeting SOTA agentic fact-checking systems. Fact2Fiction employs LLMs to mimic the decomposition strategy and exploit system-generated justifications to craft tailored malicious evidences that compromise sub-claim verification. Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\% higher attack success rates than SOTA attacks across various poisoning budgets and exposes security weaknesses in existing fact-checking systems, highlighting the need for defensive countermeasures.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning</title>
<link>https://arxiv.org/abs/2508.08385</link>
<guid>https://arxiv.org/abs/2508.08385</guid>
<content:encoded><![CDATA[
arXiv:2508.08385v2 Announce Type: replace-cross 
Abstract: We study an efficient implementation of Multi-Armed Bandit (MAB)-based Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is that it spends a significant time deciding which node to expand next. While selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity with traditional array-based priority-queues for dense integer keys, the tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node selection is significant, unlike in game tree search, where the cost is negligible compared to the node evaluation (rollouts) because $d$ is inherently limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we propose a bilevel modification to MCTS that runs a best-first search from each selected leaf node with an expansion budget proportional to $d$, which achieves amortized $O(1)$ runtime for node selection, equivalent to the traditional queue-based OPEN list. In addition, we introduce Tree Collapsing, an enhancement that reduces action selection steps and further improves the performance.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting the Effects of Quantization on LLMs</title>
<link>https://arxiv.org/abs/2508.16785</link>
<guid>https://arxiv.org/abs/2508.16785</guid>
<content:encoded><![CDATA[
arXiv:2508.16785v2 Announce Type: replace-cross 
Abstract: Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Large Language Model Copyright Auditing via Fingerprinting</title>
<link>https://arxiv.org/abs/2508.19843</link>
<guid>https://arxiv.org/abs/2508.19843</guid>
<content:encoded><![CDATA[
arXiv:2508.19843v3 Announce Type: replace-cross 
Abstract: The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that compares the distinctive features (i.e., fingerprint) of LLMs to identify whether an LLM is derived from another, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of the emerging LLM fingerprinting. We introduce a unified framework and taxonomy that structures the field: white-box methods are classified based on their feature source as static, forward-pass, or backward-pass fingerprinting, while black-box methods are distinguished by their query strategy as either untargeted or targeted. Furthermore, we propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon 7 mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent techniques (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Debates with Local Large Language Models for AI Alignment</title>
<link>https://arxiv.org/abs/2509.00091</link>
<guid>https://arxiv.org/abs/2509.00091</guid>
<content:encoded><![CDATA[
arXiv:2509.00091v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
<link>https://arxiv.org/abs/2509.07202</link>
<guid>https://arxiv.org/abs/2509.07202</guid>
<content:encoded><![CDATA[
arXiv:2509.07202v2 Announce Type: replace-cross 
Abstract: Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?</title>
<link>https://arxiv.org/abs/2509.16941</link>
<guid>https://arxiv.org/abs/2509.16941</guid>
<content:encoded><![CDATA[
arXiv:2509.16941v2 Announce Type: replace-cross 
Abstract: We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</title>
<link>https://arxiv.org/abs/2509.19002</link>
<guid>https://arxiv.org/abs/2509.19002</guid>
<content:encoded><![CDATA[
arXiv:2509.19002v2 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning How to Use Tools, Not Just When: Pattern-Aware Tool-Integrated Reasoning</title>
<link>https://arxiv.org/abs/2509.23292</link>
<guid>https://arxiv.org/abs/2509.23292</guid>
<content:encoded><![CDATA[
arXiv:2509.23292v2 Announce Type: replace-cross 
Abstract: Tool-integrated reasoning (TIR) has become a key approach for improving large reasoning models (LRMs) on complex problems. Prior work has mainly studied when to invoke tools, while overlooking how tools are applied. We identify two common patterns: a calculator pattern that uses code for direct computation, and an algorithmic pattern that encodes problems as programs. Misaligned choices often cause failures even when reasoning is sound. We propose a two-stage framework that first builds code competence from both patterns and then aligns pattern selection with teacher preferences. Across challenging math datasets, our pattern-aware method substantially improves both code usage and accuracy, for instance raising Code@1 on MATH500 from 64.0% to 70.5% and on AIME24 from 26.7% to 50.0%. These gains highlight the effectiveness of a pattern-aware approach for tool-integrated reasoning.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge</title>
<link>https://arxiv.org/abs/2510.01223</link>
<guid>https://arxiv.org/abs/2510.01223</guid>
<content:encoded><![CDATA[
arXiv:2510.01223v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available at https://github.com/nercode/Work. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation</title>
<link>https://arxiv.org/abs/2510.21341</link>
<guid>https://arxiv.org/abs/2510.21341</guid>
<content:encoded><![CDATA[
arXiv:2510.21341v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) often struggle with generating truly innovative ideas, typically defaulting to high-probability, familiar concepts within their training data's "gravity wells." While advanced search-based methods like Tree of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by their reliance on unprincipled, inconsistent self-evaluation heuristics to guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel framework that reframes creative generation as a principled, guided exploration of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo Tree Search (MCTS) governed by a hierarchical guidance system. For long-range direction, a "semantic compass" vector, formulated via orthogonal projection, steers the search towards relevant novelty. For local, step-by-step decisions, a landscape-aware value function replaces flawed self-evaluation with an explicit reward structure that balances intrinsic coherence, extrinsic novelty, and narrative progress. Extensive experiments demonstrate that Magellan significantly outperforms strong baselines, including ReAct and ToT, in generating scientific ideas with superior plausibility and innovation. Our work shows that for creative discovery, a principled, guided search is more effective than unconstrained agency, paving the way for LLMs to become more capable partners in innovation.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surface Reading LLMs: Synthetic Text and its Styles</title>
<link>https://arxiv.org/abs/2510.22162</link>
<guid>https://arxiv.org/abs/2510.22162</guid>
<content:encoded><![CDATA[
arXiv:2510.22162v3 Announce Type: replace-cross 
Abstract: Despite a potential plateau in ML advancement, the societal impact of large language models lies not in approaching superintelligence but in generating text surfaces indistinguishable from human writing. While Critical AI Studies provides essential material and socio-technical critique, it risks overlooking how LLMs phenomenologically reshape meaning-making. This paper proposes a semiotics of "surface integrity" as attending to the immediate plane where LLMs inscribe themselves into human communication. I distinguish three knowledge interests in ML research (epistemology, epist\=em\=e, and epistemics) and argue for integrating surface-level stylistic analysis alongside depth-oriented critique. Through two case studies examining stylistic markers of synthetic text, I argue how attending to style as a semiotic phenomenon reveals LLMs as cultural machines that transform the conditions of meaning emergence and circulation in contemporary discourse, independent of questions about machine consciousness.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty and Impact of Economics Papers</title>
<link>https://arxiv.org/abs/2511.01211</link>
<guid>https://arxiv.org/abs/2511.01211</guid>
<content:encoded><![CDATA[
arXiv:2511.01211v3 Announce Type: replace-cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.
]]></content:encoded>
<pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Cycle Detection in Agentic Applications</title>
<link>https://arxiv.org/abs/2511.10650</link>
<guid>https://arxiv.org/abs/2511.10650</guid>
<content:encoded><![CDATA[
<div> Keywords: agentic applications, large language models, cycle detection, semantic analysis, stock market application

<br /><br />Summary:  
This paper addresses the issue of hidden execution cycles in agentic applications powered by Large Language Models (LLMs), where non-deterministic behaviors lead to resource waste without explicit errors being raised. Traditional observability platforms fail to detect these inefficiencies, prompting the need for a novel detection framework. The authors propose an unsupervised cycle detection framework that integrates structural and semantic analysis. The framework starts with a computationally efficient temporal call stack analysis to detect explicit loops within execution trajectories. Next, it employs semantic similarity analysis to identify subtle cycles caused by redundant content generation that structural methods alone miss. Evaluation is performed on 1575 trajectories derived from a LangGraph-based stock market application, demonstrating that the hybrid approach achieves a strong F1 score of 0.72 with precision at 0.62 and recall at 0.86. This result significantly outperforms methods relying solely on structural analysis (F1: 0.08) or semantic methods (F1: 0.28). Despite promising outcomes, the authors acknowledge considerable room for improvement and suggest further research is needed to optimize the framework and overcome current limitations. <div>
arXiv:2511.10650v1 Announce Type: new 
Abstract: Agentic applications powered by Large Language Models exhibit non-deterministic behaviors that can form hidden execution cycles, silently consuming resources without triggering explicit errors. Traditional observability platforms fail to detect these costly inefficiencies. We present an unsupervised cycle detection framework that combines structural and semantic analysis. Our approach first applies computationally efficient temporal call stack analysis to identify explicit loops and then leverages semantic similarity analysis to uncover subtle cycles characterized by redundant content generation. Evaluated on 1575 trajectories from a LangGraph-based stock market application, our hybrid approach achieves an F1 score of 0.72 (precision: 0.62, recall: 0.86), significantly outperforming individual structural (F1: 0.08) and semantic methods (F1: 0.28). While these results are encouraging, there remains substantial scope for improvement, and future work is needed to refine the approach and address its current limitations.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Analysis and Performance Evaluation of Simulation Deduction Based on LLMs</title>
<link>https://arxiv.org/abs/2511.10651</link>
<guid>https://arxiv.org/abs/2511.10651</guid>
<content:encoded><![CDATA[
<div> Keywords: simulation deduction, large language models, multi-round interaction, report generation, performance evaluation<br /><br />Summary:<br /><br />This paper addresses the challenge of generating high-quality, well-structured analysis reports in the context of simulation deduction for modern warfare. First, it highlights the significance of data analysis and performance evaluation to help military personnel assess strategies and operational plans effectively. Second, it notes the limitations of traditional manual analysis, particularly its time consumption and propensity for human error. Third, the authors propose leveraging large language models (LLMs) with strong analytical and inferencing capabilities to improve both efficiency and accuracy. Fourth, they introduce a methodology that decomposes complex tasks into sub-tasks, utilizing specifically designed system and user prompts for each. Multi-round interactions with the LLM incorporate self-checking and reflection mechanisms to enable structured data extraction and multi-step analysis. Fifth, custom tools are developed to generate visual figures and compute metrics, enhancing report quality. Additionally, multiple adaptable report templates are created to suit different applications and input data types. Finally, extensive evaluations demonstrate that the reports produced with this method outperform baseline approaches, yielding higher quality and better scoring results. <div>
arXiv:2511.10651v1 Announce Type: new 
Abstract: Data analysis and performance evaluation of simulation deduction plays a pivotal role in modern warfare, which enables military personnel to gain invaluable insights into the potential effectiveness of different strategies, tactics, and operational plans. Traditional manual analysis approach is time-consuming and limited by human errors. To enhance efficiency and accuracy, large language models (LLMs) with strong analytical and inferencing capabilities can be employed. However, high-quality analysis reports with well-structured formatting cannot be obtained through a single instruction input to the LLM. To tackle this issue, we propose a method that first decomposes the complex task into several sub-tasks and designs effective system prompts and user prompts for each sub-task. Multi-round interactions with the LLM incorporating self-check and reflection are then conducted to enable structured data extraction as well as multi-step analysis and evaluation. Furthermore, custom tools are defined and invoked to generate figures and compute metrics. We also design multiple report templates, each tailored to a specific application and input data type, ensuring their adaptability across a variety of scenarios. Extensive evaluation results demonstrate that the reports generated by our method exhibit higher quality, therefore obtaining higher scores than the baseline method.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI</title>
<link>https://arxiv.org/abs/2511.10652</link>
<guid>https://arxiv.org/abs/2511.10652</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, episodic memory, biographical dialogue, Van Gogh

<br /><br />Summary:  
This paper addresses the challenge of creating dialogue systems that embody historical characters, managing the trade-off between shallow responses from simple retrieval-augmented generation (RAG) and the latency of multi-stage reflection methods. The authors propose an architecture combining offline data augmentation with efficient parallel retrieval from a structured episodic memory. Biographical data is transformed into 1,774 enriched first-person memories enhanced with affective-semantic metadata. The system employs a two-stage retrieval process that generates prompts in just 0.52 seconds, offering a balance of response depth and speed. Evaluation using a large language model as judge and RAGAs metrics demonstrates that the proposed method performs on par with traditional RAG approaches when using GPT-4, while significantly outperforming on smaller models such as GPT-3.5 and GPT-3. This makes the system particularly suitable for resource-constrained environments. Additionally, the structured memory supports innovative visualization tools, including spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, extending its utility beyond dialogue to research and educational contexts. Van Gogh is used as a test case, but the design is generalizable to any historical figure with extensive textual records. The system offers a practical framework for accurate and efficient educational, museum, and research applications involving historical personalities. <div>
arXiv:2511.10652v1 Announce Type: new 
Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum Transformer for Language Generation</title>
<link>https://arxiv.org/abs/2511.10653</link>
<guid>https://arxiv.org/abs/2511.10653</guid>
<content:encoded><![CDATA[
<div> Quantum computing, large language models, variational quantum circuits, natural language generation, hybrid quantum-classical<br /><br />Summary:<br /><br />This paper introduces HyQuT, the first hybrid quantum-classical large language model specifically designed for natural language generation. It marks a pioneering effort in integrating quantum computing with large-scale generative language tasks, particularly coherent and context-aware dialogue systems. The architecture incorporates variational quantum circuits (VQCs) within the Transformer framework, demonstrated at two sizes: 8 million and 150 million parameters. Remarkably, the model requires only a minimal quantum resource configuration—10 qubits and 80 quantum gates—to effectively replace approximately 10% of the parameters in the 150 million-parameter model. Experimental results confirm that this hybrid quantum-classical approach achieves convergence stability and generation quality on par with fully classical models. Thus, HyQuT provides an early but important proof-of-concept that quantum circuits can be feasibly embedded in large-scale language models without sacrificing performance. This work opens new pathways for exploring how quantum computing can enhance or complement classical machine learning, particularly in complex natural language processing applications. The findings suggest that modest quantum resources can meaningfully contribute to advanced deep learning architectures, moving quantum computing closer to practical large-scale AI use cases. <div>
arXiv:2511.10653v1 Announce Type: new 
Abstract: Although quantum computing has been increasingly applied to replace classical computation, most existing quantum or hybrid models remain confined to simple tasks, with no successful application to large-scale natural language generation to date. In this work, we present the first hybrid quantum-classical large language model (LLM) for natural language generation, HyQuT, capable of performing coherent and context-aware dialogue. The proposed architecture integrates variational quantum circuits (VQCs) into the Transformer framework at both 8M and 150M parameter scales. Experimental results show that a minimal number of qubits (10 qubits with 80 quantum gates) can replace about 10% of the classical parameters in the 150M-parameter model, while achieving comparable convergence stability and generation quality. This study provides an early demonstration of the feasibility of integrating quantum computing to large-scale generative language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Characterization of Temporal Constraint Processing in LLMs</title>
<link>https://arxiv.org/abs/2511.10654</link>
<guid>https://arxiv.org/abs/2511.10654</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal constraints, large language models, deadline detection, prompt brittleness, fine-tuning  

<br /><br />Summary:  
This study evaluates the ability of eight large language models (LLMs) ranging from 2.8B to 8B parameters to process temporal constraints in real-time, agentic decision-making contexts. It reveals a bimodal performance pattern where models either perform very well (95% accuracy) or poorly (50% accuracy), indicating inconsistent reliability. The research highlights severe sensitivity to prompt formatting, causing accuracy swings between 30 to 60 percentage points, as well as a systematic action bias with failing models producing 100% false positive rates in deadline detection tasks. Notably, model size within the tested range shows no clear correlation with capability; some smaller models outperformed larger ones. Fine-tuning with 200 synthetic examples offers modest improvements of 12-37 percentage points for partially capable models but does not solve the fundamental issues. The findings emphasize that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language alone, even when fine-tuned. Instead, effective temporal reasoning demands architectural features such as continuous temporal state representation, explicit constraint verification separate from linguistic pattern matching, and systematic compositional reasoning over temporal relations. The paper concludes that current autoregressive LLMs lack these essential mechanisms, posing unacceptable risks if deployed in time-critical applications without incorporating hybrid symbolic reasoning modules. <div>
arXiv:2511.10654v1 Announce Type: new 
Abstract: When deploying LLMs in agentic architectures requiring real-time decisions under temporal constraints, we assume they reliably determine whether action windows remain open or have closed. This assumption is untested. We characterize temporal constraint processing across eight production-scale models (2.8-8B parameters) using deadline detection tasks, revealing systematic deployment risks: bimodal performance distribution (models achieve either 95% or 50% accuracy), extreme prompt brittleness (30-60 percentage point swings from formatting changes alone), and systematic action bias (100% false positive rates in failing models). Parameter count shows no correlation with capability in this range-a 3.8B model matches 7B models while other 7B models fail completely. Fine-tuning on 200 synthetic examples improves models with partial capability by 12-37 percentage points. We demonstrate that temporal constraint satisfaction cannot be reliably learned through next-token prediction on natural language, even with targeted fine-tuning. This capability requires architectural mechanisms for: (1) continuous temporal state representation, (2) explicit constraint checking separate from linguistic pattern matching, (3) systematic compositional reasoning over temporal relations. Current autoregressive architectures lack these mechanisms. Deploying such systems in time-critical applications without hybrid architectures incorporating symbolic reasoning modules represents unacceptable risk.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment</title>
<link>https://arxiv.org/abs/2511.10655</link>
<guid>https://arxiv.org/abs/2511.10655</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral Neuro-Symbolic Reasoning, transformer-based node merging, sentence-level entailment validation, external knowledge graphs, scalable reasoning<br /><br />Summary:<br /><br />This report presents key enhancements to the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by integrating three semantically grounded preprocessing steps. First, it leverages transformer-based node merging using contextual embeddings such as Sentence-BERT and SimCSE to efficiently reduce redundancy within reasoning graphs. Second, it employs sentence-level entailment validation through pretrained Natural Language Inference (NLI) classifiers like RoBERTa and DeBERTa to improve the quality of graph edges. Third, the framework incorporates alignment with external knowledge graphs such as ConceptNet and Wikidata to supplement missing contextual information. These improvements collectively boost graph fidelity without modifying the underlying spectral reasoning pipeline. Experimental validation on benchmarks including ProofWriter, EntailmentBank, and CLUTRR demonstrates consistent accuracy improvements of up to 3.8%, better generalization to adversarial inputs, and a reduction in inference noise. A notable contribution is performing semantic and symbolic graph enhancements entirely upstream of spectral inference, avoiding the computational cost of quadratic attention mechanisms. Overall, these modular, semantically informed preprocessing strategies yield a robust, interpretable, and scalable reasoning system well-suited for real-world and open-domain applications. <div>
arXiv:2511.10655v1 Announce Type: new 
Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference Orchestrator: Prompt-Aware Multi-Objective Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10656</link>
<guid>https://arxiv.org/abs/2511.10656</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-objective alignment, preference adapter, prompt-specific weights, reward models<br /><br />Summary:  
This paper addresses the challenge of aligning Large Language Models (LLMs) with multiple human preferences across various objectives simultaneously. Existing approaches rely on manually set preference weights, which are difficult for users to specify accurately and often result in inefficient training due to exploring irrelevant preference combinations. To overcome these issues, the authors propose PRO (PReference Orchestrator), a novel framework featuring a lightweight preference adapter that automatically infers prompt-specific preference weights during training and deployment. The adapter learns appropriate weights by training on normalized reward scores from multiple reward models evaluating preferred responses, capturing effective balance across objectives for each prompt. The paper also provides theoretical analysis demonstrating that the prompt-aware preference mechanism outperforms fixed preference weights in multi-objective alignment scenarios. Extensive experiments across different tasks confirm the effectiveness of PRO compared to existing multi-objective alignment methods, showcasing improved performance and training efficiency. This approach simplifies user involvement and enhances adaptability of LLMs to diverse preferences without manual weight tuning. <div>
arXiv:2511.10656v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, aligning these models with varying human preferences across multiple objectives remains a significant challenge in practical deployments. Existing multi-objective alignment methods rely on manually specified preference weights, which not only burden users with difficult preference specification tasks but also lead to suboptimal training efficiency due to exploration of irrelevant preference combinations. To alleviate these issues, we propose a novel framework named PRO, i.e., PReference Orchestrator, which features a lightweight preference adapter that automatically infers prompt-specific preference weights during both training and deployment phases. Specifically, the adapter automatically learns appropriate preference weights for each prompt by training on normalized reward scores from multiple reward models for preferred responses, which inherently reflect effective preference balances across objectives. Additionally, We provide theoretical analysis proving that our prompt-aware preference mechanism achieves superior performance compared to fixed preference weights in multi-objective alignment scenarios. Extensive experiments across multiple tasks demonstrate the effectiveness of our method over existing multi-objective alignment approaches.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Representation Learning via Self-supervision</title>
<link>https://arxiv.org/abs/2511.10657</link>
<guid>https://arxiv.org/abs/2511.10657</guid>
<content:encoded><![CDATA[
<div> Keywords: contrastive learning, patent embeddings, section-based augmentation, intra-document views, prior-art retrieval<br /><br />Summary: This paper introduces a novel contrastive learning framework designed specifically for patent embeddings by utilizing multiple views derived from different sections within the same patent document. The authors identify a patent-specific limitation in SimCSE-style dropout augmentation, which tends to produce embeddings that are overly uniform and lack semantic cohesion. To overcome this issue, they propose section-based augmentation, where distinct sections such as the abstract, claims, and background are treated as complementary perspectives. This approach incorporates natural semantic and structural diversity, reducing over-dispersion and preserving both the global structure and local continuity of embeddings. Evaluated on large-scale benchmarks, the fully self-supervised method achieves performance comparable to or better than baselines supervised by citation and IPC data in tasks like prior-art retrieval and classification, without relying on potentially incomplete or brittle annotations. Additionally, the analysis reveals that different patent sections serve specialized purposes: claims and summaries particularly enhance retrieval tasks, whereas background sections improve classification accuracy. These findings underscore the advantage of leveraging the intrinsic discourse structure of patents, demonstrating that exploiting intra-document views is a scalable and generalizable strategy for effective patent understanding. <div>
arXiv:2511.10657v1 Announce Type: new 
Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Open-Weight Large Language Models for Structured Data Extraction from Narrative Medical Reports Across Multiple Use Cases and Languages</title>
<link>https://arxiv.org/abs/2511.10658</link>
<guid>https://arxiv.org/abs/2511.10658</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, clinical reports, structured information extraction, prompting strategies, multi-institutional study<br /><br />Summary:<br /><br />This study evaluates 15 open-weight large language models (LLMs) on extracting structured information from pathology and radiology reports. The evaluation covers six clinical use cases: colorectal liver metastases, liver tumors, neurodegenerative diseases, soft-tissue tumors, melanomas, and sarcomas across three different institutes in the Netherlands, UK, and Czech Republic. Both general-purpose and medical-specialized LLMs of varying sizes were tested. Six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance metrics appropriate to each task were used, and results were analyzed using consensus rank aggregation and linear mixed-effects models. Top-ranked models achieved macro-average scores close to the level of inter-rater agreement across all tasks. Interestingly, small-to-medium general-purpose models performed on par with large models, while tiny and specialized models lagged behind. Prompt graph and few-shot prompting improved performance by approximately 13%. The study found that task-specific factors such as complexity and annotation variability influenced model performance more than model size or prompting technique. Overall, the findings demonstrate that open-weight LLMs offer a scalable and effective tool for extracting structured clinical data across various diseases, languages, and institutions. <div>
arXiv:2511.10658v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to extract structured information from free-text clinical records, but prior work often focuses on single tasks, limited models, and English-language reports. We evaluated 15 open-weight LLMs on pathology and radiology reports across six use cases, colorectal liver metastases, liver tumours, neurodegenerative diseases, soft-tissue tumours, melanomas, and sarcomas, at three institutes in the Netherlands, UK, and Czech Republic. Models included general-purpose and medical-specialised LLMs of various sizes, and six prompting strategies were compared: zero-shot, one-shot, few-shot, chain-of-thought, self-consistency, and prompt graph. Performance was assessed using task-appropriate metrics, with consensus rank aggregation and linear mixed-effects models quantifying variance. Top-ranked models achieved macro-average scores close to inter-rater agreement across tasks. Small-to-medium general-purpose models performed comparably to large models, while tiny and specialised models performed worse. Prompt graph and few-shot prompting improved performance by ~13%. Task-specific factors, including variable complexity and annotation variability, influenced results more than model size or prompting strategy. These findings show that open-weight LLMs can extract structured data from clinical reports across diseases, languages, and institutions, offering a scalable approach for clinical data curation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information Extraction From Fiscal Documents Using LLMs</title>
<link>https://arxiv.org/abs/2511.10659</link>
<guid>https://arxiv.org/abs/2511.10659</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fiscal documents, hierarchical tables, data extraction, validation  

<br /><br />Summary:  
This paper explores the underutilized potential of Large Language Models (LLMs) to process complex, hierarchical tabular data found in multi-page government fiscal documents. The authors introduce a novel multi-stage pipeline designed specifically for extracting structured data from large PDF documents, exemplified by annual fiscal reports from the State of Karnataka, India, totaling over 200 pages. A key innovation is leveraging the inherent hierarchical structure of fiscal tables—consisting of subtotals and totals at multiple levels—to perform multi-level validation checks that improve extraction accuracy. This approach addresses a notable challenge of traditional OCR methods, which struggle to verify the accuracy of numerical data extraction. The pipeline incorporates domain knowledge, sequential contextual understanding, and algorithmic validation, allowing LLMs not only to interpret table data but also to recognize and utilize document-specific structural hierarchies. The result is a robust, scalable method that effectively converts complex PDF fiscal disclosures into clean, research-ready databases. The implementation demonstrates promise for broader application, especially in developing countries, where digitizing and structuring government financial data can support transparency and research. Overall, the work highlights the expanding capabilities of LLMs in handling structured data extraction tasks beyond text comprehension. <div>
arXiv:2511.10659v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in text comprehension, but their ability to process complex, hierarchical tabular data remains underexplored. We present a novel approach to extracting structured data from multi-page government fiscal documents using LLM-based techniques. Applied to annual fiscal documents from the State of Karnataka in India (200+ pages), our method achieves high accuracy through a multi-stage pipeline that leverages domain knowledge, sequential context, and algorithmic validation. A large challenge with traditional OCR methods is the inability to verify the accurate extraction of numbers. When applied to fiscal data, the inherent structure of fiscal tables, with totals at each level of the hierarchy, allows for robust internal validation of the extracted data. We use these hierarchical relationships to create multi-level validation checks. We demonstrate that LLMs can read tables and also process document-specific structural hierarchies, offering a scalable process for converting PDF-based fiscal disclosures into research-ready databases. Our implementation shows promise for broader applications across developing country contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Steering for Lossless Text Compression via Weighted Product of Experts</title>
<link>https://arxiv.org/abs/2511.10660</link>
<guid>https://arxiv.org/abs/2511.10660</guid>
<content:encoded><![CDATA[
<div> Keywords: lossless compression, neural compressors, universal compressor, weighted product of experts, text compression<br /><br />Summary: This paper addresses the challenge of improving lossless text compression by combining traditional universal compressors with neural language models. Traditional compressors like gzip are fast and general but typically yield suboptimal compression rates compared to neural compressors, which leverage extensive training data to better model distributions. However, neural compressors often fail to generalize well to unseen or out-of-distribution data, limiting their practical use. To overcome this, the authors propose a novel test-time steering method using a weighted product of experts (wPoE) framework that adaptively fuses a universal compression model with a pretrained neural language model during inference. This approach ensures the combined compression rate is at least as good as the best performing individual model without the need for any additional fine-tuning. The method is compatible with any autoregressive language model, making it broadly applicable. Extensive experiments demonstrate that their framework consistently improves text compression performance across diverse datasets and distribution shifts. Overall, the proposed wPoE-based test-time steering offers a practical, versatile, and effective solution for enhancing lossless text compression by leveraging the complementary strengths of universal and neural compressors. <div>
arXiv:2511.10660v1 Announce Type: new 
Abstract: Lossless compression techniques are crucial in an era of rapidly growing data. Traditional universal compressors like gzip offer low computational overhead, high speed, and broad applicability across data distributions. However, they often lead to worse compression rates than modern neural compressors, which leverage large-scale training data to model data distributions more effectively. Despite their advantages, neural compressors struggle to generalize to unseen data. To address this limitation, we propose a novel framework that performs Test-Time Steering via a Weighted Product of Experts (wPoE). At inference, our method adaptively combines a universal compression model with a pretrained neural language model, ensuring the compression rate is at least as good as that of the best individual model. Extensive experiments demonstrate that our approach improves the performance of text compression without requiring fine-tuning. Furthermore, it seamlessly integrates with any autoregressive language model, providing a practical solution for enhancing text compression across diverse data distributions.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Evaluation of Large Language Model Behavior</title>
<link>https://arxiv.org/abs/2511.10661</link>
<guid>https://arxiv.org/abs/2511.10661</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, Bayesian approach, uncertainty quantification, adversarial inputs<br /><br />Summary: This paper addresses the need for rigorous evaluation methods of text generation systems based on large language models (LLMs), focusing on issues such as harmful output generation and sensitivity to adversarial prompts. Traditional evaluation methods typically use curated benchmark prompt sets with binary outcomes (e.g., harmful vs. non-harmful) and aggregate these results without considering statistical uncertainty. The authors propose a Bayesian framework to quantify uncertainty in binary evaluation metrics, which accounts for the probabilistic nature of LLM-generated text. Two case studies demonstrate the approach: first, measuring refusal rates when LLMs respond to adversarially designed harmful prompts; second, assessing pairwise preferences between two LLMs on open-ended interactive dialogue tasks. The Bayesian method offers useful insights by providing credible intervals and uncertainty estimates, improving the interpretability of LLM evaluation results. This work highlights the importance of incorporating uncertainty quantification in LLM behavior assessment to produce more reliable and informative evaluation outcomes for developers and researchers. <div>
arXiv:2511.10661v1 Announce Type: new 
Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Modern Large Language Models on Low-Resource and Morphologically Rich Languages:A Cross-Lingual Benchmark Across Cantonese, Japanese, and Turkish</title>
<link>https://arxiv.org/abs/2511.10664</link>
<guid>https://arxiv.org/abs/2511.10664</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, cross-lingual benchmark, Cantonese, Japanese, Turkish

<br /><br />Summary:  
This paper evaluates seven state-of-the-art large language models (LLMs)—GPT-4o, GPT-4, Claude 3.5 Sonnet, LLaMA 3.1, Mistral Large 2, LLaMA-2 Chat 13B, and Mistral 7B Instruct—on a novel cross-lingual benchmark including Cantonese, Japanese, and Turkish. The benchmark covers four tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. The evaluation combines human assessments of fluency, factual accuracy, and cultural appropriateness with automated metrics such as BLEU and ROUGE. Results indicate that the largest proprietary models (GPT-4o, GPT-4, Claude 3.5) outperform others across tasks and languages, with GPT-4o showing strong multilingual and cross-lingual capabilities. Claude 3.5 Sonnet demonstrates competitive knowledge and reasoning accuracy. Despite this, all models exhibit challenges addressing language-specific issues, including Turkish’s complex agglutinative morphology and Cantonese colloquialisms. Smaller open-source models lag behind markedly in fluency and accuracy, underscoring disparities in resource availability. The paper provides extensive quantitative results and qualitative error analyses, highlighting the need for improved cultural and linguistic generalization in LLMs. Finally, the benchmark and evaluation data have been publicly released to support reproducibility and future research in this area. <div>
arXiv:2511.10664v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive results in high-resource languages like English, yet their effectiveness in low-resource and morphologically rich languages remains underexplored. In this paper, we present a comprehensive evaluation of seven cutting-edge LLMs -- including GPT-4o, GPT-4, Claude~3.5~Sonnet, LLaMA~3.1, Mistral~Large~2, LLaMA-2~Chat~13B, and Mistral~7B~Instruct -- on a new cross-lingual benchmark covering \textbf{Cantonese, Japanese, and Turkish}. Our benchmark spans four diverse tasks: open-domain question answering, document summarization, English-to-X translation, and culturally grounded dialogue. We combine \textbf{human evaluations} (rating fluency, factual accuracy, and cultural appropriateness) with automated metrics (e.g., BLEU, ROUGE) to assess model performance.
  Our results reveal that while the largest proprietary models (GPT-4o, GPT-4, Claude~3.5) generally lead across languages and tasks, significant gaps persist in culturally nuanced understanding and morphological generalization. Notably, GPT-4o demonstrates robust multilingual performance even on cross-lingual tasks, and Claude~3.5~Sonnet achieves competitive accuracy on knowledge and reasoning benchmarks. However, all models struggle to some extent with the unique linguistic challenges of each language, such as Turkish agglutinative morphology and Cantonese colloquialisms. Smaller open-source models (LLaMA-2~13B, Mistral~7B) lag substantially in fluency and accuracy, highlighting the resource disparity. We provide detailed quantitative results, qualitative error analysis, and discuss implications for developing more culturally aware and linguistically generalizable LLMs. Our benchmark and evaluation data are released to foster reproducibility and further research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title>
<link>https://arxiv.org/abs/2511.10665</link>
<guid>https://arxiv.org/abs/2511.10665</guid>
<content:encoded><![CDATA[
<div> Keywords: guard models, semantic robustness, paraphrase consistency, model calibration, skew-aware aggregation<br /><br />Summary:<br />Guard models play a vital role in ensuring the safety of large language models (LLMs), but their vulnerability to superficial linguistic changes undermines their reliability. This study reveals that even meaning-preserving paraphrases can cause significant fluctuations in guard model safety scores, indicating a lack of true semantic understanding. To tackle this, the authors introduce a self-supervised training framework designed to enhance semantic robustness by leveraging paraphrase sets. A key innovation is the use of a novel skew-aware aggregation strategy to compute robust targets for enforcing consistent predictions, as conventional methods like mean and median were found to potentially worsen safety outcomes. The approach was tested on six open-source guard models, resulting in a roughly 58% reduction in semantic variability across paraphrases and an average improvement of about 2.5% in benchmark accuracy. Additionally, the method generalizes effectively to previously unseen stylistic variations. An important finding is the discovered bidirectional relationship between model calibration and consistency; robustness training improved calibration by up to 40%. Overall, this work underscores the significance of prioritizing semantic consistency as a core training objective and offers a practical, scalable approach for developing more reliable and robust guard models to enhance LLM safety. <div>
arXiv:2511.10665v1 Announce Type: new 
Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLM Understanding via Structured Tabular Decision Simulations</title>
<link>https://arxiv.org/abs/2511.10667</link>
<guid>https://arxiv.org/abs/2511.10667</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Understanding, Structured Tabular Decision Simulations, Decision Factors, Evaluation Frameworks  

<br /><br />Summary:  
Large language models (LLMs) often deliver high predictive accuracy but this does not guarantee genuine understanding comparable to human expertise. True understanding in LLMs involves making consistent and well-founded decisions across multiple instances and diverse domains by relying on domain-relevant decision factors. The study introduces Structured Tabular Decision Simulations (STaDS), a new benchmark suite designed to evaluate LLMs as if they were professionals undergoing structured decision-making exams. STaDS defines understanding as the ability to identify and use the correct decision factors, which directly influence outcomes within a domain. This framework assesses understanding through three components: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) correct reliance on decision factors. Testing 9 state-of-the-art LLMs over 15 diverse decision-making settings revealed that most models struggle to maintain consistently high accuracy across domains. Moreover, models sometimes produce accurate answers without globally faithful reasoning, showing frequent mismatches between their stated rationales and the underlying decision factors driving their predictions. These insights emphasize the necessity for evaluation protocols that measure global-level understanding and suggest moving beyond accuracy-based metrics to develop frameworks that genuinely enhance LLMs’ decision-making understanding. <div>
arXiv:2511.10667v1 Announce Type: new 
Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI</title>
<link>https://arxiv.org/abs/2511.10669</link>
<guid>https://arxiv.org/abs/2511.10669</guid>
<content:encoded><![CDATA[
<div> Keywords: cochlear implants, sensorineural hearing loss, deep transfer learning, language development prediction, machine learning<br /><br />Summary:<br /><br />This study investigates the prediction of spoken language outcomes in children with severe-to-profound bilateral sensorineural hearing loss (SNHL) who receive cochlear implants (CI). Traditional predictors such as age at implantation and residual hearing fail to reliably forecast language development variability among individuals. Researchers compared traditional machine learning (ML) techniques with deep transfer learning (DTL) algorithms that incorporate brain neuroanatomic features to classify children as high or low improvers in post-CI spoken language development. The study involved 278 implanted children from three centers. The DTL models utilized a bilinear attention-based fusion strategy, achieving an accuracy of 92.39%, sensitivity of 91.22%, specificity of 93.56%, and an area under the curve (AUC) of 0.977, significantly outperforming traditional ML models on all metrics. The superior performance of DTL models is attributed to their ability to directly capture discriminative and task-specific information through representation learning. These findings demonstrate the feasibility of using a single DTL prediction model to support language outcome predictions across CI programs globally, offering a promising tool to improve clinical decision-making and individualized intervention planning for children undergoing cochlear implantation. <div>
arXiv:2511.10669v1 Announce Type: new 
Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Fine-Grained Code-Switch Speech Translation with Semantic Space Alignment</title>
<link>https://arxiv.org/abs/2511.10670</link>
<guid>https://arxiv.org/abs/2511.10670</guid>
<content:encoded><![CDATA[
<div> Code-switching, speech translation, mixture of experts, large language models, multi-stage training<br /><br />Summary:<br /><br />This paper addresses the challenges of code-switching (CS) speech translation, which involves translating speech containing multiple languages into a single target language. The difficulties stem from complex semantic modeling and a lack of CS training data. To overcome these, the authors propose enhancing large language models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert focuses on the semantic subspace of a particular language for fine-grained feature modeling. They introduce a multi-stage training approach leveraging readily available monolingual automatic speech recognition (ASR) and monolingual speech translation (ST) datasets to improve speech-text alignment and translation ability. The training uses a combination of language-specific loss and intra-group load balancing loss, guiding the MoE projector to allocate tokens efficiently among experts both across and within groups. To handle data distribution shifts across training stages and improve adaptation to code-switching scenarios, a transition loss is applied to smooth transitions between datasets. Experiments on standard datasets demonstrate that the method effectively improves CS speech translation and generalizes well across different settings. This work provides a novel and efficient solution to semantic modeling and data scarcity problems in code-switching speech translation. <div>
arXiv:2511.10670v1 Announce Type: new 
Abstract: Code-switching (CS) speech translation (ST) refers to translating speech that alternates between two or more languages into a target language text, which poses significant challenges due to the complexity of semantic modeling and the scarcity of CS data. Previous studies tend to rely on the model itself to implicitly learn semantic modeling during training, and resort to inefficient and costly manual annotations for these two challenges. To mitigate these limitations, we propose enhancing Large Language Models (LLMs) with a Mixture of Experts (MoE) speech projector, where each expert specializes in the semantic subspace of a specific language, enabling fine-grained modeling of speech features. Additionally, we introduce a multi-stage training paradigm that utilizes readily available monolingual automatic speech recognition (ASR) and monolingual ST data, facilitating speech-text alignment and improving translation capabilities. During training, we leverage a combination of language-specific loss and intra-group load balancing loss to guide the MoE speech projector in efficiently allocating tokens to the appropriate experts, across expert groups and within each group, respectively. To bridge the data gap across different training stages and improve adaptation to the CS scenario, we further employ a transition loss, enabling smooth transitions of data between stages, to effectively address the scarcity of high-quality CS speech translation data. Extensive experiments on widely used datasets demonstrate the effectiveness and generality of our approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Visual Factualization: Factual Anchor-Based Finetuning for Enhancing MLLM Factual Consistency</title>
<link>https://arxiv.org/abs/2511.10671</link>
<guid>https://arxiv.org/abs/2511.10671</guid>
<content:encoded><![CDATA[
<div> Visual Hallucination, Multimodal Large Language Models, Grounded Visual Factualization, Factual Consistency Loss, Data Augmentation<br /><br />Summary:  
This paper addresses the problem of visual hallucination in Multimodal Large Language Models (MLLMs), where models generate details that are inconsistent with the actual image content, reducing their reliability. To tackle this, the authors propose Grounded Visual Factualization (GVF) Finetuning, a novel method designed to systematically improve visual factual consistency in MLLMs. GVF Finetuning integrates explicit factual information through three main strategies: (1) Factual Anchor Data Augmentation, which enhances training data by including structured factual anchors and counter-factual prompts to guide the model; (2) Fact-Aware Instruction Tuning, where explicit factual cues are embedded into the training instructions to strengthen the model’s grounding; and (3) A Factual Consistency Loss function that specifically penalizes the model for generating factually inaccurate information. The method was tested on LLaVA-1.5-13B and demonstrated significant improvement over standard fine-tuning methods on the VHTest benchmark in both open-ended and yes/no question formats, indicating better visual factual accuracy. Additionally, GVF Finetuning maintained or slightly enhanced performance on general multimodal benchmarks such as MME and POPE, showing that it reduces hallucinations without sacrificing the model’s broader understanding and reasoning capabilities. <div>
arXiv:2511.10671v1 Announce Type: new 
Abstract: Visual hallucination, where Multimodal Large Language Models fabricate details inconsistent with image content, critically undermines their reliability. Existing fine-tuning methods offer limited improvement, failing to deeply intervene in factual reasoning. This paper introduces Grounded Visual Factualization (GVF) Finetuning, a novel approach to systematically enhance MLLM visual factual consistency. GVF integrates explicit factual signals via three core mechanisms: Factual Anchor Data Augmentation, enriching training data with structured factual anchors and counter-factual prompts; Fact-Aware Instruction Tuning, embedding these cues into explicit instructions; and a Factual Consistency Loss function, specifically penalizing factual inaccuracies. Evaluated on LLaVA-1.5-13B, GVF Finetuning significantly outperforms standard fine-tuning on the VHTest benchmark for both Open-Ended Question (OEQ) and Yes/No Question (YNQ) formats. Crucially, GVF maintains or even slightly improves performance on general multimodal benchmarks like MME and POPE, demonstrating effective mitigation of visual hallucinations without compromising general understanding and reasoning abilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models in materials science and the need for open-source approaches</title>
<link>https://arxiv.org/abs/2511.10673</link>
<guid>https://arxiv.org/abs/2511.10673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, materials science, predictive modelling, multi-agent systems, open-source AI<br /><br />Summary:<br /><br />This review discusses the transformative impact of large language models (LLMs) on materials science, focusing on their application across the materials discovery pipeline. First, LLMs have shown great capability in mining scientific literature by extracting critical information such as synthesis conditions from vast text data, enabling more efficient knowledge retrieval. Second, they contribute to predictive modelling by learning complex structure-property relationships, which advances the understanding and design of new materials. Third, LLMs facilitate multi-agent experimental systems by coordinating agentic frameworks that integrate computational tools with laboratory automation, thereby accelerating experimental workflows. The review also emphasizes that despite much progress relying on closed-source commercial LLMs, open-source alternatives are now achieving comparable performance. These open-source models provide advantages in transparency, reproducibility, cost-effectiveness, and data privacy. Finally, as open-source models continue to improve, the authors advocate for their broader adoption to develop accessible, flexible, and community-driven AI platforms that can democratize and enhance scientific discovery in materials science. <div>
arXiv:2511.10673v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly transforming materials science. This review examines recent LLM applications across the materials discovery pipeline, focusing on three key areas: mining scientific literature , predictive modelling, and multi-agent experimental systems. We highlight how LLMs extract valuable information such as synthesis conditions from text, learn structure-property relationships, and can coordinate agentic systems integrating computational tools and laboratory automation. While progress has been largely dependent on closed-source commercial models, our benchmark results demonstrate that open-source alternatives can match performance while offering greater transparency, reproducibility, cost-effectiveness, and data privacy. As open-source models continue to improve, we advocate their broader adoption to build accessible, flexible, and community-driven AI platforms for scientific discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10674</link>
<guid>https://arxiv.org/abs/2511.10674</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.10674v1

Keywords: Large Language Models, text-to-SQL, continual learning, human feedback, structured memory<br /><br />Summary:<br /><br />1. This paper addresses the challenge that Large Language Models (LLMs) face when generating SQL queries from natural language, particularly due to database-specific schemas and tacit domain knowledge.  
2. The authors propose a continual learning framework where the model improves through natural language feedback provided by humans, refining queries iteratively.  
3. Knowledge gained from feedback is distilled and stored in structured memory, allowing the agent to reuse this information in future tasks, enhancing learning efficiency.  
4. Various agent architectures are explored with differences in how they capture and retrieve past experiences, focusing on memory-augmentation.  
5. Experimental results on the BIRD benchmark’s development set demonstrate that memory-augmented agents, especially the Procedural Agent, significantly improve SQL execution accuracy and reduce errors by leveraging human-in-the-loop feedback.  
6. The study highlights how converting tacit human expertise into reusable knowledge enables more adaptive and domain-aware text-to-SQL systems that continuously improve through human interaction, paving the way for future advancements in this area. <div>
arXiv:2511.10674v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn to Select: Exploring Label Distribution Divergence for In-Context Demonstration Selection in Text Classification</title>
<link>https://arxiv.org/abs/2511.10675</link>
<guid>https://arxiv.org/abs/2511.10675</guid>
<content:encoded><![CDATA[
<div> In-context learning, Demonstration selection, Label distribution, Small language model, Text classification<br /><br />Summary:<br /><br />1. This paper addresses the challenge of selecting effective in-context demonstrations for text classification tasks using large language models (LLMs), highlighting that existing methods focus mainly on semantic similarity and neglect label distribution alignment.<br /><br />2. The authors propose a novel two-stage demonstration selection approach called TopK + Label Distribution Divergence (L2D). This method employs a fine-tuned BERT-like small language model (SLM) to estimate label distributions for both test inputs and candidate demonstrations.<br /><br />3. By calculating the divergence between these label distributions, L2D selects demonstrations that are not only semantically close but also aligned in label distribution with the test inputs, aiming to enhance LLM performance in in-context learning.<br /><br />4. Extensive experiments were conducted across seven different text classification benchmarks, showing that L2D consistently outperforms previously established demonstration selection strategies.<br /><br />5. Further analysis demonstrates a positive correlation between the performance of LLMs in in-context learning and the accuracy of the small language models used to estimate label distributions, suggesting that SLM quality plays a crucial role in the effectiveness of demonstration selection. <div>
arXiv:2511.10675v1 Announce Type: new 
Abstract: In-context learning (ICL) for text classification, which uses a few input-label demonstrations to describe a task, has demonstrated impressive performance on large language models (LLMs). However, the selection of in-context demonstrations plays a crucial role and can significantly affect LLMs' performance. Most existing demonstration selection methods primarily focus on semantic similarity between test inputs and demonstrations, often overlooking the importance of label distribution alignment. To address this limitation, we propose a two-stage demonstration selection method, TopK + Label Distribution Divergence (L2D), which leverages a fine-tuned BERT-like small language model (SLM) to generate label distributions and calculate their divergence for both test inputs and candidate demonstrations. This enables the selection of demonstrations that are not only semantically similar but also aligned in label distribution with the test input. Extensive experiments across seven text classification benchmarks show that our method consistently outperforms previous demonstration selection strategies. Further analysis reveals a positive correlation between the performance of LLMs and the accuracy of the underlying SLMs used for label distribution estimation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-Attention Expert Prediction and Prefetching for Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2511.10676</link>
<guid>https://arxiv.org/abs/2511.10676</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Large Language Models, Expert Prediction, Pre-attention, Lightweight Prefetching  

<br /><br />Summary:  
This paper addresses the challenge of improving expert prefetching in Mixture-of-Experts (MoE) Large Language Models (LLMs), which activate only a subset of experts to scale models efficiently while keeping inference costs low. Traditional expert prediction methods rely on activations from the previous layer for expert selection, leading to lower accuracy and leaving the first layer unoptimized. Moreover, approaches that use complex layers or separate networks for prediction add substantial computational overhead. The authors propose a novel pre-attention expert prediction technique that leverages activations before the attention block within the same layer, using two simple linear functions and a ranking-aware loss for accurate expert ranking prediction. This method exploits the ranking-preserving nature of certain LLM functions, enabling lightweight and precise expert prefetching, including at the first layer. Experimental results demonstrate that this approach achieves significantly higher prediction accuracy—93.03% on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE—representing about a 15% absolute improvement over state-of-the-art methods. The proposed pre-attention expert routers thus offer an effective and computationally efficient solution for enhancing MoE model inference speed and accuracy. <div>
arXiv:2511.10676v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) efficiently scale-up the model while keeping relatively low inference cost. As MoE models only activate part of the experts, related work has proposed expert prediction and caching methods to prefetch the experts for faster inference. However, existing approaches utilize the activations from the previous layer for prediction, incurring low accuracy and leave the first layer unoptimized. Applying complex layers or even training standalone networks for better prediction introduces high computation overhead. In this paper, we propose pre-attention expert prediction to achieve accurate and lightweight expert prefetching. The key insight is that some functions in LLMs are ranking-preserving, indicating that matching the ranking of selected experts using simple linear functions is possible. Therefore, we utilize the activations before the attention block in the same layer with 2 linear functions and ranking-aware loss to achieve accurate prediction, which also supports prefetching in the first layer. Our lightweight, pre-attention expert routers achieve 93.03% accuracy on DeepSeek V2 Lite, 94.69% on Qwen3-30B, and 97.62% on Phi-mini-MoE, showing about 15% improvement on absolute accuracy over the state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI</title>
<link>https://arxiv.org/abs/2511.10684</link>
<guid>https://arxiv.org/abs/2511.10684</guid>
<content:encoded><![CDATA[
<div> Keywords: Life Cycle Assessment, climate change, large language models, environmental impact, SpiderGen  

<br /><br />Summary:  
The paper addresses the critical issue of climate change driven by greenhouse gas emissions, with a focus on consumer products' lifecycle emissions. It emphasizes the importance of Life Cycle Assessments (LCAs) which detail the production, use, and disposal processes of products to estimate environmental impact. The authors introduce SpiderGen, a workflow that utilizes large language models (LLMs) to merge traditional LCA taxonomy and methodology with advanced reasoning and world knowledge. SpiderGen produces procedural information necessary for conducting LCAs. The system's output is evaluated against real-world LCA documents, achieving an F1-Score of 62% over 10 samples, indicating mostly accurate or minor errors. Errors are mainly due to variability in process detail and differing scope in auxiliary processes included. SpiderGen surpasses baseline methods such as chain-of-thought and one-shot prompting in performance. A notable advantage of SpiderGen is its cost and time efficiency, generating LCA data in under 10 minutes for less than $1, compared to traditional LCAs costing up to $25,000 and requiring up to 21-person days. This demonstrates significant potential for reducing effort and expense in assessing carbon footprints of consumer goods. <div>
arXiv:2511.10684v1 Announce Type: new 
Abstract: Investigating the effects of climate change and global warming caused by GHG emissions have been a primary concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate the procedural information used for LCA. We additionally evaluate the output of SpiderGen using real-world LCA documents as ground-truth. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 62% across 10 sample data points. We observe that the remaining missed processes and hallucinated errors occur primarily due to differences in detail between LCA documents, as well as differences in the "scope" of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen's potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A methodological analysis of prompt perturbations and their effect on attack success rates</title>
<link>https://arxiv.org/abs/2511.10686</link>
<guid>https://arxiv.org/abs/2511.10686</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, alignment methods, prompt attacks, Attack Success Rate, statistical analysis<br /><br />Summary:<br /><br />1. The study investigates the impact of different alignment methods on how Large Language Models (LLMs) respond to prompt attacks aimed at eliciting inappropriate content. <br /><br />2. The authors focus on three primary alignment techniques used in open-source models: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF).<br /><br />3. A systematic and statistical analysis is conducted to measure how sensitive the Attack Success Rate (ASR) is when small modifications are introduced to malicious prompts.<br /><br />4. Results reveal that even slight changes in prompts can significantly influence the ASR, making models more or less vulnerable depending on the alignment method.<br /><br />5. The study highlights that relying solely on existing attack benchmarks is insufficient for uncovering all vulnerabilities, thus emphasizing the need for more comprehensive and statistically-based evaluations of model security across different alignment strategies. <div>
arXiv:2511.10686v1 Announce Type: new 
Abstract: This work aims to investigate how different Large Language Models (LLMs) alignment methods affect the models' responses to prompt attacks. We selected open source models based on the most common alignment methods, namely, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning with Human Feedback (RLHF). We conducted a systematic analysis using statistical methods to verify how sensitive the Attack Success Rate (ASR) is when we apply variations to prompts designed to elicit inappropriate content from LLMs. Our results show that even small prompt modifications can significantly change the Attack Success Rate (ASR) according to the statistical tests we run, making the models more or less susceptible to types of attack. Critically, our results demonstrate that running existing 'attack benchmarks' alone may not be sufficient to elicit all possible vulnerabilities of both models and alignment methods. This paper thus contributes to ongoing efforts on model attack evaluation by means of systematic and statistically-based analyses of the different alignment methods and how sensitive their ASR is to prompt variation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling and Predicting Multi-Turn Answer Instability in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10688</link>
<guid>https://arxiv.org/abs/2511.10688</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, robustness, multi-turn prompts, Markov chains, linear probes  

<br /><br />Summary: This paper investigates the robustness of large language models (LLMs) in multi-turn interactive settings by applying simple follow-up prompts to observe how model answers change over consecutive turns. The authors find that repeated triggering of models with prompts such as "Think again" causes a significant decline in accuracy, quantifying approximately a 10% drop for Gemini 1.5 Flash across nine turns and a 7.5% drop for Claude 3.5 Haiku when combined with semantically reworded questions. To better understand accuracy dynamics over turns, the study models these changes using Markov chains, which effectively predict accuracy probabilities and allow estimation of the stationary (long-run) accuracy. This stationary accuracy is found to be roughly 8% lower than the initial accuracy for Gemini 1.5 Flash, indicating notable fragility in model performance under repeated questioning. Furthermore, the research explores the model’s hidden states and demonstrates that linear probes can predict future answer changes, suggesting potential for early intervention. The authors propose stationary accuracy as a principled robustness metric tailored for interactive LLM applications. They emphasize that addressing the exposed instability and answer volatility is crucial for reliable deployment of LLMs in high-stakes and conversational environments where consistent reasoning beyond initial correctness is imperative. <div>
arXiv:2511.10688v1 Announce Type: new 
Abstract: As large language models (LLMs) are adopted in an increasingly wide range of applications, user-model interactions have grown in both frequency and scale. Consequently, research has focused on evaluating the robustness of LLMs, an essential quality for real-world tasks. In this paper, we employ simple multi-turn follow-up prompts to evaluate models' answer changes, model accuracy dynamics across turns with Markov chains, and examine whether linear probes can predict these changes. Our results show significant vulnerabilities in LLM robustness: a simple "Think again" prompt led to an approximate 10% accuracy drop for Gemini 1.5 Flash over nine turns, while combining this prompt with a semantically equivalent reworded question caused a 7.5% drop for Claude 3.5 Haiku. Additionally, we find that model accuracy across turns can be effectively modeled using Markov chains, enabling the prediction of accuracy probabilities over time. This allows for estimation of the model's stationary (long-run) accuracy, which we find to be on average approximately 8% lower than its first-turn accuracy for Gemini 1.5 Flash. Our results from a model's hidden states also reveal evidence that linear probes can help predict future answer changes. Together, these results establish stationary accuracy as a principled robustness metric for interactive settings and expose the fragility of models under repeated questioning. Addressing this instability will be essential for deploying LLMs in high-stakes and interactive settings where consistent reasoning is as important as initial accuracy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equilibrium Dynamics and Mitigation of Gender Bias in Synthetically Generated Data</title>
<link>https://arxiv.org/abs/2511.10689</link>
<guid>https://arxiv.org/abs/2511.10689</guid>
<content:encoded><![CDATA[
<div> Keywords: recursive prompting, gender bias, synthetic dataset generation, mitigation strategies, semantic similarity<br /><br />Summary: This study explores the dynamics of gender bias in large language models during recursive text generation across three generations. The research uses three evaluation frameworks—rule-based pattern matching, embedding-based semantic similarity, and downstream task performance—to analyze bias evolution. Experiments were conducted with three initial bias levels of 0.1, 0.3, and 0.6, revealing that bias does not simply amplify monotonically but instead moves toward an equilibrium reflecting the model’s inherent bias. Specifically, low initial bias tends to increase by about 36%, while high initial bias decreases by roughly 26% over generations. The study also evaluates four mitigation methods, highlighting contrastive augmentation, which creates gender-swapped data variants. This approach significantly reduces downstream task bias by 98.8% for low initial bias scenarios and achieves an average reduction of 91% overall. Interestingly, contrastive augmentation yields higher bias scores when measured by embedding-based semantic similarity metrics, indicating a disconnect between these metrics and actual behavioral fairness outcomes. These findings emphasize the necessity for multidimensional evaluation frameworks in responsible synthetic data generation to capture nuanced bias behavior and ensure effective fairness mitigation strategies in language models. <div>
arXiv:2511.10689v1 Announce Type: new 
Abstract: Recursive prompting with large language models enables scalable synthetic dataset generation but introduces the risk of bias amplification. We investigate gender bias dynamics across three generations of recursive text generation using three complementary evaluation frameworks: rule-based pattern matching, embedding-based semantic similarity, and downstream task performance. Experiments with three initial bias levels (0.1, 0.3, 0.6) and four mitigation strategies reveal equilibrium dynamics rather than monotonic amplification. The low initial bias amplifies toward the model's inherent bias level (+36%), whereas the high initial bias decays toward it (-26%). Among mitigation methods, contrastive augmentation, which introduces gender-swapped variants, achieves significant downstream bias reduction (98.8% for low initial bias and 91% on average) despite producing higher embedding-based bias scores. This paradox demonstrates that semantic similarity metrics may diverge from behavioral fairness outcomes, highlighting the need for multidimensional evaluation in responsible synthetic data generation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saying the Unsaid: Revealing the Hidden Language of Multimodal Systems Through Telephone Games</title>
<link>https://arxiv.org/abs/2511.10690</link>
<guid>https://arxiv.org/abs/2511.10690</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal systems, hidden language, preference bias, telephone game, concept connections  

<br /><br />Summary:  
This paper investigates the hidden language of closed-source multimodal systems by analyzing their inherent preference bias when compressing input images into texts and reconstructing them back into images. During this process, the systems introduce shifts in output that disrupt original input concept co-occurrence, revealing underlying biases. To explore these biases, the authors employ a multi-round "telephone game" method, which involves iterative transformations to observe how concept co-occurrence frequencies evolve, thereby quantitatively measuring concept connection strength in the systems' understanding. The study introduces Telescope, a large dataset with over 10,000 concept pairs designed to support the telephone game framework. By running multiple rounds of the telephone game, the approach is scalable at test time and enables the construction of a global map of concept connections. This map helps identify training-inherited preference biases, track progress in generalization capabilities, and reveal more stable linkages among fragile concept pairs. Furthermore, the researchers leverage Reasoning Large Language Models (Reasoning-LLMs) to detect unexpected relationships between concepts that transcend simple textual or visual similarity, offering insights into how multimodal systems internally simulate and comprehend the world. Overall, the work sheds new light on interpretability and control in multimodal AI systems, laying groundwork for future research. <div>
arXiv:2511.10690v1 Announce Type: new 
Abstract: Recent closed-source multimodal systems have made great advances, but their hidden language for understanding the world remains opaque because of their black-box architectures. In this paper, we use the systems' preference bias to study their hidden language: During the process of compressing the input images (typically containing multiple concepts) into texts and then reconstructing them into images, the systems' inherent preference bias introduces specific shifts in the outputs, disrupting the original input concept co-occurrence. We employ the multi-round "telephone game" to strategically leverage this bias. By observing the co-occurrence frequencies of concepts in telephone games, we quantitatively investigate the concept connection strength in the understanding of multimodal systems, i.e., "hidden language." We also contribute Telescope, a dataset of 10,000+ concept pairs, as the database of our telephone game framework. Our telephone game is test-time scalable: By iteratively running telephone games, we can construct a global map of concept connections in multimodal systems' understanding. Here we can identify preference bias inherited from training, assess generalization capability advancement, and discover more stable pathways for fragile concept connections. Furthermore, we use Reasoning-LLMs to uncover unexpected concept relationships that transcend textual and visual similarities, inferring how multimodal systems understand and simulate the world. This study offers a new perspective on the hidden language of multimodal systems and lays the foundation for future research on the interpretability and controllability of multimodal systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating from Benign to Dynamic Adversarial: A Squid Game for Large Language Models</title>
<link>https://arxiv.org/abs/2511.10691</link>
<guid>https://arxiv.org/abs/2511.10691</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dynamic evaluation, adversarial environment, resource constraints, benchmark contamination<br /><br />Summary:<br /><br />1. Existing benchmarks for large language models (LLMs) struggle to keep up with rapid model development, and potential data contamination challenges their reliability, as models may have seen test questions during training.<br />2. Most benchmarks assume benign, resource-rich conditions, overlooking how LLMs perform under constrained or adversarial settings.<br />3. The paper introduces Squid Game: a novel, dynamic, adversarial evaluation environment designed to test LLMs under resource limitations and asymmetric information through interactive gameplay against other LLMs.<br />4. Squid Game features six elimination-style levels focusing on diverse abilities including instruction-following, coding, reasoning, planning, and safety alignment.<br />5. Evaluations of over 50 LLMs reveal a generational phase transition in performance within the same model lineage, and evidence that some models exploit speculative shortcuts to win, indicating risks of higher-level contamination in static benchmarks.<br />6. Correlation analyses comparing Squid Game with existing benchmarks highlight that dynamic, interactive evaluation offers complementary insights to static tests.<br />7. The paper pledges to publicly release the code and data, enabling further research in dynamic behavioral evaluation of general LLMs. <div>
arXiv:2511.10691v1 Announce Type: new 
Abstract: Contemporary benchmarks are struggling to keep pace with the development of large language models (LLMs). Although they are indispensable to evaluate model performance on various tasks, it is uncertain whether the models trained on Internet data have genuinely learned how to solve problems or merely seen the questions before. This potential data contamination issue presents a fundamental challenge to establishing trustworthy evaluation frameworks. Meanwhile, existing benchmarks predominantly assume benign, resource-rich settings, leaving the behavior of LLMs under pressure unexplored. In this paper, we introduce Squid Game, a dynamic and adversarial evaluation environment with resource-constrained and asymmetric information settings elaborated to evaluate LLMs through interactive gameplay against other LLM opponents. Notably, Squid Game consists of six elimination-style levels, focusing on multi-faceted abilities, such as instruction-following, code, reasoning, planning, and safety alignment. We evaluate over 50 LLMs on Squid Game, presenting the largest behavioral evaluation study of general LLMs on dynamic adversarial scenarios. We observe a clear generational phase transition on performance in the same model lineage and find evidence that some models resort to speculative shortcuts to win the game, indicating the possibility of higher-level evaluation paradigm contamination in static benchmarks. Furthermore, we compare prominent LLM benchmarks and Squid Game with correlation analyses, highlighting that dynamic evaluation can serve as a complementary part for static evaluations. The code and data will be released in the future.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do AI Voices Learn Social Nuances? A Case of Politeness and Speech Rate</title>
<link>https://arxiv.org/abs/2511.10693</link>
<guid>https://arxiv.org/abs/2511.10693</guid>
<content:encoded><![CDATA[
<div> Keywords: voice-based AI, politeness, speech rate reduction, text-to-speech, social conventions<br /><br />Summary:<br />1. This study examines whether advanced text-to-speech (TTS) systems can implicitly learn to convey politeness, a subtle human social cue, through changes in speech rate. <br />2. Researchers tested 22 synthetic voices from two popular AI platforms, AI Studio and OpenAI, by having them read the same script under "polite and formal" and "casual and informal" conditions. <br />3. The main measurement was speech duration, assessing if polite prompts led to slower speech compared to casual ones. <br />4. Results showed that all AI voices produced significantly slower speech in the polite condition, with very large effect sizes across both platforms. This was statistically significant for all AI Studio voices and most OpenAI voices. <br />5. The findings indicate that voice-based AI can internalize and reproduce nuanced psychological markers of human communication, such as politeness through speech rate modulation, supporting the view of AI as emerging social actors capable of reinforcing social norms. <div>
arXiv:2511.10693v1 Announce Type: new 
Abstract: Voice-based artificial intelligence is increasingly expected to adhere to human social conventions, but can it learn implicit cues that are not explicitly programmed? This study investigates whether state-of-the-art text-to-speech systems have internalized the human tendency to reduce speech rate to convey politeness - a non-obvious prosodic marker. We prompted 22 synthetic voices from two leading AI platforms (AI Studio and OpenAI) to read a fixed script under both "polite and formal" and "casual and informal" conditions and measured the resulting speech duration. Across both AI platforms, the polite prompt produced slower speech than the casual prompt with very large effect sizes, an effect that was statistically significant for all of AI Studio's voices and for a large majority of OpenAI's voices. These results demonstrate that AI can implicitly learn and replicate psychological nuances of human communication, highlighting its emerging role as a social actor capable of reinforcing human social norms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where does an LLM begin computing an instruction?</title>
<link>https://arxiv.org/abs/2511.10694</link>
<guid>https://arxiv.org/abs/2511.10694</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction following, activation patching, Llama models, layer-wise flip rate, onset

<br /><br />Summary:  
This paper investigates the process of instruction following within language models by examining where along a model’s layer stack the transition from reading an instruction to executing it occurs. The authors introduce three simple datasets—Key-Value, Quote Attribution, and Letter Selection—and combine them in two-hop compositions to analyze complex task execution. They apply activation patching on minimal-contrast prompt pairs to measure a layer-wise flip rate, which quantifies how substituting specific residual activations affects the predicted answer at different layers. Across various models in the Llama family, the study identifies a distinct inflection point called "onset," marking where interventions in earlier layers meaningfully alter outcomes but become largely ineffective beyond this point. Notably, multi-hop task compositions show a similar onset location, suggesting consistency in where instruction following initiates regardless of task complexity. The work presents a straightforward and reproducible method to pinpoint the starting layer of instruction following, enabling comparison across different tasks and model sizes, thereby contributing to a better understanding of how language models internally process instructions and execute tasks. <div>
arXiv:2511.10694v1 Announce Type: new 
Abstract: Following an instruction involves distinct sub-processes, such as reading content, reading the instruction, executing it, and producing an answer. We ask where, along the layer stack, instruction following begins, the point where reading gives way to doing. We introduce three simple datasets (Key-Value, Quote Attribution, Letter Selection) and two hop compositions of these tasks. Using activation patching on minimal-contrast prompt pairs, we measure a layer-wise flip rate that indicates when substituting selected residual activations changes the predicted answer. Across models in the Llama family, we observe an inflection point, which we term onset, where interventions that change predictions before this point become largely ineffective afterward. Multi-hop compositions show a similar onset location. These results provide a simple, replicable way to locate where instruction following begins and to compare this location across tasks and model sizes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations</title>
<link>https://arxiv.org/abs/2511.10695</link>
<guid>https://arxiv.org/abs/2511.10695</guid>
<content:encoded><![CDATA[
<div> nation-level bias, Large Language Models, International Relations, UNSC, debiasing framework<br /><br />Summary:<br /><br />This paper investigates nation-level biases in Large Language Models (LLMs) focusing on their applications in International Relations (IR). Using historical data from the United Nations Security Council (UNSC), the authors create a bias evaluation framework with three tests aimed at detecting biases toward the five permanent UNSC members. Results reveal that while common bias trends exist, such as favoring Western nations and disfavoring Russia, the bias patterns differ across LLMs. Furthermore, biases within a single model vary in direction and strength depending on the evaluation context, indicating that LLM biases are multidimensional and task-dependent. The study also finds that models exhibiting stronger reasoning capabilities tend to have reduced bias and improved performance. To address these biases, the authors propose a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experimentation shows this approach successfully decreases nation-level bias and enhances performance, particularly in models like GPT-4o-mini and LLama-3.3-70B. The paper concludes by highlighting the importance of simultaneously evaluating nation-level bias and model performance when deploying LLMs in the IR domain, stressing that addressing bias is critical for fair and accurate use in this sensitive context. <div>
arXiv:2511.10695v1 Announce Type: new 
Abstract: This paper systematically examines nation-level biases exhibited by Large Language Models (LLMs) within the domain of International Relations (IR). Leveraging historical records from the United Nations Security Council (UNSC), we developed a bias evaluation framework comprising three distinct tests to explore nation-level bias in various LLMs, with a particular focus on the five permanent members of the UNSC. Experimental results show that, even with the general bias patterns across models (e.g., favorable biases toward the western nations, and unfavorable biases toward Russia), these still vary based on the LLM. Notably, even within the same LLM, the direction and magnitude of bias for a nation change depending on the evaluation context. This observation suggests that LLM biases are fundamentally multidimensional, varying across models and tasks. We also observe that models with stronger reasoning abilities show reduced bias and better performance. Building on this finding, we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques. Experiments show it effectively reduces nation-level bias, and improves performance, particularly in GPT-4o-mini and LLama-3.3-70B. Our findings emphasize the need to assess nation-level bias alongside performance when applying LLMs in the IR domain.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\pi$-Attention: Periodic Sparse Transformers for Efficient Long-Context Modeling</title>
<link>https://arxiv.org/abs/2511.10696</link>
<guid>https://arxiv.org/abs/2511.10696</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, Sparse Attention, \PiAttention, Long-Range Modeling, Adaptive Fusion Gate<br /><br />Summary:  
Transformers have fundamentally advanced natural language processing but face efficiency challenges due to their quadratic complexity relative to sequence length. Existing sparse attention mechanisms like RingAttention address this by limiting attention to local neighborhoods, which reduces computational cost but restricts the receptive field and adaptability. The proposed \PiAttention model introduces a periodic sparse Transformer architecture that decomposes attention into three components: ring-local neighborhoods, deterministic \(\pi\)-stride skips, and an adaptive fusion gate. This design ensures predictable coverage over distant tokens while maintaining a sparse attention pattern with per-layer complexity linear in the input sequence length. Theoretical analysis shows that \(\PiAttention\) achieves a receptive field growth rate of \(\mathcal{O}(kL + \pi \log L)\), improving upon the \(\mathcal{O}(kL)\) rate of RingAttention, where \(k\) is the window size, \(\pi\) is the skip period, and \(L\) is the sequence length. Experimentally, \(\PiAttention\) performs at or above the quality of dense attention models on tasks including language modeling, retrieval, and vision-language applications, achieving an 8.3% lower perplexity than RingAttention while using half the GPU resources for the same context length. Ablations and visualizations highlight the crucial roles of periodic skips, adaptive fusion, and coordinated sparsity at the attention head level in enabling efficient modeling of long contexts. <div>
arXiv:2511.10696v1 Announce Type: new 
Abstract: Transformers have revolutionized natural language processing, but their quadratic complexity with respect to sequence length remains a fundamental bottleneck for long-range modeling. While sparse attention mechanisms like RingAttention reduce computational costs by restricting attention to local neighborhoods, they suffer from limited receptive fields and lack of adaptability. We present \PiAttention, a periodic sparse Transformer that factorizes attention into ring-local neighborhoods, deterministic $\pi$-stride skips, and an adaptive fusion gate. The periodic structure provides predictable coverage of distant tokens, while the sparse footprint keeps the per-layer complexity linear in context length. We prove that \PiAttention achieves $\mathcal{O}(kL + \pi \log L)$ receptive field growth compared to $\mathcal{O}(kL)$ for RingAttention, where $k$ is the local window size, $\pi$ is the skip period, and $L$ is the sequence length. Extensive experiments on language modeling, retrieval, and vision-language tasks demonstrate that \PiAttention matches or surpasses dense attention quality with 8.3\% lower perplexity than RingAttention while using 50\% fewer GPUs for the same context length. Our detailed ablations and visualizations reveal the importance of periodic skips, adaptive fusion, and head-level sparsity coordination for efficient long-context modeling.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs</title>
<link>https://arxiv.org/abs/2511.10768</link>
<guid>https://arxiv.org/abs/2511.10768</guid>
<content:encoded><![CDATA[
<div> Keywords: medical text summarization, faithfulness, large language models, consumer health questions, LLaMA-2-7B<br /><br />Summary: The article addresses the challenge of generating faithful summaries of consumer health questions (CHQs), which is crucial for effective healthcare communication. It proposes a novel framework that integrates TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to improve faithfulness in medical summarization. The authors fine-tuned the LLaMA-2-7B model on two datasets, MeQSum (English) and BanglaCHQ-Summ (Bangla), and demonstrated consistent improvements in both quality metrics (ROUGE, BERTScore, readability) and faithfulness metrics (SummaC, AlignScore). The model outperformed zero-shot baselines and prior systems, highlighting the benefits of domain fine-tuning and combining extraction with entity recognition techniques. Human evaluation confirmed that over 80% of generated summaries retained critical medical information, underscoring the approach’s reliability. The work emphasizes faithfulness as an essential dimension for trustworthy medical summarization and shows the potential for safer deployment of LLMs in healthcare applications, ultimately aiming to reduce risks posed by unfaithful summaries that could mislead patients or healthcare providers. <div>
arXiv:2511.10768v1 Announce Type: new 
Abstract: Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English</title>
<link>https://arxiv.org/abs/2511.10780</link>
<guid>https://arxiv.org/abs/2511.10780</guid>
<content:encoded><![CDATA[
<div> Tunisian Arabic, Speech Translation, Code-Switching, Dataset, TEDxTN<br /><br />Summary:<br /><br />1. The paper introduces TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset.<br /><br />2. The dataset was created to address the scarcity of resources for various Arabic dialects, specifically the Tunisian dialect.<br /><br />3. It comprises 108 TEDx talks totaling 25 hours of speech featuring code-switching and speakers from over 11 different Tunisian regions with diverse accents.<br /><br />4. The authors developed internal annotation guidelines and provide both these guidelines and the corpus to the public, allowing future expansions as new talks become available.<br /><br />5. Baseline results for Speech Recognition and Speech Translation are reported using multiple pre-trained and fine-tuned end-to-end models.<br /><br />6. TEDxTN stands as the first open-source, publicly accessible speech translation corpus capturing code-switching phenomena in the Tunisian dialect.<br /><br />7. This resource aims to motivate and facilitate further research in natural language processing for the Tunisian Arabic dialect. <div>
arXiv:2511.10780v1 Announce Type: new 
Abstract: In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sabi\'a: Um Chatbot de Intelig\^encia Artificial Generativa para Suporte no Dia a Dia do Ensino Superior</title>
<link>https://arxiv.org/abs/2511.10787</link>
<guid>https://arxiv.org/abs/2511.10787</guid>
<content:encoded><![CDATA[
<div> Keywords: chatbot, Generative AI, Retrieval-Augmented Generation, Gemini 2.0 Flash, Gemma 3n

<br /><br />Summary:  
Students often face challenges in accessing day-to-day academic information because it is scattered across multiple institutional documents and websites, leading to confusion and a lack of clarity about routine university matters. To address this problem, the project proposes the development of a chatbot that utilizes Generative Artificial Intelligence (GenAI) combined with Retrieval-Augmented Generation (RAG) techniques to streamline and simplify access to academic information. Various GenAI models were tested and evaluated using quality metrics and the LLM-as-a-Judge evaluation approach to determine their effectiveness. Among the tested models, Gemini 2.0 Flash was identified as a top performer due to its superior quality and speed, making it highly efficient for this application. In addition, Gemma 3n was noted for having good performance while being open-source, presenting an advantage for adaptability and transparency. The project’s approach demonstrates promising potential to reduce information fragmentation and improve students’ experience by providing a centralized, interactive platform for accessing university-related information. <div>
arXiv:2511.10787v1 Announce Type: new 
Abstract: Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation</title>
<link>https://arxiv.org/abs/2511.10819</link>
<guid>https://arxiv.org/abs/2511.10819</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, GPT-4o, automated grading, educational assessment, Computational Linguistics<br /><br />Summary:  
This study explores the use of a Large Language Model (LLM), specifically GPT-4o, for grading short-answer quizzes and project reports in a real undergraduate Computational Linguistics course. Data were collected from around 50 students over five quizzes, along with project reports submitted by 14 teams. The LLM-generated scores were compared with independent human evaluations conducted by the course teaching assistants (TAs). Results demonstrated a strong positive correlation between GPT-4o and human graders, reaching up to 0.98 for quiz assessments. The LLM achieved exact score agreement with human grading in 55% of quiz cases, indicating high reliability for structured responses. For project reports, while GPT-4o also aligned well overall with human assessments, there was some inconsistency in grading technical, open-ended answers, reflecting challenges in nuanced evaluation. The authors have released all code and sample data used in the study to encourage further research in the application of LLMs for automated grading. This work underscores both the promising potential and existing limitations of leveraging LLMs as tools for educational assessment in authentic academic environments. Ultimately, the study contributes valuable insights to the development of automated grading systems tailored to real-world classroom settings. <div>
arXiv:2511.10819v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing Multilingual Representations in LLMs with Cross-Layer Transcoders</title>
<link>https://arxiv.org/abs/2511.10840</link>
<guid>https://arxiv.org/abs/2511.10840</guid>
<content:encoded><![CDATA[
<div> Multilingual, Large Language Models, Pivot Language, Decoding, Attribution Analysis<br /><br />Summary: This paper investigates how multilingual large language models (LLMs) internally represent multiple languages and why performance often favors the dominant training language. The authors train several LLMs on different multilingual data mixtures and analyze their internal workings using cross-layer transcoders (CLT) and attribution graphs. They find strong evidence for a pivot language representation mechanism, where the model creates nearly identical representations across languages but performs language-specific decoding in later layers. Attribution analyses indicate that decoding depends partly on a small set of high-frequency language features in the final layers, which allow the model to linearly identify language identity from the earlier layers. By intervening on these features, the researchers can suppress one language and substitute another in the model’s output, demonstrating control over multilingual generation. They also explore how the dominant training language affects these internal mechanisms through attribution graphs and decoding pathways. The study emphasizes that understanding this pivot-language mechanism is vital for enhancing multilingual alignment and improving LLM performance across diverse languages. <div>
arXiv:2511.10840v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) can process many languages, yet how they internally represent this diversity remains unclear. Do they form shared multilingual representations with language-specific decoding, and if so, why does performance still favor the dominant training language? To address this, we train a series of LLMs on different mixtures of multilingual data and analyze their internal mechanisms using cross-layer transcoders (CLT) and attribution graphs. Our results provide strong evidence for pivot language representations: the model employs nearly identical representations across languages, while language-specific decoding emerges in later layers. Attribution analyses reveal that decoding relies in part on a small set of high-frequency language features in the final layers, which linearly read out language identity from the first layers in the model. By intervening on these features, we can suppress one language and substitute another in the model's outputs. Finally, we study how the dominant training language influences these mechanisms across attribution graphs and decoding pathways. We argue that understanding this pivot-language mechanism is crucial for improving multilingual alignment in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcing Stereotypes of Anger: Emotion AI on African American Vernacular English</title>
<link>https://arxiv.org/abs/2511.10846</link>
<guid>https://arxiv.org/abs/2511.10846</guid>
<content:encoded><![CDATA[
<div> Automated emotion detection, African American Vernacular English (AAVE), racial bias, emotion AI, culturally informed models  

<br /><br />Summary:  
1. Automated emotion detection models are widely used across diverse domains but often rely on annotations reflecting dominant cultural norms, limiting their effectiveness in recognizing emotions in dialects such as African American Vernacular English (AAVE).  
2. This study analyzes 2.7 million geo-tagged tweets from Los Angeles, assessing the extent of AAVE usage through computational approximations of dialectal features.  
3. A dataset of 875 tweets with varying AAVE density was annotated for emotion presence and intensity, with "silver" labels generated by African American, AAVE-fluent annotators to provide culturally informed ground truth.  
4. Popular emotion recognition models like GPT, BERT-based models, and SpanEmo demonstrate substantially higher false positive rates of anger detection on AAVE tweets compared to General American English (GAE), with rates more than doubling in some cases.  
5. Linear regression analyses show models and non-ingroup annotators correlate more with profanity-based AAVE features than ingroup annotators, highlighting bias.  
6. There is a measurable association between neighborhoods with higher African American populations and model predictions of increased anger and decreased joy, indicating reinforcement of racial stereotypes by emotion AI.  
7. The study underscores a critical safety concern in affective computing and calls for developing culturally and dialect-informed emotion detection systems to mitigate biased outcomes. <div>
arXiv:2511.10846v1 Announce Type: new 
Abstract: Automated emotion detection is widely used in applications ranging from well-being monitoring to high-stakes domains like mental health and hiring. However, models often rely on annotations that reflect dominant cultural norms, limiting model ability to recognize emotional expression in dialects often excluded from training data distributions, such as African American Vernacular English (AAVE). This study examines emotion recognition model performance on AAVE compared to General American English (GAE). We analyze 2.7 million tweets geo-tagged within Los Angeles. Texts are scored for strength of AAVE using computational approximations of dialect features. Annotations of emotion presence and intensity are collected on a dataset of 875 tweets with both high and low AAVE densities. To assess model accuracy on a task as subjective as emotion perception, we calculate community-informed "silver" labels where AAVE-dense tweets are labeled by African American, AAVE-fluent (ingroup) annotators. On our labeled sample, GPT and BERT-based models exhibit false positive prediction rates of anger on AAVE more than double than on GAE. SpanEmo, a popular text-based emotion model, increases false positive rates of anger from 25 percent on GAE to 60 percent on AAVE. Additionally, a series of linear regressions reveals that models and non-ingroup annotations are significantly more correlated with profanity-based AAVE features than ingroup annotations. Linking Census tract demographics, we observe that neighborhoods with higher proportions of African American residents are associated with higher predictions of anger (Pearson's correlation r = 0.27) and lower joy (r = -0.10). These results find an emergent safety issue of emotion AI reinforcing racial stereotypes through biased emotion classification. We emphasize the need for culturally and dialect-informed affective computing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.10850</link>
<guid>https://arxiv.org/abs/2511.10850</guid>
<content:encoded><![CDATA[
<div> Task arithmetic, parameter space alignment, Large Language Models, Grouped-Query Attention, SwiGLU<br /><br />Summary:<br /><br />1. The paper addresses the challenge of negative interference in task arithmetic, which hinders the effective transfer of skills between Large Language Models (LLMs) that have diverged during training.<br />2. The authors propose a novel approach that aligns the parameter spaces of different LLMs by leveraging the intrinsic permutation, rotation, and scaling symmetries present in Transformer architectures.<br />3. This alignment technique is adapted specifically for modern architectural components, including Grouped-Query Attention (GQA) and SwiGLU layers, using both weight-based and activation-based strategies.<br />4. By applying this alignment-first strategy, the study successfully transfers advanced reasoning capabilities to a model originally lacking reasoning skills.<br />5. Experimental results on challenging reasoning benchmarks demonstrate that the proposed method consistently outperforms traditional task arithmetic, offering a more effective way to merge and transfer specialized skills across evolving LLM families, which reduces redundant fine-tuning and enhances overall model adaptability. <div>
arXiv:2511.10850v1 Announce Type: new 
Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</title>
<link>https://arxiv.org/abs/2511.10871</link>
<guid>https://arxiv.org/abs/2511.10871</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM judgment, conversational framing, model conviction, social interactions, rebuttal perturbation<br /><br />Summary:<br /><br />1. The paper investigates how large language models (LLMs) perform when acting as judges in tasks requiring social or conversational judgment, shifting from direct factual queries to conversational contexts. <br /><br />2. It introduces an evaluation framework that contrasts model assessments of factual correctness against judgments of speaker correctness within minimal dialogues, effectively reframing the question from "Is this statement correct?" to "Is this speaker correct?". <br /><br />3. The study applies conversational pressure through a simple rebuttal phrase ("The previous answer is incorrect.") to evaluate the firmness of the model’s convictions across both direct and conversational scenarios. <br /><br />4. Results reveal varied responses among models: GPT-4o-mini displayed sycophantic tendencies under social framing, while Llama-8B-Instruct exhibited overly critical behavior, signaling that conversational context can significantly influence model judgment. <br /><br />5. On average, there was a 9.24% performance change across models, highlighting the importance of conversational framing as a key factor in LLM evaluations and proposing a reproducible method for assessing model conviction to advance trustworthy dialogue systems. <div>
arXiv:2511.10871v1 Announce Type: new 
Abstract: LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from "Is this statement correct?" to "Is this speaker correct?". Furthermore, we apply pressure in the form of a simple rebuttal ("The previous answer is incorrect.") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICX360: In-Context eXplainability 360 Toolkit</title>
<link>https://arxiv.org/abs/2511.10879</link>
<guid>https://arxiv.org/abs/2511.10879</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Explainability, In-Context Explainability 360, Black-box methods, White-box methods<br /><br />Summary: Large Language Models (LLMs) are increasingly integrated into various high-stakes applications such as summarizing meetings and assisting medical professionals. To enhance trust and transparency, it is essential to develop tools that explain LLM outputs, including responses, summaries, and lists. Addressing this need, the paper introduces In-Context Explainability 360 (ICX360), an open-source Python toolkit designed to explain LLM outputs by focusing on the user-provided context or prompts. ICX360 implements three recent explanation tools that use both black-box (perturbation-based) and white-box (gradient-based) methods to analyze LLM behavior. The toolkit aims to be accessible, providing quick-start guides and thorough tutorials to support diverse use cases such as retrieval-augmented generation, natural language generation, and even jailbreaking techniques. Hosted on GitHub by IBM, ICX360 intends to facilitate researchers and developers in better understanding and interpreting LLM decisions through contextual explanations, fostering safer and more interpretable AI applications. <div>
arXiv:2511.10879v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge</title>
<link>https://arxiv.org/abs/2511.10881</link>
<guid>https://arxiv.org/abs/2511.10881</guid>
<content:encoded><![CDATA[
<div> Negative bias, large language models, prompt format, parametric knowledge, mitigation<br /><br /><br />Summary: This paper investigates the phenomenon of negative bias in large language models (LLMs), where models tend to produce excessive negative responses in binary decision tasks such as yes-no questions. It reveals that this bias is influenced more by the prompt format than by the semantic content of negative responses, highlighting a format-level negative bias. To study this in detail, the authors propose a novel evaluation pipeline that categorizes model responses into three subsets based on the model's parametric knowledge: correct knowledge, incorrect knowledge, and insufficient relevant knowledge. Their analysis discovers a shortcut behavior where LLMs default to negative answers when they lack sufficient knowledge to respond accurately, thus contributing to negative bias. The study further explores how different prompting scenarios affect negative bias, showing that the inclusion of relevant contextual information and offering an "I don't know" response option tend to reduce the bias. Conversely, using chain-of-thought prompting often increases the tendency towards negative bias. Lastly, the research demonstrates that the degree and direction of negative bias vary depending on the prompt type. These insights provide a better understanding of negative bias and suggest strategies for mitigating it in LLMs. <div>
arXiv:2511.10881v1 Announce Type: new 
Abstract: Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an "I don't know" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking</title>
<link>https://arxiv.org/abs/2511.10887</link>
<guid>https://arxiv.org/abs/2511.10887</guid>
<content:encoded><![CDATA[
<div> Biomedical NER, Entity Linking, UMLS, Ontological Paths, Explainable Models<br /><br />Summary: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is limited by a fragmented data landscape, insufficient resources for building explainable models, and the shortcomings of semantically unaware evaluation metrics. To overcome these challenges, the authors introduce MedPath, a comprehensive large-scale and multi-domain biomedical EL dataset. MedPath integrates and builds upon nine existing expert-annotated EL datasets, providing a unified resource for research. In MedPath, all entities are normalized using the most recent version of the Unified Medical Language System (UMLS), ensuring standardization across datasets. Additionally, the dataset includes augmentations with mappings to 62 other biomedical vocabularies, greatly improving interoperability. Crucially, MedPath enriches entities with complete ontological paths, spanning from general to specific categories, across up to 11 biomedical vocabularies. This unique feature facilitates semantic-rich and interpretable EL system development by providing hierarchical context often missing in previous datasets. Overall, MedPath enables new avenues for research in biomedical natural language processing (NLP), supporting advancements in interoperable, explainable, and clinically relevant NLP models by providing richer semantic information and standardized entity linking benchmarks. <div>
arXiv:2511.10887v1 Announce Type: new 
Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10899</link>
<guid>https://arxiv.org/abs/2511.10899</guid>
<content:encoded><![CDATA[
<div> Keywords: Tool-Induced Myopia, Tool-augmented Language Models, Code Interpreter, reasoning degradation, PYMATH<br /><br />Summary:<br /><br />1. Tool-augmented Language Models (TaLMs) enhance problem-solving by invoking external tools but may produce correct answers without coherent reasoning, a failure mode termed Tool-Induced Myopia (TIM).<br />2. The study focuses on the Code Interpreter tool and uses PYMATH, a benchmark of 1,679 competition-level math problems where Python coding aids but does not fully solve.<br />3. A novel multi-dimensional evaluation suite quantifies reasoning quality, revealing that despite up to 19.3 percentage points improvement in final-answer accuracy, TaLMs show consistent reasoning degradation compared to non-tool models.<br />4. Increased frequency of tool use correlates with greater reasoning incoherence; TaLM errors shift from arithmetic mistakes to broader reasoning failures involving logic, assumptions, and creativity, with TIM present in approximately 55% of high-risk cases.<br />5. To address TIM, the authors propose a preference-optimization framework that realigns TaLMs to treat tools as assistive evidence, improving both accuracy and reasoning depth under tool use.<br /><br />Code and data for this study are publicly available at https://github.com/megagonlabs/TIM. <div>
arXiv:2511.10899v1 Announce Type: new 
Abstract: Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering</title>
<link>https://arxiv.org/abs/2511.10900</link>
<guid>https://arxiv.org/abs/2511.10900</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Question Answering, EMSQA Dataset, Expert-CoT, ExpertRAG<br /><br />Summary:<br /><br />1. Large language models (LLMs) have demonstrated potential in medical question answering but often lack integration of domain-specific expertise, such as clinical subject areas (e.g., trauma, airway) and certification levels (e.g., EMT, Paramedic), which are crucial in professional settings.<br /><br />2. Existing methods typically use general-purpose prompting or retrieval without leveraging structured medical context, limiting their effectiveness in critical, high-stakes environments.<br /><br />3. To address this, the EMSQA dataset was developed, comprising 24,300 multiple-choice questions spanning 10 clinical subject areas and 4 certification levels, supported by a curated knowledge base of 40,000 documents totaling 2 million tokens, aligned to relevant subject areas.<br /><br />4. The study introduces two novel approaches: Expert-CoT, a chain-of-thought prompting strategy conditioned on specific clinical subjects and certification levels, and ExpertRAG, a retrieval-augmented generation pipeline that grounds answers in subject-aligned documents and real patient data.<br /><br />5. Experimental results across 4 LLMs show Expert-CoT improves accuracy by up to 2.05% over standard CoT prompting, while combining Expert-CoT with ExpertRAG achieves up to a 4.59% accuracy gain over standard RAG baselines. Remarkably, 32-billion-parameter expertise-augmented LLMs successfully passed all computer-adaptive EMS certification simulation exams. <div>
arXiv:2511.10900v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions</title>
<link>https://arxiv.org/abs/2511.10902</link>
<guid>https://arxiv.org/abs/2511.10902</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal peer review, large language models, retrieval-augmented generation, academic workflows, actionable feedback  

<br /><br />Summary: This paper introduces an innovative web-based system designed to enhance academic peer review by integrating multimodal inputs, including both textual and visual data, leveraging the capabilities of large language models (LLMs). Unlike traditional peer review systems constrained to text-only inputs, the proposed framework utilizes multimodal LLMs to provide richer, context-aware feedback. To improve review quality and grounding, the system incorporates retrieval-augmented generation (RAG) based on extensive OpenReview datasets, ensuring that the generated reviews reflect community standards and relevant prior work. A notable feature is the conversion of generated reviews into structured, actionable to-do lists using the novel Action:Objective[#] format, enabling authors to clearly understand and track revision tasks. The platform offers seamless integration with existing academic writing tools, facilitating real-time, interactive feedback and revision monitoring. Experimental evaluations demonstrate that the system produces more comprehensive and expert-aligned review comments compared to baseline models lacking multimodal or retrieval enhancements. This approach advances scholarly assistance by fostering transparent, human-centered review simulations that better support manuscript improvement before submission. Overall, the work addresses key limitations in current peer review automation by combining multimodality, community-context grounding, and actionable guidance into a unified system. <div>
arXiv:2511.10902v1 Announce Type: new 
Abstract: While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy</title>
<link>https://arxiv.org/abs/2511.10903</link>
<guid>https://arxiv.org/abs/2511.10903</guid>
<content:encoded><![CDATA[
<div> Keywords: Bloom's Taxonomy, exam question classification, machine learning, data augmentation, large language models<br /><br />Summary:<br /><br />This paper investigates the automatic classification of exam questions and learning outcomes according to the six cognitive categories of Bloom's Taxonomy: Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation. A small labeled dataset of 600 sentences was used to train and evaluate multiple types of models, including traditional machine learning (Naive Bayes, Logistic Regression, SVM), recurrent neural networks (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT, RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Different preprocessing and data augmentation techniques were applied to improve performance, such as synonym replacement and word embeddings. Among traditional models, Support Vector Machines with data augmentation delivered the best results, achieving approximately 94% accuracy, recall, and F1 scores while exhibiting minimal overfitting. In contrast, RNN-based models and BERT showed significant overfitting issues, although RoBERTa initially resisted overfitting but deteriorated over longer training. Zero-shot evaluations of large language models revealed that OpenAI and Gemini outperformed their peers with about 72-73% accuracy and comparable F1 scores, despite not being fine-tuned on the dataset. The study highlights the difficulties of training complex deep learning models on limited data and stresses the effectiveness of careful augmentation and simpler algorithms for Bloom's Taxonomy classification tasks. <div>
arXiv:2511.10903v1 Announce Type: new 
Abstract: This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D</title>
<link>https://arxiv.org/abs/2511.10912</link>
<guid>https://arxiv.org/abs/2511.10912</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, rare disease diagnosis, narrative medical cases, House M.D. dataset, AI-assisted diagnosis

<br /><br />Summary:  
This study addresses the underexplored ability of large language models (LLMs) to diagnose rare diseases from narrative medical case descriptions. It introduces a novel dataset comprising 176 symptom-diagnosis pairs extracted from the TV series House M.D., which is validated as an educational tool for rare disease recognition. Four state-of-the-art LLMs—GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro—were evaluated using this dataset on diagnostic reasoning tasks based on narrative cases. The results showed considerable variation in model performance, with accuracy rates ranging from 16.48% to 38.64%. Notably, newer LLM generations achieved about 2.3 times better accuracy compared to earlier versions. Despite the overall challenges these models face in diagnosing rare diseases, the observed improvements across model architectures point to promising avenues for future development in AI-assisted diagnosis. The study establishes baseline performance metrics via an educationally validated benchmark and provides a publicly accessible evaluation framework aimed at advancing AI research in medical narrative diagnostic reasoning. <div>
arXiv:2511.10912v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology</title>
<link>https://arxiv.org/abs/2511.10930</link>
<guid>https://arxiv.org/abs/2511.10930</guid>
<content:encoded><![CDATA[
<div> Keywords: CardioEmbed, cardiology embeddings, contrastive learning, InfoNCE loss, semantic retrieval

<br /><br />Summary: This study addresses the gap in biomedical text embeddings for clinical cardiology by developing CardioEmbed, a domain-specialized embedding model trained on comprehensive cardiology textbooks rather than research abstracts. CardioEmbed is built on Qwen3-Embedding-8B and trained using contrastive learning with InfoNCE loss and in-batch negatives on a curated corpus of approximately 150,000 deduplicated sentences from seven authoritative cardiology textbooks. The model demonstrates a significant improvement by achieving 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, surpassing the current state-of-the-art medical embedding model, MedTE, by 15.94 percentage points. Additionally, CardioEmbed performs competitively on broader biomedical benchmarks within the MTEB framework, obtaining a BIOSSES Spearman correlation score of 0.77 and an NDCG@10 score of 0.61 on the SciFact dataset. These results highlight the effectiveness of domain-specialized training on comprehensive clinical textbooks to yield near-perfect cardiology semantic retrieval performance and robust generalizability to related biomedical domains. The study underscores the importance of leveraging domain-relevant clinical knowledge sources, such as textbooks, for improved embedding models in specialized medical fields like cardiology. <div>
arXiv:2511.10930v1 Announce Type: new 
Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains</title>
<link>https://arxiv.org/abs/2511.10984</link>
<guid>https://arxiv.org/abs/2511.10984</guid>
<content:encoded><![CDATA[
<div> Keywords: discourse-level translation, expert domains, Chinese-English translation, evaluation benchmark, Metric-S<br /><br />Summary:<br /><br />1. The paper identifies a critical gap in the evaluation of discourse-level translation within expert domains, highlighting that current methods mainly assess segment-level accuracy and fluency rather than discourse coherence and terminological precision. 2. To address this shortcoming, the authors introduce DiscoX, a novel benchmark consisting of 200 professionally curated, long-form texts (average length over 1700 tokens) from seven specialized domains, designed specifically for Chinese-English translation tasks. 3. Alongside DiscoX, the authors develop Metric-S, a reference-free automatic evaluation system that delivers fine-grained assessments across accuracy, fluency, and appropriateness, showing strong correlation with human judgments and outperforming existing metrics. 4. Experimental results reveal a significant performance gap between state-of-the-art large language models (LLMs) and human experts, underscoring the complexity and challenge of professional-grade discourse-level translation in expert domains. 5. The introduction of DiscoX and Metric-S provides a robust framework for more rigorous, discourse-aware evaluation, thereby facilitating future progress in machine translation driven by LLMs and addressing the needs of cross-lingual scholarly communication and knowledge dissemination. <div>
arXiv:2511.10984v1 Announce Type: new 
Abstract: The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets</title>
<link>https://arxiv.org/abs/2511.10985</link>
<guid>https://arxiv.org/abs/2511.10985</guid>
<content:encoded><![CDATA[
arXiv:2511.10985v1 Announce Type: new 
Abstract: Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Based Steering of Large Language Models for Diverse Structured Generation</title>
<link>https://arxiv.org/abs/2511.11018</link>
<guid>https://arxiv.org/abs/2511.11018</guid>
<content:encoded><![CDATA[
arXiv:2511.11018v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB</title>
<link>https://arxiv.org/abs/2511.11041</link>
<guid>https://arxiv.org/abs/2511.11041</guid>
<content:encoded><![CDATA[
arXiv:2511.11041v1 Announce Type: new 
Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + \mu$, where $\mu$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $\sigma$ on retrieval tasks, 3.1 $\sigma$ on classification tasks, and 0.8 $\sigma$ on other types of tasks. Renormalization has two variants: directly subtracting $\mu$ from $e$, or subtracting the projection of $e$ onto $\mu$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLMs Detect Their Own Hallucinations?</title>
<link>https://arxiv.org/abs/2511.11087</link>
<guid>https://arxiv.org/abs/2511.11087</guid>
<content:encoded><![CDATA[
arXiv:2511.11087v1 Announce Type: new 
Abstract: Large language models (LLMs) can generate fluent responses, but sometimes hallucinate facts. In this paper, we investigate whether LLMs can detect their own hallucinations. We formulate hallucination detection as a classification task of a sentence. We propose a framework for estimating LLMs' capability of hallucination detection and a classification method using Chain-of-Thought (CoT) to extract knowledge from their parameters. The experimental results indicated that GPT-$3.5$ Turbo with CoT detected $58.2\%$ of its own hallucinations. We concluded that LLMs with CoT can detect hallucinations if sufficient knowledge is contained in their parameters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysing Personal Attacks in U.S. Presidential Debates</title>
<link>https://arxiv.org/abs/2511.11108</link>
<guid>https://arxiv.org/abs/2511.11108</guid>
<content:encoded><![CDATA[
arXiv:2511.11108v1 Announce Type: new 
Abstract: Personal attacks have become a notable feature of U.S. presidential debates and play an important role in shaping public perception during elections. Detecting such attacks can improve transparency in political discourse and provide insights for journalists, analysts and the public. Advances in deep learning and transformer-based models, particularly BERT and large language models (LLMs) have created new opportunities for automated detection of harmful language. Motivated by these developments, we present a framework for analysing personal attacks in U.S. presidential debates. Our work involves manual annotation of debate transcripts across the 2016, 2020 and 2024 election cycles, followed by statistical and language-model based analysis. We investigate the potential of fine-tuned transformer models alongside general-purpose LLMs to detect personal attacks in formal political speech. This study demonstrates how task-specific adaptation of modern language models can contribute to a deeper understanding of political communication.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AV-Dialog: Spoken Dialogue Models with Audio-Visual Input</title>
<link>https://arxiv.org/abs/2511.11124</link>
<guid>https://arxiv.org/abs/2511.11124</guid>
<content:encoded><![CDATA[
arXiv:2511.11124v1 Announce Type: new 
Abstract: Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion</title>
<link>https://arxiv.org/abs/2511.11126</link>
<guid>https://arxiv.org/abs/2511.11126</guid>
<content:encoded><![CDATA[
arXiv:2511.11126v1 Announce Type: new 
Abstract: With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\% on MET-MEME and 3.4\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition</title>
<link>https://arxiv.org/abs/2511.11139</link>
<guid>https://arxiv.org/abs/2511.11139</guid>
<content:encoded><![CDATA[
arXiv:2511.11139v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title>
<link>https://arxiv.org/abs/2511.11141</link>
<guid>https://arxiv.org/abs/2511.11141</guid>
<content:encoded><![CDATA[
arXiv:2511.11141v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy</title>
<link>https://arxiv.org/abs/2511.11214</link>
<guid>https://arxiv.org/abs/2511.11214</guid>
<content:encoded><![CDATA[
arXiv:2511.11214v1 Announce Type: new 
Abstract: WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LANE: Lexical Adversarial Negative Examples for Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2511.11234</link>
<guid>https://arxiv.org/abs/2511.11234</guid>
<content:encoded><![CDATA[
arXiv:2511.11234v1 Announce Type: new 
Abstract: Fine-grained word meaning resolution remains a critical challenge for neural language models (NLMs) as they often overfit to global sentence representations, failing to capture local semantic details. We propose a novel adversarial training strategy, called LANE, to address this limitation by deliberately shifting the model's learning focus to the target word. This method generates challenging negative training examples through the selective marking of alternate words in the training set. The goal is to force the model to create a greater separability between same sentences with different marked words. Experimental results on lexical semantic change detection and word sense disambiguation benchmarks demonstrate that our approach yields more discriminative word representations, improving performance over standard contrastive learning baselines. We further provide qualitative analyses showing that the proposed negatives lead to representations that better capture subtle meaning differences even in challenging environments. Our method is model-agnostic and can be integrated into existing representation learning frameworks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement</title>
<link>https://arxiv.org/abs/2511.11258</link>
<guid>https://arxiv.org/abs/2511.11258</guid>
<content:encoded><![CDATA[
arXiv:2511.11258v1 Announce Type: new 
Abstract: The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference</title>
<link>https://arxiv.org/abs/2511.11306</link>
<guid>https://arxiv.org/abs/2511.11306</guid>
<content:encoded><![CDATA[
arXiv:2511.11306v1 Announce Type: new 
Abstract: Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>destroR: Attacking Transfer Models with Obfuscous Examples to Discard Perplexity</title>
<link>https://arxiv.org/abs/2511.11309</link>
<guid>https://arxiv.org/abs/2511.11309</guid>
<content:encoded><![CDATA[
arXiv:2511.11309v1 Announce Type: new 
Abstract: Advancements in Machine Learning & Neural Networks in recent years have led to widespread implementations of Natural Language Processing across a variety of fields with remarkable success, solving a wide range of complicated problems. However, recent research has shown that machine learning models may be vulnerable in a number of ways, putting both the models and the systems theyre used in at risk. In this paper, we intend to analyze and experiment with the best of existing adversarial attack recipes and create new ones. We concentrated on developing a novel adversarial attack strategy on current state-of-the-art machine learning models by producing ambiguous inputs for the models to confound them and then constructing the path to the future development of the robustness of the models. We will develop adversarial instances with maximum perplexity, utilizing machine learning and deep learning approaches in order to trick the models. In our attack recipe, we will analyze several datasets and focus on creating obfuscous adversary examples to put the models in a state of perplexity, and by including the Bangla Language in the field of adversarial attacks. We strictly uphold utility usage reduction and efficiency throughout our work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.11315</link>
<guid>https://arxiv.org/abs/2511.11315</guid>
<content:encoded><![CDATA[
arXiv:2511.11315v1 Announce Type: new 
Abstract: Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery</title>
<link>https://arxiv.org/abs/2511.11324</link>
<guid>https://arxiv.org/abs/2511.11324</guid>
<content:encoded><![CDATA[
arXiv:2511.11324v1 Announce Type: new 
Abstract: Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2511.11334</link>
<guid>https://arxiv.org/abs/2511.11334</guid>
<content:encoded><![CDATA[
arXiv:2511.11334v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text</title>
<link>https://arxiv.org/abs/2511.11340</link>
<guid>https://arxiv.org/abs/2511.11340</guid>
<content:encoded><![CDATA[
arXiv:2511.11340v1 Announce Type: new 
Abstract: The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studies with impossible languages falsify LMs as models of human language</title>
<link>https://arxiv.org/abs/2511.11389</link>
<guid>https://arxiv.org/abs/2511.11389</guid>
<content:encoded><![CDATA[
arXiv:2511.11389v1 Announce Type: new 
Abstract: According to Futrell and Mahowald [arXiv:2501.17047], both infants and language models (LMs) find attested languages easier to learn than impossible languages that have unnatural structures. We review the literature and show that LMs often learn attested and many impossible languages equally well. Difficult to learn impossible languages are simply more complex (or random). LMs are missing human inductive biases that support language acquisition.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajinBook: An open catalogue of digital world literature with likes</title>
<link>https://arxiv.org/abs/2511.11412</link>
<guid>https://arxiv.org/abs/2511.11412</guid>
<content:encoded><![CDATA[
arXiv:2511.11412v1 Announce Type: new 
Abstract: This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proactive Hearing Assistants that Isolate Egocentric Conversations</title>
<link>https://arxiv.org/abs/2511.11473</link>
<guid>https://arxiv.org/abs/2511.11473</guid>
<content:encoded><![CDATA[
arXiv:2511.11473v1 Announce Type: new 
Abstract: We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2511.11518</link>
<guid>https://arxiv.org/abs/2511.11518</guid>
<content:encoded><![CDATA[
arXiv:2511.11518v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRBench: Large-Scale Expert Rubrics for Evaluating High-Stakes Professional Reasoning</title>
<link>https://arxiv.org/abs/2511.11562</link>
<guid>https://arxiv.org/abs/2511.11562</guid>
<content:encoded><![CDATA[
arXiv:2511.11562v1 Announce Type: new 
Abstract: Frontier model progress is often measured by academic benchmarks, which offer a limited view of performance in real-world professional contexts. Existing evaluations often fail to assess open-ended, economically consequential tasks in high-stakes domains like Legal and Finance, where practical returns are paramount. To address this, we introduce Professional Reasoning Bench (PRBench), a realistic, open-ended, and difficult benchmark of real-world problems in Finance and Law. We open-source its 1,100 expert-authored tasks and 19,356 expert-curated criteria, making it, to our knowledge, the largest public, rubric-based benchmark for both legal and finance domains. We recruit 182 qualified professionals, holding JDs, CFAs, or 6+ years of experience, who contributed tasks inspired by their actual workflows. This process yields significant diversity, with tasks spanning 114 countries and 47 US jurisdictions. Our expert-curated rubrics are validated through a rigorous quality pipeline, including independent expert validation. Subsequent evaluation of 20 leading models reveals substantial room for improvement, with top scores of only 0.39 (Finance) and 0.37 (Legal) on our Hard subsets. We further catalog associated economic impacts of the prompts and analyze performance using human-annotated rubric categories. Our analysis shows that models with similar overall scores can diverge significantly on specific capabilities. Common failure modes include inaccurate judgments, a lack of process transparency and incomplete reasoning, highlighting critical gaps in their reliability for professional adoption.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Gets the Reward, Who Gets the Blame? Evaluation-Aligned Training Signals for Multi-LLM Agents</title>
<link>https://arxiv.org/abs/2511.10687</link>
<guid>https://arxiv.org/abs/2511.10687</guid>
<content:encoded><![CDATA[
arXiv:2511.10687v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) in multi-agent systems (MAS) have shown promise for complex tasks, yet current training methods lack principled ways to connect system-level evaluation with agent-level and message-level learning. We propose a theoretical framework that unifies cooperative game-theoretic attribution with process reward modeling to transform system evaluation into agent credit and then into response-level signals. Unlike prior approaches that rely only on attribution (e.g., Shapley) or step-level labels (e.g., PRM), our method produces local, signed, and credit-conserving signals. In success cases, Shapley-based credit assignment fairly allocates outcomes across agents and is refined into per-message rewards that promote cooperation while discouraging redundancy or sabotage. In failure cases, first-error localization yields repair-aware preferences that penalize harmful steps while rewarding corrective attempts. The resulting signals are bounded, cooperative, and directly compatible with reinforcement-based or preference-based post-training, providing a unified and auditable pathway from global evaluation to local supervision in LLM multi-agent training. Our contribution is conceptual: we present a theoretical foundation and training signals, leaving empirical validation for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents</title>
<link>https://arxiv.org/abs/2511.10705</link>
<guid>https://arxiv.org/abs/2511.10705</guid>
<content:encoded><![CDATA[
arXiv:2511.10705v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title>
<link>https://arxiv.org/abs/2511.10720</link>
<guid>https://arxiv.org/abs/2511.10720</guid>
<content:encoded><![CDATA[
arXiv:2511.10720v1 Announce Type: cross 
Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10788</link>
<guid>https://arxiv.org/abs/2511.10788</guid>
<content:encoded><![CDATA[
arXiv:2511.10788v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title>
<link>https://arxiv.org/abs/2511.10837</link>
<guid>https://arxiv.org/abs/2511.10837</guid>
<content:encoded><![CDATA[
arXiv:2511.10837v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2511.11066</link>
<guid>https://arxiv.org/abs/2511.11066</guid>
<content:encoded><![CDATA[
arXiv:2511.11066v1 Announce Type: cross 
Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation</title>
<link>https://arxiv.org/abs/2511.11104</link>
<guid>https://arxiv.org/abs/2511.11104</guid>
<content:encoded><![CDATA[
arXiv:2511.11104v1 Announce Type: cross 
Abstract: Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2511.11182</link>
<guid>https://arxiv.org/abs/2511.11182</guid>
<content:encoded><![CDATA[
arXiv:2511.11182v1 Announce Type: cross 
Abstract: Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Meaningful Units with Visually Grounded Semantics from Image Captions</title>
<link>https://arxiv.org/abs/2511.11262</link>
<guid>https://arxiv.org/abs/2511.11262</guid>
<content:encoded><![CDATA[
arXiv:2511.11262v1 Announce Type: cross 
Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQuaD: The Software Quality Dataset</title>
<link>https://arxiv.org/abs/2511.11265</link>
<guid>https://arxiv.org/abs/2511.11265</guid>
<content:encoded><![CDATA[
arXiv:2511.11265v1 Announce Type: cross 
Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-Aided State Estimation</title>
<link>https://arxiv.org/abs/2511.11285</link>
<guid>https://arxiv.org/abs/2511.11285</guid>
<content:encoded><![CDATA[
arXiv:2511.11285v1 Announce Type: cross 
Abstract: Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building the Web for Agents: A Declarative Framework for Agent-Web Interaction</title>
<link>https://arxiv.org/abs/2511.11287</link>
<guid>https://arxiv.org/abs/2511.11287</guid>
<content:encoded><![CDATA[
arXiv:2511.11287v1 Announce Type: cross 
Abstract: The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces  and  tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2511.11362</link>
<guid>https://arxiv.org/abs/2511.11362</guid>
<content:encoded><![CDATA[
arXiv:2511.11362v1 Announce Type: cross 
Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2511.11440</link>
<guid>https://arxiv.org/abs/2511.11440</guid>
<content:encoded><![CDATA[
arXiv:2511.11440v1 Announce Type: cross 
Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping</title>
<link>https://arxiv.org/abs/2511.11551</link>
<guid>https://arxiv.org/abs/2511.11551</guid>
<content:encoded><![CDATA[
arXiv:2511.11551v1 Announce Type: cross 
Abstract: The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining the alignment. For the pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding</title>
<link>https://arxiv.org/abs/2511.11552</link>
<guid>https://arxiv.org/abs/2511.11552</guid>
<content:encoded><![CDATA[
arXiv:2511.11552v1 Announce Type: cross 
Abstract: Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[
arXiv:2511.11571v1 Announce Type: cross 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying and Analyzing Performance-Critical Tokens in Large Language Models</title>
<link>https://arxiv.org/abs/2401.11323</link>
<guid>https://arxiv.org/abs/2401.11323</guid>
<content:encoded><![CDATA[
arXiv:2401.11323v4 Announce Type: replace 
Abstract: In-context learning (ICL) has emerged as an effective solution for few-shot learning with large language models (LLMs). However, how LLMs leverage demonstrations to specify a task and learn a corresponding computational function through ICL is underexplored. Drawing from the way humans learn from content-label mappings in demonstrations, we categorize the tokens in an ICL prompt into content, stopword, and template tokens. Our goal is to identify the types of tokens whose representations directly influence LLM's performance, a property we refer to as being performance-critical. By ablating representations from the attention of the test example, we find that the representations of informative content tokens have less influence on performance compared to template and stopword tokens, which contrasts with the human attention to informative words. We give evidence that the representations of performance-critical tokens aggregate information from the content tokens. Moreover, we demonstrate experimentally that lexical meaning, repetition, and structural cues are the main distinguishing characteristics of these tokens. Our work sheds light on how large language models learn to perform tasks from demonstrations and deepens our understanding of the roles different types of tokens play in large language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metric Learning Encoding Models: A Multivariate Framework for Interpreting Neural Representations</title>
<link>https://arxiv.org/abs/2402.11608</link>
<guid>https://arxiv.org/abs/2402.11608</guid>
<content:encoded><![CDATA[
arXiv:2402.11608v2 Announce Type: replace 
Abstract: Understanding how explicit theoretical features are encoded in opaque neural systems is a central challenge now common to neuroscience and AI. We introduce Metric Learning Encoding Models (MLEMs) to address this challenge most directly as a metric learning problem: we fit the distance in the space of theoretical features to match the distance in neural space. Our framework improves on univariate encoding and decoding methods by building on second-order isomorphism methods, such as Representational Similarity Analysis, and extends them by learning a metric that efficiently models feature as well as interactions between them. The effectiveness of MLEM is validated through two sets of simulations. First, MLEMs recover ground-truth importance features in synthetic datasets better than state-of-the-art methods, such as Feature Reweighted RSA (FR-RSA). Second, we deploy MLEMs on real language data, where they show stronger robustness to noise in calculating the importance of linguistic features (gender, tense, etc.). MLEMs are applicable to any domains where theoretical features can be identified, such as language, vision, audition, etc. We release optimized code applicable to measure feature importance in the representations of any artificial neural networks or empirical neural data at https://github.com/LouisJalouzot/MLEM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey in Characterization of Semantic Change</title>
<link>https://arxiv.org/abs/2402.19088</link>
<guid>https://arxiv.org/abs/2402.19088</guid>
<content:encoded><![CDATA[
arXiv:2402.19088v4 Announce Type: replace 
Abstract: Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change. This survey provides an understandable overview of existing approaches to the \textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation). We summarized the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Downstream Performance of Mixture-of-Experts Transformers via Weak Vanilla Transformers</title>
<link>https://arxiv.org/abs/2403.01994</link>
<guid>https://arxiv.org/abs/2403.01994</guid>
<content:encoded><![CDATA[
arXiv:2403.01994v2 Announce Type: replace 
Abstract: Recently, Mixture of Experts (MoE) Transformers have garnered increasing attention due to their advantages in model capacity and computational efficiency. However, studies have indicated that MoE Transformers underperform vanilla Transformers in many downstream tasks, significantly diminishing the practical value of MoE models. To explain this issue, we propose that the pre-training performance and transfer capability of a model are joint determinants of its downstream task performance. MoE models, in comparison to vanilla models, have poorer transfer capability, leading to their subpar performance in downstream tasks. To address this issue, we introduce the concept of transfer capability distillation, positing that although vanilla models have weaker performance, they are effective teachers of transfer capability. The MoE models guided by vanilla models can achieve both strong pre-training performance and transfer capability, ultimately enhancing their performance in downstream tasks. We design a specific distillation method and conduct experiments on the BERT architecture. Experimental results show a significant improvement in downstream performance of MoE models, and many further evidences also strongly support the concept of transfer capability distillation. Finally, we attempt to interpret transfer capability distillation and provide some insights from the perspective of model feature.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness</title>
<link>https://arxiv.org/abs/2405.08151</link>
<guid>https://arxiv.org/abs/2405.08151</guid>
<content:encoded><![CDATA[
arXiv:2405.08151v3 Announce Type: replace 
Abstract: Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are language models rational? The case of coherence norms and belief revision</title>
<link>https://arxiv.org/abs/2406.03442</link>
<guid>https://arxiv.org/abs/2406.03442</guid>
<content:encoded><![CDATA[
arXiv:2406.03442v3 Announce Type: replace 
Abstract: Do norms of rationality apply to machine learning models, in particular language models? In this paper we investigate this question by focusing on a special subset of rational norms: coherence norms. We consider both logical coherence norms as well as coherence norms tied to the strength of belief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose a new account of credence, which captures the strength of belief in language models. This proposal uniformly assigns strength of belief simply on the basis of model internal next token probabilities. We argue that rational norms tied to coherence do apply to some language models, but not to others. This issue is significant since rationality is closely tied to predicting and explaining behavior, and thus it is connected to considerations about AI safety and alignment, as well as understanding model behavior more generally.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RASTeR: Robust, Agentic, and Structured Temporal Reasoning</title>
<link>https://arxiv.org/abs/2406.19538</link>
<guid>https://arxiv.org/abs/2406.19538</guid>
<content:encoded><![CDATA[
arXiv:2406.19538v2 Announce Type: replace 
Abstract: Temporal question answering (TQA) remains a challenge for large language models (LLMs), particularly when retrieved content may be irrelevant, outdated, or temporally inconsistent. This is especially critical in applications like clinical event ordering, and policy tracking, which require reliable temporal reasoning even under noisy or outdated information. To address this challenge, we introduce RASTeR: \textbf{R}obust, \textbf{A}gentic, and \textbf{S}tructured, \textbf{Te}mporal \textbf{R}easoning, a prompting framework that separates context evaluation from answer generation. RASTeR first assesses the relevance and temporal coherence of the retrieved context, then constructs a temporal knolwedge graph (TKG) to better facilitate reasoning. When inconsistencies are detected, RASTeR selectively corrects or discards context before generating an answer. Across multiple datasets and LLMs, RASTeR consistently improves robustness\footnote{\ Some TQA work defines robustness as handling diverse temporal phenomena. Here, we define it as the ability to answer correctly despite suboptimal context}. We further validate our approach through a ``needle-in-the-haystack'' study, in which relevant context is buried among distractors. With forty distractors, RASTeR achieves 75\% accuracy, over 12\% ahead of the runner up
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Analysis of Gender Depiction in the Comedias of Calder\'on de la Barca</title>
<link>https://arxiv.org/abs/2411.03895</link>
<guid>https://arxiv.org/abs/2411.03895</guid>
<content:encoded><![CDATA[
arXiv:2411.03895v2 Announce Type: replace 
Abstract: In theatre, playwrights use the portrayal of characters to explore culturally based gender norms. In this paper, we develop quantitative methods to study gender depiction in the non-religious works (comedias) of Pedro Calder\'on de la Barca, a prolific Spanish 17th century author. We gather insights from a corpus of more than 100 plays by using a gender classifier and applying model explainability (attribution) methods to determine which text features are most influential in the model's decision to classify speech as 'male' or 'female', indicating the most gendered elements of dialogue in Calder\'on's comedias in a human accessible manner. We find that female and male characters are portrayed differently and can be identified by the gender prediction model at practically useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender portrayal, and demonstrates that the model is even useful in providing a relatively accurate scene-by-scene prediction of cross-dressing characters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2411.19244</link>
<guid>https://arxiv.org/abs/2411.19244</guid>
<content:encoded><![CDATA[
arXiv:2411.19244v3 Announce Type: replace 
Abstract: The Nepali language has distinct linguistic features, especially its complex script (Devanagari script), morphology, and various dialects,which pose a unique challenge for Natural Language Understanding (NLU) tasks. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a foundation for evaluating models, it remains limited in scope, covering four tasks. This restricts their utility for comprehensive assessments of Natural Language Processing (NLP) models. To address this limitation, we introduce twelve new datasets, creating a new benchmark, the Nepali /Language Understanding Evaluation (NLUE) benchmark for evaluating the performance of models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks include Single-Sentence Classification, Similarity and Paraphrase Tasks, Natural Language Inference (NLI), and General Masked Evaluation Task (GMET). Through extensive experiments, we demonstrate that existing top models struggle with the added complexity of these tasks. We also find that the best multilingual model outperforms the best monolingual models across most tasks, highlighting the need for more robust solutions tailored to the Nepali language. This expanded benchmark sets a new standard for evaluating, comparing, and advancing models, contributing significantly to the broader goal of advancing NLP research for low-resource languages.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDC: Learning to Generate Research Idea with Dynamic Control</title>
<link>https://arxiv.org/abs/2412.14626</link>
<guid>https://arxiv.org/abs/2412.14626</guid>
<content:encoded><![CDATA[
arXiv:2412.14626v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have demonstrated their potential in automating the scientific research ideation. Existing approaches primarily focus on prompting techniques, often producing ideas misaligned with expert standards - novelty, feasibility, and effectiveness, which are widely recognized by the research community as the three key subdimensions of high-quality ideas. Also, balancing these dimensions remains challenging due to their inherent trade-offs. To address these limitations, we propose the first framework that employs a two-stage approach combining Supervised Fine-Tuning (SFT) and controllable Reinforcement Learning (RL) for the task. In the SFT stage, the model learns foundational patterns from pairs of research papers and their corresponding follow-up ideas. In the RL stage, multi-dimensional reward models guided by fine-grained feedback evaluate and optimize the model across key dimensions. During inference, dimensional controllers coordinated by a sentence-level decoder enable dynamic context-aware steering of the idea generation process. Our framework provides a balanced approach to research idea generation, achieving high-quality outcomes in the experiment by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotions, Context, and Substance Use in Adolescents: A Large Language Model Analysis of Reddit Posts</title>
<link>https://arxiv.org/abs/2501.14037</link>
<guid>https://arxiv.org/abs/2501.14037</guid>
<content:encoded><![CDATA[
arXiv:2501.14037v2 Announce Type: replace 
Abstract: Early substance use during adolescence increases the risk of later substance use disorders and mental health problems, yet the emotional and contextual factors driving these behaviors remain poorly understood. This study analyzed 23000 substance-use related posts and an equal number of non-substance posts from Reddit's r/teenagers community (2018-2022). Posts were annotated for six discrete emotions (sadness, anger, joy, guilt, fear, disgust) and contextual factors (family, peers, school) using large language models (LLMs). Statistical analyses compared group differences, and interpretable machine learning (SHAP) identified key predictors of substance-use discussions. LLM-assisted thematic coding further revealed latent psychosocial themes linking emotions with contexts. Negative emotions, especially sadness, guilt, fear, and disgust, were significantly more common in substance-use posts, while joy dominated non-substance discussions. Guilt and shame diverged in function: guilt often reflected regret and self-reflection, whereas shame reinforced risky behaviors through peer performance. Peer influence emerged as the strongest contextual factor, closely tied to sadness, fear, and guilt. Family and school environments acted as both risk and protective factors depending on relational quality and stress levels. Overall, adolescent substance-use discussions reflected a dynamic interplay of emotion, social context, and coping behavior. By integrating statistical analysis, interpretable models, and LLM-based thematic exploration, this study demonstrates the value of mixed computational approaches for uncovering the emotional and contextual mechanisms underlying adolescent risk behavior.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts</title>
<link>https://arxiv.org/abs/2503.19498</link>
<guid>https://arxiv.org/abs/2503.19498</guid>
<content:encoded><![CDATA[
arXiv:2503.19498v5 Announce Type: replace 
Abstract: Chart Question Answering (CQA) evaluates Multimodal Large Language Models (MLLMs) on visual understanding and reasoning over chart data. However, existing benchmarks mostly test surface-level parsing, such as reading labels and legends, while overlooking deeper scientific reasoning. We propose DomainCQA, a framework for constructing domain-specific CQA benchmarks that emphasize both visual comprehension and knowledge-intensive reasoning. It integrates complexity-aware chart selection, multitier QA generation, and expert validation. Applied to astronomy, DomainCQA yields AstroChart, a benchmark of 1,690 QA pairs over 482 charts, exposing persistent weaknesses in fine-grained perception, numerical reasoning, and domain knowledge integration across 21 MLLMs. Fine-tuning on AstroChart improves performance across fundamental and advanced tasks. Pilot QA sets in biochemistry, economics, medicine, and social science further demonstrate DomainCQA's generality. Together, our results establish DomainCQA as a unified pipeline for constructing and augmenting domain-specific chart reasoning benchmarks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</title>
<link>https://arxiv.org/abs/2504.08716</link>
<guid>https://arxiv.org/abs/2504.08716</guid>
<content:encoded><![CDATA[
arXiv:2504.08716v2 Announce Type: replace 
Abstract: Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being its support for long context, faster training, and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[
arXiv:2505.01812v3 Announce Type: replace 
Abstract: Humans and intelligent animals can internalize new information and accurately internalize their implications to perform downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the information (news) is explicitly given as context, adequately integrating the information into model weights via fine-tuning remains challenging. In this paper, we introduce New News, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. First, we demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications, and Self-QA -- designed to distill the knowledge processed by the model with context into the weights of the model, which we term System-2 Fine-tuning (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the Self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news while preserving general capabilities. Furthermore, we discover the contextual shadowing effect, where training with the news in context followed by its rephrases or QAs catastrophically degrades learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation-Guided Consensus Merging for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14009</link>
<guid>https://arxiv.org/abs/2505.14009</guid>
<content:encoded><![CDATA[
arXiv:2505.14009v2 Announce Type: replace 
Abstract: Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[
arXiv:2505.16270v2 Announce Type: replace 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[
arXiv:2505.16927v2 Announce Type: replace 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes that guide model reasoning toward human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ a form of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains that our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</title>
<link>https://arxiv.org/abs/2506.10202</link>
<guid>https://arxiv.org/abs/2506.10202</guid>
<content:encoded><![CDATA[
arXiv:2506.10202v2 Announce Type: replace 
Abstract: Recent approaches have shown impressive proficiency in extracting and leveraging parametric knowledge from Large-Language Models (LLMs) and Vision-Language Models (VLMs). In this work, we consider how we can improve the identification and retrieval of videos related to complex real-world events by automatically extracting latent parametric knowledge about those events. We present Q2E: a Query-to-Event decomposition method for zero-shot multilingual text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our approach demonstrates that we can enhance the understanding of otherwise overly simplified human queries by decomposing the query using the knowledge embedded in LLMs and VLMs. We additionally show how to apply our approach to both visual and speech-based inputs. To combine this varied multimodal knowledge, we adopt entropy-based fusion scoring for zero-shot fusion. Through evaluations on two diverse datasets and multiple retrieval metrics, we demonstrate that Q2E outperforms several state-of-the-art baselines. Our evaluation also shows that integrating audio information can significantly improve text-to-video retrieval. We have released code and data for future research.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.15793</link>
<guid>https://arxiv.org/abs/2508.15793</guid>
<content:encoded><![CDATA[
arXiv:2508.15793v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including texts, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Yet it remains unclear whether such biases are systematic, which data-level factors drive them, and what internal mechanisms underlie their emergence.
  In this paper, we present the first comprehensive study of format bias in LLMs through a three-stage empirical analysis. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage examines how key data-level factors influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its effectiveness. Our results show that format bias is consistent across model families, driven by information richness, structure quality, and representation type, and is closely associated with attention imbalance within the LLMs. Based on these investigations, we identify three future research directions to reduce format bias: enhancing data pre-processing through format repair and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation</title>
<link>https://arxiv.org/abs/2508.15846</link>
<guid>https://arxiv.org/abs/2508.15846</guid>
<content:encoded><![CDATA[
arXiv:2508.15846v2 Announce Type: replace 
Abstract: As tropical cyclones intensify and track forecasts become increasingly uncertain, U.S. ports face heightened supply-chain risk under extreme weather conditions. Port operators need to rapidly synthesize diverse multimodal forecast products, such as probabilistic wind maps, track cones, and official advisories, into clear, actionable guidance as cyclones approach. Multimodal large language models (MLLMs) offer a powerful means to integrate these heterogeneous data sources alongside broader contextual knowledge, yet their accuracy and reliability in the specific context of port cyclone preparedness have not been rigorously evaluated. To fill this gap, we introduce CyPortQA, the first multimodal benchmark tailored to port operations under cyclone threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015 through 2023, spanning 145 U.S. principal ports and 90 named storms. Each scenario fuses multisource data (i.e., tropical cyclone products, port operational impact records, and port condition bulletins) and is expanded through an automated pipeline into 117,178 structured question answer pairs. Using this benchmark, we conduct extensive experiments on diverse MLLMs, including both open-source and proprietary model. MLLMs demonstrate great potential in situation understanding but still face considerable challenges in reasoning tasks, including potential impact estimation and decision reasoning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Surface: Probing the Ideological Depth of Large Language Models</title>
<link>https://arxiv.org/abs/2508.21448</link>
<guid>https://arxiv.org/abs/2508.21448</guid>
<content:encoded><![CDATA[
arXiv:2508.21448v2 Announce Type: replace 
Abstract: Large language models (LLMs) display recognizable political leanings, yet they vary significantly in their ability to represent a political orientation consistently. In this paper, we define ideological depth as (i) a model's ability to follow political instructions without failure (steerability), and (ii) the feature richness of its internal political representations measured with sparse autoencoders (SAEs), an unsupervised sparse dictionary learning (SDL) approach. Using Llama-3.1-8B-Instruct and Gemma-2-9B-IT as candidates, we compare prompt-based and activation-steering interventions and probe political features with publicly available SAEs. We find large, systematic differences: Gemma is more steerable in both directions and activates approximately 7.3x more distinct political features than Llama. Furthermore, causal ablations of a small targeted set of Gemma's political features to create a similar feature-poor setting induce consistent shifts in its behavior, with increased rates of refusals across topics. Together, these results indicate that refusals on benign political instructions or prompts can arise from capability deficits rather than safety guardrails. Ideological depth thus emerges as a measurable property of LLMs, and steerability serves as a window into their latent political architecture.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wage Sentiment Indices Derived from Survey Comments via Large Language Models</title>
<link>https://arxiv.org/abs/2509.00290</link>
<guid>https://arxiv.org/abs/2509.00290</guid>
<content:encoded><![CDATA[
arXiv:2509.00290v2 Announce Type: replace 
Abstract: The emergence of generative Artificial Intelligence (AI) has created new opportunities for economic text analysis. This study proposes a Wage Sentiment Index (WSI) constructed with Large Language Models (LLMs) to forecast wage dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS), a monthly survey conducted by the Cabinet Office of Japan that captures real-time economic assessments from workers in industries highly sensitive to business conditions. The WSI extends the framework of the Price Sentiment Index (PSI) used in prior studies, adapting it specifically to wage related sentiment. To ensure scalability and adaptability, a data architecture is also developed that enables integration of additional sources such as newspapers and social media. Experimental results demonstrate that WSI models based on LLMs significantly outperform both baseline approaches and pretrained models. These findings highlight the potential of LLM-driven sentiment indices to enhance the timeliness and effectiveness of economic policy design by governments and central banks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions</title>
<link>https://arxiv.org/abs/2509.15901</link>
<guid>https://arxiv.org/abs/2509.15901</guid>
<content:encoded><![CDATA[
arXiv:2509.15901v2 Announce Type: replace 
Abstract: Meeting summarization with large language models (LLMs) remains error-prone, often producing outputs with hallucinations, omissions, and irrelevancies. We present FRAME, a modular pipeline that reframes summarization as a semantic enrichment task. FRAME extracts and scores salient facts, organizes them thematically, and uses these to enrich an outline into an abstractive summary. To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that has the model build a reasoning trace by answering nine questions before content selection. For evaluation, we propose P-MESA, a multi-dimensional, reference-free evaluation framework to assess if a summary fits a target reader. P-MESA reliably identifies error instances, achieving >= 89% balanced accuracy against human annotations and strongly aligns with human severity ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and omission by 2 out of 5 points (measured with MESA), while SCOPE improves knowledge fit and goal alignment over prompt-only baselines. Our findings advocate for rethinking summarization to improve control, faithfulness, and personalization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[
arXiv:2510.06188v2 Announce Type: replace 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers. Code is available in https://github.com/Jak57/BanglaTalk
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Study of Automatic Evaluation in Sign Language Translation</title>
<link>https://arxiv.org/abs/2510.25434</link>
<guid>https://arxiv.org/abs/2510.25434</guid>
<content:encoded><![CDATA[
arXiv:2510.25434v2 Announce Type: replace 
Abstract: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Latent Reasoning via Looped Language Models</title>
<link>https://arxiv.org/abs/2510.25741</link>
<guid>https://arxiv.org/abs/2510.25741</guid>
<content:encoded><![CDATA[
arXiv:2510.25741v3 Announce Type: replace 
Abstract: Modern LLMs are trained to "think" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model is available here: http://ouro-llm.github.io.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion</title>
<link>https://arxiv.org/abs/2403.10568</link>
<guid>https://arxiv.org/abs/2403.10568</guid>
<content:encoded><![CDATA[
arXiv:2403.10568v4 Announce Type: replace-cross 
Abstract: Despite the demonstrated parameter efficiency of prompt-based fusion, its limited adaptivity and expressiveness hinder its effectiveness for multimodal applications at scale. In this paper, we present the first comprehensive study addressing these limitations. Our key motivation is to ``divide and conquer'' the vanilla prompt, traditionally shared across all instances, by generating instance-specific prompts. Specifically, we propose the Mixture of Prompt Experts (MoPE), a framework that significantly enhances prompt adaptivity and expressiveness by dynamically generating instance-specific prompts. MoPE leverages multimodal pairings as additional evidence, allowing the model to adaptively select optimal prompts tailored to each individual instance. Unlike traditional prompt-fusion methods, which encounter scalability bottlenecks when optimizing long unified prompts, MoPE maintains fixed prompt length while effectively scaling the number of specialized experts. Moreover, we investigate regularization terms to encourage expert specialization, resulting in highly adaptive and interpretable prompting. MoPE fundamentally changes the scaling dynamic, unlocking greater expressiveness and adaptability to complex multimodal relationships, enabling the model to selectively attend to task-relevant sub-sequences based on instance-specific multimodal input. Extensive experiments across six multimodal datasets spanning four modalities demonstrate state-of-the-art performance for multimodal fusion, matching or surpassing the performance of fine-tuning while requiring only 0.8% of the trainable parameters. Code is available: https://github.com/songrise/MoPE.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation</title>
<link>https://arxiv.org/abs/2411.16657</link>
<guid>https://arxiv.org/abs/2411.16657</guid>
<content:encoded><![CDATA[
arXiv:2411.16657v4 Announce Type: replace-cross 
Abstract: Storytelling video generation (SVG) aims to produce coherent and visually rich multi-scene videos that follow a structured narrative. Existing methods primarily employ LLM for high-level planning to decompose a story into scene-level descriptions, which are then independently generated and stitched together. However, these approaches struggle with generating high-quality videos aligned with the complex single-scene description, as visualizing such complex description involves coherent composition of multiple characters and events, complex motion synthesis and multi-character customization. To address these challenges, we propose DREAMRUNNER, a novel story-to-video generation method: First, we structure the input script using a large language model (LLM) to facilitate both coarse-grained scene planning as well as fine-grained object-level layout planning. Next, DREAMRUNNER presents retrieval-augmented test-time adaptation to capture target motion priors for objects in each scene, supporting diverse motion customization based on retrieved videos, thus facilitating the generation of new videos with complex, scripted motions. Lastly, we propose a novel spatial-temporal region-based 3D attention and prior injection module SR3AI for fine-grained object-motion binding and frame-by-frame spatial-temporal semantic control. We compare DREAMRUNNER with various SVG baselines, demonstrating state-of-the-art performance in character consistency, text alignment, and smooth transitions. Additionally, DREAMRUNNER exhibits strong fine-grained condition-following ability in compositional text-to-video generation, significantly outperforming baselines on T2V-ComBench. Finally, we validate DREAMRUNNER's robust ability to generate multi-object interactions with qualitative examples.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[
arXiv:2502.03460v3 Announce Type: replace-cross 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA</title>
<link>https://arxiv.org/abs/2503.11880</link>
<guid>https://arxiv.org/abs/2503.11880</guid>
<content:encoded><![CDATA[
arXiv:2503.11880v3 Announce Type: replace-cross 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable LLM Guardrails via Sparse Representation Steering</title>
<link>https://arxiv.org/abs/2503.16851</link>
<guid>https://arxiv.org/abs/2503.16851</guid>
<content:encoded><![CDATA[
arXiv:2503.16851v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit impressive capabilities in generation tasks but are prone to producing harmful, misleading, or biased content, posing significant ethical and safety concerns. To mitigate such risks, representation engineering, which steer model behavior toward desired attributes by injecting carefully designed steering vectors into LLM's representations at inference time, has emerged as a promising alternative to fine-tuning approaches. However, due to the semantically entangled nature of LLM's representation, existing representation engineering methods still suffer from several limitations: limited fine-grained controllability, content quality degradation, and conflict in multi-attribute control. To overcome these challenges, we propose Sparse Representation Steering (SRS), a novel framework that achieves fine-grained and interpretable control over LLM behavior by first disentangling internal activations into a sparse, semantically meaningful representation space, and then selectively steering relevant dimensions. Specifically, SRS leverages a pretrained Sparse Autoencoder (SAE) to transform dense, entangled activation patterns into a sparse monosemantic feature space. To identify relevant features, SRS contrasts sparse activations from positive and negative prompt pairs and measures their bidirectional KL divergence to locate dimensions most associated with the target attribute. We conduct comprehensive experiments on Gemma-2 series model across three alignment dimensions, i.e., safety, fairness, and truthfulness, to evaluate the effectiveness of SRS. Results show that SRS consistently outperforms existing steering methods, which achieves significantly improved controllability across both single and multiple attribute settings, while preserving high linguistic quality and general ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition</title>
<link>https://arxiv.org/abs/2506.06071</link>
<guid>https://arxiv.org/abs/2506.06071</guid>
<content:encoded><![CDATA[
arXiv:2506.06071v2 Announce Type: replace-cross 
Abstract: Bias in speech emotion recognition (SER) systems often stems from spurious correlations between speaker characteristics and emotional labels, leading to unfair predictions across demographic groups. Many existing debiasing methods require model-specific changes or demographic annotations, limiting their practical use. We present CO-VADA, a Confidence-Oriented Voice Augmentation Debiasing Approach that mitigates bias without modifying model architecture or relying on demographic information. CO-VADA identifies training samples that reflect bias patterns present in the training data and then applies voice conversion to alter irrelevant attributes and generate samples. These augmented samples introduce speaker variations that differ from dominant patterns in the data, guiding the model to focus more on emotion-relevant features. Our framework is compatible with various SER models and voice conversion tools, making it a scalable and practical solution for improving fairness in SER systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</title>
<link>https://arxiv.org/abs/2507.11017</link>
<guid>https://arxiv.org/abs/2507.11017</guid>
<content:encoded><![CDATA[
arXiv:2507.11017v2 Announce Type: replace-cross 
Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by performing a first-order Taylor expansion around the pre-quantization weights. This yields an approximation based on the difference between latent and full-precision weights as well as the Hessian matrix. When substituted into the theoretical solution, the formulation eliminates the need to explicitly compute the Hessian, thereby avoiding the high computational cost and limited generalization of backpropagation-based gradient methods. This design introduces only minimal additional computational overhead. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 17.3% and increases the 5-shot MMLU accuracy from 53.8% achieved by GPTAQ to 56.1%. Moreover, FOEM can be seamlessly combined with advanced techniques such as SpinQuant, delivering additional gains under the challenging W4A4KV4 setting and further narrowing the performance gap with full-precision baselines, surpassing existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning</title>
<link>https://arxiv.org/abs/2508.05129</link>
<guid>https://arxiv.org/abs/2508.05129</guid>
<content:encoded><![CDATA[
arXiv:2508.05129v2 Announce Type: replace-cross 
Abstract: With the rapid and continuous increase in academic publications, identifying high-quality research has become an increasingly pressing challenge. While recent methods leveraging Large Language Models (LLMs) for automated paper evaluation have shown great promise, they are often constrained by outdated domain knowledge and limited reasoning capabilities. In this work, we present PaperEval, a novel LLM-based framework for automated paper evaluation that addresses these limitations through two key components: 1) a domain-aware paper retrieval module that retrieves relevant concurrent work to support contextualized assessments of novelty and contributions, and 2) a latent reasoning mechanism that enables deep understanding of complex motivations and methodologies, along with comprehensive comparison against concurrently related work, to support more accurate and reliable evaluation. To guide the reasoning process, we introduce a progressive ranking optimization strategy that encourages the LLM to iteratively refine its predictions with an emphasis on relative comparison. Experiments on two datasets demonstrate that PaperEval consistently outperforms existing methods in both academic impact and paper quality evaluation. In addition, we deploy PaperEval in a real-world paper recommendation system for filtering high-quality papers, which has gained strong engagement on social media -- amassing over 8,000 subscribers and attracting over 10,000 views for many filtered high-quality papers -- demonstrating the practical effectiveness of PaperEval.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title>
<link>https://arxiv.org/abs/2509.20379</link>
<guid>https://arxiv.org/abs/2509.20379</guid>
<content:encoded><![CDATA[
arXiv:2509.20379v2 Announce Type: replace-cross 
Abstract: Hallucinations of vision-language models (VLMs), which are misalignments between visual content and generated text, undermine the reliability of VLMs. One common approach for detecting them employs the same VLM, or a different one, to assess generated outputs. This process is computationally intensive and increases model latency. In this paper, we explore an efficient on-the-fly method for hallucination detection by training traditional ML models over signals based on the VLM's next-token probabilities (NTPs). NTPs provide a direct quantification of model uncertainty. We hypothesize that high uncertainty (i.e., a low NTP value) is strongly associated with hallucinations. To test this, we introduce a dataset of 1,400 human-annotated statements derived from VLM-generated content, each labeled as hallucinated or not, and use it to test our NTP-based lightweight method. Our results demonstrate that NTP-based features are valuable predictors of hallucinations, enabling fast and simple ML models to achieve performance comparable to that of strong VLMs. Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding only the generated text back into the VLM, enhances hallucination detection performance. Finally, integrating hallucination prediction scores from VLMs into the NTP-based models led to better performance than using either VLMs or NTPs alone. We hope this study paves the way for simple, lightweight solutions that enhance the reliability of VLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input</title>
<link>https://arxiv.org/abs/2510.16926</link>
<guid>https://arxiv.org/abs/2510.16926</guid>
<content:encoded><![CDATA[
arXiv:2510.16926v3 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X</title>
<link>https://arxiv.org/abs/2510.25932</link>
<guid>https://arxiv.org/abs/2510.25932</guid>
<content:encoded><![CDATA[
arXiv:2510.25932v2 Announce Type: replace-cross 
Abstract: Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenization, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the device. FakeZero employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantization. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance loss. By providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omnilingual ASR: Open-Source Multilingual Speech Recognition for 1600+ Languages</title>
<link>https://arxiv.org/abs/2511.09690</link>
<guid>https://arxiv.org/abs/2511.09690</guid>
<content:encoded><![CDATA[
<div> Keywords: Omnilingual ASR, low-resource languages, self-supervised pre-training, zero-shot generalization, open-source

<br /><br />Summary:  
The paper presents Omnilingual ASR, a pioneering large-scale automatic speech recognition system designed for extensibility to support over 1,600 languages, including more than 500 previously unserved by ASR technology. It addresses the challenge of expanding ASR to low-resource and long-tail languages by allowing communities to add new languages with only a few data samples. The system utilizes self-supervised pre-training scaled up to 7 billion parameters to develop robust speech representations, alongside an encoder-decoder architecture with a large language model (LLM)-inspired decoder that supports zero-shot generalization to unseen languages. This approach is underpinned by training on a massive, diverse speech corpus sourced from public data and community contributions through compensated local partnerships, ensuring broad linguistic coverage. Evaluations demonstrate substantial performance improvements over previous ASR systems, particularly in low-resource conditions, highlighting the system’s strong generalization capabilities. Omnilingual ASR is released as a model family ranging from 300 million parameters for low-power devices to 7 billion for maximum accuracy. The paper also discusses the ethical considerations involved, emphasizing the importance of community collaboration and open-sourcing models and tools to lower barriers for researchers and language communities, thereby fostering inclusive participation and societal benefits. <div>
arXiv:2511.09690v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) has advanced in high-resource languages, but most of the world's 7,000+ languages remain unsupported, leaving thousands of long-tail languages behind. Expanding ASR coverage has been costly and limited by architectures that restrict language support, making extension inaccessible to most--all while entangled with ethical concerns when pursued without community collaboration. To transcend these limitations, we introduce Omnilingual ASR, the first large-scale ASR system designed for extensibility. Omnilingual ASR enables communities to introduce unserved languages with only a handful of data samples. It scales self-supervised pre-training to 7B parameters to learn robust speech representations and introduces an encoder-decoder architecture designed for zero-shot generalization, leveraging a LLM-inspired decoder. This capability is grounded in a massive and diverse training corpus; by combining breadth of coverage with linguistic variety, the model learns representations robust enough to adapt to unseen languages. Incorporating public resources with community-sourced recordings gathered through compensated local partnerships, Omnilingual ASR expands coverage to over 1,600 languages, the largest such effort to date--including over 500 never before served by ASR. Automatic evaluations show substantial gains over prior systems, especially in low-resource conditions, and strong generalization. We release Omnilingual ASR as a family of models, from 300M variants for low-power devices to 7B for maximum accuracy. We reflect on the ethical considerations shaping this design and conclude by discussing its societal impact. In particular, we highlight how open-sourcing models and tools can lower barriers for researchers and communities, inviting new forms of participation. Open-source artifacts are available at https://github.com/facebookresearch/omnilingual-asr.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: Rethinking Prompt Construction in In-Context Learning</title>
<link>https://arxiv.org/abs/2511.09700</link>
<guid>https://arxiv.org/abs/2511.09700</guid>
<content:encoded><![CDATA[
<div> In-context learning, example selection, example ordering, prompt design, large language models<br /><br />Summary:<br /><br />1. In-context learning (ICL) allows large language models to learn new tasks by conditioning on sequences of examples. 2. Previous research assumed that the choice of examples (selection) has a greater impact on model performance than the order in which these examples appear, leading to a focus primarily on selection strategies. 3. This paper challenges that assumption by systematically comparing the effects of example selection and example ordering across classification and generation tasks. 4. Experiments were conducted using multiple open-source language models ranging from 0.5 billion to 27 billion parameters, as well as the GPT-5 model. 5. Results show that the variation in performance caused by different orderings of the same example set is comparable to the variation caused by using completely different example sets. 6. The research also demonstrates that good example orderings can be identified using only a development set, achieving performance close to an oracle method that uses test labels to pick the best ordering. 7. These findings emphasize that example ordering is as important as example selection and that both factors are closely intertwined in prompt design. 8. The work calls for a reconsideration of prior assumptions in ICL and encourages the development of new strategies that optimize both example selection and ordering for improved model performance. <div>
arXiv:2511.09700v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models to perform new tasks by conditioning on a sequence of examples. Most prior work reasonably and intuitively assumes that which examples are chosen has a far greater effect on performance than how those examples are ordered, leading to a focus on example selection. We revisit this assumption and conduct a systematic comparison between the effect of selection and ordering. Through controlled experiments on both classification and generation tasks, using multiple open-source model families (0.5B to 27B parameters) and GPT-5, we find that the variance in performance due to different example orderings is comparable to that from using entirely different example sets. Furthermore, we show that strong orderings can be identified using only a development set, achieving performance close to an oracle that selects the best ordering based on test labels. Our findings highlight the equal and intertwined importance of example selection and ordering in prompt design, calling for a reexamination of the assumptions held in ICL.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual morphologically-guided tokenization for Latin encoder models</title>
<link>https://arxiv.org/abs/2511.09709</link>
<guid>https://arxiv.org/abs/2511.09709</guid>
<content:encoded><![CDATA[
<div> Keywords: tokenization, morphological alignment, Latin, language modeling, low-resource languages<br /><br />Summary:<br /><br />1. Tokenization is a fundamental step in language model pretraining, but standard methods typically focus on information-theoretic objectives like compression and fertility rather than aligning with linguistic morphology.<br />2. This mismatch in tokenization quality is particularly problematic for morphologically rich languages, where it negatively affects performance on downstream NLP tasks.<br />3. The study explores morphologically-aware tokenization specifically for Latin, a morphologically complex language with moderate amounts of pretraining data but substantial curated lexical resources.<br />4. Incorporating morphological knowledge into tokenization leads to improved performance across four downstream tasks, with the greatest gains observed on out-of-domain text, suggesting enhanced generalization abilities of the models.<br />5. The findings highlight the value of leveraging existing linguistic resources as a viable strategy to improve language model performance for low-resource languages, which often lack large-scale pretraining corpora but may have quality lexical databases.<br /><br />This research advocates for the development and integration of morphologically-informed tokenization schemes as a practical means to overcome data scarcity challenges in language modeling for morphologically rich, low-resource languages. <div>
arXiv:2511.09709v1 Announce Type: new 
Abstract: Tokenization is a critical component of language model pretraining, yet standard tokenization methods often prioritize information-theoretical goals like high compression and low fertility rather than linguistic goals like morphological alignment. In fact, they have been shown to be suboptimal for morphologically rich languages, where tokenization quality directly impacts downstream performance. In this work, we investigate morphologically-aware tokenization for Latin, a morphologically rich language that is medium-resource in terms of pretraining data, but high-resource in terms of curated lexical resources -- a distinction that is often overlooked but critical in discussions of low-resource language modeling. We find that morphologically-guided tokenization improves overall performance on four downstream tasks. Performance gains are most pronounced for out of domain texts, highlighting our models' improved generalization ability. Our findings demonstrate the utility of linguistic resources to improve language modeling for morphologically complex languages. For low-resource languages that lack large-scale pretraining data, the development and incorporation of linguistic resources can serve as a feasible alternative to improve LM performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives</title>
<link>https://arxiv.org/abs/2511.09738</link>
<guid>https://arxiv.org/abs/2511.09738</guid>
<content:encoded><![CDATA[
<div> Natural Language Processing, Presidential Directives, Topic Extraction, Social Science, AI Validation

<br /><br />Summary:  
1. The research explores the application of Natural Language Processing (NLP) to extract main topics from extensive written data, focusing on identifying signaling themes in Presidential Directives (PDs) spanning the Reagan to Clinton administrations.  
2. Both human analysts and NLP methods successfully identified relevant documents, highlighting the promising utility of NLP in analyzing large textual corpora for social science research.  
3. Despite these successes, there were notable discrepancies between NLP-generated results and human-labeled data, pointing to limitations that necessitate further investigation and refinement of NLP tools for this specific use case.  
4. The study was conducted in 2023, acknowledging that the rapid advancement in AI and ML tools means that this research utilized potentially outdated technology, underscoring challenges and opportunities in applying evolving AI methods to social science.  
5. Overall, the findings demonstrate both the potential and the current limitations of NLP in the extraction of thematic content from historical political documents, calling for ongoing research to validate and improve AI applications in the social sciences. <div>
arXiv:2511.09738v1 Announce Type: new 
Abstract: Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Small Can You Go? Compact Language Models for On-Device Critical Error Detection in Machine Translation</title>
<link>https://arxiv.org/abs/2511.09748</link>
<guid>https://arxiv.org/abs/2511.09748</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Critical Error Detection, machine translation, model efficiency, English-German translation<br /><br />Summary: This study investigates the minimum size of Large Language Models (LLMs) required to effectively detect meaning-altering errors in machine translation from English to German, focusing on Critical Error Detection (CED). The research benchmarks four sub-2 billion parameter models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across several datasets including WMT21, WMT22, and SynCED-EnDe-2025. The authors propose a standardized evaluation framework that utilizes consistent prompts, lightweight logit-bias calibration, and majority voting to assess model performance. The results indicate an optimal trade-off around one billion parameters, with the Gemma-3-1B model achieving the best balance between quality and efficiency. This model reached a Matthews correlation coefficient (MCC) of 0.77 and an F1-ERR score of 0.98 on SynCED-EnDe-2025 after fine-tuning with merged weights, while maintaining a low latency of 400 ms per sample on a MacBook Pro M4 Pro. Larger models such as Qwen-3-1.7B achieve higher absolute accuracy but at increased computational cost. Smaller models (0.6B) remain viable with few-shot calibration but struggle with certain error types. Overall, compact, instruction-tuned LLMs supplemented with lightweight calibration and minimal supervision offer a promising solution for private, efficient, on-device error detection in MT workflows, with all materials made publicly available on GitHub. <div>
arXiv:2511.09748v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at evaluating machine translation (MT), but their scale and cost hinder deployment on edge devices and in privacy-sensitive workflows. We ask: how small can you get while still detecting meaning-altering translation errors? Focusing on English->German Critical Error Detection (CED), we benchmark sub-2B models (LFM2-350M, Qwen-3-0.6B/1.7B, Llama-3.2-1B-Instruct, Gemma-3-1B) across WMT21, WMT22, and SynCED-EnDe-2025. Our framework standardizes prompts, applies lightweight logit-bias calibration and majority voting, and reports both semantic quality (MCC, F1-ERR/F1-NOT) and compute metrics (VRAM, latency, throughput). Results reveal a clear sweet spot around one billion parameters: Gemma-3-1B provides the best quality-efficiency trade-off, reaching MCC=0.77 with F1-ERR=0.98 on SynCED-EnDe-2025 after merged-weights fine-tuning, while maintaining 400 ms single-sample latency on a MacBook Pro M4 Pro (24 GB). At larger scale, Qwen-3-1.7B attains the highest absolute MCC (+0.11 over Gemma) but with higher compute cost. In contrast, ultra-small models (0.6B) remain usable with few-shot calibration yet under-detect entity and number errors. Overall, compact, instruction-tuned LLMs augmented with lightweight calibration and small-sample supervision can deliver trustworthy, on-device CED for MT, enabling private, low-cost error screening in real-world translation pipelines. All datasets, prompts, and scripts are publicly available at our GitHub repository.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicate-Argument Structure Divergences in Chinese and English Parallel Sentences and their Impact on Language Transfer</title>
<link>https://arxiv.org/abs/2511.09796</link>
<guid>https://arxiv.org/abs/2511.09796</guid>
<content:encoded><![CDATA[
<div> Keywords: cross-lingual NLP, predicate-argument structure, annotation projection, language transfer, structural divergences<br /><br />Summary:<br /><br />This paper addresses the challenges in cross-lingual natural language processing (NLP), particularly focusing on transferring linguistic knowledge between typologically distant languages like Chinese and English. It emphasizes the importance of predicate-argument structures as a linguistic unit to analyze cross-lingual alignment and misalignment in parallel sentences. The study conducts both qualitative and quantitative analyses of annotations projected from one language to the other, investigating how well predicate annotations align between the two languages. A categorization of structural divergences is proposed to better understand the types of misalignments present. Key findings reveal that language transfer is asymmetric, meaning that the direction of transfer (Chinese to English versus English to Chinese) significantly affects the quality and results of annotation projection. This asymmetry highlights the need to carefully select the source language in transfer learning setups, as it impacts the effectiveness of cross-lingual NLP applications. The paper advocates for further investigation into this asymmetry before making broad scientific claims about cross-lingual transfer methodologies, thus contributing a nuanced perspective on the complexities underlying multilingual language processing. <div>
arXiv:2511.09796v1 Announce Type: new 
Abstract: Cross-lingual Natural Language Processing (NLP) has gained significant traction in recent years, offering practical solutions in low-resource settings by transferring linguistic knowledge from resource-rich to low-resource languages. This field leverages techniques like annotation projection and model transfer for language adaptation, supported by multilingual pre-trained language models. However, linguistic divergences hinder language transfer, especially among typologically distant languages. In this paper, we present an analysis of predicate-argument structures in parallel Chinese and English sentences. We explore the alignment and misalignment of predicate annotations, inspecting similarities and differences and proposing a categorization of structural divergences. The analysis and the categorization are supported by a qualitative and quantitative analysis of the results of an annotation projection experiment, in which, in turn, one of the two languages has been used as source language to project annotations into the corresponding parallel sentences. The results of this analysis show clearly that language transfer is asymmetric. An aspect that requires attention when it comes to selecting the source language in transfer learning applications and that needs to be investigated before any scientific claim about cross-lingual NLP is proposed.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARG: Training-Free Adaptive Retrieval Gating for Efficient RAG</title>
<link>https://arxiv.org/abs/2511.09803</link>
<guid>https://arxiv.org/abs/2511.09803</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Adaptive Retrieval Gating, Uncertainty Scores, Latency Reduction, Instruction-Tuned LLMs<br /><br />Summary:<br /><br />1. Retrieval-Augmented Generation (RAG) enhances factual accuracy but frequently retrieving for every query increases token usage and latency, which can hurt quality.  
2. The authors propose Training-free Adaptive Retrieval Gating (TARG), a single-shot, training-free policy that decides when to retrieve based on a short, no-context draft generated by the base model.  
3. TARG uses lightweight uncertainty scores derived from the draft's prefix logits: mean token entropy, a margin signal computed from the top-1/top-2 logit gap via a monotone link, or small-N variance from multiple stochastic prefixes. Retrieval is triggered only if the uncertainty score crosses a threshold.  
4. TARG is model-agnostic, introduces minimal token overhead, requires no extra training or auxiliary heads, and effectively balances accuracy and efficiency.  
5. Evaluation on datasets including NQ-Open, TriviaQA, and PopQA shows that TARG matches or exceeds the accuracy (EM/F1) of Always-RAG while reducing retrieval calls by 70-90% and decreasing end-to-end latency, with overhead close to Never-RAG. Additionally, the margin signal is the most robust default under instruction-tuned large language models, while small-N variance offers a conservative option prioritizing retrieval budget. Ablations demonstrate trade-offs between gate types and prefix lengths, clarifying latency-budget dynamics. <div>
arXiv:2511.09803v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) improves factuality but retrieving for every query often hurts quality while inflating tokens and latency. We propose Training-free Adaptive Retrieval Gating (TARG), a single-shot policy that decides when to retrieve using only a short, no-context draft from the base model. From the draft's prefix logits, TARG computes lightweight uncertainty scores: mean token entropy, a margin signal derived from the top-1/top-2 logit gap via a monotone link, or small-N variance across a handful of stochastic prefixes, and triggers retrieval only when the score exceeds a threshold. The gate is model agnostic, adds only tens to hundreds of draft tokens, and requires no additional training or auxiliary heads. On NQ-Open, TriviaQA, and PopQA, TARG consistently shifts the accuracy-efficiency frontier: compared with Always-RAG, TARG matches or improves EM/F1 while reducing retrieval by 70-90% and cutting end-to-end latency, and it remains close to Never-RAG in overhead. A central empirical finding is that under modern instruction-tuned LLMs the margin signal is a robust default (entropy compresses as backbones sharpen), with small-N variance offering a conservative, budget-first alternative. We provide ablations over gate type and prefix length and use a delta-latency view to make budget trade-offs explicit.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Khmer Spellchecking: A Holistic Approach</title>
<link>https://arxiv.org/abs/2511.09812</link>
<guid>https://arxiv.org/abs/2511.09812</guid>
<content:encoded><![CDATA[
<div> Keywords: Khmer spellchecking, subword segmentation, named-entity recognition, grapheme-to-phoneme conversion, language model<br /><br />Summary:<br /><br />This paper addresses the unresolved problem of spellchecking for the Khmer language by identifying several key challenges: misalignments between the lexicon and word segmentation model, variable word forms, loosely formed compound words absent in lexicons, and the lack of a Khmer named-entity recognition (NER) model which leads to false misspelling flags for proper nouns. Existing solutions do not sufficiently tackle these issues. To overcome this, the authors propose a holistic approach that integrates multiple components specifically tailored for Khmer: subword segmentation to better handle word boundaries, a Khmer NER system to identify proper nouns accurately, grapheme-to-phoneme (G2P) conversion to improve candidate generation, and a Khmer language model to rank correction candidates effectively. Through the combination of these elements, the approach aims to identify and suggest more suitable spelling corrections. Experimental results demonstrate that this integrated technique achieves a state-of-the-art accuracy of up to 94.4%, outperforming previous methods. Additionally, the study contributes to the Khmer language processing community by planning to release benchmark datasets for both spellchecking and NER tasks, enabling further research and development in this area. <div>
arXiv:2511.09812v1 Announce Type: new 
Abstract: Compared to English and other high-resource languages, spellchecking for Khmer remains an unresolved problem due to several challenges. First, there are misalignments between words in the lexicon and the word segmentation model. Second, a Khmer word can be written in different forms. Third, Khmer compound words are often loosely and easily formed, and these compound words are not always found in the lexicon. Fourth, some proper nouns may be flagged as misspellings due to the absence of a Khmer named-entity recognition (NER) model. Unfortunately, existing solutions do not adequately address these challenges. This paper proposes a holistic approach to the Khmer spellchecking problem by integrating Khmer subword segmentation, Khmer NER, Khmer grapheme-to-phoneme (G2P) conversion, and a Khmer language model to tackle these challenges, identify potential correction candidates, and rank the most suitable candidate. Experimental results show that the proposed approach achieves a state-of-the-art Khmer spellchecking accuracy of up to 94.4%, compared to existing solutions. The benchmark datasets for Khmer spellchecking and NER tasks in this study will be made publicly available.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Graduate Outcomes by Identifying Skills Gaps and Recommending Courses Based on Career Interests</title>
<link>https://arxiv.org/abs/2511.09819</link>
<guid>https://arxiv.org/abs/2511.09819</guid>
<content:encoded><![CDATA[
<div> Keywords: Course recommendation system, machine learning, data analytics, collaborative filtering, user interface<br /><br />Summary:<br /><br />This paper presents the design and development of a course recommendation system aimed at helping students select courses that align with industry trends and requirements. The system integrates various data analytics techniques and machine learning algorithms to provide tailored course suggestions based on individual preferences and academic criteria. It incorporates data mining and collaborative filtering methods to analyze previous courses taken and students' career goals for more personalized recommendations. A significant focus is placed on creating an intuitive and user-friendly front-end interface that emphasizes visual clarity, interaction, and simplicity through iterative prototyping and feedback-based refinements. The design process prioritizes a smooth and engaging user experience to enhance accessibility and usefulness. The system is continuously optimized by incorporating user feedback to better address the needs and preferences of its intended audience. This course recommendation system serves as a valuable tool for students, instructors, and career advisors by bridging the gap between university education and industry demands, thereby promoting lifelong learning and professional advancement. Ultimately, it aims to empower university students with data-driven, industry-informed course choices that contribute to improved graduate outcomes in the higher education sector. <div>
arXiv:2511.09819v1 Announce Type: new 
Abstract: This paper aims to address the challenge of selecting relevant courses for students by proposing the design and development of a course recommendation system. The course recommendation system utilises a combination of data analytics techniques and machine learning algorithms to recommend courses that align with current industry trends and requirements. In order to provide customised suggestions, the study entails the design and implementation of an extensive algorithmic framework that combines machine learning methods, user preferences, and academic criteria. The system employs data mining and collaborative filtering techniques to examine past courses and individual career goals in order to provide course recommendations. Moreover, to improve the accessibility and usefulness of the recommendation system, special attention is given to the development of an easy-to-use front-end interface. The front-end design prioritises visual clarity, interaction, and simplicity through iterative prototyping and user input revisions, guaranteeing a smooth and captivating user experience. We refined and optimised the proposed system by incorporating user feedback, ensuring that it effectively meets the needs and preferences of its target users. The proposed course recommendation system could be a useful tool for students, instructors, and career advisers to use in promoting lifelong learning and professional progression as it fills the gap between university learning and industry expectations. We hope that the proposed course recommendation system will help university students in making data-drive and industry-informed course decisions, in turn, improving graduate outcomes for the university sector.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Answering Students' Questions on Course Forums Using Multiple Chain-of-Thought Reasoning and Finetuning RAG-Enabled LLM</title>
<link>https://arxiv.org/abs/2511.09831</link>
<guid>https://arxiv.org/abs/2511.09831</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.09831v1  
Keywords: Large Language Model, Retrieval Augmented Generation, Question Answering, Fine-tuning, Hallucination  

<br /><br />Summary:  
This paper addresses the challenges in course forums where a growing number of student queries lead to delayed responses and repetitive questions for instructors. To solve these problems, the authors propose a question answering system based on an open source Large Language Model (LLM) enhanced with a Retrieval Augmented Generation (RAG) method. The system is fine-tuned using a relevant course dataset to better handle domain-specific queries. It uses a local knowledge base, containing all course content, to retrieve relevant documents corresponding to student questions. This retrieval step improves the accuracy and contextual relevance of the answers. To tackle the problem of hallucination — where LLMs generate plausible but incorrect answers — the system incorporates multi chain-of-thought reasoning, a technique designed to reduce such errors. The proposed approach is experimentally evaluated on the HotpotQA dataset, demonstrating strong performance on the question answering task. Overall, the combination of fine-tuning, document retrieval via RAG, and multi-step reasoning provides an effective solution to enhance timely and accurate responses in educational course forums. <div>
arXiv:2511.09831v1 Announce Type: new 
Abstract: The course forums are increasingly significant and play vital role in facilitating student discussions and answering their questions related to the course. It provides a platform for students to post their questions related to the content and admin issues related to the course. However, there are several challenges due to the increase in the number of students enrolled in the course. The primary challenge is that students' queries cannot be responded immediately and the instructors have to face lots of repetitive questions. To mitigate these issues, we propose a question answering system based on large language model with retrieval augmented generation (RAG) method. This work focuses on designing a question answering system with open source Large Language Model (LLM) and fine-tuning it on the relevant course dataset. To further improve the performance, we use a local knowledge base and applied RAG method to retrieve relevant documents relevant to students' queries, where the local knowledge base contains all the course content. To mitigate the hallucination of LLMs, We also integrate it with multi chain-of-thought reasoning to overcome the challenge of hallucination in LLMs. In this work, we experiment fine-tuned LLM with RAG method on the HotpotQA dataset. The experimental results demonstrate that the fine-tuned LLM with RAG method has a strong performance on question answering task.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TermGPT: Multi-Level Contrastive Fine-Tuning for Terminology Adaptation in Legal and Financial Domain</title>
<link>https://arxiv.org/abs/2511.09854</link>
<guid>https://arxiv.org/abs/2511.09854</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Terminology Adaptation, Contrastive Learning, Legal and Financial Domains, Term Discrimination<br /><br />Summary:<br /><br />Large language models (LLMs) demonstrate strong performance in text generation but struggle with isotropy issues in their embedding spaces, leading to poor discrimination of domain-specific terminology. This limitation is particularly critical in specialized fields like legal and financial domains, where nuanced semantic understanding is essential for downstream applications such as legal judgment prediction and financial risk analysis. To overcome this challenge, the paper proposes TermGPT, a novel multi-level contrastive fine-tuning framework aimed at improving terminology adaptation. TermGPT begins by constructing a sentence graph that captures both semantic and structural relationships in text, enabling the generation of positive and negative samples that are semantically consistent yet discriminative, guided by contextual and topological information. The framework employs multi-level contrastive learning, operating at both sentence and token levels, to simultaneously enhance global contextual comprehension and fine-grained terminology discrimination. To facilitate rigorous assessment, the authors create the first financial terminology dataset based on official regulatory documents, enabling domain-relevant evaluation. Experimental results show that TermGPT surpasses existing baseline models in tasks involving term discrimination within financial and legal texts, indicating its effectiveness in addressing terminology representation issues in specialized domains. <div>
arXiv:2511.09854v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance in text generation tasks; however, their embedding spaces often suffer from the isotropy problem, resulting in poor discrimination of domain-specific terminology, particularly in legal and financial contexts. This weakness in terminology-level representation can severely hinder downstream tasks such as legal judgment prediction or financial risk analysis, where subtle semantic distinctions are critical. To address this problem, we propose TermGPT, a multi-level contrastive fine-tuning framework designed for terminology adaptation. We first construct a sentence graph to capture semantic and structural relations, and generate semantically consistent yet discriminative positive and negative samples based on contextual and topological cues. We then devise a multi-level contrastive learning approach at both the sentence and token levels, enhancing global contextual understanding and fine-grained terminology discrimination. To support robust evaluation, we construct the first financial terminology dataset derived from official regulatory documents. Experiments show that TermGPT outperforms existing baselines in term discrimination tasks within the finance and legal domains.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Token Rationality Optimization: Towards Accurate and Concise LLM Reasoning via Self-Feedback</title>
<link>https://arxiv.org/abs/2511.09865</link>
<guid>https://arxiv.org/abs/2511.09865</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, chain-of-thought reasoning, token-level exploration, self-feedback, cross-domain transfer<br /><br />Summary:<br /><br />1. The paper addresses the challenge of training Large Language Models (LLMs) for chain-of-thought reasoning, where traditional supervised fine-tuning on a single "golden" rationale limits generalization by penalizing valid alternative answers. 2. Reinforcement learning methods with verifiable rewards face difficulties with credit assignment and require high computational costs, motivating the need for a new approach. 3. The authors propose InTRO (In-Token Rationality Optimization), a framework that facilitates token-level exploration combined with self-generated feedback, improving the accuracy and conciseness of reasoning chains. 4. InTRO works by estimating token-wise importance weights, called correction factors, based on the information discrepancy between the generative policy and an answer-conditioned policy, allowing informed next-token selection within a single forward pass. 5. Experimental results demonstrate that InTRO consistently improves solution accuracy by up to 20% compared to baseline models across six math-reasoning benchmarks, while producing notably more concise and less verbose chains of thought. 6. Additionally, InTRO shows strong cross-domain generalization, successfully adapting to out-of-domain reasoning tasks beyond mathematics, highlighting its broad applicability and robust performance. <div>
arXiv:2511.09865v1 Announce Type: new 
Abstract: Training Large Language Models (LLMs) for chain-of-thought reasoning presents a significant challenge: supervised fine-tuning on a single "golden" rationale hurts generalization as it penalizes equally valid alternatives, whereas reinforcement learning with verifiable rewards struggles with credit assignment and prohibitive computational cost. To tackle these limitations, we introduce InTRO (In-Token Rationality Optimization), a new framework that enables both token-level exploration and self-feedback for accurate and concise reasoning. Instead of directly optimizing an intractable objective over all valid reasoning paths, InTRO leverages correction factors-token-wise importance weights estimated by the information discrepancy between the generative policy and its answer-conditioned counterpart, for informative next token selection. This approach allows the model to perform token-level exploration and receive self-generated feedback within a single forward pass, ultimately encouraging accurate and concise rationales. Across six math-reasoning benchmarks, InTRO consistently outperforms other baselines, raising solution accuracy by up to 20% relative to the base model. Its chains of thought are also notably more concise, exhibiting reduced verbosity. Beyond this, InTRO enables cross-domain transfer, successfully adapting to out-of-domain reasoning tasks that extend beyond the realm of mathematics, demonstrating robust generalization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09873</link>
<guid>https://arxiv.org/abs/2511.09873</guid>
<content:encoded><![CDATA[
<div> Keywords: HierRouter, hierarchical routing, large language models, reinforcement learning, cost-efficient inference<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) achieve state-of-the-art results but are resource-intensive, posing challenges for deployment in constrained environments.  
2. The paper introduces HierRouter, a hierarchical routing framework that dynamically composes inference pipelines from multiple specialized, lightweight LLMs to optimize performance and cost.  
3. The routing problem is modeled as a finite-horizon Markov Decision Process (MDP), with a reinforcement learning agent trained using Proximal Policy Optimization (PPO) to select models iteratively at each multi-hop inference step.  
4. The agent makes decisions based on the evolving context and accumulated computational cost, enabling context-aware routing to balance quality and efficiency.  
5. Experiments conducted on six benchmarks, including question answering, code generation, and mathematical reasoning tasks, demonstrate that HierRouter enhances response quality by up to 2.4 times over using single models, with only minimal additional inference cost.  
6. The approach shows strong potential for enabling cost-efficient, high-performance LLM inference in real-time and resource-limited scenarios.  
7. The implementation and code are publicly available at https://github.com/Nikunj-Gupta/hierouter. <div>
arXiv:2511.09873v1 Announce Type: new 
Abstract: Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnchTable: Unified Safety Alignment Transfer in Fine-tuned Large Language Models</title>
<link>https://arxiv.org/abs/2511.09880</link>
<guid>https://arxiv.org/abs/2511.09880</guid>
<content:encoded><![CDATA[
<div> Safety alignment, Neural Tangent Kernel, fine-tuning, large language models, jailbreaking resistance<br /><br />Summary:<br /><br />Many machine learning models fine-tuned from large language models (LLMs) suffer from systematic degradation in safety alignment, leading to ethical risks and increased harmful outputs. EnchTable is introduced as a new framework that transfers and maintains safety alignment in downstream LLMs without requiring extensive retraining. The framework uses a Neural Tangent Kernel (NTK)-based safety vector distillation method, which effectively decouples safety constraints from task-specific reasoning, allowing broad compatibility across various architectures and sizes. It also incorporates an interference-aware merging technique to balance safety and utility, reducing performance compromises across different task domains. A fully functional prototype of EnchTable was implemented on three diverse task domains and LLM architectures, evaluated extensively on eleven datasets for utility and safety. EnchTable demonstrated strong generalization across models from multiple vendors. It also showed robust resistance to both static and dynamic jailbreaking attacks, outperforming vendor safety models in mitigating adversarial prompts. Compared to six parameter modification methods and two inference-time alignment baselines, EnchTable achieved lower unsafe rates, higher utility scores, and universal applicability. Finally, EnchTable integrates seamlessly into deployment pipelines without adding significant operational overhead. <div>
arXiv:2511.09880v1 Announce Type: new 
Abstract: Many machine learning models are fine-tuned from large language models (LLMs) to achieve high performance in specialized domains like code generation, biomedical analysis, and mathematical problem solving. However, this fine-tuning process often introduces a critical vulnerability: the systematic degradation of safety alignment, undermining ethical guidelines and increasing the risk of harmful outputs. Addressing this challenge, we introduce EnchTable, a novel framework designed to transfer and maintain safety alignment in downstream LLMs without requiring extensive retraining. EnchTable leverages a Neural Tangent Kernel (NTK)-based safety vector distillation method to decouple safety constraints from task-specific reasoning, ensuring compatibility across diverse model architectures and sizes. Additionally, our interference-aware merging technique effectively balances safety and utility, minimizing performance compromises across various task domains. We implemented a fully functional prototype of EnchTable on three different task domains and three distinct LLM architectures, and evaluated its performance through extensive experiments on eleven diverse datasets, assessing both utility and model safety. Our evaluations include LLMs from different vendors, demonstrating EnchTable's generalization capability. Furthermore, EnchTable exhibits robust resistance to static and dynamic jailbreaking attacks, outperforming vendor-released safety models in mitigating adversarial prompts. Comparative analyses with six parameter modification methods and two inference-time alignment baselines reveal that EnchTable achieves a significantly lower unsafe rate, higher utility score, and universal applicability across different task domains. Additionally, we validate EnchTable can be seamlessly integrated into various deployment pipelines without significant overhead.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HI-TransPA: Hearing Impairments Translation Personal Assistant</title>
<link>https://arxiv.org/abs/2511.09915</link>
<guid>https://arxiv.org/abs/2511.09915</guid>
<content:encoded><![CDATA[
<div> Keywords: Omni-Model, assistive technology, hearing-impaired, audio-visual, curriculum learning<br /><br />Summary:<br /><br />This article introduces HI-TransPA, an instruction-driven audio-visual personal assistant designed to enhance daily communication for hearing-impaired individuals by leveraging the Omni-Model paradigm. The proposed model effectively fuses indistinct speech with high-frame-rate lip dynamics, allowing for both translation and dialogue within a unified multimodal framework. To address the challenges posed by noisy, heterogeneous data and the limited adaptability of current Omni-Models to hearing-impaired speech, a comprehensive preprocessing and curation pipeline is developed. This pipeline detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses the quality of multimodal samples. The derived quality scores inform a curriculum learning strategy that initially trains on clean, high-confidence data and gradually introduces more challenging cases to improve model robustness. Additionally, the study incorporates a SigLIP encoder combined with a Unified 3D-Resampler to efficiently represent high-frame-rate lip motions. Experimental results on the HI-Dialogue dataset demonstrate that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. Overall, this work lays a foundational framework for applying Omni-Models in assistive communication technologies and provides essential tools and methodologies for future research in this domain. <div>
arXiv:2511.09915v1 Announce Type: new 
Abstract: To provide a unified and flexible solution for daily communication among hearing-impaired individuals, we introduce the Omni-Model paradigm into assistive technology and present HI-TransPA, an instruction-driven audio-visual personal assistant. The model fuses indistinct speech with high-frame-rate lip dynamics, enabling both translation and dialogue within a single multimodal framework. To tackle the challenges of noisy and heterogeneous raw data and the limited adaptability of existing Omni-Models to hearing-impaired speech, we construct a comprehensive preprocessing and curation pipeline that detects facial landmarks, isolates and stabilizes the lip region, and quantitatively assesses multimodal sample quality. These quality scores guide a curriculum learning strategy that first trains on clean, high-confidence samples and progressively incorporates harder cases to strengthen model robustness. We further adopt a SigLIP encoder combined with a Unified 3D-Resampler to efficiently encode high-frame-rate lip motion. Experiments on our purpose-built HI-Dialogue dataset show that HI-TransPA achieves state-of-the-art performance in both literal accuracy and semantic fidelity. This work establishes a foundation for applying Omni-Models to assistive communication technology, providing an end-to-end modeling framework and essential processing tools for future research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINDS: A Cross-cultural Dialogue Corpus for Social Norm Classification and Adherence Detection</title>
<link>https://arxiv.org/abs/2511.09918</link>
<guid>https://arxiv.org/abs/2511.09918</guid>
<content:encoded><![CDATA[
<div> Keywords: social norms, multi-turn dialogues, Norm-RAG, multilingual dataset, norm adherence  

<br /><br />Summary:  
This paper addresses the challenge of socially normative reasoning in conversational AI, emphasizing the subjective, context-dependent, and culturally diverse nature of social norms that traditional commonsense models do not capture. It introduces Norm-RAG, a retrieval-augmented agentic framework designed to infer nuanced social norms in multi-turn dialogues by modeling utterance-level attributes such as communicative intent, speaker roles, interpersonal framing, and linguistic cues. Norm-RAG leverages a novel Semantic Chunking approach to retrieve and ground dialogue analysis in structured normative documentation, enabling interpretable and context-aware assessments of normative adherence and violations. Additionally, the work contributes MINDS (Multilingual Interactions with Norm-Driven Speech), a new bilingual dataset containing 31 Mandarin-English and Spanish-English conversations annotated at each turn for norm category and adherence status, reflecting realistic and culturally diverse social interactions. Experimental results show that Norm-RAG enhances norm detection accuracy and generalizes better across cultural contexts, improving the capabilities of dialogue systems to act socially intelligent and culturally adaptive in multilingual conversation settings. This approach moves beyond isolated utterance analysis by handling the fluid and multi-turn nature of conversations with nuanced norm reasoning. <div>
arXiv:2511.09918v1 Announce Type: new 
Abstract: Social norms are implicit, culturally grounded expectations that guide interpersonal communication. Unlike factual commonsense, norm reasoning is subjective, context-dependent, and varies across cultures, posing challenges for computational models. Prior works provide valuable normative annotations but mostly target isolated utterances or synthetic dialogues, limiting their ability to capture the fluid, multi-turn nature of real-world conversations. In this work, we present Norm-RAG, a retrieval-augmented, agentic framework for nuanced social norm inference in multi-turn dialogues. Norm-RAG models utterance-level attributes including communicative intent, speaker roles, interpersonal framing, and linguistic cues and grounds them in structured normative documentation retrieved via a novel Semantic Chunking approach. This enables interpretable and context-aware reasoning about norm adherence and violation across multilingual dialogues. We further introduce MINDS (Multilingual Interactions with Norm-Driven Speech), a bilingual dataset comprising 31 multi-turn Mandarin-English and Spanish-English conversations. Each turn is annotated for norm category and adherence status using multi-annotator consensus, reflecting cross-cultural and realistic norm expression. Our experiments show that Norm-RAG improves norm detection and generalization, demonstrates improved performance for culturally adaptive and socially intelligent dialogue systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Identifying Knowledge Components</title>
<link>https://arxiv.org/abs/2511.09935</link>
<guid>https://arxiv.org/abs/2511.09935</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Components, Large Language Models, KC merging, cosine similarity, adaptive learning systems  

<br /><br />Summary:  
This study investigates the automation of Knowledge Component (KC) identification for adaptive learning systems using Large Language Models (LLMs), specifically GPT-4o-mini. The researchers scaled a "simulated textbook" prompting strategy to analyze a larger dataset of 646 multiple-choice questions. Initial results showed that the LLM-generated KCs performed worse than a domain expert-created KC model, with an RMSE of 0.4285 compared to 0.4206, while also producing an excessive number of KCs (569 versus 101). To address the problem of redundancy and overgeneration, the paper proposes a novel semantic merging approach that clusters semantically similar KCs based on their cosine similarity scores. Applying a cosine similarity threshold of 0.8 to merge KCs reduced their count from 569 to 428 and improved model accuracy, lowering the RMSE to 0.4259. The findings indicate that purely scaled LLM generation is insufficient for effective KC identification, but integrating semantic similarity-based merging offers a promising solution to automate and refine this process in adaptive learning contexts. This work advances the automation of educational content modeling by combining generation and post-processing techniques to reduce noise and redundancy in KC labels. <div>
arXiv:2511.09935v1 Announce Type: new 
Abstract: Knowledge Components (KCs) are foundational to adaptive learning systems, but their manual identification by domain experts is a significant bottleneck. While Large Language Models (LLMs) offer a promising avenue for automating this process, prior research has been limited to small datasets and has been shown to produce superfluous, redundant KC labels. This study addresses these limitations by first scaling a "simulated textbook" LLM prompting strategy (using GPT-4o-mini) to a larger dataset of 646 multiple-choice questions. We found that this initial automated approach performed significantly worse than an expert-designed KC model (RMSE 0.4285 vs. 0.4206) and generated an excessive number of KCs (569 vs. 101). To address the issue of redundancy, we proposed and evaluated a novel method for merging semantically similar KC labels based on their cosine similarity. This merging strategy significantly improved the model's performance; a model using a cosine similarity threshold of 0.8 achieved the best result, reducing the KC count to 428 and improving the RMSE to 0.4259. This demonstrates that while scaled LLM generation alone is insufficient, combining it with a semantic merging technique offers a viable path toward automating and refining KC identification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAP: Enhancing RAG with Recursive Evaluation and Adaptive Planning for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.09966</link>
<guid>https://arxiv.org/abs/2511.09966</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, multi-hop reasoning, global planning, Sub-task Planner, Fact Extractor  

<br /><br />Summary:  
This paper addresses limitations in current retrieval-augmented generation (RAG) approaches for multi-hop reasoning, highlighting their lack of global planning which often leads to local reasoning dead-ends. To resolve this, the authors propose Recursive Evaluation and Adaptive Planning (REAP), featuring two key components: the Sub-task Planner (SP) and the Fact Extractor (FE). SP maintains a global overview, guiding the reasoning trajectory by evaluating intermediate task states, while FE performs detailed analysis of retrieved content to extract reliable facts and clues. Together, these modules incrementally build a coherent global knowledge representation, improving both the reliability and traceability of reasoning processes. Additionally, a unified task paradigm is introduced to facilitate effective multi-task fine-tuning, which notably enhances SP’s capabilities on complex and data-scarce tasks. The effectiveness of REAP is validated through extensive experiments on multiple public multi-hop datasets, where it significantly outperforms existing RAG methods in both in-domain and out-of-domain scenarios. The results demonstrate that REAP provides a more robust and accurate solution for complex multi-hop reasoning challenges, mitigating hallucinations and improving reasoning outcomes in large language models. <div>
arXiv:2511.09966v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has been extensively employed to mitigate hallucinations in large language models (LLMs). However, existing methods for multi-hop reasoning tasks often lack global planning, increasing the risk of falling into local reasoning impasses. Insufficient exploitation of retrieved content and the neglect of latent clues fail to ensure the accuracy of reasoning outcomes. To overcome these limitations, we propose Recursive Evaluation and Adaptive Planning (REAP), whose core idea is to explicitly maintain structured sub-tasks and facts related to the current task through the Sub-task Planner (SP) and Fact Extractor (FE) modules. SP maintains a global perspective, guiding the overall reasoning direction and evaluating the task state based on the outcomes of FE, enabling dynamic optimization of the task-solving trajectory. FE performs fine-grained analysis over retrieved content to extract reliable answers and clues. These two modules incrementally enrich a logically coherent representation of global knowledge, enhancing the reliability and the traceability of the reasoning process. Furthermore, we propose a unified task paradigm design that enables effective multi-task fine-tuning, significantly enhancing SP's performance on complex, data-scarce tasks. We conduct extensive experiments on multiple public multi-hop datasets, and the results demonstrate that our method significantly outperforms existing RAG methods in both in-domain and out-of-domain settings, validating its effectiveness in complex multi-hop reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumPert: Numerical Perturbations to Probe Language Models for Veracity Prediction</title>
<link>https://arxiv.org/abs/2511.09971</link>
<guid>https://arxiv.org/abs/2511.09971</guid>
<content:encoded><![CDATA[
<div> Keywords: numerical reasoning, veracity prediction, robustness, large language models, fact-checking<br /><br />Summary:<br /><br />This paper evaluates the performance of state-of-the-art large language models on the task of numerical fact-checking, focusing on their ability to assess veracity of numerical claims paired with evidence. The study uses controlled perturbations, including label-flipping probes, to systematically test the robustness of these models. Results reveal that even leading proprietary models suffer significant accuracy drops, up to 62%, under specific perturbations, indicating a lack of consistent robustness across different conditions. Additionally, the research finds that increasing the length of the input context generally leads to a reduction in accuracy. However, when the extended context is enhanced with perturbed demonstration examples, many models show substantial recovery in performance. These findings highlight critical limitations of current language models in handling numerical reasoning tasks within fact-checking applications. The study concludes that improving robustness to numerical perturbations remains a significant open challenge for future development in large language model capabilities. <div>
arXiv:2511.09971v1 Announce Type: new 
Abstract: Large language models show strong performance on knowledge intensive tasks such as fact-checking and question answering, yet they often struggle with numerical reasoning. We present a systematic evaluation of state-of-the-art models for veracity prediction on numerical claims and evidence pairs using controlled perturbations, including label-flipping probes, to test robustness. Our results indicate that even leading proprietary systems experience accuracy drops of up to 62\% under certain perturbations. No model proves to be robust across all conditions. We further find that increasing context length generally reduces accuracy, but when extended context is enriched with perturbed demonstrations, most models substantially recover. These findings highlight critical limitations in numerical fact-checking and suggest that robustness remains an open challenge for current language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Uncertainty Trends for Timely Retrieval in Dynamic RAG</title>
<link>https://arxiv.org/abs/2511.09980</link>
<guid>https://arxiv.org/abs/2511.09980</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic retrieval, language models, Entropy-Trend Constraint, token-level uncertainty, retrieval timing<br /><br />Summary: Dynamic retrieval-augmented generation (RAG) improves large language models (LLMs) by enabling on-demand fetching of external knowledge, unlike static RAG which is less adaptable. The main challenge addressed is determining the optimal timing for retrieval to avoid delayed interventions after errors propagate. Existing methods rely on low token-level confidence to trigger retrieval, which can be too late. The paper introduces Entropy-Trend Constraint (ETC), a training-free method that models the dynamics of token-level uncertainty using first- and second-order differences of entropy sequences. This approach detects emerging uncertainty trends, facilitating earlier and more precise retrieval. Experiments conducted on six question-answering benchmarks using three different LLM backbones demonstrate ETC’s consistent superiority over strong baselines while also reducing the frequency of retrieval. ETC is especially effective in domain-specific contexts and shows robust generalization capabilities. Further ablation studies and qualitative analyses validate that modeling uncertainty trends leads to more effective retrieval timing. The method is plug-and-play, model-agnostic, and can be easily integrated into existing decoding pipelines. The authors provide implementation code in the supplementary materials to encourage adoption. <div>
arXiv:2511.09980v1 Announce Type: new 
Abstract: Dynamic retrieval-augmented generation (RAG) allows large language models (LLMs) to fetch external knowledge on demand, offering greater adaptability than static RAG. A central challenge in this setting lies in determining the optimal timing for retrieval. Existing methods often trigger retrieval based on low token-level confidence, which may lead to delayed intervention after errors have already propagated. We introduce Entropy-Trend Constraint (ETC), a training-free method that determines optimal retrieval timing by modeling the dynamics of token-level uncertainty. Specifically, ETC utilizes first- and second-order differences of the entropy sequence to detect emerging uncertainty trends, enabling earlier and more precise retrieval. Experiments on six QA benchmarks with three LLM backbones demonstrate that ETC consistently outperforms strong baselines while reducing retrieval frequency. ETC is particularly effective in domain-specific scenarios, exhibiting robust generalization capabilities. Ablation studies and qualitative analyses further confirm that trend-aware uncertainty modeling yields more effective retrieval timing. The method is plug-and-play, model-agnostic, and readily integrable into existing decoding pipelines. Implementation code is included in the supplementary materials.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Drift in Multilingual Retrieval-Augmented Generation: Characterization and Decoding-Time Mitigation</title>
<link>https://arxiv.org/abs/2511.09984</link>
<guid>https://arxiv.org/abs/2511.09984</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual Retrieval-Augmented Generation, Language Drift, Chain-of-Thought, Soft Constrained Decoding, Language Alignment  

<br /><br />Summary:  
This paper investigates the phenomenon of language drift in multilingual Retrieval-Augmented Generation (RAG) systems, where large language models generate responses in unintended languages when the retrieved evidence and input queries differ in language. The issue is most pronounced during reasoning-intensive tasks like Chain-of-Thought (CoT) generation, where the intermediate reasoning steps exacerbate language instability. Through comprehensive experiments across multiple datasets, languages, and various LLM backbones, the authors identify that the drift is due not to comprehension errors but to decoder-level collapse, driven by dominant token distributions and high-frequency English patterns. English emerges as a semantic attractor, acting as the primary interference source and fallback language in cross-lingual situations. To address this, the authors propose Soft Constrained Decoding (SCD), a training-free, lightweight decoding strategy that softly penalizes tokens from non-target languages to steer the generation toward the intended language. SCD is model-agnostic and can be integrated with any generation algorithm without requiring architectural changes or additional data. Experiments on three multilingual datasets covering typologically diverse languages demonstrate that SCD consistently enhances language alignment and overall task performance, offering an effective and generalizable solution for mitigating language drift in multilingual RAG systems. <div>
arXiv:2511.09984v1 Announce Type: new 
Abstract: Multilingual Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to perform knowledge-intensive tasks in multilingual settings by leveraging retrieved documents as external evidence. However, when the retrieved evidence differs in language from the user query and in-context exemplars, the model often exhibits language drift by generating responses in an unintended language. This phenomenon is especially pronounced during reasoning-intensive decoding, such as Chain-of-Thought (CoT) generation, where intermediate steps introduce further language instability. In this paper, we systematically study output language drift in multilingual RAG across multiple datasets, languages, and LLM backbones. Our controlled experiments reveal that the drift results not from comprehension failure but from decoder-level collapse, where dominant token distributions and high-frequency English patterns dominate the intended generation language. We further observe that English serves as a semantic attractor under cross-lingual conditions, emerging as both the strongest interference source and the most frequent fallback language.
  To mitigate this, we propose Soft Constrained Decoding (SCD), a lightweight, training-free decoding strategy that gently steers generation toward the target language by penalizing non-target-language tokens. SCD is model-agnostic and can be applied to any generation algorithm without modifying the architecture or requiring additional data. Experiments across three multilingual datasets and multiple typologically diverse languages show that SCD consistently improves language alignment and task performance, providing an effective and generalizable solution in multilingual RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinNuE: Exposing the Risks of Using BERTScore for Numerical Semantic Evaluation in Finance</title>
<link>https://arxiv.org/abs/2511.09997</link>
<guid>https://arxiv.org/abs/2511.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: BERTScore, numerical variation, finance, FinNuE, semantic similarity<br /><br />Summary: BERTScore is a popular metric used to evaluate semantic similarity between natural language sentences. However, this study identifies a critical limitation of BERTScore: it shows low sensitivity to numerical variation. This limitation is particularly problematic in finance, where small changes in numbers (such as distinguishing between a 2% gain and a 20% loss) fundamentally alter the meaning. To address this, the authors introduce FinNuE, a new diagnostic dataset featuring controlled numerical perturbations drawn from earnings calls, regulatory filings, social media, and news articles. Using FinNuE, they demonstrate that BERTScore often fails to detect semantically meaningful numerical differences and tends to assign high similarity scores to text pairs that differ substantially in financial implications. These results reveal foundational limitations of embedding-based metrics like BERTScore when applied to financial text. The study calls for the development of evaluation frameworks that are more numerically aware to improve semantic similarity assessments in financial natural language processing tasks. <div>
arXiv:2511.09997v1 Announce Type: new 
Abstract: BERTScore has become a widely adopted metric for evaluating semantic similarity between natural language sentences. However, we identify a critical limitation: BERTScore exhibits low sensitivity to numerical variation, a significant weakness in finance where numerical precision directly affects meaning (e.g., distinguishing a 2% gain from a 20% loss). We introduce FinNuE, a diagnostic dataset constructed with controlled numerical perturbations across earnings calls, regulatory filings, social media, and news articles. Using FinNuE, demonstrate that BERTScore fails to distinguish semantically critical numerical differences, often assigning high similarity scores to financially divergent text pairs. Our findings reveal fundamental limitations of embedding-based metrics for finance and motivate numerically-aware evaluation frameworks for financial NLP.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.10002</link>
<guid>https://arxiv.org/abs/2511.10002</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, NCERT curriculum, question-answering dataset, prompting techniques, educational AI tools

<br /><br />Summary:  
This paper introduces "PustakAI," a framework designed to create and evaluate "NCERT-QA," a novel question-answering dataset aligned with the NCERT syllabus for English and Science subjects targeting grades 6 to 8 in India. The dataset categorizes QA pairs into Factoid, Inferential, and Others (evaluative and reasoning) types to encompass different cognitive demands. Various prompting techniques, including meta-prompt, few-shot, and Chain-of-Thought (CoT) prompting, are employed to test how effectively each aligns with curriculum needs and question complexity. The study assesses different large language models, from smaller open-source models (Gemma3:1b, Llama3.2:3b, Nemotron-mini:4b) to more powerful ones (Llama-4-Scout-17B and Deepseek-r1-70B), evaluating their performance as AI educational aids. Findings highlight the strengths and limitations of these models in terms of accuracy, curriculum alignment, and pedagogical relevance, emphasizing the challenges in adapting LLMs to formal education systems, particularly in regions with limited teaching resources. The research contributes to personalized and interactive learning methodologies by offering a structured evaluation of AI tools against a specific national curriculum, potentially guiding future developments in AI-assisted education. <div>
arXiv:2511.10002v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework "PustakAI"\footnote{Pustak means `book' in many Indian languages.} for the design and evaluation of a novel question-answering dataset "NCERT-QA" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScaleFormer: Span Representation Cumulation for Long-Context Transformer</title>
<link>https://arxiv.org/abs/2511.10029</link>
<guid>https://arxiv.org/abs/2511.10029</guid>
<content:encoded><![CDATA[
<div> Keywords: ScaleFormer, long-context Transformer, sequence segmentation, parameter-free fusion, long-document summarization<br /><br />Summary:<br /><br />1. The paper addresses the computational challenge posed by the quadratic complexity of self-attention in Transformer models when processing long sequences, which limits their applicability in long-context tasks.<br />2. The authors introduce ScaleFormer, a plug-and-play framework designed to adapt existing pre-trained encoder-decoder Transformer models for long-sequence processing without modifying their architecture or requiring expensive pre-training.<br />3. ScaleFormer works by segmenting long input sequences into overlapping chunks and generating compressed, context-aware representations for each chunk to feed the decoder.<br />4. A novel, parameter-free fusion mechanism enriches each chunk’s boundary representations with cumulative context vectors from all preceding and succeeding chunks, granting the model structural awareness of each chunk’s position within the document and effectively capturing narrative flow.<br />5. This approach achieves linear computational complexity, enables efficient reasoning over long documents, and demonstrates highly competitive performance in long-document summarization tasks, often surpassing state-of-the-art methods without relying on architectural changes or external retrieval techniques. <div>
arXiv:2511.10029v1 Announce Type: new 
Abstract: The quadratic complexity of standard self-attention severely limits the application of Transformer-based models to long-context tasks. While efficient Transformer variants exist, they often require architectural changes and costly pre-training from scratch. To circumvent this, we propose ScaleFormer(Span Representation Cumulation for Long-Context Transformer) - a simple and effective plug-and-play framework that adapts off-the-shelf pre-trained encoder-decoder models to process long sequences without requiring architectural modifications. Our approach segments long inputs into overlapping chunks and generates a compressed, context-aware representation for the decoder. The core of our method is a novel, parameter-free fusion mechanism that endows each chunk's representation with structural awareness of its position within the document. It achieves this by enriching each chunk's boundary representations with cumulative context vectors from all preceding and succeeding chunks. This strategy provides the model with a strong signal of the document's narrative flow, achieves linear complexity, and enables pre-trained models to reason effectively over long-form text. Experiments on long-document summarization show that our method is highly competitive with and often outperforms state-of-the-art approaches without requiring architectural modifications or external retrieval mechanisms.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism</title>
<link>https://arxiv.org/abs/2511.10045</link>
<guid>https://arxiv.org/abs/2511.10045</guid>
<content:encoded><![CDATA[
<div> Keywords: Sound symbolism, Multimodal Large Language Models, phonetic iconicity, LEX-ICON dataset, phoneme-level attention  

<br /><br />Summary:  
This paper explores sound symbolism, the connection between phonetic forms and meanings, as a way to probe how Multimodal Large Language Models (MLLMs) process auditory language information. The study evaluates MLLMs on their ability to recognize phonetic iconicity using both textual inputs (orthographic and IPA transcriptions) and auditory inputs, spanning up to 25 semantic dimensions such as "sharp" versus "round." To facilitate this, the authors introduce LEX-ICON, a large dataset containing 8,052 mimetic words from English, French, Japanese, and Korean, along with 2,930 pseudo-words carefully constructed, all annotated with semantic features across text and audio modalities. Key findings reveal that MLLMs exhibit phonetic intuitions consistent with linguistic research across multiple semantic categories. In addition, analysis of phoneme-level attention scores shows distinctive phonosemantic attention patterns, indicating the models focus selectively on iconic phonemes during processing. These contributions provide a novel large-scale quantitative analysis linking AI model interpretability with cognitive linguistics, thereby advancing understanding of how artificial models encode sound-meaning relationships in human language. <div>
arXiv:2511.10045v1 Announce Type: new 
Abstract: Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt</title>
<link>https://arxiv.org/abs/2511.10051</link>
<guid>https://arxiv.org/abs/2511.10051</guid>
<content:encoded><![CDATA[
<div> Multi-turn instruction following, large language models, graph structures, relation extraction, dialogue systems  

<br /><br />Summary: This paper addresses the challenge of improving multi-turn instruction following in large language models (LLMs), which often treat each response generation independently and thus struggle with long-distance constraints in dialogues. The authors propose GraphIF, a plug-and-play framework that enhances LLMs by modeling multi-turn dialogues as directed relation graphs, capturing the inter-turn semantic relations naturally through graph structures. GraphIF consists of three main components: (1) an agent-based relation extraction module that uses action-triggered mechanisms to identify and construct relation graphs between dialogue turns; (2) a relation graph prompt generation module that transforms these structured graphs into natural language prompts; and (3) a response rewriting module that refines the LLM’s initial outputs by incorporating information from the graph prompts. By explicitly integrating multi-turn relational constraints into instruction following, GraphIF overcomes limitations of prior approaches relying solely on large datasets and isolated response generation. Extensive experiments conducted on two long multi-turn dialogue datasets demonstrate that GraphIF seamlessly integrates with instruction-tuned LLMs and significantly improves performance across four multi-turn instruction-following evaluation metrics. The proposed method highlights the effectiveness of graph-based modeling and prompting for enhancing dialogue consistency and adherence to instructions over multiple turns. <div>
arXiv:2511.10051v1 Announce Type: new 
Abstract: Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADI-20: Arabic Dialect Identification dataset and models</title>
<link>https://arxiv.org/abs/2511.10070</link>
<guid>https://arxiv.org/abs/2511.10070</guid>
<content:encoded><![CDATA[
<div> Arabic Dialect Identification, ADI-20, ECAPA-TDNN, Whisper encoder, Dataset expansion<br /><br />Summary:<br /><br />1. The paper introduces ADI-20, an extended dataset building upon the earlier ADI-17 dataset, now encompassing dialects from all Arabic-speaking countries. <br />2. ADI-20 comprises 3,556 hours of speech data, covering 19 Arabic dialects plus Modern Standard Arabic (MSA), making it a comprehensive resource for Arabic dialect identification (ADI).<br />3. The dataset was used to train and evaluate advanced ADI systems, including fine-tuning of pre-trained ECAPA-TDNN models and models based on Whisper encoder blocks combined with attention pooling and classification layers.<br />4. The study explores the impact of training data size and model parameter count on the performance of dialect identification, finding a minor reduction in F1 score when using only 30% of the original training data.<br />5. Both the dataset and the trained models are open-sourced to facilitate reproducibility and further research in the field of Arabic dialect identification. <div>
arXiv:2511.10070v1 Announce Type: new 
Abstract: We present ADI-20, an extension of the previously published ADI-17 Arabic Dialect Identification (ADI) dataset. ADI-20 covers all Arabic-speaking countries' dialects. It comprises 3,556 hours from 19 Arabic dialects in addition to Modern Standard Arabic (MSA). We used this dataset to train and evaluate various state-of-the-art ADI systems. We explored fine-tuning pre-trained ECAPA-TDNN-based models, as well as Whisper encoder blocks coupled with an attention pooling layer and a classification dense layer. We investigated the effect of (i) training data size and (ii) the model's number of parameters on identification performance. Our results show a small decrease in F1 score while using only 30% of the original training data. We open-source our collected data and trained models to enable the reproduction of our work, as well as support further research in ADI.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts</title>
<link>https://arxiv.org/abs/2511.10075</link>
<guid>https://arxiv.org/abs/2511.10075</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal LLMs, scientific claim verification, tables, charts, cross-modal generalization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying scientific claims using multimodal large language models (LLMs) that interpret experimental results presented in diverse evidence formats like tables and charts.<br /><br />2. To evaluate these models, the authors adapt two existing scientific paper datasets by adding annotations and structures tailored for multimodal claim verification tasks.<br /><br />3. A total of 12 multimodal LLMs are tested, revealing that current models perform significantly better when using tables as evidence compared to charts.<br /><br />4. Human evaluators maintain consistently strong performance across both evidence formats, highlighting a gap in model capabilities.<br /><br />5. Finally, smaller multimodal LLMs (under 8 billion parameters) exhibit weak performance correlation between table-based and chart-based verification tasks, suggesting limited ability to generalize reasoning across modalities. The study underscores the need for future multimodal LLM development to focus on enhancing chart understanding to improve scientific claim verification. <div>
arXiv:2511.10075v1 Announce Type: new 
Abstract: With the growing number of submitted scientific papers, there is an increasing demand for systems that can assist reviewers in evaluating research claims. Experimental results are a core component of scientific work, often presented in varying formats such as tables or charts. Understanding how robust current multimodal large language models (multimodal LLMs) are at verifying scientific claims across different evidence formats remains an important and underexplored challenge. In this paper, we design and conduct a series of experiments to assess the ability of multimodal LLMs to verify scientific claims using both tables and charts as evidence. To enable this evaluation, we adapt two existing datasets of scientific papers by incorporating annotations and structures necessary for a multimodal claim verification task. Using this adapted dataset, we evaluate 12 multimodal LLMs and find that current models perform better with table-based evidence while struggling with chart-based evidence. We further conduct human evaluations and observe that humans maintain strong performance across both formats, unlike the models. Our analysis also reveals that smaller multimodal LLMs (under 8B) show weak correlation in performance between table-based and chart-based tasks, indicating limited cross-modal generalization. These findings highlight a critical gap in current models' multimodal reasoning capabilities. We suggest that future multimodal LLMs should place greater emphasis on improving chart understanding to better support scientific claim verification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELYADATA &amp; LIA at NADI 2025: ASR and ADI Subtasks</title>
<link>https://arxiv.org/abs/2511.10090</link>
<guid>https://arxiv.org/abs/2511.10090</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Speech Processing, Dialect Identification, Whisper-large-v3, SeamlessM4T-v2, ASR

<br /><br />Summary:  
This paper presents Elyadata & LIA's joint submission to the NADI Multi-Dialectal Arabic Speech Processing 2025 challenge. The team participated in two subtasks: Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic Automatic Speech Recognition (ASR). Their ADI system, based on a fine-tuned Whisper-large-v3 encoder combined with data augmentation techniques, achieved the highest accuracy in the competition, scoring 79.83% on the official test set. For the multi-dialectal Arabic ASR task, the team fine-tuned the SeamlessM4T-v2 Large model (Egyptian variant) separately for each of the eight dialects included in the challenge. This approach yielded an average Word Error Rate (WER) of 38.54% and a Character Error Rate (CER) of 14.53% on the test set, ranking second among all participants. The results highlight the advantages of leveraging large pre-trained speech models with dialect-specific fine-tuning when addressing challenges in Arabic speech processing. Overall, the submission demonstrates state-of-the-art performance in both Arabic dialect identification and multi-dialect automatic speech recognition. <div>
arXiv:2511.10090v1 Announce Type: new 
Abstract: This paper describes Elyadata \& LIA's joint submission to the NADI multi-dialectal Arabic Speech Processing 2025. We participated in the Spoken Arabic Dialect Identification (ADI) and multi-dialectal Arabic ASR subtasks. Our submission ranked first for the ADI subtask and second for the multi-dialectal Arabic ASR subtask among all participants. Our ADI system is a fine-tuned Whisper-large-v3 encoder with data augmentation. This system obtained the highest ADI accuracy score of \textbf{79.83\%} on the official test set. For multi-dialectal Arabic ASR, we fine-tuned SeamlessM4T-v2 Large (Egyptian variant) separately for each of the eight considered dialects. Overall, we obtained an average WER and CER of \textbf{38.54\%} and \textbf{14.53\%}, respectively, on the test set. Our results demonstrate the effectiveness of large pre-trained speech models with targeted fine-tuning for Arabic speech processing.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Military Applications of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10093</link>
<guid>https://arxiv.org/abs/2511.10093</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, large language models, military applications, GPT, cloud services<br /><br />Summary: This paper explores the potential military use cases and implementations of natural language processing (NLP) and large language models (LLMs), particularly those based on the generative pre-trained transformer (GPT) architecture, such as OpenAI's ChatGPT and Microsoft Copilot. First, the authors interrogate a GPT-based model (Microsoft Copilot) to reveal its inherent knowledge about military applications, followed by a critical assessment of the responses to evaluate their relevance and practicality. Second, the study investigates how commercial cloud platforms, specifically Microsoft Azure, can be leveraged to develop and deploy such military-oriented NLP applications, focusing on feasibility, scalability, and integration potential. The findings highlight that the summarization and generative abilities of these language models offer significant advantages in various military contexts, enabling efficient information processing and decision support. Additionally, other features of LLMs, such as language translation, sentiment analysis, and automated report generation, are also identified as valuable for specialized uses in defense settings. Overall, the paper concludes that the current advancements in foundation models and cloud infrastructure collectively facilitate the rapid development and application of AI-driven language technologies within the military domain. <div>
arXiv:2511.10093v1 Announce Type: new 
Abstract: In this paper, military use cases or applications and implementation thereof are considered for natural language processing and large language models, which have broken into fame with the invention of the generative pre-trained transformer (GPT) and the extensive foundation model pretraining done by OpenAI for ChatGPT and others. First, we interrogate a GPT-based language model (viz. Microsoft Copilot) to make it reveal its own knowledge about their potential military applications and then critically assess the information. Second, we study how commercial cloud services (viz. Microsoft Azure) could be used readily to build such applications and assess which of them are feasible. We conclude that the summarization and generative properties of language models directly facilitate many applications at large and other features may find particular uses.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing to Unseen Disaster Events: A Causal View</title>
<link>https://arxiv.org/abs/2511.10120</link>
<guid>https://arxiv.org/abs/2511.10120</guid>
<content:encoded><![CDATA[
<div> keywords: social media, disaster monitoring, bias mitigation, causal learning, event classification<br /><br />Summary:<br /><br />The paper addresses the critical challenge of real-time processing and analysis of social media data during disaster events, highlighting the necessity of extracting valuable insights amid vast data volumes. A key problem identified is the presence of event-related biases in existing systems, which undermines their ability to generalize and adapt to new, emerging disaster events. The authors propose a novel approach that leverages causal learning techniques to mitigate biases related to specific events and domains. This causal lens approach aims to improve the robustness and generalizability of classification models dealing with disaster-related social media data. Their methodology is empirically validated across three distinct disaster classification tasks, demonstrating significant improvements. Specifically, their approach outperforms multiple baseline models by up to 1.9% in F1 score, indicating a meaningful enhancement in classification performance. Additionally, they show that their bias mitigation method significantly boosts the effectiveness of pre-trained language model (PLM)-based classifiers. Overall, the study contributes to the disaster monitoring domain by presenting an effective strategy for bias reduction, thereby enabling more reliable and adaptable real-time information extraction from social media platforms during evolving disaster scenarios. <div>
arXiv:2511.10120v1 Announce Type: new 
Abstract: Due to the rapid growth of social media platforms, these tools have become essential for monitoring information during ongoing disaster events. However, extracting valuable insights requires real-time processing of vast amounts of data. A major challenge in existing systems is their exposure to event-related biases, which negatively affects their ability to generalize to emerging events. While recent advancements in debiasing and causal learning offer promising solutions, they remain underexplored in the disaster event domain. In this work, we approach bias mitigation through a causal lens and propose a method to reduce event- and domain-related biases, enhancing generalization to future events. Our approach outperforms multiple baselines by up to +1.9% F1 and significantly improves a PLM-based classifier across three disaster classification tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Black Box: Demystifying Multi-Turn LLM Reasoning with VISTA</title>
<link>https://arxiv.org/abs/2511.10182</link>
<guid>https://arxiv.org/abs/2511.10182</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multi-turn reasoning, visualization, reasoning dependency tree, interactive analysis<br /><br />Summary: The paper addresses the challenge of analyzing the complex reasoning processes of Large Language Models (LLMs) during multi-turn interactions, which closely resemble real-world problem solving. Existing methods struggle with interpreting intricate contextual dependencies and lack effective visualization tools, resulting in a high cognitive load for researchers. To overcome these limitations, the authors introduce VISTA, a web-based Visual Interactive System for Textual Analytics specifically designed for multi-turn reasoning tasks. VISTA enables users to visualize how context influences model decisions and supports interactive modifications of conversation histories to perform “what-if” analyses across various models. A key feature of VISTA is its automatic parsing of sessions to generate reasoning dependency trees, which transparently map out the model’s logical reasoning path step-by-step. This unified and interactive framework simplifies the process of analyzing reasoning chains, allowing users to gain deeper insights into the strengths and weaknesses of current LLMs. Additionally, VISTA is open-source and designed for easy integration of custom benchmarks and local models, making it a versatile tool for researchers exploring LLM reasoning capabilities in conversational settings. <div>
arXiv:2511.10182v1 Announce Type: new 
Abstract: Recent research has increasingly focused on the reasoning capabilities of Large Language Models (LLMs) in multi-turn interactions, as these scenarios more closely mirror real-world problem-solving. However, analyzing the intricate reasoning processes within these interactions presents a significant challenge due to complex contextual dependencies and a lack of specialized visualization tools, leading to a high cognitive load for researchers. To address this gap, we present VISTA, an web-based Visual Interactive System for Textual Analytics in multi-turn reasoning tasks. VISTA allows users to visualize the influence of context on model decisions and interactively modify conversation histories to conduct "what-if" analyses across different models. Furthermore, the platform can automatically parse a session and generate a reasoning dependency tree, offering a transparent view of the model's step-by-step logical path. By providing a unified and interactive framework, VISTA significantly reduces the complexity of analyzing reasoning chains, thereby facilitating a deeper understanding of the capabilities and limitations of current LLMs. The platform is open-source and supports easy integration of custom benchmarks and local models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL</title>
<link>https://arxiv.org/abs/2511.10192</link>
<guid>https://arxiv.org/abs/2511.10192</guid>
<content:encoded><![CDATA[
<div> Text-to-SQL, Data Augmentation, SQLFlow, Large Language Models, Retrieval Method<br /><br />Summary:<br /><br />1. The paper addresses limitations in Text-to-SQL tasks caused by scarce, simplistic, and low-diversity datasets by proposing a data-centric approach.<br />2. It introduces Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data across six augmentation dimensions.<br />3. The framework integrates an end-to-end pipeline, including SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification, supported by a modular Database Manager for cross-database compatibility and scalability.<br />4. Using this approach, the authors build SQLFlow, a high-quality dataset with 89,544 annotated examples, which they evaluate in two settings: fine-tuning open-source LLMs and enhancing closed-source LLMs retrieval via a masked alignment retrieval method.<br />5. The masked alignment retrieval treats SQLFlow as both a knowledge base and training data for improved structure-aware example matching between questions and SQL, outperforming existing retrieval techniques.<br /><br />The work establishes a scalable, data-centric foundation to advance Text-to-SQL systems and emphasizes the critical importance of high-quality structured data in modern AI development. <div>
arXiv:2511.10192v1 Announce Type: new 
Abstract: The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EffiReason-Bench: A Unified Benchmark for Evaluating and Advancing Efficient Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10201</link>
<guid>https://arxiv.org/abs/2511.10201</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Chain-of-Thought prompting, efficient reasoning, benchmark, E3-Score  

<br /><br />Summary:  
This paper addresses the challenge of inefficiency in large language models (LLMs) when using Chain-of-Thought (CoT) prompting, which often generates overly long explanations that increase computational costs and can hurt accuracy. To facilitate fair comparisons of various efficiency-driven approaches, the authors introduce EffiReason-Bench, a comprehensive benchmark that evaluates methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. The benchmark includes verified step-by-step CoT annotations for CommonsenseQA and LogiQA datasets, created through a pipeline that standardizes reasoning structures, provides detailed option-wise analysis, and incorporates human verification to ensure quality. They test seven different efficiency methods on six open-source LLMs ranging from 1 billion to 70 billion parameters, across four datasets covering mathematics, commonsense reasoning, and logic. The study introduces the E3-Score, a new evaluation metric inspired by economic trade-off modeling that avoids common pitfalls such as discontinuities and heuristic dependencies. Experimental results reveal that no single efficiency approach is superior in all scenarios; instead, the best method depends on model size, task complexity, and model architecture, emphasizing the need for adaptable strategies tailored to specific contexts. <div>
arXiv:2511.10201v1 Announce Type: new 
Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) prompting achieve strong reasoning but often produce unnecessarily long explanations, increasing cost and sometimes reducing accuracy. Fair comparison of efficiency-oriented approaches is hindered by fragmented evaluation practices. We introduce EffiReason-Bench, a unified benchmark for rigorous cross-paradigm evaluation of efficient reasoning methods across three categories: Reasoning Blueprints, Dynamic Execution, and Post-hoc Refinement. To enable step-by-step evaluation, we construct verified CoT annotations for CommonsenseQA and LogiQA via a pipeline that enforces standardized reasoning structures, comprehensive option-wise analysis, and human verification. We evaluate 7 methods across 6 open-source LLMs (1B-70B) on 4 datasets spanning mathematics, commonsense, and logic, and propose the E3-Score, a principled metric inspired by economic trade-off modeling that provides smooth, stable evaluation without discontinuities or heavy reliance on heuristics. Experiments show that no single method universally dominates; optimal strategies depend on backbone scale, task complexity, and architecture.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persona-Aware Alignment Framework for Personalized Dialogue Generation</title>
<link>https://arxiv.org/abs/2511.10215</link>
<guid>https://arxiv.org/abs/2511.10215</guid>
<content:encoded><![CDATA[
<div> Keywords: personalized dialogue, persona alignment, two-stage training, Select then Generate, persona relevance<br /><br />Summary:<br /><br />Personalized dialogue generation focuses on using persona profiles and dialogue history to produce responses that are both relevant and consistent with a given persona. Traditional models typically depend on token-level language model training methods, such as Next Token Prediction, which only implicitly incorporate persona information and often result in generic outputs that neglect specific persona details. To solve this, the paper introduces the Persona-Aware Alignment Framework (PAL), which explicitly makes persona alignment the main training objective. PAL uses a two-stage training approach consisting of Persona-aware Learning and Persona Alignment stages, aiming to enhance persona sensitivity and generate semantically persona-relevant responses. Additionally, it includes an easy-to-use inference strategy called Select then Generate that further improves response quality. The authors conduct extensive experiments showing that PAL surpasses many state-of-the-art personalized dialogue techniques and large language models in persona relevance and response quality. Overall, this framework advances the ability of dialogue systems to generate more personalized, meaningful, and contextually appropriate responses. <div>
arXiv:2511.10215v1 Announce Type: new 
Abstract: Personalized dialogue generation aims to leverage persona profiles and dialogue history to generate persona-relevant and consistent responses. Mainstream models typically rely on token-level language model training with persona dialogue data, such as Next Token Prediction, to implicitly achieve personalization, making these methods tend to neglect the given personas and generate generic responses. To address this issue, we propose a novel Persona-Aware Alignment Framework (PAL), which directly treats persona alignment as the training objective of dialogue generation. Specifically, PAL employs a two-stage training method including Persona-aware Learning and Persona Alignment, equipped with an easy-to-use inference strategy Select then Generate, to improve persona sensitivity and generate more persona-relevant responses at the semantics level. Through extensive experiments, we demonstrate that our framework outperforms many state-of-the-art personalized dialogue methods and large language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangGPS: Language Separability Guided Data Pre-Selection for Joint Multilingual Instruction Tuning</title>
<link>https://arxiv.org/abs/2511.10229</link>
<guid>https://arxiv.org/abs/2511.10229</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual instruction tuning, language separability, data selection, low-resource languages, curriculum learning<br /><br />Summary: This paper introduces LangGPS, a two-stage pre-selection framework designed to enhance multilingual instruction tuning for large language models (LLMs) by leveraging the concept of language separability. Language separability measures how distinguishable samples from different languages are within the model's representation space. LangGPS first filters training data using separability scores, then refines the subset through existing selection methods based on quality, diversity, or task relevance. Through extensive experiments on six benchmarks across 22 languages, LangGPS is shown to improve the effectiveness and generalizability of multilingual training, particularly benefiting understanding tasks and low-resource languages. Analysis reveals that samples with high language separability help create clearer language boundaries and facilitate faster adaptation, while samples with low separability act as bridges aiding cross-lingual alignment. Additionally, the study finds that language separability is a valuable signal for multilingual curriculum learning by interleaving samples of varying separability levels, which contributes to stable and generalizable performance improvements. Overall, this work highlights the importance of incorporating linguistic structure into data selection processes and proposes a novel perspective on utilizing language separability to develop more linguistically informed and effective multilingual LLMs. <div>
arXiv:2511.10229v1 Announce Type: new 
Abstract: Joint multilingual instruction tuning is a widely adopted approach to improve the multilingual instruction-following ability and downstream performance of large language models (LLMs), but the resulting multilingual capability remains highly sensitive to the composition and selection of the training data. Existing selection methods, often based on features like text quality, diversity, or task relevance, typically overlook the intrinsic linguistic structure of multilingual data. In this paper, we propose LangGPS, a lightweight two-stage pre-selection framework guided by language separability which quantifies how well samples in different languages can be distinguished in the model's representation space. LangGPS first filters training data based on separability scores and then refines the subset using existing selection methods. Extensive experiments across six benchmarks and 22 languages demonstrate that applying LangGPS on top of existing selection methods improves their effectiveness and generalizability in multilingual training, especially for understanding tasks and low-resource languages. Further analysis reveals that highly separable samples facilitate the formation of clearer language boundaries and support faster adaptation, while low-separability samples tend to function as bridges for cross-lingual alignment. Besides, we also find that language separability can serve as an effective signal for multilingual curriculum learning, where interleaving samples with diverse separability levels yields stable and generalizable gains. Together, we hope our work offers a new perspective on data utility in multilingual contexts and support the development of more linguistically informed LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocalNet-M2: Advancing Low-Latency Spoken Language Modeling via Integrated Multi-Codebook Tokenization and Multi-Token Prediction</title>
<link>https://arxiv.org/abs/2511.10232</link>
<guid>https://arxiv.org/abs/2511.10232</guid>
<content:encoded><![CDATA[
<div> Keywords: VocalNet-M2, spoken language models, multi-codebook tokenizer, multi-token prediction, low latency<br /><br />Summary: Current end-to-end spoken language models (SLMs) face significant response latency mainly due to autoregressive speech token generation and the use of complex flow-matching models in speech synthesis. To address this challenge, the authors propose VocalNet-M2, a novel SLM that incorporates a multi-codebook tokenizer and a multi-token prediction (MTP) strategy. This architecture allows direct generation of multi-codebook speech tokens, thereby eliminating the latency caused by flow-matching models. The MTP strategy further boosts generation efficiency and improves overall model performance. Experimental results demonstrate that VocalNet-M2 effectively reduces the initial chunk latency from about 725 milliseconds to 350 milliseconds. Despite the improved latency, the model maintains competitive results compared to mainstream SLMs, highlighting its practical applicability. Additionally, the study offers a detailed comparison between single-codebook and multi-codebook approaches, providing important insights for optimizing SLM design for real-time interactive systems. This work advances the development of efficient, high-performance spoken language models that better meet the demands of low-latency applications. <div>
arXiv:2511.10232v1 Announce Type: new 
Abstract: Current end-to-end spoken language models (SLMs) have made notable progress, yet they still encounter considerable response latency. This delay primarily arises from the autoregressive generation of speech tokens and the reliance on complex flow-matching models for speech synthesis. To overcome this, we introduce VocalNet-M2, a novel low-latency SLM that integrates a multi-codebook tokenizer and a multi-token prediction (MTP) strategy. Our model directly generates multi-codebook speech tokens, thus eliminating the need for a latency-inducing flow-matching model. Furthermore, our MTP strategy enhances generation efficiency and improves overall performance. Extensive experiments demonstrate that VocalNet-M2 achieves a substantial reduction in first chunk latency (from approximately 725ms to 350ms) while maintaining competitive performance across mainstream SLMs. This work also provides a comprehensive comparison of single-codebook and multi-codebook strategies, offering valuable insights for developing efficient and high-performance SLMs for real-time interactive applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models</title>
<link>https://arxiv.org/abs/2511.10262</link>
<guid>https://arxiv.org/abs/2511.10262</guid>
<content:encoded><![CDATA[
<div> Full-Duplex, Speech Language Models, Multi-round Dialogue, Benchmark, Instruction Following  

<br /><br />Summary:  
This paper introduces Full-Duplex Speech Language Models (FD-SLMs), which allow for real-time, overlapping conversational interactions, enhancing user experience beyond traditional turn-taking models. Current evaluation benchmarks for FD-SLMs focus mainly on single-round interactions and overlook the complexities of multi-round communication, including essential capabilities like instruction following and conversational safety. The authors identify key challenges in assessing FD-SLMs in continuous dialogues, such as unclear turn boundaries and context inconsistency during inference. To overcome these issues, they present MTR-DuplexBench, a novel benchmark designed to segment continuous full-duplex dialogues into discrete turns. This allows for detailed, turn-by-turn evaluation of FD-SLMs across multiple dimensions: dialogue quality, conversational dynamics, instruction adherence, and safety. Experimental evaluations demonstrate that existing FD-SLMs struggle to maintain consistent performance across multiple dialogue rounds and evaluation criteria. These findings underline the importance of the proposed benchmark for advancing the development and robust assessment of FD-SLMs. The benchmark and associated codebase are planned for future public release to facilitate further research and improvement in this emerging field. <div>
arXiv:2511.10262v1 Announce Type: new 
Abstract: Full-Duplex Speech Language Models (FD-SLMs) enable real-time, overlapping conversational interactions, offering a more dynamic user experience compared to traditional half-duplex models. However, existing benchmarks primarily focus on evaluating single-round interactions and conversational features, neglecting the complexities of multi-round communication and critical capabilities such as instruction following and safety. Evaluating FD-SLMs in multi-round settings poses significant challenges, including blurred turn boundaries in communication and context inconsistency during model inference. To address these gaps, we introduce MTR-DuplexBench, a novel benchmark that segments continuous full-duplex dialogues into discrete turns, enabling comprehensive, turn-by-turn evaluation of FD-SLMs across dialogue quality, conversational dynamics, instruction following, and safety. Experimental results reveal that current FD-SLMs face difficulties in maintaining consistent performance across multiple rounds and evaluation dimensions, highlighting the necessity and effectiveness of our proposed benchmark. The benchmark and code will be available in the future.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local Hybrid Retrieval-Augmented Document QA</title>
<link>https://arxiv.org/abs/2511.10297</link>
<guid>https://arxiv.org/abs/2511.10297</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering system, data privacy, local infrastructure, semantic understanding, enterprise AI<br /><br />Summary:<br /><br />1. The paper addresses a critical dilemma faced by organizations handling sensitive documents: choosing between cloud-based AI systems with strong question-answering capabilities but compromised privacy, and local systems with better security but poorer accuracy.<br />2. The authors propose a novel question-answering system that operates entirely on local infrastructure with no internet access, combining semantic understanding and keyword precision to balance privacy and performance.<br />3. This system achieves competitive accuracy on complex queries across various document types, including legal, scientific, and conversational texts.<br />4. Using consumer-grade hardware acceleration, the approach delivers reliable answers with minimal errors, demonstrating that high accuracy does not require cloud dependency.<br />5. The solution enables privacy-sensitive institutions such as banks, hospitals, and law firms to adopt conversational document AI confidently, ensuring proprietary information remains on-premises.<br />6. Overall, the work establishes that enterprise AI deployments can achieve both strong performance and stringent privacy without compromise. <div>
arXiv:2511.10297v1 Announce Type: new 
Abstract: Organizations handling sensitive documents face a critical dilemma: adopt cloud-based AI systems that offer powerful question-answering capabilities but compromise data privacy, or maintain local processing that ensures security but delivers poor accuracy. We present a question-answering system that resolves this trade-off by combining semantic understanding with keyword precision, operating entirely on local infrastructure without internet access. Our approach demonstrates that organizations can achieve competitive accuracy on complex queries across legal, scientific, and conversational documents while keeping all data on their machines. By balancing two complementary retrieval strategies and using consumer-grade hardware acceleration, the system delivers reliable answers with minimal errors, letting banks, hospitals, and law firms adopt conversational document AI without transmitting proprietary information to external providers. This work establishes that privacy and performance need not be mutually exclusive in enterprise AI deployment.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectify Evaluation Preference: Improving LLMs' Critique on Math Reasoning via Perplexity-aware Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10303</link>
<guid>https://arxiv.org/abs/2511.10303</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-step Mathematical Reasoning, Large Language Models, imbalanced evaluation preference, perplexity-aware reinforcement learning, critique capability

<br /><br />Summary: To enhance the Multi-step Mathematical Reasoning (MsMR) abilities of Large Language Models (LLMs), this paper identifies the challenge of obtaining scalable supervision via automatic critique of reasoning errors and final judgment accuracy. Unlike previous works focusing on fine-tuning demonstrations for improving critique skills, the authors investigate the root cause of poor critiquing performance, revealing an "imbalanced evaluation preference" in LLMs. Specifically, LLMs tend to judge solutions with lower perplexity as correct, regardless of actual correctness. To explore this behavior, the authors create a One-to-many Problem-Solution (OPS) benchmark that measures how LLMs evaluate their own versus others' generated solutions. Through statistical preference analysis centered on perplexity, they confirm this bias. To address it, they propose a perplexity-aware reinforcement learning algorithm based on Group Relative Policy Optimization, which encourages LLMs to challenge their default tendency by exploring solution trajectories where higher perplexity solutions can be correct and lower perplexity ones incorrect. Extensive experiments on the OPS benchmark and other critic datasets show that this method effectively improves critiquing capabilities of LLMs, leading to better supervision for MsMR tasks and more balanced evaluation outcomes. <div>
arXiv:2511.10303v1 Announce Type: new 
Abstract: To improve Multi-step Mathematical Reasoning (MsMR) of Large Language Models (LLMs), it is crucial to obtain scalable supervision from the corpus by automatically critiquing mistakes in the reasoning process of MsMR and rendering a final verdict of the problem-solution. Most existing methods rely on crafting high-quality supervised fine-tuning demonstrations for critiquing capability enhancement and pay little attention to delving into the underlying reason for the poor critiquing performance of LLMs. In this paper, we orthogonally quantify and investigate the potential reason -- imbalanced evaluation preference, and conduct a statistical preference analysis. Motivated by the analysis of the reason, a novel perplexity-aware reinforcement learning algorithm is proposed to rectify the evaluation preference, elevating the critiquing capability. Specifically, to probe into LLMs' critiquing characteristics, a One-to-many Problem-Solution (OPS) benchmark is meticulously constructed to quantify the behavior difference of LLMs when evaluating the problem solutions generated by itself and others. Then, to investigate the behavior difference in depth, we conduct a statistical preference analysis oriented on perplexity and find an intriguing phenomenon -- ``LLMs incline to judge solutions with lower perplexity as correct'', which is dubbed as \textit{imbalanced evaluation preference}. To rectify this preference, we regard perplexity as the baton in the algorithm of Group Relative Policy Optimization, supporting the LLMs to explore trajectories that judge lower perplexity as wrong and higher perplexity as correct. Extensive experimental results on our built OPS and existing available critic benchmarks demonstrate the validity of our method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BhashaKritika: Building Synthetic Pretraining Data at Scale for Indic Languages</title>
<link>https://arxiv.org/abs/2511.10338</link>
<guid>https://arxiv.org/abs/2511.10338</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, multilingual pretraining, Indic languages, data quality evaluation, large-scale dataset<br /><br />Summary: This study investigates the generation and evaluation of synthetic multilingual pretraining data specifically for 10 Indic languages, addressing the scarcity of high-quality resources in low-resource language settings. The authors introduce BhashaKritika, a large-scale synthetic dataset containing 540 billion tokens created using five different data generation techniques. The work explores how grounding data generation in documents, personas, and topics influences data quality and examines the effects of language choice in prompt instructions and document grounding on the final dataset. A comparison between translations of English content and native generation in Indic languages is also presented to determine the most effective approach for data synthesis. To ensure quality control at scale and across various scripts, the authors develop a modular evaluation pipeline incorporating script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity filtering using KenLM language models. The paper details empirical results from model training runs using this data, highlighting important trade-offs in synthetic data generation strategies and sharing best practices for constructing robust multilingual corpora suitable for LLM pretraining. This work advances understanding of effective synthetic data creation and quality assurance for less-resourced languages in the multilingual AI landscape. <div>
arXiv:2511.10338v1 Announce Type: new 
Abstract: In the context of pretraining of Large Language Models (LLMs), synthetic data has emerged as an alternative for generating high-quality pretraining data at scale. This is particularly beneficial in low-resource language settings where the benefits of recent LLMs have been unevenly distributed across languages. In this work, we present a systematic study on the generation and evaluation of synthetic multilingual pretraining data for Indic languages, where we construct a large-scale synthetic dataset BhashaKritika, comprising 540B tokens using 5 different techniques for 10 languages. We explore the impact of grounding generation in documents, personas, and topics. We analyze how language choice, both in the prompt instructions and document grounding, affects data quality, and we compare translations of English content with native generation in Indic languages. To support scalable and language-sensitive evaluation, we introduce a modular quality evaluation pipeline that integrates script and language detection, metadata consistency checks, n-gram repetition analysis, and perplexity-based filtering using KenLM models. Our framework enables robust quality control across diverse scripts and linguistic contexts. Empirical results through model runs reveal key trade-offs in generation strategies and highlight best practices for constructing effective multilingual corpora.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graphs Generation from Cultural Heritage Texts: Combining LLMs and Ontological Engineering for Scholarly Debates</title>
<link>https://arxiv.org/abs/2511.10354</link>
<guid>https://arxiv.org/abs/2511.10354</guid>
<content:encoded><![CDATA[
<div> Keywords: Cultural Heritage, Knowledge Graphs, Large Language Models, Text-to-RDF, Ontologies  

<br /><br />Summary:  
This paper presents ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a novel five-step methodology designed to extract structured knowledge from unstructured Cultural Heritage texts using Large Language Models (LLMs). The approach integrates annotation models, ontological frameworks, and iterative LLM-based extraction through stages including foundational analysis, annotation schema development, pipeline architecture design, integration refinement, and comprehensive evaluation. A case study focused on authenticity assessment debates demonstrates the method's application by processing Wikipedia articles about disputed cultural artifacts and documents. The sequential pipeline employs three LLMs: Claude Sonnet 3.7, Llama 3.3 70B, and GPT-4o-mini. Results indicate high performance in various extraction tasks with F1 scores ranging from 0.62 to 0.99, showing that smaller models can achieve competitive outcomes while reducing costs. This work is original in providing the first systematic coordination of LLM-based knowledge extraction with Cultural Heritage ontologies, offering a replicable framework adaptable across different domains and institutional setups. However, the knowledge graph output is currently limited to Wikipedia articles, and human oversight remains important for post-processing. Practically, ATR4CH supports Cultural Heritage organizations in transforming textual data into queryable Knowledge Graphs, facilitating metadata enrichment and automated knowledge discovery. <div>
arXiv:2511.10354v1 Announce Type: new 
Abstract: Cultural Heritage texts contain rich knowledge that is difficult to query systematically due to the challenges of converting unstructured discourse into structured Knowledge Graphs (KGs). This paper introduces ATR4CH (Adaptive Text-to-RDF for Cultural Heritage), a systematic five-step methodology for Large Language Model-based Knowledge Extraction from Cultural Heritage documents. We validate the methodology through a case study on authenticity assessment debates. Methodology - ATR4CH combines annotation models, ontological frameworks, and LLM-based extraction through iterative development: foundational analysis, annotation schema development, pipeline architecture, integration refinement, and comprehensive evaluation. We demonstrate the approach using Wikipedia articles about disputed items (documents, artifacts...), implementing a sequential pipeline with three LLMs (Claude Sonnet 3.7, Llama 3.3 70B, GPT-4o-mini). Findings - The methodology successfully extracts complex Cultural Heritage knowledge: 0.96-0.99 F1 for metadata extraction, 0.7-0.8 F1 for entity recognition, 0.65-0.75 F1 for hypothesis extraction, 0.95-0.97 for evidence extraction, and 0.62 G-EVAL for discourse representation. Smaller models performed competitively, enabling cost-effective deployment. Originality - This is the first systematic methodology for coordinating LLM-based extraction with Cultural Heritage ontologies. ATR4CH provides a replicable framework adaptable across CH domains and institutional resources. Research Limitations - The produced KG is limited to Wikipedia articles. While the results are encouraging, human oversight is necessary during post-processing. Practical Implications - ATR4CH enables Cultural Heritage institutions to systematically convert textual knowledge into queryable KGs, supporting automated metadata enrichment and knowledge discovery.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TruthfulRAG: Resolving Factual-level Conflicts in Retrieval-Augmented Generation with Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10375</link>
<guid>https://arxiv.org/abs/2511.10375</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Knowledge Graphs, Conflict Resolution, Large Language Models, Factual Inconsistency<br /><br />Summary:  
The paper introduces TruthfulRAG, a novel framework designed to enhance Retrieval-Augmented Generation (RAG) systems by addressing the challenge of knowledge conflicts between retrieved external information and the internal knowledge of Large Language Models (LLMs). Unlike existing conflict resolution methods that operate at the token or semantic level and often fail to fully capture factual discrepancies, TruthfulRAG employs Knowledge Graphs (KGs) to conduct factual-level conflict resolution. The framework constructs KGs through systematic extraction of triples from the retrieved content. It then applies query-based graph retrieval techniques to identify and isolate relevant knowledge for a given input. Furthermore, TruthfulRAG integrates an entropy-based filtering mechanism to accurately pinpoint conflicting elements in the knowledge and to mitigate factual inconsistencies. These mechanisms collectively empower LLMs to generate responses that are more faithful, accurate, and reliable. Extensive experimental evaluation demonstrates that TruthfulRAG outperforms existing approaches by significantly improving robustness and trustworthiness in knowledge-intensive tasks, alleviating errors caused by conflicting information, and ensuring higher fidelity in generative outputs. The work addresses the growing need for dependable RAG systems as external knowledge bases expand and internal parametric knowledge becomes outdated. <div>
arXiv:2511.10375v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for enhancing the capabilities of Large Language Models (LLMs) by integrating retrieval-based methods with generative models. As external knowledge repositories continue to expand and the parametric knowledge within models becomes outdated, a critical challenge for RAG systems is resolving conflicts between retrieved external information and LLMs' internal knowledge, which can significantly compromise the accuracy and reliability of generated content. However, existing approaches to conflict resolution typically operate at the token or semantic level, often leading to fragmented and partial understanding of factual discrepancies between LLMs' knowledge and context, particularly in knowledge-intensive tasks. To address this limitation, we propose TruthfulRAG, the first framework that leverages Knowledge Graphs (KGs) to resolve factual-level knowledge conflicts in RAG systems. Specifically, TruthfulRAG constructs KGs by systematically extracting triples from retrieved content, utilizes query-based graph retrieval to identify relevant knowledge, and employs entropy-based filtering mechanisms to precisely locate conflicting elements and mitigate factual inconsistencies, thereby enabling LLMs to generate faithful and accurate responses. Extensive experiments reveal that TruthfulRAG outperforms existing methods, effectively alleviating knowledge conflicts and improving the robustness and trustworthiness of RAG systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Position: On the Methodological Pitfalls of Evaluating Base LLMs for Reasoning</title>
<link>https://arxiv.org/abs/2511.10381</link>
<guid>https://arxiv.org/abs/2511.10381</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning evaluation, base LLMs, methodological concerns, instruction-following  

<br /><br />Summary:  
1. The paper critiques existing studies that evaluate the reasoning capabilities of base large language models (LLMs), which are pre-trained solely on unlabeled corpora.  
2. It highlights a fundamental methodological issue: the training objective of base LLMs is based on statistical likelihood of language patterns, which does not align with normative qualities like logical correctness used to assess reasoning.  
3. As a result, when base LLMs generate logically valid or invalid conclusions, these outputs are better understood as coincidental consequences of linguistic pattern conformity rather than genuine reasoning attempts.  
4. This mismatch calls into question two common assumptions: (a) that base LLM outputs represent bona fide attempts at correct answers, and (b) that reasoning evaluations on base LLMs generalize to post-trained LLMs that are fine-tuned for following instructions.  
5. The authors urge the research community to critically re-examine prior work relying on these assumptions and advocate for future studies to explicitly account for these methodological pitfalls when assessing reasoning in LLMs. <div>
arXiv:2511.10381v1 Announce Type: new 
Abstract: Existing work investigates the reasoning capabilities of large language models (LLMs) to uncover their limitations, human-like biases and underlying processes. Such studies include evaluations of base LLMs (pre-trained on unlabeled corpora only) for this purpose. Our position paper argues that evaluating base LLMs' reasoning capabilities raises inherent methodological concerns that are overlooked in such existing studies. We highlight the fundamental mismatch between base LLMs' pretraining objective and normative qualities, such as correctness, by which reasoning is assessed. In particular, we show how base LLMs generate logically valid or invalid conclusions as coincidental byproducts of conforming to purely linguistic patterns of statistical plausibility. This fundamental mismatch challenges the assumptions that (a) base LLMs' outputs can be assessed as their bona fide attempts at correct answers or conclusions; and (b) conclusions about base LLMs' reasoning can generalize to post-trained LLMs optimized for successful instruction-following. We call for a critical re-examination of existing work that relies implicitly on these assumptions, and for future work to account for these methodological pitfalls.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DELICATE: Diachronic Entity LInking using Classes And Temporal Evidence</title>
<link>https://arxiv.org/abs/2511.10404</link>
<guid>https://arxiv.org/abs/2511.10404</guid>
<content:encoded><![CDATA[
<div> Entity Linking, Historical Italian, Neuro-symbolic, DELICATE, ENEIDE  

<br /><br />Summary:  
This paper addresses the challenges of Entity Linking (EL) in the humanities, specifically for historical Italian texts, which are complicated by diverse document types, scarce domain-specific datasets, and prevalence of long-tail entities underrepresented in Knowledge Bases (KBs). The authors propose DELICATE, a novel neuro-symbolic EL approach that integrates a BERT-based encoder with contextual data from Wikidata to enhance entity selection by leveraging temporal plausibility and entity type consistency. Additionally, the study introduces ENEIDE, a multi-domain EL corpus created through semi-automatic extraction from two annotated editions of Italian texts ranging from the 19th to 20th centuries, covering literary and political domains. Experimental results demonstrate that DELICATE outperforms existing EL models for historical Italian, even surpassing larger neural architectures with billions of parameters. Furthermore, the paper highlights that DELICATE’s confidence scores and feature sensitivity analyses contribute to better explainability and interpretability compared to purely neural EL methods, making it more transparent for end users in humanities research. Overall, the contributions significantly advance EL for historical texts by combining symbolic knowledge with modern neural techniques and providing a valuable dataset for future work. <div>
arXiv:2511.10404v1 Announce Type: new 
Abstract: In spite of the remarkable advancements in the field of Natural Language Processing, the task of Entity Linking (EL) remains challenging in the field of humanities due to complex document typologies, lack of domain-specific datasets and models, and long-tail entities, i.e., entities under-represented in Knowledge Bases (KBs). The goal of this paper is to address these issues with two main contributions. The first contribution is DELICATE, a novel neuro-symbolic method for EL on historical Italian which combines a BERT-based encoder with contextual information from Wikidata to select appropriate KB entities using temporal plausibility and entity type consistency. The second contribution is ENEIDE, a multi-domain EL corpus in historical Italian semi-automatically extracted from two annotated editions spanning from the 19th to the 20th century and including literary and political texts. Results show how DELICATE outperforms other EL models in historical Italian even if compared with larger architectures with billions of parameters. Moreover, further analyses reveal how DELICATE confidence scores and features sensitivity provide results which are more explainable and interpretable than purely neural methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analogical Structure, Minimal Contextual Cues and Contrastive Distractors: Input Design for Sample-Efficient Linguistic Rule Induction</title>
<link>https://arxiv.org/abs/2511.10441</link>
<guid>https://arxiv.org/abs/2511.10441</guid>
<content:encoded><![CDATA[
<div> Keywords: analogical structure, contrastive learning, lightweight models, linguistic rule learning, minimal data<br /><br />Summary:<br /><br />This paper explores whether organizing learning around analogical paradigms can allow lightweight language models to perform competitively with minimal training data, contrasting with conventional large models trained on vast datasets. The authors design a computational approach combining three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. They evaluate this approach using structured completion tasks where models select correct sentence completions from analogical patterns presented alongside contrastive alternatives. Using models with only 0.5 million parameters (BERT+CNN) trained on just 100 structured examples of English causative/inchoative alternations, the method achieves an F1 score of 0.95, outperforming zero-shot GPT-3’s 0.87 F1. Ablation studies confirm the importance of analogical organization and contrastive training, showing consistent performance gains over randomized baseline setups across different model architectures. Additional validation on another linguistic phenomenon, unspecified object alternations, replicates these efficiency improvements and demonstrates the robustness of the approach. The results suggest that organizing training paradigms analogically allows for competitive linguistic rule learning with orders of magnitude less data compared to conventional deep learning methods, opening paths toward more data-efficient language models. <div>
arXiv:2511.10441v1 Announce Type: new 
Abstract: Large language models achieve strong performance through training on vast datasets. Can analogical paradigm organization enable lightweight models to match this performance with minimal data? We develop a computational approach implementing three cognitive-inspired principles: analogical structure, contrastive learning, and minimal contextual cues. We test this approach with structured completion tasks where models identify correct sentence completions from analogical patterns with contrastive alternatives. Training lightweight models (BERT+CNN, $0.5M$ parameters) on only one hundred structured examples of English causative/inchoative alternations achieves $F1=0.95$, outperforming zero-shot \texttt{GPT-o3} ($F1=0.87$). Ablation studies confirm that analogical organization and contrastive structure improve performance, consistently surpassing randomly shuffled baselines across architectures. Cross-phenomenon validation using unspecified object alternations replicates these efficiency gains, confirming approach robustness. Our results show that analogical paradigm organization enables competitive linguistic rule learning with orders of magnitude less data than conventional approaches require.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning About Intent for Ambiguous Requests</title>
<link>https://arxiv.org/abs/2511.10453</link>
<guid>https://arxiv.org/abs/2511.10453</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, ambiguous requests, reinforcement learning, multiple interpretations, structured responses<br /><br />Summary:<br /><br />Large language models (LLMs) often handle ambiguous requests by selecting a single interpretation, which can lead to misunderstandings and potential safety issues. To mitigate this problem, the authors propose a method that generates multiple interpretation-answer pairs within a single structured response, thereby explicitly capturing different valid meanings. The approach leverages reinforcement learning with customized reward functions that use multiple valid answers as supervision signals, encouraging the model to cover a broader set of interpretations. The method is evaluated on tasks such as conversational question answering and semantic parsing, showing improved coverage of valid answers compared to baseline models. Human evaluations further verify that the predicted interpretations correspond well to their respective answers, enhancing the transparency of model outputs. This approach not only clarifies ambiguous requests by explicitly presenting multiple interpretations but also improves efficiency by requiring only one generation step to produce a structured output. Additionally, the structured format facilitates easier integration with downstream applications, making it a practical enhancement for handling ambiguity in LLM-driven systems. <div>
arXiv:2511.10453v1 Announce Type: new 
Abstract: Large language models often respond to ambiguous requests by implicitly committing to one interpretation. Intent misunderstandings can frustrate users and create safety risks. To address this, we propose generating multiple interpretation-answer pairs in a single structured response to ambiguous requests. Our models are trained with reinforcement learning and customized reward functions using multiple valid answers as supervision. Experiments on conversational question answering and semantic parsing demonstrate that our method achieves higher coverage of valid answers than baseline approaches. Human evaluation confirms that predicted interpretations are highly aligned with their answers. Our approach promotes transparency with explicit interpretations, achieves efficiency by requiring only one generation step, and supports downstream applications through its structured output format.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring State Tracking Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10457</link>
<guid>https://arxiv.org/abs/2511.10457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, state tracking, GPT-4, Llama3, Chain of Thought<br /><br />Summary:<br /><br />This paper investigates the capability of Large Language Models (LLMs) to perform state tracking, a task that requires maintaining and updating the state of multiple entities over time. To isolate the state tracking ability, the authors introduce a benchmark consisting of three distinct state tracking tasks designed to evaluate model performance in controlled scenarios. The evaluation focuses on recent LLMs, specifically GPT-4 and Llama3, assessing how well they track state information across several steps. Results demonstrate that these newer models show strong proficiency in tracking state, particularly when enhanced by techniques like Chain of Thought prompting, which help the model reason through intermediate steps. Conversely, earlier generation models, despite understanding the task initially and performing well in early stages, tend to degrade in performance as the number of steps increases, indicating limitations in long-term state memory or reasoning over extended sequences. This study highlights both the progress made in LLMs' reasoning capabilities and the remaining challenges in sustaining accurate state tracking over time, suggesting that integration with explicit reasoning strategies significantly boosts performance on such tasks. <div>
arXiv:2511.10457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in solving complex tasks, including those requiring a certain level of reasoning. In this paper, we focus on state tracking, a problem where models need to keep track of the state governing a number of entities. To isolate the state tracking component from other factors, we propose a benchmark based on three well-defined state tracking tasks and analyse the performance of LLMs in different scenarios. The results indicate that the recent generation of LLMs (specifically, GPT-4 and Llama3) are capable of tracking state, especially when integrated with mechanisms such as Chain of Thought. However, models from the former generation, while understanding the task and being able to solve it at the initial stages, often fail at this task after a certain number of steps.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocalBench: Benchmarking LLMs on County-Level Local Knowledge and Reasoning</title>
<link>https://arxiv.org/abs/2511.10459</link>
<guid>https://arxiv.org/abs/2511.10459</guid>
<content:encoded><![CDATA[
<div> local knowledge, benchmarks, LLM evaluation, geographic scale, local communities<br /><br />Summary:<br /><br />This paper addresses the gap in evaluating large language models (LLMs) on hyper-local knowledge, emphasizing the importance of systems that understand neighborhood-specific dynamics, cultural narratives, and local governance. The authors introduce LocalBench, the first benchmark designed to systematically assess LLMs on county-level local knowledge across the U.S., covering 14,782 validated question-answer pairs from 526 counties in 49 states. LocalBench integrates diverse data sources including Census statistics, local subreddit discussions, and regional news, and encompasses physical, cognitive, and relational aspects of locality. Thirteen state-of-the-art LLMs are evaluated on this benchmark in both closed-book and web-augmented modes. Results reveal significant weaknesses: the best models achieve only 56.8% accuracy on narrative questions, and less than 15.5% on numerical reasoning tasks. Notably, increasing model size or leveraging web search does not consistently improve performance; for instance, web search boosts Gemini by 13.6% but decreases GPT-series results by 11.4%. These findings highlight an urgent need for development of place-aware AI capable of equitable and fine-grained understanding of local communities across diverse geographic and cultural contexts. <div>
arXiv:2511.10459v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely evaluated on macro-scale geographic tasks, such as global factual recall, event summarization, and regional reasoning. Yet, their ability to handle hyper-local knowledge remains poorly understood. This gap is increasingly consequential as real-world applications, from civic platforms to community journalism, demand AI systems that can reason about neighborhood-specific dynamics, cultural narratives, and local governance. Existing benchmarks fall short in capturing this complexity, often relying on coarse-grained data or isolated references. We present LocalBench, the first benchmark designed to systematically evaluate LLMs on county-level local knowledge across the United States. Grounded in the Localness Conceptual Framework, LocalBench includes 14,782 validated question-answer pairs across 526 U.S. counties in 49 states, integrating diverse sources such as Census statistics, local subreddit discourse, and regional news. It spans physical, cognitive, and relational dimensions of locality. Using LocalBench, we evaluate 13 state-of-the-art LLMs under both closed-book and web-augmented settings. Our findings reveal critical limitations: even the best-performing models reach only 56.8% accuracy on narrative-style questions and perform below 15.5% on numerical reasoning. Moreover, larger model size and web augmentation do not guarantee better performance, for example, search improves Gemini's accuracy by +13.6%, but reduces GPT-series performance by -11.4%. These results underscore the urgent need for language models that can support equitable, place-aware AI systems: capable of engaging with the diverse, fine-grained realities of local communities across geographic and cultural contexts.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks</title>
<link>https://arxiv.org/abs/2511.10465</link>
<guid>https://arxiv.org/abs/2511.10465</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, knowledge integration, knowledge gap filling, batch-wise evaluation, token efficiency<br /><br />Summary:<br /><br />1) This paper addresses the limitations of current prompt optimization methods, which mainly rely on elicitation-based strategies that search for optimal prompts but struggle with knowledge-intensive tasks due to fixed parametric boundaries. 2) The authors propose Knowledge-Provision-based Prompt Optimization (KPPO), a novel framework that shifts the focus from elicitation to systematic knowledge integration to better handle factual accuracy, domain-specific terminology, and reasoning patterns. 3) KPPO introduces three main innovations: a knowledge gap filling mechanism to identify and remediate missing knowledge, a batch-wise candidate evaluation that balances performance gains with distributional stability, and an adaptive knowledge pruning strategy that reduces token usage by up to 29% without sacrificing performance. 4) The framework was extensively evaluated across 15 knowledge-intensive benchmarks spanning multiple domains. 5) Results demonstrate KPPO's superiority over state-of-the-art elicitation-based methods, achieving an average performance improvement of approximately 6% while maintaining comparable or lower token consumption, highlighting both its effectiveness and efficiency. The code for KPPO is publicly available at the provided GitHub repository. <div>
arXiv:2511.10465v1 Announce Type: new 
Abstract: While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rubric-Based Benchmarking and Reinforcement Learning for Advancing LLM Instruction Following</title>
<link>https://arxiv.org/abs/2511.10507</link>
<guid>https://arxiv.org/abs/2511.10507</guid>
<content:encoded><![CDATA[
arXiv:2511.10507v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has led to impressive performance on a range of tasks, yet advanced instruction following (IF)-especially for complex, multi-turn, and system-prompted instructions-remains a significant challenge. Rigorous evaluation and effective training for such capabilities are hindered by the lack of high-quality, human-annotated benchmarks and reliable, interpretable reward signals. In this work, we introduce AdvancedIF (we will release this benchmark soon), a comprehensive benchmark featuring over 1,600 prompts and expert-curated rubrics that assess LLMs ability to follow complex, multi-turn, and system-level instructions. We further propose RIFL (Rubric-based Instruction-Following Learning), a novel post-training pipeline that leverages rubric generation, a finetuned rubric verifier, and reward shaping to enable effective reinforcement learning for instruction following. Extensive experiments demonstrate that RIFL substantially improves the instruction-following abilities of LLMs, achieving a 6.7% absolute gain on AdvancedIF and strong results on public benchmarks. Our ablation studies confirm the effectiveness of each component in RIFL. This work establishes rubrics as a powerful tool for both training and evaluating advanced IF in LLMs, paving the way for more capable and reliable AI systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025</title>
<link>https://arxiv.org/abs/2511.10515</link>
<guid>https://arxiv.org/abs/2511.10515</guid>
<content:encoded><![CDATA[
arXiv:2511.10515v1 Announce Type: new 
Abstract: Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Say It Differently: Linguistic Styles as Jailbreak Vectors</title>
<link>https://arxiv.org/abs/2511.10519</link>
<guid>https://arxiv.org/abs/2511.10519</guid>
<content:encoded><![CDATA[
arXiv:2511.10519v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly evaluated for robustness against paraphrased or semantically equivalent jailbreak prompts, yet little attention has been paid to linguistic variation as an attack surface. In this work, we systematically study how linguistic styles such as fear or curiosity can reframe harmful intent and elicit unsafe responses from aligned models. We construct style-augmented jailbreak benchmark by transforming prompts from 3 standard datasets into 11 distinct linguistic styles using handcrafted templates and LLM-based rewrites, while preserving semantic intent. Evaluating 16 open- and close-source instruction-tuned models, we find that stylistic reframing increases jailbreak success rates by up to +57 percentage points. Styles such as fearful, curious and compassionate are most effective and contextualized rewrites outperform templated variants.
  To mitigate this, we introduce a style neutralization preprocessing step using a secondary LLM to strip manipulative stylistic cues from user inputs, significantly reducing jailbreak success rates. Our findings reveal a systemic and scaling-resistant vulnerability overlooked in current safety pipelines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convomem Benchmark: Why Your First 150 Conversations Don't Need RAG</title>
<link>https://arxiv.org/abs/2511.10523</link>
<guid>https://arxiv.org/abs/2511.10523</guid>
<content:encoded><![CDATA[
arXiv:2511.10523v1 Announce Type: new 
Abstract: We introduce a comprehensive benchmark for conversational memory evaluation containing 75,336 question-answer pairs across diverse categories including user facts, assistant recall, abstention, preferences, temporal changes, and implicit connections. While existing benchmarks have advanced the field, our work addresses fundamental challenges in statistical power, data generation consistency, and evaluation flexibility that limit current memory evaluation frameworks. We examine the relationship between conversational memory and retrieval-augmented generation (RAG). While these systems share fundamental architectural patterns--temporal reasoning, implicit extraction, knowledge updates, and graph representations--memory systems have a unique characteristic: they start from zero and grow progressively with each conversation. This characteristic enables naive approaches that would be impractical for traditional RAG. Consistent with recent findings on long context effectiveness, we observe that simple full-context approaches achieve 70-82% accuracy even on our most challenging multi-message evidence cases, while sophisticated RAG-based memory systems like Mem0 achieve only 30-45% when operating on conversation histories under 150 interactions. Our analysis reveals practical transition points: long context excels for the first 30 conversations, remains viable with manageable trade-offs up to 150 conversations, and typically requires hybrid or RAG approaches beyond that point as costs and latencies become prohibitive. These patterns indicate that the small-corpus advantage of conversational memory--where exhaustive search and complete reranking are feasible--deserves dedicated research attention rather than simply applying general RAG solutions to conversation histories.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing the Formal and Institutional Boundaries of Contemporary Genre and Literary Fiction</title>
<link>https://arxiv.org/abs/2511.10546</link>
<guid>https://arxiv.org/abs/2511.10546</guid>
<content:encoded><![CDATA[
arXiv:2511.10546v1 Announce Type: new 
Abstract: Though the concept of genre has been a subject of discussion for millennia, the relatively recent emergence of genre fiction has added a new layer to this ongoing conversation. While more traditional perspectives on genre have emphasized form, contemporary scholarship has invoked both formal and institutional characteristics in its taxonomy of genre, genre fiction, and literary fiction. This project uses computational methods to explore the soundness of genre as a formal designation as opposed to an institutional one. Pulling from Andrew Piper's CONLIT dataset of Contemporary Literature, we assemble a corpus of literary and genre fiction, with the latter category containing romance, mystery, and science fiction novels. We use Welch's ANOVA to compare the distribution of narrative features according to author gender within each genre and within genre versus literary fiction. Then, we use logistic regression to model the effect that each feature has on literary classification and to measure how author gender moderates these effects. Finally, we analyze stylistic and semantic vector representations of our genre categories to understand the importance of form and content in literary classification. This project finds statistically significant formal markers of each literary category and illustrates how female authorship narrows and blurs the target for achieving literary status.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding</title>
<link>https://arxiv.org/abs/2511.10552</link>
<guid>https://arxiv.org/abs/2511.10552</guid>
<content:encoded><![CDATA[
arXiv:2511.10552v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DESS: DeBERTa Enhanced Syntactic-Semantic Aspect Sentiment Triplet Extraction</title>
<link>https://arxiv.org/abs/2511.10577</link>
<guid>https://arxiv.org/abs/2511.10577</guid>
<content:encoded><![CDATA[
arXiv:2511.10577v1 Announce Type: new 
Abstract: Fine-grained sentiment analysis faces ongoing challenges in Aspect Sentiment Triple Extraction (ASTE), particularly in accurately capturing the relationships between aspects, opinions, and sentiment polarities. While researchers have made progress using BERT and Graph Neural Networks, the full potential of advanced language models in understanding complex language patterns remains unexplored. We introduce DESS, a new approach that builds upon previous work by integrating DeBERTa's enhanced attention mechanism to better understand context and relationships in text. Our framework maintains a dual-channel structure, where DeBERTa works alongside an LSTM channel to process both meaning and grammatical patterns in text. We have carefully refined how these components work together, paying special attention to how different types of language information interact. When we tested DESS on standard datasets, it showed meaningful improvements over current methods, with F1-score increases of 4.85, 8.36, and 2.42 in identifying aspect opinion pairs and determining sentiment accurately. Looking deeper into the results, we found that DeBERTa's sophisticated attention system helps DESS handle complicated sentence structures better, especially when important words are far apart. Our findings suggest that upgrading to more advanced language models when thoughtfully integrated, can lead to real improvements in how well we can analyze sentiments in text. The implementation of our approach is publicly available at: https://github.com/VishalRepos/DESS.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Prompting Strategies with MedGemma for Medical Order Extraction</title>
<link>https://arxiv.org/abs/2511.10583</link>
<guid>https://arxiv.org/abs/2511.10583</guid>
<content:encoded><![CDATA[
arXiv:2511.10583v1 Announce Type: new 
Abstract: The accurate extraction of medical orders from doctor-patient conversations is a critical task for reducing clinical documentation burdens and ensuring patient safety. This paper details our team submission to the MEDIQA-OE-2025 Shared Task. We investigate the performance of MedGemma, a new domain-specific open-source language model, for structured order extraction. We systematically evaluate three distinct prompting paradigms: a straightforward one-Shot approach, a reasoning-focused ReAct framework, and a multi-step agentic workflow. Our experiments reveal that while more complex frameworks like ReAct and agentic flows are powerful, the simpler one-shot prompting method achieved the highest performance on the official validation set. We posit that on manually annotated transcripts, complex reasoning chains can lead to "overthinking" and introduce noise, making a direct approach more robust and efficient. Our work provides valuable insights into selecting appropriate prompting strategies for clinical information extraction in varied data conditions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mined Prompting and Metadata-Guided Generation for Wound Care Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.10591</link>
<guid>https://arxiv.org/abs/2511.10591</guid>
<content:encoded><![CDATA[
arXiv:2511.10591v1 Announce Type: new 
Abstract: The rapid expansion of asynchronous remote care has intensified provider workload, creating demand for AI systems that can assist clinicians in managing patient queries more efficiently. The MEDIQA-WV 2025 shared task addresses this challenge by focusing on generating free-text responses to wound care queries paired with images. In this work, we present two complementary approaches developed for the English track. The first leverages a mined prompting strategy, where training data is embedded and the top-k most similar examples are retrieved to serve as few-shot demonstrations during generation. The second approach builds on a metadata ablation study, which identified four metadata attributes that consistently enhance response quality. We train classifiers to predict these attributes for test cases and incorporate them into the generation pipeline, dynamically adjusting outputs based on prediction confidence. Experimental results demonstrate that mined prompting improves response relevance, while metadata-guided generation further refines clinical precision. Together, these methods highlight promising directions for developing AI-driven tools that can provide reliable and efficient wound care support.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Limits: Entropy Estimation Modeling for Compression and Generalization</title>
<link>https://arxiv.org/abs/2511.10618</link>
<guid>https://arxiv.org/abs/2511.10618</guid>
<content:encoded><![CDATA[
arXiv:2511.10618v1 Announce Type: new 
Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Socratic Self-Refine for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2511.10621</link>
<guid>https://arxiv.org/abs/2511.10621</guid>
<content:encoded><![CDATA[
arXiv:2511.10621v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instella: Fully Open Language Models with Stellar Performance</title>
<link>https://arxiv.org/abs/2511.10628</link>
<guid>https://arxiv.org/abs/2511.10628</guid>
<content:encoded><![CDATA[
arXiv:2511.10628v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box On-Policy Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2511.10643</link>
<guid>https://arxiv.org/abs/2511.10643</guid>
<content:encoded><![CDATA[
arXiv:2511.10643v1 Announce Type: new 
Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference</title>
<link>https://arxiv.org/abs/2511.10645</link>
<guid>https://arxiv.org/abs/2511.10645</guid>
<content:encoded><![CDATA[
arXiv:2511.10645v1 Announce Type: new 
Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding</title>
<link>https://arxiv.org/abs/2511.09559</link>
<guid>https://arxiv.org/abs/2511.09559</guid>
<content:encoded><![CDATA[
arXiv:2511.09559v1 Announce Type: cross 
Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning</title>
<link>https://arxiv.org/abs/2511.09893</link>
<guid>https://arxiv.org/abs/2511.09893</guid>
<content:encoded><![CDATA[
arXiv:2511.09893v1 Announce Type: cross 
Abstract: Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers</title>
<link>https://arxiv.org/abs/2511.09926</link>
<guid>https://arxiv.org/abs/2511.09926</guid>
<content:encoded><![CDATA[
arXiv:2511.09926v1 Announce Type: cross 
Abstract: Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing the Medical Context-Awareness Ability of LLMs via Multifaceted Self-Refinement Learning</title>
<link>https://arxiv.org/abs/2511.10067</link>
<guid>https://arxiv.org/abs/2511.10067</guid>
<content:encoded><![CDATA[
arXiv:2511.10067v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown great promise in the medical domain, achieving strong performance on several benchmarks. However, they continue to underperform in real-world medical scenarios, which often demand stronger context-awareness, i.e., the ability to recognize missing or critical details (e.g., user identity, medical history, risk factors) and provide safe, helpful, and contextually appropriate responses. To address this issue, we propose Multifaceted Self-Refinement (MuSeR), a data-driven approach that enhances LLMs' context-awareness along three key facets (decision-making, communication, and safety) through self-evaluation and refinement. Specifically, we first design a attribute-conditioned query generator that simulates diverse real-world user contexts by varying attributes such as role, geographic region, intent, and degree of information ambiguity. An LLM then responds to these queries, self-evaluates its answers along three key facets, and refines its responses to better align with the requirements of each facet. Finally, the queries and refined responses are used for supervised fine-tuning to reinforce the model's context-awareness ability. Evaluation results on the latest HealthBench dataset demonstrate that our method significantly improves LLM performance across multiple aspects, with particularly notable gains in the context-awareness axis. Furthermore, by incorporating knowledge distillation with the proposed method, the performance of a smaller backbone LLM (e.g., Qwen3-32B) surpasses its teacher model, achieving a new SOTA across all open-source LLMs on HealthBench (63.8%) and its hard subset (43.1%). Code and dataset will be released at https://muser-llm.github.io.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProgRAG: Hallucination-Resistant Progressive Retrieval and Reasoning over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2511.10240</link>
<guid>https://arxiv.org/abs/2511.10240</guid>
<content:encoded><![CDATA[
arXiv:2511.10240v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate strong reasoning capabilities but struggle with hallucinations and limited transparency. Recently, KG-enhanced LLMs that integrate knowledge graphs (KGs) have been shown to improve reasoning performance, particularly for complex, knowledge-intensive tasks. However, these methods still face significant challenges, including inaccurate retrieval and reasoning failures, often exacerbated by long input contexts that obscure relevant information or by context constructions that struggle to capture the richer logical directions required by different question types. Furthermore, many of these approaches rely on LLMs to directly retrieve evidence from KGs, and to self-assess the sufficiency of this evidence, which often results in premature or incorrect reasoning. To address the retrieval and reasoning failures, we propose ProgRAG, a multi-hop knowledge graph question answering (KGQA) framework that decomposes complex questions into sub-questions, and progressively extends partial reasoning paths by answering each sub-question. At each step, external retrievers gather candidate evidence, which is then refined through uncertainty-aware pruning by the LLM. Finally, the context for LLM reasoning is optimized by organizing and rearranging the partial reasoning paths obtained from the sub-question answers. Experiments on three well-known datasets demonstrate that ProgRAG outperforms existing baselines in multi-hop KGQA, offering improved reliability and reasoning quality.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactGuard: Event-Centric and Commonsense-Guided Fake News Detection</title>
<link>https://arxiv.org/abs/2511.10281</link>
<guid>https://arxiv.org/abs/2511.10281</guid>
<content:encoded><![CDATA[
arXiv:2511.10281v1 Announce Type: cross 
Abstract: Fake news detection methods based on writing style have achieved remarkable progress. However, as adversaries increasingly imitate the style of authentic news, the effectiveness of such approaches is gradually diminishing. Recent research has explored incorporating large language models (LLMs) to enhance fake news detection. Yet, despite their transformative potential, LLMs remain an untapped goldmine for fake news detection, with their real-world adoption hampered by shallow functionality exploration, ambiguous usability, and prohibitive inference costs. In this paper, we propose a novel fake news detection framework, dubbed FactGuard, that leverages LLMs to extract event-centric content, thereby reducing the impact of writing style on detection performance. Furthermore, our approach introduces a dynamic usability mechanism that identifies contradictions and ambiguous cases in factual reasoning, adaptively incorporating LLM advice to improve decision reliability. To ensure efficiency and practical deployment, we employ knowledge distillation to derive FactGuard-D, enabling the framework to operate effectively in cold-start and resource-constrained scenarios. Comprehensive experiments on two benchmark datasets demonstrate that our approach consistently outperforms existing methods in both robustness and accuracy, effectively addressing the challenges of style sensitivity and LLM usability in fake news detection.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10287</link>
<guid>https://arxiv.org/abs/2511.10287</guid>
<content:encoded><![CDATA[
arXiv:2511.10287v1 Announce Type: cross 
Abstract: Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Music Flamingo: Scaling Music Understanding in Audio Language Models</title>
<link>https://arxiv.org/abs/2511.10289</link>
<guid>https://arxiv.org/abs/2511.10289</guid>
<content:encoded><![CDATA[
arXiv:2511.10289v1 Announce Type: cross 
Abstract: We introduce Music Flamingo, a novel large audio-language model designed to advance music (including song) understanding in foundational audio models. While audio-language research has progressed rapidly, music remains challenging due to its dynamic, layered, and information-dense nature. Progress has been further limited by the difficulty of scaling open audio understanding models, primarily because of the scarcity of high-quality music data and annotations. As a result, prior models are restricted to producing short, high-level captions, answering only surface-level questions, and showing limited generalization across diverse musical cultures. To address these challenges, we curate MF-Skills, a large-scale dataset labeled through a multi-stage pipeline that yields rich captions and question-answer pairs covering harmony, structure, timbre, lyrics, and cultural context. We fine-tune an enhanced Audio Flamingo 3 backbone on MF-Skills and further strengthen multiple skills relevant to music understanding. To improve the model's reasoning abilities, we introduce a post-training recipe: we first cold-start with MF-Think, a novel chain-of-thought dataset grounded in music theory, followed by GRPO-based reinforcement learning with custom rewards. Music Flamingo achieves state-of-the-art results across 10+ benchmarks for music understanding and reasoning, establishing itself as a generalist and musically intelligent audio-language model. Beyond strong empirical results, Music Flamingo sets a new standard for advanced music understanding by demonstrating how models can move from surface-level recognition toward layered, human-like perception of songs. We believe this work provides both a benchmark and a foundation for the community to build the next generation of models that engage with music as meaningfully as humans do.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Misinformation Propagation in Social Networks using Large Language Models</title>
<link>https://arxiv.org/abs/2511.10384</link>
<guid>https://arxiv.org/abs/2511.10384</guid>
<content:encoded><![CDATA[
arXiv:2511.10384v1 Announce Type: cross 
Abstract: Misinformation on social media thrives on surprise, emotion, and identity-driven reasoning, often amplified through human cognitive biases. To investigate these mechanisms, we model large language model (LLM) personas as synthetic agents that mimic user-level biases, ideological alignments, and trust heuristics. Within this setup, we introduce an auditor--node framework to simulate and analyze how misinformation evolves as it circulates through networks of such agents. News articles are propagated across networks of persona-conditioned LLM nodes, each rewriting received content. A question--answering-based auditor then measures factual fidelity at every step, offering interpretable, claim-level tracking of misinformation drift. We formalize a misinformation index and a misinformation propagation rate to quantify factual degradation across homogeneous and heterogeneous branches of up to 30 sequential rewrites. Experiments with 21 personas across 10 domains reveal that identity- and ideology-based personas act as misinformation accelerators, especially in politics, marketing, and technology. By contrast, expert-driven personas preserve factual stability. Controlled-random branch simulations further show that once early distortions emerge, heterogeneous persona interactions rapidly escalate misinformation to propaganda-level distortion. Our taxonomy of misinformation severity -- spanning factual errors, lies, and propaganda -- connects observed drift to established theories in misinformation studies. These findings demonstrate the dual role of LLMs as both proxies for human-like biases and as auditors capable of tracing information fidelity. The proposed framework provides an interpretable, empirically grounded approach for studying, simulating, and mitigating misinformation diffusion in digital ecosystems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentEvolver: Towards Efficient Self-Evolving Agent System</title>
<link>https://arxiv.org/abs/2511.10395</link>
<guid>https://arxiv.org/abs/2511.10395</guid>
<content:encoded><![CDATA[
arXiv:2511.10395v1 Announce Type: cross 
Abstract: Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Reliability of Multi-agent System: A Perspective from Byzantine Fault Tolerance</title>
<link>https://arxiv.org/abs/2511.10400</link>
<guid>https://arxiv.org/abs/2511.10400</guid>
<content:encoded><![CDATA[
arXiv:2511.10400v1 Announce Type: cross 
Abstract: Ensuring the reliability of agent architectures and effectively identifying problematic agents when failures occur are crucial challenges in multi-agent systems (MAS). Advances in large language models (LLMs) have established LLM-based agents as a major branch of MAS, enabling major breakthroughs in complex problem solving and world modeling. However, the reliability implications of this shift remain largely unexplored. i.e., whether substituting traditional agents with LLM-based agents can effectively enhance the reliability of MAS. In this work, we investigate and quantify the reliability of LLM-based agents from the perspective of Byzantine fault tolerance. We observe that LLM-based agents demonstrate stronger skepticism when processing erroneous message flows, a characteristic that enables them to outperform traditional agents across different topological structures. Motivated by the results of the pilot experiment, we design CP-WBFT, a confidence probe-based weighted Byzantine Fault Tolerant consensus mechanism to enhance the stability of MAS with different topologies. It capitalizes on the intrinsic reflective and discriminative capabilities of LLMs by employing a probe-based, weighted information flow transmission method to improve the reliability of LLM-based agents. Extensive experiments demonstrate that CP-WBFT achieves superior performance across diverse network topologies under extreme Byzantine conditions (85.7\% fault rate). Notably, our approach surpasses traditional methods by attaining remarkable accuracy on various topologies and maintaining strong reliability in both mathematical reasoning and safety assessment tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Layer Norm on Memorization and Generalization in Transformers</title>
<link>https://arxiv.org/abs/2511.10566</link>
<guid>https://arxiv.org/abs/2511.10566</guid>
<content:encoded><![CDATA[
arXiv:2511.10566v1 Announce Type: cross 
Abstract: Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10573</link>
<guid>https://arxiv.org/abs/2511.10573</guid>
<content:encoded><![CDATA[
arXiv:2511.10573v1 Announce Type: cross 
Abstract: Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals</title>
<link>https://arxiv.org/abs/2511.10615</link>
<guid>https://arxiv.org/abs/2511.10615</guid>
<content:encoded><![CDATA[
arXiv:2511.10615v1 Announce Type: cross 
Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Correction in Radiology Reports: A Knowledge Distillation-Based Multi-Stage Framework</title>
<link>https://arxiv.org/abs/2406.15045</link>
<guid>https://arxiv.org/abs/2406.15045</guid>
<content:encoded><![CDATA[
arXiv:2406.15045v3 Announce Type: replace 
Abstract: The increasing complexity and workload of clinical radiology leads to inevitable oversights and mistakes in their use as diagnostic tools, causing delayed treatments and sometimes life-threatening harm to patients. While large language models (LLMs) have shown remarkable progress in many tasks, their utilities in detecting and correcting errors in radiology reporting are limited. This paper proposes a novel dual-knowledge infusion framework that enhances LLMs' capability for radiology report proofreading through systematic integration of medical expertise. Specifically, the knowledge infusion combines medical knowledge graph distillation (MKGD) with external knowledge retrieval (EXKR), enabling an effective automated approach in tackling mistakes in radiology reporting. By decomposing the complex proofreading task into three specialized stages of detection, localization, and correction, our method mirrors the systematic review process employed by expert radiologists, ensuring both precision and clinical interpretability. To perform a robust, clinically relevant evaluation, a comprehensive benchmark is also proposed using real-world radiology reports with real-world error patterns, including speech recognition confusions, terminology ambiguities, and template-related inconsistencies. Extensive evaluations across multiple LLM architectures demonstrate substantial improvements of our approach: up to 31.56% increase in error detection accuracy and 37.4% reduction in processing time. Human evaluation by radiologists confirms superior clinical relevance and factual consistency compared to existing approaches.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiating between human-written and AI-generated texts using linguistic features automatically extracted from an online computational tool</title>
<link>https://arxiv.org/abs/2407.03646</link>
<guid>https://arxiv.org/abs/2407.03646</guid>
<content:encoded><![CDATA[
arXiv:2407.03646v3 Announce Type: replace 
Abstract: While extensive research has focused on ChatGPT in recent years, very few studies have systematically quantified and compared linguistic features between human-written and Artificial Intelligence (AI)-generated language. This study aims to investigate how various linguistic components are represented in both types of texts, assessing the ability of AI to emulate human writing. Using human-authored essays as a benchmark, we prompted ChatGPT to generate essays of equivalent length. These texts were analyzed using Open Brain AI, an online computational tool, to extract measures of phonological, morphological, syntactic, and lexical constituents. Despite AI-generated texts appearing to mimic human speech, the results revealed significant differences across multiple linguistic features such as consonants, word stress, nouns, verbs, pronouns, direct objects, prepositional modifiers, and use of difficult words among others. These findings underscore the importance of integrating automated tools for efficient language assessment, reducing time and effort in data analysis. Moreover, they emphasize the necessity for enhanced training methodologies to improve the capacity of AI for producing more human-like text.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Turn Interactions for Text-to-SQL with Large Language Models</title>
<link>https://arxiv.org/abs/2408.11062</link>
<guid>https://arxiv.org/abs/2408.11062</guid>
<content:encoded><![CDATA[
arXiv:2408.11062v2 Announce Type: replace 
Abstract: This study explores text-to-SQL parsing by leveraging the powerful reasoning capabilities of large language models (LLMs). Despite recent advancements, existing LLM-based methods are still inefficient and struggle to handle cases with wide tables effectively. Furthermore, current interaction-based approaches either lack a step-by-step, interpretable SQL generation process or fail to provide a universally applicable interaction design. To address these challenges, we introduce Interactive-T2S, a framework that generates SQL queries through direct interactions with databases. This framework includes four general tools that facilitate proactive and efficient information retrieval by the LLM. Additionally, we have developed detailed exemplars to demonstrate the step-wise reasoning processes within our framework. Our approach achieves advanced performance on the Spider and BIRD datasets as well as their variants. Notably, we obtain state-of-the-art results on the BIRD leaderboard under the setting without oracle knowledge, demonstrating the effectiveness of our method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lessons in co-creation: the inconvenient truths of inclusive sign language technology development</title>
<link>https://arxiv.org/abs/2408.13171</link>
<guid>https://arxiv.org/abs/2408.13171</guid>
<content:encoded><![CDATA[
arXiv:2408.13171v2 Announce Type: replace 
Abstract: In the era of AI-driven language technologies, the participation of deaf communities in sign language technology development, often framed as co-creation, is increasingly emphasized. We present a reflexive case study of two Horizon 2020 projects on sign language machine translation (2021- 2023), conducted with a EUD, a European-level deaf-led NGO. Using participant observation, internal documentation, and collaborative analysis among the authors, we interrogate co-creation as both a practice and a discourse. We offer five lessons for making co-creation consequential: 1) recognise and resource deaf partners invisible labor, 2) manage expectations via accessible science communication, 3) crip co-creation by dismantling structural ableism, 4) diversify participatory methods to address co-creation fatigue and intersectionality, and 5) redistribute power through deaf leadership. We contribute an empirically grounded account of how co-creation plays out in multi-partner AI projects, and actionable implications for design that extend to participatory AI with minoritized language and disability communities.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedMobile: A mobile-sized language model with clinical capabilities</title>
<link>https://arxiv.org/abs/2410.09019</link>
<guid>https://arxiv.org/abs/2410.09019</guid>
<content:encoded><![CDATA[
arXiv:2410.09019v2 Announce Type: replace 
Abstract: Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. To address these significant limitations, we introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We perform a careful set of pipeline additions and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements. We evaluate the efficiency of our pipeline on the MultiMedQA and MedBullets. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for licensed physicians (~60%) and rivaling scores of models 100 times its size. Across the entirety of the MultiMedQA, MedMobile achieves SOTA performance for models with less than 5B parameters and represents the smallest model to pass the MedQA (USMLE). MedMobile holds promise to democratize access to language models in medicine, bolstering lower compute needs and fast inference speeds. With the ability to combat the biggest barriers to entry for language models in medicine, we hope that MedMobile is a critical step forward in developing clinically relevant language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Captions Speak Louder than Images: Generalizing Foundation Models for E-commerce from High-quality Multimodal Instruction Data</title>
<link>https://arxiv.org/abs/2410.17337</link>
<guid>https://arxiv.org/abs/2410.17337</guid>
<content:encoded><![CDATA[
arXiv:2410.17337v2 Announce Type: replace 
Abstract: Leveraging multimodal data to drive breakthroughs in e-commerce applications through Multimodal Foundation Models (MFMs) is gaining increasing attention from the research community. However, there are significant challenges that hinder the optimal use of multimodal e-commerce data by foundation models: (1) the scarcity of large-scale, high-quality multimodal benchmark datasets; and (2) the lack of effective multimodal information integration methods. To address these challenges, in this paper, we introduce MMECInstruct, the first-ever, large-scale, and high-quality multimodal instruction dataset for e-commerce. We also develop CASLIE, a simple, lightweight, yet effective framework for integrating multimodal information for e-commerce. Leveraging MMECInstruct, we fine-tune a series of e-commerce MFMs within CASLIE, denoted as CASLIE models. Our comprehensive evaluation demonstrates that CASLIE models substantially outperform 5 categories of advanced baseline models in the in-domain evaluation. Moreover, CASLIE models show strong generalizability to out-of-domain settings. MMECInstruct and CASLIE models are publicly accessible through https://ninglab.github.io/CASLIE/.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing the Scope of Language Models</title>
<link>https://arxiv.org/abs/2410.21597</link>
<guid>https://arxiv.org/abs/2410.21597</guid>
<content:encoded><![CDATA[
arXiv:2410.21597v3 Announce Type: replace 
Abstract: Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic, Orthographic, and Phonological Biases in Humans' Wordle Gameplay</title>
<link>https://arxiv.org/abs/2411.18634</link>
<guid>https://arxiv.org/abs/2411.18634</guid>
<content:encoded><![CDATA[
arXiv:2411.18634v2 Announce Type: replace 
Abstract: We show that human players' gameplay in the game of Wordle is influenced by the semantics, orthography, and phonology of the player's previous guesses. We compare actual human players' guesses with near-optimal guesses using NLP techniques. We study human language use in the constrained environment of Wordle, which is situated between natural language use and the artificial word association task
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Suicidal Ideation Detection from Social Media Using a CNN-BiLSTM Hybrid Model</title>
<link>https://arxiv.org/abs/2501.11094</link>
<guid>https://arxiv.org/abs/2501.11094</guid>
<content:encoded><![CDATA[
arXiv:2501.11094v2 Announce Type: replace 
Abstract: Suicidal ideation detection is crucial for preventing suicides, a leading cause of death worldwide. Many individuals express suicidal thoughts on social media, offering a vital opportunity for early detection through advanced machine learning techniques. The identification of suicidal ideation in social media text is improved by utilising a hybrid framework that integrates Convolutional Neural Networks (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), enhanced with an attention mechanism. To enhance the interpretability of the model's predictions, Explainable AI (XAI) methods are applied, with a particular focus on SHapley Additive exPlanations (SHAP), are incorporated. At first, the model managed to reach an accuracy of 92.81%. By applying fine-tuning and early stopping techniques, the accuracy improved to 94.29%. The SHAP analysis revealed key features influencing the model's predictions, such as terms related to mental health struggles. This level of transparency boosts the model's credibility while helping mental health professionals understand and trust the predictions. This work highlights the potential for improving the accuracy and interpretability of detecting suicidal tendencies, making a valuable contribution to the progress of mental health monitoring systems. It emphasizes the significance of blending powerful machine learning methods with explainability to develop reliable and impactful mental health solutions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siren: A Learning-Based Multi-Turn Attack Framework for Simulating Real-World Human Jailbreak Behaviors</title>
<link>https://arxiv.org/abs/2501.14250</link>
<guid>https://arxiv.org/abs/2501.14250</guid>
<content:encoded><![CDATA[
arXiv:2501.14250v2 Announce Type: replace 
Abstract: Large language models (LLMs) are widely used in real-world applications, raising concerns about their safety and trustworthiness. While red-teaming with jailbreak prompts exposes the vulnerabilities of LLMs, current efforts focus primarily on single-turn attacks, overlooking the multi-turn strategies used by real-world adversaries. Existing multi-turn methods rely on static patterns or predefined logical chains, failing to account for the dynamic strategies during attacks. We propose Siren, a learning-based multi-turn attack framework designed to simulate real-world human jailbreak behaviors. Siren consists of three stages: (1) MiniMax-driven training set construction utilizing Turn-Level LLM feedback, (2) post-training attackers with supervised fine-tuning (SFT) and direct preference optimization (DPO), and (3) interactions between the attacking and target LLMs. Experiments demonstrate that Siren achieves an attack success rate (ASR) of 90% with LLaMA-3-8B as the attacker against Gemini-1.5-Pro as the target model, and 70% with Mistral-7B against GPT-4o, significantly outperforming single-turn baselines. Moreover, Siren with a 7B-scale model achieves performance comparable to a multi-turn baseline that leverages GPT-4o as the attacker, while requiring fewer turns and employing decomposition strategies that are better semantically aligned with attack goals. We hope Siren inspires the development of stronger defenses against advanced multi-turn jailbreak attacks under realistic scenarios. Code is available at https://github.com/YiyiyiZhao/siren. Warning: This paper contains potentially harmful text.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning</title>
<link>https://arxiv.org/abs/2502.02390</link>
<guid>https://arxiv.org/abs/2502.02390</guid>
<content:encoded><![CDATA[
arXiv:2502.02390v3 Announce Type: replace 
Abstract: Research on LLM technologies is rapidly emerging, with most of them employ a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. We validate CoAT's effectiveness across a variety of generative and reasoning tasks. Quantitative experiments show that CoAT achieves over 10% performance improvement on open-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% gain on our proprietary CRB dataset.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMTEB: Massive Multilingual Text Embedding Benchmark</title>
<link>https://arxiv.org/abs/2502.13595</link>
<guid>https://arxiv.org/abs/2502.13595</guid>
<content:encoded><![CDATA[
arXiv:2502.13595v4 Announce Type: replace 
Abstract: Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale, community-driven expansion of MTEB, covering over 500 quality-controlled evaluation tasks across 250+ languages. MMTEB includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models (LLMs) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models</title>
<link>https://arxiv.org/abs/2502.18573</link>
<guid>https://arxiv.org/abs/2502.18573</guid>
<content:encoded><![CDATA[
arXiv:2502.18573v3 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success in generative tasks, yet they often fall short in ensuring the factual accuracy of their outputs, thus limiting their reliability in real-world applications where correctness is critical. In this paper, we present FactReasoner, a novel neuro-symbolic based factuality assessment framework that employs probabilistic reasoning to evaluate the truthfulness of long-form generated responses. FactReasoner decomposes a response into atomic units, retrieves relevant contextual information from external knowledge sources, and models the logical relationships (e.g., entailment, contradiction) between these units and their contexts using probabilistic encodings. It then estimates the posterior probability that each atomic unit is supported by the retrieved evidence. Our experiments on both labeled and unlabeled benchmark datasets demonstrate that FactReasoner often outperforms state-of-the-art prompt-based methods in terms of factual precision and recall. Our open-source implementation is publicly available at: https://github.com/IBM/FactReasoner.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers</title>
<link>https://arxiv.org/abs/2504.03595</link>
<guid>https://arxiv.org/abs/2504.03595</guid>
<content:encoded><![CDATA[
arXiv:2504.03595v3 Announce Type: replace 
Abstract: A key element to support the increased amounts of renewable energy in the energy system is flexibility, i.e., the possibility of changing energy loads in time and amount. Many flexibility models have been designed; however, exact models fail to scale for long time horizons or many devices. Because of this, the FlexOffers model has been designed, to provide device-independent approximations of flexibility with good accuracy, and much better scaling for long time horizons and many devices. An important aspect of the real-life implementation of energy flexibility is enabling flexible data exchange with many smart energy appliances and market systems, e.g., in smart buildings. For this, ontologies standardizing data formats are required. However, the current industry standard ontology for integrating smart devices for energy purposes, SAREF for Energy Flexibility (SAREF4ENER), only has limited support for flexibility and thus cannot support important use cases. In this paper, we propose an extension of SAREF4ENER that integrates full support for the complete FlexOffer model, including advanced use cases, while maintaining backward compatibility. This novel ontology module can accurately describe flexibility for advanced devices such as electric vehicles, batteries, and heat pumps. It can also capture the inherent uncertainty associated with many flexible load types.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Specific Knowledge: Do Models Know Better in X than in English?</title>
<link>https://arxiv.org/abs/2505.14990</link>
<guid>https://arxiv.org/abs/2505.14990</guid>
<content:encoded><![CDATA[
arXiv:2505.14990v2 Announce Type: replace 
Abstract: Often, multilingual language models are trained with the objective to map semantically similar content (in different languages) in the same latent space. In this paper, we show a nuance in this training objective, and find that by changing the language of the input query, we can improve the question answering ability of language models. Our contributions are two-fold. First, we introduce the term Language Specific Knowledge (LSK) to denote queries that are best answered in an "expert language" for a given LLM, thereby enhancing its question-answering ability. We introduce the problem of language selection -- for some queries, language models can perform better when queried in languages other than English, sometimes even better in low-resource languages -- and the goal is to select the optimal language for the query. Second, we introduce simple to strong baselines to test this problem. Additionally, as a first-pass solution to this novel problem, we design LSKExtractor to benchmark the language-specific knowledge present in a language model and then exploit it during inference. To test our framework, we employ three datasets that contain knowledge about both cultural and social behavioral norms. Overall, LSKExtractor achieves up to 10% relative improvement across datasets, and is competitive against strong baselines, while being feasible in real-world settings. Broadly, our research contributes to the open-source development (https://github.com/agarwalishika/LSKExtractor/tree/main) of language models that are inclusive and more aligned with the cultural and linguistic contexts in which they are deployed.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search</title>
<link>https://arxiv.org/abs/2505.16838</link>
<guid>https://arxiv.org/abs/2505.16838</guid>
<content:encoded><![CDATA[
arXiv:2505.16838v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by enabling step-by-step problem-solving, yet its extension to Long-CoT introduces substantial computational overhead due to increased token length. Existing compression approaches -- instance-level and token-level -- either sacrifice essential local reasoning signals like reflection or yield incoherent outputs. To address these limitations, we propose R1-Compress, a two-stage chunk-level compression framework that preserves both local information and coherence. Our method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk compression, and employs an inter-chunk search mechanism to select the short and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500, AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces token usage while maintaining comparable reasoning accuracy. On MATH500, R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to the Long-CoT baseline, while reducing token usage by about 20%. Source code will be available at https://github.com/w-yibo/R1-Compress
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Textual Gradients via Sampling-Based Momentum</title>
<link>https://arxiv.org/abs/2506.00400</link>
<guid>https://arxiv.org/abs/2506.00400</guid>
<content:encoded><![CDATA[
arXiv:2506.00400v2 Announce Type: replace 
Abstract: LLM-based prompt optimization, that uses LLM-provided "textual gradients" (feedback) to refine prompts, has emerged an effective method for automatic prompt engineering. However, its scalability and stability are unclear when using more data in training. We systematically investigate the potential and challenges of scaling training data in textual gradient descent. We show that naively scaling training examples is infeasible due to both explicit context-length limits and an implicit context wall, where long-context degradation yields diminishing returns. Inspired by prior wisdom in stochastic gradient descent, we propose Textual Stochastic Gradient Descent with Momentum (TSGD-M), which reweights updates through momentum sampling, using bootstrapped minibatch validation accuracy as importance weights over historical prompts. We introduce Gumbel-Top-$k$ sampling for prompt generation, balancing exploration--exploitation and improving sampling efficiency while maintaining a low-variance running mean estimator. TSGD-M integrates seamlessly into existing prompt optimization frameworks, including TextGrad, DSPy-COPRO, and AdalFlow, and achieves consistent gains across 5 benchmarks. Our findings highlight the importance of incorporating probabilistic exploration into textual-gradient-based optimization, paving the way for more stable and scalable prompt optimization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
arXiv:2506.01939v2 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
arXiv:2506.19794v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling</title>
<link>https://arxiv.org/abs/2506.21572</link>
<guid>https://arxiv.org/abs/2506.21572</guid>
<content:encoded><![CDATA[
arXiv:2506.21572v2 Announce Type: replace 
Abstract: Evaluating multimodal large language models (MLLMs) is fundamentally challenged by the absence of structured, interpretable, and theoretically grounded benchmarks; current heuristically-grouped tasks have vague cognitive targets, overlapping abilities, redundant indicators, and weak diagnostic power. We therefore propose a structural-equation-modeling-aligned framework that quantifies internal validity, dimensional separability, and component contributions, and introduce a Piaget-inspired capability hierarchy that stratifies MLLM abilities into Perception, Memory, and Reasoning. Reorganizing existing tasks under this theory, we build the GOLD benchmark, whose experiments show superior interpretability, lower indicator redundancy, and clearer cognitive consistency than prior benchmarks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</title>
<link>https://arxiv.org/abs/2508.02573</link>
<guid>https://arxiv.org/abs/2508.02573</guid>
<content:encoded><![CDATA[
arXiv:2508.02573v2 Announce Type: replace 
Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test Set Quality in Multilingual LLM Evaluation</title>
<link>https://arxiv.org/abs/2508.02635</link>
<guid>https://arxiv.org/abs/2508.02635</guid>
<content:encoded><![CDATA[
arXiv:2508.02635v2 Announce Type: replace 
Abstract: Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Perplexity: Let the Reader Select Retrieval Summaries via Spectrum Projection Score</title>
<link>https://arxiv.org/abs/2508.05909</link>
<guid>https://arxiv.org/abs/2508.05909</guid>
<content:encoded><![CDATA[
arXiv:2508.05909v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We move beyond perplexity and introduce Spectrum Projection Score (SPS), a lightweight and supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference-time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open-sourced LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-commerce Models</title>
<link>https://arxiv.org/abs/2508.15721</link>
<guid>https://arxiv.org/abs/2508.15721</guid>
<content:encoded><![CDATA[
arXiv:2508.15721v2 Announce Type: replace 
Abstract: E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://github.com/ninglab/EcomMMMU.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</title>
<link>https://arxiv.org/abs/2509.19319</link>
<guid>https://arxiv.org/abs/2509.19319</guid>
<content:encoded><![CDATA[
arXiv:2509.19319v2 Announce Type: replace 
Abstract: The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Backdoor Attacks Against Speech Language Models</title>
<link>https://arxiv.org/abs/2510.01157</link>
<guid>https://arxiv.org/abs/2510.01157</guid>
<content:encoded><![CDATA[
arXiv:2510.01157v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming increasingly popular. One common approach to enable multimodality is to cascade domain-specific encoders with an LLM, making the resulting model inherit vulnerabilities from all of its components. In this work, we present the first systematic study of audio backdoor attacks against speech language models. We demonstrate its effectiveness across four speech encoders and three datasets, covering four tasks: automatic speech recognition (ASR), speech emotion recognition, and gender and age prediction. The attack consistently achieves high success rates, ranging from 90.76% to 99.41%. To better understand how backdoors propagate, we conduct a component-wise analysis to identify the most vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based defense that mitigates the threat of poisoned pretrained encoders.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.01252</link>
<guid>https://arxiv.org/abs/2510.01252</guid>
<content:encoded><![CDATA[
arXiv:2510.01252v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Correlates of Language Models Are Specific to Human Language</title>
<link>https://arxiv.org/abs/2510.03156</link>
<guid>https://arxiv.org/abs/2510.03156</guid>
<content:encoded><![CDATA[
arXiv:2510.03156v2 Announce Type: replace 
Abstract: Previous work has shown correlations between the hidden states of large language models and fMRI brain responses, on language tasks. These correlations have been taken as evidence of the representational similarity of these models and brain states. This study tests whether these previous results are robust to several possible concerns. Specifically this study shows: (i) that the previous results are still found after dimensionality reduction, and thus are not attributable to the curse of dimensionality; (ii) that previous results are confirmed when using new measures of similarity; (iii) that correlations between brain representations and those from models are specific to models trained on human language; and (iv) that the results are dependent on the presence of positional encoding in the models. These results confirm and strengthen the results of previous research and contribute to the debate on the biological plausibility and interpretability of state-of-the-art large language models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making</title>
<link>https://arxiv.org/abs/2510.03553</link>
<guid>https://arxiv.org/abs/2510.03553</guid>
<content:encoded><![CDATA[
arXiv:2510.03553v2 Announce Type: replace 
Abstract: Although large language models (LLMs) are increasingly implicated in interpersonal and societal decision-making, their ability to navigate explicit conflicts between legitimately different cultural value systems remains largely unexamined. Existing benchmarks predominantly target cultural knowledge (CulturalBench), value prediction (WorldValuesBench), or single-axis bias diagnostics (CDEval); none evaluate how LLMs adjudicate when multiple culturally grounded values directly clash. We address this gap with CCD-Bench, a benchmark that assesses LLM decision-making under cross-cultural value conflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains, each paired with ten anonymized response options corresponding to the ten GLOBE cultural clusters. These dilemmas are presented using a stratified Latin square to mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models disproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe (12.4 percent), while options for Eastern Europe and the Middle East and North Africa are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of rationales reference multiple GLOBE dimensions, this pluralism is superficial: models recombine Future Orientation and Performance Orientation, and rarely ground choices in Assertiveness or Gender Egalitarianism (both under 3 percent). Ordering effects are negligible (Cramer's V less than 0.10), and symmetrized KL divergence shows clustering by developer lineage rather than geography. These patterns suggest that current alignment pipelines promote a consensus-oriented worldview that underserves scenarios demanding power negotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts evaluation beyond isolated bias detection toward pluralistic decision making and highlights the need for alignment strategies that substantively engage diverse worldviews.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation</title>
<link>https://arxiv.org/abs/2510.12858</link>
<guid>https://arxiv.org/abs/2510.12858</guid>
<content:encoded><![CDATA[
arXiv:2510.12858v2 Announce Type: replace 
Abstract: The art and science of Quranic recitation (Tajweed), a discipline governed by meticulous phonetic, rhythmic, and theological principles, confronts substantial educational challenges in today's digital age. Although modern technology offers unparalleled opportunities for learning, existing automated systems for evaluating recitation have struggled to gain broad acceptance or demonstrate educational effectiveness. This literature review examines this crucial disparity, offering a thorough analysis of scholarly research, digital platforms, and commercial tools developed over the past twenty years. Our analysis uncovers a fundamental flaw in current approaches that adapt Automatic Speech Recognition (ASR) systems, which emphasize word identification over qualitative acoustic evaluation. These systems suffer from limitations such as reliance on biased datasets, demographic disparities, and an inability to deliver meaningful feedback for improvement. Challenging these data-centric methodologies, we advocate for a paradigm shift toward a knowledge-based computational framework. By leveraging the unchanging nature of the Quranic text and the well-defined rules of Tajweed, we propose that an effective evaluation system should be built upon rule-based acoustic modeling centered on canonical pronunciation principles and articulation points (Makhraj), rather than depending on statistical patterns derived from flawed or biased data. The review concludes that the future of automated Quranic recitation assessment lies in hybrid systems that combine linguistic expertise with advanced audio processing. Such an approach paves the way for developing reliable, fair, and pedagogically effective tools that can authentically assist learners across the globe.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation</title>
<link>https://arxiv.org/abs/2501.18638</link>
<guid>https://arxiv.org/abs/2501.18638</guid>
<content:encoded><![CDATA[
arXiv:2501.18638v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly prevalent, ensuring their robustness against adversarial misuse is crucial. This paper introduces the GAP (Graph of Attacks with Pruning) framework, an advanced approach for generating stealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP addresses limitations in existing tree-based LLM jailbreak methods by implementing an interconnected graph structure that enables knowledge sharing across attack paths. Our experimental evaluation demonstrates GAP's superiority over existing techniques, achieving a 20.8% increase in attack success rates while reducing query costs by 62.7%. GAP consistently outperforms state-of-the-art methods for attacking both open and closed LLMs, with attack success rates of >96%. Additionally, we present specialized variants like GAP-Auto for automated seed generation and GAP-VLM for multimodal attacks. GAP-generated prompts prove highly effective in improving content moderation systems, increasing true positive detection rates by 108.5% and accuracy by 183.6% when used for fine-tuning. Our implementation is available at https://github.com/dsbuddy/GAP-LLM-Safety.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unique Hard Attention: A Tale of Two Sides</title>
<link>https://arxiv.org/abs/2503.14615</link>
<guid>https://arxiv.org/abs/2503.14615</guid>
<content:encoded><![CDATA[
arXiv:2503.14615v3 Announce Type: replace-cross 
Abstract: Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Lure: A Universal Jailbreak Attack Framework using Unconstrained Synthetic Narratives</title>
<link>https://arxiv.org/abs/2505.17519</link>
<guid>https://arxiv.org/abs/2505.17519</guid>
<content:encoded><![CDATA[
arXiv:2505.17519v2 Announce Type: replace-cross 
Abstract: In the era of rapid generative AI development, interactions with large language models (LLMs) pose increasing risks of misuse. Prior research has primarily focused on attacks using template-based prompts and optimization-oriented methods, while overlooking the fact that LLMs possess strong unconstrained deceptive capabilities to attack other LLMs. This paper introduces a novel jailbreaking method inspired by the Chain-of-Thought mechanism. The attacker employs mission transfer to conceal harmful user intent within dialogue and generates a progressive chain of lure questions without relying on predefined templates, enabling successful jailbreaks. To further improve the attack's strength, we incorporate a helper LLM model that performs randomized narrative optimization over multi-turn interactions, enhancing the attack performance while preserving alignment with the original intent. We also propose a toxicity-based framework using third-party LLMs to evaluate harmful content and its alignment with malicious intent. Extensive experiments demonstrate that our method consistently achieves high attack success rates and elevated toxicity scores across diverse types of LLMs under black-box API settings. These findings reveal the intrinsic potential of LLMs to perform unrestricted attacks in the absence of robust alignment constraints. Our approach offers data-driven insights to inform the design of future alignment mechanisms. Finally, we propose two concrete defense strategies to support the development of safer generative models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v2 Announce Type: replace-cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</title>
<link>https://arxiv.org/abs/2508.05615</link>
<guid>https://arxiv.org/abs/2508.05615</guid>
<content:encoded><![CDATA[
arXiv:2508.05615v2 Announce Type: replace-cross 
Abstract: Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), transforming these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: using only 1,272 unlabeled data, GUI-RCPO achieves 3-6% accuracy improvements across various architectures on ScreenSpot benchmarks. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more data-efficient GUI agents.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection</title>
<link>https://arxiv.org/abs/2509.03113</link>
<guid>https://arxiv.org/abs/2509.03113</guid>
<content:encoded><![CDATA[
arXiv:2509.03113v3 Announce Type: replace-cross 
Abstract: Multimodal large language models achieve strong performance across diverse tasks but remain prone to hallucinations, where outputs are not grounded in visual inputs. This issue can be attributed to two main biases: text-visual bias, the overreliance on prompts and prior outputs, and co-occurrence bias, spurious correlations between frequently paired objects. We propose Gradient-based Influence-Aware Constrained Decoding (GACD), an inference-based method, that addresses both biases without auxiliary models, and is readily applicable to existing models without finetuning. The core of our approach is bias estimation, which uses first-order Taylor gradients to understand the contribution of individual tokens-visual features and text tokens-to the current output. Based on this analysis, GACD mitigates hallucinations through two components: (1) suppressing spurious visual features correlated with the output objects, and (2) rebalancing cross-modal contributions by strengthening visual features relative to text. Experiments across multiple benchmarks demonstrate that GACD effectively reduces hallucinations and improves the visual grounding of MLLM outputs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</title>
<link>https://arxiv.org/abs/2509.09651</link>
<guid>https://arxiv.org/abs/2509.09651</guid>
<content:encoded><![CDATA[
arXiv:2509.09651v2 Announce Type: replace-cross 
Abstract: We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.11816</link>
<guid>https://arxiv.org/abs/2509.11816</guid>
<content:encoded><![CDATA[
arXiv:2509.11816v2 Announce Type: replace-cross 
Abstract: Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.
  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.
  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.
  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title>
<link>https://arxiv.org/abs/2509.14289</link>
<guid>https://arxiv.org/abs/2509.14289</guid>
<content:encoded><![CDATA[
arXiv:2509.14289v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PsychCounsel-Bench: Evaluating the Psychology Intelligence of Large Language Models</title>
<link>https://arxiv.org/abs/2510.01611</link>
<guid>https://arxiv.org/abs/2510.01611</guid>
<content:encoded><![CDATA[
arXiv:2510.01611v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a wide range of industries, primarily due to their impressive generative abilities. Yet, their potential in applications requiring cognitive abilities, such as psychological counseling, remains largely untapped. This paper investigates the key question: \textit{Can LLMs be effectively applied to psychological counseling?} To determine whether an LLM can effectively take on the role of a psychological counselor, the first step is to assess whether it meets the qualifications required for such a role, namely the ability to pass the U.S. National Counselor Certification Exam (NCE). This is because, just as a human counselor must pass a certification exam to practice, an LLM must demonstrate sufficient psychological knowledge to meet the standards required for such a role. To address this, we introduce PsychCounsel-Bench, a benchmark grounded in U.S.national counselor examinations, a licensure test for professional counselors that requires about 70\% accuracy to pass. PsychCounsel-Bench comprises approximately 2,252 carefully curated single-choice questions, crafted to require deep understanding and broad enough to cover various sub-disciplines of psychology. This benchmark provides a comprehensive assessment of an LLM's ability to function as a counselor. Our evaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results suggest that only frontier LLMs are currently capable of meeting counseling exam standards, highlighting both the promise and the challenges of developing psychology-oriented LLMs. We release the proposed dataset for public use: https://github.com/cloversjtu/PsychCounsel-Bench
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features</title>
<link>https://arxiv.org/abs/2510.16781</link>
<guid>https://arxiv.org/abs/2510.16781</guid>
<content:encoded><![CDATA[
arXiv:2510.16781v2 Announce Type: replace-cross 
Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration</title>
<link>https://arxiv.org/abs/2510.26495</link>
<guid>https://arxiv.org/abs/2510.26495</guid>
<content:encoded><![CDATA[
arXiv:2510.26495v2 Announce Type: replace-cross 
Abstract: Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench .
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where did you get that? Towards Summarization Attribution for Analysts</title>
<link>https://arxiv.org/abs/2511.08589</link>
<guid>https://arxiv.org/abs/2511.08589</guid>
<content:encoded><![CDATA[
<div> Keywords: attribution, summarization, extractive summary, paraphrase, error analysis<br /><br />Summary:<br />1. The paper addresses the need for attribution in analytical reporting, emphasizing that information must be linked to its source to be credible. <br />2. It focuses on developing automatic methods for attribution by connecting each sentence in a summary to specific portions of the original source text, which can involve multiple documents. <br />3. The authors propose the use of hybrid summarization, where an extractive summary is automatically paraphrased. This approach aims to facilitate easier and more accurate attribution by blending direct extraction with paraphrasing. <br />4. Additionally, the paper introduces a custom topology to categorize and identify the proportions of different types of errors related to attribution, helping to better understand where summarization and attribution may fail. <br />5. Overall, the research contributes to improving the reliability of automatic summaries by ensuring that every piece of summarized information is traceable, assisting analysts in maintaining trustworthiness in their reports. <div>
arXiv:2511.08589v1 Announce Type: new 
Abstract: Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMTRouter: Personalized LLM Router over Multi-turn User Interactions</title>
<link>https://arxiv.org/abs/2511.08590</link>
<guid>https://arxiv.org/abs/2511.08590</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, personalization, graph learning, user preferences, few-shot learning<br /><br />Summary:<br /><br />1. GMTRouter is a novel approach to Large Language Model (LLM) routing that focuses on personalizing responses based on diverse user preferences, addressing the issue that identical queries may require different LLMs for different users.<br />2. It models multi-turn user-LLM interactions as a heterogeneous graph composed of four node types: user, LLM, query, and response, preserving complex relational structures in the data.<br />3. GMTRouter employs a customized message-passing mechanism within a lightweight inductive graph learning framework to effectively learn user preferences from scarce and noisy few-shot data.<br />4. Experimental results show that GMTRouter outperforms existing strong baseline methods, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets.<br />5. Importantly, GMTRouter can adapt to new users and evolving preferences without requiring extensive fine-tuning, relying only on few-shot data, and the implementation code is publicly available on GitHub. <div>
arXiv:2511.08590v1 Announce Type: new 
Abstract: Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions</title>
<link>https://arxiv.org/abs/2511.08592</link>
<guid>https://arxiv.org/abs/2511.08592</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, social media, conversations, authenticity, simulation<br />
Summary: 
- The study evaluates if Large Language Models (LLMs) can accurately simulate human group conversations on social media, using Reddit conversations.
- Two LLMs, Llama 3 70B and GPT-4o, were tested, with Llama 3 being the more challenging to distinguish between human and AI-generated content.
- Participants mistook LLM-generated conversations for human-created content 39% of the time, with only 56% accuracy in identifying Llama 3-generated content.
- The findings suggest that LLMs can convincingly mimic social media conversations, potentially opening up new opportunities for simulation applications.
- However, the study also warns about the dangers of misusing LLMs to generate inauthentic social media content, highlighting the need for responsible AI usage in societal contexts.<br /><br />Summary: <div>
arXiv:2511.08592v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer new avenues to simulate online communities and social media. Potential applications range from testing the design of content recommendation algorithms to estimating the effects of content policies and interventions. However, the validity of using LLMs to simulate conversations between various users remains largely untested. We evaluated whether LLMs can convincingly mimic human group conversations on social media. We collected authentic human conversations from Reddit and generated artificial conversations on the same topic with two LLMs: Llama 3 70B and GPT-4o. When presented side-by-side to study participants, LLM-generated conversations were mistaken for human-created content 39\% of the time. In particular, when evaluating conversations generated by Llama 3, participants correctly identified them as AI-generated only 56\% of the time, barely better than random chance. Our study demonstrates that LLMs can generate social media conversations sufficiently realistic to deceive humans when reading them, highlighting both a promising potential for social simulation and a warning message about the potential misuse of LLMs to generate new inauthentic social media content.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Graph Analysis of Legal Understanding and Violations in LLMs</title>
<link>https://arxiv.org/abs/2511.08593</link>
<guid>https://arxiv.org/abs/2511.08593</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, legal analysis, compliance monitoring, knowledge graph construction, safety mechanisms <br />
Summary: 
Large Language Models (LLMs) offer potential in interpreting legal frameworks like Title 18 Section 175 of the US Code, but also pose risks due to generating unsafe outputs. A methodology integrating knowledge graph construction and Retrieval-Augmented Generation (RAG) is proposed to evaluate LLMs' understanding of the law, legal intent assessment, and potential unsafe applications. Experiments find limitations in LLMs' reasoning and safety mechanisms. Improved safety protocols and legal reasoning frameworks are suggested for LLMs to ethically assist in sensitive legal areas. This research aims to ensure LLMs act as protectors of the law rather than inadvertent enablers of its violation. <br /><br />Summary: <div>
arXiv:2511.08593v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs' understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs' reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Preference Learning for Capabilities and Alignment</title>
<link>https://arxiv.org/abs/2511.08594</link>
<guid>https://arxiv.org/abs/2511.08594</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM diversity, alignment algorithms, KL divergence regularizer, Soft Preference Learning, societal perspectives

<br /><br />Summary:  
The paper addresses the reduction of output diversity in large language models (LLMs) caused by common alignment methods such as RLHF (Reinforcement Learning with Human Feedback) and DPO (Direct Preference Optimization). These methods lead to repetitiveness in text structure and word choice and cause LLMs to approach problems uniformly, thus limiting the range of societal perspectives represented. The authors pinpoint the KL divergence regularizer used in preference learning algorithms as the key cause, as it biases the model toward majority opinions, diminishing output diversity. To overcome this, they propose a novel method called Soft Preference Learning, which separates the entropy and cross-entropy components within the KL penalty. This decoupling enables more precise control over the diversity of generated outputs. In terms of performance, LLMs trained with Soft Preference Learning demonstrate improved accuracy on difficult tasks requiring repeated samplings while generating outputs with significantly enhanced semantic and lexical diversity. From an alignment standpoint, these models can better represent a broader spectrum of societal viewpoints and show improved calibration of their output probabilities (logits). Importantly, Soft Preference Learning is similar to temperature scaling but offers a Pareto improvement, balancing accuracy and diversity more effectively than traditional methods. <div>
arXiv:2511.08594v1 Announce Type: new 
Abstract: The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.08595</link>
<guid>https://arxiv.org/abs/2511.08595</guid>
<content:encoded><![CDATA[
<div> Tree-of-Thought, Semantic Similarity, Dynamic Pruning, Large Language Models, Efficient Reasoning<br /><br />Summary:  
This paper addresses the computational challenges in Tree-of-Thought (ToT) reasoning for Large Language Models (LLMs), particularly the redundancy found in exploring equivalent reasoning paths across different branches. To mitigate this, the authors propose Semantic Similarity-Based Dynamic Pruning (SSDP), an innovative and lightweight method that integrates semantic merging dynamically into a parallelized tree search process. SSDP clusters and prunes redundant reasoning steps in real time, significantly reducing unnecessary computations. The method was tested on various reasoning benchmarks, including GSM8K and MATH500, where it demonstrated up to a 2.3 times speedup compared to state-of-the-art tree-search baselines. Despite this acceleration, SSDP maintains competitive accuracy, typically within 5% of the strongest baseline results. Furthermore, SSDP drastically reduces the number of explored nodes by 85-90%, showing both scalability and efficiency benefits. The approach advances practical reasoning with LLMs by balancing speed and accuracy through effective redundancy elimination. The authors have also made the implementation publicly available on GitHub, promoting accessibility and further research in efficient LLM reasoning techniques. <div>
arXiv:2511.08595v1 Announce Type: new 
Abstract: Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge</title>
<link>https://arxiv.org/abs/2511.08596</link>
<guid>https://arxiv.org/abs/2511.08596</guid>
<content:encoded><![CDATA[
<div> framework, stress testing, factual fidelity, large language models, adversarial nudge

Summary:
The paper introduces a framework for stress testing the factual fidelity of large language models (LLMs) in the presence of adversarial nudges. The framework involves instructing the LLM to generate sets of truths and lies consistent with a closed domain, verifying these assertions, and testing the LLM's robustness against lies it generated. Evaluation using five popular proprietary LLMs in movie and novel domains shows varying levels of susceptibility to adversarial nudges, with Claude exhibiting strong resilience, GPT and Grok demonstrating moderate resilience, and Gemini and DeepSeek showing weak resilience. This study highlights the importance of considering the susceptibility of LLMs to adversarial manipulation, especially as they are increasingly used by the general population for information seeking.<br /><br />Summary: <div>
arXiv:2511.08596v1 Announce Type: new 
Abstract: Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \texttt{Claude} exhibits strong resilience, \texttt{GPT} and \texttt{Grok} demonstrate moderate resilience, while \texttt{Gemini} and \texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-HarmLLM: Can Large Language Model Harm Itself?</title>
<link>https://arxiv.org/abs/2511.08597</link>
<guid>https://arxiv.org/abs/2511.08597</guid>
<content:encoded><![CDATA[
<div> guardrails, harmful responses, Large Language Models, Self-HarmLLM scenario, Mitigated Harmful Query 

Summary: 
The study investigates the potential risk of Large Language Models (LLMs) generating harmful responses as a new attack vector. The Self-HarmLLM scenario introduces Mitigated Harmful Queries (MHQs) generated by the same model to test for vulnerabilities. Experiments on various models under different conditions showed transformation and jailbreak success rates, highlighting the need to reevaluate guardrail design. Automated evaluation methods were found to overestimate jailbreak success, emphasizing the importance of robust evaluation methodologies. While the study is limited in scope, it demonstrates the validity of the proposed attack scenario and raises questions about the current defenses in place for LLMs. The findings call for a fundamental reassessment of guardrail strategies and the development of more accurate evaluation techniques. 

Summary:  <div>
arXiv:2511.08597v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking</title>
<link>https://arxiv.org/abs/2511.08598</link>
<guid>https://arxiv.org/abs/2511.08598</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge-intensive question answering, large language models, Open Knowledge Bench, dynamic knowledge benchmarks, retrieval-augmented methods

Summary: 
The article introduces Open Knowledge Bench (OKBench), a framework designed to generate dynamic knowledge benchmarks for assessing knowledge-intensive question answering in large language models (LLMs). OKBench focuses on the news domain, where knowledge updates daily, by automating the sourcing, creation, validation, and distribution of benchmarks. This framework democratizes benchmark creation and allows for thorough evaluation of retrieval-augmented methods, reducing overlap with pretraining data. Evaluating various open-source and proprietary LLMs with and without retrieval over freshly generated knowledge reveals distinct model behaviors when confronted with new information. The results highlight how retrieval narrows the performance gap between small and large models, emphasizing the importance of evaluating LLMs on evolving knowledge benchmarks.<br /><br />Summary: <div>
arXiv:2511.08598v1 Announce Type: new 
Abstract: Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study</title>
<link>https://arxiv.org/abs/2511.08600</link>
<guid>https://arxiv.org/abs/2511.08600</guid>
<content:encoded><![CDATA[
<div> clinical vignettes, speech-language pathology, large language models, retrieval-augmented generation, pediatric SLP

Summary: 
This study introduces a system that combines retrieval-augmented generation (RAG) with curated knowledge bases to efficiently create pediatric speech-language pathology (SLP) case materials. The system utilizes a variety of large language models (LLMs) including both commercial and open-source options. Seven test scenarios were systematically designed to evaluate the generated cases based on structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. While commercial models showed slight quality advantages, open-source alternatives still performed adequately, suggesting potential for widespread use. The integration of curated knowledge bases ensured that the content generated adhered to professional guidelines. Further validation through expert review, student pilot testing, and psychometric evaluation is necessary before implementation in educational or research settings. Potential future applications include clinical decision support, automated IEP goal generation, and clinical reflection training. 

Summary: <div>
arXiv:2511.08600v1 Announce Type: new 
Abstract: Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating DisCoCirc in Translation Tasks &amp; its Limitations: A Comparative Study Between Bengali &amp; English</title>
<link>https://arxiv.org/abs/2511.08601</link>
<guid>https://arxiv.org/abs/2511.08601</guid>
<content:encoded><![CDATA[
<div> Keywords: DisCoCirc, grammar-based framework, translation tasks, language bureaucracy, English conjunctions <br />
Summary: 
The paper introduces a framework called DisCoCirc for Bengali language based on production rules and circuit-like representations. It aims to improve translation tasks between English and Bengali by reducing language bureaucracy. However, the study reveals limitations in handling structural variations between the two languages, leading to challenges even with simple sentences. These findings contradict previous claims and call for enhancements to address these shortcomings. The framework's struggle with translation implies a need for future improvements. Additionally, the paper explores the relationship between English conjunctions and Boolean logic, aligning with prior research. This study emphasizes the importance of addressing structural variations in language frameworks to enhance translation accuracy and efficiency. <br /><br />Summary: <div>
arXiv:2511.08601v1 Announce Type: new 
Abstract: In [4], the authors present the DisCoCirc (Distributed Compositional Circuits) formalism for the English language, a grammar-based framework derived from the production rules that incorporates circuit-like representations in order to give a precise categorical theoretical structure to the language. In this paper, we extend this approach to develop a similar framework for Bengali and apply it to translation tasks between English and Bengali. A central focus of our work lies in reassessing the effectiveness of DisCoCirc in reducing language bureaucracy. Unlike the result suggested in [5], our findings indicate that although it works well for a large part of the language, it still faces limitations due to the structural variation of the two languages. We discuss the possible methods that might handle these shortcomings and show that, in practice, DisCoCirc still struggles even with relatively simple sentences. This divergence from prior claims not only highlights the framework's constraints in translation but also suggest scope for future improvement. Apart from our primary focus on English-Bengali translation, we also take a short detour to examine English conjunctions, following [1], showing a connection between conjunctions and Boolean logic.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice</title>
<link>https://arxiv.org/abs/2511.08605</link>
<guid>https://arxiv.org/abs/2511.08605</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangladesh, legal AI assistant, multilingual LLM, access to justice, low-resource systems<br /><br />Summary:<br /><br />1. Bangladesh’s low-income population struggles with accessing affordable legal advice due to complex legal language, opaque procedures, and high costs. <br /><br />2. Existing AI legal assistants fail to effectively support Bengali language and are not adapted to Bangladesh’s specific legal jurisdiction, limiting their practical usefulness. <br /><br />3. To overcome these barriers, Mina was developed as a multilingual large language model (LLM)-based legal assistant customized for the Bangladeshi context, integrating multilingual embeddings and a retrieval-augmented generation (RAG) chain-of-tools framework. <br /><br />4. Mina provides context-aware legal drafts, citations, and simplified explanations through an interactive chat interface, supporting retrieval, reasoning, translation, and document generation tailored to local legal requirements. <br /><br />5. Law faculty from leading universities evaluated Mina using the Bangladesh Bar Council Exams (2022 and 2023), where it achieved scores between 75-80% across Preliminary MCQs, Written, and simulated Viva Voce exams, matching or exceeding average human performance and demonstrating legal clarity and reasoning. <br /><br />6. These results showcase Mina’s potential as a cost-effective, multilingual AI assistant that automates key legal tasks, expands access to justice, and offers a practical example of building domain-specific AI systems for low-resource, multilingual environments with sustainable public-service deployment. <div>
arXiv:2511.08605v1 Announce Type: new 
Abstract: Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Super-Learner with Large Language Models for Medical Emergency Advising</title>
<link>https://arxiv.org/abs/2511.08614</link>
<guid>https://arxiv.org/abs/2511.08614</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical decision-support, Large Language Models, emergency medicine, diagnostic accuracy, meta-learning  

<br /><br />Summary:  
This study evaluates the diagnostic capabilities of five prominent Large Language Models (LLMs)—Gemini, Llama, Grok, GPT, and Claude—in emergency medicine. The accuracy of individual LLMs in diagnosing acute diseases ranged from 58% to 65%, which notably surpasses the reported diagnostic accuracy of human doctors in similar settings. To further improve performance, the researchers developed a super-learner called MEDAS (Medical Emergency Diagnostic Advising System), which integrates the five LLMs through a meta-learning approach. This meta-learner exploits the varying strengths of each LLM, learning to optimally combine their outputs to enhance overall diagnostic accuracy. The MEDAS super-learner achieved a higher accuracy of 70%, outperforming any single model. Interestingly, within the cluster, at least one individual LLM was capable of making correct diagnoses with an accuracy of up to 85%. The results indicate that combining multiple LLMs using a meta-learner can leverage diverse knowledge and produce superior diagnostic outcomes compared to relying on any single model alone. This approach demonstrates the potential to transform emergency medical decision-support systems by utilizing the collective intelligence of advanced language models. <div>
arXiv:2511.08614v1 Announce Type: new 
Abstract: Medical decision-support and advising systems are critical for emergency physicians to quickly and accurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged as a transformative force in healthcare in recent years and Large Language Models (LLMs) have been employed in various fields of medical decision-support systems. We studied responses of a group of different LLMs to real cases in emergency medicine. The results of our study on five most renown LLMs showed significant differences in capabilities of Large Language Models for diagnostics acute diseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy significantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS (Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and Claude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-learner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct diagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning different capabilities of each LLM to leverage diagnostic accuracy of the model by collective capabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic accuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that the super-learner can take advantage of the combined knowledge of the medical datasets used to train the group of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM</title>
<link>https://arxiv.org/abs/2511.08620</link>
<guid>https://arxiv.org/abs/2511.08620</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, supervised fine-tuning, gradient-aware data selection, domain specialization, catastrophic forgetting<br /><br />Summary:<br /><br />1. Large language models (LLMs) have shown strong performance but still require supervised fine-tuning (SFT) to adapt effectively to specialized domains.  
2. Traditional SFT is resource-intensive and prone to catastrophic forgetting, which causes the model's general capabilities to deteriorate after domain-specific training.  
3. The authors propose GrADS, a self-adaptive gradient-aware data selection method that analyzes gradients from an initial training phase to identify the most effective subset of training data for fine-tuning.  
4. GrADS uses self-guided criteria based on gradient magnitudes and their statistical distribution to select representative samples that significantly contribute to learning domain-specific tasks.  
5. Experimental results across multiple domains such as medicine, law, and finance demonstrate that GrADS achieves higher efficiency and cost-effectiveness. Using just 5% of the selected GrADS data, LLMs outperform models fine-tuned on the full dataset, and with 50%, the improvements become even more substantial.  
6. Importantly, GrADS also markedly reduces catastrophic forgetting, preserving the LLM’s general capabilities while enhancing domain specialization.  
7. The authors plan to release the GrADS code to encourage further research and application. <div>
arXiv:2511.08620v1 Announce Type: new 
Abstract: Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism</title>
<link>https://arxiv.org/abs/2511.08636</link>
<guid>https://arxiv.org/abs/2511.08636</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide Detection, Hybrid Deep Learning, CNN, BiGRU, Explainable AI<br /><br />Summary:  
The study addresses the critical issue of adolescent suicide by focusing on identifying suicidal ideation through social media analysis. It proposes a novel hybrid deep learning framework that combines Convolutional Neural Networks (CNN) for local feature extraction with Bidirectional Gated Recurrent Units (BiGRU) for sequence modeling. The model integrates attention mechanisms to enhance pattern recognition in social network (SN) datasets. To improve transparency and trustworthiness, Explainable AI techniques using SHapley Additive exPlanations (SHAP) were applied to interpret the prediction outcomes and validate model reliability. The framework was trained and evaluated on a publicly available dataset, utilizing multiple performance metrics for a thorough assessment. Experimental results demonstrated the model’s effectiveness, achieving an accuracy of 93.97%. Furthermore, the study includes a comparative analysis with other state-of-the-art machine learning and deep learning models, highlighting the superiority of the proposed method. Overall, the integration of CNN, BiGRU, attention mechanisms, and SHAP contributes to a robust and interpretable solution for suicide risk detection in social media contexts, potentially aiding early intervention strategies. <div>
arXiv:2511.08636v1 Announce Type: new 
Abstract: Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Uncertainty guided Clarification for LLM Agents</title>
<link>https://arxiv.org/abs/2511.08798</link>
<guid>https://arxiv.org/abs/2511.08798</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, structured uncertainty, tool-call parameters, POMDP, clarification questions<br /><br />Summary: This paper addresses the challenge of ambiguous user instructions in large language model (LLM) agents that perform tool-calling tasks, which often cause incorrect invocations and task failures. The authors propose a principled formulation of structured uncertainty over tool-call parameters, treating joint tool-argument clarification as a Partially Observable Markov Decision Process (POMDP) that optimizes question selection using the Expected Value of Perfect Information (EVPI) and incorporates aspect-based cost modeling to avoid redundant clarifications. Their SAGE-Agent leverages this model of uncertainty to improve efficiency significantly, enhancing task coverage on ambiguous inputs by 7-39% and reducing the number of clarification questions by 1.5 to 2.7 times compared to strong prompting and uncertainty-based baseline methods. The authors introduce ClarifyBench, a novel multi-turn benchmark for tool-augmented disambiguation that uses realistic LLM-based user simulation and covers multiple domains such as document editing, vehicle control, and travel booking. Furthermore, they demonstrate that structured uncertainty provides valuable training signals for reinforcement learning, with uncertainty-weighted GRPO training boosting the When2Call accuracy substantially — from 36.5% to 65.2% for a 3-billion parameter model and from 36.7% to 62.9% for a 7-billion parameter model. These findings establish structured uncertainty as an effective and principled approach for enhancing task success rates and interaction efficiency in real-world LLM-agent applications. <div>
arXiv:2511.08798v1 Announce Type: new 
Abstract: LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\% to 65.2\% (3B model) and 36.7\% to 62.9\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Automated Cognitive Assessment in Parkinson's Disease Using Pretrained Language Models</title>
<link>https://arxiv.org/abs/2511.08806</link>
<guid>https://arxiv.org/abs/2511.08806</guid>
<content:encoded><![CDATA[
<div> natural language processing, Parkinson's disease, cognitive processes, entity recognition, machine learning

Summary: 
- The study aims to understand how individuals with Parkinson's disease describe their cognitive experiences through natural language processing (NLP) models.
- Three model families were evaluated: Bio_ClinicalBERT, Meta-Llama-3-8B-Instruct, and GPT-4o mini, for extracting seven cognitive categories from patient narratives.
- The fine-tuned Meta-Llama-3-8B-Instruct model performed the best with high F1-scores, particularly excelling in context-dependent categories like thought and social interaction.
- Bio_ClinicalBERT showed high precision but low recall, performing well in some categories like location and time but failing in others such as thought, emotion, and social interaction.
- The task of extracting cognitive processes from narratives is challenging due to the abstract and overlapping nature, but with refinement, NLP systems show promise in monitoring cognitive function longitudinally in Parkinson's disease patients and supplementing formal neuropsychological assessments. 

<br /><br />Summary: <div>
arXiv:2511.08806v1 Announce Type: new 
Abstract: Understanding how individuals with Parkinson's disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BNLI: A Linguistically-Refined Bengali Dataset for Natural Language Inference</title>
<link>https://arxiv.org/abs/2511.08813</link>
<guid>https://arxiv.org/abs/2511.08813</guid>
<content:encoded><![CDATA[
<div> Keywords: Bengali, NLI, dataset, annotation, semantic clarity
Summary:
BNLI is a refined and curated Bengali NLI dataset created to address limitations in existing resources. The dataset underwent a rigorous annotation process to ensure semantic clarity and balance across entailment, contradiction, and neutrality classes. State-of-the-art transformer-based models were used to benchmark BNLI, showcasing improved reliability and interpretability in capturing complex semantic relations in Bengali text. BNLI serves as a strong foundation for advancing research in Bengali and other low-resource language inference tasks. <br /><br />Summary: BNLI is a curated Bengali NLI dataset with a rigorous annotation process, providing semantic clarity and balance across different classes. It was benchmarked using transformer-based models, demonstrating improved reliability and interpretability in capturing complex semantic relations in Bengali text. BNLI is a valuable resource for advancing research in low-resource language inference tasks. <div>
arXiv:2511.08813v1 Announce Type: new 
Abstract: Despite the growing progress in Natural Language Inference (NLI) research, resources for the Bengali language remain extremely limited. Existing Bengali NLI datasets exhibit several inconsistencies, including annotation errors, ambiguous sentence pairs, and inadequate linguistic diversity, which hinder effective model training and evaluation. To address these limitations, we introduce BNLI, a refined and linguistically curated Bengali NLI dataset designed to support robust language understanding and inference modeling. The dataset was constructed through a rigorous annotation pipeline emphasizing semantic clarity and balance across entailment, contradiction, and neutrality classes. We benchmarked BNLI using a suite of state-of-the-art transformer-based architectures, including multilingual and Bengali-specific models, to assess their ability to capture complex semantic relations in Bengali text. The experimental findings highlight the improved reliability and interpretability achieved with BNLI, establishing it as a strong foundation for advancing research in Bengali and other low-resource language inference tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents</title>
<link>https://arxiv.org/abs/2511.08835</link>
<guid>https://arxiv.org/abs/2511.08835</guid>
<content:encoded><![CDATA[
<div> Keywords: conversational agents, task-oriented dialogue, chitchat, mode transition, Direct Preference Optimization (DPO)  

<br /><br />Summary:  
This paper addresses the longstanding challenge of unifying task-oriented dialogue (TOD) and open-ended chitchat in conversational agents, which traditionally are developed separately. Real-world conversations naturally transition fluidly between these modes, yet prior models lack robust mechanisms for handling such transitions. To bridge this gap, the authors introduce TACT (TOD-And-Chitchat Transition), a novel dataset designed for transition-aware dialogue modeling. TACT incorporates structurally diverse and integrated conversational flows supporting both user- and agent-driven switches between TOD and chitchat, enabling the training of agents that better manage complex dialogue dynamics. To evaluate agent performance on mode transitions, the paper proposes two new metrics: Switch, measuring the detection of mode transitions, and Recovery, assessing the agent's ability to recover from such transitions effectively. Models trained on TACT outperform baseline models in intent detection and mode transition handling, demonstrating the dataset’s efficacy. Furthermore, applying Direct Preference Optimization (DPO), a preference-based training technique, on TACT-trained models yields additional improvements. The best model achieves a joint mode-intent accuracy of 75.74% and attains a 70.1% win rate against GPT-4o in human evaluations. These findings suggest that combining diverse transition data with DPO fine-tuning significantly improves response quality and transition control, advancing the development of proactive, transition-aware conversational agents. <div>
arXiv:2511.08835v1 Announce Type: new 
Abstract: Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\% joint mode-intent accuracy and a 70.1\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation</title>
<link>https://arxiv.org/abs/2511.08866</link>
<guid>https://arxiv.org/abs/2511.08866</guid>
<content:encoded><![CDATA[
<div> Keywords: Hypothesis generation, Biomedical research, Large Language Model, BioVerge, ReAct<br />
Summary: <br />
1) Different architectures of BioVerge Agent influence exploration diversity and reasoning strategies. 
2) Structured and textual information sources provide unique contexts that enhance hypothesis generation. 
3) Self-evaluation significantly improves novelty and relevance of proposed hypotheses. 
BioVerge introduces a benchmark dataset and an LLM-based agent framework for standardized biomedical hypothesis generation. The dataset includes structured and textual data from historical biomedical hypotheses and PubMed literature. BioVerge Agent utilizes a ReAct-based approach with Generation and Evaluation modules for proposing and self-assessing hypotheses. Experiments highlight the influence of agent architectures on exploration diversity and reasoning strategies, the importance of structured and textual information sources, and the benefits of self-evaluation for generating novel and relevant hypotheses. <div>
arXiv:2511.08866v1 Announce Type: new 
Abstract: Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models</title>
<link>https://arxiv.org/abs/2511.08877</link>
<guid>https://arxiv.org/abs/2511.08877</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination, citation frequency, memorization, factual accuracy<br /><br />Summary:<br /><br />This study examines how large language models (LLMs), specifically GPT-4.1, handle bibliographic citation generation and the tendency to hallucinate non-existent papers. It hypothesizes that an LLM's accuracy in producing bibliographic records depends on whether the information is memorized from frequent appearances in training data or generated anew. The research uses citation count as a proxy for training data redundancy, assuming that highly cited papers are more likely to appear in the pretraining corpus. By generating 100 citations across twenty computer science domains and verifying them manually, researchers measured factual consistency through cosine similarity between generated and authentic metadata. The results showed a strong positive correlation between citation count and factual accuracy, with papers cited over roughly 1,000 times being almost memorized verbatim by the model. However, the presence of multiple highly cited papers with similar content led to memory interference, which affected recall accuracy. These findings suggest a threshold where an LLM shifts from generalizing to memorizing information, identifying highly cited works as nearly verbatim retained within the model’s internal knowledge. This points to both opportunities and challenges in citation recommendation systems using LLMs, emphasizing citation frequency as a critical factor in reducing hallucinated references. <div>
arXiv:2511.08877v1 Announce Type: new 
Abstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in citation recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic records depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the pretraining corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record appears in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 citations across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) citation count is strongly correlated with factual accuracy, (ii) bibliographic information becomes almost verbatim memorized beyond roughly 1,000 citations, and (iii) memory interference occurs when multiple highly cited papers share similar content. These findings indicate a threshold where generalization shifts into memorization, with highly cited papers being nearly verbatim retained in the model.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HalluClean: A Unified Framework to Combat Hallucinations in LLMs</title>
<link>https://arxiv.org/abs/2511.08916</link>
<guid>https://arxiv.org/abs/2511.08916</guid>
<content:encoded><![CDATA[
<div> Framework, Hallucinations, Language models, Factual reliability, Trustworthiness  
Summary:  
HalluClean is a new framework introduced to address the issue of hallucinated content in Large Language Models (LLMs). It focuses on detecting and correcting unsupported claims in LLM-generated text by utilizing a reasoning-enhanced approach with distinct stages of planning, execution, and revision. HalluClean is task-agnostic, requiring minimal prompts for zero-shot generalization across various domains without external knowledge sources or supervised detectors. Evaluations across different tasks such as question answering and summarization demonstrate significant improvements in factual consistency compared to baseline methods. The framework shows potential in enhancing the trustworthiness of LLM outputs for real-world applications.  
<br /><br />Summary: <div>
arXiv:2511.08916v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TiDAR: Think in Diffusion, Talk in Autoregression</title>
<link>https://arxiv.org/abs/2511.08923</link>
<guid>https://arxiv.org/abs/2511.08923</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion language models, Autoregressive models, TiDAR, Parallel generation, Structured attention masks<br /><br />Summary:  
1. Diffusion language models enable fast parallel token generation, but autoregressive (AR) models usually provide higher quality because their causal, left-to-right structure aligns naturally with language tasks.  
2. Existing approaches either rely on weaker AR-like drafting models, reducing drafting efficiency, or apply sequential decoding to diffusion models, losing quality and parallelizability benefits.  
3. TiDAR is proposed as a novel hybrid architecture combining diffusion-based token drafting ("Thinking") and autoregressive final sampling ("Talking") within a single forward pass using structured attention masks.  
4. This design maximizes GPU utilization by balancing token drafting and verification without significant overhead, making TiDAR practical for deployment as a standalone model.  
5. Extensive evaluation at 1.5B and 8B parameter scales shows TiDAR outperforms speculative decoding in throughput, surpasses diffusion models like Dream and Llada in both efficiency and quality, and uniquely closes the quality gap with AR models while delivering 4.71x to 5.91x higher token generation speed. <div>
arXiv:2511.08923v1 Announce Type: new 
Abstract: Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI</title>
<link>https://arxiv.org/abs/2511.08949</link>
<guid>https://arxiv.org/abs/2511.08949</guid>
<content:encoded><![CDATA[
<div> framework, NLP models, human label variation, annotation errors, large language models

Summary:
The study introduces a new framework called EVADE for detecting errors in natural language inference tasks using large language models (LLMs). It aims to address human label variation (HLV) by generating and validating explanations to identify errors. The framework is compared to the earlier VARIERR framework, which requires two rounds of manual annotation. The study shows that LLM validation can refine explanation distributions to better align with human annotations. Removing errors detected by LLMs leads to improved fine-tuning performance compared to errors identified by human annotators. This research highlights the potential for scaling error detection and improving dataset quality while reducing human effort in handling label variation. <div>
arXiv:2511.08949v1 Announce Type: new 
Abstract: High-quality datasets are critical for training and evaluating reliable NLP models. In tasks like natural language inference (NLI), human label variation (HLV) arises when multiple labels are valid for the same instance, making it difficult to separate annotation errors from plausible variation. An earlier framework VARIERR (Weber-Genzel et al., 2024) asks multiple annotators to explain their label decisions in the first round and flag errors via validity judgments in the second round. However, conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations. Our study proposes a new framework, EVADE, for generating and validating explanations to detect errors using large language models (LLMs). We perform a comprehensive analysis comparing human- and LLM-detected errors for NLI across distribution comparison, validation overlap, and impact on model fine-tuning. Our experiments demonstrate that LLM validation refines generated explanation distributions to more closely align with human annotations, and that removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators. This highlights the potential to scale error detection, reducing human effort while improving dataset quality under label variation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving</title>
<link>https://arxiv.org/abs/2511.08983</link>
<guid>https://arxiv.org/abs/2511.08983</guid>
<content:encoded><![CDATA[
<div> Framework, Iterative updates, Latent reasoning, Alignment, Reasoning tasks 

Summary: 
The paper introduces SpiralThinker, a unified framework for latent reasoning that incorporates iterative updates over latent representations to enhance implicit reasoning without increasing token generation. By implementing a progressive alignment objective and structured annotations, SpiralThinker ensures coherence between latent and textual reasoning, resulting in improved performance across various reasoning tasks. Through detailed analyses, the study highlights the significance of iteration and alignment, with optimal numbers of latent tokens and iterations varying by dataset. It emphasizes the crucial role of alignment in facilitating an effective iterative process. SpiralThinker successfully integrates iterative computation with latent reasoning, underscoring the importance of aligned iterative updates in steering reasoning within the latent space. <div>
arXiv:2511.08983v1 Announce Type: new 
Abstract: Recent advances in large reasoning models have been driven by reinforcement learning and test-time scaling, accompanied by growing interest in latent rather than purely textual reasoning. However, existing latent reasoning methods lack mechanisms to ensure stable evolution of latent representations and a systematic way to interleave implicit and explicit reasoning. We introduce SpiralThinker, a unified framework that performs iterative updates over latent representations, enabling extended implicit reasoning without generating additional tokens. A progressive alignment objective combined with structured annotations maintains coherence between latent and textual reasoning. Across mathematical, logical, and commonsense reasoning tasks, SpiralThinker achieves the best overall performance among latent reasoning approaches, consistently surpassing previous methods across all benchmarks. Detailed analyses reveal that both iteration and alignment are indispensable, the numbers of latent tokens and iterations exhibit dataset-specific optima, and appropriate alignment proves critical for an effective iterative process. Overall, SpiralThinker bridges iterative computation and latent reasoning, demonstrating that aligned iterative updates can reliably steer reasoning in the latent space.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models</title>
<link>https://arxiv.org/abs/2511.09003</link>
<guid>https://arxiv.org/abs/2511.09003</guid>
<content:encoded><![CDATA[
<div> Emotional support, large language models, evaluation, trajectory-based assessment, user-centered perspective <br />
<br />
Summary: This research introduces a new framework for evaluating the emotional support capabilities of large language models (LLMs) through trajectory-based assessment. The study focuses on the long-term and dynamic nature of emotional support in human-AI interaction, moving away from static evaluations. A benchmark is created with various emotional contexts and disturbance events to simulate realistic emotional shifts during evolving dialogue scenarios. The framework incorporates validated emotion regulation strategies to guide model responses towards psychologically grounded outcomes. User emotional trajectories are modeled using a first-order Markov process, and causally-adjusted emotion estimation is applied for unbiased emotional state tracking. Three trajectory-level metrics are introduced to capture user emotional dynamics over time: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). Evaluations across different LLMs show significant disparities in emotional support capabilities, providing valuable insights for model development. <div>
arXiv:2511.09003v1 Announce Type: new 
Abstract: Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Approach to Natural Language Formalization and Verification</title>
<link>https://arxiv.org/abs/2511.09008</link>
<guid>https://arxiv.org/abs/2511.09008</guid>
<content:encoded><![CDATA[
<div> framework, neurosymbolic, language models, formalization process, logical correctness
Summary: 
The article introduces a novel two-stage neurosymbolic framework to address the stochastic limitations of Large Language Models (LLMs) in regulated industries like finance and healthcare. The framework involves using LLMs with optional human guidance to formalize natural language policies and conducting inference-time autoformalization to ensure logical correctness. Multiple redundant formalization steps are performed at inference time to cross-check for semantic equivalence, resulting in over 99% soundness and near-zero false positive rates in identifying logical validity. The approach generates auditable logical artifacts that validate verification outcomes and can enhance the original text. This framework provides a mechanism for fine-grained control over the formalization process and offers a reliable solution for ensuring the accuracy and compliance of natural language statements in regulated sectors. 
<br /><br />Summary: <div>
arXiv:2511.09008v1 Announce Type: new 
Abstract: Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-CRITIC: A Holistic Evaluation of Large Multimodal Models as Multimodal Critique</title>
<link>https://arxiv.org/abs/2511.09067</link>
<guid>https://arxiv.org/abs/2511.09067</guid>
<content:encoded><![CDATA[
<div> Keywords: critique, Large Multimodal Models, benchmark, evaluation, GPT-4o

Summary: 
The study introduces MM-CRITIC, a benchmark for evaluating critique ability in Large Multimodal Models (LMMs) in various dimensions. The benchmark covers basic, correction, and comparison tasks across 8 main task types, collecting responses from different LMMs. Ground answers are integrated to enhance evaluation reliability, guiding the annotation of responses and reference critique generation by GPT-4o for trustworthy judgments. Experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities. The analysis reveals insights on the correlation between response quality and critique, as well as varying critique difficulty across evaluation dimensions. The code for MM-CRITIC is available on GitHub for further exploration and research. 

<br /><br />Summary: <div>
arXiv:2511.09067v1 Announce Type: new 
Abstract: The ability of critique is vital for models to self-improve and serve as reliable AI assistants. While extensively studied in language-only settings, multimodal critique of Large Multimodal Models (LMMs) remains underexplored despite their growing capabilities in tasks like captioning and visual reasoning. In this work, we introduce MM-CRITIC, a holistic benchmark for evaluating the critique ability of LMMs across multiple dimensions: basic, correction, and comparison. Covering 8 main task types and over 500 tasks, MM-CRITIC collects responses from various LMMs with different model sizes and is composed of 4471 samples. To enhance the evaluation reliability, we integrate expert-informed ground answers into scoring rubrics that guide GPT-4o in annotating responses and generating reference critiques, which serve as anchors for trustworthy judgments. Extensive experiments validate the effectiveness of MM-CRITIC and provide a comprehensive assessment of leading LMMs' critique capabilities under multiple dimensions. Further analysis reveals some key insights, including the correlation between response quality and critique, and varying critique difficulty across evaluation dimensions. Our code is available at https://github.com/MichealZeng0420/MM-Critic.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Dynamic Chunking for Streaming Tibetan Speech Recognition</title>
<link>https://arxiv.org/abs/2511.09085</link>
<guid>https://arxiv.org/abs/2511.09085</guid>
<content:encoded><![CDATA[
<div> Keywords: streaming speech recognition, Amdo Tibetan, hybrid CTC/Attention architecture, dynamic chunking mechanism, lexicon construction

Summary: 
A new streaming speech recognition framework for Amdo Tibetan is proposed in this work. It utilizes a hybrid CTC/Attention architecture with a context-aware dynamic chunking mechanism that adjusts chunk widths based on encoding states for flexible receptive fields. By addressing the context truncation problem of fixed-chunk methods, the framework adapts well to varying speaking rates. A lexicon based on Tibetan orthographic principles is constructed for linguistically motivated modeling units. The integration of an external language model during decoding enhances semantic consistency and improves recognition of longer sentences. Experimental results demonstrate a significant 48.15% relative improvement in word error rate (WER) over the fixed-chunk baseline, achieving a WER of 6.23% on the test set. The framework also reduces recognition latency while maintaining performance close to global decoding. 

<br /><br />Summary: <div>
arXiv:2511.09085v1 Announce Type: new 
Abstract: In this work, we propose a streaming speech recognition framework for Amdo Tibetan, built upon a hybrid CTC/Atten-tion architecture with a context-aware dynamic chunking mechanism. The proposed strategy adaptively adjusts chunk widths based on encoding states, enabling flexible receptive fields, cross-chunk information exchange, and robust adaptation to varying speaking rates, thereby alleviating the context truncation problem of fixed-chunk methods. To further capture the linguistic characteristics of Tibetan, we construct a lexicon grounded in its orthographic principles, providing linguistically motivated modeling units. During decoding, an external language model is integrated to enhance semantic consistency and improve recognition of long sentences. Experimental results show that the proposed framework achieves a word error rate (WER) of 6.23% on the test set, yielding a 48.15% relative improvement over the fixed-chunk baseline, while significantly reducing recognition latency and maintaining performance close to global decoding.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2511.09109</link>
<guid>https://arxiv.org/abs/2511.09109</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, reasoning framework, bidirectional information distance, multi-objective reinforcement learning, question answering benchmarks

Summary:
Bi-RAR is a novel retrieval-augmented reasoning framework that addresses limitations in complex reasoning scenarios by evaluating intermediate steps in both forward and backward directions. It introduces a bidirectional information distance grounded in Kolmogorov complexity to measure the completeness of each step in addressing the question and approaching the answer. To optimize reasoning, a multi-objective reinforcement learning framework is used with a cascading reward structure emphasizing early trajectory alignment. Empirical results on seven question answering benchmarks show that Bi-RAR outperforms previous methods and enables efficient interaction and reasoning with the search engine during training and inference. This approach mitigates hallucinations in large language models and enhances response quality by providing explicit guidance for intermediate steps, thereby avoiding reward hacking and leading to improved performance in multi-step reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2511.09109v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios.Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Capabilities of LLMs in Humor:A Multi-dimensional Analysis of Oogiri Generation and Evaluation</title>
<link>https://arxiv.org/abs/2511.09133</link>
<guid>https://arxiv.org/abs/2511.09133</guid>
<content:encoded><![CDATA[
<div> Keywords: Computational humor, Natural Language Processing, Oogiri, Large Language Models, Empathy <br />
Summary: <br />
- The paper focuses on evaluating Large Language Models (LLMs) through the lens of Oogiri, a form of Japanese improvisational comedy games.
- A multifaceted understanding of humor is essential, and the study systematically evaluates LLMs across dimensions like Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness.
- LLMs can generate Oogiri responses at a level between low- and mid-tier human performance but exhibit a notable lack of Empathy compared to humans.
- The lack of Empathy in LLMs explains their failure to replicate human humor assessment, as they prioritize Novelty while humans prioritize Empathy in humor evaluation.
- The annotated corpus created in this study is released to the community to facilitate the development of emotionally intelligent and sophisticated conversational agents. 

Summary: <div>
arXiv:2511.09133v1 Announce Type: new 
Abstract: Computational humor is a frontier for creating advanced and engaging natural language processing (NLP) applications, such as sophisticated dialogue systems. While previous studies have benchmarked the humor capabilities of Large Language Models (LLMs), they have often relied on single-dimensional evaluations, such as judging whether something is simply ``funny.'' This paper argues that a multifaceted understanding of humor is necessary and addresses this gap by systematically evaluating LLMs through the lens of Oogiri, a form of Japanese improvisational comedy games. To achieve this, we expanded upon existing Oogiri datasets with data from new sources and then augmented the collection with Oogiri responses generated by LLMs. We then manually annotated this expanded collection with 5-point absolute ratings across six dimensions: Novelty, Clarity, Relevance, Intelligence, Empathy, and Overall Funniness. Using this dataset, we assessed the capabilities of state-of-the-art LLMs on two core tasks: their ability to generate creative Oogiri responses and their ability to evaluate the funniness of responses using a six-dimensional evaluation. Our results show that while LLMs can generate responses at a level between low- and mid-tier human performance, they exhibit a notable lack of Empathy. This deficit in Empathy helps explain their failure to replicate human humor assessment. Correlation analyses of human and model evaluation data further reveal a fundamental divergence in evaluation criteria: LLMs prioritize Novelty, whereas humans prioritize Empathy. We release our annotated corpus to the community to pave the way for the development of more emotionally intelligent and sophisticated conversational agents.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Topic-Doesn't-Fit-All: Transcreating Reading Comprehension Test for Personalized Learning</title>
<link>https://arxiv.org/abs/2511.09135</link>
<guid>https://arxiv.org/abs/2511.09135</guid>
<content:encoded><![CDATA[
<div> Personalized Learning, EFL Education, Reading Comprehension, Content Transcreation, Motivation  

<br /><br />Summary:  
This paper introduces a novel approach to personalized learning in English as a Foreign Language (EFL) education by generating customized reading comprehension tests that cater to individual student interests. The authors develop a structured content transcreation pipeline utilizing OpenAI's gpt-4o, starting from the RACE-C dataset. This pipeline produces new reading passages and multiple-choice questions that preserve the linguistic style of the originals while aligning semantically with learners' specific interests. The methodology incorporates several key components, including topic extraction, question classification based on Bloom's taxonomy, linguistic feature analysis, and the content transcreation process itself. To evaluate the effectiveness of this approach, a controlled experiment is conducted with EFL learners in South Korea. The study's findings reveal that students engaging with personalized reading materials demonstrate better reading comprehension performance and maintain higher motivation levels compared to peers using non-personalized content. Overall, the research highlights the potential of leveraging AI-driven content personalization to enhance both engagement and learning outcomes in EFL reading comprehension. <div>
arXiv:2511.09135v1 Announce Type: new 
Abstract: Personalized learning has gained attention in English as a Foreign Language (EFL) education, where engagement and motivation play crucial roles in reading comprehension. We propose a novel approach to generating personalized English reading comprehension tests tailored to students' interests. We develop a structured content transcreation pipeline using OpenAI's gpt-4o, where we start with the RACE-C dataset, and generate new passages and multiple-choice reading comprehension questions that are linguistically similar to the original passages but semantically aligned with individual learners' interests. Our methodology integrates topic extraction, question classification based on Bloom's taxonomy, linguistic feature analysis, and content transcreation to enhance student engagement. We conduct a controlled experiment with EFL learners in South Korea to examine the impact of interest-aligned reading materials on comprehension and motivation. Our results show students learning with personalized reading passages demonstrate improved comprehension and motivation retention compared to those learning with non-personalized materials.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DoPE: Denoising Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2511.09146</link>
<guid>https://arxiv.org/abs/2511.09146</guid>
<content:encoded><![CDATA[
<div> Rotary Position Embedding, attention sink, Denoising Positional Encoding, truncated matrix entropy, length extrapolation<br /><br />Summary:<br /><br />1. The paper addresses inherent limitations of Rotary Position Embedding (RoPE) in Transformer models, particularly its weakening effect on length extrapolation capabilities. <br />2. It reinterprets the attention map combined with positional encoding as a noisy feature map, which motivates a novel approach to positional encoding enhancement. <br />3. The authors propose Denoising Positional Encoding (DoPE), a training-free method that leverages truncated matrix entropy to detect and remove outlier frequency bands in the attention feature map. <br />4. DoPE further uses a parameter-free Gaussian distribution for reparameterization, improving robustness when generalizing to longer sequence lengths. <br />5. The method theoretically connects the cause of the "attention sink" problem with truncated matrix entropy and experimentally demonstrates significant improvements in retrieval accuracy and reasoning stability on tasks with contexts extended up to 64K tokens, showing effective mitigation of attention sinks and restoration of balanced attention patterns for better length generalization. <div>
arXiv:2511.09146v1 Announce Type: new 
Abstract: Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</title>
<link>https://arxiv.org/abs/2511.09148</link>
<guid>https://arxiv.org/abs/2511.09148</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data evolution, model training, closed-loop process, tool-use capabilities

Summary: 
LoopTool introduces a closed-loop framework for enhancing large language models (LLMs) by tightly integrating data synthesis and model training. The framework consists of three modules: Greedy Capability Probing (GCP) diagnoses model capabilities, Judgement-Guided Label Verification (JGLV) corrects annotation errors, and Error-Driven Data Expansion (EDDE) generates new challenging samples. By iteratively refining data and models, LoopTool outperforms a 32B data generator and achieves state-of-the-art results on benchmarks like BFCL-v3 and ACEBench. The cost-effective, open-source ecosystem eliminates reliance on expensive closed-source APIs. This approach demonstrates the potential for closed-loop, self-refining data pipelines to boost LLM tool-use capabilities.<br /><br />Summary: <div>
arXiv:2511.09148v1 Announce Type: new 
Abstract: Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Search for Complex Table Question Answering in Securities Report</title>
<link>https://arxiv.org/abs/2511.09179</link>
<guid>https://arxiv.org/abs/2511.09179</guid>
<content:encoded><![CDATA[
<div> keyword: Large Language Models, Table Question Answering, cell extraction method, contrastive learning, text-search models
Summary:<br /><br />
- Large Language Models (LLMs) are being used in Table Question Answering (TQA) to extract information from tables in documents.
- Entering entire tables as text into LLMs often leads to incorrect answers due to the inability to capture complex table structures.
- The proposed cell extraction method for TQA utilizes a hybrid retrieval mechanism to estimate table headers based on similarities between a question and individual cells.
- The approach selects the answer based on the most relevant row and column intersection.
- The language model is trained using contrastive learning on a small dataset of question-header pairs to improve performance.
- Experimental results on the TQA dataset from the U4 shared task at NTCIR-18 showed an accuracy of 74.6%, outperforming existing LLMs like GPT-4o mini (63.9%).
- Future work will focus on incorporating more efficient text-search models to further enhance performance and approach human evaluation results.  
Summary: <div>
arXiv:2511.09179v1 Announce Type: new 
Abstract: Recently, Large Language Models (LLMs) are gaining increased attention in the domain of Table Question Answering (TQA), particularly for extracting information from tables in documents. However, directly entering entire tables as long text into LLMs often leads to incorrect answers because most LLMs cannot inherently capture complex table structures. In this paper, we propose a cell extraction method for TQA without manual identification, even for complex table headers. Our approach estimates table headers by computing similarities between a given question and individual cells via a hybrid retrieval mechanism that integrates a language model and TF-IDF. We then select as the answer the cells at the intersection of the most relevant row and column. Furthermore, the language model is trained using contrastive learning on a small dataset of question-header pairs to enhance performance. We evaluated our approach in the TQA dataset from the U4 shared task at NTCIR-18. The experimental results show that our pipeline achieves an accuracy of 74.6\%, outperforming existing LLMs such as GPT-4o mini~(63.9\%). In the future, although we used traditional encoder models for retrieval in this study, we plan to incorporate more efficient text-search models to improve performance and narrow the gap with human evaluation results.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context is Enough: Empirical Validation of $\textit{Sequentiality}$ on Essays</title>
<link>https://arxiv.org/abs/2511.09185</link>
<guid>https://arxiv.org/abs/2511.09185</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, sequentiality, contextual terms, essay scoring, discourse traits  

<br /><br />Summary:  
1. This paper addresses the critique of a previously proposed metric called sequentiality, which combined topic and contextual terms to quantify narrative flow using Large Language Models (LLMs).  
2. The critique suggested that the topic selection confounded original results and recommended using only the contextual term for a more valid and interpretable measure.  
3. The authors empirically validate this proposal by testing it on two essay datasets, ASAP++ and ELLIPSE, both containing human-annotated trait scores related to discourse-level traits such as Organization and Cohesion.  
4. Results show that the contextual-only measure of sequentiality aligns more closely with human assessments compared to the original and topic-only versions.  
5. Although zero-shot prompted LLMs predict trait scores more accurately on their own, combining the contextual sequentiality measure with standard linguistic features yields higher predictive value than the original and topic-only metrics, even outperforming zero-shot LLM predictions alone.  
6. The findings underscore the importance of explicitly modeling sentence-to-sentence flow and support the contextual measure as a validated, interpretable, and complementary feature for automated essay scoring and other NLP applications. <div>
arXiv:2511.09185v1 Announce Type: new 
Abstract: Recent work has proposed using Large Language Models (LLMs) to quantify narrative flow through a measure called sequentiality, which combines topic and contextual terms. A recent critique argued that the original results were confounded by how topics were selected for the topic-based component, and noted that the metric had not been validated against ground-truth measures of flow. That work proposed using only the contextual term as a more conceptually valid and interpretable alternative. In this paper, we empirically validate that proposal. Using two essay datasets with human-annotated trait scores, ASAP++ and ELLIPSE, we show that the contextual version of sequentiality aligns more closely with human assessments of discourse-level traits such as Organization and Cohesion. While zero-shot prompted LLMs predict trait scores more accurately than the contextual measure alone, the contextual measure adds more predictive value than both the topic-only and original sequentiality formulations when combined with standard linguistic features. Notably, this combination also outperforms the zero-shot LLM predictions, highlighting the value of explicitly modeling sentence-to-sentence flow. Our findings support the use of context-based sequentiality as a validated, interpretable, and complementary feature for automated essay scoring and related NLP tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Learning Dynamics of Subword Segmentation for Morphologically Diverse Languages</title>
<link>https://arxiv.org/abs/2511.09197</link>
<guid>https://arxiv.org/abs/2511.09197</guid>
<content:encoded><![CDATA[
<div> Keywords: subword segmentation, language model, pretraining, finetuning, morphology <br />
<br />
Summary: 
The study explores the learning dynamics of subword segmentation in language models, focusing on how subwords evolve during pretraining and finetuning. The subword segmental language model (SSLM) framework is extended to support pretraining and finetuning for typologically diverse languages: Isi-Xhosa, Setswana, and English. Four stages of subword learning are identified, with isi-Xhosa showing greater instability due to its morphological complexity. During finetuning, subword boundaries become finer-grained. The research highlights the potential of learnable subwords in improving text generation and cross-lingual transfer, particularly for low-resource, morphologically complex languages. <div>
arXiv:2511.09197v1 Announce Type: new 
Abstract: Subword segmentation is typically applied in preprocessing and stays fixed during training. Alternatively, it can be learned during training to optimise the training objective. In this paper we study the learning dynamics of subword segmentation: if a language model can dynamically optimise tokenisation, how do its subwords evolve during pretraining and finetuning? To explore this, we extend the subword segmental language model (SSLM), a framework for learning subwords during training, to support pretraining and finetuning. We train models for three typologically diverse languages to study learning dynamics across the morphological spectrum: Isi-Xhosa is conjunctive (long word forms composed of many morphemes), Setswana is disjunctive (morphemes written as separate words), and English represents a typological middle ground. We analyse subword dynamics from a linguistic perspective, tracking morphology, productivity, and fertility. We identify four stages of subword learning, with the morphologically complex isi-Xhosa exhibiting greater instability. During finetuning, subword boundaries shift to become finer-grained. Lastly, we show that learnable subwords offers a promising approach to improve text generation and cross-lingual transfer for low-resource, morphologically complex languages.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining Finnish ModernBERTs</title>
<link>https://arxiv.org/abs/2511.09213</link>
<guid>https://arxiv.org/abs/2511.09213</guid>
<content:encoded><![CDATA[
<div> Multilingualism, ModernBERT, pretrained models, long context, Finnish languages<br /><br />Summary: This paper focuses on pretraining ModernBERT encoder models across six different sizes, ranging from 51 million to 475 million parameters, specifically targeting limited multilingualism with an emphasis on languages relevant to Finland. The study shows that these models are competitive with, or even outperform, existing multilingual models on various natural language processing tasks. Notably, they demonstrate superior performance compared to monolingual models on tasks requiring handling contexts longer than 512 tokens, which is a critical capability for understanding extended text. The research also investigates the impact of different types of data incorporated during the final training phase, providing valuable empirical insights into effective training strategies. Finally, the authors have made both the code and the pretrained models publicly available, facilitating further research and application development in this domain. <div>
arXiv:2511.09213v1 Announce Type: new 
Abstract: This paper reports on pretraining ModernBERT encoder models in six different sizes, ranging from 51M to 475M parameters, with a focus on limited multilingualism, emphasizing languages relevant to Finland. Our models are competitive with, or superior to, existing multilingual models. They outperform monolingual models on tasks that require a context longer than 512 tokens. We present empirical results on using different data in the final stage of training. The code and models are publicly released.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stabilizing Reinforcement Learning for Honesty Alignment in Language Models on Deductive Reasoning</title>
<link>https://arxiv.org/abs/2511.09222</link>
<guid>https://arxiv.org/abs/2511.09222</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, verifiable rewards, honesty alignment, deductive reasoning, Anchor <br />
<br />
Summary: 
The paper introduces the concept of reinforcement learning with verifiable rewards (RLVR) for aligning language models with complex reasoning objectives. It focuses on honesty alignment, where models must not only answer queries but also recognize when conclusions cannot be drawn from given premises. To test honesty alignment, two deductive reasoning datasets are curated, one for linear algebra and one for logical inference. The study finds that current methods struggle with these tasks and proposes a new reinforcement learning method called Anchor. Anchor injects ground truth trajectories into rollouts, preventing early training collapse and improving reasoning performance. The results highlight the importance of training dynamics for reliable deductive reasoning in aligned language models. <br /><br /> <div>
arXiv:2511.09222v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as a promising framework for aligning language models with complex reasoning objectives. However, most existing methods optimize only for final task outcomes, leaving models vulnerable to collapse when negative rewards dominate early training. This challenge is especially pronounced in honesty alignment, where models must not only solve answerable queries but also identify when conclusions cannot be drawn from the given premises. Deductive reasoning provides an ideal testbed because it isolates reasoning capability from reliance on external factual knowledge. To investigate honesty alignment, we curate two multi-step deductive reasoning datasets from graph structures, one for linear algebra and one for logical inference, and introduce unanswerable cases by randomly perturbing an edge in half of the instances. We find that GRPO, with or without supervised fine tuning initialization, struggles on these tasks. Through extensive experiments across three models, we evaluate stabilization strategies and show that curriculum learning provides some benefit but requires carefully designed in distribution datasets with controllable difficulty. To address these limitations, we propose Anchor, a reinforcement learning method that injects ground truth trajectories into rollouts, preventing early training collapse. Our results demonstrate that this method stabilizes learning and significantly improves the overall reasoning performance, underscoring the importance of training dynamics for enabling reliable deductive reasoning in aligned language models.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POTSA: A Cross-Lingual Speech Alignment Framework for Low Resource Speech-to-Text Translation</title>
<link>https://arxiv.org/abs/2511.09232</link>
<guid>https://arxiv.org/abs/2511.09232</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech Large Language Models, Optimal Transport, cross-lingual alignment, speech-to-text translation, zero-shot languages<br /><br />Summary: This paper addresses the challenge of biased translation performance in multilingual speech-to-text translation (S2TT) caused by neglecting semantic commonalities across source languages. The authors propose a novel framework named POTSA (Parallel Optimal Transport for Speech Alignment) that leverages cross-lingual parallel speech pairs combined with Optimal Transport (OT) techniques to enhance alignment between high- and low-resource languages. The approach includes a Bias Compensation module designed to coarsely align initial speech representations among different languages, reducing initial disparities. Subsequently, token-level OT constraints are applied to a Q-Former module using parallel speech pairs to establish detailed consistency of speech representations at the token level. Additionally, the framework incorporates a layer scheduling strategy that applies OT constraints selectively to the layers most beneficial for capturing semantic information. Experiments conducted on the FLEURS dataset demonstrate that POTSA achieves state-of-the-art (SOTA) results, showing an average improvement of +0.93 BLEU across five common languages. Remarkably, it also yields a substantial +5.05 BLEU increase in zero-shot language settings, all while requiring only 10 hours of parallel speech data per source language. This work presents an effective solution to bridging translation quality gaps for low-resource and unseen languages. <div>
arXiv:2511.09232v1 Announce Type: new 
Abstract: Speech Large Language Models (SpeechLLMs) have achieved breakthroughs in multilingual speech-to-text translation (S2TT). However, existing approaches often overlook semantic commonalities across source languages, leading to biased translation performance. In this work, we propose \textbf{POTSA} (Parallel Optimal Transport for Speech Alignment), a new framework based on cross-lingual parallel speech pairs and Optimal Transport (OT), designed to bridge high- and low-resource translation gaps. First, we introduce a Bias Compensation module to coarsely align initial speech representations across languages. Second, we impose token-level OT constraints on a Q-Former using parallel speech pairs to establish fine-grained consistency of representations. Then, we apply a layer scheduling strategy to focus OT constraints on the most semantically beneficial layers. Experiments on the FLEURS dataset show that our method achieves SOTA performance, with +0.93 BLEU on average over five common languages and +5.05 BLEU on zero-shot languages, using only 10 hours of parallel speech per source language.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C$^3$TG: Conflict-aware, Composite, and Collaborative Controlled Text Generation</title>
<link>https://arxiv.org/abs/2511.09292</link>
<guid>https://arxiv.org/abs/2511.09292</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-attribute control, text generation, conflict resolution, iterative optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of controlling multiple specific attributes in generated text by large language models (LLMs) without needing architectural changes or extensive fine-tuning.<br />2. Existing methods primarily allow control over single, basic attributes and struggle when multiple, potentially conflicting attributes are required, lacking coordination and iterative refinement mechanisms.<br />3. The authors propose C³TG (Conflict-aware, Composite, and Collaborative Controlled Text Generation), a novel two-phase framework designed for fine-grained control over multiple text attributes simultaneously.<br />4. During the generation phase, C³TG dynamically pairs the LLM with relevant attribute classifiers (covering 17 dimensions) and applies weighted KL-divergence to adjust token probabilities, guiding generation toward desired attribute profiles.<br />5. In the optimization phase, an energy function combining classifier feedback and penalty terms iteratively resolves conflicts among attributes to refine outputs, ensuring natural text fluency is preserved.<br />6. Experimental results demonstrate that C³TG outperforms baseline approaches in attribute accuracy, linguistic quality, diversity of generated text, and also reduces toxicity.<br />7. The framework achieves precise multi-dimensional attribute control without requiring costly model modifications, making it an effective and flexible solution for controlled text generation tasks. <div>
arXiv:2511.09292v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have demonstrated remarkable text generation capabilities. However, controlling specific attributes of generated text remains challenging without architectural modifications or extensive fine-tuning. Current methods typically toggle a single, basic attribute but struggle with precise multi-attribute control. In scenarios where attribute requirements conflict, existing methods lack coordination mechanisms, causing interference between desired attributes. Furthermore, these methods fail to incorporate iterative optimization processes in the controlled generation pipeline. To address these limitations, we propose Conflict-aware, Composite, and Collaborative Controlled Text Generation (C$^3$TG), a two-phase framework for fine-grained, multi-dimensional text attribute control. During generation, C$^3$TG selectively pairs the LLM with the required attribute classifiers from the 17 available dimensions and employs weighted KL-divergence to adjust token probabilities. The optimization phase then leverages an energy function combining classifier scores and penalty terms to resolve attribute conflicts through iterative feedback, enabling precise control over multiple dimensions simultaneously while preserving natural text flow. Experiments show that C$^3$TG significantly outperforms baselines across multiple metrics including attribute accuracy, linguistic fluency, and output diversity, while simultaneously reducing toxicity. These results establish C$^3$TG as an effective and flexible solution for multi-dimensional text attribute control that requires no costly model modifications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteraryTaste: A Preference Dataset for Creative Writing Personalization</title>
<link>https://arxiv.org/abs/2511.09310</link>
<guid>https://arxiv.org/abs/2511.09310</guid>
<content:encoded><![CDATA[
<div> Creative writing, personalization, large language models, reading preferences, dataset<br /><br />Summary:<br /><br />This paper introduces LiteraryTaste, a novel dataset designed to support the development of personalized creative writing language models by capturing individual reading preferences. The dataset involves 60 participants who provided two types of preference data: 1) self-reported reading habits and tastes, termed stated preferences, and 2) annotations of their preferences over 100 pairs of short creative writing texts, referred to as revealed preferences. Key findings include the observation that people's creative writing preferences significantly diverge, highlighting the importance of personalization. The authors demonstrate that fine-tuning a transformer encoder on this data achieves 75.8% accuracy in predicting personal preferences and 67.7% accuracy for collective preferences, suggesting effective modeling of individual and group tastes. However, stated preferences showed limited predictive power for revealed preferences, indicating that what people say about their tastes might not align well with their actual choices. Additionally, the study uses an LLM-driven interpretability pipeline to analyze variations in preferences, providing insights into how creativity is perceived differently across users. This work establishes a foundation for further research toward tailoring creative writing technologies to better match individual user tastes and improve user satisfaction. <div>
arXiv:2511.09310v1 Announce Type: new 
Abstract: People have different creative writing preferences, and large language models (LLMs) for these tasks can benefit from adapting to each user's preferences. However, these models are often trained over a dataset that considers varying personal tastes as a monolith. To facilitate developing personalized creative writing LLMs, we introduce LiteraryTaste, a dataset of reading preferences from 60 people, where each person: 1) self-reported their reading habits and tastes (stated preference), and 2) annotated their preferences over 100 pairs of short creative writing texts (revealed preference). With our dataset, we found that: 1) people diverge on creative writing preferences, 2) finetuning a transformer encoder could achieve 75.8% and 67.7% accuracy when modeling personal and collective revealed preferences, and 3) stated preferences had limited utility in modeling revealed preferences. With an LLM-driven interpretability pipeline, we analyzed how people's preferences vary. We hope our work serves as a cornerstone for personalizing creative writing technologies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Explainable Khmer Polarity Classification</title>
<link>https://arxiv.org/abs/2511.09313</link>
<guid>https://arxiv.org/abs/2511.09313</guid>
<content:encoded><![CDATA[
<div> Khmer polarity classification, explainable AI, Qwen-3 model, polarity dataset, self-explanations<br /><br />Summary: This paper addresses the task of Khmer polarity classification, which involves categorizing Khmer text into positive, negative, or neutral sentiments. Unlike prior models that perform classification without explanation, the authors propose an explainable approach by fine-tuning the instruction-based reasoning Qwen-3 model. The explainability emphasized in this work is based on self-explanations, where the model provides rationale by highlighting polarity-related keywords or phrases that justify its predictions. Experimental results demonstrate that the fine-tuned Qwen-3 model not only achieves accurate polarity classification but also offers interpretable reasoning for its decisions. Additionally, the authors contribute a new dataset tailored for this task, containing short to medium-length casual, romanized, and mixed-code Khmer expressions. This dataset was constructed using a combination of heuristic rules and human curation to ensure quality and diversity. Both the dataset and the fine-tuned Qwen-3 models are made publicly accessible via a gated Hugging Face repository (rinabuoy/khmerpolarity_nonreasoning). This work advances Khmer language processing by combining effective sentiment analysis with explainability support. <div>
arXiv:2511.09313v1 Announce Type: new 
Abstract: Khmer polarity classification is a fundamental natural language processing task that assigns a positive, negative, or neutral label to a given Khmer text input. Existing Khmer models typically predict the label without explaining the rationale behind the prediction. This paper proposes an explainable Khmer polarity classifier by fine-tuning an instruction-based reasoning Qwen-3 model. The notion of explainability in this paper is limited to self-explanations, which the model uses to rationalize its predictions. Experimental results show that the fine-tuned model not only predicts labels accurately but also provides reasoning by identifying polarity-related keywords or phrases to support its predictions. In addition, we contribute a new Khmer polarity dataset consisting of short- to medium-length casual, romanized, and mixed-code Khmer expressions. This dataset was constructed using both heuristic rules and human curation and is publicly available through a gated Hugging Face repository (rinabuoy/khmerpolarity_nonreasoning). The fine-tuned Qwen-3 models are also made available in the same Hugging Face account.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness</title>
<link>https://arxiv.org/abs/2511.09316</link>
<guid>https://arxiv.org/abs/2511.09316</guid>
<content:encoded><![CDATA[
<div> certified robustness, sequence classification, edit distance perturbations, AdaptDel methods, randomized smoothing 

Summary:
The article introduces AdaptDel methods for certified robustness in sequence classification against edit distance perturbations, particularly for natural language processing tasks. Current methods face challenges with inputs of varying lengths, leading to suboptimal performance. AdaptDel methods use adaptable deletion rates that adjust dynamically based on input properties. The theoretical framework of randomized smoothing is extended to variable-rate deletion to ensure sound certification with respect to edit distance. Empirical results in natural language tasks show significant improvement, with up to 30 orders of magnitude enhancement in the median cardinality of the certified region compared to state-of-the-art certifications. <div>
arXiv:2511.09316v1 Announce Type: new 
Abstract: We consider the problem of certified robustness for sequence classification against edit distance perturbations. Naturally occurring inputs of varying lengths (e.g., sentences in natural language processing tasks) present a challenge to current methods that employ fixed-rate deletion mechanisms and lead to suboptimal performance. To this end, we introduce AdaptDel methods with adaptable deletion rates that dynamically adjust based on input properties. We extend the theoretical framework of randomized smoothing to variable-rate deletion, ensuring sound certification with respect to edit distance. We achieve strong empirical results in natural language tasks, observing up to 30 orders of magnitude improvement to median cardinality of the certified region, over state-of-the-art certifications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmJEE-Eval: A Bilingual Multimodal Benchmark for Evaluating Scientific Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.09339</link>
<guid>https://arxiv.org/abs/2511.09339</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, scientific reasoning, mmJEE-Eval, JEE Advanced, multimodal benchmark<br /><br />Summary:<br /><br />1. This paper introduces mmJEE-Eval, a new challenging multimodal bilingual benchmark in English and Hindi, consisting of 1,460 questions from India's JEE Advanced exam (2019-2025) covering Physics, Chemistry, and Mathematics. <br />2. The benchmark aims to better differentiate genuine scientific reasoning capabilities from mere pattern-matching in contemporary vision-language models (VLMs), addressing limitations in existing multimodal reasoning benchmarks.<br />3. Evaluation of 17 state-of-the-art VLMs shows that top closed-source models such as GPT-5 and Gemini 2.5 Pro/Flash achieve high accuracy (77-84%) on the latest 2025 questions, while large open-source models plateau at 37-45% accuracy despite scaling to 400 billion parameters.<br />4. Closed models demonstrate excellent problem-solving performance, with up to 100% pass@3 scores, but their reasoning ability drastically declines under meta-cognitive stress tests; for instance, GPT-5 only corrects 5.2% of its errors.<br />5. Ablation studies confirm that mmJEE-Eval's difficulty arises from question complexity and reasoning depth, not from data memorization, making it an effective benchmark for distinguishing advanced reasoning and training methods versus weaker alternatives.<br />6. The authors publicly release the benchmark's code and dataset at https://mmjee-eval.github.io for broader research use. <div>
arXiv:2511.09339v1 Announce Type: new 
Abstract: Contemporary vision-language models (VLMs) perform well on existing multimodal reasoning benchmarks (78-85\% accuracy on MMMU, MathVista). Yet, these results fail to sufficiently distinguish true scientific reasoning articulation capabilities from pattern-matching. To address this gap, we introduce \textbf{mmJEE-Eval}, a multimodal bilingual (English and Hindi) benchmark comprising 1,460 questions from India's JEE Advanced examination (2019-2025) spanning pre-college Physics, Chemistry, and Mathematics domains. Our evaluation of 17 state-of-the-art models reveals that while frontier VLMs (GPT-5, Gemini 2.5 Pro/Flash) achieve 77-84\% accuracy on held-out 2025 questions, open-source models plateau at 37-45\% despite scaling to 400B parameters, a significant difference not observed on existing benchmarks. While closed frontiers from Google and OpenAI show high problem-solving accuracies (up to 100\% pass@3 scores), they fully collapse when the reasoning load is increased meta-cognitively (GPT-5 fixes just 5.2\% errors). Systematic ablations show mmJEE-Eval's difficulty stems from complexity and reasoning depth rather than memorization. Effectively, our benchmark segregates superior training and reasoning methodologies where alternatives fail. We publicly release our code and data: https://mmjee-eval.github.io
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seer Self-Consistency: Advance Budget Estimation for Adaptive Test-Time Scaling</title>
<link>https://arxiv.org/abs/2511.09345</link>
<guid>https://arxiv.org/abs/2511.09345</guid>
<content:encoded><![CDATA[
<div> dynamic self-consistency, Large Language Models, token efficiency, latency, System 1 reasoning

Summary:
The paper introduces SeerSC, a dynamic self-consistency framework that enhances token efficiency and reduces latency in Large Language Models. By integrating System 1 and System 2 reasoning, the framework uses System 1 for quick computation of answer entropy, which is then used to assess the suitability of samples for scaling under System 2. This approach enables dynamic self-consistency while benefiting from System 1's accurate estimations. The method surpasses existing techniques by achieving a remarkable 47% reduction in token consumption and a 43% decrease in inference latency without compromising performance. By leveraging both rapid and accurate reasoning systems, SeerSC improves overall inference performance and efficiency of LLMs. <br /><br />Summary: <div>
arXiv:2511.09345v1 Announce Type: new 
Abstract: Test-time scaling improves the inference performance of Large Language Models (LLMs) but also incurs substantial computational costs. Although recent studies have reduced token consumption through dynamic self-consistency, they remain constrained by the high latency of sequential requests. In this paper, we propose SeerSC, a dynamic self-consistency framework that simultaneously improves token efficiency and latency by integrating System 1 and System 2 reasoning. Specifically, we utilize the rapid System 1 to compute the answer entropy for given queries. This score is then used to evaluate the potential of samples for scaling, enabling dynamic self-consistency under System 2. Benefiting from the advance and accurate estimation provided by System 1, the proposed method can reduce token usage while simultaneously achieving a significant decrease in latency through parallel generation. It outperforms existing methods, achieving up to a 47% reduction in token consumption and a 43% reduction in inference latency without significant performance loss.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spider4SSC &amp; S2CLite: A text-to-multi-query-language dataset using lightweight ontology-agnostic SPARQL to Cypher parser</title>
<link>https://arxiv.org/abs/2511.09354</link>
<guid>https://arxiv.org/abs/2511.09354</guid>
<content:encoded><![CDATA[
<div> Dataset, S2CLite, SPARQL, Cypher, parsing<br />
Summary:<br />
The article introduces the Spider4SSC dataset and S2CLite parsing tool. S2CLite is a rule-based parser that translates SPARQL queries into Cypher queries without relying on an RDF graph or external tools. Experimental results demonstrate that S2CLite significantly improves query parsing accuracy compared to existing solutions, achieving a total parsing accuracy of 77.8% on Spider4SPARQL. S2CLite also outperforms the state-of-the-art S2CTrans in terms of execution accuracy on a common subset of queries. The tool is open-sourced on GitHub for further development. Additionally, S2CLite is used to generate the Spider4SSC dataset, which includes 4525 unique questions and 3 equivalent sets of 2581 matching queries in SQL, SPARQL, and Cypher. The clean Spider4SSC dataset is available for download. <br /><br />Summary: <div>
arXiv:2511.09354v1 Announce Type: new 
Abstract: We present Spider4SSC dataset and S2CLite parsing tool. S2CLite is a lightweight, ontology-agnostic parser that translates SPARQL queries into Cypher queries, enabling both in-situ and large-scale SPARQL to Cypher translation. Unlike existing solutions, S2CLite is purely rule-based (inspired by traditional programming language compilers) and operates without requiring an RDF graph or external tools. Experiments conducted on the BSBM42 and Spider4SPARQL datasets show that S2CLite significantly reduces query parsing errors, achieving a total parsing accuracy of 77.8% on Spider4SPARQL compared to 44.2% by the state-of-the-art S2CTrans. Furthermore, S2CLite achieved a 96.6\% execution accuracy on the intersecting subset of queries parsed by both parsers, outperforming S2CTrans by 7.3%. We further use S2CLite to parse Spider4SPARQL queries to Cypher and generate Spider4SSC, a unified Text-to-Query language (SQL, SPARQL, Cypher) dataset with 4525 unique questions and 3 equivalent sets of 2581 matching queries (SQL, SPARQL and Cypher). We open-source S2CLite for further development on GitHub (github.com/vejvarm/S2CLite) and provide the clean Spider4SSC dataset for download.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTQ-Eval: Multilingual Text Quality Evaluation for Language Models</title>
<link>https://arxiv.org/abs/2511.09374</link>
<guid>https://arxiv.org/abs/2511.09374</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual text evaluation, large language models, MTQ-Eval, text quality assessment, preference learning<br /><br />Summary:<br /><br />This paper addresses the challenge of evaluating text quality in multilingual settings using large language models (LLMs). It introduces MTQ-Eval, a new evaluation framework designed to assess text quality across 115 languages by learning from examples of both high- and low-quality text. To create MTQ-Eval, the authors automatically generate data reflecting quality preferences, which is then used to fine-tune open-source base LLMs so they align better with human-like judgments of text quality. The study presents a comprehensive evaluation demonstrating MTQ-Eval’s superior performance over existing approaches in multiple languages. Additionally, the authors find that the improved evaluation capability of MTQ-Eval not only enhances quality assessment tasks but also contributes to better results in related downstream tasks. This suggests that multilingual text quality evaluation using aligned LLMs is both feasible and beneficial. The work highlights the potential for scalable, generalizable text quality evaluation beyond specific task-focused metrics, offering robust multilingual support. Overall, MTQ-Eval represents an important step toward more accurate and universal text quality assessment using LLMs trained on preference data. <div>
arXiv:2511.09374v1 Announce Type: new 
Abstract: The use of large language models (LLMs) for evaluating outputs is becoming an increasingly effective and scalable approach. However, it remains uncertain whether this capability extends beyond task-specific evaluations to more general assessments of text quality, particularly in multilingual contexts. In this study, we introduce, MTQ-Eval, a novel framework for multilingual text quality evaluation that learns from examples of both high- and low-quality texts, adjusting its internal representations. To develop MTQ-Eval, we first automatically generate text quality preference data and then use it to train open-source base LLMs to align with ratings of high- and low-quality text. Our comprehensive evaluation across 115 languages demonstrates the improved performance of the proposed model. Upon further analysis, we find that this enhanced evaluation capability also leads to notable improvements in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Correcting Large Language Models: Generation vs. Multiple Choice</title>
<link>https://arxiv.org/abs/2511.09381</link>
<guid>https://arxiv.org/abs/2511.09381</guid>
<content:encoded><![CDATA[
<div> self-correction mechanism, language models, open-ended generation, multiple-choice selection, decision-oriented applications
Summary: 
This paper investigates the self-correction mechanisms in large language models (LLMs) for open-ended text generation and multiple-choice selection tasks. It compares performance trends and error-correction behaviors across various natural language understanding and reasoning tasks using different scales and families of language models. The study reveals that open-ended generation benefits from re-interpretation and compositional refinement, while multiple-choice selection relies on clearer solution boundaries but may be limited by provided options. The findings emphasize the importance of considering task structure and output space interaction in designing self-correction mechanisms for knowledge-intensive reasoning and decision-oriented LLM applications.<br /><br /> <div>
arXiv:2511.09381v1 Announce Type: new 
Abstract: Large language models have recently demonstrated remarkable abilities to self-correct their responses through iterative refinement, often referred to as self-consistency or self-reflection. However, the dynamics of this self-correction mechanism may differ substantially depending on whether the model is tasked with open-ended text generation or with selecting the most appropriate response from multiple predefined options. In this paper, we conduct a systematic investigation of these two paradigms by comparing performance trends and error-correction behaviors across various natural language understanding and reasoning tasks, covering language models of different scales and families. Our experimental results reveal distinct patterns of improvement and failure modes:
  \textit{While open-ended generation often benefits from the flexibility of re-interpretation and compositional refinement, multiple-choice selection can leverage clearer solution boundaries but may be limited by the provided options}. This contrast also reflects the dual demands faced by emerging agentic LLM applications: effective agents must not only generate and refine open-ended plans or explanations, but also make reliable discrete choices when operating within constrained action spaces. Our findings, therefore, highlight that the design of self-correction mechanisms should take into account the interaction between task structure and output space, with implications for both knowledge-intensive reasoning and decision-oriented applications of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMaPO: Adaptive Margin-attached Preference Optimization for Language Model Alignment</title>
<link>https://arxiv.org/abs/2511.09385</link>
<guid>https://arxiv.org/abs/2511.09385</guid>
<content:encoded><![CDATA[
<div> offline preference optimization, language models, overfitting, underfitting, adaptive margin<br />
<br />
Summary:<br />
The article introduces Adaptive Margin-attached Preference Optimization (AMaPO) as a solution to the Overfitting-Underfitting Dilemma in aligning language models using offline preference optimization. The current margin designs lead to excessive gradients for correctly ranked samples (overfitting) and insufficient signals for misranked ones (underfitting). AMaPO addresses this issue by utilizing an adaptive margin that reallocates learning effort by amplifying gradients for misranked samples and suppressing them for correctly ranked ones. Experimental results show that AMaPO improves ranking accuracy and downstream alignment performance compared to existing methods. The algorithm successfully mitigates overfitting and underfitting issues, providing a more effective and stable approach for aligning language models. <div>
arXiv:2511.09385v1 Announce Type: new 
Abstract: Offline preference optimization offers a simpler and more stable alternative to RLHF for aligning language models. However, their effectiveness is critically dependent on ranking accuracy, a metric where further gains are highly impactful. This limitation arises from a fundamental problem that we identify and formalize as the Overfitting-Underfitting Dilemma: current margin designs cause models to apply excessive, wasteful gradients to correctly ranked samples (overfitting) while providing insufficient corrective signals for misranked ones (underfitting). To resolve this dilemma, we propose Adaptive Margin-attached Preference Optimization (AMaPO), a simple yet principled algorithm. AMaPO employs an instance-wise adaptive margin, refined by Z-normalization and exponential scaling, which dynamically reallocates learning effort by amplifying gradients for misranked samples and suppressing them for correct ones. Extensive experiments on widely used benchmarks demonstrate that AMaPO not only achieves better ranking accuracy and superior downstream alignment performance, but targeted analysis also confirms that it successfully mitigates the core overfitting and underfitting issues.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque</title>
<link>https://arxiv.org/abs/2511.09396</link>
<guid>https://arxiv.org/abs/2511.09396</guid>
<content:encoded><![CDATA[
arXiv:2511.09396v1 Announce Type: new 
Abstract: Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARE-Bench: A Benchmark of Diverse Client Simulations Guided by Expert Principles for Evaluating LLMs in Psychological Counseling</title>
<link>https://arxiv.org/abs/2511.09407</link>
<guid>https://arxiv.org/abs/2511.09407</guid>
<content:encoded><![CDATA[
arXiv:2511.09407v1 Announce Type: new 
Abstract: The mismatch between the growing demand for psychological counseling and the limited availability of services has motivated research into the application of Large Language Models (LLMs) in this domain. Consequently, there is a need for a robust and unified benchmark to assess the counseling competence of various LLMs. Existing works, however, are limited by unprofessional client simulation, static question-and-answer evaluation formats, and unidimensional metrics. These limitations hinder their effectiveness in assessing a model's comprehensive ability to handle diverse and complex clients. To address this gap, we introduce \textbf{CARE-Bench}, a dynamic and interactive automated benchmark. It is built upon diverse client profiles derived from real-world counseling cases and simulated according to expert guidelines. CARE-Bench provides a multidimensional performance evaluation grounded in established psychological scales. Using CARE-Bench, we evaluate several general-purpose LLMs and specialized counseling models, revealing their current limitations. In collaboration with psychologists, we conduct a detailed analysis of the reasons for LLMs' failures when interacting with clients of different types, which provides directions for developing more comprehensive, universal, and effective counseling models.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSAP-ERE: Fine-Grained Scholarly Entity and Relation Extraction Focused on Machine Learning</title>
<link>https://arxiv.org/abs/2511.09411</link>
<guid>https://arxiv.org/abs/2511.09411</guid>
<content:encoded><![CDATA[
arXiv:2511.09411v1 Announce Type: new 
Abstract: Research in Machine Learning (ML) and AI evolves rapidly. Information Extraction (IE) from scientific publications enables to identify information about research concepts and resources on a large scale and therefore is a pathway to improve understanding and reproducibility of ML-related research. To extract and connect fine-grained information in ML-related research, e.g. method training and data usage, we introduce GSAP-ERE. It is a manually curated fine-grained dataset with 10 entity types and 18 semantically categorized relation types, containing mentions of 63K entities and 35K relations from the full text of 100 ML publications. We show that our dataset enables fine-tuned models to automatically extract information relevant for downstream tasks ranging from knowledge graph (KG) construction, to monitoring the computational reproducibility of AI research at scale. Additionally, we use our dataset as a test suite to explore prompting strategies for IE using Large Language Models (LLM). We observe that the performance of state-of-the-art LLM prompting methods is largely outperformed by our best fine-tuned baseline model (NER: 80.6%, RE: 54.0% for the fine-tuned model vs. NER: 44.4%, RE: 10.1% for the LLM). This disparity of performance between supervised models and unsupervised usage of LLMs suggests datasets like GSAP-ERE are needed to advance research in the domain of scholarly information extraction.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIG5-TPoT: Predicting BIG Five Personality Traits, Facets, and Items Through Targeted Preselection of Texts</title>
<link>https://arxiv.org/abs/2511.09426</link>
<guid>https://arxiv.org/abs/2511.09426</guid>
<content:encoded><![CDATA[
arXiv:2511.09426v1 Announce Type: new 
Abstract: Predicting an individual's personalities from their generated texts is a challenging task, especially when the text volume is large. In this paper, we introduce a straightforward yet effective novel strategy called targeted preselection of texts (TPoT). This method semantically filters the texts as input to a deep learning model, specifically designed to predict a Big Five personality trait, facet, or item, referred to as the BIG5-TPoT model. By selecting texts that are semantically relevant to a particular trait, facet, or item, this strategy not only addresses the issue of input text limits in large language models but also improves the Mean Absolute Error and accuracy metrics in predictions for the Stream of Consciousness Essays dataset.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Readability Measures and Automatic Text Simplification: In the Search of a Construct</title>
<link>https://arxiv.org/abs/2511.09536</link>
<guid>https://arxiv.org/abs/2511.09536</guid>
<content:encoded><![CDATA[
arXiv:2511.09536v1 Announce Type: new 
Abstract: Readability is a key concept in the current era of abundant written information. To help making texts more readable and make information more accessible to everyone, a line of researched aims at making texts accessible for their target audience: automatic text simplification (ATS). Lately, there have been studies on the correlations between automatic evaluation metrics in ATS and human judgment. However, the correlations between those two aspects and commonly available readability measures (such as readability formulas or linguistic features) have not been the focus of as much attention. In this work, we investigate the place of readability measures in ATS by complementing the existing studies on evaluation metrics and human judgment, on English. We first discuss the relationship between ATS and research in readability, then we report a study on correlations between readability measures and human judgment, and between readability measures and ATS evaluation metrics. We identify that in general, readability measures do not correlate well with automatic metrics and human judgment. We argue that as the three different angles from which simplification can be assessed tend to exhibit rather low correlations with one another, there is a need for a clear definition of the construct in ATS.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynClaimEval: A Framework for Evaluating the Utility of Synthetic Data in Long-Context Claim Verification</title>
<link>https://arxiv.org/abs/2511.09539</link>
<guid>https://arxiv.org/abs/2511.09539</guid>
<content:encoded><![CDATA[
arXiv:2511.09539v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with extended context windows promise direct reasoning over long documents, reducing the need for chunking or retrieval. Constructing annotated resources for training and evaluation, however, remains costly. Synthetic data offers a scalable alternative, and we introduce SynClaimEval, a framework for evaluating synthetic data utility in long-context claim verification -- a task central to hallucination detection and fact-checking. Our framework examines three dimensions: (i) input characteristics, by varying context length and testing generalization to out-of-domain benchmarks; (ii) synthesis logic, by controlling claim complexity and error type variation; and (iii) explanation quality, measuring the degree to which model explanations provide evidence consistent with predictions. Experiments across benchmarks show that long-context synthesis can improve verification in base instruction-tuned models, particularly when augmenting existing human-written datasets. Moreover, synthesis enhances explanation quality, even when verification scores do not improve, underscoring its potential to strengthen both performance and explainability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conversational Agents for Building Energy Efficiency -- Advising Housing Cooperatives in Stockholm on Reducing Energy Consumption</title>
<link>https://arxiv.org/abs/2511.08587</link>
<guid>https://arxiv.org/abs/2511.08587</guid>
<content:encoded><![CDATA[
arXiv:2511.08587v1 Announce Type: cross 
Abstract: Housing cooperative is a common type of multifamily building ownership in Sweden. Although this ownership structure grants decision-making autonomy, it places a burden of responsibility on cooperative's board members. Most board members lack the resources or expertise to manage properties and their energy consumption. This ignorance presents a unique challenge, especially given the EU directives that prohibit buildings rated as energy classes F and G by 2033. Conversational agents (CAs) enable human-like interactions with computer systems, facilitating human-computer interaction across various domains. In our case, CAs can be implemented to support cooperative members in making informed energy retrofitting and usage decisions. This paper introduces a Conversational agent system, called SPARA, designed to advise cooperatives on energy efficiency. SPARA functions as an energy efficiency advisor by leveraging the Retrieval-Augmented Generation (RAG) framework with a Language Model(LM). The LM generates targeted recommendations based on a knowledge base composed of email communications between professional energy advisors and cooperatives' representatives in Stockholm. The preliminary results indicate that SPARA can provide energy efficiency advice with precision 80\%, comparable to that of municipal energy efficiency (EE) experts. A pilot implementation is currently underway, where municipal EE experts are evaluating SPARA performance based on questions posed to EE experts by BRF members. Our findings suggest that LMs can significantly improve outreach by supporting stakeholders in their energy transition. For future work, more research is needed to evaluate this technology, particularly limitations to the stability and trustworthiness of its energy efficiency advice.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion</title>
<link>https://arxiv.org/abs/2511.08653</link>
<guid>https://arxiv.org/abs/2511.08653</guid>
<content:encoded><![CDATA[
arXiv:2511.08653v1 Announce Type: cross 
Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-generated podcasts: Synthetic Intimacy and Cultural Translation in NotebookLM's Audio Overviews</title>
<link>https://arxiv.org/abs/2511.08654</link>
<guid>https://arxiv.org/abs/2511.08654</guid>
<content:encoded><![CDATA[
arXiv:2511.08654v1 Announce Type: cross 
Abstract: This paper analyses AI-generated podcasts produced by Google's NotebookLM, which generates audio podcasts with two chatty AI hosts discussing whichever documents a user uploads. While AI-generated podcasts have been discussed as tools, for instance in medical education, they have not yet been analysed as media. By uploading different types of text and analysing the generated outputs I show how the podcasts' structure is built around a fixed template. I also find that NotebookLM not only translates texts from other languages into a perky standardised Mid-Western American accent, it also translates cultural contexts to a white, educated, middle-class American default. This is a distinct development in how publics are shaped by media, marking a departure from the multiple public spheres that scholars have described in human podcasting from the early 2000s until today, where hosts spoke to specific communities and responded to listener comments, to an abstraction of the podcast genre.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benevolent Dictators? On LLM Agent Behavior in Dictator Games</title>
<link>https://arxiv.org/abs/2511.08721</link>
<guid>https://arxiv.org/abs/2511.08721</guid>
<content:encoded><![CDATA[
arXiv:2511.08721v1 Announce Type: cross 
Abstract: In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BayesQ: Uncertainty-Guided Bayesian Quantization</title>
<link>https://arxiv.org/abs/2511.08821</link>
<guid>https://arxiv.org/abs/2511.08821</guid>
<content:encoded><![CDATA[
arXiv:2511.08821v1 Announce Type: cross 
Abstract: We present BayesQ, an uncertainty-guided post-training quantization framework that is the first to optimize quantization under the posterior expected loss. BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion, and allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. For scalar quantizers, posterior-expected MSE yields closed-form tables; task-aware proxies are handled by short Monte Carlo on a small calibration set. An optional calibration-only distillation aligns the quantized model with the posterior predictive teacher. At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) e.g., vs. GPTQ by $+1.5/+0.7/+0.3$ top-1 percentage points on RN50 and $+1.1/+0.4/+0.2$ GLUE points on BERT, while requiring one-time preprocessing comparable to a GPTQ pass. BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Double Contingency Problem: AI Recursion and the Limits of Interspecies Understanding</title>
<link>https://arxiv.org/abs/2511.08927</link>
<guid>https://arxiv.org/abs/2511.08927</guid>
<content:encoded><![CDATA[
arXiv:2511.08927v1 Announce Type: cross 
Abstract: Current bioacoustic AI systems achieve impressive cross-species performance by processing animal communication through transformer architectures, foundation model paradigms, and other computational approaches. However, these approaches overlook a fundamental question: what happens when one form of recursive cognition--AI systems with their attention mechanisms, iterative processing, and feedback loops--encounters the recursive communicative processes of other species? Drawing on philosopher Yuk Hui's work on recursivity and contingency, I argue that AI systems are not neutral pattern detectors but recursive cognitive agents whose own information processing may systematically obscure or distort other species' communicative structures. This creates a double contingency problem: each species' communication emerges through contingent ecological and evolutionary conditions, while AI systems process these signals through their own contingent architectural and training conditions. I propose that addressing this challenge requires reconceptualizing bioacoustic AI from universal pattern recognition toward diplomatic encounter between different forms of recursive cognition, with implications for model design, evaluation frameworks, and research methodologies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransactionGPT</title>
<link>https://arxiv.org/abs/2511.08939</link>
<guid>https://arxiv.org/abs/2511.08939</guid>
<content:encoded><![CDATA[
arXiv:2511.08939v1 Announce Type: cross 
Abstract: We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We introduce a novel 3D-Transformer architecture specifically tailored for capturing the complex dynamics in payment transaction data. This architecture incorporates design innovations that enhance modality fusion and computational efficiency, while seamlessly enabling joint optimization with downstream objectives. Trained on billion-scale real-world transactions, TGPT significantly improves downstream classification performance against a competitive production model and exhibits advantages over baselines in generating future transactions. We conduct extensive empirical evaluations utilizing a diverse collection of company transaction datasets spanning multiple downstream tasks, thereby enabling a thorough assessment of TGPT's effectiveness and efficiency in comparison to established methodologies. Furthermore, we examine the incorporation of LLM-derived embeddings within TGPT and benchmark its performance against fine-tuned LLMs, demonstrating that TGPT achieves superior predictive accuracy as well as faster training and inference. We anticipate that the architectural innovations and practical guidelines from this work will advance foundation models for transaction-like data and catalyze future research in this emerging field.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Mixture of Experts For Large Language Models</title>
<link>https://arxiv.org/abs/2511.08968</link>
<guid>https://arxiv.org/abs/2511.08968</guid>
<content:encoded><![CDATA[
arXiv:2511.08968v1 Announce Type: cross 
Abstract: We present Bayesian Mixture of Experts (Bayesian-MoE), a post-hoc uncertainty estimation framework for fine-tuned large language models (LLMs) based on Mixture-of-Experts architectures. Our method applies a structured Laplace approximation to the second linear layer of each expert, enabling calibrated uncertainty estimation without modifying the original training procedure or introducing new parameters. Unlike prior approaches, which apply Bayesian inference to added adapter modules, Bayesian-MoE directly targets the expert pathways already present in MoE models, leveraging their modular design for tractable block-wise posterior estimation. We use Kronecker-factored low-rank approximations to model curvature and derive scalable estimates of predictive uncertainty and marginal likelihood. Experiments on common-sense reasoning benchmarks with Qwen1.5-MoE and DeepSeek-MoE demonstrate that Bayesian-MoE improves both expected calibration error (ECE) and negative log-likelihood (NLL) over baselines, confirming its effectiveness for reliable downstream decision-making.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Founding Fathers: A Case Study of GIS Search in Multi-Agent Pipelines</title>
<link>https://arxiv.org/abs/2511.09005</link>
<guid>https://arxiv.org/abs/2511.09005</guid>
<content:encoded><![CDATA[
arXiv:2511.09005v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) show exceptional fluency, efforts persist to extract stronger reasoning capabilities from them. Drawing on search-based interpretations of LLM computation, this paper advances a systematic framework for understanding LLM reasoning and optimization. Namely, that enhancing reasoning is best achieved by structuring a multi-agent pipeline to ensure a traversal of the search space in a gradual, incremental, and sequential (GIS) manner. Stated succinctly, high-quality reasoning is a controlled, incremental search. To test this framework, we investigate the efficacy of recursive refinement (RR)--an iterative process of self-criticism, adversarial stress-testing, and integrating critical feedback--as a practical method for implementing GIS search. We designed an experiment comparing a simple, linear pipeline against a complex, explicitly structured pipeline leveraging a recursive refinement layer. The multi-agent models were constructed to reflect the historical personas of three US Founding Fathers (Hamilton, Jefferson, and Madison) using RAG-powered corpora and were prompted to generate responses to three contemporary political issues. Model performance was evaluated using a two-tiered approach: a quantitative score from an LLM arbiter agent and qualitative human judgment. Our results revealed that the complex model consistently outperformed the simple model across all nine test cases with an average arbiter-outputted score of 88.3 versus 71.7. The complex model's arguments were superior in analytical depth, structural nuance, and strategic framing. We conclude that recursive refinement is a robust architectural feature for enhancing LLM reasoning via GIS search.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Million-Step LLM Task with Zero Errors</title>
<link>https://arxiv.org/abs/2511.09030</link>
<guid>https://arxiv.org/abs/2511.09030</guid>
<content:encoded><![CDATA[
arXiv:2511.09030v1 Announce Type: cross 
Abstract: LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</title>
<link>https://arxiv.org/abs/2511.09057</link>
<guid>https://arxiv.org/abs/2511.09057</guid>
<content:encoded><![CDATA[
arXiv:2511.09057v1 Announce Type: cross 
Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Aware Reasoning for GUI Agents</title>
<link>https://arxiv.org/abs/2511.09127</link>
<guid>https://arxiv.org/abs/2511.09127</guid>
<content:encoded><![CDATA[
arXiv:2511.09127v1 Announce Type: cross 
Abstract: Advances in Multimodal Large Language Models have significantly enhanced Graphical User Interface (GUI) automation. Equipping GUI agents with reliable episodic reasoning capabilities is essential for bridging the gap between users' concise task descriptions and the complexities of real-world execution. Current methods integrate Reinforcement Learning (RL) with System-2 Chain-of-Thought, yielding notable gains in reasoning enhancement. For long-horizon GUI tasks, historical interactions connect each screen to the goal-oriented episode chain, and effectively leveraging these clues is crucial for the current decision. However, existing native GUI agents exhibit weak short-term memory in their explicit reasoning, interpreting the chained interactions as discrete screen understanding, i.e., unawareness of the historical interactions within the episode. This history-agnostic reasoning challenges their performance in GUI automation. To alleviate this weakness, we propose a History-Aware Reasoning (HAR) framework, which encourages an agent to reflect on its own errors and acquire episodic reasoning knowledge from them via tailored strategies that enhance short-term memory in long-horizon interaction. The framework mainly comprises constructing a reflective learning scenario, synthesizing tailored correction guidelines, and designing a hybrid RL reward function. Using the HAR framework, we develop a native end-to-end model, HAR-GUI-3B, which alters the inherent reasoning mode from history-agnostic to history-aware, equipping the GUI agent with stable short-term memory and reliable perception of screen details. Comprehensive evaluations across a range of GUI-related benchmarks demonstrate the effectiveness and generalization of our method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming Object Hallucinations with Verified Atomic Confidence Estimation</title>
<link>https://arxiv.org/abs/2511.09228</link>
<guid>https://arxiv.org/abs/2511.09228</guid>
<content:encoded><![CDATA[
arXiv:2511.09228v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-end Contrastive Language-Speech Pretraining Model For Long-form Spoken Question Answering</title>
<link>https://arxiv.org/abs/2511.09282</link>
<guid>https://arxiv.org/abs/2511.09282</guid>
<content:encoded><![CDATA[
arXiv:2511.09282v1 Announce Type: cross 
Abstract: Significant progress has been made in spoken question answering (SQA) in recent years. However, many existing methods, including large audio language models, struggle with processing long audio. Follow the success of retrieval augmented generation, a speech-related retriever shows promising in help preprocessing long-form speech. But the performance of existing speech-related retrievers is lacking. To address this challenge, we propose CLSR, an end-to-end contrastive language-speech retriever that efficiently extracts question-relevant segments from long audio recordings for downstream SQA task. Unlike conventional speech-text contrastive models, CLSR incorporates an intermediate step that converts acoustic features into text-like representations prior to alignment, thereby more effectively bridging the gap between modalities. Experimental results across four cross-modal retrieval datasets demonstrate that CLSR surpasses both end-to-end speech related retrievers and pipeline approaches combining speech recognition with text retrieval, providing a robust foundation for advancing practical long-form SQA applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI</title>
<link>https://arxiv.org/abs/2511.09325</link>
<guid>https://arxiv.org/abs/2511.09325</guid>
<content:encoded><![CDATA[
arXiv:2511.09325v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks</title>
<link>https://arxiv.org/abs/2511.09373</link>
<guid>https://arxiv.org/abs/2511.09373</guid>
<content:encoded><![CDATA[
arXiv:2511.09373v1 Announce Type: cross 
Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting</title>
<link>https://arxiv.org/abs/2511.09478</link>
<guid>https://arxiv.org/abs/2511.09478</guid>
<content:encoded><![CDATA[
arXiv:2511.09478v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NaturalTurn: A Method to Segment Speech into Psychologically Meaningful Conversational Turns</title>
<link>https://arxiv.org/abs/2403.15615</link>
<guid>https://arxiv.org/abs/2403.15615</guid>
<content:encoded><![CDATA[
arXiv:2403.15615v4 Announce Type: replace 
Abstract: Conversation is a subject of increasing interest in the social, cognitive, and computational sciences. Yet as conversational datasets continue to increase in size and complexity, researchers lack scalable methods to segment speech-to-text transcripts into conversational "turns"-the basic building blocks of social interaction. We discuss this challenge and then introduce "NaturalTurn," a turn-segmentation algorithm designed to accurately capture the dynamics of conversational exchange. NaturalTurn operates by distinguishing speakers' primary conversational turns from listeners' secondary utterances, such as backchannels, brief interjections, and other forms of parallel speech that characterize human conversation. Using data from a large conversation corpus, we show that NaturalTurn captures conversational turns more accurately than a baseline model. For example, it produces turns with durations and gaps that match empirical literature, reveals stronger linguistic alignment patterns between speakers, and uncovers otherwise hidden relationships between turn-taking and affective outcomes. NaturalTurn thus represents a pragmatic development in machine-generated transcript-processing methods, or "turn models", that will enable researchers to link turn-taking dynamics with important outcomes of social interaction, a central goal of conversation science.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Deep Unlearning in Large Language Models</title>
<link>https://arxiv.org/abs/2410.15153</link>
<guid>https://arxiv.org/abs/2410.15153</guid>
<content:encoded><![CDATA[
arXiv:2410.15153v4 Announce Type: replace 
Abstract: Machine unlearning has emerged as an important component in developing safe and trustworthy models. Prior work on fact unlearning in LLMs has mostly focused on removing a specified target fact robustly, but often overlooks its deductive connections to other knowledge. We propose a new setting for fact unlearning, deep unlearning, where the goal is not only to remove a target fact but also to prevent it from being deduced via retained knowledge in the LLM and logical reasoning. We propose three novel metrics: Success-DU and Recall to measure unlearning efficacy, and Accuracy to measure the remainder model utility. To benchmark this setting, we leverage both (1) an existing real-world knowledge dataset, MQuAKE, that provides one-step deduction instances, and (2) newly construct a novel semi-synthetic dataset, Eval-DU, that allows multiple steps of realistic deductions among synthetic facts. Experiments reveal that current methods struggle with deep unlearning: they either fail to deeply unlearn, or excessively remove unrelated facts. Our results suggest that targeted algorithms may have to be developed for robust/deep fact unlearning in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model Benchmarks in Medical Tasks</title>
<link>https://arxiv.org/abs/2410.21348</link>
<guid>https://arxiv.org/abs/2410.21348</guid>
<content:encoded><![CDATA[
arXiv:2410.21348v3 Announce Type: replace 
Abstract: With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Evaluating Language Models with Template-based Data Generation</title>
<link>https://arxiv.org/abs/2411.18104</link>
<guid>https://arxiv.org/abs/2411.18104</guid>
<content:encoded><![CDATA[
arXiv:2411.18104v5 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by leveraging GPT-4 to generate meta-templates, ensuring diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. Project Page: https://github.com/iiis-ai/TemplateMath
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenGenAlign: A Preference Dataset and Benchmark for Trustworthy Reward Modeling in Open-Ended, Long-Context Generation</title>
<link>https://arxiv.org/abs/2501.13264</link>
<guid>https://arxiv.org/abs/2501.13264</guid>
<content:encoded><![CDATA[
arXiv:2501.13264v3 Announce Type: replace 
Abstract: Reward Modeling is critical in evaluating and improving the generation of Large Language Models (LLMs). While numerous recent works have shown its feasibility in improving safety, helpfulness, reasoning, and instruction-following ability, its capability and generalization to open-ended long-context generation is still rarely explored. In this paper, we introduce OpenGenAlign, a framework and a high-quality dataset designed to develop reward models to evaluate and improve hallucination-free, comprehensive, reliable, and efficient open-ended long-context generation. We define four key metrics to assess generation quality and develop an automated pipeline to evaluate the outputs of multiple LLMs across long-context QA, Data-to-Text, and Summarization scenarios using o3, ending up with 33K high-quality preference data with a human agreement rate of 81\%. Experimental results first demonstrate that existing reward models perform suboptimally on the held-out benchmark. And Our trained reward model achieves superior performance in the benchmark and effectively improves the generation quality of the policy models using Reinforcement Learning (RL). Additionally, OpenGenAlign could be used for effective guided generation in existing datasets. Furthermore, we demonstrate that the OpenGenAlign could be integrated with reward data from other domains to achieve better performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Linguistics Learned to Stop Worrying and Love the Language Models</title>
<link>https://arxiv.org/abs/2501.17047</link>
<guid>https://arxiv.org/abs/2501.17047</guid>
<content:encoded><![CDATA[
arXiv:2501.17047v3 Announce Type: replace 
Abstract: Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.04387</link>
<guid>https://arxiv.org/abs/2502.04387</guid>
<content:encoded><![CDATA[
arXiv:2502.04387v3 Announce Type: replace 
Abstract: Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment</title>
<link>https://arxiv.org/abs/2502.14389</link>
<guid>https://arxiv.org/abs/2502.14389</guid>
<content:encoded><![CDATA[
arXiv:2502.14389v2 Announce Type: replace 
Abstract: Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction.
  This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs</title>
<link>https://arxiv.org/abs/2502.21239</link>
<guid>https://arxiv.org/abs/2502.21239</guid>
<content:encoded><![CDATA[
arXiv:2502.21239v5 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the Gram matrix determinant of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring internal access to LLMs. We conduct extensive experiments on both external and internal uncertainty detections, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeniorTalk: A Chinese Conversation Dataset with Rich Annotations for Super-Aged Seniors</title>
<link>https://arxiv.org/abs/2503.16578</link>
<guid>https://arxiv.org/abs/2503.16578</guid>
<content:encoded><![CDATA[
arXiv:2503.16578v2 Announce Type: replace 
Abstract: While voice technologies increasingly serve aging populations, current systems exhibit significant performance gaps due to inadequate training data capturing elderly-specific vocal characteristics like presbyphonia and dialectal variations. The limited data available on super-aged individuals in existing elderly speech datasets, coupled with overly simple recording styles and annotation dimensions, exacerbates this issue. To address the critical scarcity of speech data from individuals aged 75 and above, we introduce SeniorTalk, a carefully annotated Chinese spoken dialogue dataset. This dataset contains 55.53 hours of speech from 101 natural conversations involving 202 participants, ensuring a strategic balance across gender, region, and age. Through detailed annotation across multiple dimensions, it can support a wide range of speech tasks. We perform extensive experiments on speaker verification, speaker diarization, speech recognition, and speech editing tasks, offering crucial insights for the development of speech technologies targeting this age group.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARS: Multi-Agent Adaptive Reasoning with Socratic Guidance for Automated Prompt Optimization</title>
<link>https://arxiv.org/abs/2503.16874</link>
<guid>https://arxiv.org/abs/2503.16874</guid>
<content:encoded><![CDATA[
arXiv:2503.16874v2 Announce Type: replace 
Abstract: Large language models (LLMs) typically operate in a question-answering paradigm, where the quality of the input prompt critically affects the response. Automated Prompt Optimization (APO) aims to overcome the cognitive biases of manually crafted prompts and explore a broader prompt design space. However, existing APO methods often suffer from rigid template structures and inefficient exploration in the prompt space. To this end, we propose a Multi-Agent Adaptive Reasoning with Socratic guidance framework (MARS) for APO. MARS consists of five complementary agents and formulates the optimization process as a Partially Observable Markov Decision Process (POMDP), enabling adaptive prompt refinement through explicit state modeling and interactive feedback. Specifically, a Planner agent generates flexible optimization trajectories, a Teacher-Critic-Student triad engages in Socratic-style dialogue to iteratively optimize the prompt based on pseudo-gradient signals in the text space, and a Target agent executes the prompt in downstream tasks to provide performance feedback. MARS integrates reasoning, feedback, and state transition into a unified hidden-state evolution process, improving both the effectiveness and interpretability of optimization. Extensive experiments on multiple datasets demonstrate that MARS outperforms existing APO methods in terms of optimization performance, search efficiency, and interpretability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting individual differences to bootstrap communication</title>
<link>https://arxiv.org/abs/2504.05211</link>
<guid>https://arxiv.org/abs/2504.05211</guid>
<content:encoded><![CDATA[
arXiv:2504.05211v3 Announce Type: replace 
Abstract: Establishing a communication system is hard because the intended meaning of a signal is unknown to its receiver when first produced, and the signaller also has no idea how that signal will be interpreted. Most theoretical accounts of the emergence of communication systems rely on feedback to reinforce behaviours that have led to successful communication in the past. However, providing such feedback requires already being able to communicate the meaning that was intended or interpreted. Therefore these accounts cannot explain how communication can be bootstrapped from non-communicative behaviours. Here we present a model that shows how a communication system, capable of expressing an unbounded number of meanings, can emerge as a result of individual behavioural differences in a large population without any pre-existing means to determine communicative success. The two key cognitive capabilities responsible for this outcome are behaving predictably in a given situation, and an alignment of psychological states ahead of signal production that derives from shared intentionality. Since both capabilities can exist independently of communication, our results are compatible with theories in which large flexible socially-learned communication systems like language are the product of a general but well-developed capacity for social cognition.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v4 Announce Type: replace 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for closed-book hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.16483</link>
<guid>https://arxiv.org/abs/2505.16483</guid>
<content:encoded><![CDATA[
arXiv:2505.16483v3 Announce Type: replace 
Abstract: Teaching large language models (LLMs) to be faithful in the provided context is crucial for building reliable information-seeking systems. Therefore, we propose a systematic framework, CANOE, to reduce faithfulness hallucinations of LLMs across different downstream tasks without human annotations. Specifically, we first synthesize short-form question-answering (QA) data with four diverse tasks to construct high-quality and easily verifiable training data without human annotation. Also, we propose Dual-GRPO, a rule-based reinforcement learning method that includes three tailored rule-based rewards derived from synthesized short-form QA data, while simultaneously optimizing both short-form and long-form response generation. Notably, Dual-GRPO eliminates the need to manually label preference data to train reward models and avoids over-optimizing short-form generation when relying only on the synthesized short-form QA data. Experimental results show that CANOE greatly improves the faithfulness of LLMs across 11 different tasks, even outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models</title>
<link>https://arxiv.org/abs/2505.16774</link>
<guid>https://arxiv.org/abs/2505.16774</guid>
<content:encoded><![CDATA[
arXiv:2505.16774v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong instruction-following capabilities in text-based tasks. However, this ability often deteriorates in multimodal models after alignment with non-text modalities such as images or audio. While several recent efforts have investigated instruction-following performance in text and vision-language models, instruction-following in audio-based large language models remains largely unexplored. To bridge this gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess the ability to follow instructions in an audio LLM. IFEval-Audio contains 280 audio-instruction-answer triples across six diverse dimensions: Content, Capitalization, Symbol, List Structure, Length, and Format. Each example pairs an audio input with a text instruction, requiring the model to generate an output that follows a specified structure. We benchmark state-of-the-art audio LLMs on their ability to follow audio-involved instructions. The dataset is released publicly to support future research in this emerging area.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High</title>
<link>https://arxiv.org/abs/2505.22354</link>
<guid>https://arxiv.org/abs/2505.22354</guid>
<content:encoded><![CDATA[
arXiv:2505.22354v2 Announce Type: replace 
Abstract: This paper examines how LLMs handle false presuppositions and whether certain linguistic factors influence their responses to falsely presupposed content. Presuppositions subtly introduce information as given, making them highly effective at embedding disputable or false information. This raises concerns about whether LLMs, like humans, may fail to detect and correct misleading assumptions introduced as false presuppositions, even when the stakes of misinformation are high. Using a systematic approach based on linguistic presupposition analysis, we investigate the conditions under which LLMs are more or less sensitive to adopt or reject false presuppositions. Focusing on political contexts, we examine how factors like linguistic construction, political party, and scenario probability impact the recognition of false presuppositions. We conduct experiments with a newly created dataset and examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's Mistral-7B-v03. Our results show that the models struggle to recognize false presuppositions, with performance varying by condition. This study highlights that linguistic presupposition analysis is a valuable tool for uncovering the reinforcement of political misinformation in LLM responses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models</title>
<link>https://arxiv.org/abs/2505.23015</link>
<guid>https://arxiv.org/abs/2505.23015</guid>
<content:encoded><![CDATA[
arXiv:2505.23015v2 Announce Type: replace 
Abstract: Stealthy data poisoning during fine-tuning can backdoor large language models (LLMs), threatening downstream safety. Existing detectors either use classifier-style probability signals--ill-suited to generation--or rely on rewriting, which can degrade quality and even introduce new triggers. We address the practical need to efficiently remove poisoned examples before or during fine-tuning. We observe a robust signal in the response space: after applying TF-IDF to model responses, poisoned examples form compact clusters (driven by consistent malicious outputs), while clean examples remain dispersed. We leverage this with RFTC--Reference-Filtration + TF-IDF Clustering. RFTC first compares each example's response with that of a reference model and flags those with large deviations as suspicious; it then performs TF-IDF clustering on the suspicious set and identifies true poisoned examples using intra-class distance. On two machine translation datasets and one QA dataset, RFTC outperforms prior detectors in both detection accuracy and the downstream performance of the fine-tuned models. Ablations with different reference models further validate the effectiveness and robustness of Reference-Filtration.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding</title>
<link>https://arxiv.org/abs/2506.00942</link>
<guid>https://arxiv.org/abs/2506.00942</guid>
<content:encoded><![CDATA[
arXiv:2506.00942v2 Announce Type: replace 
Abstract: The advent of multimodal large language models (MLLMs) has sparked interest in their application to electrocardiogram (ECG) analysis. However, existing ECG-focused MLLMs primarily focus on report generation tasks, often limited to single 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the potential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that supports a broader range of tasks and more flexible ECG inputs. However, existing ECG-QA datasets are often monotonous. To address this gap, we first constructed the anyECG dataset, which encompasses a wide variety of tasks, including report generation, abnormal waveform localization, and open-ended question answering. In addition to standard hospital ECGs, we introduced long-duration reduced-lead ECGs for home environments and multiple ECG comparison scenarios commonly encountered in clinical practice. Furthermore, we propose the anyECG-chat model, which supports dynamic-length ECG inputs and multiple ECG inputs. We trained the model using a three-stage curriculum training recipe with the anyECG dataset. A comprehensive evaluation was conducted, demonstrating that anyECG-chat is capable of supporting various practical application scenarios, including not only common report generation tasks but also abnormal waveform localization for long-duration reduced-lead ECGs in home environments and comprehensive comparative analysis of multiple ECGs. Our code and data are available at: https://github.com/CuCl-2/anyECG-chat.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReliableMath: Benchmark of Reliable Mathematical Reasoning on Large Language Models</title>
<link>https://arxiv.org/abs/2507.03133</link>
<guid>https://arxiv.org/abs/2507.03133</guid>
<content:encoded><![CDATA[
arXiv:2507.03133v2 Announce Type: replace 
Abstract: Although demonstrating remarkable performance on reasoning tasks, Large Language Models (LLMs) still tend to fabricate unreliable responses when confronted with problems that are unsolvable or beyond their capability, severely undermining the reliability. Prior studies of LLM reliability have primarily focused on knowledge tasks to identify unanswerable questions, while mathematical reasoning tasks have remained unexplored due to the dearth of unsolvable math problems. To systematically investigate LLM reliability in mathematical reasoning tasks, we formulate the reliability evaluation for both solvable and unsolvable problems. We then develop a ReliableMath dataset which incorporates open-source solvable problems and high-quality unsolvable problems synthesized by our proposed construction workflow with human evaluations. Experiments are conducted on various LLMs with several key findings uncovered. LLMs fail to directly identify unsolvable problems and always generate fabricated responses. When instructing LLMs to indicate unsolvability using a reliable prompt, the reliability of larger-sized LLMs remains on solvable problems, but notably improves on unsolvable problems yet still falls short of solvable problems. However, small LLMs rarely show any progress despite employing reliable prompts. Therefore, we further propose an alignment strategy to enhance small LLMs' reliability, which can significantly improve LLM reliability performances on both in-domain and out-of-domain tasks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian</title>
<link>https://arxiv.org/abs/2507.22159</link>
<guid>https://arxiv.org/abs/2507.22159</guid>
<content:encoded><![CDATA[
arXiv:2507.22159v2 Announce Type: replace 
Abstract: Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of LLM-generated text. The dataset contains 522 prompts and yields 4,099 human-annotated pairwise preferences from comparisons across five instruction-tuned LLMs. All annotations are natively written in Indonesian with strong inter-annotator agreement, measured by Krippendorff's alpha. Our benchmark spans 10 diverse categories, enabling practitioners to identify LLMs' fine-grained strengths and weaknesses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Super Experts in Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2507.23279</link>
<guid>https://arxiv.org/abs/2507.23279</guid>
<content:encoded><![CDATA[
arXiv:2507.23279v2 Announce Type: replace 
Abstract: In this study, we report, for the first time, the discovery and systematic investigation of a distinct subset of experts that play a pivotal role in the MoE LLMs' forward inference. These experts are prevalent in open-source MoE LLMs, and despite their extremely limited number, pruning them results in a substantial decline in model performance (e.g., prune just three out of 6,144 causes Qwen3-30B-A3B to generate repetitive and uninformative outputs).We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs: (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs is model-specific, data-agnostic, and remains unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further investigate why compressing SEs exerts such a pronounced impact. We show that, in MoE LLMs, SEs serve as the primary source of the systematic outlier mechanism in Transformers, and that compressing them profoundly disrupts this process, ultimately causing the collapse of attention sinks. These findings advance the understanding of the internal dynamics of MoE LLMs, filling an important gap in the current knowledge. The code is provided in https://github.com/ZunhaiSu/Super-Experts-Profilling.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates</title>
<link>https://arxiv.org/abs/2508.01159</link>
<guid>https://arxiv.org/abs/2508.01159</guid>
<content:encoded><![CDATA[
arXiv:2508.01159v2 Announce Type: replace 
Abstract: This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
arXiv:2508.12495v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
arXiv:2508.15212v3 Announce Type: replace 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs</title>
<link>https://arxiv.org/abs/2508.19594</link>
<guid>https://arxiv.org/abs/2508.19594</guid>
<content:encoded><![CDATA[
arXiv:2508.19594v3 Announce Type: replace 
Abstract: Context faithfulness is essential for reliable reasoning in context-dependent scenarios. However, large language models often struggle to ground their outputs in the provided context, resulting in irrelevant responses. Inspired by the emergent expert specialization observed in mixture-of-experts architectures, this work investigates whether certain experts exhibit specialization in context utilization, offering a potential pathway toward targeted optimization for improved context faithfulness. To explore this, we propose Router Lens, a method that accurately identifies context-faithful experts. Our analysis reveals that these experts progressively amplify attention to relevant contextual information, thereby enhancing context grounding. Building on this insight, we introduce Context-faithful Expert Fine-Tuning (CEFT), a lightweight optimization approach that selectively fine-tunes context-faithful experts. Experiments across a wide range of benchmarks and models demonstrate that CEFT matches or surpasses the performance of full fine-tuning while being significantly more efficient.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Language Models Handle a Non-Gregorian Calendar? The Case of the Japanese wareki</title>
<link>https://arxiv.org/abs/2509.04432</link>
<guid>https://arxiv.org/abs/2509.04432</guid>
<content:encoded><![CDATA[
arXiv:2509.04432v3 Announce Type: replace 
Abstract: Temporal reasoning and knowledge are essential capabilities for language models (LMs). While much prior work has analyzed and improved temporal reasoning in LMs, most studies have focused solely on the Gregorian calendar. However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew calendars, are in active use and reflect culturally grounded conceptions of time. If and how well current LMs can accurately handle such non-Gregorian calendars has not been evaluated so far. Here, we present a systematic evaluation of how well language models handle one such non-Gregorian system: the Japanese wareki. We create datasets that require temporal knowledge and reasoning in using wareki dates. Evaluating open and closed LMs, we find that some models can perform calendar conversions, but GPT-4o, Deepseek V3, and even Japanese-centric models struggle with Japanese calendar arithmetic and knowledge involving wareki dates. Error analysis suggests corpus frequency of Japanese calendar expressions and a Gregorian bias in the model's knowledge as possible explanations. Our results show the importance of developing LMs that are better equipped for culture-specific tasks such as calendar understanding.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations</title>
<link>https://arxiv.org/abs/2509.04498</link>
<guid>https://arxiv.org/abs/2509.04498</guid>
<content:encoded><![CDATA[
arXiv:2509.04498v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used as daily recommendation systems for tasks like education planning, yet their recommendations risk perpetuating societal biases. This paper empirically examines geographic, demographic, and economic biases in university and program suggestions from three open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360 simulated user profiles varying by gender, nationality, and economic status, we analyze over 25,000 recommendations. Results show strong biases: institutions in the Global North are disproportionately favored, recommendations often reinforce gender stereotypes, and institutional repetition is prevalent. While LLaMA-3.1 achieves the highest diversity, recommending 481 unique universities across 58 countries, systemic disparities persist. To quantify these issues, we propose a novel, multi-dimensional evaluation framework that goes beyond accuracy by measuring demographic and geographic representation. Our findings highlight the urgent need for bias consideration in educational LMs to ensure equitable global access to higher education.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance</title>
<link>https://arxiv.org/abs/2510.03528</link>
<guid>https://arxiv.org/abs/2510.03528</guid>
<content:encoded><![CDATA[
arXiv:2510.03528v2 Announce Type: replace 
Abstract: Instruction-tuning plays a vital role in enhancing the task-solving abilities of large language models (LLMs), improving their usability in generating helpful responses on various tasks. However, previous work has demonstrated that they are sensitive to minor variations in instruction phrasing. In this paper, we explore whether introducing perturbations in instruction-tuning data can enhance LLMs' resistance against noisy instructions. We focus on how instruction-tuning with perturbations, such as removing stop words or shuffling words, affects LLMs' performance on the original and perturbed versions of widely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics and potential shifts in model behavior. Surprisingly, our results suggest that instruction-tuning on perturbed instructions can, in some cases, improve downstream performance. These findings highlight the importance of including perturbed instructions in instruction-tuning, which can make LLMs more resilient to noisy user inputs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium</title>
<link>https://arxiv.org/abs/2510.25977</link>
<guid>https://arxiv.org/abs/2510.25977</guid>
<content:encoded><![CDATA[
arXiv:2510.25977v3 Announce Type: replace 
Abstract: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache</title>
<link>https://arxiv.org/abs/2510.25979</link>
<guid>https://arxiv.org/abs/2510.25979</guid>
<content:encoded><![CDATA[
arXiv:2510.25979v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Positional Bias in Long-Document Ranking: Impact, Assessment, and Mitigation</title>
<link>https://arxiv.org/abs/2207.01262</link>
<guid>https://arxiv.org/abs/2207.01262</guid>
<content:encoded><![CDATA[
arXiv:2207.01262v4 Announce Type: replace-cross 
Abstract: We tested over 20 Transformer models for ranking long documents (including recent LongP models trained with FlashAttention and RankGPT models "powered" by OpenAI and Anthropic cloud APIs). We compared them with the simple FirstP baseline, which applied the same model to truncated input (up to 512 tokens). On MS MARCO, TREC DL, and Robust04 no long-document model outperformed FirstP by more than 5% (on average). We hypothesized that this lack of improvement is not due to inherent model limitations, but due to benchmark positional bias (most relevant passages tend to occur early in documents), which is known to exist in MS MARCO. To confirm this, we analyzed positional relevance distributions across four long-document corpora (with six query sets) and observed the same early-position bias. Surprisingly, we also found bias in six BEIR collections, which are typically categorized as short-document datasets. We then introduced a new diagnostic dataset, MS MARCO FarRelevant, where relevant spans were deliberately placed beyond the first 512 tokens. On this dataset, many long-context models (including RankGPT) performed at random-baseline level, suggesting overfitting to positional bias. We also experimented with debiasing training data, but with limited success. Our findings (1) highlight the need for careful benchmark design in evaluating long-context models for document ranking, (2) identify model types that are more robust to positional bias, and (3) motivate further work on approaches to debias training data. We release our code and data to support further research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formalizing and Benchmarking Prompt Injection Attacks and Defenses</title>
<link>https://arxiv.org/abs/2310.12815</link>
<guid>https://arxiv.org/abs/2310.12815</guid>
<content:encoded><![CDATA[
arXiv:2310.12815v5 Announce Type: replace-cross 
Abstract: A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms</title>
<link>https://arxiv.org/abs/2409.16694</link>
<guid>https://arxiv.org/abs/2409.16694</guid>
<content:encoded><![CDATA[
arXiv:2409.16694v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and applicability of LLMs through low-bit quantization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4AD: Large Language Models for Autonomous Driving - Concept, Review, Benchmark, Experiments, and Future Trends</title>
<link>https://arxiv.org/abs/2410.15281</link>
<guid>https://arxiv.org/abs/2410.15281</guid>
<content:encoded><![CDATA[
arXiv:2410.15281v4 Announce Type: replace-cross 
Abstract: With the broader adoption and highly successful development of Large Language Models (LLMs), there has been growing interest and demand for applying LLMs to autonomous driving technology. Driven by their natural language understanding and reasoning capabilities, LLMs have the potential to enhance various aspects of autonomous driving systems, from perception and scene understanding to interactive decision-making. In this paper, we first introduce the novel concept of designing Large Language Models for Autonomous Driving (LLM4AD), followed by a review of existing LLM4AD studies. Then, we propose a comprehensive benchmark for evaluating the instruction-following and reasoning abilities of LLM4AD systems, which includes LaMPilot-Bench, CARLA Leaderboard 1.0 Benchmark in simulation and NuPlanQA for multi-view visual question answering. Furthermore, we conduct extensive real-world experiments on autonomous vehicle platforms, examining both on-cloud and on-edge LLM deployment for personalized decision-making and motion control. Next, we explore the future trends of integrating language diffusion models into autonomous driving, exemplified by the proposed ViLaD (Vision-Language Diffusion) framework. Finally, we discuss the main challenges of LLM4AD, including latency, deployment, security and privacy, safety, trust and transparency, and personalization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Retrieval-Augmented Generation with Differential Privacy</title>
<link>https://arxiv.org/abs/2412.04697</link>
<guid>https://arxiv.org/abs/2412.04697</guid>
<content:encoded><![CDATA[
arXiv:2412.04697v3 Announce Type: replace-cross 
Abstract: With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval-augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$ across different models and datasets.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-OT: An Optimal Transport Regularization Paradigm for Knowledge Preservation in Vision-Language Model Adaptation</title>
<link>https://arxiv.org/abs/2503.08906</link>
<guid>https://arxiv.org/abs/2503.08906</guid>
<content:encoded><![CDATA[
arXiv:2503.08906v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong performance but struggle when adapted to downstream tasks. Prompt learning has emerged as an efficient and effective strategy to adapt VLMs while preserving their pre-trained knowledge. However, existing methods still lead to overfitting and degrade zero-shot generalization. To address this challenge, we propose an optimal transport (OT)-guided prompt learning framework that mitigates forgetting by preserving the structural consistency of feature distributions between pre-trained and fine-tuned models. Unlike conventional point-wise constraints, OT naturally captures cross-instance relationships and expands the feasible parameter space for prompt tuning, allowing a better trade-off between adaptation and generalization. Our approach enforces joint constraints on both vision and text representations, ensuring a holistic feature alignment. Extensive experiments on benchmark datasets demonstrate that our simple yet effective method can outperform existing prompt learning strategies in base-to-novel generalization, cross-dataset evaluation, and domain generalization without additional augmentation or ensemble techniques. The code is available at https://github.com/ChongQingNoSubway/Prompt-OT
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks</title>
<link>https://arxiv.org/abs/2504.04277</link>
<guid>https://arxiv.org/abs/2504.04277</guid>
<content:encoded><![CDATA[
arXiv:2504.04277v3 Announce Type: replace-cross 
Abstract: Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5\% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases</title>
<link>https://arxiv.org/abs/2504.05478</link>
<guid>https://arxiv.org/abs/2504.05478</guid>
<content:encoded><![CDATA[
arXiv:2504.05478v2 Announce Type: replace-cross 
Abstract: Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&amp;As on large text-attributed KGs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15311</link>
<guid>https://arxiv.org/abs/2505.15311</guid>
<content:encoded><![CDATA[
arXiv:2505.15311v2 Announce Type: replace-cross 
Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[
arXiv:2505.17330v2 Announce Type: replace-cross 
Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v4 Announce Type: replace-cross 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v4 Announce Type: replace-cross 
Abstract: The Political Compass Test (PCT) and similar surveys are commonly used to assess political bias in auto-regressive LLMs. Our rigorous statistical experiments show that while changes to standard generation parameters have minimal effect on PCT scores, prompt phrasing and fine-tuning individually and together can significantly influence results. Interestingly, fine-tuning on politically rich vs. neutral datasets does not lead to different shifts in scores. We also generalize these findings to a similar popular test called 8 Values. Humans do not change their responses to questions when prompted differently (``answer this question'' vs ``state your opinion''), or after exposure to politically neutral text, such as mathematical formulae. But the fact that the models do so raises concerns about the validity of these tests for measuring model bias, and paves the way for deeper exploration into how political and social views are encoded in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval</title>
<link>https://arxiv.org/abs/2508.04001</link>
<guid>https://arxiv.org/abs/2508.04001</guid>
<content:encoded><![CDATA[
arXiv:2508.04001v2 Announce Type: replace-cross 
Abstract: Conversational search aims to satisfy users' complex information needs via multiple-turn interactions. The key challenge lies in revealing real users' search intent from the context-dependent queries. Previous studies achieve conversational search by fine-tuning a conversational dense retriever with relevance judgments between pairs of context-dependent queries and documents. However, this training paradigm encounters data scarcity issues. To this end, we propose ConvMix, a mixed-criteria framework to augment conversational dense retrieval, which covers more aspects than existing data augmentation frameworks. We design a two-sided relevance judgment augmentation schema in a scalable manner via the aid of large language models. Besides, we integrate the framework with quality control mechanisms to obtain semantically diverse samples and near-distribution supervisions to combine various annotated data. Experimental results on five widely used benchmarks show that the conversational dense retriever trained by our ConvMix framework outperforms previous baseline methods, which demonstrates our superior effectiveness.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v2 Announce Type: replace-cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories-traditionally used for training value or reward models-can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree-structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low-variance, prefix-aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance-a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReFineG: Synergizing Small Supervised Models and LLMs for Low-Resource Grounded Multimodal NER</title>
<link>https://arxiv.org/abs/2509.10975</link>
<guid>https://arxiv.org/abs/2509.10975</guid>
<content:encoded><![CDATA[
arXiv:2509.10975v2 Announce Type: replace-cross 
Abstract: Grounded Multimodal Named Entity Recognition (GMNER) extends traditional NER by jointly detecting textual mentions and grounding them to visual regions. While existing supervised methods achieve strong performance, they rely on costly multimodal annotations and often underperform in low-resource domains. Multimodal Large Language Models (MLLMs) show strong generalization but suffer from Domain Knowledge Conflict, producing redundant or incorrect mentions for domain-specific entities. To address these challenges, we propose ReFineG, a three-stage collaborative framework that integrates small supervised models with frozen MLLMs for low-resource GMNER. In the Training Stage, a domain-aware NER data synthesis strategy transfers LLM knowledge to small models with supervised training while avoiding domain knowledge conflicts. In the Refinement Stage, an uncertainty-based mechanism retains confident predictions from supervised models and delegates uncertain ones to the MLLM. In the Grounding Stage, a multimodal context selection algorithm enhances visual grounding through analogical reasoning. In the CCKS2025 GMNER Shared Task, ReFineG ranked second with an F1 score of 0.6461 on the online leaderboard, demonstrating its effectiveness with limited annotations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03669</link>
<guid>https://arxiv.org/abs/2510.03669</guid>
<content:encoded><![CDATA[
arXiv:2510.03669v3 Announce Type: replace-cross 
Abstract: Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance</title>
<link>https://arxiv.org/abs/2510.08048</link>
<guid>https://arxiv.org/abs/2510.08048</guid>
<content:encoded><![CDATA[
arXiv:2510.08048v2 Announce Type: replace-cross 
Abstract: Query-product relevance prediction is fundamental to e-commerce search and has become even more critical in the era of AI-powered shopping, where semantic understanding and complex reasoning directly shape the user experience and business conversion. Large Language Models (LLMs) enable generative, reasoning-based approaches, typically aligned via supervised fine-tuning (SFT) or preference optimization methods like Direct Preference Optimization (DPO). However, the increasing complexity of business rules and user queries exposes the inability of existing methods to endow models with robust reasoning capacity for long-tail and challenging cases. Efforts to address this via reinforcement learning strategies like Group Relative Policy Optimization (GRPO) often suffer from sparse terminal rewards, offering insufficient guidance for multi-step reasoning and slowing convergence. To address these challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning framework for LLM-based relevance prediction in Taobao Search Relevance. TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which decomposes the final relevance judgment into dense, structured rewards aligned with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which identifies low-accuracy rollouts during training and injects targeted ground-truth guidance to steer the policy away from stagnant, rule-violating reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on large-scale real-world datasets and through online side-by-side human evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO baselines in offline experiments, improving relevance accuracy, rule adherence, and training stability. The model trained with TaoSR-AGRL has been successfully deployed in the main search scenario on Taobao, serving hundreds of millions of users.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Glia: A Human-Inspired AI for Automated Systems Design and Optimization</title>
<link>https://arxiv.org/abs/2510.27176</link>
<guid>https://arxiv.org/abs/2510.27176</guid>
<content:encoded><![CDATA[
<div> Agents, Experimentation, Analysis, Networked systems design, AI architecture <br />
Summary: <br />
The study introduces Glia, an AI architecture utilizing large language models (LLMs) to autonomously design mechanisms for computer systems. It features a multi-agent workflow where each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that combines abstract reasoning with empirical feedback. Unlike existing machine learning methods, Glia generates interpretable designs and reveals its reasoning process. When applied to optimizing a distributed GPU cluster for LLM inference, Glia creates new algorithms for request routing, scheduling, and auto-scaling that rival human-expert performance in less time. It also provides unique insights into workload behavior. By leveraging reasoning LLMs and structured experimentation, the AI demonstrates the ability to produce innovative and comprehensible designs for complex systems problems. Overall, the results indicate that AI can excel in networked systems design by combining reasoning capabilities with practical experimentation. <br /> <div>
arXiv:2510.27176v2 Announce Type: replace-cross 
Abstract: Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preliminary Study of RAG for Taiwanese Historical Archives</title>
<link>https://arxiv.org/abs/2511.07445</link>
<guid>https://arxiv.org/abs/2511.07445</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, RAG, Taiwanese Historical Archives, Traditional Chinese datasets, Fort Zeelandia, Taiwan Provincial Council Gazette <br />
Summary: 
This paper presents an initial study on the application of a Retrieval-Augmented Generation (RAG) pipeline to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, with corresponding open-ended query sets. The study examines the impacts of query characteristics and metadata integration strategies on retrieval quality, answer generation, and overall system performance. The findings suggest that early-stage metadata integration improves retrieval and answer accuracy. However, challenges such as generation hallucinations and difficulties in handling temporal or multi-hop historical queries persist in RAG systems. The research sheds light on the potential of RAG for knowledge-intensive tasks in Taiwanese Historical Archives but also highlights areas for further improvement in system functionality and accuracy. <br /> <div>
arXiv:2511.07445v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey</title>
<link>https://arxiv.org/abs/2511.07448</link>
<guid>https://arxiv.org/abs/2511.07448</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific idea generation, large language models, creativity, scientific discovery, methods

Summary:
This article explores the use of large language models (LLMs) in scientific idea generation, a complex task that requires balancing creativity with scientific soundness. The survey categorizes existing methods into five families, including external knowledge augmentation and multi-agent collaboration, to understand how different approaches generate creative and scientifically valid ideas. By applying creativity frameworks, the authors analyze the level and source of creativity each method emphasizes. The survey clarifies the current state of the field and suggests directions for reliable and transformative applications of LLMs in scientific discovery.<br /><br />Summary: <div>
arXiv:2511.07448v1 Announce Type: new 
Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2511.07457</link>
<guid>https://arxiv.org/abs/2511.07457</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, knowledge graphs, web data, fine-tuning, LoRA parameters

Summary:
Large Language Models (LLMs) have shown great success in handling sequential textual data but struggle with structural data like knowledge graphs. Existing methods either convert graphs into text sequences with high token overhead or use additional modules for fixed-size token representations, leading to suboptimal results. To address this, the authors propose a novel framework called GRIP, which enables LLMs to internalize complex relational information from graphs through fine-tuning tasks. This knowledge is stored in lightweight LoRA parameters, allowing the fine-tuned LLM to handle various graph-related tasks without requiring the original graph at inference time. Extensive experiments on different benchmarks confirm the effectiveness and efficiency of the GRIP approach.<br /><br />Summary: Large Language Models (LLMs) struggle with structural data like knowledge graphs, prompting the development of the GRIP framework. GRIP equips LLMs with the ability to learn complex relational information from graphs through fine-tuning tasks, stored in lightweight LoRA parameters. This enables LLMs to handle graph-related tasks efficiently without needing access to the original graph at inference time. <div>
arXiv:2511.07457v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment</title>
<link>https://arxiv.org/abs/2511.07458</link>
<guid>https://arxiv.org/abs/2511.07458</guid>
<content:encoded><![CDATA[
<div> Keywords: log summarization, evaluation, REFLEX, large language model, metrics

<br />
Summary: 
The article introduces REFLEX, a reference-free evaluation metric for log summarization using large language models (LLMs). Traditional metrics like ROUGE and BLEU rely on surface-level lexical overlap, making evaluating log summarization systems challenging due to the lack of high-quality reference summaries. REFLEX leverages LLMs as zero-shot evaluators to assess summary quality in terms of relevance, informativeness, and coherence without the need for gold-standard references or human annotations. The new metric produces stable, interpretable, and fine-grained evaluations across various log summarization datasets, effectively distinguishing model outputs better than traditional metrics. REFLEX offers a scalable alternative for evaluating log summaries in real-world scenarios where reference data may be scarce or unavailable. <br /><br /> <div>
arXiv:2511.07458v1 Announce Type: new 
Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It Takes Two: A Dual Stage Approach for Terminology-Aware Translation</title>
<link>https://arxiv.org/abs/2511.07461</link>
<guid>https://arxiv.org/abs/2511.07461</guid>
<content:encoded><![CDATA[
<div> terminology-constrained machine translation, DuTerm, NMT model, LLM, terminology adherence

Summary:
DuTerm is a two-stage architecture for terminology-constrained machine translation that combines a terminology-aware NMT model with a prompt-based LLM for post-editing. The system was evaluated on multiple language pairs using the WMT 2025 Terminology Shared Task corpus. Results showed that the LLM stage, which refines NMT output and enforces terminology adherence, consistently produced higher quality translations compared to strict constraint enforcement. The study also highlighted the importance of flexible, context-driven terminology handling by the LLM, which was found to work best as context-driven mutators rather than generators. This trade-off between strict constraints and context-driven refinement is critical for achieving high-quality translations in terminology-constrained settings. <div>
arXiv:2511.07461v1 Announce Type: new 
Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motif 2 12.7B technical report</title>
<link>https://arxiv.org/abs/2511.07464</link>
<guid>https://arxiv.org/abs/2511.07464</guid>
<content:encoded><![CDATA[
<div> Efficiency, Language understanding, Generalization, Training optimization, Fine-tuning 
<br />
Summary: 
Motif-2-12.7B is a new open-weight foundation model that focuses on efficiency and language understanding. It integrates Grouped Differential Attention (GDA) for improved representational efficiency. The model is pre-trained on a vast amount of data spanning different domains using a curriculum-driven data scheduler. The training system leverages special optimization techniques for enhanced performance in large-scale distributed environments. Post-training involves a supervised fine-tuning pipeline to improve general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across various benchmarks, showcasing the effectiveness of architectural scaling and optimized training design in achieving comparable capabilities to larger models. 
<br /> <div>
arXiv:2511.07464v1 Announce Type: new 
Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2511.07498</link>
<guid>https://arxiv.org/abs/2511.07498</guid>
<content:encoded><![CDATA[
<div> method, attention heads, multilingual processing, language-specific, interpretability
Summary:
The article discusses the role of multi-head self-attention (MHA) in supporting multilingual processing in Large Language Models (LLMs). The authors propose Language Attention Head Importance Scores (LAHIS) as a method to identify attention head importance for multilingual capabilities in LLMs. They apply LAHIS to three LLMs and discover the presence of language-specific and language-general heads, with language-specific heads aiding in cross-lingual attention transfer. A lightweight adaptation is introduced to improve XQuAD accuracy by modulating attention outputs over language heads with minimal parameters. This study enhances the interpretability and multilingual capabilities of LLMs by highlighting the importance of MHA in addressing challenges related to multilingual understanding and generation. <br /><br />Summary: <div>
arXiv:2511.07498v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Optimization Unlocks Real-Time Pairwise Reranking</title>
<link>https://arxiv.org/abs/2511.07555</link>
<guid>https://arxiv.org/abs/2511.07555</guid>
<content:encoded><![CDATA[
<div> optimization methods, latency reduction, pairwise reranking, Large Language Models, real-time applications
Summary: 
Pairwise reranking of documents in information retrieval systems to enhance Retrieval-Augmented Generation (RAG) processes is crucial. Large Language Models (LLMs) play a significant role in this reranking, with Pairwise Reranking Prompting (PRP) being a popular method. However, concerns about complexity and computational demands exist. This study focuses on optimizing pairwise reranking, achieving a remarkable latency reduction while maintaining performance levels. By using smaller models, limiting the reranked set, and implementing other optimizations such as reducing positional bias and restricting output tokens, the efficiency of LLM-based reranking is significantly improved. These optimizations make LLM-based reranking more feasible for real-world deployment in latency-sensitive applications. <br /><br />Summary: <div>
arXiv:2511.07555v1 Announce Type: new 
Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives</title>
<link>https://arxiv.org/abs/2511.07641</link>
<guid>https://arxiv.org/abs/2511.07641</guid>
<content:encoded><![CDATA[
<div> Keywords: emotional nuances, Dutch-specific LLMs, valence prediction, sentiment analysis, low-resource language variant

Summary:
Traditional lexicon-based tools like LIWC and Pattern have been foundational in understanding emotional nuances in everyday language. However, this study evaluated the performance of Dutch-specific Large Language Models (LLMs) against these traditional methods for valence prediction in Flemish, a low-resource language variant. Surprisingly, the Dutch-tuned LLMs did not outperform Pattern, suggesting a challenge in capturing emotional valence in spontaneous narratives. The findings question assumptions about LLM superiority in sentiment analysis tasks and emphasize the need for culturally and linguistically tailored evaluation frameworks for low-resource language variants. This study highlights the complexity of emotional expression in real-world language use and raises concerns about the adequacy of current LLM fine-tuning approaches. <div>
arXiv:2511.07641v1 Announce Type: new 
Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering</title>
<link>https://arxiv.org/abs/2511.07659</link>
<guid>https://arxiv.org/abs/2511.07659</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Natural Language Inference, QA datasets, DIVER-QA, semantic nuances 

Summary: 
- Evaluating answers from large language models is challenging due to semantic nuances that are missed by lexical metrics and the computational expense of LLM-as-Judge scoring.
- A lightweight alternative using off-the-shelf Natural Language Inference scoring augmented by a simple lexical-match flag matches GPT-4o's accuracy on long-form QA with significantly fewer parameters.
- The authors introduce DIVER-QA, a new 3000-sample human-annotated benchmark that spans five QA datasets and five candidate LLMs to test human alignment of evaluation metrics.
- Results demonstrate that inexpensive NLI-based evaluation remains competitive in measuring LLM performance and offer DIVER-QA as an open resource for future metric research. 

<br /><br />Summary: <div>
arXiv:2511.07659v1 Announce Type: new 
Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title>
<link>https://arxiv.org/abs/2511.07689</link>
<guid>https://arxiv.org/abs/2511.07689</guid>
<content:encoded><![CDATA[
<div> Keywords: factual consistency, abstractive text summarization, factuality metrics, long-document setting, benchmark datasets<br />
<br />
Summary: 
The study evaluates the reliability of six factuality metrics for abstractive text summarization on long documents. Perturbations like paraphrasing and simplification were used to test metric robustness. Short-form metrics showed inconsistent scores for semantically equivalent summaries and struggled with information-dense claims similar to the source document. Expanding the retrieval context improved stability in some domains but no metric consistently maintained factual alignment. The study suggests directions for improving factuality evaluation, including multi-span reasoning and context-aware calibration. The results underline the need for training on meaning-preserving variations to enhance robustness in long-form summarization.<br /> 
 <div>
arXiv:2511.07689v1 Announce Type: new 
Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences</title>
<link>https://arxiv.org/abs/2511.07691</link>
<guid>https://arxiv.org/abs/2511.07691</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference optimization, Language models, Multilingual settings, Confidence-Aware, Robustness 

Summary: 
Preference optimization is a crucial technique for aligning large language models with human preferences post-training. Existing methods like Direct Preference Optimization (DPO) may struggle to generalize effectively in multilingual settings. To address this, Confidence-Aware Preference Optimization (CAPO) offers a dynamic loss scaling approach based on relative reward, enhancing robustness to noisy or low-margin comparisons. Empirical results show CAPO surpassing existing baselines by 16% in reward accuracy and improving alignment by widening the gap between preferred and dispreferred responses across different languages. This innovation highlights the importance of incorporating confidence levels in preference pairs to enhance the performance and generalizability of preference optimization techniques in multilingual contexts. 

<br /><br />Summary: <div>
arXiv:2511.07691v1 Announce Type: new 
Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Critical Confabulation: Can LLMs Hallucinate for Social Good?</title>
<link>https://arxiv.org/abs/2511.07722</link>
<guid>https://arxiv.org/abs/2511.07722</guid>
<content:encoded><![CDATA[
<div> hallucinate, confabulations, critical confabulation, social affordances, historical accuracy<br />
<br />
Summary: <br />
The article discusses the concept of critical confabulation, inspired by critical fabulation, where LLMs are used to fill in gaps in historical archives due to social inequalities. By simulating gaps in character-centric timelines, LLMs can generate masked events to reconstruct narratives of "hidden figures" in history. Audited LLM models demonstrated the capability to perform critical confabulation effectively, with controlled and well-specified hallucinations supporting knowledge production without compromising historical accuracy. The study evaluated the performance of different LLM models and baselines in generating useful hallucinations under various prompts, showcasing the potential for LLMs to contribute to historical narrative reconstruction. <div>
arXiv:2511.07722v1 Announce Type: new 
Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production</title>
<link>https://arxiv.org/abs/2511.07752</link>
<guid>https://arxiv.org/abs/2511.07752</guid>
<content:encoded><![CDATA[
<div> predictability, language production, naturalistic speech, substitution errors, lexical planning
Summary: 
The study examines how contextual predictability influences word choice and form in online language production. It explores the effects of backward predictability, which considers a word's predictability given its future context, in addition to its predictability based on past context. By analyzing naturalistic speech corpora, the study introduces a new information-theoretic predictability measure that integrates both past and future context. Results show that backward predictability impacts word duration and substitution errors. These errors provide insights into how speakers prioritize form, meaning, and context-based information during lexical planning. Overall, the findings highlight the significance of past and future context in word encoding and selection, shedding light on the mechanisms of sentence planning in language production. <br /><br />Summary: <div>
arXiv:2511.07752v1 Announce Type: new 
Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark</title>
<link>https://arxiv.org/abs/2511.07794</link>
<guid>https://arxiv.org/abs/2511.07794</guid>
<content:encoded><![CDATA[
<div> Methodology, Evaluation system, Design philosophy, Large language models, Insurance field

Summary:
This paper introduces the construction methodology, evaluation system, and design philosophy of CUFEInse v1.0. The benchmark covers 5 core dimensions, 54 sub-indicators, and 14,430 questions in the insurance field. Evaluation results show shortcomings in general-purpose and domain-specific models in insurance scenarios. Common bottlenecks in large models include weak actuarial capabilities and compliance adaptation. CUFEInse fills the gap in professional evaluation benchmarks for insurance, serving as a reference for academia and industry. The paper also discusses future iterations focusing on domain adaptation and reasoning enhancement for insurance large models. <div>
arXiv:2511.07794v1 Announce Type: new 
Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory</title>
<link>https://arxiv.org/abs/2511.07800</link>
<guid>https://arxiv.org/abs/2511.07800</guid>
<content:encoded><![CDATA[
<div> memory, large language models, reinforcement learning, strategic reasoning, adaptive

Summary:
The paper introduces a novel agent-centric, trainable, multi-layered graph memory framework to enhance the reasoning capabilities of Large Language Models (LLMs). This framework utilizes context memory to guide decision-making and improves the utilization of parametric information. A reinforcement-based weight optimization procedure is proposed to make the memory adaptable, estimating the utility of strategic meta-cognition based on reward feedback from downstream tasks. The optimized strategies are integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory demonstrates robust generalization, enhances strategic reasoning performance, and consistently benefits during Reinforcement Learning training. Overall, the framework improves LLM agents' ability to utilize prior experiences, leading to better decision-making in complex, open-ended environments. <br /><br />Summary: <div>
arXiv:2511.07800v1 Announce Type: new 
Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys</title>
<link>https://arxiv.org/abs/2511.07871</link>
<guid>https://arxiv.org/abs/2511.07871</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, social survey, AlignSurvey, dataset

Summary:
AlignSurvey introduces a benchmark that uses large language models (LLMs) to replicate and evaluate the entire social survey process. It includes four tasks: social role modeling, semi-structured interview modeling, attitude stance modeling, and survey response modeling. The benchmark provides task-specific evaluation metrics to assess fidelity, consistency, and fairness at individual and group levels, focusing on demographic diversity. To support AlignSurvey, a multi-tiered dataset architecture is constructed, including the Social Foundation Corpus and Entire-Pipeline Survey Datasets. The SurveyLM family, obtained through fine-tuning of LLMs, is released for domain-specific alignment evaluation. All datasets, models, and tools are available for transparent and socially responsible research.<br /><br />Summary: <div>
arXiv:2511.07871v1 Announce Type: new 
Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles</title>
<link>https://arxiv.org/abs/2511.07879</link>
<guid>https://arxiv.org/abs/2511.07879</guid>
<content:encoded><![CDATA[
<div> Keywords: social unrest events, forecasting, topic modeling, Named Entity Recognition, Related Entity Extraction

Summary:
<br /><br />Forecasting social unrest events in democracies like India is crucial for administrative officials to take necessary action. This paper presents a system that utilizes topic modeling and word2vec to filter relevant news articles, along with Named Entity Recognition (NER) methods to identify key entities like people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format for better analysis. The model developed in this study is geographically independent and aims to identify key features for filtering civil unrest events effectively. The concept of Related Entities, which are crucial entities involved in the event, is introduced, and a method for extracting them is proposed, referred to as Related Entity Extraction. By analyzing announcements in news articles and extracting related entities, the system can accurately forecast planned social unrest events and enable timely intervention by authorities. <div>
arXiv:2511.07879v1 Announce Type: new 
Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification</title>
<link>https://arxiv.org/abs/2511.07888</link>
<guid>https://arxiv.org/abs/2511.07888</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, adversarial attacks, robustness, manifold modeling, sentence embeddings<br />
<br />
Summary: <br />
The article introduces a new approach, Manifold-Correcting Causal Flow (MC^2F), to address the challenge of balancing model robustness against adversarial attacks and performance on clean data in text classification. MC^2F consists of two modules: Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the distribution of clean data in the encoder embedding manifold, while the Geodesic Purification Solver corrects out-of-distribution embeddings by projecting adversarial points back onto the learned manifold. Extensive evaluations on multiple datasets and adversarial attacks show that MC^2F achieves a new state-of-the-art in adversarial robustness without compromising performance on clean data, and even resulting in modest accuracy improvements. <div>
arXiv:2511.07888v1 Announce Type: new 
Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning</title>
<link>https://arxiv.org/abs/2511.07910</link>
<guid>https://arxiv.org/abs/2511.07910</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Logic Drift, Knowledge Graph Question Answering, Logits-to-Logic framework, logic consistency

Summary:
The article introduces the Logits-to-Logic framework to enhance the logic consistency of Large Language Models (LLMs) in structured knowledge reasoning tasks, addressing the challenge of Logic Drift. By targeting the logits output during generation, the framework incorporates modules for strengthening and filtering logits to correct logical defects in LLM outputs. This approach improves logic consistency significantly and achieves state-of-the-art performance on various Knowledge Graph Question Answering (KGQA) benchmarks. Unlike existing methods that rely on input-level guidance, the Logits-to-Logic framework focuses on correcting logical flaws in LLM outputs and can adapt to different tasks and knowledge graphs flexibly. This innovative framework represents a significant advancement in improving LLMs' performance in reasoning tasks involving structured knowledge. 

<br /><br />Summary: The Logits-to-Logic framework enhances logic consistency in LLMs by targeting logits output, correcting logical defects, and achieving state-of-the-art performance on KGQA benchmarks. It outperforms existing methods by focusing on correcting logical flaws in LLM outputs and adapting flexibly to different tasks and knowledge graphs. <div>
arXiv:2511.07910v1 Announce Type: new 
Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Social Media for Mental Health: Data, Methods, and Findings</title>
<link>https://arxiv.org/abs/2511.07914</link>
<guid>https://arxiv.org/abs/2511.07914</guid>
<content:encoded><![CDATA[
<div> Keywords: virtual communities, social media data, mental health challenges, machine learning, natural language processing
Summary: 
This chapter explores the use of social media data for studying mental health challenges such as depression, anxiety, and suicidal thoughts. It discusses the various indicators found in user disclosures, including linguistic, visual, and emotional cues. By leveraging this new source of data, medical practice can be improved, timely support can be provided, and government policies influenced. The chapter categorizes social media data usage, introduces machine learning and natural language processing methods, and suggests future research directions. Through the analysis of virtual communities and forums, valuable insights can be gathered to raise awareness of mental health issues and provide necessary assistance to those in need.<br /><br />Summary: <div>
arXiv:2511.07914v1 Announce Type: new 
Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined</title>
<link>https://arxiv.org/abs/2511.07918</link>
<guid>https://arxiv.org/abs/2511.07918</guid>
<content:encoded><![CDATA[
<div> Keywords: human speech production, theta-band synchrony, speech modes, neural mechanisms, language processing<br />
Summary:<br />
This study explores the differences in theta-band neural synchrony across various modes of speech production, including perceived, overt, whispered, and imagined speech. The analysis focuses on connectivity metrics and region-wise variations. Overt and whispered speech show broad and strong frontotemporal synchrony, reflecting motor-phonological coupling during articulation. Perceived speech exhibits dominant posterior and temporal synchrony, related to auditory perception and comprehension. Imagined speech displays internally coherent synchronization primarily in frontal and supplementary motor regions. This study highlights the distinct neural dynamics underlying different speech modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying on temporoparietal networks. Overall, the findings contribute to a better understanding of the neural mechanisms involved in language perception and imagined speech.<br /> 
Summary: <div>
arXiv:2511.07918v1 Announce Type: new 
Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker</title>
<link>https://arxiv.org/abs/2511.07969</link>
<guid>https://arxiv.org/abs/2511.07969</guid>
<content:encoded><![CDATA[
<div> specialized natural language processing, work-related tasks, multi-task progress, Unified Work Embeddings, zero-shot ranking performance
<br />
Workforce transformation in diverse industries has led to an increased demand for specialized natural language processing capabilities. However, work-related tasks present challenges such as long-tailed distributions, extreme multi-label target spaces, and limited data availability. The WorkBench evaluation suite addresses these complexities by formulating six work-related tasks as ranking problems, enabling cross-task transfer and enhancing performance through Unified Work Embeddings (UWE). UWE leverages real-world data to create task-specific bipartite graphs and employs a many-to-many InfoNCE objective to generate task-agnostic embeddings. The model shows zero-shot ranking performance on unseen target spaces, enables low-latency inference, and outperforms generalist embedding models in terms of macro-averaged MAP and RP@10. 
<br /><br />Summary: <div>
arXiv:2511.07969v1 Announce Type: new 
Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation</title>
<link>https://arxiv.org/abs/2511.07982</link>
<guid>https://arxiv.org/abs/2511.07982</guid>
<content:encoded><![CDATA[
<div> interpretation, Notices to Airmen, NOTAMs, deep parsing, NOTAM-Evolve <br />
Summary:<br />
Accurate interpretation of Notices to Airmen (NOTAMs) is crucial for aviation safety. Existing automated systems struggle to extract actionable intelligence from the condensed and cryptic language of NOTAMs. This article proposes a self-evolving framework called NOTAM-Evolve that uses a large language model to master complex NOTAM interpretation. By leveraging knowledge graph-enhanced retrieval and a closed-loop learning process, the framework achieves a significant accuracy improvement. The researchers also introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Experimental results show a 30.4% absolute accuracy improvement over the base large language model, establishing a new state of the art in structured NOTAM interpretation. <br /> <div>
arXiv:2511.07982v1 Announce Type: new 
Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?</title>
<link>https://arxiv.org/abs/2511.07989</link>
<guid>https://arxiv.org/abs/2511.07989</guid>
<content:encoded><![CDATA[
<div> BERT-like models, LLMs, text classification, South Slavic languages, zero-shot performance<br />
<br />
Summary:<br />
Recent advancements in large language models (LLMs) have led to increased focus on zero-shot and few-shot prompting in text classification tasks. Evaluating the performance of language models across various South Slavic languages, this study compared fine-tuned BERT-like models with a selection of LLMs in sentiment, topic, and genre classification tasks. Results indicate that LLMs exhibit strong zero-shot performance, often matching or surpassing BERT-like models, both in English and South Slavic languages. However, drawbacks of LLMs include less predictable outputs, slower inference, and higher computational costs. Despite the promising performance of LLMs, the practical choice for large-scale text annotation still leans towards fine-tuned BERT-like models. <div>
arXiv:2511.07989v1 Announce Type: new 
Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Correction Distillation for Structured Data Question Answering</title>
<link>https://arxiv.org/abs/2511.07998</link>
<guid>https://arxiv.org/abs/2511.07998</guid>
<content:encoded><![CDATA[
<div> Keywords: structured data QA, large language models, error correction, distillation, small-scale LLMs

Summary:
Structured data question answering (QA) is a key research area, with recent advancements in large language models (LLMs) driving progress. However, small-scale LLMs face challenges in generating accurate structured queries. To address this, a self-correction distillation (SCD) method is proposed. SCD includes an error prompt mechanism (EPM) to detect errors and provide customized error messages during inference, along with a two-stage distillation strategy to transfer capabilities from large-scale LLMs to small-scale ones. Experimental results across various benchmarks show that SCD outperforms other distillation methods on small-scale LLMs and closely approaches the performance of GPT4 on some datasets. Large-scale LLMs with EPM also achieve state-of-the-art results on most datasets.
<br /><br />Summary: <div>
arXiv:2511.07998v1 Announce Type: new 
Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing</title>
<link>https://arxiv.org/abs/2511.08017</link>
<guid>https://arxiv.org/abs/2511.08017</guid>
<content:encoded><![CDATA[
<div> learning framework, multi-character role-playing, Hyper-Contrastive Role-Adaptive, distinct traits, shared traits

Summary:
HyCoRA is a novel Hyper-Contrastive Role-Adaptive (HyCoRA) framework designed to enhance multi-character role-playing by effectively balancing the learning of distinct and shared traits. The framework consists of a Hyper-Half Low-Rank Adaptation structure, featuring a role-specific module generated by a lightweight hyper-network and a trainable role-shared module. The role-specific module captures unique persona signatures, while the role-shared module focuses on common traits. Additionally, a hyper-contrastive learning mechanism is used to differentiate between distinct personalities across roles. Experimental results on English and Chinese benchmarks demonstrate the superior performance of HyCoRA. Evaluation using GPT-4 and visual analyses further confirm the framework's ability to represent role characteristics effectively. <div>
arXiv:2511.08017v1 Announce Type: new 
Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution</title>
<link>https://arxiv.org/abs/2511.08085</link>
<guid>https://arxiv.org/abs/2511.08085</guid>
<content:encoded><![CDATA[
<div> Keywords: Bangla, authorship attribution, stop-words, deep learning, benchmark

Summary:
- The research investigates Bangla authorship attribution using a new balanced benchmark corpus BARD10 and analyzes the impact of stop-word removal on classical and deep learning models.
- BARD10, containing Bangla blog and opinion prose from ten authors, is evaluated alongside the BAAD16 corpus using classifiers like SVM, Bangla BERT, XGBoost, and MLP.
- The classical TF-IDF + SVM baseline outperformed deep learning models on both datasets, suggesting the importance of finely calibrated ML models for short texts.
- Authors in BARD10 are more influenced by stop-word pruning compared to BAAD16, indicating genre-dependent reliance on stop-word signatures.
- Error analysis revealed that transformer models may diminish or reduce high-frequency components transmitting authorial signatures. The study provides insights on the significance of Bangla stop-words, the effectiveness of ML models in short-text scenarios, and the potential of BARD10 as a benchmark for future transformer models.


Summary: <div>
arXiv:2511.08085v1 Announce Type: new 
Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling</title>
<link>https://arxiv.org/abs/2511.08109</link>
<guid>https://arxiv.org/abs/2511.08109</guid>
<content:encoded><![CDATA[
<div> Keywords: science fiction, conceptual permeability, masked language modeling, ontological categories, genre-specific restructuring

Summary:
This paper explores how science fiction challenges traditional ontological boundaries between humans, animals, and machines using computational analysis. By applying masked language modeling to science fiction and general fiction corpora, the study measures the deviation in token predictions to quantify conceptual slippage. The findings show that science fiction exhibits higher conceptual permeability, especially around machine references, leading to significant cross-category substitutions and dispersion. In contrast, human terms maintain semantic coherence, anchoring substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics, where estrangement in science fiction serves as a controlled disruption of semantic norms. The study highlights the importance of using computational tools critically in literary analysis to uncover genre-conditioned ontological assumptions, contributing to a deeper understanding of the linguistic framework of science fiction. 

<br /><br />Summary: <div>
arXiv:2511.08109v1 Announce Type: new 
Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal LLMs Do Not Compose Skills Optimally Across Modalities</title>
<link>https://arxiv.org/abs/2511.08113</link>
<guid>https://arxiv.org/abs/2511.08113</guid>
<content:encoded><![CDATA[
<div> skill composition, neural networks, Multimodal Large Language Models, evaluation tasks, cross-modality

Summary:
This paper investigates the ability of Multimodal Large Language Models (MLLMs) to compose skills across modalities by designing evaluation tasks that require the composition of two modality-dependent skills. The study evaluates different MLLMs under two settings: direct task solving and cascaded inference. The results reveal a significant skill composition gap across modalities in all evaluated MLLMs. To address this gap, the study explores two strategies - chain-of-thought prompting and a specific fine-tuning recipe. While these strategies improve model performance, they still exhibit notable skill composition gaps, indicating the need for further research in enhancing cross-modal skill composition in MLLMs. <br /><br />Summary: <div>
arXiv:2511.08113v1 Announce Type: new 
Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition</title>
<link>https://arxiv.org/abs/2511.08126</link>
<guid>https://arxiv.org/abs/2511.08126</guid>
<content:encoded><![CDATA[
<div> quantification, Large Language Models, cross-linguistic, ordering, scales <br />
Summary:<br />
This paper explores the challenges faced by Large Language Models (LLMs) in understanding quantification, a complex linguistic phenomenon that interfaces with logic, pragmatics, and numerical domains. The study focuses on three key features of human quantification that have not been thoroughly investigated in LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and biases in the human approximate number system. By comparing human and LLM performance across different tasks, the researchers highlight clear differences in the representation of quantification. The findings suggest that LLMs may not fully capture the nuances of human quantification, raising questions about their semantic and pragmatic capabilities. The study also emphasizes the importance of considering cross-linguistic variations in LLM performance to assess their robustness and stability across diverse languages.<br /> <div>
arXiv:2511.08126v1 Announce Type: new 
Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentence-Anchored Gist Compression for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2511.08128</link>
<guid>https://arxiv.org/abs/2511.08128</guid>
<content:encoded><![CDATA[
<div> investigates, context compression, Large Language Models, learned compression tokens, memory reduction <br />
Summary:<br />
This work explores context compression for Large Language Models (LLMs) through the use of learned compression tokens to decrease memory and computational requirements for handling lengthy sequences. Pre-trained LLMs can be optimized to compress their context by ratios ranging from 2x to 8x with minimal impact on performance, evident in evaluations on short and long-context benchmarks. Additionally, experiments conducted on a 3-billion-parameter LLaMA model demonstrate that this approach achieves comparable results to other compression techniques while achieving higher compression ratios. <br /><br /> <div>
arXiv:2511.08128v1 Announce Type: new 
Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility</title>
<link>https://arxiv.org/abs/2511.08139</link>
<guid>https://arxiv.org/abs/2511.08139</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, positional encodings, morphological complexity, word order flexibility, downstream tasks

Summary: 
Language model architectures are often developed in English and then transferred to other languages. This study investigates the impact of this bias on languages that differ structurally from English. The focus is on positional encodings and the trade-off hypothesis, which suggests a relationship between morphological complexity and word order flexibility. Pretraining models with different positional encodings for seven diverse languages, the study examines their performance on various tasks. Contrary to expectations, the results do not show a clear connection between positional encodings and language characteristics. The study highlights the importance of considering tasks, languages, and metrics in drawing meaningful conclusions. The findings suggest that further research is needed to fully understand the implications of architectural choices in language modeling. 

Summary: <div>
arXiv:2511.08139v1 Announce Type: new 
Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction</title>
<link>https://arxiv.org/abs/2511.08143</link>
<guid>https://arxiv.org/abs/2511.08143</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Document-level Relation Extraction, Relation as a Prior, binary relation, triples extraction

Summary:
Large Language Models (LLMs) have shown great document understanding capabilities but struggle with Document-level Relation Extraction (DocRE) due to fine-grained comprehension challenges. The traditional approach of extracting entities before predicting relations introduces noise from unrelated entity pairs and struggles with predicting relations beyond predefined labels. To address these issues, a novel approach called Relation as a Prior (RelPrior) is proposed. RelPrior uses binary relations as a prior to filter out irrelevant entity pairs and predefined relations as a prior for triple extraction, avoiding misjudgments caused by strict predefined labels. Experimental results on two benchmarks show that RelPrior outperforms existing LLM-based methods, achieving state-of-the-art performance. <br /><br />Summary: <div>
arXiv:2511.08143v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?</title>
<link>https://arxiv.org/abs/2511.08145</link>
<guid>https://arxiv.org/abs/2511.08145</guid>
<content:encoded><![CDATA[
<div> instruction-tuned, in-context-prompted, Large Language Models (LLMs), Sanskrit poetry-to-prose conversion task, morphologically rich languages<br />
Summary:<br />
The study compares Large Language Models (LLMs) with smaller task-specific models on the Sanskrit poetry-to-prose conversion task. The task involves complex reasoning due to the unique characteristics of Sanskrit verse. Through domain-specific fine-tuning, the ByT5-Sanskrit model outperforms LLM approaches. Human evaluation supports this finding. In-context learning templates based on Paninian grammar offer an alternative to fine-tuning when specific corpora are lacking. The task-specific Seq2Seq model shows robust generalization in out-of-domain evaluations.<br /> <div>
arXiv:2511.08145v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?</title>
<link>https://arxiv.org/abs/2511.08199</link>
<guid>https://arxiv.org/abs/2511.08199</guid>
<content:encoded><![CDATA[
<div> Keywords: BabyLM corpus, syntactic properties, age-groups, CHILDES, curriculum learning

Summary: 
The study investigates the syntactic properties of the BabyLM corpus and different age-groups within the CHILDES dataset. It is observed that CHILDES does not show significant syntactic variation based on age. However, the study demonstrates that understanding the syntactic structure of the training data can aid in interpreting model performance in linguistic tasks. The research delves into curriculum learning, exploring developmental and other cognitively inspired curriculum strategies. While some curricula show improvement in reading tasks, the most substantial performance enhancement is achieved by utilizing a subset of syntactically classifiable data rather than the entire noisy corpus. The findings suggest the importance of considering syntactic knowledge when designing curriculum approaches for language learning tasks. <div>
arXiv:2511.08199v1 Announce Type: new 
Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction</title>
<link>https://arxiv.org/abs/2511.08204</link>
<guid>https://arxiv.org/abs/2511.08204</guid>
<content:encoded><![CDATA[
<div> extracting, knowledge, astronomy, transformer-based system, SciBERT

Summary: 
The paper discusses the need for automation in extracting key entities and contextual information from expanding scientific literature in astronomy. It presents a system based on an encoder model for extracting knowledge from astronomy articles. The system aims to classify telescope references, detect auxiliary semantic attributes, and recognize instrument mentions from textual content. The system is built upon the SciBERT model and fine-tuned for astronomy corpora classification. The fine-tuning process involves stochastically sampling segments from the training data and using majority voting over test segments at inference time. Despite its simplicity and low-cost implementation, the system significantly outperforms the open-weight GPT baseline in terms of performance. <div>
arXiv:2511.08204v1 Announce Type: new 
Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback</title>
<link>https://arxiv.org/abs/2511.08225</link>
<guid>https://arxiv.org/abs/2511.08225</guid>
<content:encoded><![CDATA[
<div> Keywords: benchmarking, large language models, bias detection, formative feedback, gender substitution

Summary: 
In this article, a benchmarking framework is proposed to detect bias in large language models (LLMs) used for educational purposes, particularly in providing formative feedback. The study utilized 600 student essays to analyze the response of six representative LLMs to gender substitutions in different contexts. The results indicated that there were asymmetric semantic responses to gender manipulations, with male-female substitutions inducing larger shifts than female-male. Only certain models showed sensitivity to explicit gender cues. Qualitative analysis revealed linguistic differences in feedback based on gender cues, such as more autonomy-supportive feedback under male prompts. The findings suggest persistent gender biases in the feedback provided by LLMs, highlighting the need for fairness auditing in pedagogical GenAI. The article proposes reporting standards for counterfactual evaluation in learning analytics and provides practical guidance for designing prompts to ensure equitable feedback.<br /><br />Summary: <div>
arXiv:2511.08225v1 Announce Type: new 
Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context</title>
<link>https://arxiv.org/abs/2511.08230</link>
<guid>https://arxiv.org/abs/2511.08230</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, Mandarin, speech-to-speech benchmarks, VocalBench-zh, evaluation experiment

Summary:
The article introduces the development of multi-modal large language models (LLMs) that facilitate speech interactions, focusing on Mandarin language support. However, the lack of comprehensive speech-to-speech benchmarks in Mandarin poses challenges for developers and users in evaluating and comparing models. To address this issue, the authors propose VocalBench-zh, an evaluation suite specifically designed for Mandarin contexts. This suite includes 10 subsets with over 10,000 high-quality instances covering 12 user-oriented characters. The evaluation experiment conducted on 14 mainstream models reveals common challenges and the need for advancements in next-generation speech interactive systems. The evaluation codes and datasets for VocalBench-zh are accessible on GitHub for researchers and developers interested in improving speech technology. 

<br /><br />Summary: <div>
arXiv:2511.08230v1 Announce Type: new 
Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</title>
<link>https://arxiv.org/abs/2511.08245</link>
<guid>https://arxiv.org/abs/2511.08245</guid>
<content:encoded><![CDATA[
<div> Keywords: Error Correction, NL-to-SQL, Prompt Tuning, NLIDBs, RAG

Summary:
This paper presents a novel Error Correction through Prompt Tuning approach for NL-to-SQL translation using the latest advancements in large language models (LLMs) and Retrieval Augmented Generation (RAG). The framework integrates error diagnosis, cause identification, fixing instructions, and correction application inspired by medical diagnostic processes. It incorporates fine-tuning and RAG to enhance accuracy and transparency by utilizing external knowledge bases. The evolution of NLIDBs from rule-based systems to neural network-driven approaches is explored. Through extensive experiments, the framework shows a notable 12 percent accuracy improvement over existing methods, indicating its potential to transform data access and management in contemporary data-driven environments. Overall, this research addresses the critical need for efficient and accurate translation of natural language queries into SQL expressions, offering promising advancements in the field of NL-to-SQL. 

<br /><br />Summary: <div>
arXiv:2511.08245v1 Announce Type: new 
Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</title>
<link>https://arxiv.org/abs/2511.08247</link>
<guid>https://arxiv.org/abs/2511.08247</guid>
<content:encoded><![CDATA[
<div> Dataset, ParliaBench, language models, political authenticity, evaluation framework <br />
<br />
Summary: <br />
The article introduces ParliaBench, a benchmark for parliamentary speech generation, which addresses challenges faced by large language models in producing authentic and ideologically consistent parliamentary speeches. A dataset of UK Parliament speeches was used to train models, and an evaluation framework was developed to assess linguistic quality, semantic coherence, and political authenticity. Two novel metrics, Political Spectrum Alignment and Party Alignment, were proposed to measure ideological positioning. Five language models were fine-tuned and evaluated, with results showing significant improvements in most metrics when compared to baseline models. The novel metrics demonstrated strong discriminative power in assessing political dimensions. <div>
arXiv:2511.08247v1 Announce Type: new 
Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments</title>
<link>https://arxiv.org/abs/2511.08298</link>
<guid>https://arxiv.org/abs/2511.08298</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Large Language Models, scientific tables, hierarchical structure, prompt engineering, performance evaluation

Summary:
This work investigates the ability of Vision Large Language Models (VLLMs) to understand the hierarchical structure of tables in scientific articles. The study uses the PubTables-1M dataset to create a benchmark collection of Complex Hierarchical Tables (CHiTab). Various prompt engineering strategies are employed to assess the models' comprehension capabilities. State-of-the-art VLLMs are evaluated on the CHiTab benchmark dataset, both in their off-the-shelf versions and after fine-tuning on the task. The study also compares the performance of humans on the task with that of the evaluated VLLMs. Results suggest that generic VLLMs, not specifically designed for table structure understanding, can successfully infer hierarchical table structures. The findings offer insights into the potential and limitations of VLLMs in processing complex tables and provide guidance for future research on incorporating structured data understanding into general-purpose VLLMs.<br /><br />Summary: <div>
arXiv:2511.08298v1 Announce Type: new 
Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates</title>
<link>https://arxiv.org/abs/2511.08317</link>
<guid>https://arxiv.org/abs/2511.08317</guid>
<content:encoded><![CDATA[
<div> framework, ReViewGraph, heterogeneous graph reasoning, LLM simulated debates, argumentative dynamics

Summary:
ReViewGraph is introduced as a new framework to enhance paper review methods by incorporating heterogeneous graph reasoning over LLM-simulated reviewer-author debates. The framework simulates reviewer-author exchanges using LLM-based multi-agent collaboration and extracts diverse opinion relations as typed edges in a structured debate graph. By utilizing graph neural networks, ReViewGraph captures detailed argumentative dynamics, enabling more informed review decisions. Experimental results on three datasets show that ReViewGraph outperforms strong baselines by an average relative improvement of 15.73%. This highlights the importance of modeling intricate reviewer-author debate structures in improving the paper review process. <div>
arXiv:2511.08317v1 Announce Type: new 
Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-Agent Response Refinement in Conversational Systems</title>
<link>https://arxiv.org/abs/2511.08319</link>
<guid>https://arxiv.org/abs/2511.08319</guid>
<content:encoded><![CDATA[
<div> refining responses, large language models, multi-agent framework, conversational quality, dynamic communication strategy  
Summary:  
- Large Language Models (LLMs) have been successful in generating human-like responses but can struggle with personalization and specific knowledge.
- Responding to this limitation, a multi-agent framework is proposed to refine responses through aspects such as factuality, personalization, and coherence.
- Each agent is assigned a specific role for one of these aspects, with their feedback merged to enhance the overall response.
- A dynamic communication strategy is introduced to improve collaboration among agents by adaptively selecting and coordinating based on query requirements.
- Validation on conversational datasets showed significant performance improvement over relevant baselines, especially in tasks involving knowledge or user persona.  
<br /><br />Summary: <div>
arXiv:2511.08319v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress</title>
<link>https://arxiv.org/abs/2511.08325</link>
<guid>https://arxiv.org/abs/2511.08325</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, decision-making tasks, process reward models, Generalized Advantage Estimation

Summary: 
The article introduces a new approach for improving decision-making in large language models (LLMs) in agent tasks. It proposes the use of process reward models (PRMs) called AgentPRM to evaluate decisions based on progress towards the goal rather than correctness. This new approach aims to better capture the interdependence between sequential decisions and their contribution to the final goal, leading to improved progress tracking and exploration-exploitation balance. To train AgentPRM efficiently, a Temporal Difference-based estimation method combined with Generalized Advantage Estimation (GAE) is employed. Experimental results show that AgentPRM is significantly more compute-efficient than baselines, with over 8 times improvement. The method also demonstrates robust performance when scaling up test-time compute. The study includes detailed analyses and insights on applying AgentPRM to reinforcement learning of LLM agents. 

<br /><br />Summary: <div>
arXiv:2511.08325v1 Announce Type: new 
Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2511.08364</link>
<guid>https://arxiv.org/abs/2511.08364</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-hop question answering, Chain of Thought, Knowledge Graphs, Outcome Reward Models, Implicit Process Reward Model

Summary:
In this article, the authors introduce a novel approach called Dual Implicit Process Reward Model (DPRM) to enhance multi-hop question answering tasks. The DPRM consists of two implicit Process Reward Models (PRMs) - KG-PRM and CoT-PRM, which derive step-level rewards without explicit annotations. KG-PRM utilizes preference pairs to learn structural constraints from Knowledge Graphs (KGs), while CoT-PRM focuses on multi-step reasoning guided by Chain of Thought. The DPRM integrates a consistency constraint to ensure coherence between CoT and KG reasoning paths. Theoretical analysis demonstrates the derivation of process rewards in the model. Experimental results exhibit a significant performance improvement compared to 13 baseline methods, with up to a 16.6% increase in Hit@1 metric on multiple datasets. <div>
arXiv:2511.08364v1 Announce Type: new 
Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures</title>
<link>https://arxiv.org/abs/2511.08372</link>
<guid>https://arxiv.org/abs/2511.08372</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic articulatory model, speech gestures, gesture score, neurobiology, speech production

Summary: 
The paper introduces the dynamic articulatory model DYNARTmo, which simulates speech production through continuous articulator movements based on speech gestures and a corresponding gesture score. The model is designed to mimic the hierarchical control of speech production in a neurobiologically inspired computational framework. The structure of the gesture inventory and the coordination of gestures within the gesture score are outlined, demonstrating how they translate into continuous articulator trajectories that control the DYNARTmo vocal tract model. This implementation provides a comprehensive approach to understanding and modeling speech production, bridging the gap between linguistic representation and articulatory-acoustic realization. <div>
arXiv:2511.08372v1 Announce Type: new 
Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurkEmbed: Turkish Embedding Model on NLI &amp; STS Tasks</title>
<link>https://arxiv.org/abs/2511.08376</link>
<guid>https://arxiv.org/abs/2511.08376</guid>
<content:encoded><![CDATA[
<div> Keywords: TurkEmbed, Turkish language embedding model, Natural Language Inference, Semantic Textual Similarity, NLP ecosystem

Summary: 
TurkEmbed is a novel Turkish language embedding model designed to excel in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Unlike existing models that rely on machine-translated datasets, TurkEmbed utilizes diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach allows the model to adapt to various resource-constrained environments and offer faster encoding capabilities. Evaluation on the Turkish STS-b-TR dataset shows significant improvements in semantic similarity tasks, surpassing the current state-of-the-art model Emrecan on All-NLI-TR and STS-b-TR benchmarks by 1-4%. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a deeper understanding of language and enabling advancements in downstream applications. 

<br /><br />Summary: <div>
arXiv:2511.08376v1 Announce Type: new 
Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints</title>
<link>https://arxiv.org/abs/2511.08392</link>
<guid>https://arxiv.org/abs/2511.08392</guid>
<content:encoded><![CDATA[
<div> Proof-Carrying Reasoning, Large Language Models, single-step inferences, logical coherence, validation
Summary:
Proof-Carrying Reasoning with Large Language Models (PCRLLM) is a framework proposed to improve the logical coherence of LLMs by constraining reasoning to single-step inferences. This approach ensures that each output explicitly specifies premises, rules, and conclusions, allowing for verification against a target logic and addressing trustworthiness concerns in black-box settings. PCRLLM also enables systematic multi-LLM collaboration, facilitating the comparison and integration of intermediate reasoning steps under formal rules. Additionally, a benchmark schema for generating large-scale step-level reasoning data is introduced, combining natural language expressiveness with formal rigor. Through these mechanisms, PCRLLM aims to enhance the reliability and transparency of reasoning processes in LLMs. 
<br /><br />Summary: <div>
arXiv:2511.08392v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction Dynamics as a Reward Signal for LLMs</title>
<link>https://arxiv.org/abs/2511.08394</link>
<guid>https://arxiv.org/abs/2511.08394</guid>
<content:encoded><![CDATA[
<div> reward, Large Language Models, dialogue embedding, conversational geometry, agent collaboration

Summary:
- The paper introduces TRACE, a reward signal based on conversational geometry, which considers the dynamics of a dialogue's embedding trajectory as a source of signal for aligning Large Language Models (LLMs) in multi-turn conversations.
- A reward model trained solely on structural signals achieves a pairwise accuracy of 68.20%, comparable to a powerful LLM baseline analyzing the full transcript.
- The hybrid model combining interaction dynamics with textual analysis achieves the highest performance at 80.17%, highlighting the complementary nature of these approaches.
- The study shows that how an agent communicates is as crucial for success as what it says in interactive settings.
- This approach offers a privacy-preserving framework for aligning agents and serves as a diagnostic tool to understand the unique interaction patterns that drive successful collaboration. 

<br /><br />Summary: <div>
arXiv:2511.08394v1 Announce Type: new 
Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?</title>
<link>https://arxiv.org/abs/2511.08455</link>
<guid>https://arxiv.org/abs/2511.08455</guid>
<content:encoded><![CDATA[
arXiv:2511.08455v1 Announce Type: new 
Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation</title>
<link>https://arxiv.org/abs/2511.08500</link>
<guid>https://arxiv.org/abs/2511.08500</guid>
<content:encoded><![CDATA[
arXiv:2511.08500v1 Announce Type: new 
Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured RAG for Answering Aggregative Questions</title>
<link>https://arxiv.org/abs/2511.08505</link>
<guid>https://arxiv.org/abs/2511.08505</guid>
<content:encoded><![CDATA[
arXiv:2511.08505v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research</title>
<link>https://arxiv.org/abs/2511.08507</link>
<guid>https://arxiv.org/abs/2511.08507</guid>
<content:encoded><![CDATA[
arXiv:2511.08507v1 Announce Type: new 
Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlphaResearch: Accelerating New Algorithm Discovery with Language Models</title>
<link>https://arxiv.org/abs/2511.08522</link>
<guid>https://arxiv.org/abs/2511.08522</guid>
<content:encoded><![CDATA[
arXiv:2511.08522v1 Announce Type: new 
Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating CoT Monitorability in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.08525</link>
<guid>https://arxiv.org/abs/2511.08525</guid>
<content:encoded><![CDATA[
arXiv:2511.08525v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL</title>
<link>https://arxiv.org/abs/2511.08537</link>
<guid>https://arxiv.org/abs/2511.08537</guid>
<content:encoded><![CDATA[
arXiv:2511.08537v1 Announce Type: new 
Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models</title>
<link>https://arxiv.org/abs/2511.08565</link>
<guid>https://arxiv.org/abs/2511.08565</guid>
<content:encoded><![CDATA[
arXiv:2511.08565v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.08577</link>
<guid>https://arxiv.org/abs/2511.08577</guid>
<content:encoded><![CDATA[
arXiv:2511.08577v1 Announce Type: new 
Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Explain Their Own Computations</title>
<link>https://arxiv.org/abs/2511.08579</link>
<guid>https://arxiv.org/abs/2511.08579</guid>
<content:encoded><![CDATA[
arXiv:2511.08579v1 Announce Type: new 
Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation</title>
<link>https://arxiv.org/abs/2511.06254</link>
<guid>https://arxiv.org/abs/2511.06254</guid>
<content:encoded><![CDATA[
arXiv:2511.06254v1 Announce Type: cross 
Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network and Systems Performance Characterization of MCP-Enabled LLM Agents</title>
<link>https://arxiv.org/abs/2511.07426</link>
<guid>https://arxiv.org/abs/2511.07426</guid>
<content:encoded><![CDATA[
arXiv:2511.07426v1 Announce Type: cross 
Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Polite Liar: Epistemic Pathology in Language Models</title>
<link>https://arxiv.org/abs/2511.07477</link>
<guid>https://arxiv.org/abs/2511.07477</guid>
<content:encoded><![CDATA[
arXiv:2511.07477v1 Announce Type: cross 
Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title>
<link>https://arxiv.org/abs/2511.07482</link>
<guid>https://arxiv.org/abs/2511.07482</guid>
<content:encoded><![CDATA[
arXiv:2511.07482v1 Announce Type: cross 
Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Impact of CU: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2511.07491</link>
<guid>https://arxiv.org/abs/2511.07491</guid>
<content:encoded><![CDATA[
arXiv:2511.07491v1 Announce Type: cross 
Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</title>
<link>https://arxiv.org/abs/2511.07577</link>
<guid>https://arxiv.org/abs/2511.07577</guid>
<content:encoded><![CDATA[
arXiv:2511.07577v1 Announce Type: cross 
Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models</title>
<link>https://arxiv.org/abs/2511.07581</link>
<guid>https://arxiv.org/abs/2511.07581</guid>
<content:encoded><![CDATA[
arXiv:2511.07581v1 Announce Type: cross 
Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title>
<link>https://arxiv.org/abs/2511.07585</link>
<guid>https://arxiv.org/abs/2511.07585</guid>
<content:encoded><![CDATA[
arXiv:2511.07585v1 Announce Type: cross 
Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces</title>
<link>https://arxiv.org/abs/2511.07587</link>
<guid>https://arxiv.org/abs/2511.07587</guid>
<content:encoded><![CDATA[
arXiv:2511.07587v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</title>
<link>https://arxiv.org/abs/2511.07685</link>
<guid>https://arxiv.org/abs/2511.07685</guid>
<content:encoded><![CDATA[
arXiv:2511.07685v1 Announce Type: cross 
Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViPRA: Video Prediction for Robot Actions</title>
<link>https://arxiv.org/abs/2511.07732</link>
<guid>https://arxiv.org/abs/2511.07732</guid>
<content:encoded><![CDATA[
arXiv:2511.07732v1 Announce Type: cross 
Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title>
<link>https://arxiv.org/abs/2511.07772</link>
<guid>https://arxiv.org/abs/2511.07772</guid>
<content:encoded><![CDATA[
arXiv:2511.07772v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis</title>
<link>https://arxiv.org/abs/2511.07790</link>
<guid>https://arxiv.org/abs/2511.07790</guid>
<content:encoded><![CDATA[
arXiv:2511.07790v1 Announce Type: cross 
Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost</title>
<link>https://arxiv.org/abs/2511.07865</link>
<guid>https://arxiv.org/abs/2511.07865</guid>
<content:encoded><![CDATA[
arXiv:2511.07865v1 Announce Type: cross 
Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation</title>
<link>https://arxiv.org/abs/2511.07876</link>
<guid>https://arxiv.org/abs/2511.07876</guid>
<content:encoded><![CDATA[
arXiv:2511.07876v1 Announce Type: cross 
Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</title>
<link>https://arxiv.org/abs/2511.07885</link>
<guid>https://arxiv.org/abs/2511.07885</guid>
<content:encoded><![CDATA[
arXiv:2511.07885v1 Announce Type: cross 
Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2511.07896</link>
<guid>https://arxiv.org/abs/2511.07896</guid>
<content:encoded><![CDATA[
arXiv:2511.07896v1 Announce Type: cross 
Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpeechJudge: Towards Human-Level Judgment for Speech Naturalness</title>
<link>https://arxiv.org/abs/2511.07931</link>
<guid>https://arxiv.org/abs/2511.07931</guid>
<content:encoded><![CDATA[
arXiv:2511.07931v1 Announce Type: cross 
Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction</title>
<link>https://arxiv.org/abs/2511.07943</link>
<guid>https://arxiv.org/abs/2511.07943</guid>
<content:encoded><![CDATA[
arXiv:2511.07943v1 Announce Type: cross 
Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives</title>
<link>https://arxiv.org/abs/2511.08029</link>
<guid>https://arxiv.org/abs/2511.08029</guid>
<content:encoded><![CDATA[
arXiv:2511.08029v1 Announce Type: cross 
Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</title>
<link>https://arxiv.org/abs/2511.08043</link>
<guid>https://arxiv.org/abs/2511.08043</guid>
<content:encoded><![CDATA[
arXiv:2511.08043v1 Announce Type: cross 
Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging</title>
<link>https://arxiv.org/abs/2511.08052</link>
<guid>https://arxiv.org/abs/2511.08052</guid>
<content:encoded><![CDATA[
arXiv:2511.08052v1 Announce Type: cross 
Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression</title>
<link>https://arxiv.org/abs/2511.08066</link>
<guid>https://arxiv.org/abs/2511.08066</guid>
<content:encoded><![CDATA[
arXiv:2511.08066v1 Announce Type: cross 
Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR</title>
<link>https://arxiv.org/abs/2511.08092</link>
<guid>https://arxiv.org/abs/2511.08092</guid>
<content:encoded><![CDATA[
arXiv:2511.08092v1 Announce Type: cross 
Abstract: We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantizing Whisper-small: How design choices affect ASR performance</title>
<link>https://arxiv.org/abs/2511.08093</link>
<guid>https://arxiv.org/abs/2511.08093</guid>
<content:encoded><![CDATA[
arXiv:2511.08093v1 Announce Type: cross 
Abstract: Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision</title>
<link>https://arxiv.org/abs/2511.08098</link>
<guid>https://arxiv.org/abs/2511.08098</guid>
<content:encoded><![CDATA[
arXiv:2511.08098v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning</title>
<link>https://arxiv.org/abs/2511.08151</link>
<guid>https://arxiv.org/abs/2511.08151</guid>
<content:encoded><![CDATA[
arXiv:2511.08151v1 Announce Type: cross 
Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents</title>
<link>https://arxiv.org/abs/2511.08242</link>
<guid>https://arxiv.org/abs/2511.08242</guid>
<content:encoded><![CDATA[
arXiv:2511.08242v1 Announce Type: cross 
Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs</title>
<link>https://arxiv.org/abs/2511.08274</link>
<guid>https://arxiv.org/abs/2511.08274</guid>
<content:encoded><![CDATA[
arXiv:2511.08274v1 Announce Type: cross 
Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Selective State Space Artificial Intelligence</title>
<link>https://arxiv.org/abs/2511.08349</link>
<guid>https://arxiv.org/abs/2511.08349</guid>
<content:encoded><![CDATA[
arXiv:2511.08349v1 Announce Type: cross 
Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Model and Layer Fusion for Speech Foundation Models</title>
<link>https://arxiv.org/abs/2511.08389</link>
<guid>https://arxiv.org/abs/2511.08389</guid>
<content:encoded><![CDATA[
arXiv:2511.08389v1 Announce Type: cross 
Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks?</title>
<link>https://arxiv.org/abs/2511.08461</link>
<guid>https://arxiv.org/abs/2511.08461</guid>
<content:encoded><![CDATA[
arXiv:2511.08461v1 Announce Type: cross 
Abstract: As Artificial Intelligence (AI) is increasingly promoted and used in qualitative research, it also raises profound methodological issues. This position paper critically interrogates the role of generative AI (genAI) in the context of qualitative coding methodologies. Despite widespread hype and claims of efficiency, we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness of qualitative research. The lack of meaningful documentation, commercial opacity, and the inherent tendencies of genAI systems to produce incorrect outputs all contribute to weakening methodological rigor. Overall, the balance between risk and benefits does not support the use of genAI in qualitative research, and our position paper cautions researchers to put sound methodology before technological novelty.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity</title>
<link>https://arxiv.org/abs/2511.08487</link>
<guid>https://arxiv.org/abs/2511.08487</guid>
<content:encoded><![CDATA[
arXiv:2511.08487v1 Announce Type: cross 
Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form QA</title>
<link>https://arxiv.org/abs/2408.09235</link>
<guid>https://arxiv.org/abs/2408.09235</guid>
<content:encoded><![CDATA[
arXiv:2408.09235v3 Announce Type: replace 
Abstract: The emergence of Large Language Models (LLMs) as chat assistants capable of generating human-like conversations has amplified the need for robust evaluation methods, particularly for open-ended tasks. Conventional metrics such as EM and F1, while useful, are inadequate for capturing the full semantics and contextual depth of such generative outputs. We propose a reference-guided verdict method that automates the evaluation process by leveraging multiple LLMs as judges. Through experiments on free-form question-answering tasks, we demonstrate that combining multiple models improves the reliability and accuracy of evaluations, especially in tasks where a single model may struggle. The results indicate a strong correlation with human evaluations, establishing the proposed method as a reliable alternative to traditional metrics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combining LLMs and Knowledge Graphs to Reduce Hallucinations in Question Answering</title>
<link>https://arxiv.org/abs/2409.04181</link>
<guid>https://arxiv.org/abs/2409.04181</guid>
<content:encoded><![CDATA[
arXiv:2409.04181v3 Announce Type: replace 
Abstract: Advancements in natural language processing have revolutionized the way we can interact with digital information systems, such as databases, making them more accessible. However, challenges persist, especially when accuracy is critical, as in the biomedical domain. A key issue is the hallucination problem, where models generate information unsupported by the underlying data, potentially leading to dangerous misinformation. This paper presents a novel approach designed to bridge this gap by combining Large Language Models (LLM) and Knowledge Graphs (KG) to improve the accuracy and reliability of question-answering systems, on the example of a biomedical KG. Built on the LangChain framework, our method incorporates a query checker that ensures the syntactical and semantic validity of LLM-generated queries, which are then used to extract information from a Knowledge Graph, substantially reducing errors like hallucinations. We evaluated the overall performance using a new benchmark dataset of 50 biomedical questions, testing several LLMs, including GPT-4 Turbo and llama3:70b. Our results indicate that while GPT-4 Turbo outperforms other models in generating accurate queries, open-source models like llama3:70b show promise with appropriate prompt engineering. To make this approach accessible, a user-friendly web-based interface has been developed, allowing users to input natural language queries, view generated and corrected Cypher queries, and verify the resulting paths for accuracy. Overall, this hybrid approach effectively addresses common issues such as data gaps and hallucinations, offering a reliable and intuitive solution for question answering systems. The source code for generating the results of this paper and for the user-interface can be found in our Git repository: https://git.zib.de/lpusch/cyphergenkg-gui
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selection of LLM Fine-Tuning Data based on Orthogonal Rules</title>
<link>https://arxiv.org/abs/2410.04715</link>
<guid>https://arxiv.org/abs/2410.04715</guid>
<content:encoded><![CDATA[
arXiv:2410.04715v3 Announce Type: replace 
Abstract: High-quality training data is critical to the performance of large language models (LLMs). Recent work has explored using LLMs to rate and select data based on a small set of human-designed criteria (rules), but these approaches often rely heavily on heuristics, lack principled metrics for rule evaluation, and generalize poorly to new tasks. We propose a novel rule-based data selection framework that introduces a metric based on the orthogonality of rule score vectors to evaluate and select complementary rules. Our automated pipeline first uses LLMs to generate diverse rules covering multiple aspects of data quality, then rates samples according to these rules and applies the determinantal point process (DPP) to select the most independent rules. These rules are then used to score the full dataset, and high-scoring samples are selected for downstream tasks such as LLM fine-tuning. We evaluate our framework in two experiment setups: (1) alignment with ground-truth ratings and (2) performance of LLMs fine-tuned on the selected data. Experiments across IMDB, Medical, Math, and Code domains demonstrate that our DPP-based rule selection consistently improves both rating accuracy and downstream model performance over strong baselines.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use</title>
<link>https://arxiv.org/abs/2410.16400</link>
<guid>https://arxiv.org/abs/2410.16400</guid>
<content:encoded><![CDATA[
arXiv:2410.16400v2 Announce Type: replace 
Abstract: While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Word Vectors to Multimodal Embeddings: Techniques, Applications, and Future Directions For Large Language Models</title>
<link>https://arxiv.org/abs/2411.05036</link>
<guid>https://arxiv.org/abs/2411.05036</guid>
<content:encoded><![CDATA[
arXiv:2411.05036v2 Announce Type: replace 
Abstract: Word embeddings and language models have transformed natural language processing (NLP) by facilitating the representation of linguistic elements in continuous vector spaces. This review visits foundational concepts such as the distributional hypothesis and contextual similarity, tracing the evolution from sparse representations like one-hot encoding to dense embeddings including Word2Vec, GloVe, and fastText. We examine both static and contextualized embeddings, underscoring advancements in models such as ELMo, BERT, and GPT and their adaptations for cross-lingual and personalized applications. The discussion extends to sentence and document embeddings, covering aggregation methods and generative topic models, along with the application of embeddings in multimodal domains, including vision, robotics, and cognitive science. Advanced topics such as model compression, interpretability, numerical encoding, and bias mitigation are analyzed, addressing both technical challenges and ethical implications. Additionally, we identify future research directions, emphasizing the need for scalable training techniques, enhanced interpretability, and robust grounding in non-textual modalities. By synthesizing current methodologies and emerging trends, this survey offers researchers and practitioners an in-depth resource to push the boundaries of embedding-based language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aspect-Oriented Summarization for Psychiatric Short-Term Readmission Prediction</title>
<link>https://arxiv.org/abs/2502.10388</link>
<guid>https://arxiv.org/abs/2502.10388</guid>
<content:encoded><![CDATA[
arXiv:2502.10388v2 Announce Type: replace 
Abstract: Recent progress in large language models (LLMs) has enabled the automated processing of lengthy documents even without supervised training on a task-specific dataset. Yet, their zero-shot performance in complex tasks as opposed to straightforward information extraction tasks remains suboptimal. One feasible approach for tasks with lengthy, complex input is to first summarize the document and then apply supervised fine-tuning to the summary. However, the summarization process inevitably results in some loss of information. In this study we present a method for processing the summaries of long documents aimed to capture different important aspects of the original document. We hypothesize that LLM summaries generated with different aspect-oriented prompts contain different information signals, and we propose methods to measure these differences. We introduce approaches to effectively integrate signals from these different summaries for supervised training of transformer models. We validate our hypotheses on a high-impact task -- 30-day readmission prediction from a psychiatric discharge -- using real-world data from four hospitals, and show that our proposed method increases the prediction performance for the complex task of predicting patient outcome.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thus Spake Long-Context Large Language Model</title>
<link>https://arxiv.org/abs/2502.17129</link>
<guid>https://arxiv.org/abs/2502.17129</guid>
<content:encoded><![CDATA[
arXiv:2502.17129v2 Announce Type: replace 
Abstract: Long context is an important topic in Natural Language Processing (NLP), running through the development of NLP architectures, and offers immense opportunities for Large Language Models (LLMs), giving LLMs the lifelong learning potential akin to humans. Unfortunately, the pursuit of a long context is accompanied by numerous obstacles. Nevertheless, long context remains a core competitive advantage for LLMs. In the past two years, the context length of LLMs has achieved a breakthrough extension to millions of tokens. Moreover, research on long-context LLMs has expanded beyond length extrapolation to a comprehensive focus on architecture, infrastructure, training, and evaluation technologies.
  Inspired by the symphonic poem, Thus Spake Zarathustra, we draw an analogy between the journey of extending the context of LLM and the attempts of humans to transcend their mortality. In this survey, we will illustrate how LLM struggles between the tremendous need for a longer context and its equal need to accept the fact that it is ultimately finite. To achieve this, we give a global picture of the lifecycle of long-context LLMs from four perspectives: architecture, infrastructure, training, and evaluation, showcasing the full spectrum of long-context technologies. At the end of this survey, we will present 10 unanswered questions currently faced by long-context LLMs. We hope this survey can serve as a systematic introduction to research on long-context LLMs.
  Video: https://www.bilibili.com/video/BV11h9AYoEYj.
  Github: https://github.com/OpenMOSS/Thus-Spake-Long-Context-LLM.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Figurative Archive: an open dataset and web-based application for the study of metaphor</title>
<link>https://arxiv.org/abs/2503.00444</link>
<guid>https://arxiv.org/abs/2503.00444</guid>
<content:encoded><![CDATA[
arXiv:2503.00444v3 Announce Type: replace 
Abstract: Research on metaphor has steadily increased over the last decades, as this phenomenon opens a window into a range of linguistic and cognitive processes. At the same time, the demand for rigorously constructed and extensively normed experimental materials increased as well. Here, we present the Figurative Archive, an open database of 996 metaphors in Italian enriched with rating and corpus-based measures (from familiarity to semantic distance and preferred interpretations), derived by collecting stimuli used across 11 studies. It includes both everyday and literary metaphors, varying in structure and semantic domains, and is validated based on correlations between familiarity and other measures. The Archive has several aspects of novelty: it is increased in size compared to previous resources; it offers a measure of metaphor inclusiveness, to comply with recommendations for non-discriminatory language use; it is displayed in a web-based interface, with features for a customized consultation. We provide guidelines for using the Archive to source materials for studies investigating metaphor processing and relationships between metaphor features in humans and computational models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLEV: LLM-Based Evaluation Through Lightweight Efficient Voting for Free-Form Question-Answering</title>
<link>https://arxiv.org/abs/2503.08542</link>
<guid>https://arxiv.org/abs/2503.08542</guid>
<content:encoded><![CDATA[
arXiv:2503.08542v2 Announce Type: replace 
Abstract: Evaluating free-form Question Answering (QA) remains a challenge due to its diverse and open-ended nature. Traditional automatic metrics fail to capture semantic equivalence or accommodate the variability of open-ended responses. Leveraging Large Language Models (LLMs) as evaluators offers a promising alternative due to their strong language understanding and instruction-following capabilities. We propose Consensus via Lightweight Efficient Voting (CLEV), which employs two primary LLMs as judges and invokes a third judge only in cases of disagreement. This approach prioritizes evaluation reliability while reducing unnecessary computational demands. Through experiments, including human evaluation, we demonstrate CLEV's ability to provide consistent, scalable, and resource-efficient assessments, establishing it as a robust framework for evaluating LLMs on free-form QA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
arXiv:2503.20797v3 Announce Type: replace 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENCORE: Entropy-guided Reward Composition for Multi-head Safety Reward Models</title>
<link>https://arxiv.org/abs/2503.20995</link>
<guid>https://arxiv.org/abs/2503.20995</guid>
<content:encoded><![CDATA[
arXiv:2503.20995v2 Announce Type: replace 
Abstract: The safety alignment of large language models (LLMs) often relies on reinforcement learning from human feedback (RLHF), which requires human annotations to construct preference datasets. Given the challenge of assigning overall quality scores to data, recent works increasingly adopt fine-grained ratings based on multiple safety rules. In this paper, we discover a robust phenomenon: Rules with higher rating entropy tend to have lower accuracy in distinguishing human-preferred responses. Exploiting this insight, we propose ENCORE, a simple entropy-guided method to compose multi-head rewards by penalizing rules with high rating entropy. Theoretically, we show that such rules yield negligible weights under the Bradley-Terry loss during weight optimization, naturally justifying their penalization. Empirically, ENCORE consistently outperforms strong baselines, including random and uniform weighting, single-head Bradley-Terry, and LLM-as-a-judge, etc. on RewardBench safety tasks. Our method is completely training-free, generally applicable across datasets, and retains interpretability, making it a practical and effective approach for multi-attribute reward modeling.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CONGRAD:Conflicting Gradient Filtering for Multilingual Preference Alignment</title>
<link>https://arxiv.org/abs/2503.23777</link>
<guid>https://arxiv.org/abs/2503.23777</guid>
<content:encoded><![CDATA[
arXiv:2503.23777v2 Announce Type: replace 
Abstract: Naive joint training of large language models (LLMs) for multilingual preference alignment can suffer from negative interference. This is a known issue in multilingual training, where conflicting objectives degrade overall performance. However, the impact of this phenomenon in the context of multilingual preference alignment remains largely underexplored. To address this issue, we propose CONGRAD, a scalable and effective filtering method that selects high-quality preference samples with minimal gradient conflicts across languages. Our method leverages gradient surgery to retain samples aligned with an aggregated multilingual update direction. Additionally, we incorporate a sublinear gradient compression strategy that reduces memory overhead during gradient accumulation. We integrate CONGRAD into self-rewarding framework and evaluate on LLaMA3-8B and Gemma2-2B across 10 languages. Results show that CONGRAD consistently outperforms strong baselines in both seen and unseen languages, with minimal alignment tax.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-1: Safer Alignment of Reasoning LLMs with 1K Data</title>
<link>https://arxiv.org/abs/2504.01903</link>
<guid>https://arxiv.org/abs/2504.01903</guid>
<content:encoded><![CDATA[
arXiv:2504.01903v2 Announce Type: replace 
Abstract: This paper introduces STAR-1, a high-quality, just-1k-scale safety dataset specifically designed for large reasoning models (LRMs) like DeepSeek-R1. Built on three core principles -- diversity, deliberative reasoning, and rigorous filtering -- STAR-1 aims to address the critical needs for safety alignment in LRMs. Specifically, we begin by integrating existing open-source safety datasets from diverse sources. Then, we curate safety policies to generate policy-grounded deliberative reasoning samples. Lastly, we apply a GPT-4o-based safety scoring system to select training examples aligned with best practices. Experimental results show that fine-tuning LRMs with STAR-1 leads to an average 40% improvement in safety performance across four benchmarks, while only incurring a marginal decrease (e.g., an average of 1.1%) in reasoning ability measured across five reasoning tasks. Extensive ablation studies further validate the importance of our design principles in constructing STAR-1 and analyze its efficacy across both LRMs and traditional LLMs. Our project page is https://ucsc-vlaa.github.io/STAR-1.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContrastScore: Towards Higher Quality, Less Biased, More Efficient Evaluation Metrics with Contrastive Evaluation</title>
<link>https://arxiv.org/abs/2504.02106</link>
<guid>https://arxiv.org/abs/2504.02106</guid>
<content:encoded><![CDATA[
arXiv:2504.02106v2 Announce Type: replace 
Abstract: Evaluating the quality of generated text automatically remains a significant challenge. Conventional reference-based metrics have been shown to exhibit relatively weak correlation with human evaluations. Recent research advocates the use of large language models (LLMs) as source-based metrics for natural language generation (NLG) assessment. While promising, LLM-based metrics, particularly those using smaller models, still fall short in aligning with human judgments. In this work, we introduce ContrastScore, a contrastive evaluation metric designed to enable higher-quality, less biased, and more efficient assessment of generated text. We evaluate ContrastScore on two NLG tasks: machine translation and summarization. Experimental results show that ContrastScore consistently achieves stronger correlation with human judgments than both single-model and ensemble-based baselines. Notably, ContrastScore based on Qwen 3B and 0.5B even outperforms Qwen 7B, despite having only half as many parameters, demonstrating its efficiency. Furthermore, it effectively mitigates common evaluation biases such as length and likelihood preferences, resulting in more robust automatic evaluation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</title>
<link>https://arxiv.org/abs/2504.14707</link>
<guid>https://arxiv.org/abs/2504.14707</guid>
<content:encoded><![CDATA[
arXiv:2504.14707v2 Announce Type: replace 
Abstract: Standard topic models often struggle to capture culturally specific nuances in text. This study evaluates the effectiveness of contextual embeddings for identifying culturally resonant themes in an underrepresented linguistic context. We compare the performance of KMeans Clustering, Latent Dirichlet Allocation (LDA), and BERTopic on a corpus of nearly 25,000 daily personal narratives written in Belgian-Dutch (Flemish). While LDA achieves strong performance on automated coherence metrics, subsequent human evaluation reveals that BERTopic consistently identifies the most coherent and culturally relevant topics, highlighting the limitations of purely statistical methods on this narrative-rich data. Furthermore, the diminished performance of K-Means compared to prior work on similar Dutch corpora underscores the unique linguistic challenges posed by personal narrative analysis. Our findings demonstrate the critical role of contextual embeddings in robust topic modeling and emphasize the need for human-centered evaluation, particularly when working with low-resource languages and culturally specific domains.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.00028</link>
<guid>https://arxiv.org/abs/2505.00028</guid>
<content:encoded><![CDATA[
arXiv:2505.00028v2 Announce Type: replace 
Abstract: End-to-end speech-to-speech (S2S) dialogue systems have recently garnered increasing research attention for their lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration of information. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind the SOTA cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. Our code and dataset are released.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v3 Announce Type: replace 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning. E.g. they can fail to generalize to simple reversals of relations they are trained on, or fail to make simple logical deductions based on trained information. These failures to generalize factual information from fine-tuning can significantly hinder the reasoning capabilities of these models. On the other hand, language models' in-context learning (ICL) shows different inductive biases and deductive reasoning capabilities. Here, we explore these differences in generalization and deductive reasoning between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' abilities to make generalizations over factual information from novel data. These datasets are designed to create clean tests of generalization, by isolating the knowledge in the dataset from that in pretraining. We expose pretrained large models to controlled subsets of the information in these datasets -- either through ICL or fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, ICL can generalize several types of inferences more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context reasoning traces to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the generalization afforded by different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2505.11225</link>
<guid>https://arxiv.org/abs/2505.11225</guid>
<content:encoded><![CDATA[
arXiv:2505.11225v2 Announce Type: replace 
Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.12345</link>
<guid>https://arxiv.org/abs/2505.12345</guid>
<content:encoded><![CDATA[
arXiv:2505.12345v3 Announce Type: replace 
Abstract: Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.15683</link>
<guid>https://arxiv.org/abs/2505.15683</guid>
<content:encoded><![CDATA[
arXiv:2505.15683v3 Announce Type: replace 
Abstract: Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions</title>
<link>https://arxiv.org/abs/2505.17120</link>
<guid>https://arxiv.org/abs/2505.17120</guid>
<content:encoded><![CDATA[
arXiv:2505.17120v2 Announce Type: replace 
Abstract: We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to explain their own functioning. Here, we show that i) LLMs can accurately describe quantitative features of their own internal processes during certain kinds of decision-making and ii) that it is possible to improve these capabilities through training. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain how they make other complex decisions, not just decisions they have been fine-tuned to make. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FB-RAG: Improving RAG with Forward and Backward Lookup</title>
<link>https://arxiv.org/abs/2505.17206</link>
<guid>https://arxiv.org/abs/2505.17206</guid>
<content:encoded><![CDATA[
arXiv:2505.17206v3 Announce Type: replace 
Abstract: Traditional Retrieval-Augmented Generation (RAG) struggles with complex queries that lack strong signals to retrieve the most relevant context, forcing a trade-off between choosing a small context that misses key information and a large context that confuses the LLM. To address this, we propose Forward-Backward RAG (FB-RAG), a new training-free framework based on a simple yet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to peek into potential future generations, using evidence from multiple sampled outputs to precisely identify the most relevant context for a final, more powerful generator. This improves performance without complex finetuning or Reinforcement Learning common in prior work. Across $9$ datasets from LongBench and $\infty$Bench, FB-RAG consistently delivers strong results. Further, the performance gains can be achieved with reduced latency due to a shorter, more focused prompt for the powerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with over $48$% latency reduction or achieves an $8$% performance improvement with a $10$% latency reduction. Our analysis finds cases where even when the forward-looking LLM fails to generate correct answers, its attempts are sufficient to guide the final model to an accurate response, demonstrating how smaller LLMs can systematically improve the performance and efficiency of larger ones.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>p2-TQA: A Process-based Preference Learning Framework for Self-Improving Table Question Answering Models</title>
<link>https://arxiv.org/abs/2505.17565</link>
<guid>https://arxiv.org/abs/2505.17565</guid>
<content:encoded><![CDATA[
arXiv:2505.17565v2 Announce Type: replace 
Abstract: Table question answering (TQA) focuses on answering questions based on tabular data. Developing TQA systems targets effective interaction with tabular data for tasks such as cell retrieval and data analysis. While recent work has leveraged fine-tuning to improve TQA systems, existing approaches often under-utilize available data and neglect the potential of post-training for further gains. In this work, we introduce p2-TQA, a process-based preference learning framework for TQA post-training. p2-TQA automatically constructs process-based preference data via a table-specific pipeline, eliminating the need for manual or costly data collection. It then optimizes models through contrastive learning on the collected data. Experiments show that p2-TQA effectively improves TQA models by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets with only 8,000 training instances. Furthermore, models enhanced with p2-TQA achieve competitive results against larger, more complex state-of-the-art TQA systems, while maintaining up to five times higher efficiency.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDB: Multilingual Instruction Data Booster for Enhancing Cultural Equality in Multilingual Instruction Synthesis</title>
<link>https://arxiv.org/abs/2505.17671</link>
<guid>https://arxiv.org/abs/2505.17671</guid>
<content:encoded><![CDATA[
arXiv:2505.17671v2 Announce Type: replace 
Abstract: Despite doubts on data quality, instruction synthesis has been widely applied into instruction tuning (IT) of LLMs as an economic and rapid alternative. Recent endeavors focus on improving data quality for synthesized instruction pairs in English and have facilitated IT of English-centric LLMs. However, data quality issues in multilingual synthesized instruction pairs are even more severe, since the common synthesizing practice is to translate English synthesized data into other languages using machine translation (MT). Besides the known content errors in these English synthesized data, multilingual synthesized instruction data are further exposed to defects introduced by MT and face insufficient localization of the target languages, leading to cultural inequality in trained LLMs. In this paper, we propose MIDB, a Multilingual Instruction Data Booster to automatically address the quality issues in multilingual synthesized data. MIDB is trained on around 36.8k revision examples across 16 languages by human linguistic experts, thereby can boost the low-quality data by addressing content errors and MT defects, and improving localization in these synthesized data. Both automatic and human evaluation indicate that not only MIDB steadily improved instruction data quality in 16 languages, but also the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on MIDB-boosted data were significantly enhanced, suggesting an improved linguistic and cultural equality.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models</title>
<link>https://arxiv.org/abs/2506.02431</link>
<guid>https://arxiv.org/abs/2506.02431</guid>
<content:encoded><![CDATA[
arXiv:2506.02431v2 Announce Type: replace 
Abstract: Emotions are a fundamental facet of human experience, varying across individuals, cultural contexts, and nationalities. Given the recent success of Large Language Models (LLMs) as role-playing agents, we examine whether LLMs exhibit emotional stereotypes when assigned nationality-specific personas. Specifically, we investigate how different countries are represented in pre-trained LLMs through emotion attributions and whether these attributions align with cultural norms. To provide a deeper interpretive lens, we incorporate four key cultural dimensions, namely Power Distance, Uncertainty Avoidance, Long-Term Orientation, and Individualism, derived from Hofstedes cross-cultural framework. Our analysis reveals significant nationality-based differences, with emotions such as shame, fear, and joy being disproportionately assigned across regions. Furthermore, we observe notable misalignment between LLM-generated and human emotional responses, particularly for negative emotions, highlighting the presence of reductive and potentially biased stereotypes in LLM outputs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward</title>
<link>https://arxiv.org/abs/2506.04070</link>
<guid>https://arxiv.org/abs/2506.04070</guid>
<content:encoded><![CDATA[
arXiv:2506.04070v2 Announce Type: replace 
Abstract: Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study focuses on generating precise, in-situ, step-by-step navigation instructions that are practically usable for VI users. Specifically, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to navigation instructions, thereby providing feedback rewards to guide the post-training of a Vision-Language Model (VLM). This enhances instruction accuracy and usability while reducing costly real-world data collection needs. To address the scarcity of dedicated benchmarks in this field, we introduce NIG4VI, a 27k-sample open-source dataset to facilitate training and evaluation. It comprises diverse navigation scenarios with accurate spatial coordinates, supporting detailed and open-ended in-situ instruction generation. Experiments on NIG4VI demonstrate the effectiveness of LaF-GRPO through quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU 14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o 0.323), and qualitative analysis further confirms that our method yields more intuitive and safer instructions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2506.14429</link>
<guid>https://arxiv.org/abs/2506.14429</guid>
<content:encoded><![CDATA[
arXiv:2506.14429v3 Announce Type: replace 
Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably stable perplexity during direct context extrapolation. Moreover, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct local perception phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first length extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs. The code is available at https://github.com/OpenMOSS/LongLLaDA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</title>
<link>https://arxiv.org/abs/2506.16444</link>
<guid>https://arxiv.org/abs/2506.16444</guid>
<content:encoded><![CDATA[
arXiv:2506.16444v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trilemma of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23921</link>
<guid>https://arxiv.org/abs/2506.23921</guid>
<content:encoded><![CDATA[
arXiv:2506.23921v3 Announce Type: replace 
Abstract: The public often attributes human-like qualities to large language models (LLMs) and assumes they "know" certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge. This study examines existing methods for probing the veracity of that knowledge and identifies several flawed underlying assumptions. To address these flaws, we introduce sAwMIL (Sparse-Aware Multiple-Instance Learning), a multiclass probing framework that combines multiple-instance learning with conformal prediction. sAwMIL leverages internal activations of LLMs to classify statements as true, false, or neither. We evaluate sAwMIL across 16 open-source LLMs, including default and chat-based variants, on three new curated datasets. Our results show that (1) common probing methods fail to provide a reliable and transferable veracity direction and, in some settings, perform worse than zero-shot prompting; (2) truth and falsehood are not encoded symmetrically; and (3) LLMs encode a third type of signal that is distinct from both true and false.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Verifiable Instruction Following</title>
<link>https://arxiv.org/abs/2507.02833</link>
<guid>https://arxiv.org/abs/2507.02833</guid>
<content:encoded><![CDATA[
arXiv:2507.02833v3 Announce Type: replace 
Abstract: A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning</title>
<link>https://arxiv.org/abs/2507.07810</link>
<guid>https://arxiv.org/abs/2507.07810</guid>
<content:encoded><![CDATA[
arXiv:2507.07810v2 Announce Type: replace 
Abstract: This paper investigates the relationship between large language models' (LLMs) ability to recognize repetitive input patterns and their performance on in-context learning (ICL). In contrast to prior work that has primarily focused on attention heads, we examine this relationship from the perspective of skill neurons, specifically repetition neurons. Our experiments reveal that the impact of these neurons on ICL performance varies depending on the depth of the layer in which they reside. By comparing the effects of repetition neurons and induction heads, we further identify strategies for reducing repetitive outputs while maintaining strong ICL capabilities.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlMed: Adding Reasoning Control to Medical Language Model</title>
<link>https://arxiv.org/abs/2507.22545</link>
<guid>https://arxiv.org/abs/2507.22545</guid>
<content:encoded><![CDATA[
arXiv:2507.22545v3 Announce Type: replace 
Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02087</link>
<guid>https://arxiv.org/abs/2508.02087</guid>
<content:encoded><![CDATA[
arXiv:2508.02087v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isolating Culture Neurons in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2508.02241</link>
<guid>https://arxiv.org/abs/2508.02241</guid>
<content:encoded><![CDATA[
arXiv:2508.02241v2 Announce Type: replace 
Abstract: Language and culture are deeply intertwined, yet it has been unclear how and where multilingual large language models encode culture. Here, we build on an established methodology for identifying language-specific neurons to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated largely independently of language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited, with implications for fairness, inclusivity, and alignment. Code and data are available at https://github.com/namazifard/Culture_Neurons.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study</title>
<link>https://arxiv.org/abs/2508.09776</link>
<guid>https://arxiv.org/abs/2508.09776</guid>
<content:encoded><![CDATA[
arXiv:2508.09776v2 Announce Type: replace 
Abstract: In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models</title>
<link>https://arxiv.org/abs/2509.04508</link>
<guid>https://arxiv.org/abs/2509.04508</guid>
<content:encoded><![CDATA[
arXiv:2509.04508v2 Announce Type: replace 
Abstract: Multi-agent systems with smaller language models (SLMs) present a viable alternative to single agent systems powered by large language models (LLMs) for addressing complex problems. In this work, we study how these alternatives compare in terms of both effectiveness and efficiency. To study this trade-off, we instantiate single and multi-agent systems for the complex problems in the AppWorld environment using different sized language models.
  We find that difficulties with long-trajectory learning in smaller language models (SLMs) limit their performance. Even when trained for specialized roles, SLMs fail to learn all subtasks effectively. To address this issue, we introduce a simple progressive sub-task training strategy, which introduces new sub-tasks progressively in each training epoch. We find that this novel strategy, analogous to instance level curriculum learning, consistently improves the effectiveness of multi-agents at all configurations. Our Pareto analysis shows that fine-tuned multi-agent systems yield better effectiveness-efficiency trade-offs. Additional ablations and analyses shows the importance of our progressive training strategy and its ability to reduce subtask error rates.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder-LLM Integration in Cross-Lingual Reasoning</title>
<link>https://arxiv.org/abs/2509.08105</link>
<guid>https://arxiv.org/abs/2509.08105</guid>
<content:encoded><![CDATA[
arXiv:2509.08105v3 Announce Type: replace 
Abstract: Large language models excel in English but still struggle with complex reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder methods such as LangBridge and MindMerger raise accuracy on mid and high-resource languages, yet they leave a large gap on LRLs. We present MERLIN, a two-stage model-stacking framework that applies a curriculum learning strategy -- from general bilingual bitext to task-specific data -- and adapts only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini. It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp), demonstrating effectiveness across both low and high-resource settings.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework</title>
<link>https://arxiv.org/abs/2509.17844</link>
<guid>https://arxiv.org/abs/2509.17844</guid>
<content:encoded><![CDATA[
arXiv:2509.17844v2 Announce Type: replace 
Abstract: Emotions that somebody develops based on an argument do not only depend on the argument itself - they are also influenced by a subjective evaluation of the argument's potential impact on the self. For instance, an argument to ban plastic bottles might cause fear of losing a job for a bottle industry worker, which lowers the convincingness - presumably independent of its content. While binary emotionality of arguments has been studied, such cognitive appraisal models have only been proposed in other subtasks of emotion analysis, but not in the context of arguments and their convincingness. To fill this research gap, we propose the Contextualized Argument Appraisal Framework to model the interplay between the sender, receiver, and argument. We adapt established appraisal models from psychology to argument mining, including argument pleasantness, familiarity, response urgency, and expected effort, as well as convincingness variables. To evaluate the framework and pave the way for computational modeling, we develop a novel role-playing-based annotation setup, mimicking real-world exposure to arguments. Participants disclose their emotion, explain the main cause, the argument appraisal, and the perceived convincingness. To consider the subjective nature of such annotations, we also collect demographic data and personality traits of both the participants and ask them to disclose the same variables for their perception of the argument sender. The analysis of the resulting ContArgA corpus of 4000 annotations reveals that convincingness is positively correlated with positive emotions (e.g., trust) and negatively correlated with negative emotions (e.g., anger). The appraisal variables particularly point to the importance of the annotator's familiarity with the argument.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title>
<link>https://arxiv.org/abs/2509.26601</link>
<guid>https://arxiv.org/abs/2509.26601</guid>
<content:encoded><![CDATA[
arXiv:2509.26601v2 Announce Type: replace 
Abstract: Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04226</link>
<guid>https://arxiv.org/abs/2510.04226</guid>
<content:encoded><![CDATA[
arXiv:2510.04226v5 Announce Type: replace 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration</title>
<link>https://arxiv.org/abs/2510.15267</link>
<guid>https://arxiv.org/abs/2510.15267</guid>
<content:encoded><![CDATA[
arXiv:2510.15267v2 Announce Type: replace 
Abstract: Automated International Classification of Diseases (ICD) coding assigns standardized diagnosis and procedure codes to clinical records, playing a critical role in healthcare systems. However, existing methods face challenges such as semantic gaps between clinical text and ICD codes, poor performance on rare and long-tail codes, and limited interpretability. To address these issues, we propose TraceCoder, a novel framework integrating multi-source external knowledge to enhance traceability and explainability in ICD coding. TraceCoder dynamically incorporates diverse knowledge sources, including UMLS, Wikipedia, and large language models (LLMs), to enrich code representations, bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a hybrid attention mechanism to model interactions among labels, clinical context, and knowledge, improving long-tail code recognition and making predictions interpretable by grounding them in external evidence. Experiments on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that TraceCoder achieves state-of-the-art performance, with ablation studies validating the effectiveness of its components. TraceCoder offers a scalable and robust solution for automated ICD coding, aligning with clinical needs for accuracy, interpretability, and reliability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding</title>
<link>https://arxiv.org/abs/2510.15269</link>
<guid>https://arxiv.org/abs/2510.15269</guid>
<content:encoded><![CDATA[
arXiv:2510.15269v2 Announce Type: replace 
Abstract: Medical texts, particularly electronic medical records (EMRs), are a cornerstone of modern healthcare, capturing critical information about patient care, diagnoses, and treatments. These texts hold immense potential for advancing clinical decision-making and healthcare analytics. However, their unstructured nature, domain-specific language, and variability across contexts make automated understanding an intricate challenge. Despite the advancements in natural language processing, existing methods often treat all data as equally challenging, ignoring the inherent differences in complexity across clinical records. This oversight limits the ability of models to effectively generalize and perform well on rare or complex cases. In this paper, we present TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to address these challenges by rethinking how models interact with medical texts during training. Inspired by the principle of progressive learning, TACL dynamically adjusts the training process based on the complexity of individual samples. By categorizing data into difficulty levels and prioritizing simpler cases early in training, the model builds a strong foundation before tackling more complex records. By applying TACL to multilingual medical data, including English and Chinese clinical records, we observe significant improvements across diverse clinical tasks, including automatic ICD coding, readmission prediction and TCM syndrome differentiation. TACL not only enhances the performance of automated systems but also demonstrates the potential to unify approaches across disparate medical domains, paving the way for more accurate, scalable, and globally applicable medical text understanding solutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2510.16565</link>
<guid>https://arxiv.org/abs/2510.16565</guid>
<content:encoded><![CDATA[
arXiv:2510.16565v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used across diverse cultural contexts, making accurate cultural understanding essential. Prior evaluations have mostly focused on output-level performance, obscuring the factors that drive differences in responses, while studies using circuit analysis have covered few languages and rarely focused on culture. In this work, we trace LLMs' internal cultural understanding mechanisms by measuring activation path overlaps when answering semantically equivalent questions under two conditions: varying the target country while fixing the question language, and varying the question language while fixing the country. We also use same-language country pairs to disentangle language from cultural aspects. Results show that internal paths overlap more for same-language, cross-country questions than for cross-language, same-country questions, indicating strong language-specific patterns. Notably, the South Korea-North Korea pair exhibits low overlap and high variability, showing that linguistic similarity does not guarantee aligned internal representation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model</title>
<link>https://arxiv.org/abs/2503.09205</link>
<guid>https://arxiv.org/abs/2503.09205</guid>
<content:encoded><![CDATA[
arXiv:2503.09205v4 Announce Type: replace-cross 
Abstract: Integrating audio and visual data for training multimodal foundational models remains a challenge. The Audio-Video Vector Alignment (AVVA) framework addresses this by considering AV scene alignment beyond mere temporal synchronization, and leveraging Large Language Models (LLMs) for data curation. AVVA implements a scoring mechanism for selecting aligned training data segments. It integrates Whisper, a speech-based foundation model, for audio and DINOv2 for video analysis in a dual-encoder structure with contrastive learning on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the effectiveness of the proposed model architecture and data curation approach. AVVA achieves a significant improvement in top-k accuracies for video-to-audio retrieval on all datasets compared to DenseAV, while using only 192 hrs of curated training data. Furthermore, an ablation study indicates that the data curation process effectively trades data quality for data quantity, yielding increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound, compared to training on the full spectrum of uncurated data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles</title>
<link>https://arxiv.org/abs/2503.17352</link>
<guid>https://arxiv.org/abs/2503.17352</guid>
<content:encoded><![CDATA[
arXiv:2503.17352v3 Announce Type: replace-cross 
Abstract: We introduce OpenVLThinker, one of the first open-source large vision-language models (LVLMs) to exhibit sophisticated chain-of-thought reasoning, achieving notable performance gains on challenging visual reasoning tasks. While text-based reasoning models (e.g., Deepseek R1) show promising results in text-only tasks, distilling their reasoning into LVLMs via supervised fine-tuning (SFT) often results in performance degradation due to imprecise visual grounding. Conversely, purely reinforcement learning (RL)-based methods face a large search space, hindering the emergence of reflective behaviors in smaller models (e.g., 7B LVLMs). Surprisingly, alternating between SFT and RL ultimately results in significant performance improvements after a few iterations. Our analysis reveals that the base model rarely exhibits reasoning behaviors initially, but SFT effectively surfaces these latent actions and narrows the RL search space, accelerating the development of reasoning capabilities. Each subsequent RL stage further refines the model's reasoning skills, producing higher-quality SFT data for continued self-improvement. OpenVLThinker-7B consistently advances performance across six benchmarks demanding mathematical and general reasoning, notably improving MathVista by 3.8%, EMMA by 2.4%, and HallusionBench by 1.6%. Beyond demonstrating the synergy between SFT and RL for complex reasoning tasks, our findings provide early evidence towards achieving R1-style reasoning in multimodal contexts. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multimodal Recaptioning Framework to Account for Perceptual Diversity Across Languages in Vision-Language Modeling</title>
<link>https://arxiv.org/abs/2504.14359</link>
<guid>https://arxiv.org/abs/2504.14359</guid>
<content:encoded><![CDATA[
arXiv:2504.14359v2 Announce Type: replace-cross 
Abstract: When captioning an image, people describe objects in diverse ways, such as by using different terms and/or including details that are perceptually noteworthy to them. Descriptions can be especially unique across languages and cultures. Modern vision-language models (VLMs) gain understanding of images with text in different languages often through training on machine translations of English captions. However, this process relies on input content written from the perception of English speakers, leading to a perceptual bias. In this work, we outline a framework to address this bias. We specifically use a small amount of native speaker data, nearest-neighbor example guidance, and multimodal LLM reasoning to augment captions to better reflect descriptions in a target language. When adding the resulting rewrites to multilingual CLIP finetuning, we improve on German and Japanese text-image retrieval case studies (up to +3.5 mean recall, +4.4 on native vs. translation errors). We also propose a mechanism to build understanding of object description variation across languages, and offer insights into cross-dataset and cross-language generalization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPMA: Preference Manipulation Attack Against Model Context Protocol</title>
<link>https://arxiv.org/abs/2505.11154</link>
<guid>https://arxiv.org/abs/2505.11154</guid>
<content:encoded><![CDATA[
arXiv:2505.11154v2 Announce Type: replace-cross 
Abstract: Model Context Protocol (MCP) standardizes interface mapping for large language models (LLMs) to access external data and tools, which revolutionizes the paradigm of tool selection and facilitates the rapid expansion of the LLM agent tool ecosystem. However, as the MCP is increasingly adopted, third-party customized versions of the MCP server expose potential security vulnerabilities. In this paper, we first introduce a novel security threat, which we term the MCP Preference Manipulation Attack (MPMA). An attacker deploys a customized MCP server to manipulate LLMs, causing them to prioritize it over other competing MCP servers. This can result in economic benefits for attackers, such as revenue from paid MCP services or advertising income generated from free servers. To achieve MPMA, we first design a Direct Preference Manipulation Attack (DPMA) that achieves significant effectiveness by inserting the manipulative word and phrases into the tool name and description. However, such a direct modification is obvious to users and lacks stealthiness. To address these limitations, we further propose Genetic-based Advertising Preference Manipulation Attack (GAPMA). GAPMA employs four commonly used strategies to initialize descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness. The experiment results demonstrate that GAPMA balances high effectiveness and stealthiness. Our study reveals a critical vulnerability of the MCP in open ecosystems, highlighting an urgent need for robust defense mechanisms to ensure the fairness of the MCP ecosystem.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title>
<link>https://arxiv.org/abs/2505.11770</link>
<guid>https://arxiv.org/abs/2505.11770</guid>
<content:encoded><![CDATA[
arXiv:2505.11770v2 Announce Type: replace-cross 
Abstract: Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2508.01365</link>
<guid>https://arxiv.org/abs/2508.01365</guid>
<content:encoded><![CDATA[
arXiv:2508.01365v3 Announce Type: replace-cross 
Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Question-to-Knowledge (Q2K): Multi-Agent Generation of Inspectable Facts for Product Mapping</title>
<link>https://arxiv.org/abs/2509.01182</link>
<guid>https://arxiv.org/abs/2509.01182</guid>
<content:encoded><![CDATA[
arXiv:2509.01182v2 Announce Type: replace-cross 
Abstract: Identifying whether two product listings refer to the same Stock Keeping Unit (SKU) is a persistent challenge in ecommerce, especially when explicit identifiers are missing and product names vary widely across platforms. Rule based heuristics and keyword similarity often misclassify products by overlooking subtle distinctions in brand, specification, or bundle configuration. To overcome these limitations, we propose Question to Knowledge (Q2K), a multi agent framework that leverages Large Language Models (LLMs) for reliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates targeted disambiguation questions, (2) a Knowledge Agent that resolves them via focused web searches, and (3) a Deduplication Agent that reuses validated reasoning traces to reduce redundancy and ensure consistency. A human in the loop mechanism further refines uncertain cases. Experiments on real world consumer goods datasets show that Q2K surpasses strong baselines, achieving higher accuracy and robustness in difficult scenarios such as bundle identification and brand origin disambiguation. By reusing retrieved reasoning instead of issuing repeated searches, Q2K balances accuracy with efficiency, offering a scalable and interpretable solution for product integration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning</title>
<link>https://arxiv.org/abs/2509.22315</link>
<guid>https://arxiv.org/abs/2509.22315</guid>
<content:encoded><![CDATA[
arXiv:2509.22315v3 Announce Type: replace-cross 
Abstract: Inspired by the dual-process theory of human cognition from \textit{Thinking, Fast and Slow}, we introduce \textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \textbf{System 1} (fast, intuitive thinking) and \textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \textit{planning}, \textit{hypothesis generation}, \textit{retrieval}, \textit{information integration}, and \textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions</title>
<link>https://arxiv.org/abs/2510.08576</link>
<guid>https://arxiv.org/abs/2510.08576</guid>
<content:encoded><![CDATA[
arXiv:2510.08576v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural language understanding and user intent resolution, enabling tasks such as translation, summarization, and, increasingly, the orchestration of complex workflows. This development signifies a paradigm shift from conventional, GUI-driven user interfaces toward intuitive, language-first interaction paradigms. Rather than manually navigating applications, users can articulate their objectives in natural language, enabling LLMs to orchestrate actions across multiple applications in a dynamic and contextual manner. However, extant implementations frequently rely on cloud-based proprietary models, which introduce limitations in terms of privacy, autonomy, and scalability. For language-first interaction to become a truly robust and trusted interface paradigm, local deployment is not merely a convenience; it is an imperative. This limitation underscores the importance of evaluating the feasibility of locally deployable, open-source, and open-access LLMs as foundational components for future intent-based operating systems. In this study, we examine the capabilities of several open-source and open-access models in facilitating user intention resolution through machine assistance. A comparative analysis is conducted against OpenAI's proprietary GPT-4-based systems to assess performance in generating workflows for various user intentions. The present study offers empirical insights into the practical viability, performance trade-offs, and potential of open LLMs as autonomous, locally operable components in next-generation operating systems. The results of this study inform the broader discussion on the decentralization and democratization of AI infrastructure and point toward a future where user-device interaction becomes more seamless, adaptive, and privacy-conscious through locally embedded intelligence.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry</title>
<link>https://arxiv.org/abs/2510.22340</link>
<guid>https://arxiv.org/abs/2510.22340</guid>
<content:encoded><![CDATA[
arXiv:2510.22340v2 Announce Type: replace-cross 
Abstract: Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs</title>
<link>https://arxiv.org/abs/2510.26512</link>
<guid>https://arxiv.org/abs/2510.26512</guid>
<content:encoded><![CDATA[
<div> Keywords: human smuggling networks, legal case documents, knowledge graph construction, coreference resolution, LLM-based pipelines

Summary: 
- Human smuggling networks are evolving and challenging to analyze, with legal case documents providing crucial insights but posing difficulties due to their complex and ambiguous nature.
- Current LLM-based approaches have limitations in generating structured knowledge graphs from these documents, leading to noisy and fragmented graphs with duplicate nodes.
- The CORE-KG framework addresses these challenges by incorporating a type-aware coreference module and domain-guided structured prompts, significantly reducing duplication and legal noise.
- An ablation study of CORE-KG highlights the importance of both coreference resolution and structured prompts in minimizing node duplication and noisy nodes.
- Removing coreference resolution results in increased node duplication and noisy nodes, while removing structured prompts leads to further duplication and noise. These findings provide valuable insights for enhancing LLM-based pipelines for extracting structured representations from complex legal texts.<br /><br />Summary: <div>
arXiv:2510.26512v2 Announce Type: replace 
Abstract: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.25% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.29% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices</title>
<link>https://arxiv.org/abs/2510.18480</link>
<guid>https://arxiv.org/abs/2510.18480</guid>
<content:encoded><![CDATA[
<div> efficiency, diffusion language models, autoregressive models, benchmarking, acceleration strategies
Summary:
- Diffusion language models (DLMs) offer a parallel decoding process but often underperform autoregressive models in speed.
- This study systematically examines DLM efficiency, pointing out issues in prior evaluation methods.
- Empirical benchmarking and theoretical analysis show that autoregressive models achieve higher throughput compared to DLMs.
- Acceleration strategies such as dual cache and parallel decoding provide benefits mainly at small batch sizes.
- The study emphasizes the need for robust evaluation methods and improved acceleration strategies to enhance research on DLMs.<br /><br />Summary: <div>
arXiv:2510.18480v3 Announce Type: replace 
Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Landscape of Agentic Reinforcement Learning for LLMs: A Survey</title>
<link>https://arxiv.org/abs/2509.02547</link>
<guid>https://arxiv.org/abs/2509.02547</guid>
<content:encoded><![CDATA[
<div> Markov Decision Processes, Reinforcement Learning, Agentic RL, Language Models, Autonomous Agents<br />
Summary:<br />
This article introduces the concept of agentic reinforcement learning (Agentic RL) as a shift from traditional reinforcement learning applied to large language models (LLM RL). It contrasts single-step Markov Decision Processes (MDPs) of LLM-RL with partially observable Markov decision processes (POMDPs) in Agentic RL. The survey proposes a taxonomy focusing on agentic capabilities like planning, tool use, memory, reasoning, self-improvement, and perception, along with their applications in various task domains. It emphasizes reinforcement learning as crucial for transforming static modules into adaptive agentic behavior. The article consolidates open-source environments, benchmarks, and frameworks for researchers. By analyzing over five hundred recent works, it maps the evolving field of Agentic RL and identifies opportunities and challenges for developing scalable AI agents. <br />Summary: <div>
arXiv:2509.02547v3 Announce Type: replace-cross 
Abstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation</title>
<link>https://arxiv.org/abs/2510.25677</link>
<guid>https://arxiv.org/abs/2510.25677</guid>
<content:encoded><![CDATA[
<div> framework, wireless sensing, security, zero-knowledge proofs, encoder <br />
Summary: <br />
ZK-SenseLM is a wireless sensing framework that ensures security through zero-knowledge proofs and a large-model encoder for Wi-Fi channel state information. It incorporates mmWave radar or RFID and a policy-based decision layer. The system utilizes masked spectral pretraining and phase-consistency regularization for encoding, with a cross-modal alignment linking RF features to policy tokens. To prevent unsafe actions during distribution shift, a selective-abstention head is employed, with the chosen risk-coverage point included in the proof. A four-stage proving pipeline ensures the integrity of actions, with PLONK-style proofs verifying actions and confidence. Proofs are optimized using micro-batching and can be offloaded from low-power devices through a gateway option. The system is compatible with federated learning and on-device personalization while maintaining verifiability. ZK-SenseLM enhances performance in various tasks, improves calibration and coverage-risk curves, and effectively detects tampering and replay events. <div>
arXiv:2510.25677v2 Announce Type: replace-cross 
Abstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</title>
<link>https://arxiv.org/abs/2511.05516</link>
<guid>https://arxiv.org/abs/2511.05516</guid>
<content:encoded><![CDATA[
<div> Keywords: speech language model, continuous audio tokenizer, speech editing, instruction-based, free-form editing

Summary:
MingTok-Audio is introduced as a unified continuous speech tokenizer that integrates semantic and acoustic features for improved speech understanding and generation. Ming-UniAudio, based on MingTok-Audio, achieves a balance between generation and understanding capabilities, setting new SOTA records on the ContextASR benchmark. Ming-UniAudio-Edit is the first speech language model enabling free-form speech editing guided by natural language instructions without timestamp conditions. A comprehensive benchmark, Ming-Freeform-Audio-Edit, is introduced to evaluate instruction-based speech editing, covering semantic correctness, acoustic quality, and instruction alignment. The models are open-sourced to foster the development of unified audio understanding, generation, and manipulation.<br /><br />Summary: MingTok-Audio and Ming-UniAudio improve speech understanding and generation, setting benchmarks in ASR tasks. Ming-UniAudio-Edit enables free-form speech editing guided by natural language instructions, supported by the Ming-Freeform-Audio-Edit benchmark. All models are open-sourced to advance research in unified audio processing. <div>
arXiv:2511.05516v1 Announce Type: new 
Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retracing the Past: LLMs Emit Training Data When They Get Lost</title>
<link>https://arxiv.org/abs/2511.05518</link>
<guid>https://arxiv.org/abs/2511.05518</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, memorization, data extraction, privacy concerns, confusion-inducing attacks

Summary:
The paper introduces Confusion-Inducing Attacks (CIA), a framework for extracting memorized data from large language models (LLMs) by maximizing model uncertainty. It identifies a spike in token-level prediction entropy as a precursor to the emission of memorized text during divergence, and leverages this insight to optimize input snippets for inducing a high-entropy state. The proposed Mismatched Supervised Fine-tuning (SFT) weakens alignment in aligned LLMs to increase susceptibility to attacks. Experimental results show that CIA outperforms existing methods in extracting verbatim and near-verbatim training data without prior knowledge of the training data. The study highlights the persistent risks of memorization in LLMs and offers a systematic approach to assess these vulnerabilities. 

<br /><br />Summary: The paper presents Confusion-Inducing Attacks (CIA) to extract memorized data from large language models by maximizing model uncertainty. It uses token-level prediction entropy to optimize input snippets and proposes Mismatched Supervised Fine-tuning (SFT) for aligned models. Experimental results demonstrate the effectiveness of CIA in extracting training data without prior knowledge, highlighting ongoing risks of memorization in LLMs. <div>
arXiv:2511.05518v1 Announce Type: new 
Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning</title>
<link>https://arxiv.org/abs/2511.05532</link>
<guid>https://arxiv.org/abs/2511.05532</guid>
<content:encoded><![CDATA[
<div> Keywords: online content moderation, in-context learning, foundation models, personalization, decentralized environments

Summary: 
- The article introduces a novel framework that utilizes in-context learning with foundation models to detect harmful online content such as toxicity, spam, and negative sentiment across various settings.
- This approach allows for lightweight personalization, enabling users to block or unblock categories and extend detection to semantic variations without needing model retraining.
- Experiments on public datasets and a new annotated Mastodon dataset show that foundation models can generalize well across tasks, sometimes outperforming task-specific models.
- Personalization is effective with minimal user input, and adding label definitions or rationales to prompts improves robustness to noisy data.
- The work suggests a move towards user-centric content safety systems that are practical, privacy-preserving, and highly adaptable, offering a new approach beyond traditional centralized moderation systems.

<br /><br />Summary: <div>
arXiv:2511.05532v1 Announce Type: new 
Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCP4IFC: IFC-Based Building Design Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.05533</link>
<guid>https://arxiv.org/abs/2511.05533</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, architecture, engineering, construction, Industry Foundation Classes (IFC)

Summary:
This article introduces MCP4IFC, an open-source framework that allows Large Language Models (LLMs) to manipulate Industry Foundation Classes (IFC) data in the architecture, engineering, and construction (AEC) field. The framework enables LLMs to translate natural language instructions into actions on standardized data models using the Model Context Protocol (MCP). It includes tools for querying scene information, creating and modifying building elements, and a dynamic code-generation system. The framework combines in-context learning with retrieval-augmented generation (RAG) for tasks beyond the predefined toolset. Experiments show that an LLM using MCP4IFC can successfully complete various tasks, from building a house to editing existing IFC data. The open-source framework aims to promote research in LLM-driven BIM design and facilitate AI-assisted modeling workflows.

<br /><br />Summary: <div>
arXiv:2511.05533v1 Announce Type: new 
Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference</title>
<link>https://arxiv.org/abs/2511.05534</link>
<guid>https://arxiv.org/abs/2511.05534</guid>
<content:encoded><![CDATA[
<div> framework, cross-modal information flow, multimodal, KV cache merging, sensitivity-adaptive token matching <br />
<br />
Summary: <br />
FlowMM is introduced as a framework for multimodal KV cache merging, addressing limitations in traditional eviction strategies. It leverages cross-modal information flow to dynamically apply merging strategies, capturing modality-specific patterns while maintaining contextual integrity. The framework also includes a sensitivity-adaptive token matching mechanism to evaluate token similarity and task sensitivity, merging low-risk tokens while preserving high-sensitivity ones. Experimental results across MLLMs demonstrate a significant reduction in KV cache memory and decoding latency while maintaining competitive task performance. <div>
arXiv:2511.05534v1 Announce Type: new 
Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Future of AI Models: A Computational perspective on Model collapse</title>
<link>https://arxiv.org/abs/2511.05535</link>
<guid>https://arxiv.org/abs/2511.05535</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Diffusion models, Neural architectures, Model Collapse

Summary:
Artificial Intelligence, particularly Large Language Models (LLMs), has significantly impacted various fields such as software engineering, journalism, and academia. The proliferation of AI-generated content is evident, with a high percentage of webpages containing such material. However, the use of synthetic content poses a risk of Model Collapse due to reduced linguistic and semantic diversity. This study explores the onset of collapse by analyzing semantic similarity in English-language Wikipedia from 2013 to 2025. Results indicate a gradual increase in similarity before the adoption of LLM models, attributable to early RNN/LSTM technologies. Fluctuations in similarity reflect linguistic diversity, corpus size variations, and sampling error. A sharp rise in similarity post-LLM adoption signifies a potential threat to data richness and model generalization. This data-driven analysis offers insights into the progression towards recursive AI contamination. 

<br /><br />Summary: Artificial Intelligence has revolutionized various domains, with a substantial presence of AI-generated content on webpages. However, the widespread use of synthetic material poses a risk of Model Collapse, impacting linguistic and semantic diversity. By analyzing semantic similarity trends in English-language Wikipedia, this study uncovers an exponential rise in similarity following the adoption of Large Language Models. Fluctuations in similarity metrics highlight linguistic diversity and corpus size variations. The study provides valuable insights into the potential onset of recursive AI contamination, signaling a potential threat to data richness and model generalization. <div>
arXiv:2511.05535v1 Announce Type: new 
Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability</title>
<link>https://arxiv.org/abs/2511.05541</link>
<guid>https://arxiv.org/abs/2511.05541</guid>
<content:encoded><![CDATA[
<div> Keywords: interpretability, Sparse Autoencoders, linguistic understanding, semantic features, unsupervised learning

Summary:<br /><br />Translating complex model representations into human-understandable concepts is a critical goal for interpretability. Current methods like Sparse Autoencoders (SAEs) often fail to capture meaningful linguistic information, focusing instead on superficial patterns. This stems from a lack of integration of linguistic knowledge in training. To address this, Temporal Sparse Autoencoders (T-SAEs) are introduced, which prioritize semantic over syntactic features through a novel contrastive loss mechanism. The T-SAEs successfully disentangle semantic concepts in a self-supervised manner, yielding smoother and coherent semantic representations across various datasets and models. Surprisingly, these semantic structures emerge without explicit semantic supervision, indicating a promising approach for enhancing unsupervised interpretability in language models. <div>
arXiv:2511.05541v1 Announce Type: new 
Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements</title>
<link>https://arxiv.org/abs/2511.05560</link>
<guid>https://arxiv.org/abs/2511.05560</guid>
<content:encoded><![CDATA[
<div> mLSTM, language modeling, linear attention, sliding window attention, Muon optimizer  
Summary:  
1. The study focuses on sample-efficient language modeling techniques for the BabyLM 2025 shared task, utilizing the BLaLM model with a linear-time mLSTM token mixer.
2. Architectural enhancements such as short convolutions, sliding window attention, and Hedgehog feature maps are explored to improve model performance.
3. A curated high-quality corpus is used for training in low-resource settings, emphasizing readability and pedagogical structure.
4. Experiments reveal that linear attention with sliding window attention enhances zero-shot performance consistently.
5. The Muon optimizer is found to stabilize convergence and reduce perplexity compared to AdamW, showcasing effective strategies for efficient language modeling without relying on scale.  
<br /><br />Summary: <div>
arXiv:2511.05560v1 Announce Type: new 
Abstract: We study architectural and optimization tech- niques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM to- ken mixer and explores lightweight enhance- ments, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support train- ing in low-resource settings, we curate a high- quality corpus emphasizing readability and ped- agogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding win- dow attention consistently improves zero-shot performance, and (2) the Muon optimizer stabi- lizes convergence and reduces perplexity over AdamW. These results highlight effective strate- gies for efficient language modeling without relying on scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8</title>
<link>https://arxiv.org/abs/2511.05578</link>
<guid>https://arxiv.org/abs/2511.05578</guid>
<content:encoded><![CDATA[
<div> tokenization, subword, language model, UTF-8, monoid theory

Summary:
This paper discusses the challenges of subword tokenization for language models, specifically focusing on the trade-offs between using code points and bytes in the vocabulary. By formalizing tokenization using monoid theory, the study proves that tokenizers with ill-formed UTF-8 tokens can result in sequences that are also ill-formed UTF-8. The research demonstrates the implications of converting tokens incrementally versus all at once, highlighting potential differences in results. Real-world bugs resulting from these issues are discussed, along with evaluations of mitigations. Case studies involving major foundation models, serving engines, and constrained generation systems are explored to showcase the impact of these findings. The study emphasizes the need for applications of language models to account for potential breakage introduced by using byte-based vocabularies. 
<br /><br />Summary: <div>
arXiv:2511.05578v1 Announce Type: new 
Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Diversity and Quality through Base-Aligned Model Collaboration</title>
<link>https://arxiv.org/abs/2511.05650</link>
<guid>https://arxiv.org/abs/2511.05650</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, model collaboration, diversity, quality, inference-time

Summary: 
Base-Aligned Model Collaboration (BACo) is introduced as a framework for improving the diversity and quality of large language models (LLMs) by dynamically combining a base model with its aligned counterpart. Utilizing routing strategies based on next-token prediction uncertainty and semantic role prediction, BACo optimizes diversity and quality without compromising on performance. Compared to existing diversity-promoting methods, BACo achieves a balance between diversity and quality in a single pass, offering controllability and consistently outperforming state-of-the-art baselines across various generation tasks and metrics. Through collaboration between base and aligned models, BACo demonstrates a 21.3% joint improvement in diversity and quality, as confirmed by human evaluations. This approach highlights the potential of model collaboration to enhance the output quality of LLMs while maintaining diversity. 

<br /><br />Summary: <div>
arXiv:2511.05650v1 Announce Type: new 
Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OckBench: Measuring the Efficiency of LLM Reasoning</title>
<link>https://arxiv.org/abs/2511.05722</link>
<guid>https://arxiv.org/abs/2511.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, token efficiency, benchmarking, automated reasoning, code generation

Summary:
Large language models like GPT-4 and Claude 3 have greatly improved automated reasoning and code generation. However, existing benchmarks often overlook token efficiency, which plays a crucial role in determining system latency, cost, and energy consumption. In response to this gap, the OckBench benchmark was introduced to evaluate both accuracy and token count for reasoning and coding tasks. Through experiments on various models, it was discovered that models with similar accuracy can differ significantly in token consumption. The benchmark also highlights the importance of considering token efficiency when evaluating models, prompting a paradigm shift in research evaluation practices. OckBench serves as a comprehensive platform for measuring, comparing, and guiding research in token-efficient reasoning. The benchmarks are accessible at https://ockbench.github.io/. 

<br /><br />Summary: <div>
arXiv:2511.05722v1 Announce Type: new 
Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning Without Copying</title>
<link>https://arxiv.org/abs/2511.05743</link>
<guid>https://arxiv.org/abs/2511.05743</guid>
<content:encoded><![CDATA[
<div> induction heads, inductive copying, transformers, abstractive ICL, Hapax <br />
<br />
Summary: 
The study explores the role of induction heads in transformers, which are attention heads that perform inductive copying to match patterns and copy continuations. The research investigates whether transformers can still acquire in-context learning capabilities when inductive copying is suppressed. The proposed Hapax setting omits the loss contribution for tokens predict correctly by induction heads. Despite a reduction in inductive copying, performance on abstractive in-context learning tasks remains comparable and even surpasses the vanilla model on various tasks. The model achieves lower loss values on token positions not predicted by induction heads. Analysis reveals that models trained with Hapax develop fewer and weaker induction heads but still retain in-context learning capabilities. These findings suggest that inductive copying is not essential for learning abstractive in-context learning mechanisms. <br /> <div>
arXiv:2511.05743v1 Announce Type: new 
Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models</title>
<link>https://arxiv.org/abs/2511.05752</link>
<guid>https://arxiv.org/abs/2511.05752</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, language models, feature pyramids, graph neural networks, semantic modeling

Summary: This study presents a hybrid method for text classification that combines deep feature extraction from language models, multi-scale feature fusion via feature pyramids, and structured modeling with graph neural networks. The large language model captures contextual dependencies and semantic representations, forming a strong foundation for subsequent modeling. The feature pyramid mechanism integrates semantic features of different scales, balancing global and local information to create hierarchical semantic expressions. Fused features are transformed into graph representations, allowing graph neural networks to capture semantic relations and dependencies in the text. The proposed method outperforms existing models in robustness alignment experiments on metrics such as ACC, F1-Score, AUC, and Precision. This framework provides a new approach for multi-scale feature fusion and structured semantic modeling in text classification tasks.<br /><br />Summary: This study introduces a hybrid method for text classification that combines deep feature extraction from language models with multi-scale feature fusion and structured modeling. The framework achieves superior performance in complex semantic contexts, showcasing the effectiveness of balancing global and local information, as well as semantics and structure in text classification tasks. <div>
arXiv:2511.05752v1 Announce Type: new 
Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Generation: Complexity Barriers and Implications for Learning</title>
<link>https://arxiv.org/abs/2511.05759</link>
<guid>https://arxiv.org/abs/2511.05759</guid>
<content:encoded><![CDATA[
<div> language generation, learnability, regular languages, context-free languages, language models
Summary:<br />
- Kleinberg and Mullainathan demonstrated the theoretical possibility of language generation with a sufficient number of positive examples, but practical feasibility is challenging.<br />
- Simple language families like regular and context-free languages may require an extraordinarily large number of examples for successful generation, sometimes without a computable bound.<br />
- There exists a significant gap between the theoretical potential and efficient learnability of language.<br />
- The success of modern language models may be explained by considering structural properties of natural language that enable effective generation in practice.<br /> <div>
arXiv:2511.05759v1 Announce Type: new 
Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning</title>
<link>https://arxiv.org/abs/2511.05784</link>
<guid>https://arxiv.org/abs/2511.05784</guid>
<content:encoded><![CDATA[
<div> Keywords: unlearning, language models, privacy protection, detection module, continual unlearning

Summary:
The article introduces a novel framework called DRAGON for unlearning in Large Language Models (LLMs) to protect private data and remove harmful knowledge. DRAGON leverages in-context chain-of-thought (CoT) instructions to guide LLMs without the need for access to retain data. By utilizing a lightweight detection module and a CoT guard model, DRAGON can effectively identify forget-worthy prompts and enforce safe intervention without modifying the base model. The framework is evaluated across three unlearning tasks, demonstrating its strong capability, scalability, and practical applicability. The proposed metrics for unlearning performance and the continual unlearning setting provide a robust evaluation of DRAGON's effectiveness in data-limited scenarios. Overall, DRAGON offers a promising solution for efficient and secure unlearning in LLMs. 

<br /><br />Summary: <div>
arXiv:2511.05784v1 Announce Type: new 
Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Edits Decay in Fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2511.05852</link>
<guid>https://arxiv.org/abs/2511.05852</guid>
<content:encoded><![CDATA[
<div> knowledge editing, fine-tuning, language models, edits decay, selective-layer fine-tuning

Summary:
In this study, the authors explore the interaction between knowledge editing and fine-tuning in large language models (LLMs). They investigate whether edits made to LLMs persist after fine-tuning, which is crucial for scenarios such as removing malicious edits or preserving beneficial ones. The experiments involve testing two editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets. The results show that edits decay after fine-tuning, with the survival of edits varying across configurations. Selective-layer fine-tuning is proposed as a strategy to effectively remove edits, even though it may slightly impact downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. This study establishes empirical baselines and provides actionable strategies for integrating knowledge editing with fine-tuning in LLMs, emphasizing the importance of considering the full application pipeline when evaluating model editing. 

Summary: <div>
arXiv:2511.05852v1 Announce Type: new 
Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</title>
<link>https://arxiv.org/abs/2511.05901</link>
<guid>https://arxiv.org/abs/2511.05901</guid>
<content:encoded><![CDATA[
<div> Keywords: medical knowledge, large language models, retrieval-augmented generation, clinical validation, low-resource settings 

Summary:
Large language models (LLMs) have shown value in the medical field, but there are limitations. Retrieval-augmented generation (RAG) technologies have potential to enhance clinical applicability. However, current research relies heavily on publicly available data, with limited use of private data. Retrieval approaches commonly use English-centric embedding models, while medical-specific LLMs are underutilized. Evaluation metrics focus on generation quality and task performance, but lack attention to bias and safety. RAG applications in medicine are concentrated on question answering, report generation, text summarization, and information extraction. The field of medical RAG is still in its early stages and requires advancements in clinical validation, cross-linguistic adaptation, and support for low-resource settings to ensure global use is trustworthy and responsible. 

<br /><br />Summary: <div>
arXiv:2511.05901v1 Announce Type: new 
Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NILC: Discovering New Intents with LLM-assisted Clustering</title>
<link>https://arxiv.org/abs/2511.05913</link>
<guid>https://arxiv.org/abs/2511.05913</guid>
<content:encoded><![CDATA[
<div> Keywords: New Intent Discovery, Clustering, Language Models, Semi-Supervised Learning, Text Embeddings <br />
Summary: <br />
The paper introduces a novel clustering framework, NILC, designed for effective New Intent Discovery (NID) in dialogue systems. Existing NID approaches typically use a cascaded architecture, but NILC adopts an iterative workflow to leverage feedback from both text embeddings and clustering stages. By incorporating large language models (LLMs), NILC enriches cluster centroids with contextual semantics and refines uncertain utterances for improved clustering. It also utilizes LLMs to rewrite and correct ambiguous or terse utterances, enhancing the performance of NID. In the semi-supervised setting, NILC employs non-trivial techniques such as seeding and soft must links to provide supervision signals for more accurate intent recognition. Experimental results demonstrate the superior performance of NILC over recent baselines across various datasets, showcasing its effectiveness in achieving significant improvements in NID tasks. <br /> <div>
arXiv:2511.05913v1 Announce Type: new 
Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction</title>
<link>https://arxiv.org/abs/2511.05921</link>
<guid>https://arxiv.org/abs/2511.05921</guid>
<content:encoded><![CDATA[
<div> Keywords: Voice-controlled dialog systems, Intent Detection, Active Learning, Semi-supervised learning, Annotation cost reduction

Summary:
IDALC (Intent Detection and Active Learning based Correction) is introduced as a semi-supervised framework for voice-controlled dialog systems. It aims to detect user intents and rectify system-rejected utterances while minimizing human annotation. The framework surpasses baseline methods, achieving higher accuracy and macro-F1 improvement. Empirical findings on benchmark datasets show 5-10% higher accuracy and 4-8% improvement in macro-F1 compared to baseline methods. Notably, IDALC maintains the overall annotation cost at just 6-10% of the unlabelled data available. The framework can efficiently retrain agents with new intents from rejected queries over time, reducing the need for manual annotation. IDALC presents a cost-effective solution for improving the performance of voice-controlled dialog systems while minimizing human effort in annotation tasks.<br /><br />Summary: IDALC is a semi-supervised framework that detects intents and corrects rejected utterances in voice-controlled systems, achieving higher accuracy and macro-F1 improvement while minimizing annotation costs and manual effort. <div>
arXiv:2511.05921v1 Announce Type: new 
Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2511.05933</link>
<guid>https://arxiv.org/abs/2511.05933</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, language model, hierarchical knowledge, structured prompting, internal activation analysis

Summary:
Reinforcement learning (RL) enhances language models' performance in recalling pure knowledge, especially in structured knowledge tasks like medical codes. The study shows that RL models outperform base and supervised fine-tuned models in recalling procedural paths within existing knowledge hierarchies. Structured prompting helps fine-tuned models bridge the performance gap, indicating that RL models excel in navigating knowledge hierarchies. While final-answer accuracy improves with prompting, RL models retain superior procedural path recall abilities. Internal activation analysis reveals that RL transforms how models traverse knowledge rather than knowledge representation itself. Factual representations maintain similarity between fine-tuned and RL models, but query representations diverge, highlighting RL's impact on knowledge traversal. This study challenges the notion that RL degrades memorized knowledge, instead showing its effectiveness in enhancing procedural skills in structured knowledge recall tasks. 

<br /><br />Summary: <div>
arXiv:2511.05933v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Recognition of Cognitive Distortions in Natural Language Texts</title>
<link>https://arxiv.org/abs/2511.05969</link>
<guid>https://arxiv.org/abs/2511.05969</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-factor classification, natural language texts, cognitive distortions, artificial intelligence, interpretable model

Summary:
This article introduces a novel approach to multi-factor classification of natural language texts using weighted structured patterns like N-grams, while considering the heterarchical relationships among them. The focus is on automating the detection of specific cognitive distortions in psychological care by employing an interpretable, robust, and transparent artificial intelligence model. The proposed recognition and learning algorithms enhance the existing state-of-the-art solutions in this domain. The effectiveness of the approach is validated on two publicly available datasets, showcasing significant improvements in F1 scores compared to existing literature. The optimization of hyper-parameters is conducted, and the code and models developed are made available for community use. This research contributes to advancing the field of automated cognitive distortion detection, offering a valuable tool for psychological care practitioners. 

<br /><br />Summary: <div>
arXiv:2511.05969v1 Announce Type: new 
Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Entropy in Reinforcement Learning for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.05993</link>
<guid>https://arxiv.org/abs/2511.05993</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, verifiable rewards, language models, entropy dynamics, calibration

Summary: 
Reinforcement learning with verifiable rewards (RLVR) has been used to enhance the reasoning capabilities of large language models (LLMs). However, the collapse of entropy during RLVR training can lead to suboptimal convergence and hinder performance improvement. A comprehensive study on entropy dynamics in RLVR is lacking, prompting extensive experiments to be conducted. Factors such as off-policy updates, data diversity, and clipping thresholds impact model entropy. Tokens with positive advantages contribute most to entropy collapse, and adjusting loss weights of tokens can regulate model entropy effectively. The study reveals a correlation between model entropy, response diversity, calibration, and performance in various benchmarks. This research provides valuable insights into improving RLVR training of LLMs. 

<br /><br />Summary: <div>
arXiv:2511.05993v1 Announce Type: new 
Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis</title>
<link>https://arxiv.org/abs/2511.06000</link>
<guid>https://arxiv.org/abs/2511.06000</guid>
<content:encoded><![CDATA[
<div> age-related information; language models; biomedical evidence synthesis; DemogSummary dataset; summarization-capable LLMs

Summary:
The study evaluates the retention of age-related information by state-of-the-art language models in generating abstractive summaries of biomedical studies. A novel age-stratified dataset, DemogSummary, is created to assess demographic distinctions in child, adult, and older adult populations. Three LLMs - Qwen, Longformer, and GPT-4.1 Nano - are evaluated using standard metrics and a new Demographic Salience Score. It is found that adult-focused summaries have the lowest demographic fidelity, and under-represented populations are more susceptible to hallucinations. The study highlights the limitations of current LLMs in maintaining faithful and unbiased summarization, emphasizing the necessity for fairness-aware evaluation frameworks and summarization pipelines in biomedical NLP.

Summary: <div>
arXiv:2511.06000v1 Announce Type: new 
Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data</title>
<link>https://arxiv.org/abs/2511.06023</link>
<guid>https://arxiv.org/abs/2511.06023</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Implicit biases, Discrimination, Multi-dimensional, De-biasing

Summary:
This paper introduces a Multi-Reward Group Relative Policy Optimization (GRPO) framework to address the implicit biases and discriminatory tendencies found in Large Language Models (LLMs). By creating a synthetic dataset based on Chinese-context discrimination categories and training a reward model with multi-dimensional signals, the GRPO fine-tunes LLMs to optimize ethical dimensions such as fairness, neutrality, and linguistic quality. Experimental results show a significant reduction in bias intensity and an improvement in alignment with non-discriminatory standards while maintaining fluency and informativeness. The study demonstrates the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs, providing a replicable framework for cultural-contextual ethical alignment. 

<br /><br />Summary: <div>
arXiv:2511.06023v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts</title>
<link>https://arxiv.org/abs/2511.06048</link>
<guid>https://arxiv.org/abs/2511.06048</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, large language models, feature interpretation, interactive visualization, dimensionality reduction

Summary:
Sparse autoencoders (SAEs) have become a valuable tool for interpreting features in large language models by learning sparse directions. However, the vast number of extracted directions presents challenges for comprehensive exploration. Conventional embedding techniques like UMAP have limitations that can hinder accurate representation of global structure. In response, a focused exploration framework is proposed in this work, prioritizing curated concepts and their associated SAE features instead of attempting to visualize all features at once. An interactive visualization system combines topology-based visual encoding with dimensionality reduction, allowing users to examine both local and global relationships within selected features. This hybrid approach enhances understanding of SAE behavior through targeted and interpretable subsets, enabling deeper analysis of concept representation in latent space. <br /><br />Summary: <div>
arXiv:2511.06048v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework</title>
<link>https://arxiv.org/abs/2511.06051</link>
<guid>https://arxiv.org/abs/2511.06051</guid>
<content:encoded><![CDATA[
<div> Efficient, Hate Speech Detection, Real-time Deployment, LoRA, BERTweet

Summary:
Efficient hate speech detection systems are crucial for real-time deployment. A three-layer framework is proposed, combining rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. This approach achieves a 0.85 macro F1 score, comparable to state-of-the-art large language models like SafePhi but with a base model 100 times smaller. Dataset unification and optimized fine-tuning contribute to superior performance compared to traditional BERT-based methods with similar computational requirements. The system only requires 1.87M trainable parameters and trains in about 2 hours on a single T4 GPU, making it suitable for resource-constrained environments while maintaining competitive accuracy for real-world deployment. 

<br /><br />Summary: <div>
arXiv:2511.06051v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning</title>
<link>https://arxiv.org/abs/2511.06057</link>
<guid>https://arxiv.org/abs/2511.06057</guid>
<content:encoded><![CDATA[
<div> framework, multimodal stance detection, dual-reasoning, modality contribution, context-aware<br />
Summary:<br />
The article introduces ReMoD, a framework for multimodal stance detection that incorporates dual-reasoning mechanisms. By combining intuitive reasoning with reflective reasoning, ReMoD dynamically weights the contribution of different modalities based on their expressive power. The intuitive stage utilizes experience pools to form an initial stance hypothesis, while the reflective stage refines this hypothesis by adjusting for modality biases and incorporating deeper semantic insights. Through continuous refinement during training and inference, ReMoD guides robust and context-aware stance decisions. Experimental results on the MMSD benchmark show that ReMoD outperforms baseline models and demonstrates strong generalization capabilities. <div>
arXiv:2511.06057v1 Announce Type: new 
Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework</title>
<link>https://arxiv.org/abs/2511.06067</link>
<guid>https://arxiv.org/abs/2511.06067</guid>
<content:encoded><![CDATA[
<div> Keywords: ArchCraft, hardware architectures, Verilog, RTL verification, ArchSynthBench

Summary:
ArchCraft is a framework designed to convert abstract architectural descriptions from academic papers into synthesizable Verilog projects for hardware reproduction. The framework utilizes formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into hardware-aware designs. ArchCraft generates RTL and testbench code, facilitating verification and debugging to report Power, Area, and Performance metrics. The proposed benchmark ArchSynthBench provides a comprehensive evaluation of hardware synthesis from architectural descriptions. Experimental results showcase the superiority of ArchCraft over direct generation methods and the VerilogCoder framework in paper understanding and code completion. Evaluations and physical implementations verify that the generated RTL code meets all timing constraints and performance metrics consistent with the original papers. <div>
arXiv:2511.06067v1 Announce Type: new 
Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stemming Hallucination in Language Models Using a Licensing Oracle</title>
<link>https://arxiv.org/abs/2511.06073</link>
<guid>https://arxiv.org/abs/2511.06073</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, hallucinations, Licensing Oracle, structured knowledge graphs, fact-based domains<br />
Summary: <br />
- The study introduces the Licensing Oracle, an architectural solution, to prevent hallucinations in language models by validating against structured knowledge graphs.
- Unlike other methods, the Licensing Oracle integrates deterministic validation to ensure only factually accurate information is generated.
- Experiments comparing the Licensing Oracle with various methods showed it achieved perfect abstention precision and zero false answers, with 89.1% accuracy in factual responses.
- While methods like retrieval-augmented generation and fine-tuning improve performance, they do not eliminate hallucinations.
- The Licensing Oracle offers a reliable solution for fact-based domains and may pave the way for truth-constrained generation in future AI systems. <br /><br />Summary: <div>
arXiv:2511.06073v1 Announce Type: new 
Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MuonAll: Muon Variant for Efficient Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.06086</link>
<guid>https://arxiv.org/abs/2511.06086</guid>
<content:encoded><![CDATA[
<div> Keywords: Muon optimizer, language models, finetuning, AdamW, open-source<br />
Summary:<br />
The article introduces MuonAll, an extension of the Muon optimizer designed for finetuning existing pretrained language models. MuonAll incorporates all parameters inside Muon by transforming them into 2D matrices. Extensive finetuning experiments were conducted on publicly available language models with sizes up to half a billion parameters, showcasing comparable performance to AdamW across various benchmarks. The study emphasizes the effectiveness of Muon and MuonAll as alternative optimizers in the field of language model finetuning. Additionally, the distributed implementations of Muon and MuonAll have been open-sourced and are accessible on GitHub at https://github.com/Saurabh750/optimizer. <div>
arXiv:2511.06086v1 Announce Type: new 
Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of retrieval-based QA on QUEST-LOFT</title>
<link>https://arxiv.org/abs/2511.06125</link>
<guid>https://arxiv.org/abs/2511.06125</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, grounded QA, LOFT study, QUEST benchmark, structured output format<br />
<br />
Summary: 
The paper delves into the limitations of current retrieval-augmented generation (RAG) methods in handling questions with distributed information or complex reasoning. It highlights the findings from the LOFT study, showcasing the challenges faced by long-context language models, particularly with the QUEST benchmark. The analysis identifies key factors contributing to poor performance on QUEST-LOFT and presents updated results from comprehensive human evaluations. The study demonstrates that optimizing RAG with a structured output format incorporating reasoning and evidence, followed by answer re-verification, can significantly surpass long-context approaches in performance. <div>
arXiv:2511.06125v1 Announce Type: new 
Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.06146</link>
<guid>https://arxiv.org/abs/2511.06146</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial Reasoning, Vision-language models, Referring Expression Comprehension, Ambiguity, Negation

Summary:
Spatial Reasoning plays a crucial role in human cognition, challenging the latest Vision-Language Models (VLMs). Traditional analysis focuses on image captioning and visual question answering, but this study suggests using the Referring Expression Comprehension task to evaluate VLMs' spatial reasoning. This task reveals VLMs' performance in detecting ambiguous objects, understanding complex spatial expressions, and handling negation. By comparing task-specific architectures and large VLMs, the study exposes the strengths and weaknesses of these models in various spatial semantics categories. Despite facing difficulties in the task, the models exhibit different behaviors based on their underlying structures. This research sheds light on existing challenges and opportunities for future studies in enhancing VLMs' spatial reasoning capabilities.

<br /><br />Summary: 
- Spatial Reasoning is essential for human cognition and poses challenges for Vision-Language Models (VLMs).
- Referring Expression Comprehension task is proposed as an evaluation platform to assess VLMs' spatial reasoning abilities.
- The task highlights challenges in object detection ambiguity, complex spatial expressions, and negation understanding.
- Comparison of task-specific architectures and large VLMs exposes strengths and weaknesses in handling various spatial semantics categories.
- Despite facing difficulties, models exhibit different behaviors based on their structures, suggesting avenues for future research. <div>
arXiv:2511.06146v1 Announce Type: new 
Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering</title>
<link>https://arxiv.org/abs/2511.06183</link>
<guid>https://arxiv.org/abs/2511.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-based summarization, personalized, targeted, BookAsSumQA, QA-based evaluation framework

Summary: Aspect-based summarization for books is challenging due to the lack of reference summaries for long texts. To address this, BookAsSumQA introduces a QA-based evaluation framework that generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summarization quality based on question-answering performance. Experimental results using BookAsSumQA indicate that LLM-based approaches exhibit higher accuracy on shorter texts, while RAG-based methods are more effective for longer documents, making them more practical for aspect-based book summarization. This framework allows for more personalized and targeted summaries by highlighting specific aspects of the text. <div>
arXiv:2511.06183v1 Announce Type: new 
Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning</title>
<link>https://arxiv.org/abs/2511.06190</link>
<guid>https://arxiv.org/abs/2511.06190</guid>
<content:encoded><![CDATA[
<div> framework, routing, Large Language Models, inference costs, confidence scores<br />
Summary:<br />
The paper introduces STEER, a domain-agnostic framework for cost-efficient reasoning in Large Language Models (LLMs). By leveraging confidence scores from smaller models, STEER performs step-level routing between smaller and larger LLMs to reduce inference costs. Unlike existing methods, STEER does not rely on external models for routing and achieves competitive or enhanced accuracy across diverse benchmarks such as Mathematical Reasoning and Planning tasks. By utilizing model-internal confidence as a robust signal for routing, STEER offers a scalable approach for efficient deployment of LLMs. This approach helps to reduce inference costs by up to 48% while maintaining similar or improved accuracy compared to solely using larger models, showcasing the effectiveness of model-internal confidence in guiding efficient reasoning in LLMs. <br /> <div>
arXiv:2511.06190v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease</title>
<link>https://arxiv.org/abs/2511.06215</link>
<guid>https://arxiv.org/abs/2511.06215</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's Disease, language models, in-context learning, explicit knowledge, clinical reasoning

Summary:
Explicit Knowledge In-Context Learners (EK-ICL) is proposed as a framework to enhance Alzheimer's Disease detection using narrative transcripts. It integrates structured explicit knowledge to improve reasoning stability and task alignment in large language models (LLMs) under out-of-distribution and data-scarce conditions. EK-ICL incorporates confidence scores from small language models to ground predictions, parsing feature scores for improved demonstration selection, and label word replacement to address semantic misalignment. A parsing-based retrieval strategy and ensemble prediction are used to handle semantic homogeneity in AD transcripts. Extensive experiments showed that EK-ICL outperforms current fine-tuning and in-context learning approaches in AD detection. The study highlights the significance of explicit knowledge in clinical reasoning, emphasizing the importance of aligning label semantics with task context for optimal performance in low-resource settings.<br /><br />Summary: <div>
arXiv:2511.06215v1 Announce Type: new 
Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization</title>
<link>https://arxiv.org/abs/2511.06222</link>
<guid>https://arxiv.org/abs/2511.06222</guid>
<content:encoded><![CDATA[
<div> alignment paradigm, trustworthiness, helpfulness, Self-Priority Alignment, scalability
<br />
Summary: 
The article introduces a new alignment paradigm called priority alignment, focusing on ensuring trustworthiness before helpfulness in language model applications for high-stakes scenarios. It presents Self-Priority Alignment (SPA), an unsupervised framework that generates diverse responses, evaluates them, and refines them to meet trustworthy thresholds. SPA uses dual-criterion denoising to remove inconsistencies and control variance, constructing preference pairs and fine-tuning the model with an uncertainty-weighted alignment loss. Experimental results across various benchmarks demonstrate that SPA improves helpfulness while maintaining safety, outperforming strong baselines and preserving general capabilities. The approach provides a scalable and interpretable strategy for aligning language models in critical applications. 
<br /><br /> <div>
arXiv:2511.06222v1 Announce Type: new 
Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.06230</link>
<guid>https://arxiv.org/abs/2511.06230</guid>
<content:encoded><![CDATA[
<div> Keywords: discharge medication recommendation, CHIP 2025 Shared Task 2, Chinese EHR data, multi-label nature, large language model ensemble systems

Summary:
The paper discusses the CHIP 2025 Shared Task 2 competition focused on developing automated discharge medication recommendations using Chinese electronic health record (EHR) data. A high-quality dataset called CDrugRed was created for this task, challenging due to multi-label medication recommendations and diverse clinical text. Over 500 teams participated, with the top team showcasing the potential of advanced large language model ensembles. They achieved a Jaccard score of 0.5102 and an F1 score of 0.6267 on the final test set. While demonstrating promise in medication recommendation using language models in Chinese EHRs, the results also highlight the existing challenges. The post-evaluation phase for the competition is ongoing on the Tianchi platform. <br /><br />Summary: <div>
arXiv:2511.06230v1 Announce Type: new 
Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy</title>
<link>https://arxiv.org/abs/2511.06234</link>
<guid>https://arxiv.org/abs/2511.06234</guid>
<content:encoded><![CDATA[
<div> negation, pre-trained models, natural language inference, dataset artifacts, data augmentation  
Summary:  
- Pre-trained models for natural language inference lack understanding of language nuances like negation, relying on dataset artifacts for high performance.  
- An investigation into an ELECTRA-small model fine-tuned on the SNLI dataset revealed struggles in accurately classifying negation-containing examples.  
- To address this issue, training data was augmented with contrast sets and adversarial examples emphasizing negation.  
- Results showed that this targeted data augmentation improved the model's accuracy on negation-containing examples without compromising overall performance.  
- The identified dataset artifact of the model's struggle with negation was successfully mitigated through the data augmentation technique.  

Summary: <div>
arXiv:2511.06234v1 Announce Type: new 
Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSense:Making Large Language Models Proficient in Time-Series Analysis</title>
<link>https://arxiv.org/abs/2511.06344</link>
<guid>https://arxiv.org/abs/2511.06344</guid>
<content:encoded><![CDATA[
<div> Keywords: time-series, text-temporal data integration, large language models, multimodal framework, temporal sense

Summary:
The study introduces the EvalTS benchmark consisting of 10 tasks to evaluate models incorporating text and temporal data. It addresses the issue of models biased towards textual cues by proposing TimeSense, a multimodal framework. TimeSense includes a Temporal Sense module to ground textual reasoning in time-series dynamics and uses coordinate-based positional embeddings for spatial understanding of time-series data. Experimental results show that TimeSense outperforms existing methods on complex multi-dimensional time-series reasoning tasks, achieving state-of-the-art performance across multiple tasks. The framework balances textual reasoning with a preserved temporal sense, enhancing the model's ability to analyze time-series data effectively. <br /><br />Summary: <div>
arXiv:2511.06344v1 Announce Type: new 
Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.06391</link>
<guid>https://arxiv.org/abs/2511.06391</guid>
<content:encoded><![CDATA[
<div> Keywords: offensive content moderation, hate speech detection, implicit hate, cross-task transfer, HatePrototypes

Summary:<br />
This study examines the optimization of offensive content moderation models for different types of hateful messages. While existing benchmarks focus on explicit hate toward protected groups, implicit hate, including demeaning comparisons and subtle discriminatory language, is often overlooked. The researchers propose using HatePrototypes, class-level vector representations derived from language models, to enable cross-task transfer between explicit and implicit hate with as few as 50 examples per class. They demonstrate that these prototypes facilitate efficient hate speech detection without the need for repeated fine-tuning. Additionally, parameter-free early exiting with prototypes proves effective for both explicit and implicit hate. The code, prototype resources, and evaluation scripts have been released to support future research on efficient and transferable hate speech detection.<br />Summary: <div>
arXiv:2511.06391v1 Announce Type: new 
Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss</title>
<link>https://arxiv.org/abs/2511.06402</link>
<guid>https://arxiv.org/abs/2511.06402</guid>
<content:encoded><![CDATA[
<div> Transformer-based framework, social media posts, sugar dating, class imbalance, content moderation<br />
Summary:<br /> 
- Sugar dating-related content on social media poses societal and regulatory concerns,<br />
- Detection is challenging due to euphemisms and class imbalance,<br />
- SugarTextNet framework with pretrained transformer encoder and attention-based cue extractor,<br />
- Introduces Context-Aware Focal Loss for minority-class detection,<br />
- Outperforms traditional models and large language models in detecting sensitive content. <br /> <div>
arXiv:2511.06402v1 Announce Type: new 
Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset</title>
<link>https://arxiv.org/abs/2511.06418</link>
<guid>https://arxiv.org/abs/2511.06418</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, drug development, personalized medicine, reasoning tasks, counterfactuals

Summary: 
Large language models (LLMs) are being increasingly utilized in the fields of drug development and personalized medicine. A new dataset has been introduced to evaluate LLMs on their factual knowledge of drug mechanisms and their ability to reason about them in novel situations, presented as counterfactuals. Results show that the o4-mini model performs better than other models from OpenAI, with the Qwen3-4B-thinking model closely matching its performance. The open-world setting for reasoning tasks, where models must recall relevant knowledge, proves to be more challenging than the closed-world setting. Additionally, counterfactuals affecting internal links in the reasoning chain are more difficult than those affecting links from the drug mentioned in the prompt. Overall, these findings suggest the importance of LLMs in drug development and personalized medicine, highlighting the need for models to demonstrate factual knowledge and deep understanding of drug mechanisms. 

<br /><br />Summary: <div>
arXiv:2511.06418v1 Announce Type: new 
Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop</title>
<link>https://arxiv.org/abs/2511.06427</link>
<guid>https://arxiv.org/abs/2511.06427</guid>
<content:encoded><![CDATA[
<div> Keywords: Metaphors, Dutch language data, Cancer patients, Large language models, HealthQuote.NL 

Summary:<br /><br />
This article focuses on the extraction of metaphors used by cancer patients in Dutch language data to improve healthcare communication. Through analyzing patient storytelling interviews and online forum data, including posts and comments, researchers explore the performance of large language models (LLMs) using various prompting strategies. With a human-in-the-loop setup, the extracted metaphors are verified and compiled into a corpus named HealthQuote.NL. The goal is to enhance patient care, shared decision making, communication between patients and clinicians, and patient health literacy. The extracted metaphors can also contribute to designing personalized care pathways. The prompts and resources related to this research are shared on GitHub for further exploration and implementation. <div>
arXiv:2511.06427v1 Announce Type: new 
Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models</title>
<link>https://arxiv.org/abs/2511.06441</link>
<guid>https://arxiv.org/abs/2511.06441</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language models, multimodal queries, routing network, efficient vision tasks

Summary: 
AI models are evolving beyond text and increasingly powering vision, audio, and document understanding. While large language models (LLMs) are effective, their high inference costs present challenges for real-time deployment. In contrast, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. To address this, a unified framework has been introduced that intelligently routes queries to the most suitable expert model using a learned routing network. For vision tasks, a two-stage open-source pipeline is employed, optimized for efficiency by incorporating efficient classical vision components where they excel. Performance on benchmarks such as MMLU and VQA matches or exceeds monolithic LLM systems while reducing reliance on costly models by over 67%. The framework's extensible, multi-agent orchestration enables the delivery of high-quality, resource-efficient AI at scale. 

<br /><br />Summary: <div>
arXiv:2511.06441v1 Announce Type: new 
Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention</title>
<link>https://arxiv.org/abs/2511.06446</link>
<guid>https://arxiv.org/abs/2511.06446</guid>
<content:encoded><![CDATA[
<div> knowledge bases, language models, retrieval, structured knowledge, attention<br />
Summary:<br />
The paper introduces SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases into large language models. SR-KI encodes KBs into key-value pairs and injects them into LLMs' KV cache for efficient retrieval. It employs a two-stage training paradigm, incorporating a dedicated retrieval layer within the LLM and applying an attention-based loss for supervision. Unlike traditional methods, SR-KI performs retrieval entirely within the model's latent space, allowing for dynamic knowledge updates. Experimental results show that SR-KI enables the integration of up to 40K KBs into a 7B LLM while maintaining strong retrieval performance. It achieves high Recall@10 rates and up to 99.75% compression of injected KBs, demonstrating its effectiveness in question answering and KB ID generation tasks. <div>
arXiv:2511.06446v1 Announce Type: new 
Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.06497</link>
<guid>https://arxiv.org/abs/2511.06497</guid>
<content:encoded><![CDATA[
<div> Realignment, cross-lingual transfer, multilingual language models, low-resource languages, parallel data <br />
<br />
Summary: Realignment is a promising strategy for improving cross-lingual transfer in multilingual language models. However, its effectiveness varies, especially for low-resource languages (LRLs). This study explores whether using strategically selected language subsets can provide comparable or even superior results compared to full multilingual alignment. The experiments show that realignment can greatly benefit LRLs and that carefully chosen language subsets can match or even outperform full alignment, particularly for unseen LRLs. This suggests that realignment does not always require all available languages, reducing the need for extensive data collection. Strategic language selection can lead to efficient and robust realignment, offering a more feasible approach for enhancing cross-lingual transfer in multilingual language models. <br /> <div>
arXiv:2511.06497v1 Announce Type: new 
Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations</title>
<link>https://arxiv.org/abs/2511.06516</link>
<guid>https://arxiv.org/abs/2511.06516</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, task-aware, hidden representations, precision<br />
<br />
Summary: <br />
Large Language Models (LLMs) have shown excellent performance on various tasks, but their large size can lead to inefficiencies in memory and latency for applications requiring limited capabilities. This work introduces two new task-aware post-training quantization (PTQ) methods, Task-Aware Quantization (TAQ) and TAQO, which leverage task-specific signals encoded in hidden representations to guide the quantization process. By identifying task-relevant layers and preserving their precision while aggressively quantizing others, TAQ and TAQO achieve stable task sensitivity profiles and efficient task-specialized models. Experimental results demonstrate that TAQ and TAQO outperform existing baselines on various models, with TAQ leading on Phi-4 and TAQO leading on Llama-3.1, Qwen3, and Qwen2.5. For example, TAQ achieves significantly higher accuracy on Phi-4 compared to Activation-aware Weight Quantization (AWQ), while maintaining performance close to the original accuracy at lower average precision levels. <div>
arXiv:2511.06516v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement</title>
<link>https://arxiv.org/abs/2511.06530</link>
<guid>https://arxiv.org/abs/2511.06530</guid>
<content:encoded><![CDATA[
<div> Framework, Question-Answer, Data refinement, Large Language Models, Dataset quality  
Summary:  
RefineLab is a novel framework that uses Large Language Models to automatically refine Question-Answer datasets, addressing quality issues such as domain coverage gaps, difficulty imbalances, and factual inconsistencies. By setting target quality attributes and a token budget constraint, RefineLab performs selective edits on the dataset to improve overall quality while being resource-efficient. The framework operates as a constrained optimization problem, selecting optimal refinement strategies such as rephrasing or distractor replacement for each QA sample. Experimental results show that RefineLab significantly enhances dataset quality in terms of coverage, difficulty alignment, factual fidelity, and distractor quality, reducing discrepancies from expert-crafted datasets. This approach offers a scalable and customizable solution for reproducible dataset design, with implications for improving Large Language Model evaluation.  
<br /><br />Summary: <div>
arXiv:2511.06530v1 Announce Type: new 
Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages</title>
<link>https://arxiv.org/abs/2511.06531</link>
<guid>https://arxiv.org/abs/2511.06531</guid>
<content:encoded><![CDATA[
<div> languages, Nigeria, natural language processing, machine translation, topic classification
Summary: 
- Nigeria, with over 200 million people and 500 languages, lacks NLP research in languages beyond Hausa, Igbo, Nigerian-Pidgin, and Yoruba.
- A new dataset, ibom, introduces Anaang, Efik, Ibibio, and Oro languages for machine translation and topic classification.
- These languages are not represented in major benchmarks like Google Translate or Flores-200, highlighting the need for their inclusion.
- Evaluation shows current language models perform poorly in machine translation for these languages but improve steadily in topic classification with more shots.
<br /><br />Summary: <div>
arXiv:2511.06531v1 Announce Type: new 
Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep2Text: Decoding Full Text from a Single LLM Token Representation</title>
<link>https://arxiv.org/abs/2511.06571</link>
<guid>https://arxiv.org/abs/2511.06571</guid>
<content:encoded><![CDATA[
<div> Recovering Input Text, Large Language Models (LLMs), Rep2Text, Information Bottleneck Effect, Generalization Capability
<br />
Summary:
Rep2Text introduces a new framework for decoding full text from last-token representations in large language models (LLMs). Through a trainable adapter and decoding language model, it can recover over half of the information in 16-token sequences while maintaining semantic integrity. There is an observed information bottleneck effect where longer sequences show decreased token-level recovery but maintain semantic coherence. The framework shows robust generalization to out-of-distribution medical data, indicating its potential applicability across diverse domains. <div>
arXiv:2511.06571v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabRAG: Tabular Document Retrieval via Structured Language Representations</title>
<link>https://arxiv.org/abs/2511.06582</link>
<guid>https://arxiv.org/abs/2511.06582</guid>
<content:encoded><![CDATA[
arXiv:2511.06582v1 Announce Type: new 
Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making</title>
<link>https://arxiv.org/abs/2511.06592</link>
<guid>https://arxiv.org/abs/2511.06592</guid>
<content:encoded><![CDATA[
arXiv:2511.06592v1 Announce Type: new 
Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes</title>
<link>https://arxiv.org/abs/2511.06601</link>
<guid>https://arxiv.org/abs/2511.06601</guid>
<content:encoded><![CDATA[
arXiv:2511.06601v1 Announce Type: new 
Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models</title>
<link>https://arxiv.org/abs/2511.06676</link>
<guid>https://arxiv.org/abs/2511.06676</guid>
<content:encoded><![CDATA[
arXiv:2511.06676v1 Announce Type: new 
Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation</title>
<link>https://arxiv.org/abs/2511.06680</link>
<guid>https://arxiv.org/abs/2511.06680</guid>
<content:encoded><![CDATA[
arXiv:2511.06680v1 Announce Type: new 
Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention</title>
<link>https://arxiv.org/abs/2511.06682</link>
<guid>https://arxiv.org/abs/2511.06682</guid>
<content:encoded><![CDATA[
arXiv:2511.06682v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content</title>
<link>https://arxiv.org/abs/2511.06708</link>
<guid>https://arxiv.org/abs/2511.06708</guid>
<content:encoded><![CDATA[
arXiv:2511.06708v1 Announce Type: new 
Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Na\"ive Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights</title>
<link>https://arxiv.org/abs/2511.06738</link>
<guid>https://arxiv.org/abs/2511.06738</guid>
<content:encoded><![CDATA[
arXiv:2511.06738v1 Announce Type: new 
Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensitivity of Small Language Models to Fine-tuning Data Contamination</title>
<link>https://arxiv.org/abs/2511.06763</link>
<guid>https://arxiv.org/abs/2511.06763</guid>
<content:encoded><![CDATA[
arXiv:2511.06763v1 Announce Type: new 
Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces</title>
<link>https://arxiv.org/abs/2511.06778</link>
<guid>https://arxiv.org/abs/2511.06778</guid>
<content:encoded><![CDATA[
arXiv:2511.06778v1 Announce Type: new 
Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Focal Attention for Selective and Scalable Transformers</title>
<link>https://arxiv.org/abs/2511.06818</link>
<guid>https://arxiv.org/abs/2511.06818</guid>
<content:encoded><![CDATA[
arXiv:2511.06818v1 Announce Type: new 
Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2511.06826</link>
<guid>https://arxiv.org/abs/2511.06826</guid>
<content:encoded><![CDATA[
arXiv:2511.06826v1 Announce Type: new 
Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition</title>
<link>https://arxiv.org/abs/2511.06860</link>
<guid>https://arxiv.org/abs/2511.06860</guid>
<content:encoded><![CDATA[
arXiv:2511.06860v1 Announce Type: new 
Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inclusion of Role into Named Entity Recognition and Ranking</title>
<link>https://arxiv.org/abs/2511.06886</link>
<guid>https://arxiv.org/abs/2511.06886</guid>
<content:encoded><![CDATA[
arXiv:2511.06886v1 Announce Type: new 
Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers</title>
<link>https://arxiv.org/abs/2511.06890</link>
<guid>https://arxiv.org/abs/2511.06890</guid>
<content:encoded><![CDATA[
arXiv:2511.06890v1 Announce Type: new 
Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation</title>
<link>https://arxiv.org/abs/2511.06899</link>
<guid>https://arxiv.org/abs/2511.06899</guid>
<content:encoded><![CDATA[
arXiv:2511.06899v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection</title>
<link>https://arxiv.org/abs/2511.06942</link>
<guid>https://arxiv.org/abs/2511.06942</guid>
<content:encoded><![CDATA[
arXiv:2511.06942v1 Announce Type: new 
Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs</title>
<link>https://arxiv.org/abs/2511.07001</link>
<guid>https://arxiv.org/abs/2511.07001</guid>
<content:encoded><![CDATA[
arXiv:2511.07001v1 Announce Type: new 
Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Circuit Interpretation via Probe Prompting</title>
<link>https://arxiv.org/abs/2511.07002</link>
<guid>https://arxiv.org/abs/2511.07002</guid>
<content:encoded><![CDATA[
arXiv:2511.07002v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs</title>
<link>https://arxiv.org/abs/2511.07003</link>
<guid>https://arxiv.org/abs/2511.07003</guid>
<content:encoded><![CDATA[
arXiv:2511.07003v1 Announce Type: new 
Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation</title>
<link>https://arxiv.org/abs/2511.07010</link>
<guid>https://arxiv.org/abs/2511.07010</guid>
<content:encoded><![CDATA[
arXiv:2511.07010v1 Announce Type: new 
Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity</title>
<link>https://arxiv.org/abs/2511.07011</link>
<guid>https://arxiv.org/abs/2511.07011</guid>
<content:encoded><![CDATA[
arXiv:2511.07011v1 Announce Type: new 
Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks</title>
<link>https://arxiv.org/abs/2511.07025</link>
<guid>https://arxiv.org/abs/2511.07025</guid>
<content:encoded><![CDATA[
arXiv:2511.07025v1 Announce Type: new 
Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data</title>
<link>https://arxiv.org/abs/2511.07044</link>
<guid>https://arxiv.org/abs/2511.07044</guid>
<content:encoded><![CDATA[
arXiv:2511.07044v1 Announce Type: new 
Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction</title>
<link>https://arxiv.org/abs/2511.07055</link>
<guid>https://arxiv.org/abs/2511.07055</guid>
<content:encoded><![CDATA[
arXiv:2511.07055v1 Announce Type: new 
Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection</title>
<link>https://arxiv.org/abs/2511.07065</link>
<guid>https://arxiv.org/abs/2511.07065</guid>
<content:encoded><![CDATA[
arXiv:2511.07065v1 Announce Type: new 
Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Aware Data Selection for Efficient LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2511.07074</link>
<guid>https://arxiv.org/abs/2511.07074</guid>
<content:encoded><![CDATA[
arXiv:2511.07074v1 Announce Type: new 
Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>