<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking</title>
<link>https://arxiv.org/abs/2504.16188</link>
<guid>https://arxiv.org/abs/2504.16188</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Financial Natural Language Inference, diverse financial texts, SEC Filings, Annual Reports, Earnings Call transcripts

Summary:
FinNLI is introduced as a benchmark dataset for Financial Natural Language Inference (FinNLI) that includes diverse premise-hypothesis pairs from various financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. The dataset consists of 21,304 pairs, with a high-quality test set of 3,304 instances annotated by finance experts. Evaluations reveal a notable performance degradation in general-domain NLI models when faced with domain shifts. The highest Macro F1 scores for pre-trained language models (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, indicating the dataset's complexity. Interestingly, financial LLMs fine-tuned with instructions perform poorly, highlighting limitations in generalizability. FinNLI exposes shortcomings in current LLMs for financial reasoning, suggesting a need for improvement. 

<br /><br />Summary: <div>
arXiv:2504.16188v1 Announce Type: new 
Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy</title>
<link>https://arxiv.org/abs/2504.16271</link>
<guid>https://arxiv.org/abs/2504.16271</guid>
<content:encoded><![CDATA[
<div> NLP, attachment style, psychotherapy, PACS, classification model <br />
Summary: <br />
This paper introduces the use of Natural Language Processing (NLP) techniques to automatically assess patient attachment style in psychotherapy transcripts. Currently, attachment style assessment is done manually using the PACS system, which is time-consuming and requires extensive training. By implementing NLP classification models, the process can be automated, allowing for more personalized psychotherapy and targeted research on therapy mechanisms. Mislabeling patients' attachment styles can have negative effects on therapy outcomes, so accurate automated assessment is crucial. This research paves the way for widespread adoption of attachment-informed treatment and research in the mental healthcare field. <div>
arXiv:2504.16271v1 Announce Type: new 
Abstract: The delivery of mental healthcare through psychotherapy stands to benefit immensely from developments within Natural Language Processing (NLP), in particular through the automatic identification of patient specific qualities, such as attachment style. Currently, the assessment of attachment style is performed manually using the Patient Attachment Coding System (PACS; Talia et al., 2017), which is complex, resource-consuming and requires extensive training. To enable wide and scalable adoption of attachment informed treatment and research, we propose the first exploratory analysis into automatically assessing patient attachment style from psychotherapy transcripts using NLP classification models. We further analyze the results and discuss the implications of using automated tools for this purpose -- e.g., confusing `preoccupied' patients with `avoidant' likely has a more negative impact on therapy outcomes with respect to other mislabeling. Our work opens an avenue of research enabling more personalized psychotherapy and more targeted research into the mechanisms of psychotherapy through advancements in NLP.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation</title>
<link>https://arxiv.org/abs/2504.16286</link>
<guid>https://arxiv.org/abs/2504.16286</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese-English translation, back-translation, cultural fidelity, NLP performance

Summary:
<br /><br />
1. The study evaluates the performance of large language models in Chinese-English translation, focusing on scientific terminology, historical paradoxes, and literary metaphors.
2. Scientific abstracts benefit from back-translation, while traditional tools perform better in linguistically distinct texts.
3. Large language models struggle with retaining cultural and literary nuances, showcasing challenges in preserving poetic intent.
4. Some models exhibit a tendency towards "verbatim back-translation", indicating emergent memory behavior.
5. A novel BLEU variant utilizing Jieba segmentation and n-gram weighting is proposed to improve translation accuracy and cultural fidelity. This study contributes to a better understanding of Chinese NLP performance and the importance of maintaining cultural integrity in AI-mediated translation efforts. <div>
arXiv:2504.16286v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit "verbatim back-translation", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives</title>
<link>https://arxiv.org/abs/2504.16312</link>
<guid>https://arxiv.org/abs/2504.16312</guid>
<content:encoded><![CDATA[
<div> Keywords: symmetric relations, antisymmetric relations, natural language inference, large language models, contrastive learning

Summary:
This paper introduces a new natural language inference dataset derived from Wikidata to evaluate the performance of large language models (LLMs) in capturing symmetric and antisymmetric relations. The findings indicate that LLMs perform at random chance levels on this benchmark, revealing a gap in their ability to understand relations. To address this issue, the paper explores encoder retraining through contrastive learning with k-nearest neighbors. The retrained encoder achieves performance comparable to fine-tuned classification heads while offering advantages such as improved efficiency in few-shot learning and better mitigation of catastrophic forgetting.<br /><br />Summary: <div>
arXiv:2504.16312v1 Announce Type: new 
Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformer-Based Extraction of Statutory Definitions from the U.S. Code</title>
<link>https://arxiv.org/abs/2504.16353</link>
<guid>https://arxiv.org/abs/2504.16353</guid>
<content:encoded><![CDATA[
<div> extract, definitions, legal texts, U.S. Code, NLP

Summary:
- The study presents an advanced NLP system that uses transformer-based architectures to automatically extract definitions from the U.S. Code.
- The system addresses challenges in identifying legal definitions, extracting defined terms, and determining their scope within the complex corpus.
- It employs domain-specific transformers (Legal-BERT) fine-tuned for statutory texts to improve extraction accuracy.
- The system utilizes a multi-stage pipeline combining document structure analysis and state-of-the-art language models to process legal text from the U.S. Code XML version.
- Evaluation on multiple U.S. Code titles shows the system achieves significant improvements with 96.8% precision and 98.9% recall, surpassing traditional machine learning classifiers. 

<br /><br />Summary: <div>
arXiv:2504.16353v1 Announce Type: new 
Abstract: Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions</title>
<link>https://arxiv.org/abs/2504.16358</link>
<guid>https://arxiv.org/abs/2504.16358</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-TrajVis, trajectory data visualization, dataset construction, TrajVL, Large Language Models

Summary: 
This paper introduces the Text-to-TrajVis task, which transforms natural language questions into trajectory data visualizations. The authors propose the Trajectory Visualization Language (TVL) and develop a dataset construction method to create the TrajVL dataset, containing 18,140 question-TVL pairs. The dataset combines human labeling with Large Language Models (LLMs). The performance of multiple LLMs (GPT, Qwen, Llama) is evaluated on this task, showing its feasibility and difficulty. This novel task presents challenges and opportunities for natural language interfaces for trajectory visualization systems. 

<br /><br />Summary: <div>
arXiv:2504.16358v1 Announce Type: new 
Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitReason: Learning To Offload Reasoning</title>
<link>https://arxiv.org/abs/2504.16379</link>
<guid>https://arxiv.org/abs/2504.16379</guid>
<content:encoded><![CDATA[
<div> annotate, reasoning, large language models, offloading, fine-tuning
Summary:
Large language models (LLMs) often struggle with the sequential and memory-bound decoding phase due to the extended generation length required for reasoning tasks. To address this, a new approach called SplitReason is introduced, which offloads the most challenging segments of the reasoning process to a larger model while using a smaller, more efficient model for the rest. By annotating difficult segments and leveraging supervised and reinforcement learning fine-tuning, the 1.5B-parameter reasoning model can improve reasoning accuracy by 24% and 28.3% while offloading only a small percentage of generated tokens. The approach shows promising results in enhancing efficiency and accuracy in reasoning tasks, particularly on the OpenR1-Math-220k CoT dataset. The SplitReason model, along with data, code, and logs, has been open-sourced for further research and development. 
<br /><br />Summary: <div>
arXiv:2504.16379v1 Announce Type: new 
Abstract: Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2504.16394</link>
<guid>https://arxiv.org/abs/2504.16394</guid>
<content:encoded><![CDATA[
<div> Keywords: unstructured clinical data, clinical text summarization, Contextual, Domain-Specific Knowledge Graph, precision 

Summary: 
Contextual is a new framework designed to extract key information from unstructured clinical data for improved decision-making in patient care. It combines Context-Preserving Token Filtering with a Domain-Specific Knowledge Graph to enhance linguistic coherence and clinical fidelity. By preserving important context-specific tokens and augmenting them with structured knowledge, Contextual excels in both aspects, outperforming other baselines in empirical evaluations on two public benchmark datasets. The approach showcases the significance of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity while offering a scalable solution for enhancing precision in clinical text generation. <div>
arXiv:2504.16394v1 Announce Type: new 
Abstract: Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation</title>
<link>https://arxiv.org/abs/2504.16408</link>
<guid>https://arxiv.org/abs/2504.16408</guid>
<content:encoded><![CDATA[
<div> framework, reverse-prompt induction, retrieval-augmented reasoning synthesis, dual-stage reward-guided filtering, structured inference<br />
Summary: The Less is More approach, which placed third in the XLLM@ACL2025 Shared Task-III, focuses on structured reasoning using only 24 labeled examples. It employs a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis, and dual-stage reward-guided filtering to improve structure reasoning quality. The approach leverages a unified LoRA+ setup and fine-tunes all modules from Meta-Llama-3-8B-Instruct. By combining structure validation with reward filtering across few-shot and zero-shot prompts, the pipeline consistently enhances structured inference under low-resource constraints. This highlights the importance of controllable data distillation in improving structured reasoning quality. The code for this approach is available at https://github.com/Jiahao-Yuan/Less-is-More.<br /> <div>
arXiv:2504.16408v1 Announce Type: new 
Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-the-Box Conditional Text Embeddings from Large Language Models</title>
<link>https://arxiv.org/abs/2504.16411</link>
<guid>https://arxiv.org/abs/2504.16411</guid>
<content:encoded><![CDATA[
<div> Keywords: Conditional text embedding, PonTE, unsupervised, large language model, interpretability

Summary:
PonTE is an unsupervised conditional text embedding method that utilizes a causal large language model and a conditional prompt. It aims to capture the shift in perspective on texts when conditioned on a specific aspect without the need for extensive training data or fine-tuning models. Through experiments on conditional semantic text similarity and text clustering, PonTE has been shown to generate useful conditional text embeddings and achieve performance comparable to supervised methods. The method also demonstrates interpretability of text embeddings by analyzing word generation following prompts and embedding visualization. Overall, PonTE offers a cost-effective and efficient approach to capturing conditional text embeddings without the need for labor-intensive and resource-consuming fine-tuning processes. 

<br /><br />Summary: 
PonTE is an unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt to capture the shift in perspective on texts. It generates useful conditional text embeddings and achieves performance comparable to supervised methods without fine-tuning. The method showcases interpretability through word generation analysis and embedding visualization, offering a cost-effective approach to conditional text embedding. <div>
arXiv:2504.16411v1 Announce Type: new 
Abstract: Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study</title>
<link>https://arxiv.org/abs/2504.16414</link>
<guid>https://arxiv.org/abs/2504.16414</guid>
<content:encoded><![CDATA[
<div> benchmark, compositional reasoning, large language models, chemistry domain, knowledge graph

Summary: 
This study presents a new benchmark for evaluating the compositional reasoning abilities of large language models in the field of chemistry. The researchers developed a fully automated pipeline that integrates OpenAI reasoning models with named entity recognition systems to extract chemical entities from literature and create a comprehensive knowledge graph. The experiments conducted on this benchmark reveal that even state-of-the-art models struggle with multi-hop compositional reasoning tasks. The results emphasize the importance of augmenting language models with document retrieval to improve performance. However, perfect retrieval accuracy does not eliminate all reasoning errors, highlighting the complexity of compositional reasoning tasks. This work not only showcases the limitations of current language models but also introduces a novel data generation pipeline that can be applied to various domains, advancing our understanding of reasoning in computational linguistics.

Summary: <div>
arXiv:2504.16414v1 Announce Type: new 
Abstract: In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark</title>
<link>https://arxiv.org/abs/2504.16427</link>
<guid>https://arxiv.org/abs/2504.16427</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal language analysis, large language models, cognitive-level semantics, benchmark, evaluation

Summary: 
Multimodal language analysis is a growing field that explores the use of multiple modalities to understand human conversational utterances at a deeper level. To address the lack of research on the capability of multimodal large language models (MLLMs) in comprehending cognitive-level semantics, the MMLA benchmark was introduced. This benchmark consists of over 61K multimodal utterances from various scenarios, focusing on six key dimensions of multimodal semantics. The study evaluated eight different branches of LLMs and MLLMs using three different methods. Results showed that even fine-tuned models achieve only moderate accuracy, highlighting the current limitations in understanding complex human language. The MMLA benchmark aims to provide a foundation for further exploration of large language models in multimodal language analysis and offers valuable resources for advancing this field.

<br /><br />Summary: <div>
arXiv:2504.16427v1 Announce Type: new 
Abstract: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records</title>
<link>https://arxiv.org/abs/2504.16448</link>
<guid>https://arxiv.org/abs/2504.16448</guid>
<content:encoded><![CDATA[
<div> Keywords: medical consultation dialogues, structured electronic medical records, LoRA fine-tuning, natural language processing models, information extraction benchmark<br />
Summary:<br />
The article introduces EMRModel, an innovative approach that combines LoRA-based fine-tuning with code-style prompt design to convert medical consultation dialogues into structured electronic medical records (EMRs). A high-quality dataset of medical consultation dialogues with detailed annotations is created to train and evaluate the model. A fine-grained evaluation benchmark is introduced for medical consultation information extraction, along with a systematic evaluation methodology for optimizing medical NLP models. Experimental results demonstrate that EMRModel achieves an F1 score of 88.1%, significantly outperforming standard pre-trained models and traditional LoRA fine-tuning methods. This showcases the effectiveness of EMRModel in extracting structured medical record information from unstructured dialogues.<br /><br />Summary: <div>
arXiv:2504.16448v1 Announce Type: new 
Abstract: Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.16460</link>
<guid>https://arxiv.org/abs/2504.16460</guid>
<content:encoded><![CDATA[
<div> Vectorization, Telecom, Natural Language Processing, Domain-Specific, NetoAI

Summary: 
NetoAI introduces T-VEC, a specialized vectorization model for the telecom industry. T-VEC is fine-tuned from the gte-Qwen2-1.5B-instruct model with a triplet loss objective on telecom-specific data, resulting in deep integration of domain knowledge. Weight difference analysis validates the extensive modifications across 338 layers of the base model. The open-sourcing of a telecom-specific tokenizer enhances handling of industry jargon. T-VEC outperforms established models with a leading MTEB score and superior performance on a proprietary benchmark, showcasing a strong grasp of industry nuances. This work solidifies NetoAI's position as a leader in telecom AI innovation, offering the community a powerful, open-source tool. 

<br /><br />Summary: <div>
arXiv:2504.16460v1 Announce Type: new 
Abstract: The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining</title>
<link>https://arxiv.org/abs/2504.16511</link>
<guid>https://arxiv.org/abs/2504.16511</guid>
<content:encoded><![CDATA[
<div> optimization, data selection, language models, quality, diversity
Summary:<br />
The paper introduces the QuaDMix framework, which aims to optimize the data distribution for large language model pretraining by balancing quality and diversity. It proposes criteria for measuring data quality and employs domain classification for assessing overall dataset diversity. QuaDMix utilizes a parameterized data sampling function to determine the sampling probability of each data point based on quality and diversity labels. Simulated experiments using LightGBM for parameters searching show that QuaDMix outperforms independent strategies for quality and diversity, leading to an average performance improvement of 7.2% across multiple benchmarks. The study emphasizes the importance of balancing data quality and diversity in training language models. <br />Summary: <div>
arXiv:2504.16511v1 Announce Type: new 
Abstract: Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers for Complex Query Answering over Knowledge Hypergraphs</title>
<link>https://arxiv.org/abs/2504.16537</link>
<guid>https://arxiv.org/abs/2504.16537</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, complex query answering, hyper-relational graphs, transformer model, logical operations <br />
<br />
Summary: Complex Query Answering (CQA) using knowledge graphs has advanced with the introduction of hyper-relational graphs to represent real-world data. However, existing models are limited in representing relationships of varying arities. To address this, new datasets JF17k-HCQA and M-FB15k-HCQA were created, containing diverse query types with logical operations. The Logical Knowledge Hypergraph Transformer (LKHGT) model is proposed to answer knowledge hypergraph (KHG) existential first-order queries. It consists of a Projection Encoder for atomic projection and a Logical Encoder for complex operations, both with Type Aware Bias (TAB) for capturing token interactions. Experimental results demonstrate that LKHGT is a top-performing CQA method for KHG and can generalize to new query types. <br /> <div>
arXiv:2504.16537v1 Announce Type: new 
Abstract: Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression</title>
<link>https://arxiv.org/abs/2504.16574</link>
<guid>https://arxiv.org/abs/2504.16574</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring

Summary: 
This paper introduces a novel compression framework called Prompt Importance Sampling (PIS) for large language models (LLMs). PIS dynamically compresses prompts by sampling important tokens based on attention scores of hidden states. The framework utilizes a dual-level compression mechanism, quantifying token saliency and implementing adaptive compression through reinforcement learning. It also incorporates a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple benchmarks show that PIS outperforms existing methods in compression performance. Additionally, the framework enhances reasoning efficiency by optimizing context structuring. This work contributes to advancing prompt engineering by providing a theoretical grounding and practical efficiency in managing context for LLMs. 

Summary:<br /><br />Keywords: language models, prompt compression, attention mechanism, reinforcement learning, context structuring
This paper introduces Prompt Importance Sampling (PIS) for large language models, a compression framework that samples important tokens based on attention scores. PIS utilizes a dual-level compression mechanism, including reinforcement learning for adaptive compression. It also incorporates sentence-level importance sampling. Evaluation results demonstrate superior compression performance compared to existing methods. The framework also enhances reasoning efficiency by optimizing context structuring. This work contributes to prompt engineering by offering theoretical grounding and practical efficiency for managing context in LLMs. <div>
arXiv:2504.16574v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study</title>
<link>https://arxiv.org/abs/2504.16601</link>
<guid>https://arxiv.org/abs/2504.16601</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, traditional machine translation, medical consultation summaries, evaluation, clinical relevance <br />
<br />
Summary: 
This study compared the performance of large language models (LLMs) and traditional machine translation (MT) tools in translating medical consultation summaries from English to Arabic, Chinese, and Vietnamese. The results indicated that traditional MT tools generally outperformed LLMs, especially for complex texts. LLMs showed promise in translating simpler summaries, particularly in Vietnamese and Chinese. Arabic translations improved with complexity due to the language's morphology. However, LLMs still lack consistency, and the current evaluation metrics do not accurately capture clinical relevance. The study emphasizes the importance of domain-specific training, enhanced evaluation methods, and human oversight in medical translation. Overall, while LLMs offer contextual flexibility, improvements are needed for their effectiveness in translating medical texts accurately. <br /><br />Summary: <div>
arXiv:2504.16601v1 Announce Type: new 
Abstract: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Large Language Models, Conspiracy theories, Psychological research, GPT-4o

Summary: 
Large Language Models (LLMs) like GPT-4o, Llama 3, and Mistral have the potential to help counter harmful online content such as conspiracy theories. However, the effectiveness of these models in applying expert-crafted counterspeech strategies remains under-researched. The study found that these models often generate generic, repetitive, or superficial responses when provided with structured prompts derived from psychological research. They also tend to over-acknowledge fear and frequently hallucinate facts, sources, or figures, which could pose challenges in practical applications for countering conspiracy theories. This research highlights the need for further investigation and development to improve the ability of LLMs to effectively address conspiracy theories with expert-driven counterspeech.<br /><br />Summary: <div>
arXiv:2504.16604v1 Announce Type: new 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2504.16627</link>
<guid>https://arxiv.org/abs/2504.16627</guid>
<content:encoded><![CDATA[
<div> retrieve, fact-checked claims, monolingual, crosslingual, disinformation <br />
<br />
The article addresses the challenge of retrieving previously fact-checked claims in both monolingual and crosslingual settings to combat the spread of disinformation. The proposed approach involves a two-stage strategy utilizing a baseline retrieval system with a fine-tuned embedding model and an LLM-based reranker. The key innovation lies in the use of LLM-based translation to overcome obstacles in multilingual information retrieval. Furthermore, the focus is on ensuring that the majority of the pipeline can be replicated on a consumer GPU. The integrated system achieved high success scores of 0.938 and 0.81025 on monolingual and crosslingual test sets, respectively. <br /><br />Summary: <div>
arXiv:2504.16627v1 Announce Type: new 
Abstract: We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics</title>
<link>https://arxiv.org/abs/2504.16677</link>
<guid>https://arxiv.org/abs/2504.16677</guid>
<content:encoded><![CDATA[
<div> transfer, dynamics, language models, multilingual data, post-training<br />
<br />
This study explores the dynamics of cross-lingual transfer in large language models that are fine-tuned on multilingual data. By investigating different post-training settings and tasks of varying complexity, the study examines how cross-lingual transfer and multilingual performance are influenced. The research includes two model families with up to 35B parameters trained on tasks like summarization, instruction following, and mathematical reasoning in both single-task and multi-task instruction tuning scenarios. The results show that the effectiveness of cross-lingual transfer cannot be explained by isolated variables alone and varies depending on the post-training settings. The study identifies the conditions that promote successful cross-lingual transfer in practice, shedding light on the intricate dynamics at play in enabling large language models to be useful across different languages.<br /><br />Summary: <div>
arXiv:2504.16677v1 Announce Type: new 
Abstract: In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations</title>
<link>https://arxiv.org/abs/2504.16754</link>
<guid>https://arxiv.org/abs/2504.16754</guid>
<content:encoded><![CDATA[
<div> transformer, memory architecture, coherent dialogues, factual recall accuracy, privacy-aware conversational AI
Summary:
Compact Memory and Vector Memory are combined in HEMA to create a dual-memory system for maintaining coherent dialogues in large language models. Experimental results show significant improvements in factual recall accuracy and human-rated coherence. Vector Memory with 10K indexed chunks achieves high precision and recall rates, outperforming summarization-only approaches. Ablation studies reveal the benefits of age-weighted pruning for reducing retrieval latency and the use of a two-level summary hierarchy to prevent errors in long conversations. HEMA's approach of combining verbatim recall with semantic continuity enables privacy-aware conversational AI capable of month-long dialogues without the need for model retraining. <div>
arXiv:2504.16754v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Effective are Generative Large Language Models in Performing Requirements Classification?</title>
<link>https://arxiv.org/abs/2504.16768</link>
<guid>https://arxiv.org/abs/2504.16768</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer-based large language models, requirements engineering, generative LLMs, requirements classification, experimental study

Summary:
In recent years, transformer-based large language models (LLMs) have been widely used in natural language processing, including requirements engineering (RE) tasks like trace-link detection and regulatory compliance. While non-generative LLMs such as BERT have been successful in requirements classification, there has been limited exploration of generative LLMs. This study evaluates the performance of generative LLMs (Bloom, Gemma, Llama) in binary and multi-class requirements classification using three datasets. The results highlight the importance of factors like prompt design and LLM architecture, as well as the impact of dataset variations on classification task complexity. The findings suggest the need to optimize prompt structures and align model architectures with specific task requirements for improved performance. The insights from this study can guide future model development and deployment strategies in requirements engineering. 

<br /><br />Summary: <div>
arXiv:2504.16768v1 Announce Type: new 
Abstract: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Framework for AI Systems in "the Wild"</title>
<link>https://arxiv.org/abs/2504.16778</link>
<guid>https://arxiv.org/abs/2504.16778</guid>
<content:encoded><![CDATA[
<div> Evaluation, GenAI systems, real world, holistic, continuous<br />
Summary:<br />
This white paper discusses the shortcomings of current evaluation methods for Generative AI (GenAI) models and proposes a comprehensive framework for assessing real-world GenAI systems. It emphasizes the importance of diverse and evolving inputs, as well as the need for holistic, dynamic, and ongoing assessment approaches. The paper provides guidance for practitioners on designing evaluation methods that accurately reflect real-time capabilities, and offers recommendations for policymakers to focus on societal impacts rather than fixed performance metrics. It advocates for frameworks that integrate performance, fairness, and ethics, and suggests the use of continuous, outcome-oriented assessment methods that combine human and automated evaluations. Transparency is highlighted as crucial for building trust among stakeholders. By implementing these strategies, GenAI models can be both technically proficient and ethically responsible, ensuring their overall impact on society is positive. <br /> <div>
arXiv:2504.16778v1 Announce Type: new 
Abstract: Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores</title>
<link>https://arxiv.org/abs/2504.16786</link>
<guid>https://arxiv.org/abs/2504.16786</guid>
<content:encoded><![CDATA[
<div> Keywords: token-classification, long-context compression, outlier scores, BERT-based compressor, resource-constrained environments

Summary:
MOOSComp is a proposed method for long-context compression in language models, addressing challenges in inference time and resource consumption. It enhances a BERT-based compressor by tackling the over-smoothing issue and incorporating outlier scores to preserve critical tokens. In the training phase, an inter-class cosine similarity loss term is added to improve token classification accuracy. During compression, outlier scores are used to retain rare but important tokens that may be discarded in traditional compression methods. The method demonstrates superior performance on long-context understanding and reasoning benchmarks at various compression ratios. It also achieves a 3.3x speedup at a 4x compression ratio on a resource-constrained mobile device. This approach shows promise in improving the efficiency and effectiveness of language model compression techniques. 

Summary: <div>
arXiv:2504.16786v1 Announce Type: new 
Abstract: Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credible plan-driven RAG method for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2504.16787</link>
<guid>https://arxiv.org/abs/2504.16787</guid>
<content:encoded><![CDATA[
<div> Keyword: Multi-hop question answering, Retrieval-Augmented Generation, Plan-then-Act-and-Review (PAR RAG) framework, reasoning paths, error propagation

Summary:
The article introduces the Plan-then-Act-and-Review (PAR RAG) framework for multi-hop question answering, which addresses the challenge of error propagation in complex queries. The framework consists of three stages: planning, act, and review, providing an interpretable and incremental reasoning paradigm to enhance accuracy and reliability in answering multi-hop questions. PAR RAG decomposes queries into logical reasoning paths and incorporates a plan execution mechanism based on multi-granularity verification to adjust intermediate results. This approach prevents deviations in reasoning paths and errors from accumulating, ensuring the accuracy of the entire reasoning process. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework outperforms existing state-of-the-art methods in EM and F1 scores.
<br /><br />Summary: <div>
arXiv:2504.16787v1 Announce Type: new 
Abstract: Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention</title>
<link>https://arxiv.org/abs/2504.16795</link>
<guid>https://arxiv.org/abs/2504.16795</guid>
<content:encoded><![CDATA[
<div> Hierarchical Sparse Attention, RNNs, Transformers, long sequences, attention mechanisms
<br />
Summary:
Hierarchical Sparse Attention (HSA) is proposed to enhance RNNs by allowing long-range random access flexibility while maintaining efficiency. HSA divides inputs into chunks, hierarchically aggregating information and learning token-to-chunk relevance for precise chunk selection. A hardware-aligned kernel design makes HSA efficient. Combining HSA with Mamba results in RAMba, achieving perfect accuracy in passkey retrieval despite pre-training on short contexts. RAMba shows significant improvements on downstream tasks with consistent memory usage, indicating potential in long-context modeling. <div>
arXiv:2504.16795v1 Announce Type: new 
Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-assisted Graph-RAG Information Extraction from IFC Data</title>
<link>https://arxiv.org/abs/2504.16813</link>
<guid>https://arxiv.org/abs/2504.16813</guid>
<content:encoded><![CDATA[
<div> IFC data, Graph-RAG, LLMs, building information standard, construction industry
<br />
Utilizing Graph Retrieval-Augmented Generation (Graph-RAG), this research investigates the parsing of complex IFC data using generative Large Language Models (LLMs) like GPT-4o. The goal is to extract building object properties and their relationships from the intricate hierarchy of IFC data. Despite the complexity of IFC data, the Graph-RAG parsing approach proves effective in enhancing LLMs with graph-based knowledge, allowing for natural language query-response retrieval without the need for a complicated pipeline. The study showcases how LLMs can leverage Graph-RAG to improve the understanding and utilization of IFC data within the construction industry, highlighting the potential for streamlining collaborative work processes. 
<br /><br />Summary: <div>
arXiv:2504.16813v1 Announce Type: new 
Abstract: IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning</title>
<link>https://arxiv.org/abs/2504.16832</link>
<guid>https://arxiv.org/abs/2504.16832</guid>
<content:encoded><![CDATA[
<div> reasoning model, Vietnamese, GreenMind-Medium-14B-R1, Group Relative Policy Optimization, Sentence Transformer-based models <br />
Summary: Chain-of-Thought (CoT) introduces GreenMind-Medium-14B-R1, a Vietnamese reasoning model fine-tuned using Group Relative Policy Optimization. The model utilizes a Vietnamese synthesized reasoning dataset and implements two reward functions to address language mixing and ensure factual correctness. Experimental results on the VLSP 2023 Challenge dataset prove the model's superiority over previous methods and its enhanced linguistic consistency. Evaluation on SeaExam, a multilingual multiple-choice dataset, highlights the model's effectiveness compared to few-shot prompting techniques. The model's innovative approach showcases significant advancements in tackling LLM tasks that require intermediate reasoning steps prior to generating final answers. <div>
arXiv:2504.16832v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monte Carlo Planning with Large Language Model for Text-Based Game Agents</title>
<link>https://arxiv.org/abs/2504.16855</link>
<guid>https://arxiv.org/abs/2504.16855</guid>
<content:encoded><![CDATA[
<div> Keywords: text-based games, Monte Carlo Tree Search, reinforcement learning, Large Language Models, language understanding

Summary:
The paper introduces the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm for text-based games. Traditional planning-then-learning paradigms like Monte Carlo Tree Search (MCTS) are time-consuming and lack language understanding and reasoning abilities. MC-DML combines Large Language Models (LLMs) with tree search algorithms to leverage language understanding and reasoning capabilities. The algorithm enhances LLMs with memory mechanisms for learning from past experiences and dynamically adjusting action evaluations during planning. Experiments on text-based games show that MC-DML outperforms other methods in the initial planning phase, demonstrating its effectiveness in language-grounded planning in complex environments. This research opens the door to more efficient language-driven planning in autonomous agents. 

<br /><br />Summary: <div>
arXiv:2504.16855v1 Announce Type: new 
Abstract: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification</title>
<link>https://arxiv.org/abs/2504.16856</link>
<guid>https://arxiv.org/abs/2504.16856</guid>
<content:encoded><![CDATA[
<div> data synthesis, sentiment analysis, large language models, emotion understanding, Emo Pillars  

Summary:
The article introduces a novel approach to address the limitations of sentiment analysis datasets lacking context and emotion categories. By leveraging a large language model, Mistral-7b, the researchers design a data synthesis pipeline to generate training examples for lightweight BERT-type encoder models. The focus is on increasing the semantic diversity of examples and grounding the generation in a corpus of narratives to produce unique story-character-centered utterances across 28 emotion classes. Through extensive inferences, the dataset of 100K contextual and 300K context-less examples is created. Fine-tuning pre-trained encoders results in the development of Emo Pillars models, which demonstrate high adaptability to new domains and task-specific tuning. The models achieve state-of-the-art performance in sentiment analysis tasks like GoEmotions, ISEAR, and IEMOCAP. Validation through statistical analysis and human evaluation confirms the success of measures in utterance diversification and context personalization, while highlighting the need for improved handling of out-of-taxonomy labels within the pipeline. 

<br /><br />Summary: <div>
arXiv:2504.16856v1 Announce Type: new 
Abstract: Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning with Diffusion Models for Target-Oriented Dialogue Systems</title>
<link>https://arxiv.org/abs/2504.16858</link>
<guid>https://arxiv.org/abs/2504.16858</guid>
<content:encoded><![CDATA[
<div> Dialogue Planning, Target-Oriented Dialogue, LLM era, Non-Sequential Planning, Diffusion Models <br />
Summary:<br />
The article introduces DiffTOD, a novel framework for Target-Oriented Dialogue planning in the era of Large Language Models (LLM). It addresses the limitations of existing sequential planning methods by leveraging diffusion models to enable non-sequential planning. DiffTOD formulates dialogue planning as a trajectory generation problem with guidance and utilizes a diffusion language model to estimate dialogue trajectory likelihood. It introduces tailored guidance mechanisms for different target types to optimize action strategies in diverse TOD scenarios. Through extensive experiments, DiffTOD demonstrates non-myopic lookahead exploration and effective optimization of action strategies over long horizons. The framework showcases flexibility and strong performance across complex dialogue settings. The code and data are available for accessibility. <br /> <div>
arXiv:2504.16858v1 Announce Type: new 
Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models know who did what to whom?</title>
<link>https://arxiv.org/abs/2504.16884</link>
<guid>https://arxiv.org/abs/2504.16884</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, understanding, thematic roles, sentence representations, attention heads

Summary:
Large Language Models (LLMs) are criticized for lacking language understanding, focusing on cognitive abilities different from language processing. This study investigates whether LLMs can capture thematic roles in sentences, linked closely to language. Two experiments analyze sentence representations in four LLMs. Contrary to human judgments, LLMs prioritize syntactic similarity over agent and patient assignments' accuracy. Thematic role information doesn't seem prominent in hidden units but is detected in some attention heads consistently. While LLMs can extract thematic roles, they represent them weaker compared to humans. Thematic roles are captured independently of syntax by attention heads, suggesting the potential for LLMs to understand this aspect of language, though less prominently than expected. 

<br /><br />Summary: <div>
arXiv:2504.16884v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text</title>
<link>https://arxiv.org/abs/2504.16913</link>
<guid>https://arxiv.org/abs/2504.16913</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text, detection, language model, Chain-of-Thought reasoning, interpretability

Summary:
COT Fine-tuned is a new framework for detecting AI-generated text and identifying the responsible language model. It uses a dual-task approach, classifying text as AI-generated or human-written (Task A) and identifying the specific LLM (Task B). The model utilizes Chain-of-Thought reasoning to explain its predictions, enhancing transparency and interpretability. Experimental results show high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. The CoT reasoning process significantly contributes to the model's effectiveness and interpretability. Overall, COT Fine-tuned offers a promising solution for detecting AI-generated text and attributing it to specific language models.`<br /><br />Summary:` <div>
arXiv:2504.16913v1 Announce Type: new 
Abstract: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
<div> Keywords: Optimization, Natural Language Processing, Artificial Intelligence, Multi-agent Collaboration, Performance Improvement

Summary: 
OptimAI is a framework that uses AI agents powered by LLM to solve optimization problems described in natural language, outperforming existing methods. It consists of a formulator for translating natural language to mathematical formulations, a planner for high-level solution strategies, and coder and code critic roles for interaction and feedback. Ablation studies show the importance of all roles, with removing planner or code critic resulting in productivity drops. Incorporating UCB-based debug scheduling for plan switching enhances productivity. The framework emphasizes multi-agent collaboration for exploring diverse models within a unified system. Its accuracy on NLP4LP dataset and Optibench subset demonstrates significant error rate reduction compared to prior results. <div>
arXiv:2504.16918v1 Announce Type: new 
Abstract: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IberBench: LLM Evaluation on Iberian Languages</title>
<link>https://arxiv.org/abs/2504.16921</link>
<guid>https://arxiv.org/abs/2504.16921</guid>
<content:encoded><![CDATA[
<div> evaluation, language models, benchmark, NLP tasks, Iberian Peninsula

Summary:<br />
- Large Language Models (LLMs) are challenging to evaluate comprehensively, especially in languages other than English.
- Existing benchmarks are primarily English-centric and overlook the diversity of language varieties.
- IberBench is a new benchmark designed to assess LLM performance on fundamental and industry-relevant NLP tasks in languages spoken across the Iberian Peninsula and Ibero-America.
- The benchmark integrates 101 datasets covering 22 task categories and allows for continual updates and community-driven submissions.
- Evaluating 23 LLMs of various sizes, the study reveals that LLMs perform better in fundamental NLP tasks than in industrial tasks, with varying performance across different languages and tasks. <div>
arXiv:2504.16921v1 Announce Type: new 
Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperative Speech, Semantic Competence, and AI</title>
<link>https://arxiv.org/abs/2504.16092</link>
<guid>https://arxiv.org/abs/2504.16092</guid>
<content:encoded><![CDATA[
<div> knowledge, cooperative speech, respect, language models, semantic competence
Summary:
Cooperative speech aims at transmitting knowledge with respect, entailing a mutual obligation to reciprocate respect. However, large language models (LLMs) do not exhibit the necessary respect of cooperative interlocutors. This lack of respect suggests that LLMs are not capable of making assertions, calling into question their semantic competence. Moreover, the discussion on meaning goes beyond cognitive psychology to include moral psychology. In essence, the absence of reciprocal respect in LLMs challenges their role in cooperative communication and raises doubts about their ability to convey true knowledge through language. <div>
arXiv:2504.16092v1 Announce Type: cross 
Abstract: Cooperative speech is purposive. From the speaker's perspective, one crucial purpose is the transmission of knowledge. Cooperative speakers care about getting things right for their conversational partners. This attitude is a kind of respect. Cooperative speech is an ideal form of communication because participants have respect for each other. And having respect within a cooperative enterprise is sufficient for a particular kind of moral standing: we ought to respect those who have respect for us. Respect demands reciprocity. I maintain that large language models aren't owed the kind of respect that partly constitutes a cooperative conversation. This implies that they aren't cooperative interlocutors, otherwise we would be obliged to reciprocate the attitude. Leveraging this conclusion, I argue that present-day LLMs are incapable of assertion and that this raises an overlooked doubt about their semantic competence. One upshot of this argument is that knowledge of meaning isn't just a subject for the cognitive psychologist. It's also a subject for the moral psychologist.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing</title>
<link>https://arxiv.org/abs/2504.16112</link>
<guid>https://arxiv.org/abs/2504.16112</guid>
<content:encoded><![CDATA[
<div> Keywords: attention layer, Transformer-based LLMs, High-bandwidth Processing Unit, memory-intensive co-processor, GPU offloading

Summary: 
The paper introduces the concept of a High-bandwidth Processing Unit (HPU) designed to address inefficiencies in GPU systems caused by the attention layer in Transformer-based Large Language Models (LLMs). The HPU serves as a memory-intensive co-processor that offloads memory-bound operations from the GPU, allowing it to focus on compute-intensive tasks, ultimately improving overall efficiency. By implementing the HPU as an add-on card to the GPU system, it can scale out to meet the growing memory demands of large batch sizes and extended sequence lengths. The prototype HPU, utilizing PCIe-based FPGA cards, demonstrated significant performance gains of up to 4.1x and energy efficiency improvements of 4.6x compared to a GPU-only system. This GPU-HPU heterogeneous system offers scalability without the need for additional GPUs, making it a promising solution for enhancing the performance of large-batched LLM inference. 

<br /><br />Summary: <div>
arXiv:2504.16112v1 Announce Type: cross 
Abstract: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval</title>
<link>https://arxiv.org/abs/2504.16121</link>
<guid>https://arxiv.org/abs/2504.16121</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, regulatory documents, bilingual question-answering, Retrieval Augmented Generation, Bangladesh Police Gazettes

Summary:
Our study focuses on bridging the gap in utilizing NLP and computational linguistic techniques in legal and regulatory tasks, particularly in analyzing the bilingual Bangladesh Police Gazettes. We introduce an efficient bilingual question-answering framework that leverages modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. Our advanced RAG-based approach significantly improves retrieval performance, resulting in more precise answers and making legal information more accessible. By evaluating both our proposed and conventional RAG systems on a diverse test set of Bangladesh Police Gazettes, we consistently outperform existing methods across all evaluation metrics. This framework not only streamlines the process of searching for specific government legal notices but also showcases the potential for NLP in the legal and regulatory domain. 

<br /><br />Summary: <div>
arXiv:2504.16121v1 Announce Type: cross 
Abstract: Natural Language Processing (NLP) and computational linguistic techniques are increasingly being applied across various domains, yet their use in legal and regulatory tasks remains limited. To address this gap, we develop an efficient bilingual question-answering framework for regulatory documents, specifically the Bangladesh Police Gazettes, which contain both English and Bangla text. Our approach employs modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. In addition to conventional RAG pipelines, we propose an advanced RAG-based approach that improves retrieval performance, leading to more precise answers. This system enables efficient searching for specific government legal notices, making legal information more accessible. We evaluate both our proposed and conventional RAG systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that our approach consistently outperforms existing methods across all evaluation metrics.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends</title>
<link>https://arxiv.org/abs/2504.16134</link>
<guid>https://arxiv.org/abs/2504.16134</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic safety, Multimodal Large Language Models, Advanced Driver-Assistance Systems, adversarial robustness, scene understanding

Summary: 
Multimodal Large Language Models (MLLMs) have the potential to revolutionize traffic safety by integrating cross-modal data for holistic scene understanding. This review explores how MLLMs can enhance perception, decision-making, and adversarial robustness in dynamic real-world scenarios, where traditional Advanced Driver-Assistance Systems often struggle. By leveraging datasets such as KITTI, DRAMA, and ML4RoadSafety, researchers are advancing the field to improve road safety. Future directions include real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, scalable and context-aware solutions can be developed to proactively mitigate risks on the road. <div>
arXiv:2504.16134v1 Announce Type: cross 
Abstract: Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design</title>
<link>https://arxiv.org/abs/2504.16204</link>
<guid>https://arxiv.org/abs/2504.16204</guid>
<content:encoded><![CDATA[
<div> framework, prompt engineering, AI systems, societal values, ethical considerations <br />
Summary: Responsible prompt engineering is crucial for ensuring that generative AI systems serve society's needs while minimizing potential harms. This article introduces a comprehensive framework consisting of prompt design, system selection, system configuration, performance evaluation, and prompt management. By embedding ethical and legal considerations directly into AI interactions, organizations can promote improved societal outcomes and mitigate risks. The balance between technical precision and ethical consciousness is key in responsible prompt engineering. This approach aligns with "Responsibility by Design" principles, integrating ethical considerations into the implementation process. Real-world and emerging practices demonstrate the significance of responsible prompt engineering as a bridge between AI development and deployment. The article also highlights key research directions and practical guidelines for advancing the field.<br /><br /> <div>
arXiv:2504.16204v1 Announce Type: cross 
Abstract: Responsible prompt engineering has emerged as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. This article examines how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of real-world and emerging practices, the article illustrates how responsible prompt engineering serves as a crucial bridge between AI development and deployment, enabling organizations to fine-tune AI outputs without modifying underlying model architectures. This approach aligns with broader "Responsibility by Design" principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Phonemes in cascaded S2S translation pipeline</title>
<link>https://arxiv.org/abs/2504.16234</link>
<guid>https://arxiv.org/abs/2504.16234</guid>
<content:encoded><![CDATA[
<div> phonemes, multilingual, speech-to-speech translation, sequence-to-sequence model, BLEU metric

Summary:<br /><br />This paper explores using phonemes instead of text-based language representations in a multilingual speech-to-speech translation system. The study trained a sequence-to-sequence model on the WMT17 dataset in two formats: standard textual representation and phonemic representation. Performance was evaluated using the BLEU metric, showing that the phonemic approach offers comparable quality with advantages such as lower resource requirements and better suitability for low-resource languages. This suggests that incorporating phonemes into translation pipelines can be a viable alternative to traditional text-based methods, particularly for languages with limited resources. <div>
arXiv:2504.16234v1 Announce Type: cross 
Abstract: This paper explores the idea of using phonemes as a textual representation within a conventional multilingual simultaneous speech-to-speech translation pipeline, as opposed to the traditional reliance on text-based language representations. To investigate this, we trained an open-source sequence-to-sequence model on the WMT17 dataset in two formats: one using standard textual representation and the other employing phonemic representation. The performance of both approaches was assessed using the BLEU metric. Our findings shows that the phonemic approach provides comparable quality but offers several advantages, including lower resource requirements or better suitability for low-resource languages.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents</title>
<link>https://arxiv.org/abs/2504.16264</link>
<guid>https://arxiv.org/abs/2504.16264</guid>
<content:encoded><![CDATA[
<div> dataset, cross-lingual information retrieval, academic search, retrieval methods, multilingual retrievers <br />
<br />
Summary: <br />
The paper introduces CLIRudit, a dataset for evaluating cross-lingual academic search between English queries and French documents. Various zero-shot retrieval methods were benchmarked on the dataset, revealing that large dense retrievers can achieve comparable performance to human translations without using machine translation. Sparse retrievers, like BM25 or SPLADE, combined with document translation, also show competitive results. This research enhances understanding of cross-lingual academic information retrieval and offers a framework for creating similar datasets in different languages and disciplines. The dataset and code are made publicly available to facilitate further research on improving access to scientific knowledge across language barriers. <br /> <div>
arXiv:2504.16264v1 Announce Type: cross 
Abstract: Cross-lingual information retrieval (CLIR) consists in finding relevant documents in a language that differs from the language of the queries. This paper presents CLIRudit, a new dataset created to evaluate cross-lingual academic search, focusing on English queries and French documents. The dataset is built using bilingual article metadata from \'Erudit, a Canadian publishing platform, and is designed to represent scenarios in which researchers search for scholarly content in languages other than English. We perform a comprehensive benchmarking of different zero-shot first-stage retrieval methods on the dataset, including dense and sparse retrievers, query and document machine translation, and state-of-the-art multilingual retrievers. Our results show that large dense retrievers, not necessarily trained for the cross-lingual retrieval task, can achieve zero-shot performance comparable to using ground truth human translations, without the need for machine translation. Sparse retrievers, such as BM25 or SPLADE, combined with document translation, show competitive results, providing an efficient alternative to large dense models. This research advances the understanding of cross-lingual academic information retrieval and provides a framework that others can use to build comparable datasets across different languages and disciplines. By making the dataset and code publicly available, we aim to facilitate further research that will help make scientific knowledge more accessible across language barriers.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SignX: The Foundation Model for Sign Recognition</title>
<link>https://arxiv.org/abs/2504.16315</link>
<guid>https://arxiv.org/abs/2504.16315</guid>
<content:encoded><![CDATA[
<div> framework, sign recognition, pose information, ASL signs, video processing
Summary:
SignX is a new framework for sign recognition that aims to translate ASL signs from RGB videos to English-based ID glosses using consistent glossing conventions. The framework consists of a Pose2Gloss component that combines multiple pose information sources into a single representation and a Video2Pose module that directly converts raw video into signer pose representation. SignX allows for compatibility with existing pose formats, improving sign recognition accuracy compared to previous methods. The framework's 2-stage training enables common pose estimation for sign recognition, enhancing the recognition of signs from sign language videos. <div>
arXiv:2504.16315v1 Announce Type: cross 
Abstract: The complexity of sign language data processing brings many challenges. The current approach to recognition of ASL signs aims to translate RGB sign language videos through pose information into English-based ID glosses, which serve to uniquely identify ASL signs. Note that there is no shared convention for assigning such glosses to ASL signs, so it is essential that the same glossing conventions are used for all of the data in the datasets that are employed. This paper proposes SignX, a foundation model framework for sign recognition. It is a concise yet powerful framework applicable to multiple human activity recognition scenarios. First, we developed a Pose2Gloss component based on an inverse diffusion model, which contains a multi-track pose fusion layer that unifies five of the most powerful pose information sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens Segmentation--into a single latent pose representation. Second, we trained a Video2Pose module based on ViT that can directly convert raw video into signer pose representation. Through this 2-stage training framework, we enable sign language recognition models to be compatible with existing pose formats, laying the foundation for the common pose estimation necessary for sign recognition. Experimental results show that SignX can recognize signs from sign language video, producing predicted gloss representations with greater accuracy than has been reported in prior work.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: Near-Optimal Data Attribution for Deep Learning</title>
<link>https://arxiv.org/abs/2504.16430</link>
<guid>https://arxiv.org/abs/2504.16430</guid>
<content:encoded><![CDATA[
<div> data attribution, predictive, model predictions, convex settings, metadifferentiation

Summary:
The article introduces a new data attribution method called MAGIC, designed to estimate the impact of adding or removing training data on model predictions in large-scale, non-convex settings. The goal of predictive data attribution is to accurately assess the influence of specific training datapoints on model outputs. In convex settings, existing methods like the infinitesimal jackknife provide straightforward solutions. However, these methods are less effective in non-convex scenarios, where current approaches often fall short in accurately correlating estimates with ground truth. MAGIC combines classical techniques with recent advancements in metadifferentiation to optimize the estimation of how model predictions are affected by modifications to the training data. This integration allows MAGIC to provide nearly optimal estimates and greatly improve the accuracy in large-scale, non-convex settings. <div>
arXiv:2504.16430v1 Announce Type: cross 
Abstract: The goal of predictive data attribution is to estimate how adding or removing a given set of training datapoints will affect model predictions. In convex settings, this goal is straightforward (i.e., via the infinitesimal jackknife). In large-scale (non-convex) settings, however, existing methods are far less successful -- current methods' estimates often only weakly correlate with ground truth. In this work, we present a new data attribution method (MAGIC) that combines classical methods and recent advances in metadifferentiation to (nearly) optimally estimate the effect of adding or removing training data on model predictions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title>
<link>https://arxiv.org/abs/2504.16628</link>
<guid>https://arxiv.org/abs/2504.16628</guid>
<content:encoded><![CDATA[
<div> algorithm, language models, human preferences, Pareto optimization, multiobjective alignment

Summary:
The article introduces ParetoHqD, a novel method for aligning large language models with multiple human expectations and values. It addresses issues such as inappropriate preference representations and imbalanced reward scores that limit the performance of existing algorithms. ParetoHqD represents human preferences as preference directions in the objective space and considers data near the Pareto front as "high-quality" data. It follows a two-stage supervised fine-tuning process for each preference, using individual Pareto high-quality training sets that match its preference direction. Experimental results show ParetoHqD's superiority over five baselines on two multiobjective alignment tasks, highlighting its effectiveness in aligning language models with diverse user needs. <div>
arXiv:2504.16628v1 Announce Type: cross 
Abstract: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</title>
<link>https://arxiv.org/abs/2504.16728</link>
<guid>https://arxiv.org/abs/2504.16728</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hypothesis generation, Human-in-the-loop, Monte Carlo Tree Search, open-source platform

Summary: 
IRIS is an Interactive Research Ideation System that aims to enhance scientific ideation by leveraging large language models (LLMs) and incorporating a Human-in-the-loop approach. The system includes features such as adaptive test-time compute expansion through Monte Carlo Tree Search, fine-grained feedback mechanisms, and query-based literature synthesis. Researchers can utilize IRIS to generate novel hypotheses in a more transparent and controllable manner, empowering them throughout the ideation process. A user study with researchers from various disciplines validates the effectiveness of the system in improving ideation. The code for IRIS is open-source and available on GitHub, allowing researchers to benefit from its innovative features for accelerating scientific discovery. 

<br /><br />Summary: <div>
arXiv:2504.16728v1 Announce Type: cross 
Abstract: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
<div> Keywords: PRM, ThinkPRM, verification, process labels, test-time scaling

Summary: 
ThinkPRM introduces a data-efficient approach to building process reward models (PRMs) for step-level verification tasks. By training on fewer process labels compared to discriminative PRMs, ThinkPRM leverages the reasoning abilities of long chain-of-thought models to outperform baselines on various benchmarks such as ProcessBench, MATH-500, and AIME '24. In out-of-domain evaluations on GPQA-Diamond and LiveCodeBench, ThinkPRM surpasses discriminative verifiers trained on a larger dataset. The approach also demonstrates effective scaling of verification compute compared to LLM-as-a-Judge under the same token budget. The work highlights the benefits of generative, long CoT PRMs in reducing the supervision required for training while enabling efficient test-time computation for verification tasks. The code, data, and models for ThinkPRM will be made available on the GitHub repository. 

<br /><br />Summary: <div>
arXiv:2504.16828v1 Announce Type: cross 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset</title>
<link>https://arxiv.org/abs/2504.16891</link>
<guid>https://arxiv.org/abs/2504.16891</guid>
<content:encoded><![CDATA[
<div> mathematical reasoning, AI, dataset, code execution, model training

Summary: 
This paper discusses the winning submission to the AI Mathematical Olympiad - Progress Prize 2 competition, outlining a methodology for building advanced mathematical reasoning models. The approach is grounded on a large dataset of math problems and solutions, including a novel method that integrates code execution with long reasoning models. By using iterative training and generation techniques, the researchers were able to produce high-quality Tool-Integrated Reasoning solutions. Additionally, a model training pipeline was developed to select the most promising solution from multiple candidates. The study demonstrates that generative solution selection (GenSelect) can enhance performance compared to a majority voting baseline. By adopting these strategies, the researchers achieved state-of-the-art results on mathematical reasoning benchmarks. To support further research in the field, the code, models, and the comprehensive OpenMathReasoning dataset have been made publicly available under a commercially permissive license. <div>
arXiv:2504.16891v1 Announce Type: cross 
Abstract: This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion</title>
<link>https://arxiv.org/abs/2111.15473</link>
<guid>https://arxiv.org/abs/2111.15473</guid>
<content:encoded><![CDATA[
<div> Keywords: Spectral techniques, Transformers, Fourier Transform, MultiDomain Fourier Wavelet Attention, Abstractive summarization<br />
Summary: 
The article explores the use of spectral techniques to replace the attention mechanism in Transformers by employing Fourier Transform based token mixing. It presents a novel reformulation in next-generation transformer models, introducing the MultiDomain Fourier Wavelet Attention (MDFWA) that combines frequency and time localized transforms to efficiently capture global and local dependencies. The authors provide detailed mathematical formulations, complexity bounds, gradient formulas, and demonstrate that MDFWA offers subquadratic time and memory costs while enhancing expressive power. Validation on an abstractive summarization task using the PubMed dataset showcases the effectiveness of the approach with additional enhancements like learned frequency bases, adaptive scale selection, and multi-modal extensions. Overall, the study highlights the potential of spectral techniques in improving the efficiency and performance of Transformer models in natural language processing tasks. <br /><br />Summary: <div>
arXiv:2111.15473v2 Announce Type: replace 
Abstract: We revisit the use of spectral techniques to replaces the attention mechanism in Transformers through Fourier Transform based token mixing, and present a comprehensive and novel reformulation of this technique in next generation transformer models. We provide expanded literature context, detailed mathematical formulations of Fourier mixing and causal masking, and introduce a novel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency and time localized transforms to capture both global and local dependencies efficiently. We derive the complexity bounds, gradient formulas, and show that MDFWA achieves sub quadratic time and memory cost while improving expressive power. We validate our design on an abstractive summarization task using PubMed dataset, by enhancing the proposed approach with learned frequency bases, adaptive scale selection, and multi-modal extensions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge</title>
<link>https://arxiv.org/abs/2307.08813</link>
<guid>https://arxiv.org/abs/2307.08813</guid>
<content:encoded><![CDATA[
arXiv:2307.08813v4 Announce Type: replace 
Abstract: Background: Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.
  Results: In this study, we investigate the efficacy of various LLMs in addressing biological tasks, such as the recognition of protein interactions, identification of genes linked to pathways affected by low-dose radiation, and the delineation of gene regulatory relationships. Overall, the larger models exhibited superior performance, indicating their potential for specific tasks that involve the extraction of complex interactions among genes and proteins. Although these models possessed detailed information for distinct gene and protein groups, they faced challenges in identifying groups with diverse functions and in recognizing highly correlated gene regulatory relationships.
  Conclusions: By conducting a comprehensive assessment of the state-of-the-art models using well-established molecular interaction and pathway databases, our study reveals that LLMs can identify genes/proteins associated with pathways of interest and predict their interactions to a certain extent. Furthermore, these models can provide important insights, marking a noteworthy stride toward advancing our understanding of biological systems through AI-assisted knowledge discovery.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A dataset and benchmark for hospital course summarization with adapted large language models</title>
<link>https://arxiv.org/abs/2403.05720</link>
<guid>https://arxiv.org/abs/2403.05720</guid>
<content:encoded><![CDATA[
arXiv:2403.05720v5 Announce Type: replace 
Abstract: Brief hospital course (BHC) summaries are clinical documents that summarize a patient's hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC synthesis. Furthermore, we introduce a benchmark of the summarization performance of two general-purpose LLMs and three healthcare-adapted LLMs. Using clinical notes as input, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We evaluate these LLMs across multiple context-length inputs using natural language similarity metrics. We further conduct a clinical study with five clinicians, comparing clinician-written and LLM-generated BHCs across 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We observe that the Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to increasing context lengths of clinical note inputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference for summaries generated by GPT-4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the original summaries, highlighting the need for qualitative clinical evaluation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens</title>
<link>https://arxiv.org/abs/2403.12766</link>
<guid>https://arxiv.org/abs/2403.12766</guid>
<content:encoded><![CDATA[
arXiv:2403.12766v3 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Sentinel: LLM Agent for Adversarial Purification</title>
<link>https://arxiv.org/abs/2405.20770</link>
<guid>https://arxiv.org/abs/2405.20770</guid>
<content:encoded><![CDATA[
arXiv:2405.20770v4 Announce Type: replace 
Abstract: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic Lyrics Detection Across Languages and Genres</title>
<link>https://arxiv.org/abs/2406.15231</link>
<guid>https://arxiv.org/abs/2406.15231</guid>
<content:encoded><![CDATA[
arXiv:2406.15231v3 Announce Type: replace 
Abstract: In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy</title>
<link>https://arxiv.org/abs/2407.03004</link>
<guid>https://arxiv.org/abs/2407.03004</guid>
<content:encoded><![CDATA[
arXiv:2407.03004v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation</title>
<link>https://arxiv.org/abs/2407.12022</link>
<guid>https://arxiv.org/abs/2407.12022</guid>
<content:encoded><![CDATA[
arXiv:2407.12022v3 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have demonstrated excellent performance, inspiring researchers to explore their use in automating register transfer level (RTL) code generation and improving hardware design efficiency. However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire. To mitigate these issues, we innovatively introduce an iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in current loop. Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA) open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human benchmark. Under similar conditions of data quantity and quality, our approach significantly outperforms the baseline. Extensive experiments validate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lawma: The Power of Specialization for Legal Annotation</title>
<link>https://arxiv.org/abs/2407.16615</link>
<guid>https://arxiv.org/abs/2407.16615</guid>
<content:encoded><![CDATA[
arXiv:2407.16615v2 Announce Type: replace 
Abstract: Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal annotation remains limited. To bridge this gap, we introduce CaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to the machine learning community. We demonstrate that commercial models, such as GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable accuracy, generally falling short of the performance required for legal work. We then demonstrate that small, lightly fine-tuned models outperform commercial models. A few hundred to a thousand labeled examples are usually enough to achieve higher accuracy. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal annotation tasks with some available labeled data, researchers are likely better off using a fine-tuned open-source model.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models</title>
<link>https://arxiv.org/abs/2407.17914</link>
<guid>https://arxiv.org/abs/2407.17914</guid>
<content:encoded><![CDATA[
arXiv:2407.17914v2 Announce Type: replace 
Abstract: Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to capture the multimodal nature of human concept representations.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The advantages of context specific language models: the case of the Erasmian Language Model</title>
<link>https://arxiv.org/abs/2408.06931</link>
<guid>https://arxiv.org/abs/2408.06931</guid>
<content:encoded><![CDATA[
arXiv:2408.06931v2 Announce Type: replace 
Abstract: The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model. However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse. In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam. We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context. This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>lamss: when large language models meet self-skepticism</title>
<link>https://arxiv.org/abs/2409.06601</link>
<guid>https://arxiv.org/abs/2409.06601</guid>
<content:encoded><![CDATA[
arXiv:2409.06601v2 Announce Type: replace 
Abstract: Hallucination is a major challenge for large language models (LLMs), prevent ing their further application in some fields. The skeptical thinking of humankind
  could be useful for LLMs to self-cognition, self-reflection and alleviate their hal lucinations. Inspired by this consideration, we propose a novel approach called
  LaMsS, which combines the semantic understanding capability of LLMs with
  self-skepticism. By introducing a series of skepticism tokens and augmenting
  them into the vocabulary, we conduct both pertaining and finetuning, which allow
  the LLM to decode each normal token followed by a skeptical token, represent ing different skepticism levels. By calculating the response skepticism given a
  query, one can define a new self-aware LLM which is only willing to answer
  with relative lower skepticism level than the threshold. By examining the accu racy, AUC and AP of willingly answering questions, we demonstrate that LaMsS
  achieves better performance than baselines on both multi-choice questions and
  open-domain question-answering benchmarks, and can generalize to multi-task
  and out-of-domain settings. Our study sheds some lights on the self-skepticism
  modeling on further artificial intelligence. Project code and model checkpoints
  can be found in https://anonymous.4open.science/r/SM-1E76.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems</title>
<link>https://arxiv.org/abs/2410.19572</link>
<guid>https://arxiv.org/abs/2410.19572</guid>
<content:encoded><![CDATA[
arXiv:2410.19572v5 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEG: Medical Knowledge-Augmented Large Language Models for Question Answering</title>
<link>https://arxiv.org/abs/2411.03883</link>
<guid>https://arxiv.org/abs/2411.03883</guid>
<content:encoded><![CDATA[
arXiv:2411.03883v3 Announce Type: replace 
Abstract: Question answering is a natural language understanding task that involves reasoning over both explicit context, and unstated relevant domain knowledge. Despite the high cost of training, large language models (LLMs) -- the backbone of most modern question-answering systems -- still struggle to reliably capture the nuanced relationships between concepts that are crucial for reasoning in specialized fields like medicine. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to incorporate knowledge graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain significant advantages from the factual grounding these embeddings provide. MEG attains an average of +6.7% and +9.9% accuracy over specialized models like BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's performance remains robust to the choice of graph encoder.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title>
<link>https://arxiv.org/abs/2411.05000</link>
<guid>https://arxiv.org/abs/2411.05000</guid>
<content:encoded><![CDATA[
arXiv:2411.05000v2 Announce Type: replace 
Abstract: As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</title>
<link>https://arxiv.org/abs/2411.06037</link>
<guid>https://arxiv.org/abs/2411.06037</guid>
<content:encoded><![CDATA[
arXiv:2411.06037v3 Announce Type: replace 
Abstract: Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a method to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2--10\% for Gemini, GPT, and Gemma. Key findings and the prompts used in our autorater analysis are available on our github.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement</title>
<link>https://arxiv.org/abs/2412.06845</link>
<guid>https://arxiv.org/abs/2412.06845</guid>
<content:encoded><![CDATA[
arXiv:2412.06845v4 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v3 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?</title>
<link>https://arxiv.org/abs/2502.04718</link>
<guid>https://arxiv.org/abs/2502.04718</guid>
<content:encoded><![CDATA[
arXiv:2502.04718v2 Announce Type: replace 
Abstract: Text style transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, as is common in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks, sentiment transfer and detoxification, in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of large language models (LLMs) as tools for TST evaluation. Our findings highlight newly applied advanced NLP metrics and LLM-based evaluations provide better insights than existing TST metrics. Our oracle ensemble approaches show even more potential.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization</title>
<link>https://arxiv.org/abs/2502.13108</link>
<guid>https://arxiv.org/abs/2502.13108</guid>
<content:encoded><![CDATA[
arXiv:2502.13108v2 Announce Type: replace 
Abstract: Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support.
  To address this limitation, we introduce a Multi-Task Learning (MTL) framework that jointly trains CQA models for both answer extraction and medical categorization. In addition to predicting answer spans, our model classifies responses into five standardized medical categories: Diagnosis, Medication, Symptoms, Procedure, and Lab Reports. This categorization enables more structured and interpretable outputs, making clinical QA models more useful in real-world healthcare settings.
  We evaluate our approach on emrQA, a large-scale dataset for medical question answering. Results show that MTL improves F1-score by 2.2% compared to standard fine-tuning, while achieving 90.7% accuracy in answer categorization. These findings suggest that MTL not only enhances CQA performance but also introduces an effective mechanism for categorization and structured medical information retrieval.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test</title>
<link>https://arxiv.org/abs/2503.01840</link>
<guid>https://arxiv.org/abs/2503.01840</guid>
<content:encoded><![CDATA[
arXiv:2503.01840v3 Announce Type: replace 
Abstract: The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations</title>
<link>https://arxiv.org/abs/2503.14477</link>
<guid>https://arxiv.org/abs/2503.14477</guid>
<content:encoded><![CDATA[
arXiv:2503.14477v2 Announce Type: replace 
Abstract: LLMs often adopt an assertive language style also when making false claims. Such ``overconfident hallucinations'' mislead users and erode trust. Achieving the ability to express in language the actual degree of uncertainty around a claim is therefore of great importance. We find that ``verbal uncertainty'' is governed by a single linear feature in the representation space of LLMs, and show that this has only moderate correlation with the actual ``semantic uncertainty'' of the model. We apply this insight and show that (1) the mismatch between semantic and verbal uncertainty is a better predictor of hallucinations than semantic uncertainty alone and (2) we can intervene on verbal uncertainty at inference time and reduce confident hallucinations on short-form answers, achieving an average relative reduction of ~30%.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
arXiv:2503.16419v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[
arXiv:2503.20533v3 Announce Type: replace 
Abstract: Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users</title>
<link>https://arxiv.org/abs/2504.10157</link>
<guid>https://arxiv.org/abs/2504.10157</guid>
<content:encoded><![CDATA[
arXiv:2504.10157v2 Announce Type: replace 
Abstract: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2404.04545</link>
<guid>https://arxiv.org/abs/2404.04545</guid>
<content:encoded><![CDATA[
arXiv:2404.04545v2 Announce Type: replace-cross 
Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the variation in semantic richness among different modalities, treating each modality uniformly. This approach may lead to underestimating the significance of strong modalities while overemphasizing the importance of weak ones. Motivated by these insights, we introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the predominant role of the text modality in MSA. Specifically, for each multimodal sample, by taking unaligned sequences of the three modalities as inputs, we initially allocate the extracted unimodal features into a visual-text and an acoustic-text pair. Subsequently, we implement self-attention on the text modality and apply text-queried cross-attention to the visual and acoustic modalities. To mitigate the influence of noise signals and redundant features, we incorporate a gated control mechanism into the framework. Additionally, we introduce unimodal joint learning to gain a deeper understanding of homogeneous emotional tendencies across diverse modalities through backpropagation. Experimental results demonstrate that TCAN consistently outperforms state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Situational Safety</title>
<link>https://arxiv.org/abs/2410.06172</link>
<guid>https://arxiv.org/abs/2410.06172</guid>
<content:encoded><![CDATA[
arXiv:2410.06172v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers</title>
<link>https://arxiv.org/abs/2410.22663</link>
<guid>https://arxiv.org/abs/2410.22663</guid>
<content:encoded><![CDATA[
arXiv:2410.22663v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) for text classification has been widely used in various domains. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Studies indicate that conventional metrics are insufficient to build human trust in ML models. These models often learn spurious correlations and predict based on them. In the real world, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are reasonable based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. Due to the lack of automated trustworthiness oracles, the assessment requires manual validation of the decision process disclosed by explanation methods. However, this is time-consuming, error-prone, and unscalable.
  We propose TOKI, the first automated trustworthiness oracle generation method for text classifiers. TOKI automatically checks whether the words contributing the most to a prediction are semantically related to the predicted class. Specifically, we leverage ML explanations to extract the decision-contributing words and measure their semantic relatedness with the class based on word embeddings. We also introduce a novel adversarial attack method that targets trustworthiness vulnerabilities identified by TOKI. To evaluate their alignment with human judgement, experiments are conducted. We compare TOKI with a naive baseline based solely on model confidence and TOKI-guided adversarial attack method with A2T, a SOTA adversarial attack method. Results show that relying on prediction uncertainty cannot effectively distinguish between trustworthy and untrustworthy predictions, TOKI achieves 142% higher accuracy than the naive baseline, and TOKI-guided attack method is more effective with fewer perturbations than A2T.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2411.18203</link>
<guid>https://arxiv.org/abs/2411.18203</guid>
<content:encoded><![CDATA[
arXiv:2411.18203v5 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist</title>
<link>https://arxiv.org/abs/2412.06412</link>
<guid>https://arxiv.org/abs/2412.06412</guid>
<content:encoded><![CDATA[
arXiv:2412.06412v2 Announce Type: replace-cross 
Abstract: With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants</title>
<link>https://arxiv.org/abs/2412.12661</link>
<guid>https://arxiv.org/abs/2412.12661</guid>
<content:encoded><![CDATA[
arXiv:2412.12661v2 Announce Type: replace-cross 
Abstract: Recent advancements in mixed-modal generative have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and generating multimodal patient reports. However, existing datasets face challenges such as small sizes, limited coverage of biomedical tasks and domains, and a reliance on narrow sources. To address these gaps, we present MedMax, a large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MedMax encompasses a diverse range of tasks, including interleaved image-text generation, biomedical image captioning and generation, visual chat, and report understanding. These tasks span knowledge across diverse biomedical domains, including radiology and histopathology, grounded in medical papers and YouTube videos. Subsequently, we fine-tune a mixed-modal foundation model on the MedMax dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-4o across 12 downstream biomedical visual question-answering tasks. Finally, we introduce a unified evaluation suite for biomedical tasks to guide the development of mixed-modal biomedical AI assistants. The data, model, and code is available at https://mint-medmax.github.io/.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language</title>
<link>https://arxiv.org/abs/2503.11662</link>
<guid>https://arxiv.org/abs/2503.11662</guid>
<content:encoded><![CDATA[
arXiv:2503.11662v2 Announce Type: replace-cross 
Abstract: In chip design planning, obtaining reliable performance and power forecasts for various design options is of critical importance. Traditionally, this involves using system-level models, which often lack accuracy, or trial synthesis, which is both labor-intensive and time-consuming. We introduce a new methodology, called Lorecast, which accepts English prompts as input to rapidly generate layout-aware performance and power estimates. This approach bypasses the need for HDL code development and synthesis, making it both fast and user-friendly. Experimental results demonstrate that Lorecast achieves accuracy within a few percent of error compared to post-layout analysis, while significantly reducing turnaround time.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic hashtag recommendation in social media with trend shift detection and adaptation</title>
<link>https://arxiv.org/abs/2504.00044</link>
<guid>https://arxiv.org/abs/2504.00044</guid>
<content:encoded><![CDATA[
arXiv:2504.00044v2 Announce Type: replace-cross 
Abstract: Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing 16,193 LLM Papers for Fun and Profits</title>
<link>https://arxiv.org/abs/2504.08619</link>
<guid>https://arxiv.org/abs/2504.08619</guid>
<content:encoded><![CDATA[
arXiv:2504.08619v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem.
]]></content:encoded>
<pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)</title>
<link>https://arxiv.org/abs/2504.15349</link>
<guid>https://arxiv.org/abs/2504.15349</guid>
<content:encoded><![CDATA[
<div> Keywords: Compositional Generalization, Transformer models, RASP, ReCOGS_pos, Semantic exact match

Summary: 
A study introduces the concept of Compositional Generalization, where humans understand new word combinations from different contexts. Using the Restricted Access Sequence Processing (RASP) model, researchers demonstrate that a Transformer encoder-decoder can effectively perform the ReCOGS_pos task, achieving high accuracy. The RASP model showcases the ability to handle structural generalizations systematically and compositionally without requiring a hierarchical or tree-structured solution. By employing word-level tokens with part-of-speech tagging and flat pattern-matching rules, the model achieves impressive semantic and string exact match scores on various linguistic tasks. Notably, the model exhibits proficiency in handling prepositional phrase and sentential complement recursion using a decoder loop approach. These findings highlight the potential of RASP models in language processing tasks that demand compositional and systematic generalization abilities.

<br /><br />Summary: <div>
arXiv:2504.15349v1 Announce Type: new 
Abstract: Humans understand new combinations of words encountered if they are combinations of words recognized from different contexts, an ability called Compositional Generalization. The COGS benchmark (Kim and Linzen, 2020) arXiv:2010.05465 reports 0% accuracy for Transformer models on some structural generalizations. We use (Weiss et al., 2021) arXiv:2106.06981's Restricted Access Sequence Processing (RASP), a Transformer-equivalent programming language, to prove by construction that a Transformer encoder-decoder can perform the semantically equivalent ReCOGS_pos (Wu et al., 2024) arXiv:2303.13716 variant of COGS systematically and compositionally: Our RASP model attains 100% semantic exact match on the ReCOGS test set and 100% SEM on all generalization splits except obj_pp_to_subj_pp which gets 92%. Furthermore, our RASP model shows the ReCOGS_pos task does not require a hierarchical or tree-structured solution: we use word-level tokens with an "embedding" layer that tags with possible parts of speech, applying just once per encoder pass 19 attention-head compatible flat pattern-matching rules, shown using grammar coverage (Zeller et al., 2023) to be learnable from the training data, plus general prepositional phrase (pp) handling and sentential complement (cp) handling logic, and output the next logical form (LF) token (repeating until the LF is complete). The model does not apply recursive, tree-structured rules like 'np_det pp np -> np_pp -> np', but scores 100% semantic and string exact match on pp recursion, cp recursion using the decoder loop.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection</title>
<link>https://arxiv.org/abs/2504.15392</link>
<guid>https://arxiv.org/abs/2504.15392</guid>
<content:encoded><![CDATA[
<div> investigates, hybrid intelligence, collaboration, sexism, Large Language Models (LLMs)

Summary: 
This paper explores the collaboration between researchers of sexism and Large Language Models (LLMs) through a four-component pipeline. The study involves nine sexism researchers engaging in interactive experiments with an LLM (GPT3.5) to assess the model's knowledge about sexism and its research suitability. Experts were tasked with creating different definitions of sexism, resulting in longer and more complex definitions when interacting with the LLM. While expert-written definitions initially performed poorly in classification tasks, some experts improved classification performance with co-created definitions, even those inexperienced in using LLMs. Zero-shot classification experiments using the definitions from experts tested GPT4o on 2,500 texts from five sexism benchmarks, resulting in 67,500 classification decisions analyzed. <div>
arXiv:2504.15392v1 Announce Type: new 
Abstract: This paper investigates hybrid intelligence and collaboration between researchers of sexism and Large Language Models (LLMs), with a four-component pipeline. First, nine sexism researchers answer questions about their knowledge of sexism and of LLMs. They then participate in two interactive experiments involving an LLM (GPT3.5). The first experiment has experts assessing the model's knowledge about sexism and suitability for use in research. The second experiment tasks them with creating three different definitions of sexism: an expert-written definition, an LLM-written one, and a co-created definition. Lastly, zero-shot classification experiments use the three definitions from each expert in a prompt template for sexism detection, evaluating GPT4o on 2.500 texts sampled from five sexism benchmarks. We then analyze the resulting 67.500 classification decisions. The LLM interactions lead to longer and more complex definitions of sexism. Expert-written definitions on average perform poorly compared to LLM-generated definitions. However, some experts do improve classification performance with their co-created definitions of sexism, also experts who are inexperienced in using LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trillion 7B Technical Report</title>
<link>https://arxiv.org/abs/2504.15431</link>
<guid>https://arxiv.org/abs/2504.15431</guid>
<content:encoded><![CDATA[
<div> Trillion-7B, Korean-centric, multilingual LLM, Cross-lingual Document Attention mechanism, knowledge transfer, English to Korean and Japanese, optimized data mixtures, language-specific filtering, tailored tokenizer construction, competitive performance, 10% training tokens for multilingual data, 59.4K H100 GPU hours, robust multilingual performance, cross-lingual consistency<br /><br />Summary: Trillion-7B is introduced as a highly efficient Korean-centric multilingual LLM using the XLDA mechanism for knowledge transfer between English, Korean, and Japanese. Through optimized data mixtures, language-specific filtering, and tailored tokenizer construction, it achieves competitive performance with minimal resources. Despite dedicating only 10% of training tokens to multilingual data and requiring 59.4K H100 GPU hours for full training, Trillion-7B demonstrates robust multilingual performance and exceptional cross-lingual consistency across 27 benchmarks in four languages. <div>
arXiv:2504.15431v1 Announce Type: new 
Abstract: We introduce Trillion-7B, the most token-efficient Korean-centric multilingual LLM available. Our novel Cross-lingual Document Attention (XLDA) mechanism enables highly efficient and effective knowledge transfer from English to target languages like Korean and Japanese. Combined with optimized data mixtures, language-specific filtering, and tailored tokenizer construction, Trillion-7B achieves competitive performance while dedicating only 10\% of its 2T training tokens to multilingual data and requiring just 59.4K H100 GPU hours (\$148K) for full training. Comprehensive evaluations across 27 benchmarks in four languages demonstrate Trillion-7B's robust multilingual performance and exceptional cross-lingual consistency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feeding LLM Annotations to BERT Classifiers at Your Own Risk</title>
<link>https://arxiv.org/abs/2504.15432</link>
<guid>https://arxiv.org/abs/2504.15432</guid>
<content:encoded><![CDATA[
<div> fine-tune, smaller encoder-only models, text classification, synthetic data, error propagation
Summary:<br /><br />Using LLM-generated labels to fine-tune smaller encoder-only models for text classification can lead to performance degradation, instability, and premature plateaus compared to models trained on gold labels. The study highlights the risks of training on synthetic data and the challenges of propagating errors from LLM annotations to smaller classifiers. Mitigation strategies such as entropy-based filtering and ensemble techniques provide partial relief but do not fully address the inherent risks. Caution is advised when applying this approach in high-stakes text classification tasks. <div>
arXiv:2504.15432v1 Announce Type: new 
Abstract: Using LLM-generated labels to fine-tune smaller encoder-only models for text classification has gained popularity in various settings. While this approach may be justified in simple and low-stakes applications, we conduct empirical analysis to demonstrate how the perennial curse of training on synthetic data manifests itself in this specific setup. Compared to models trained on gold labels, we observe not only the expected performance degradation in accuracy and F1 score, but also increased instability across training runs and premature performance plateaus. These findings cast doubts on the reliability of such approaches in real-world applications. We contextualize the observed phenomena through the lens of error propagation and offer several practical mitigation strategies, including entropy-based filtering and ensemble techniques. Although these heuristics offer partial relief, they do not fully resolve the inherent risks of propagating non-random errors from LLM annotations to smaller classifiers, underscoring the need for caution when applying this workflow in high-stakes text classification tasks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models</title>
<link>https://arxiv.org/abs/2504.15471</link>
<guid>https://arxiv.org/abs/2504.15471</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer language models, activation vectors, bigram predictions, subnetworks, model performance 

Summary:<br /><br />
The study explores the presence of bigram subnetworks within Transformer language models, focusing on next token predictions based solely on the current token. It is found that these subnetworks exist in fully trained models with up to 1B parameters and play a crucial role in model performance despite their small size (less than 0.2% of model parameters). The bigram subnetworks are predominantly located in the first Transformer MLP layer and are closely related to subnetworks optimized for model pruning. Mechanistically, these subnetworks align activations with next token predictions rather than current token representations, driving the transformation process in the residual stream. The discovery of bigram subnetworks offers a foundational approach to studying language model circuits, starting from a minimal subset of parameters essential for basic next token predictions, instead of the conventional method of ablating circuits from a complete model. <div>
arXiv:2504.15471v1 Announce Type: new 
Abstract: In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying language model circuits by building up from a minimal circuit rather than the traditional approach of ablating circuits from a full model.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Sampling via Exponential Races</title>
<link>https://arxiv.org/abs/2504.15475</link>
<guid>https://arxiv.org/abs/2504.15475</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, large language models, inference, channel simulation, generation speed-up <br />
<br />
Summary: Speculative decoding is a technique that accelerates large language model inference using a smaller draft model. This paper establishes a connection between speculative decoding and channel simulation, showing that they are related concepts. An information-theoretic analysis is provided to quantify the speedup achievable by speculative decoding. The relation between generation speed-up and the number of tokens generated by the draft model is derived, serving as an upper bound for all values of k. A novel speculative decoding method, ERSD, is proposed, which demonstrates performance on par with existing methods. This research contributes to improving the efficiency of large language model inference by exploiting speculative decoding techniques. <div>
arXiv:2504.15475v1 Announce Type: new 
Abstract: Speculative decoding accelerates large language model inference using a smaller draft model. In this paper, we establish a surprising connection between speculative decoding and channel simulation, which aims at simulating a noisy channel using as few bits as possible. This connection allows us to provide an information-theoretic analysis of the speed up that can be achieved by speculative decoding. Leveraging this link, we derive an explicit relation between generation speed-up and the number of tokens $k$ generated by the draft model for large $k$, which serves as an upper bound for all $k$. We also propose a novel speculative decoding method via exponential race ERSD that matches state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation</title>
<link>https://arxiv.org/abs/2504.15509</link>
<guid>https://arxiv.org/abs/2504.15509</guid>
<content:encoded><![CDATA[
<div> Simultaneous speech translation, large language models, streaming capability, SimulS2S-LLM, incremental beam search 

Summary:
Simultaneous speech translation (SST) is a challenging task that aims to provide translations in real-time while balancing quality and latency. This paper introduces SimulS2S-LLM, a method that trains speech models offline and uses a test-time policy for simultaneous inference. By extracting boundary-aware speech prompts and predicting discrete output speech tokens, SimulS2S-LLM achieves speech-to-speech translation with improved quality-latency trade-offs compared to existing methods. The use of a pre-trained vocoder and an incremental beam search expands the search space for speech token prediction without increasing latency. Experimental results on CVSS speech data demonstrate that SimulS2S-LLM outperforms other methods by improving ASR-BLEU scores by 3 points at similar latency. <br /><br />Summary: <div>
arXiv:2504.15509v1 Announce Type: new 
Abstract: Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks</title>
<link>https://arxiv.org/abs/2504.15521</link>
<guid>https://arxiv.org/abs/2504.15521</guid>
<content:encoded><![CDATA[
<div> Benchmarks, Multilingual, Evaluation, Language models, Collaborative<br />
Summary: 
The article discusses the importance of multilingual benchmarking for large language models. It analyzes over 2,000 benchmarks from 148 countries and finds that English is overrepresented in these benchmarks. Most benchmarks use original language content from high-resource countries. There are disparities in performance compared to human judgments, with STEM-related tasks showing stronger correlations. Localized benchmarks align better with local judgments than translated ones. The article identifies key limitations in current multilingual evaluation practices and proposes guiding principles for effective benchmarking. It suggests focusing on culturally and linguistically tailored benchmarks rather than translations. The research also outlines five critical directions for progress in the field and calls for a global collaborative effort to develop human-aligned benchmarks for real-world applications.<br /><br />Summary: <div>
arXiv:2504.15521v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property</title>
<link>https://arxiv.org/abs/2504.15524</link>
<guid>https://arxiv.org/abs/2504.15524</guid>
<content:encoded><![CDATA[
<div> patents, intellectual property, large language models, benchmark, tasks <br />
Summary: 
The article introduces a new benchmark called IPBench for evaluating large language models (LLMs) in the field of intellectual property (IP). This benchmark covers 8 IP mechanisms and 20 tasks, allowing for the evaluation of understanding and generation capabilities of LLMs in real-world IP applications. The study benchmarked 16 LLMs, finding that there is room for improvement in their performance, with even the best model achieving only 75.8% accuracy. The research reveals that open-source IP and law-oriented models are lagging behind closed-source general-purpose models in this domain. The authors have made the data and code of IPBench publicly available and plan to update it with additional IP-related tasks to better reflect the challenges in the IP field. <br /><br />Summary: <div>
arXiv:2504.15524v1 Announce Type: new 
Abstract: Intellectual Property (IP) is a unique domain that integrates technical and legal knowledge, making it inherently complex and knowledge-intensive. As large language models (LLMs) continue to advance, they show great potential for processing IP tasks, enabling more efficient analysis, understanding, and generation of IP-related content. However, existing datasets and benchmarks either focus narrowly on patents or cover limited aspects of the IP field, lacking alignment with real-world scenarios. To bridge this gap, we introduce the first comprehensive IP task taxonomy and a large, diverse bilingual benchmark, IPBench, covering 8 IP mechanisms and 20 tasks. This benchmark is designed to evaluate LLMs in real-world intellectual property applications, encompassing both understanding and generation. We benchmark 16 LLMs, ranging from general-purpose to domain-specific models, and find that even the best-performing model achieves only 75.8% accuracy, revealing substantial room for improvement. Notably, open-source IP and law-oriented models lag behind closed-source general-purpose models. We publicly release all data and code of IPBench and will continue to update it with additional IP-related tasks to better reflect real-world challenges in the intellectual property domain.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compass-V2 Technical Report</title>
<link>https://arxiv.org/abs/2504.15527</link>
<guid>https://arxiv.org/abs/2504.15527</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight Mixture-of-Experts model, Southeast Asian languages, e-commerce applications, high-quality dataset, hybrid reasoning model 

Summary: 
Compass-v2 is a lightweight Mixture-of-Experts (MoE) model designed specifically for Southeast Asian languages and e-commerce applications. It addresses the underrepresentation of low-resource languages in the AI field by focusing on SEA languages. The model has a total of 30B parameters, with 5B active parameters, balancing performance and inference cost. A high-quality dataset for SEA languages was curated to enhance multilingual performance, along with a dataset for e-commerce applications sourced through data mining. The model features a hybrid reasoning model that combines fast and deep thinking capabilities, diverging from the industry norm of using separate models. Through experimental evaluations, Compass-v2 demonstrates state-of-the-art performance in multilingual and e-commerce tasks among sub-30B models, while maintaining lower inference costs. 

<br /><br />Summary: <div>
arXiv:2504.15527v1 Announce Type: new 
Abstract: Predominant LLMs focus on high-resource languages while leaving low-resource languages, particularly those in Southeast Asia (SEA), underrepresented. In addition, those models are general-purpose and pay limited attention to the e-commerce domain. To overcome these limitations, we introduce Compass-v2, a lightweight Mixture-of-Experts (MoE) model specifically designed for Southeast Asian languages and e-commerce applications. To balance model performance and inference cost, the model is designed with 30B total parameters and 5B active parameters, incorporating both fine-grained and shared expert modules. To enhance multilingual performance, we curated and constructed a high-quality, industry-leading SEA dataset, to the best of our knowledge. To boost performance in the e-commerce domain, we built a dataset comprising hundreds of billions of tokens, sourced through external data mining and internal platform collection. Besides, we pioneered a hybrid reasoning model that supports both fast thinking and deep thinking within a unified framework to enhance the reasoning capabilities, diverging from the conventional industry practice of deploying two separate models. Through extensive experimental evaluations, our model demonstrates state-of-the-art SEA multilingual and e-commerce performance among sub-30B models, while maintaining significantly lower inference cost.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length</title>
<link>https://arxiv.org/abs/2504.15544</link>
<guid>https://arxiv.org/abs/2504.15544</guid>
<content:encoded><![CDATA[
<div> Transformer models, BERT, encoder-only, pre-training, long-context <br />
Summary: 
The study introduces llm-jp-modernbert, a ModernBERT model trained on a large Japanese corpus with a context length of 8192 tokens. While not outperforming existing baseline models in downstream tasks, it shows good performance in fill-mask test evaluations. The study also explores the impact of context length expansion through pseudo-perplexity experiments. Additionally, it delves into sentence embeddings, analyzing their evolution during training and comparing them with other models. The findings reveal similar trends with models of the same architecture. To encourage reproducibility and advance the development of long-context BERT models, the researchers provide their model, training, and evaluation code for public use. <br /> <div>
arXiv:2504.15544v1 Announce Type: new 
Abstract: Encoder-only transformer models like BERT are widely adopted as a pre-trained backbone for tasks like sentence classification and retrieval. However, pretraining of encoder models with large-scale corpora and long contexts has been relatively underexplored compared to decoder-only transformers. In this work, we present llm-jp-modernbert, a ModernBERT model trained on a publicly available, massive Japanese corpus with a context length of 8192 tokens. While our model does not surpass existing baselines on downstream tasks, it achieves good results on fill-mask test evaluations. We also analyze the effect of context length expansion through pseudo-perplexity experiments. Furthermore, we investigate sentence embeddings in detail, analyzing their transitions during training and comparing them with those from other existing models, confirming similar trends with models sharing the same architecture. To support reproducibility and foster the development of long-context BERT, we release our model, along with the training and evaluation code.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Semantic Augmentation for Harmful Content Detection</title>
<link>https://arxiv.org/abs/2504.15548</link>
<guid>https://arxiv.org/abs/2504.15548</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, text preprocessing, semantic augmentation, social media classification, machine learning pipeline 

Summary: 
- Recent advances in large language models (LLMs) have shown strong performance on simple text classification tasks but struggle with complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification.
- Existing work has mainly focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation.
- This paper introduces an approach that prompts LLMs to clean noisy text and provide context-rich explanations to enhance training sets without significantly increasing data volume.
- Evaluation on multiple datasets shows that zero-shot LLM classification performs poorly on high-context tasks compared to supervised models, while integrating LLM-based semantic augmentation improves performance significantly at a lower cost.
- Strategically incorporating LLMs into the machine learning pipeline for social media classification tasks can have significant implications for combating harmful content online.

<br /><br />Summary: <div>
arXiv:2504.15548v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have demonstrated strong performance on simple text classification tasks, frequently under zero-shot settings. However, their efficacy declines when tackling complex social media challenges such as propaganda detection, hateful meme classification, and toxicity identification. Much of the existing work has focused on using LLMs to generate synthetic training data, overlooking the potential of LLM-based text preprocessing and semantic augmentation. In this paper, we introduce an approach that prompts LLMs to clean noisy text and provide context-rich explanations, thereby enhancing training sets without substantial increases in data volume. We systematically evaluate on the SemEval 2024 multi-label Persuasive Meme dataset and further validate on the Google Jigsaw toxic comments and Facebook hateful memes datasets to assess generalizability. Our results reveal that zero-shot LLM classification underperforms on these high-context tasks compared to supervised models. In contrast, integrating LLM-based semantic augmentation yields performance on par with approaches that rely on human-annotated data, at a fraction of the cost. These findings underscore the importance of strategically incorporating LLMs into machine learning (ML) pipeline for social media classification tasks, offering broad implications for combating harmful content online.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction</title>
<link>https://arxiv.org/abs/2504.15573</link>
<guid>https://arxiv.org/abs/2504.15573</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, instruction-following, data synthesis, WebR, domain adaptation 

Summary: 
The article introduces Web Reconstruction (WebR), a framework for automatically synthesizing high-quality instruction-tuning (IT) data from raw web documents. By utilizing a dual-perspective paradigm - Web as Instruction and Web as Response - WebR reconstructs web content to generate instructional data for language models. Experimental results show that datasets generated by WebR outperform existing methods by up to 16.65% on instruction-following tasks. WebR offers superior compatibility, data efficiency, and scalability, making it easier to adapt language models to different domains. The framework is publicly available, providing researchers with a valuable tool for improving instruction-following capabilities of language models. <br /><br />Summary: <div>
arXiv:2504.15573v1 Announce Type: new 
Abstract: The improvement of LLMs' instruction-following capabilities depends critically on the availability of high-quality instruction-response pairs. While existing automatic data synthetic methods alleviate the burden of manual curation, they often rely heavily on either the quality of seed data or strong assumptions about the structure and content of web documents. To tackle these challenges, we propose Web Reconstruction (WebR), a fully automated framework for synthesizing high-quality instruction-tuning (IT) data directly from raw web documents with minimal assumptions. Leveraging the inherent diversity of raw web content, we conceptualize web reconstruction as an instruction-tuning data synthesis task via a novel dual-perspective paradigm--Web as Instruction and Web as Response--where each web document is designated as either an instruction or a response to trigger the reconstruction process. Comprehensive experiments show that datasets generated by WebR outperform state-of-the-art baselines by up to 16.65% across four instruction-following benchmarks. Notably, WebR demonstrates superior compatibility, data efficiency, and scalability, enabling enhanced domain adaptation with minimal effort. The data and code are publicly available at https://github.com/YJiangcm/WebR.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models</title>
<link>https://arxiv.org/abs/2504.15604</link>
<guid>https://arxiv.org/abs/2504.15604</guid>
<content:encoded><![CDATA[
<div> Model Comparison, Next-Token Prediction, Theory of Mind Tasks, Contextual Complexity, Reasoning Levels
Summary:
The study compares the next-token prediction performance of GPT-2 and Llama-2-7b-chat-hf on Theory of Mind tasks using a dataset created from short stories. The addition of infill sentences increases complexity and ambiguity, slightly reducing prediction accuracy. Llama-2 outperforms GPT-2 in accuracy, especially at lower temperatures, showing higher confidence in token selection. As reasoning complexity increases, model responses diverge. GPT-2 and Llama-2 exhibit more variability in predictions during first- and second-order reasoning tasks. These findings shed light on how model architecture, temperature, and contextual complexity impact next-token prediction, offering insights into the strengths and limitations of current language models.
<br /><br />Summary: <div>
arXiv:2504.15604v1 Announce Type: new 
Abstract: Language models have made significant progress in generating coherent text and predicting next tokens based on input prompts. This study compares the next-token prediction performance of two well-known models: OpenAI's GPT-2 and Meta's Llama-2-7b-chat-hf on Theory of Mind (ToM) tasks. To evaluate their capabilities, we built a dataset from 10 short stories sourced from the Explore ToM Dataset. We enhanced these stories by programmatically inserting additional sentences (infills) using GPT-4, creating variations that introduce different levels of contextual complexity. This setup enables analysis of how increasing context affects model performance. We tested both models under four temperature settings (0.01, 0.5, 1.0, 2.0) and evaluated their ability to predict the next token across three reasoning levels. Zero-order reasoning involves tracking the state, either current (ground truth) or past (memory). First-order reasoning concerns understanding another's mental state (e.g., "Does Anne know the apple is salted?"). Second-order reasoning adds recursion (e.g., "Does Anne think that Charles knows the apple is salted?").
  Our results show that adding more infill sentences slightly reduces prediction accuracy, as added context increases complexity and ambiguity. Llama-2 consistently outperforms GPT-2 in prediction accuracy, especially at lower temperatures, demonstrating greater confidence in selecting the most probable token. As reasoning complexity rises, model responses diverge more. Notably, GPT-2 and Llama-2 display greater variability in predictions during first- and second-order reasoning tasks. These findings illustrate how model architecture, temperature, and contextual complexity influence next-token prediction, contributing to a better understanding of the strengths and limitations of current language models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement</title>
<link>https://arxiv.org/abs/2504.15630</link>
<guid>https://arxiv.org/abs/2504.15630</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Context-aware Layer Enhancement, contextual knowledge, Question-Answering tasks, V-usable information analysis

Summary: 
Context-aware Layer Enhancement (CaLE) is proposed as a method to enhance utilization of contextual knowledge within Large Language Models (LLMs). The approach strategically amplifies contextual information growth at an optimal layer, enriching final layer representations. By focusing on internal mechanisms of processing contextual information, CaLE improves context-faithful generation in Question-Answering tasks. This intervention method addresses limitations in LLMs' ability to leverage contextual knowledge, particularly in scenarios with unknown or conflicting information. Experimental results demonstrate the effectiveness of CaLE in enhancing the generation of contextually accurate responses. The approach emphasizes the importance of refining internal representations to improve overall performance in language understanding tasks.  <br /><br />Summary: <div>
arXiv:2504.15630v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, yet they often struggle with context-faithfulness generations that properly reflect contextual knowledge. While existing approaches focus on enhancing the decoding strategies, they ignore the fundamental mechanism of how contextual information is processed within LLMs' internal states. As a result, LLMs remain limited in their ability to fully leverage contextual knowledge. In this paper, we propose Context-aware Layer Enhancement (CaLE), a novel intervention method that enhances the utilization of contextual knowledge within LLMs' internal representations. By employing V-usable information analysis, CaLE strategically amplifies the growth of contextual information at an optimal layer, thereby enriching representations in the final layer. Our experiments demonstrate that CaLE effectively improves context-faithful generation in Question-Answering tasks, particularly in scenarios involving unknown or conflicting contextual knowledge.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Effective Text Clustering with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15640</link>
<guid>https://arxiv.org/abs/2504.15640</guid>
<content:encoded><![CDATA[
<div> Keywords: Text clustering, Large language models, Cost-effective framework, Constraint-based clustering, EdgeLLM

Summary: 
TECL introduces a cost-effective framework for accurate text clustering by leveraging feedback from large language models (LLMs) while minimizing the computational and financial burden. The framework utilizes EdgeLLM and TriangleLLM to create must-link/cannot-link constraints for text pairs/triplets and incorporates them into a weighted constrained clustering approach. EdgeLLM and TriangleLLM help identify informative text pairs/triplets for querying LLMs efficiently and extract accurate pairwise constraints through prompting techniques. Experimental results demonstrate the superiority of TECL over existing solutions in unsupervised text clustering under the same query cost for LLMs. <div>
arXiv:2504.15640v1 Announce Type: new 
Abstract: Text clustering aims to automatically partition a collection of text documents into distinct clusters based on linguistic features. In the literature, this task is usually framed as metric clustering based on text embeddings from pre-trained encoders or a graph clustering problem upon pairwise similarities from an oracle, e.g., a large ML model. Recently, large language models (LLMs) bring significant advancement in this field by offering contextualized text embeddings and highly accurate similarity scores, but meanwhile, present grand challenges to cope with substantial computational and/or financial overhead caused by numerous API-based queries or inference calls to the models.
  In response, this paper proposes TECL, a cost-effective framework that taps into the feedback from LLMs for accurate text clustering within a limited budget of queries to LLMs. Under the hood, TECL adopts our EdgeLLM or TriangleLLM to construct must-link/cannot-link constraints for text pairs, and further leverages such constraints as supervision signals input to our weighted constrained clustering approach to generate clusters. Particularly, EdgeLLM (resp. TriangleLLM) enables the identification of informative text pairs (resp. triplets) for querying LLMs via well-thought-out greedy algorithms and accurate extraction of pairwise constraints through carefully-crafted prompting techniques. Our experiments on multiple benchmark datasets exhibit that TECL consistently and considerably outperforms existing solutions in unsupervised text clustering under the same query cost for LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Typology</title>
<link>https://arxiv.org/abs/2504.15642</link>
<guid>https://arxiv.org/abs/2504.15642</guid>
<content:encoded><![CDATA[
<div> computational methods, typological research, linguistic data, language structure, statistical modeling <br />
Summary:<br />
Typology in linguistics classifies languages based on structural features rather than historical relationships. Computational methods are increasingly used for large-scale linguistic data analysis, enabling hypothesis testing about language structure and evolution. This article highlights the advantages of computational statistical modeling in typological research. <div>
arXiv:2504.15642v1 Announce Type: new 
Abstract: Typology is a subfield of linguistics that focuses on the study and classification of languages based on their structural features. Unlike genealogical classification, which examines the historical relationships between languages, typology seeks to understand the diversity of human languages by identifying common properties and patterns, known as universals. In recent years, computational methods have played an increasingly important role in typological research, enabling the analysis of large-scale linguistic data and the testing of hypotheses about language structure and evolution. This article provides an illustration of the benefits of computational statistical modeling in typology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinTextSim: Enhancing Financial Text Analysis with BERTopic</title>
<link>https://arxiv.org/abs/2504.15683</link>
<guid>https://arxiv.org/abs/2504.15683</guid>
<content:encoded><![CDATA[
<div> BERTopic, FinTextSim, financial text analysis, contextual embeddings, S&amp;P 500<br />
<br />
Summary:
Recent advancements in information availability and computational capabilities have revolutionized the analysis of annual reports, combining financial metrics with textual data. BERTopic, a cutting-edge topic model using contextual embeddings, was evaluated for analyzing financial reports from S&amp;P 500 companies. FinTextSim, a specialized sentence-transformer model for financial contexts, significantly improved topic clustering and semantic search compared to all-MiniLM-L6-v2. BERTopic's performance was found to be enhanced when paired with FinTextSim's embeddings, leading to clearer and distinct economic topic clusters. The use of FinTextSim is crucial for accurate financial text analysis, as it reduces misclassification and overlapping topics. The improved insights generated by these models have the potential to enhance decision-making processes, streamline resource allocation, and benefit business valuation and stock price prediction models. <div>
arXiv:2504.15683v1 Announce Type: new 
Abstract: Recent advancements in information availability and computational capabilities have transformed the analysis of annual reports, integrating traditional financial metrics with insights from textual data. To extract valuable insights from this wealth of textual data, automated review processes, such as topic modeling, are crucial. This study examines the effectiveness of BERTopic, a state-of-the-art topic model relying on contextual embeddings, for analyzing Item 7 and Item 7A of 10-K filings from S&amp;P 500 companies (2016-2022). Moreover, we introduce FinTextSim, a finetuned sentence-transformer model optimized for clustering and semantic search in financial contexts. Compared to all-MiniLM-L6-v2, the most widely used sentence-transformer, FinTextSim increases intratopic similarity by 81% and reduces intertopic similarity by 100%, significantly enhancing organizational clarity. We assess BERTopic's performance using embeddings from both FinTextSim and all-MiniLM-L6-v2. Our findings reveal that BERTopic only forms clear and distinct economic topic clusters when paired with FinTextSim's embeddings. Without FinTextSim, BERTopic struggles with misclassification and overlapping topics. Thus, FinTextSim is pivotal for advancing financial text analysis. FinTextSim's enhanced contextual embeddings, tailored for the financial domain, elevate the quality of future research and financial information. This improved quality of financial information will enable stakeholders to gain a competitive advantage, streamlining resource allocation and decision-making processes. Moreover, the improved insights have the potential to leverage business valuation and stock price prediction models.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject islands do not reduce to construction-specific discourse function</title>
<link>https://arxiv.org/abs/2504.15688</link>
<guid>https://arxiv.org/abs/2504.15688</guid>
<content:encoded><![CDATA[
<div> Keywords: islands, linguistics, syntax, information structure, acceptability

Summary:
- The term "islands" in linguistics refers to phrases from which extracting an element results in ungrammaticality.
- Generative tradition attributes ungrammaticality in islands to abstract movement dependency between elements.
- Research emphasizing language's communicative function suggests syntactic constraints like islands can be explained based on information packaging.
- Abeillé et al. (2020) propose that subject island effects are specific to information structure in wh-questions, not movement.
- Large-scale acceptability studies on wh-questions, relative clauses, and topicalization show evidence of a subject island effect, contrary to Abeillé et al.'s prediction. 

<br /><br />Summary: This article explores the concept of islands in linguistics, focusing on the ungrammatical extraction of elements from certain phrases. While the generative tradition attributes this to abstract movement dependencies, recent research suggests that syntactic constraints like islands may be linked to information packaging in language. Despite a proposal that subject island effects are specific to the information structure of wh-questions, empirical studies across different constructions show evidence of a subject island effect. These findings support the argument for abstract, syntactic representations playing a crucial role in island effects, independent of the communicative function associated with specific constructions. <div>
arXiv:2504.15688v1 Announce Type: new 
Abstract: The term islands in linguistics refers to phrases from which extracting an element results in ungrammaticality (Ross, 1967). Grammatical subjects are considered islands because extracting a sub-part of a subject results in an ill-formed sentence, despite having a clear intended meaning (e.g., "Which topic did the article about inspire you?"). The generative tradition, which views syntax as autonomous of meaning and function, attributes this ungrammaticality to the abstract movement dependency between the wh-phrase and the subject-internal position with which it is associated for interpretation. However, research on language that emphasizes its communicative function suggests instead that syntactic constraints, including islands, can be explained based on the way different constructions package information. Accordingly, Abeill\'e et al. (2020) suggest that the islandhood of subjects is specific to the information structure of wh-questions, and propose that subjects are not islands for movement, but for focusing, due to their discourse-backgroundedness. This predicts that other constructions that differ in their information structure from wh-questions, but still involve movement, should not create a subject island effect. We test this prediction in three large-scale acceptability studies, using a super-additive design that singles out subject island violations, in three different constructions: wh-questions, relative clauses, and topicalization. We report evidence for a subject island effect in each construction type, despite only wh-questions introducing what Abeill\'e et al. (2020) call "a clash in information structure." We argue that this motivates an account of islands in terms of abstract, syntactic representations, independent of the communicative function associated with the constructions.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tina: Tiny Reasoning Models via LoRA</title>
<link>https://arxiv.org/abs/2504.15777</link>
<guid>https://arxiv.org/abs/2504.15777</guid>
<content:encoded><![CDATA[
<div> cost-effective, language models, reasoning abilities, tiny models, reinforcement learning

Summary:<br />
The study introduces Tina, a family of tiny reasoning models that achieve strong reasoning performance with minimal resources. By applying parameter-efficient updates during reinforcement learning (RL) using low-rank adaptation (LoRA) to a 1.5B parameter base model, Tina demonstrates competitive reasoning performance and surpasses state-of-the-art RL reasoning models at a significantly reduced computational cost. The best Tina model shows a >20% reasoning performance increase and 43.33% Pass@1 accuracy on AIME24, with only a $9 USD post-training and evaluation cost, representing a 260x cost reduction. The study highlights the efficiency of efficient RL reasoning via LoRA, attributing the success to rapid adaptation to the structural format of reasoning rewarded by RL while preserving the base model's underlying knowledge. All code, training logs, and model weights & checkpoints are fully open-sourced for accessibility and further research. 

<br /><br />Summary: <div>
arXiv:2504.15777v1 Announce Type: new 
Abstract: How cost-effectively can strong reasoning abilities be achieved in language models? Driven by this fundamental question, we present Tina, a family of tiny reasoning models achieved with high cost-efficiency. Notably, Tina demonstrates that substantial reasoning performance can be developed using only minimal resources, by applying parameter-efficient updates during reinforcement learning (RL), using low-rank adaptation (LoRA), to an already tiny 1.5B parameter base model. This minimalist approach produces models that achieve reasoning performance which is competitive with, and sometimes surpasses, SOTA RL reasoning models built upon the same base model. Crucially, this is achieved at a tiny fraction of the computational post-training cost employed by existing SOTA models. In fact, the best Tina model achieves a >20\% reasoning performance increase and 43.33\% Pass@1 accuracy on AIME24, at only \$9 USD post-training and evaluation cost (i.e., an estimated 260x cost reduction). Our work reveals the surprising effectiveness of efficient RL reasoning via LoRA. We validate this across multiple open-source reasoning datasets and various ablation settings starting with a single, fixed set of hyperparameters. Furthermore, we hypothesize that this effectiveness and efficiency stem from LoRA rapidly adapting the model to the structural format of reasoning rewarded by RL, while largely preserving the base model's underlying knowledge. In service of accessibility and open research, we fully open-source all code, training logs, and model weights \& checkpoints.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach</title>
<link>https://arxiv.org/abs/2504.15784</link>
<guid>https://arxiv.org/abs/2504.15784</guid>
<content:encoded><![CDATA[
<div> Keywords: Creative writing, Large Language Models, Automated evaluation method, Torrance Test of Creative Writing, Alignment with human assessments

Summary:
The study explores the challenge of evaluating the creativity of machine-generated texts, particularly in the context of creative writing by Large Language Models (LLMs). Existing methods for evaluating the creativity of such texts are either manual and costly or lack alignment with human assessments. The paper introduces an automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which assesses creativity as a product. This approach utilizes a reference-based Likert-style method to score machine-generated creative texts against high-quality reference texts across various tests. Experimental results show that the proposed method significantly enhances the alignment between evaluations of LLM-generated texts and human assessments, achieving a pairwise accuracy of 0.75 (+15%). This method has the potential to improve the evaluation of creative writing by LLMs and facilitate their use in various creative domains. 

<br /><br />Summary: <div>
arXiv:2504.15784v1 Announce Type: new 
Abstract: Creative writing is a key capability of Large Language Models (LLMs), with potential applications in literature, storytelling, and various creative domains. However, evaluating the creativity of machine-generated texts remains a significant challenge, as existing methods either rely on costly manual annotations or fail to align closely with human assessments. In this paper, we propose an effective automated evaluation method based on the Torrance Test of Creative Writing (TTCW), which evaluates creativity as product. Our method employs a reference-based Likert-style approach, scoring generated creative texts relative to high-quality reference texts across various tests. Experimental results demonstrate that our method significantly improves the alignment between LLM evaluations and human assessments, achieving a pairwise accuracy of 0.75 (+15\%).
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A closer look at how large language models trust humans: patterns and biases</title>
<link>https://arxiv.org/abs/2504.15801</link>
<guid>https://arxiv.org/abs/2504.15801</guid>
<content:encoded><![CDATA[
<div> trust dynamics, large language models, trustworthiness, demographic variables, AI-to-human trust dynamics

Summary:<br />
- The study focuses on understanding trust dynamics between humans and AI agents, specifically looking at how large language models (LLMs) develop trust in humans in decision-making contexts.
- LLM-based agents likely rely on implicit effective trust in trust-related scenarios such as evaluating loan applications to assist decision-making.
- The research uses established behavioral theories to investigate whether LLM trust depends on human trustworthiness dimensions: competence, benevolence, and integrity.
- Demographic variables like age, religion, and gender can also affect LLM trust, especially in financial scenarios.
- Across 43,200 simulated experiments involving five popular language models and five scenarios, LLM trust development shows similarities to human trust development, with trust being strongly predicted by trustworthiness in most cases but also influenced by demographic factors. Understanding AI-to-human trust dynamics and monitoring biases is crucial to prevent unintended harmful outcomes in trust-sensitive AI applications. 

Summary: <div>
arXiv:2504.15801v1 Announce Type: new 
Abstract: As large language models (LLMs) and LLM-based agents increasingly interact with humans in decision-making contexts, understanding the trust dynamics between humans and AI agents becomes a central concern. While considerable literature studies how humans trust AI agents, it is much less understood how LLM-based agents develop effective trust in humans. LLM-based agents likely rely on some sort of implicit effective trust in trust-related contexts (e.g., evaluating individual loan applications) to assist and affect decision making. Using established behavioral theories, we develop an approach that studies whether LLMs trust depends on the three major trustworthiness dimensions: competence, benevolence and integrity of the human subject. We also study how demographic variables affect effective trust. Across 43,200 simulated experiments, for five popular language models, across five different scenarios we find that LLM trust development shows an overall similarity to human trust development. We find that in most, but not all cases, LLM trust is strongly predicted by trustworthiness, and in some cases also biased by age, religion and gender, especially in financial scenarios. This is particularly true for scenarios common in the literature and for newer models. While the overall patterns align with human-like mechanisms of effective trust formation, different models exhibit variation in how they estimate trust; in some cases, trustworthiness and demographic factors are weak predictors of effective trust. These findings call for a better understanding of AI-to-human trust dynamics and monitoring of biases and trust development patterns to prevent unintended and potentially harmful outcomes in trust-sensitive applications of AI.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns</title>
<link>https://arxiv.org/abs/2504.15815</link>
<guid>https://arxiv.org/abs/2504.15815</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt engineering, large language models, token patterns, data mining, systematic differences

Summary: 
Spotlight introduces a novel approach to address the challenges in prompt engineering for large language models. By combining automation and human analysis, Spotlight uses data mining techniques to differentiate between random variations and systematic differences in model outputs, providing token patterns that guide users in analyzing the effects of prompt and model changes efficiently. Benchmarks are created to test the reliability of token pattern extraction methods, demonstrating new insights into established prompt data. Through demonstration studies and a user study, the approach enables users to understand systematic differences in language model outputs, identifying relevant differences caused by prompt and model changes related to factors such as gender or culture. This supports the prompt engineering process and facilitates human-centric model behavior research.<br /><br />Summary: <div>
arXiv:2504.15815v1 Announce Type: new 
Abstract: Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated output texts. Existing evaluation methods, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on data mining techniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompt and model changes efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs, and we are able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model</title>
<link>https://arxiv.org/abs/2504.15843</link>
<guid>https://arxiv.org/abs/2504.15843</guid>
<content:encoded><![CDATA[
<div> reference model, reinforcement learning, human feedback, language models, preference optimization

Summary:
Pre-Direct Preference Optimization (Pre-DPO) improves reinforcement learning from human feedback for large language models by utilizing a guiding reference model to enhance preference optimization performance. Direct Preference Optimization (DPO) simplifies the process by directly optimizing human preferences without an explicit reward model. However, initializing the policy and reference models identically can lead to inefficient data utilization. Simple Preference Optimization (SimPO) lacks a reference model, reducing training robustness. Pre-DPO leverages a guiding reference model to assign weights adaptively, improving the performance of both DPO and SimPO. Extensive experiments on benchmarks demonstrate the effectiveness of Pre-DPO in enhancing training performance without the need for external models or additional data.<br /><br />Summary: <div>
arXiv:2504.15843v1 Announce Type: new 
Abstract: Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2504.15848</link>
<guid>https://arxiv.org/abs/2504.15848</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal aspect-based sentiment classification, sentiment polarity prediction, cognitive and aesthetic sentiment causality understanding framework, visual features, large language model<br />
Summary: 
Chimera is a framework for multimodal aspect-based sentiment classification that focuses on understanding fine-grained visual content and cognitive rationales for sentiment expression. It aligns visual patch features with textual descriptions and incorporates coarse and fine-grained visual features to enhance sentiment analysis. The framework utilizes a large language model to improve awareness of sentimental cues and affective-cognitive resonance. Experimental results show the effectiveness of Chimera in MASC tasks and highlight its flexibility compared to existing models. The complete implementation and dataset are publicly available on GitHub.<br /> 
Summary: <div>
arXiv:2504.15848v1 Announce Type: new 
Abstract: Multimodal aspect-based sentiment classification (MASC) is an emerging task due to an increase in user-generated multimodal content on social platforms, aimed at predicting sentiment polarity toward specific aspect targets (i.e., entities or attributes explicitly mentioned in text-image pairs). Despite extensive efforts and significant achievements in existing MASC, substantial gaps remain in understanding fine-grained visual content and the cognitive rationales derived from semantic content and impressions (cognitive interpretations of emotions evoked by image content). In this study, we present Chimera: a cognitive and aesthetic sentiment causality understanding framework to derive fine-grained holistic features of aspects and infer the fundamental drivers of sentiment expression from both semantic perspectives and affective-cognitive resonance (the synergistic effect between emotional responses and cognitive interpretations). Specifically, this framework first incorporates visual patch features for patch-word alignment. Meanwhile, it extracts coarse-grained visual features (e.g., overall image representation) and fine-grained visual regions (e.g., aspect-related regions) and translates them into corresponding textual descriptions (e.g., facial, aesthetic). Finally, we leverage the sentimental causes and impressions generated by a large language model (LLM) to enhance the model's awareness of sentimental cues evoked by semantic content and affective-cognitive resonance. Experimental results on standard MASC datasets demonstrate the effectiveness of the proposed model, which also exhibits greater flexibility to MASC compared to LLMs such as GPT-4o. We have publicly released the complete implementation and dataset at https://github.com/Xillv/Chimera
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Early Exit in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.15895</link>
<guid>https://arxiv.org/abs/2504.15895</guid>
<content:encoded><![CDATA[
<div> scaling, chain-of-thought generation, reasoning language models, early exit, model behavior monitoring
Summary:
The article introduces a method for self-truncating chain-of-thought sequences generated by large reasoning language models. This method aims to prevent overthinking and improve efficiency by dynamically terminating reasoning chains when the model shows high confidence in an answer. The approach does not require additional training and can be seamlessly integrated into existing reasoning LLMs. Experimental results on various benchmarks demonstrate the effectiveness of the proposed method, reducing CoT sequence length by 31% to 43% and improving accuracy by 1.7% to 5.7% for deepseek-series reasoning LLMs. <div>
arXiv:2504.15895v1 Announce Type: new 
Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on multiple reasoning benchmarks MATH-500, AMC 2023, GPQA Diamond and AIME 2024 show that the proposed method is consistently effective on deepseek-series reasoning LLMs, reducing the length of CoT sequences by an average of 31% to 43% while improving accuracy by 1.7% to 5.7%.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.15900</link>
<guid>https://arxiv.org/abs/2504.15900</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, large language models, audio-language reasoning, structured reasoning, curriculum learning 

Summary:<br /><br />
The study explores the impact of reinforcement learning on large audio-language models, finding that it can significantly enhance reasoning abilities. Using the GRPO framework, a structured audio reasoning model named SARI is developed, achieving a notable accuracy improvement over the base model. Through supervised fine-tuning on structured and unstructured chains of thought, followed by curriculum-guided reinforcement learning, the model demonstrates the effectiveness of explicit, structured reasoning and curriculum learning in enhancing audio-language understanding. The findings highlight the importance of SFT warm-up for stable RL training, the advantage of structured chains over unstructured ones for generalization, and the benefits of easy-to-hard curricula in accelerating convergence and improving performance. Overall, the study underscores the potential of explicit, structured reasoning and curriculum learning in advancing audio-language understanding. 

<br /><br />Summary: <div>
arXiv:2504.15900v1 Announce Type: new 
Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity</title>
<link>https://arxiv.org/abs/2504.15941</link>
<guid>https://arxiv.org/abs/2504.15941</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, inclusive language translation, FairTranslate dataset, gender biases, machine translation

Summary: 
The paper introduces FairTranslate, a human-annotated dataset focusing on evaluating gender biases in machine translation systems when translating English to French text related to occupations. The dataset includes metadata such as stereotypical alignment of occupations, grammatical gender ambiguity, and ground-truth gender labels. Four prominent Large Language Models were tested on this dataset, revealing significant biases in gender representation. The results emphasize the ongoing challenges in achieving equitable translation outcomes and underscore the importance of strategies to ensure fair and inclusive language usage in machine translation systems. The FairTranslate dataset is publicly available on Hugging Face, and the experiment code is shared on GitHub. <div>
arXiv:2504.15941v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset designed to evaluate non-binary gender biases in machine translation systems from English to French. FairTranslate includes 2418 English-French sentence pairs related to occupations, annotated with rich metadata such as the stereotypical alignment of the occupation, grammatical gender indicator ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B, Llama3.3-70B) on this dataset under different prompting procedures. Our results reveal substantial biases in gender representation across LLMs, highlighting persistent challenges in achieving equitable outcomes in machine translation. These findings underscore the need for focused strategies and interventions aimed at ensuring fair and inclusive language usage in LLM-based translation systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and disclose the code for all experiments on GitHub.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15983</link>
<guid>https://arxiv.org/abs/2504.15983</guid>
<content:encoded><![CDATA[
<div> Keywords: lightweight language models, zero-shot NAS, W-PCA, efficiency, evaluation metrics

Summary:
Weight-weighted PCA (W-PCA) is introduced as a novel zero-shot NAS method for lightweight language models. It optimizes evaluation time by utilizing proxies such as parameter count and principal components in the FFN layer. The method eliminates the need for gradient computations, improving efficiency in designing and evaluating language models. Comparative analysis on GLUE and SQuAD datasets shows reduced training time and higher scores in testing compared to previous methods. Ranking evaluations on a dataset from FlexiBERT search space demonstrate superior ranking correlation and reduced solving time. Overall, W-PCA offers a more efficient approach for designing and evaluating lightweight language models, outperforming existing methods in terms of training time and performance. 

<br /><br />Summary: <div>
arXiv:2504.15983v1 Announce Type: new 
Abstract: The demand for efficient natural language processing (NLP) systems has led to the development of lightweight language models. Previous work in this area has primarily focused on manual design or training-based neural architecture search (NAS) methods. Recently, zero-shot NAS methods have been proposed for evaluating language models without the need for training. However, prevailing approaches to zero-shot NAS often face challenges such as biased evaluation metrics and computational inefficiencies. In this paper, we introduce weight-weighted PCA (W-PCA), a novel zero-shot NAS method specifically tailored for lightweight language models. Our approach utilizes two evaluation proxies: the parameter count and the number of principal components with cumulative contribution exceeding $\eta$ in the feed-forward neural (FFN) layer. Additionally, by eliminating the need for gradient computations, we optimize the evaluation time, thus enhancing the efficiency of designing and evaluating lightweight language models. We conduct a comparative analysis on the GLUE and SQuAD datasets to evaluate our approach. The results demonstrate that our method significantly reduces training time compared to one-shot NAS methods and achieves higher scores in the testing phase compared to previous state-of-the-art training-based methods. Furthermore, we perform ranking evaluations on a dataset sampled from the FlexiBERT search space. Our approach exhibits superior ranking correlation and further reduces solving time compared to other zero-shot NAS methods that require gradient computation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Few-shot Hate Speech Detection Based on the MindSpore Framework</title>
<link>https://arxiv.org/abs/2504.15987</link>
<guid>https://arxiv.org/abs/2504.15987</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech detection, few-shot learning, deep learning, prompt-based learning, adversarial augmentation

Summary: 
MS-FSLHate is a neural framework designed to detect hate speech in few-shot scenarios using prompt-enhanced deep learning models. The framework utilizes learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to enhance generalization. Experimental results on two benchmark datasets show that MS-FSLHate outperforms competitive baselines in terms of precision, recall, and F1-score. The model also demonstrates high efficiency and scalability, making it suitable for deployment in resource-constrained environments. The combination of prompt-based learning and adversarial augmentation proves to be effective in improving hate speech detection in scenarios with limited labeled data. Overall, MS-FSLHate presents a robust and adaptable solution for detecting hate speech on social media platforms. 

<br /><br />Summary: <div>
arXiv:2504.15987v1 Announce Type: new 
Abstract: The proliferation of hate speech on social media poses a significant threat to online communities, requiring effective detection systems. While deep learning models have shown promise, their performance often deteriorates in few-shot or low-resource settings due to reliance on large annotated corpora. To address this, we propose MS-FSLHate, a prompt-enhanced neural framework for few-shot hate speech detection implemented on the MindSpore deep learning platform. The model integrates learnable prompt embeddings, a CNN-BiLSTM backbone with attention pooling, and synonym-based adversarial data augmentation to improve generalization. Experimental results on two benchmark datasets-HateXplain and HSOL-demonstrate that our approach outperforms competitive baselines in precision, recall, and F1-score. Additionally, the framework shows high efficiency and scalability, suggesting its suitability for deployment in resource-constrained environments. These findings highlight the potential of combining prompt-based learning with adversarial augmentation for robust and adaptable hate speech detection in few-shot scenarios.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Cost-Aware Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.16005</link>
<guid>https://arxiv.org/abs/2504.16005</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, prompt optimization, AutoML techniques, multi-objective optimization, cost-efficiency

Summary:
Large language models (LLMs) have transformed natural language processing but are highly dependent on prompt formulation. Prompt optimization is crucial but can be expensive. CAPO (Cost-Aware Prompt Optimization) is introduced as an efficient algorithm that leverages AutoML techniques to enhance prompt optimization. It integrates LLMs as operators, utilizing racing to save evaluations and multi-objective optimization to balance performance and prompt length. CAPO optimizes instructions and few-shot examples while considering task descriptions for improved robustness. Through extensive experiments, CAPO outperforms existing discrete prompt optimization methods in multiple cases with up to 21% improvement. It achieves better performance with smaller budgets, saves evaluations, and reduces average prompt length, making it cost-efficient and aware. Even without few-shot examples, CAPO remains robust to initial prompts, showcasing its power and accessibility in prompt optimization. 

<br /><br />Summary: <div>
arXiv:2504.16005v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Methods for Recognizing Nested Terms</title>
<link>https://arxiv.org/abs/2504.16007</link>
<guid>https://arxiv.org/abs/2504.16007</guid>
<content:encoded><![CDATA[
<div> nested terms, RuTermEval competition, Binder model, term recognition, nestedness

Summary:
- The paper discusses the participation in the RuTermEval competition focusing on extracting nested terms.
- Utilizing the Binder model, originally successful in recognizing nested named entities, proved effective in extracting nested terms.
- Achieving top results in all tracks of the competition highlights the success of the approach.
- The exploration of recognizing nested terms from flat training data annotated without nestedness presents a novel task.
- The study demonstrates that the proposed approaches effectively retrieve nested terms, even without explicit nested labeling. 

<br /><br />Summary: <div>
arXiv:2504.16007v1 Announce Type: new 
Abstract: In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certified Mitigation of Worst-Case LLM Copyright Infringement</title>
<link>https://arxiv.org/abs/2504.16046</link>
<guid>https://arxiv.org/abs/2504.16046</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, copyright infringement, BloomScrub, quote detection, risk reduction

Summary:
BloomScrub introduces a simple yet effective method to prevent copyright infringement by large language models during inference time. The approach combines quote detection and rewriting techniques using efficient data sketches like Bloom filters. This allows for scalable copyright screening even in real-world corpora. When faced with long verbatim quotes from copyrighted sources, the system can abstain from responding, providing certified risk reduction. Experimental results show that BloomScrub successfully reduces infringement risk while preserving utility and adapting to different levels of enforcement stringency. The research highlights the importance of lightweight inference-time methods for copyright prevention, offering a promising solution to address worst-case copyright risks in large language models.<br /><br />Summary: BloomScrub is a highly effective inference-time approach that prevents copyright infringement by large language models through quote detection and rewriting techniques. It uses efficient data sketches like Bloom filters to enable scalable copyright screening, reducing risk while preserving utility and adapting to varying enforcement levels. The system can abstain from responding when faced with long verbatim quotes, providing certified risk reduction and demonstrating the efficacy of lightweight methods in copyright prevention. <div>
arXiv:2504.16046v1 Announce Type: new 
Abstract: The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement</title>
<link>https://arxiv.org/abs/2504.16053</link>
<guid>https://arxiv.org/abs/2504.16053</guid>
<content:encoded><![CDATA[
<div> Keywords: State space models, LongMamba, Mamba models, long-context understanding, token filtering

Summary: 
LongMamba is a new technique designed to enhance the long-context capabilities of State space models (SSMs), specifically Mamba models. These models have been shown to underperform compared to Transformers in tasks requiring understanding of long contexts. LongMamba categorizes hidden channels in Mamba models into local and global channels based on their receptive field lengths, with global channels being crucial for long-context understanding. By identifying critical tokens in global channels and applying token filtering to exclude irrelevant tokens, LongMamba prevents the accumulation of unimportant information in the memory of global channels. This prevents memory decay and significantly enhances Mamba's performance in long-context scenarios. LongMamba achieves this without the need for additional training, setting a new standard for SSMs in handling long contexts. The code for LongMamba is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2504.16053v1 Announce Type: new 
Abstract: State space models (SSMs) have emerged as an efficient alternative to Transformer models for language modeling, offering linear computational complexity and constant memory usage as context length increases. However, despite their efficiency in handling long contexts, recent studies have shown that SSMs, such as Mamba models, generally underperform compared to Transformers in long-context understanding tasks. To address this significant shortfall and achieve both efficient and accurate long-context understanding, we propose LongMamba, a training-free technique that significantly enhances the long-context capabilities of Mamba models. LongMamba builds on our discovery that the hidden channels in Mamba can be categorized into local and global channels based on their receptive field lengths, with global channels primarily responsible for long-context capability. These global channels can become the key bottleneck as the input context lengthens. Specifically, when input lengths largely exceed the training sequence length, global channels exhibit limitations in adaptively extend their receptive fields, leading to Mamba's poor long-context performance. The key idea of LongMamba is to mitigate the hidden state memory decay in these global channels by preventing the accumulation of unimportant tokens in their memory. This is achieved by first identifying critical tokens in the global channels and then applying token filtering to accumulate only those critical tokens. Through extensive benchmarking across synthetic and real-world long-context scenarios, LongMamba sets a new standard for Mamba's long-context performance, significantly extending its operational range without requiring additional training. Our code is available at https://github.com/GATECH-EIC/LongMamba.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability</title>
<link>https://arxiv.org/abs/2504.16056</link>
<guid>https://arxiv.org/abs/2504.16056</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, Knowledge Distillation, Model Performance, Explainability

Summary:
In the study, the authors explore the impact of state-of-the-art distillation methods on model performance and explainability in the context of Large Language Models (LLMs). They introduce new methods, including critique-revision prompting for data generation and a synthesis of existing training methods. The comparison is conducted using the Commonsense Question-Answering (CQA) dataset, evaluating both student model accuracy and explainability through a human-grounded study. The results aim to advance the distillation of small language models and enhance the broader applicability and faster diffusion of LLM technology.<br /><br />Summary: <div>
arXiv:2504.16056v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) has increasingly influenced modern society, recently in particular through significant advancements in Large Language Models (LLMs). However, high computational and storage demands of LLMs still limit their deployment in resource-constrained environments. Knowledge distillation addresses this challenge by training a small student model from a larger teacher model. Previous research has introduced several distillation methods for both generating training data and for training the student model. Despite their relevance, the effects of state-of-the-art distillation methods on model performance and explainability have not been thoroughly investigated and compared. In this work, we enlarge the set of available methods by applying critique-revision prompting to distillation for data generation and by synthesizing existing methods for training. For these methods, we provide a systematic comparison based on the widely used Commonsense Question-Answering (CQA) dataset. While we measure performance via student model accuracy, we employ a human-grounded study to evaluate explainability. We contribute new distillation methods and their comparison in terms of both performance and explainability. This should further advance the distillation of small language models and, thus, contribute to broader applicability and faster diffusion of LLM technology.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</title>
<link>https://arxiv.org/abs/2504.16060</link>
<guid>https://arxiv.org/abs/2504.16060</guid>
<content:encoded><![CDATA[
<div> evaluation, vision-language systems, pragmatic competence, RefOI dataset, Gricean maxims<br />
<br />
Summary: 
This paper discusses the importance of evaluating vision-language systems (VLMs) based on pragmatic competence, particularly in the task of Referring Expression Generation (REG). The authors introduce a new dataset called RefOI, consisting of annotated images with both written and spoken referring expressions. Through evaluating state-of-the-art VLMs, they identify three key failures in pragmatic competence: failure to uniquely identify the referent, inclusion of excessive or irrelevant information, and misalignment with human pragmatic preference. The study highlights the limitations of current automatic evaluations in capturing these pragmatic violations and emphasizes the need for models and evaluation frameworks that align with real human communication. The findings underscore the significance of integrating pragmatic principles into VLM development and evaluation processes.  <br /> <br />Summary: <div>
arXiv:2504.16060v1 Announce Type: new 
Abstract: Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Python Tool for Reconstructing Full News Text from GDELT</title>
<link>https://arxiv.org/abs/2504.16063</link>
<guid>https://arxiv.org/abs/2504.16063</guid>
<content:encoded><![CDATA[
<div> Keywords: news data, full-text newspaper articles, Global Database of Events, Language, and Tone, Python code, text analysis

Summary:
The paper discusses the significance of news data in various disciplines and the challenges researchers face in accessing comprehensive news corpora. It introduces a novel approach to obtaining full-text newspaper articles at minimal cost by utilizing the GDELT Web News NGrams 3.0 dataset. The method involves reconstructing articles from n-grams through intelligent merging of textual fragments. The provided Python code enables researchers to access structured, large-scale newspaper data for text analysis, overcoming limitations of existing proprietary datasets. This approach enhances the accessibility of news data for empirical research, facilitating applications in economic forecasting, computational social science, and natural language processing.

<br /><br />Summary: <div>
arXiv:2504.16063v1 Announce Type: new 
Abstract: News data have become an essential resource across various disciplines, including economics, finance, management, social sciences, and computer science. Researchers leverage newspaper articles to study economic trends, market dynamics, corporate strategies, public perception, political discourse, and the evolution of public opinion. Additionally, news datasets have been instrumental in training large-scale language models, with applications in sentiment analysis, fake news detection, and automated news summarization. Despite their significance, access to comprehensive news corpora remains a key challenge. Many full-text news providers, such as Factiva and LexisNexis, require costly subscriptions, while free alternatives often suffer from incomplete data and transparency issues. This paper presents a novel approach to obtaining full-text newspaper articles at near-zero cost by leveraging data from the Global Database of Events, Language, and Tone (GDELT). Specifically, we focus on the GDELT Web News NGrams 3.0 dataset, which provides high-frequency updates of n-grams extracted from global online news sources. We provide Python code to reconstruct full-text articles from these n-grams by identifying overlapping textual fragments and intelligently merging them. Our method enables researchers to access structured, large-scale newspaper data for text analysis while overcoming the limitations of existing proprietary datasets. The proposed approach enhances the accessibility of news data for empirical research, facilitating applications in economic forecasting, computational social science, and natural language processing.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation</title>
<link>https://arxiv.org/abs/2504.16073</link>
<guid>https://arxiv.org/abs/2504.16073</guid>
<content:encoded><![CDATA[
<div> navigation, visual language models, GUI, process supervision, task success rate

Summary:
- Recent advancements in visual language models (VLMs) have improved their capabilities in handling complex GUI tasks.
- Black-box commercial VLMs and resource-intensive fine-tuning of open-source VLMs hinder progress in GUI tasks.
- The proposed approach guides VLM agents with process supervision by a reward model during GUI navigation, optimizing actions at each step.
- Significant performance gains were observed in static and dynamic GUI environments, with a 3.4% improvement in single-step action accuracy in static environments and around a 33% increase in task success rate in a dynamic environment.
- Integration of trajectory reflection and retry mechanisms further enhances task success rates. 

<br /><br />Summary: <div>
arXiv:2504.16073v1 Announce Type: new 
Abstract: Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
<div> physics, benchmark, language models, reasoning, evaluation

Summary:
The article introduces PHYBench, a benchmark designed to assess the reasoning capabilities of large language models (LLMs) in physical contexts. Consisting of 500 physics problems covering various topics, PHYBench aims to evaluate models' understanding of realistic physical processes. It includes a novel evaluation metric, the Expression Edit Distance (EED) Score, based on mathematical expression edit distance. Results from testing LLMs on PHYBench show that current models still fall behind human experts in complex physical reasoning scenarios. The benchmark dataset and results are publicly available for further research and improvement. <div>
arXiv:2504.16074v1 Announce Type: new 
Abstract: We introduce PHYBench, a novel, high-quality benchmark designed for evaluating reasoning capabilities of large language models (LLMs) in physical contexts. PHYBench consists of 500 meticulously curated physics problems based on real-world physical scenarios, designed to assess the ability of models to understand and reason about realistic physical processes. Covering mechanics, electromagnetism, thermodynamics, optics, modern physics, and advanced physics, the benchmark spans difficulty levels from high school exercises to undergraduate problems and Physics Olympiad challenges. Additionally, we propose the Expression Edit Distance (EED) Score, a novel evaluation metric based on the edit distance between mathematical expressions, which effectively captures differences in model reasoning processes and results beyond traditional binary scoring methods. We evaluate various LLMs on PHYBench and compare their performance with human experts. Our results reveal that even state-of-the-art reasoning models significantly lag behind human experts, highlighting their limitations and the need for improvement in complex physical reasoning scenarios. Our benchmark results and dataset are publicly available at https://phybench-official.github.io/phybench-demo/.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTRL: Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.16084</link>
<guid>https://arxiv.org/abs/2504.16084</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Test-Time Scaling, Test-Time Reinforcement Learning, Unlabeled Data <br />
<br />
Summary: This paper explores Reinforcement Learning (RL) in Large Language Models (LLMs) on data lacking explicit labels. The challenge lies in estimating rewards during inference without access to ground-truth information. The authors introduce Test-Time Reinforcement Learning (TTRL) as a method for training LLMs using RL on unlabeled data, leveraging pre-trained model priors for self-evolution. TTRL significantly improves performance across diverse tasks and models, boosting pass@1 performance on the AIME 2024 by approximately 159% with only unlabeled test data. Despite being supervised by the Maj@N metric alone, TTRL consistently surpasses the initial model's upper limits and nears the performance of models trained directly on labeled test data. Experimental results validate TTRL's effectiveness across various tasks, indicating its potential for broader applications and domains. <br /><br /> <div>
arXiv:2504.16084v1 Announce Type: new 
Abstract: This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-CoDE: Medical Critique based Disagreement Evaluation Framework</title>
<link>https://arxiv.org/abs/2504.15330</link>
<guid>https://arxiv.org/abs/2504.15330</guid>
<content:encoded><![CDATA[
<div> evaluation framework, medical language models, reliability, accuracy, Med-CoDE <br />
<br />
Summary: 
The article introduces Med-CoDE, an evaluation framework designed for assessing the performance of large language models (LLMs) in medical contexts. It addresses concerns regarding reliability and accuracy by utilizing a critique-based approach to measure the agreement between model-generated responses and established medical truths. Med-CoDE aims to provide a comprehensive evaluation of medical LLMs by capturing both accuracy and reliability. Through experimental demonstrations, the framework showcases its practicality in evaluating the quality and trustworthiness of medical language models. <div>
arXiv:2504.15330v1 Announce Type: cross 
Abstract: The emergence of large language models (LLMs) has significantly influenced numerous fields, including healthcare, by enhancing the capabilities of automated systems to process and generate human-like text. However, despite their advancements, the reliability and accuracy of LLMs in medical contexts remain critical concerns. Current evaluation methods often lack robustness and fail to provide a comprehensive assessment of LLM performance, leading to potential risks in clinical settings. In this work, we propose Med-CoDE, a specifically designed evaluation framework for medical LLMs to address these challenges. The framework leverages a critique-based approach to quantitatively measure the degree of disagreement between model-generated responses and established medical ground truths. This framework captures both accuracy and reliability in medical settings. The proposed evaluation framework aims to fill the existing gap in LLM assessment by offering a systematic method to evaluate the quality and trustworthiness of medical LLMs. Through extensive experiments and case studies, we illustrate the practicality of our framework in providing a comprehensive and reliable evaluation of medical LLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception</title>
<link>https://arxiv.org/abs/2504.15362</link>
<guid>https://arxiv.org/abs/2504.15362</guid>
<content:encoded><![CDATA[
<div> dataset, reasoning models, perceptual tasks, synthetic, visual reasoning

Summary:
The paper introduces the LongPerceptualThoughts dataset, containing 30K long-thought traces for perceptual tasks. A new data synthesis framework is proposed to generate elaborate reasoning thoughts for perceptual tasks, involving multiple-choice question synthesis, CoT extraction, and frontier reasoning model usage. Controlled experiments with a 7B model show significant improvements over existing visual reasoning data-generation methods, with an average +3.4 point improvement on vision-centric benchmarks and a +2 point improvement on text reasoning benchmarks. Notably, the model trained on the dataset also enhances performance on the MMLU-Pro benchmark. <div>
arXiv:2504.15362v1 Announce Type: cross 
Abstract: Recent reasoning models through test-time scaling have demonstrated that long chain-of-thoughts can unlock substantial performance boosts in hard reasoning tasks such as math and code. However, the benefit of such long thoughts for system-2 reasoning is relatively less explored in other domains such as perceptual tasks where shallower, system-1 reasoning seems sufficient. In this paper, we introduce LongPerceptualThoughts, a new synthetic dataset with 30K long-thought traces for perceptual tasks. The key challenges in synthesizing elaborate reasoning thoughts for perceptual tasks are that off-the-shelf models are not yet equipped with such thinking behavior and that it is not straightforward to build a reliable process verifier for perceptual tasks. Thus, we propose a novel three-stage data synthesis framework that first synthesizes verifiable multiple-choice questions from dense image descriptions, then extracts simple CoTs from VLMs for those verifiable problems, and finally expands those simple thoughts to elaborate long thoughts via frontier reasoning models. In controlled experiments with a strong instruction-tuned 7B model, we demonstrate notable improvements over existing visual reasoning data-generation methods. Our model, trained on the generated dataset, achieves an average +3.4 points improvement over 5 vision-centric benchmarks, including +11.8 points on V$^*$ Bench. Notably, despite being tuned for vision tasks, it also improves performance on the text reasoning benchmark, MMLU-Pro, by +2 points.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding Camera Motions in Any Video</title>
<link>https://arxiv.org/abs/2504.15376</link>
<guid>https://arxiv.org/abs/2504.15376</guid>
<content:encoded><![CDATA[
<div> camera motion, CameraBench dataset, taxonomy, Structure-from-Motion, Video-Language Models <br />
Summary: <br />
The article introduces the CameraBench dataset, a large-scale dataset for assessing camera motion understanding. It includes ~3,000 internet videos annotated by experts with a taxonomy of camera motion primitives. A human study shows that training improves annotation performance, distinguishing between intrinsic and extrinsic motions. Evaluations on Structure-from-Motion (SfM) and Video-Language Models (VLMs) reveal strengths and weaknesses in capturing semantic and geometric primitives. Fine-tuning a VLM on CameraBench enhances performance and enables applications in motion-augmented captioning, video question answering, and video-text retrieval. The goal is to advance research in understanding camera motions in videos. <br /> <div>
arXiv:2504.15376v1 Announce Type: cross 
Abstract: We introduce CameraBench, a large-scale dataset and benchmark designed to assess and improve camera motion understanding. CameraBench consists of ~3,000 diverse internet videos, annotated by experts through a rigorous multi-stage quality control process. One of our contributions is a taxonomy of camera motion primitives, designed in collaboration with cinematographers. We find, for example, that some motions like "follow" (or tracking) require understanding scene content like moving subjects. We conduct a large-scale human study to quantify human annotation performance, revealing that domain expertise and tutorial-based training can significantly enhance accuracy. For example, a novice may confuse zoom-in (a change of intrinsics) with translating forward (a change of extrinsics), but can be trained to differentiate the two. Using CameraBench, we evaluate Structure-from-Motion (SfM) and Video-Language Models (VLMs), finding that SfM models struggle to capture semantic primitives that depend on scene content, while VLMs struggle to capture geometric primitives that require precise estimation of trajectories. We then fine-tune a generative VLM on CameraBench to achieve the best of both worlds and showcase its applications, including motion-augmented captioning, video question answering, and video-text retrieval. We hope our taxonomy, benchmark, and tutorials will drive future efforts towards the ultimate goal of understanding camera motions in any video.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2504.15415</link>
<guid>https://arxiv.org/abs/2504.15415</guid>
<content:encoded><![CDATA[
<div> image-grounded video perception reasoning benchmark evaluation model performance data analysis Keywords: MLLMs, IV-Bench, video perception, reasoning tasks, image context

Summary:
The article introduces IV-Bench, a benchmark for evaluating Image-Grounded Video Perception and Reasoning. The benchmark includes 967 videos with annotated image-text queries for 13 tasks across 5 categories. State-of-the-art MLLMs exhibit low performance on the benchmark, indicating the need for improvement in image-grounded video comprehension. Factors such as inference patterns, frame number, and resolution impact model performance. Challenges in IV-Bench extend beyond data alignment during training. The study provides valuable insights for future research in this area. The codes and data are available on GitHub for reference. <div>
arXiv:2504.15415v1 Announce Type: cross 
Abstract: Existing evaluation frameworks for Multimodal Large Language Models (MLLMs) primarily focus on image reasoning or general video understanding tasks, largely overlooking the significant role of image context in video comprehension. To bridge this gap, we propose IV-Bench, the first comprehensive benchmark for evaluating Image-Grounded Video Perception and Reasoning. IV-Bench consists of 967 videos paired with 2,585 meticulously annotated image-text queries across 13 tasks (7 perception and 6 reasoning tasks) and 5 representative categories. Extensive evaluations of state-of-the-art open-source (e.g., InternVL2.5, Qwen2.5-VL) and closed-source (e.g., GPT-4o, Gemini2-Flash and Gemini2-Pro) MLLMs demonstrate that current models substantially underperform in image-grounded video Perception and Reasoning, merely achieving at most 28.9% accuracy. Further analysis reveals key factors influencing model performance on IV-Bench, including inference pattern, frame number, and resolution. Additionally, through a simple data synthesis approach, we demonstratethe challenges of IV- Bench extend beyond merely aligning the data format in the training proecss. These findings collectively provide valuable insights for future research. Our codes and data are released in https://github.com/multimodal-art-projection/IV-Bench.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data</title>
<link>https://arxiv.org/abs/2504.15448</link>
<guid>https://arxiv.org/abs/2504.15448</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, corporate reputation, social media, NLP, machine learning

Summary: 
The paper presents a sentiment analysis system for monitoring corporate reputation using NLP and machine learning. It combines rule-based models and deep learning techniques for interpreting public opinion in real time. The system preprocesses social media data, classifies sentiment with an ensemble approach, and visualizes results for better interpretability. Significant disparities in public sentiment across major corporations are revealed, with companies like Amazon and Samsung receiving high scores, while Microsoft and Walmart show poor sentiment profiles. The analysis demonstrates the system's utility in providing actionable insights for stakeholders to make informed strategic decisions based on comprehensive sentiment analysis. <br /><br />Summary: <div>
arXiv:2504.15448v1 Announce Type: cross 
Abstract: In the age of social media, understanding public sentiment toward major corporations is crucial for investors, policymakers, and researchers. This paper presents a comprehensive sentiment analysis system tailored for corporate reputation monitoring, combining Natural Language Processing (NLP) and machine learning techniques to accurately interpret public opinion in real time. The methodology integrates a hybrid sentiment detection framework leveraging both rule-based models (VADER) and transformer-based deep learning models (DistilBERT), applied to social media data from multiple platforms. The system begins with robust preprocessing involving noise removal and text normalization, followed by sentiment classification using an ensemble approach to ensure both interpretability and contextual accuracy. Results are visualized through sentiment distribution plots, comparative analyses, and temporal sentiment trends for enhanced interpretability. Our analysis reveals significant disparities in public sentiment across major corporations, with companies like Amazon (81.2) and Samsung (45.8) receiving excellent sentiment scores, while Microsoft (21.7) and Walmart (21.9) exhibit poor sentiment profiles. These findings demonstrate the utility of our multi-source sentiment framework in providing actionable insights regarding corporate public perception, enabling stakeholders to make informed strategic decisions based on comprehensive sentiment analysis.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Parallel Reasoning with Language Models</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
<div> Keywords: scaling, language models, inference-time computation, reasoning framework, reinforcement learning <br />
<br />
Summary: 
The proposed Adaptive Parallel Reasoning (APR) framework aims to improve the reasoning capabilities of language models by combining serialized and parallel computations. By using adaptive multi-threaded inference with spawn() and join() operations, APR optimizes task success rates without predefined reasoning structures. In experiments on the Countdown reasoning task, APR showed higher performance within the same context window, superior scalability with increased computation, and improved accuracy at equivalent latency. This framework represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation. <div>
arXiv:2504.15466v1 Announce Type: cross 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting</title>
<link>https://arxiv.org/abs/2504.15485</link>
<guid>https://arxiv.org/abs/2504.15485</guid>
<content:encoded><![CDATA[
<div> Keywords: occluded objects, visual patterns, reasoning, vision-language models, spatial understanding <br />
Summary: <br />
The article introduces the CAPTURe task, which challenges vision-language models to count objects in patterns while inferring behind occluders. The task aims to evaluate models' skills in recognizing visual patterns and reasoning about occluded objects, essential for spatial understanding in real-world environments. Four VLMs were tested on CAPTURe, showing difficulties in counting both occluded and unoccluded patterns, with a notable decrease in performance with occlusion. Humans performed significantly better on the task, highlighting the models' deficiencies in handling occlusion and counting in images. Providing auxiliary information on occluded object locations improved model performance, indicating that errors stemmed from both handling occlusion and counting difficulties. Ultimately, the study suggests that current VLMs, including strong models like GPT-4o, struggle with occluded object reasoning and spatial relationships. <br /> <div>
arXiv:2504.15485v1 Announce Type: cross 
Abstract: Recognizing and reasoning about occluded (partially or fully hidden) objects is vital to understanding visual scenes, as occlusions frequently occur in real-world environments and act as obstacles for spatial comprehension. To test models' ability to reason about multiple occluded objects, we introduce a novel task, Counting Amodally for Patterns Through Unseen REgions (CAPTURe), which requires a model to count objects arranged in a pattern by inferring how the pattern continues behind an occluder (an object which blocks parts of the scene). CAPTURe requires both recognizing visual patterns and reasoning, making it a useful testbed for evaluating vision-language models (VLMs) on whether they understand occluded patterns and possess spatial understanding skills. By requiring models to reason about occluded objects, CAPTURe also tests VLMs' ability to form world models that would allow them to fill in missing information. CAPTURe consists of two parts: (1) CAPTURe-real, with manually filtered images of real objects in patterns and (2) CAPTURe-synthetic, a controlled diagnostic with generated patterned images. We evaluate four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURe, finding that models struggle to count on both occluded and unoccluded patterns. Crucially, we find that models perform worse with occlusion, suggesting that VLMs are also deficient in inferring unseen spatial relationships: even the strongest VLMs like GPT-4o fail to count with occlusion. In contrast, we find that humans achieve very little error on CAPTURe. We also find that providing auxiliary information of occluded object locations increases performance, underscoring that the model error comes both from an inability to handle occlusion as well as difficulty counting in images.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, safety, lifecycle, comprehensive, literature review

Summary: 
The paper introduces the concept of "full-stack" safety for Large Language Models (LLMs) to address concerns regarding security and safety implications throughout the entire lifecycle of LLMs. The comprehensive perspective of the survey covers data preparation, pre-training, post-training, deployment, and commercialization stages. It is supported by an extensive literature review of over 800 papers, ensuring thorough coverage and systematic organization of security issues. The unique insights gained from the analysis include identifying promising research directions such as safety in data generation, alignment techniques, model editing, and LLM-based agent systems. The paper provides valuable guidance for researchers interested in pursuing future work in the field of LLM safety.<br /><br />Summary: <div>
arXiv:2504.15585v1 Announce Type: cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction</title>
<link>https://arxiv.org/abs/2504.15629</link>
<guid>https://arxiv.org/abs/2504.15629</guid>
<content:encoded><![CDATA[
<div> inference time, citation accuracy, retrieval augmented generation, post-processing algorithms, LLMs <br />
<br />Summary: 
The article introduces post-processing algorithms to enhance citation accuracy in Large Language Models (LLMs) in Retrieval Augmented Generation (RAG) systems. Current LLMs have a 74% citation accuracy rate, prompting the development of efficient methods to improve this. The proposed approaches include cross-checking citations with retrieved articles using keyword + semantic matching, fine-tuning with BERTScore, and a lightweight LLM-based technique. Experimental results show a 15.46% relative improvement in citation accuracy, allowing for a shift to smaller, more cost-effective models with faster inference times. This enhancement is crucial for AI-generated content reliability in information retrieval and summarization tasks, particularly in commercial products where customer trust is paramount. <div>
arXiv:2504.15629v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a powerful application of Large Language Models (LLMs), revolutionizing information search and consumption. RAG systems combine traditional search capabilities with LLMs to generate comprehensive answers to user queries, ideally with accurate citations. However, in our experience of developing a RAG product, LLMs often struggle with source attribution, aligning with other industry studies reporting citation accuracy rates of only about 74% for popular generative search engines. To address this, we present efficient post-processing algorithms to improve citation accuracy in LLM-generated responses, with minimal impact on latency and cost. Our approaches cross-check generated citations against retrieved articles using methods including keyword + semantic matching, fine tuned model with BERTScore, and a lightweight LLM-based technique. Our experimental results demonstrate a relative improvement of 15.46% in the overall accuracy metrics of our RAG system. This significant enhancement potentially enables a shift from our current larger language model to a relatively smaller model that is approximately 12x more cost-effective and 3x faster in inference time, while maintaining comparable performance. This research contributes to enhancing the reliability and trustworthiness of AI-generated content in information retrieval and summarization tasks which is critical to gain customer trust especially in commercial products.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation</title>
<link>https://arxiv.org/abs/2504.15659</link>
<guid>https://arxiv.org/abs/2504.15659</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Electronic Design Automation, RTL code generation, functional validation, VERICODER

Summary:
Recent advancements in Large Language Models (LLMs) have spurred interest in employing them for Electronic Design Automation (EDA) tasks, particularly in RTL code generation. However, most existing RTL datasets prioritize syntax over functional validation, potentially leading to inaccuracies in the generated code. To address this issue, the authors introduce VERICODER, a model fine-tuned on a dataset validated for functional correctness. This dataset, created using a unique approach that combines unit test generation with iterative refinement, ensures that every example includes a natural language description, an RTL implementation, and passing tests. VERICODER, trained on this functionally validated dataset, demonstrates significant performance improvements in functional correctness metrics on established benchmarks. An ablation study confirms the superiority of models trained on functionally validated datasets, highlighting the critical role of high-quality datasets in enhancing RTL code generation.<br /><br />Summary: <div>
arXiv:2504.15659v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest in applying them to Electronic Design Automation (EDA) tasks, particularly Register Transfer Level (RTL) code generation. While several RTL datasets have been introduced, most focus on syntactic validity rather than functional validation with tests, leading to training examples that compile but may not implement the intended behavior. We present VERICODER, a model for RTL code generation fine-tuned on a dataset validated for functional correctness. This fine-tuning dataset is constructed using a novel methodology that combines unit test generation with feedback-directed refinement. Given a natural language specification and an initial RTL design, we prompt a teacher model (GPT-4o-mini) to generate unit tests and iteratively revise the RTL design based on its simulation results using the generated tests. If necessary, the teacher model also updates the tests to ensure they comply with the natural language specification. As a result of this process, every example in our dataset is functionally validated, consisting of a natural language description, an RTL implementation, and passing tests. Fine-tuned on this dataset of over 125,000 examples, VERICODER achieves state-of-the-art metrics in functional correctness on VerilogEval and RTLLM, with relative gains of up to 71.7% and 27.4% respectively. An ablation study further shows that models trained on our functionally validated dataset outperform those trained on functionally non-validated datasets, underscoring the importance of high-quality datasets in RTL code generation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving</title>
<link>https://arxiv.org/abs/2504.15780</link>
<guid>https://arxiv.org/abs/2504.15780</guid>
<content:encoded><![CDATA[
arXiv:2504.15780v1 Announce Type: cross 
Abstract: Mathematical geometric problem solving (GPS) often requires effective integration of multimodal information and verifiable logical coherence. Despite the fast development of large language models in general problem solving, it remains unresolved regarding with both methodology and benchmarks, especially given the fact that exiting synthetic GPS benchmarks are often not self-verified and contain noise and self-contradicted information due to the illusion of LLMs. In this paper, we propose a scalable data engine called TrustGeoGen for problem generation, with formal verification to provide a principled benchmark, which we believe lays the foundation for the further development of methods for GPS. The engine synthesizes geometric data through four key innovations: 1) multimodal-aligned generation of diagrams, textual descriptions, and stepwise solutions; 2) formal verification ensuring rule-compliant reasoning paths; 3) a bootstrapping mechanism enabling complexity escalation via recursive state generation and 4) our devised GeoExplore series algorithms simultaneously produce multi-solution variants and self-reflective backtracking traces. By formal logical verification, TrustGeoGen produces GeoTrust-200K dataset with guaranteed modality integrity, along with GeoTrust-test testset. Experiments reveal the state-of-the-art models achieve only 49.17\% accuracy on GeoTrust-test, demonstrating its evaluation stringency. Crucially, models trained on GeoTrust achieve OOD generalization on GeoQA, significantly reducing logical inconsistencies relative to pseudo-label annotated by OpenAI-o1. Our code is available at https://github.com/Alpha-Innovator/TrustGeoGen
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Private is Your Attention? Bridging Privacy with In-Context Learning</title>
<link>https://arxiv.org/abs/2504.16000</link>
<guid>https://arxiv.org/abs/2504.16000</guid>
<content:encoded><![CDATA[
arXiv:2504.16000v1 Announce Type: cross 
Abstract: In-context learning (ICL)-the ability of transformer-based models to perform new tasks from examples provided at inference time-has emerged as a hallmark of modern language models. While recent works have investigated the mechanisms underlying ICL, its feasibility under formal privacy constraints remains largely unexplored. In this paper, we propose a differentially private pretraining algorithm for linear attention heads and present the first theoretical analysis of the privacy-accuracy trade-off for ICL in linear regression. Our results characterize the fundamental tension between optimization and privacy-induced noise, formally capturing behaviors observed in private training via iterative methods. Additionally, we show that our method is robust to adversarial perturbations of training prompts, unlike standard ridge regression. All theoretical findings are supported by extensive simulations across diverse settings.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of Video Diffusion Models: Foundations, Implementations, and Applications</title>
<link>https://arxiv.org/abs/2504.16081</link>
<guid>https://arxiv.org/abs/2504.16081</guid>
<content:encoded><![CDATA[
arXiv:2504.16081v1 Announce Type: cross 
Abstract: Recent advances in diffusion models have revolutionized video generation, offering superior temporal consistency and visual quality compared to traditional generative adversarial networks-based approaches. While this emerging field shows tremendous promise in applications, it faces significant challenges in motion consistency, computational efficiency, and ethical considerations. This survey provides a comprehensive review of diffusion-based video generation, examining its evolution, technical foundations, and practical applications. We present a systematic taxonomy of current methodologies, analyze architectural innovations and optimization strategies, and investigate applications across low-level vision tasks such as denoising and super-resolution. Additionally, we explore the synergies between diffusionbased video generation and related domains, including video representation learning, question answering, and retrieval. Compared to the existing surveys (Lei et al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which focus on specific aspects of video generation, such as human video synthesis (Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our work provides a broader, more updated, and more fine-grained perspective on diffusion-based approaches with a special section for evaluation metrics, industry solutions, and training engineering techniques in video generation. This survey serves as a foundational resource for researchers and practitioners working at the intersection of diffusion models and video generation, providing insights into both the theoretical frameworks and practical implementations that drive this rapidly evolving field. A structured list of related works involved in this survey is also available on https://github.com/Eyeline-Research/Survey-Video-Diffusion.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift</title>
<link>https://arxiv.org/abs/2212.09409</link>
<guid>https://arxiv.org/abs/2212.09409</guid>
<content:encoded><![CDATA[
arXiv:2212.09409v3 Announce Type: replace 
Abstract: Selecting an effective training signal for machine learning tasks is difficult: expert annotations are expensive, and crowd-sourced annotations may not be reliable. Recent work has demonstrated that learning from a distribution over labels acquired from crowd annotations can be effective both for performance and uncertainty estimation. However, this has mainly been studied using a limited set of soft-labeling methods in an in-domain setting. Additionally, no one method has been shown to consistently perform well across tasks, making it difficult to know a priori which to choose. To fill these gaps, this paper provides the first large-scale empirical study on learning from crowd labels in the out-of-domain setting, systematically analyzing 8 soft-labeling methods on 4 language and vision tasks. Additionally, we propose to aggregate soft-labels via a simple average in order to achieve consistent performance across tasks. We demonstrate that this yields classifiers with improved predictive uncertainty estimation in most settings while maintaining consistent raw performance compared to learning from individual soft-labeling methods or taking a majority vote of the annotations. We additionally highlight that in regimes with abundant or minimal training data, the selection of soft labeling method is less important, while for highly subjective labels and moderate amounts of training data, aggregation yields significant improvements in uncertainty estimation over individual methods. Code can be found at https://github.com/copenlu/aggregating-crowd-annotations-ood.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Low-Rank Parametrization of Reward Models for Controlled Language Generation</title>
<link>https://arxiv.org/abs/2407.04615</link>
<guid>https://arxiv.org/abs/2407.04615</guid>
<content:encoded><![CDATA[
arXiv:2407.04615v3 Announce Type: replace 
Abstract: Language models trained on large amounts of data are known to produce inappropriate content in some cases and require careful tuning to be used in the real world. We revisit an effective and modular approach for controllability of the language models, when an external expert model guides the decoding. Particularly, we zoom in into the parametrization choice of an external expert, highlighting the difference between low-rank and higher-rank parametrizations. Higher-rank experts are designed to support high flexibility when representing the rewards, leading to higher computational costs during decoding. However, we demonstrate that they might not use their full flexibility. By analyzing the recently proposed reward-augmented decoding approach (RAD), which uses a higher-rank expert model, we introduce a simpler but more efficient low-rank parametrization of the expert model enabling fast and effective guided decoding. We empirically show that the low-rank RAD performs on par with the more flexible RAD on a detoxification and a sentiment control task, while requiring only a single reward model call per generated token.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation</title>
<link>https://arxiv.org/abs/2408.06276</link>
<guid>https://arxiv.org/abs/2408.06276</guid>
<content:encoded><![CDATA[
arXiv:2408.06276v4 Announce Type: replace 
Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities. To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles. It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions. This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-World Evaluation for Retrieving Diverse Perspectives</title>
<link>https://arxiv.org/abs/2409.18110</link>
<guid>https://arxiv.org/abs/2409.18110</guid>
<content:encoded><![CDATA[
arXiv:2409.18110v2 Announce Type: replace 
Abstract: We study retrieving a set of documents that covers various perspectives on a complex and contentious question (e.g., will ChatGPT do more harm than good?). We curate a Benchmark for Retrieval Diversity for Subjective questions (BERDS), where each example consists of a question and diverse perspectives associated with the question, sourced from survey questions and debate websites. On this data, retrievers paired with a corpus are evaluated to surface a document set that contains diverse perspectives. Our framing diverges from most retrieval tasks in that document relevancy cannot be decided by simple string matches to references. Instead, we build a language model-based automatic evaluator that decides whether each retrieved document contains a perspective. This allows us to evaluate the performance of three different types of corpus (Wikipedia, web snapshot, and corpus constructed on the fly with retrieved pages from the search engine) paired with retrievers. Retrieving diverse documents remains challenging, with the outputs from existing retrievers covering all perspectives on only 40% of the examples. We further study the effectiveness of query expansion and diversity-focused reranking approaches and analyze retriever sycophancy.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models</title>
<link>https://arxiv.org/abs/2410.02355</link>
<guid>https://arxiv.org/abs/2410.02355</guid>
<content:encoded><![CDATA[
arXiv:2410.02355v4 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit hallucinations due to incorrect or outdated knowledge. Hence, model editing methods have emerged to enable targeted knowledge updates. To achieve this, a prevailing paradigm is the locating-then-editing approach, which first locates influential parameters and then edits them by introducing a perturbation. While effective, current studies have demonstrated that this perturbation inevitably disrupt the originally preserved knowledge within LLMs, especially in sequential editing scenarios. To address this, we introduce AlphaEdit, a novel solution that projects perturbation onto the null space of the preserved knowledge before applying it to the parameters. We theoretically prove that this projection ensures the output of post-edited LLMs remains unchanged when queried about the preserved knowledge, thereby mitigating the issue of disruption. Extensive experiments on various LLMs, including LLaMA3, GPT2-XL, and GPT-J, show that AlphaEdit boosts the performance of most locating-then-editing methods by an average of 36.7% with a single line of additional code for projection solely. Our code is available at: https://github.com/jianghoucheng/AlphaEdit.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models</title>
<link>https://arxiv.org/abs/2410.19503</link>
<guid>https://arxiv.org/abs/2410.19503</guid>
<content:encoded><![CDATA[
arXiv:2410.19503v2 Announce Type: replace 
Abstract: Despite the success of Large Language Models (LLMs), they still face challenges related to high inference costs and memory requirements. To address these issues, Knowledge Distillation (KD) has emerged as a popular method for model compression, with student-generated outputs (SGOs) as training data being particularly notable for reducing the mismatch between training and inference. However, SGOs often produce noisy and biased sequences, which can lead to misguidance from the teacher model, especially in long sequences. To mitigate these challenges, we propose SWITCH (Studying WIth TeaCHer for Knowledge Distillation), a novel approach that strategically incorporates the teacher model during the student's sequence generation. SWITCH identifies discrepancies between the token probabilities of the teacher and student models, allowing the teacher to intervene selectively, particularly in long sequences that are more prone to teacher misguidance. Extensive experimental results across three model families and five instruction-following datasets show that SWITCH surpasses traditional KD methods, particularly excelling in the generation of long sequential data.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity Helps Jailbreak Large Language Models</title>
<link>https://arxiv.org/abs/2411.04223</link>
<guid>https://arxiv.org/abs/2411.04223</guid>
<content:encoded><![CDATA[
arXiv:2411.04223v2 Announce Type: replace 
Abstract: We have uncovered a powerful jailbreak technique that leverages large language models' ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree</title>
<link>https://arxiv.org/abs/2412.12639</link>
<guid>https://arxiv.org/abs/2412.12639</guid>
<content:encoded><![CDATA[
arXiv:2412.12639v3 Announce Type: replace 
Abstract: Striking an optimal balance between minimal drafting latency and high speculation accuracy to enhance the inference speed of Large Language Models remains a significant challenge in speculative decoding. In this paper, we introduce Falcon, an innovative semi-autoregressive speculative decoding framework fashioned to augment both the drafter's parallelism and output quality. Falcon incorporates the Coupled Sequential Glancing Distillation technique, which fortifies inter-token dependencies within the same block, leading to increased speculation accuracy. We offer a comprehensive theoretical analysis to illuminate the underlying mechanisms. Additionally, we introduce a Custom-Designed Decoding Tree, which permits the drafter to generate multiple tokens in a single forward pass and accommodates multiple forward passes as needed, thereby boosting the number of drafted tokens and significantly improving the overall acceptance rate. Comprehensive evaluations on benchmark datasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior acceleration capabilities. The framework achieves a lossless speedup ratio ranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model series. These results outstrip existing speculative decoding methods for LLMs, including Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact drafter architecture equivalent to merely two Transformer layers.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Whisper on Low-Resource Languages for Real-World Applications</title>
<link>https://arxiv.org/abs/2412.15726</link>
<guid>https://arxiv.org/abs/2412.15726</guid>
<content:encoded><![CDATA[
arXiv:2412.15726v3 Announce Type: replace 
Abstract: This paper presents a new approach to fine-tuning OpenAI's Whisper model for low-resource languages by introducing a novel data generation method that converts sentence-level data into a long-form corpus, using Swiss German as a case study. Non-sentence-level data, which could improve the performance of long-form audio, is difficult to obtain and often restricted by copyright laws. Our method bridges this gap by transforming more accessible sentence-level data into a format that preserves the model's ability to handle long-form audio and perform segmentation without requiring non-sentence-level data. Our data generation process improves performance in several real-world applications and leads to the development of a new state-of-the-art speech-to-text (STT) model for Swiss German. We compare our model with a non-fine-tuned Whisper and our previous state-of-the-art Swiss German STT models, where our new model achieves higher BLEU scores. Our results also indicate that the proposed method is adaptable to other low-resource languages, supported by written guidance and code that allows the creation of fine-tuned Whisper models, which keep segmentation capabilities and allow the transcription of longer audio files using only sentence-level data with high quality.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs</title>
<link>https://arxiv.org/abs/2412.15993</link>
<guid>https://arxiv.org/abs/2412.15993</guid>
<content:encoded><![CDATA[
arXiv:2412.15993v2 Announce Type: replace 
Abstract: Arguments evoke emotions, influencing the effect of the argument itself. Not only the emotional intensity but also the category influence the argument's effects, for instance, the willingness to adapt stances. While binary emotionality has been studied in arguments, there is no work on discrete emotion categories (e.g., "Anger") in such data. To fill this gap, we crowdsource subjective annotations of emotion categories in a German argument corpus and evaluate automatic LLM-based labeling methods. Specifically, we compare three prompting strategies (zero-shot, one-shot, chain-of-thought) on three large instruction-tuned language models (Falcon-7b-instruct, Llama-3.1-8B-instruct, GPT-4o-mini). We further vary the definition of the output space to be binary (is there emotionality in the argument?), closed-domain (which emotion from a given label set is in the argument?), or open-domain (which emotion is in the argument?). We find that emotion categories enhance the prediction of emotionality in arguments, emphasizing the need for discrete emotion annotations in arguments. Across all prompt settings and models, automatic predictions show a high recall but low precision for predicting anger and fear, indicating a strong bias toward negative emotions.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks</title>
<link>https://arxiv.org/abs/2502.13053</link>
<guid>https://arxiv.org/abs/2502.13053</guid>
<content:encoded><![CDATA[
arXiv:2502.13053v2 Announce Type: replace 
Abstract: As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2503.04797</link>
<guid>https://arxiv.org/abs/2503.04797</guid>
<content:encoded><![CDATA[
arXiv:2503.04797v2 Announce Type: replace 
Abstract: Parallel corpora play an important role in training machine translation (MT) models, particularly for low-resource languages where high-quality bilingual data is scarce. This review provides a comprehensive overview of available parallel corpora for Indic languages, which span diverse linguistic families, scripts, and regional variations. We categorize these corpora into text-to-text, code-switched, and various categories of multimodal datasets, highlighting their significance in the development of robust multilingual MT systems. Beyond resource enumeration, we critically examine the challenges faced in corpus creation, including linguistic diversity, script variation, data scarcity, and the prevalence of informal textual content.We also discuss and evaluate these corpora in various terms such as alignment quality and domain representativeness. Furthermore, we address open challenges such as data imbalance across Indic languages, the trade-off between quality and quantity, and the impact of noisy, informal, and dialectal data on MT performance. Finally, we outline future directions, including leveraging cross-lingual transfer learning, expanding multilingual datasets, and integrating multimodal resources to enhance translation quality. To the best of our knowledge, this paper presents the first comprehensive review of parallel corpora specifically tailored for low-resource Indic languages in the context of machine translation.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks</title>
<link>https://arxiv.org/abs/2503.09572</link>
<guid>https://arxiv.org/abs/2503.09572</guid>
<content:encoded><![CDATA[
arXiv:2503.09572v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of-the-art 57.58% success rate on the WebArena-Lite benchmark as well as a text-only state-of-the-art 81.36% success rate on WebVoyager.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques</title>
<link>https://arxiv.org/abs/2503.11816</link>
<guid>https://arxiv.org/abs/2503.11816</guid>
<content:encoded><![CDATA[
arXiv:2503.11816v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages</title>
<link>https://arxiv.org/abs/2504.00021</link>
<guid>https://arxiv.org/abs/2504.00021</guid>
<content:encoded><![CDATA[
arXiv:2504.00021v2 Announce Type: replace 
Abstract: This paper presents the winning submission of the RaaVa team to the AmericasNLP 2025 Shared Task 3 on Automatic Evaluation Metrics for Machine Translation (MT) into Indigenous Languages of America, where our system ranked first overall based on average Pearson correlation with the human annotations. We introduce Feature-Union Scorer (FUSE) for Evaluation, FUSE integrates Ridge regression and Gradient Boosting to model translation quality. In addition to FUSE, we explore five alternative approaches leveraging different combinations of linguistic similarity features and learning paradigms. FUSE Score highlights the effectiveness of combining lexical, phonetic, semantic, and fuzzy token similarity with learning-based modeling to improve MT evaluation for morphologically rich and low-resource languages. MT into Indigenous languages poses unique challenges due to polysynthesis, complex morphology, and non-standardized orthography. Conventional automatic metrics such as BLEU, TER, and ChrF often fail to capture deeper aspects like semantic adequacy and fluency. Our proposed framework, formerly referred to as FUSE, incorporates multilingual sentence embeddings and phonological encodings to better align with human evaluation. We train supervised models on human-annotated development sets and evaluate held-out test data. Results show that FUSE consistently achieves higher Pearson and Spearman correlations with human judgments, offering a robust and linguistically informed solution for MT evaluation in low-resource settings.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2504.01801</link>
<guid>https://arxiv.org/abs/2504.01801</guid>
<content:encoded><![CDATA[
arXiv:2504.01801v2 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities despite the extreme language imbalance in the pre-training data. In this paper, we closely examine the reasons behind this phenomenon, focusing on the pre-training corpus. We find that the existence of code-switching, alternating between different languages within a context, is key to multilingual capabilities. We conduct an analysis to investigate code-switching in the pre-training corpus, examining its presence and categorizing it into four types within two quadrants. We then assess its impact on multilingual performance. These types of code-switching data are unbalanced in proportions and demonstrate different effects on facilitating language transfer. To better explore the power of code-switching for language alignment during pre-training, we investigate the strategy of synthetic code-switching. We continuously scale up the synthetic code-switching data and observe remarkable improvements in both benchmarks and representation space. Extensive experiments indicate that incorporating synthetic code-switching data enables better language alignment and generalizes well to high, medium, and low-resource languages with pre-training corpora of varying qualities.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation</title>
<link>https://arxiv.org/abs/2504.04060</link>
<guid>https://arxiv.org/abs/2504.04060</guid>
<content:encoded><![CDATA[
arXiv:2504.04060v2 Announce Type: replace 
Abstract: Speech large language models (LLMs) have emerged as a prominent research focus in speech processing. We introduce VocalNet-1B and VocalNet-8B, a series of high-performance, low-latency speech LLMs enabled by a scalable and model-agnostic training framework designed for real-time voice interaction. Central to our contribution is the first application of multi-token prediction (MTP) to speech LLMs. This approach represents a paradigm shift from standard next-token prediction (NTP), offering simultaneous improvements in generation speed and quality. Informed by analysis of MTP's effect on speech generation and experimental comparisons, we designed a straightforward and highly effective MTP implementation. Experiments demonstrate that VocalNet performs on par with mainstream Omni LLMs even with limited training data, and significantly surpasses existing open-source speech LLMs. To foster reproducibility and community advancement, all model weights, inference code, training data, and framework implementations have been made publicly available at https://github.com/SJTU-OmniAgent/VocalNet
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance</title>
<link>https://arxiv.org/abs/2504.07989</link>
<guid>https://arxiv.org/abs/2504.07989</guid>
<content:encoded><![CDATA[
arXiv:2504.07989v2 Announce Type: replace 
Abstract: Small Language Models (SLMs) offer efficient alternatives to LLMs for specific domains. The 2023 TinyStories study developed an English dataset that allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our research expands this framework by translating the original dataset into Indian languages and creating synthetic data using LLMs. We focus on Hindi, Marathi, and Bengali, evaluating SLMs for regional language processing and understanding linguistic complexity. We show that SLMs efficiently process regional languages with significantly fewer parameters than LLMs, providing a complementary framework for ``inference based evaluation" of tokenization strategies and linguistic complexity. Our analysis shows that language-specific tokenizers outperform general-purpose ones for Indian languages. Empirical validations, supported by information-theoretic and morphological analyses, provides fundamental understanding behind the better performance of Hindi models over Marathi and Bengali. Additionally, we show that synthetic datasets outperform translated content for training SLMs. Correlation analyses reveal cross-linguistic patterns and language-specific relationships between creativity, grammatical precision, and narrative completeness. These findings advance both the practical application of SLMs to underserved languages and our theoretical understanding of neural language development.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol</title>
<link>https://arxiv.org/abs/2504.10284</link>
<guid>https://arxiv.org/abs/2504.10284</guid>
<content:encoded><![CDATA[
arXiv:2504.10284v2 Announce Type: replace 
Abstract: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Certifying Knowledge Comprehension in LLMs</title>
<link>https://arxiv.org/abs/2402.15929</link>
<guid>https://arxiv.org/abs/2402.15929</guid>
<content:encoded><![CDATA[
arXiv:2402.15929v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical systems where they provide answers based on in-context information derived from knowledge bases. As LLMs are increasingly envisioned as superhuman agents, their proficiency in knowledge comprehension-extracting relevant information and reasoning over it to answer questions, a key facet of human intelligence-becomes crucial. However, existing evaluations of LLMs on knowledge comprehension are typically conducted on small test sets, but these datasets represent only a tiny fraction of the vast number of possible queries. Simple empirical evaluations on these limited test sets raises concerns about the reliability and generalizability of the results. In this work, we introduce the first specification and certification framework for knowledge comprehension in LLMs, providing formal probabilistic guarantees for reliability. Instead of a fixed dataset, we design novel specifications that mathematically represent prohibitively large probability distributions of knowledge comprehension prompts with natural noise, using knowledge graphs. From these specifications, we generate quantitative certificates that offer high-confidence, tight bounds on the probability that a given LLM correctly answers any question drawn from the specification distribution. We apply our framework to certify SOTA LLMs in two domains: precision medicine and general question-answering. Our results reveal previously unrecognized vulnerabilities in SOTA LLMs due to natural noise in the prompts. Additionally, we establish performance hierarchies with formal guarantees among the SOTA LLMs, particularly in the context of precision medicine question-answering.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing RLHF Training for Large Language Models with Stage Fusion</title>
<link>https://arxiv.org/abs/2409.13221</link>
<guid>https://arxiv.org/abs/2409.13221</guid>
<content:encoded><![CDATA[
arXiv:2409.13221v3 Announce Type: replace-cross 
Abstract: We present RLHFuse, an efficient training system with stage fusion for Reinforcement Learning from Human Feedback (RLHF). Due to the intrinsic nature of RLHF training, i.e., the data skewness in the generation stage and the pipeline bubbles in the training stage, existing RLHF systems suffer from low GPU utilization. RLHFuse breaks the traditional view of RLHF workflow as a composition of individual tasks, splitting each task into finer-grained subtasks, and performing stage fusion to improve GPU utilization. RLHFuse contains two key ideas. First, for generation and inference tasks, RLHFuse splits them into sample-level subtasks, enabling efficient inter-stage fusion to overlap the execution of generation and inference stages, thus mitigating the original generation bottleneck dominated by long-tailed samples. Second, for training tasks, RLHFuse breaks them into subtasks of micro-batches and performs intra-stage fusion to concurrently execute these subtasks in the training stage with a fused pipeline schedule, effectively mitigating the pipeline bubbles. The experiments show that RLHFuse increases the training throughput by up to $3.7\times$, compared to existing systems.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct</title>
<link>https://arxiv.org/abs/2410.02064</link>
<guid>https://arxiv.org/abs/2410.02064</guid>
<content:encoded><![CDATA[
arXiv:2410.02064v3 Announce Type: replace-cross 
Abstract: It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the Llama3-8b-Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of "self" in the model, and demonstrate that the vector is causally related to the model's ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model's behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model's output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement</title>
<link>https://arxiv.org/abs/2410.13828</link>
<guid>https://arxiv.org/abs/2410.13828</guid>
<content:encoded><![CDATA[
arXiv:2410.13828v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for language model (LM) alignment. At its core, RLHF uses a margin-based loss for preference optimization, specifying ideal LM behavior only by the difference between preferred and dispreferred responses. In this paper, we identify a common pitfall of margin-based methods -- the under-specification of ideal LM behavior on preferred and dispreferred responses individually, which leads to two unintended consequences as the margin increases: (1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures. (2) The probability of preferred responses may decrease, even when those responses are ideal. We demystify the reasons behind these problematic behaviors: margin-based losses couple the change in the preferred probability to the gradient of the dispreferred one, and vice versa, often preventing the preferred probability from increasing while the dispreferred one decreases, and thus causing a synchronized increase or decrease in both probabilities. We term this effect, inherent in margin-based objectives, gradient entanglement. Formally, we derive conditions for general margin-based alignment objectives under which gradient entanglement becomes concerning: the inner product of the gradients of preferred and dispreferred log-probabilities is large relative to the individual gradient norms. We theoretically investigate why such inner products can be large when aligning language models and empirically validate our findings. Empirical implications of our framework extend to explaining important differences in the training dynamics of various preference optimization algorithms, and suggesting potential algorithm designs to mitigate the under-specification issue of margin-based methods and thereby improving language model alignment.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title>
<link>https://arxiv.org/abs/2410.14669</link>
<guid>https://arxiv.org/abs/2410.14669</guid>
<content:encoded><![CDATA[
arXiv:2410.14669v3 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term natural adversarial samples. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, NaturalBench, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing blind solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can be solved with commonsense priors. We evaluate 53 state-of-the-art VLMs on NaturalBench, showing that models like LLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o lag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is hard from two angles: (1) Compositionality: Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) Biases: NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments</title>
<link>https://arxiv.org/abs/2410.21131</link>
<guid>https://arxiv.org/abs/2410.21131</guid>
<content:encoded><![CDATA[
arXiv:2410.21131v3 Announce Type: replace-cross 
Abstract: As machine learning models evolve, maintaining transparency demands more human-centric explainable AI techniques. Counterfactual explanations, with roots in human reasoning, identify the minimal input changes needed to obtain a given output and, hence, are crucial for supporting decision-making. Despite their importance, the evaluation of these explanations often lacks grounding in user studies and remains fragmented, with existing metrics not fully capturing human perspectives. To address this challenge, we developed a diverse set of 30 counterfactual scenarios and collected ratings across 8 evaluation metrics from 206 respondents. Subsequently, we fine-tuned different Large Language Models (LLMs) to predict average or individual human judgment across these metrics. Our methodology allowed LLMs to achieve an accuracy of up to 63% in zero-shot evaluations and 85% (over a 3-classes prediction) with fine-tuning across all metrics. The fine-tuned models predicting human ratings offer better comparability and scalability in evaluating different counterfactual explanation frameworks.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Codenames as a Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2412.11373</link>
<guid>https://arxiv.org/abs/2412.11373</guid>
<content:encoded><![CDATA[
arXiv:2412.11373v2 Announce Type: replace-cross 
Abstract: In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark</title>
<link>https://arxiv.org/abs/2502.19676</link>
<guid>https://arxiv.org/abs/2502.19676</guid>
<content:encoded><![CDATA[
arXiv:2502.19676v3 Announce Type: replace-cross 
Abstract: Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents</title>
<link>https://arxiv.org/abs/2504.09723</link>
<guid>https://arxiv.org/abs/2504.09723</guid>
<content:encoded><![CDATA[
arXiv:2504.09723v2 Announce Type: replace-cross 
Abstract: A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.
]]></content:encoded>
<pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13914</link>
<guid>https://arxiv.org/abs/2504.13914</guid>
<content:encoded><![CDATA[
<div> Keywords: Seed-Thinking-v1.5, reasoning abilities, Mixture-of-Experts model, benchmarks, generalization

Summary:
Seed-Thinking-v1.5 is a new model that excels in reasoning tasks before responding, achieving impressive scores on benchmarks such as AIME 2024, Codeforces, and GPQA. The model's performance indicates strong reasoning abilities in STEM and coding fields. Additionally, Seed-Thinking-v1.5 demonstrates remarkable generalization across various domains, surpassing other models in win rates on non-reasoning tasks. It is a Mixture-of-Experts model with a compact size, featuring 20B activated and 200B total parameters. To evaluate its generalized reasoning capabilities, two internal benchmarks, BeyondAIME and Codeforces, have been developed and will be publicly released for future research. Seed-Thinking-v1.5's success in diverse tasks showcases its potential for broader applicability and sheds light on the advancement of reasoning models in artificial intelligence.
<br /><br />Summary: <div>
arXiv:2504.13914v1 Announce Type: new 
Abstract: We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Conspiratorial Narratives within Arabic Online Content</title>
<link>https://arxiv.org/abs/2504.14037</link>
<guid>https://arxiv.org/abs/2504.14037</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, Topic Modeling, Top2Vec algorithm, conspiracy theories, Arabic digital spaces
<br />
<br />
Summary: 
This study investigates the spread of conspiracy theories in Arabic online spaces using advanced Natural Language Processing techniques. By analyzing data from Arabic blogs and Facebook, the research identifies six categories of conspiratorial narratives: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The study reveals how these narratives are deeply rooted in Arabic social media discourse, influenced by regional historical and sociopolitical contexts. By focusing on Arabic content, the research fills a gap in existing conspiracy theory studies that have largely concentrated on English-language or offline data. The findings offer new insights into how conspiracy theories evolve and manifest in Arabic digital spaces, shedding light on their impact on public discourse in the Arab world. <div>
arXiv:2504.14037v1 Announce Type: new 
Abstract: This study investigates the spread of conspiracy theories in Arabic digital spaces through computational analysis of online content. By combining Named Entity Recognition and Topic Modeling techniques, specifically the Top2Vec algorithm, we analyze data from Arabic blogs and Facebook to identify and classify conspiratorial narratives. Our analysis uncovers six distinct categories: gender/feminist, geopolitical, government cover-ups, apocalyptic, Judeo-Masonic, and geoengineering. The research highlights how these narratives are deeply embedded in Arabic social media discourse, shaped by regional historical, cultural, and sociopolitical contexts. By applying advanced Natural Language Processing methods to Arabic content, this study addresses a gap in conspiracy theory research, which has traditionally focused on English-language content or offline data. The findings provide new insights into the manifestation and evolution of conspiracy theories in Arabic digital spaces, enhancing our understanding of their role in shaping public discourse in the Arab world.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MEQA: A Meta-Evaluation Framework for Question &amp; Answer LLM Benchmarks</title>
<link>https://arxiv.org/abs/2504.14039</link>
<guid>https://arxiv.org/abs/2504.14039</guid>
<content:encoded><![CDATA[
<div> framework, meta-evaluation, question and answer, benchmarks, cybersecurity  
Summary:  
MEQA is a framework proposed for meta-evaluation of question and answer benchmarks, aiming to assess their quality through standardized assessments and quantifiable scores. The need for rigorous evaluations of Large Language Models (LLMs) is emphasized due to their increasing societal impact. Despite the existence of several evaluation benchmarks, there is a gap in effectively evaluating their quality, which MEQA seeks to address. The framework enables meaningful intra-benchmark comparisons and highlights the strengths and weaknesses of benchmarks in a specific domain, such as cybersecurity. By utilizing human and LLM evaluators, MEQA aims to provide insights into the performance of benchmarks and their relevance in assessing the capabilities of AI models in both defensive and security threat contexts.<br /><br />Summary: <div>
arXiv:2504.14039v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) advance, their potential for widespread societal impact grows simultaneously. Hence, rigorous LLM evaluations are both a technical necessity and social imperative. While numerous evaluation benchmarks have been developed, there remains a critical gap in meta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a framework for the meta-evaluation of question and answer (QA) benchmarks, to provide standardized assessments, quantifiable scores, and enable meaningful intra-benchmark comparisons. We demonstrate this approach on cybersecurity benchmarks, using human and LLM evaluators, highlighting the benchmarks' strengths and weaknesses. We motivate our choice of test domain by AI models' dual nature as powerful defensive tools and security threats.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task</title>
<link>https://arxiv.org/abs/2504.14066</link>
<guid>https://arxiv.org/abs/2504.14066</guid>
<content:encoded><![CDATA[
<div> Few-shot learning, quantized Gemma 2 9B model, Reddit, mental health data, self-state evidence <br />
<br />
Summary: <br />
The article presents a baseline system for the CLPsych 2025 A.1 task of classifying self-states in mental health data from Reddit. The system utilizes few-shot learning with a 4-bit quantized Gemma 2 9B model and a preprocessing step that first identifies relevant sentences indicating self-state evidence. A binary classification is then performed to determine whether the sentence indicates an adaptive or maladaptive self-state. This approach outperforms another method that relies on an LLM to highlight spans of variable length independently. The performance of the model is attributed to the sentence chunking step, which matches the granularity of human-annotated self-states and simplifies the task for the language model. The system achieves a test-time recall of 0.579, ranking third out of fourteen systems in the task. <div>
arXiv:2504.14066v1 Announce Type: new 
Abstract: We present a baseline for the CLPsych 2025 A.1 task: classifying self-states in mental health data taken from Reddit. We use few-shot learning with a 4-bit quantized Gemma 2 9B model and a data preprocessing step which first identifies relevant sentences indicating self-state evidence, and then performs a binary classification to determine whether the sentence is evidence of an adaptive or maladaptive self-state. This system outperforms our other method which relies on an LLM to highlight spans of variable length independently. We attribute the performance of our model to the benefits of this sentence chunking step for two reasons: partitioning posts into sentences 1) broadly matches the granularity at which self-states were human-annotated and 2) simplifies the task for our language model to a binary classification problem. Our system places third out of fourteen systems submitted for Task A.1, achieving a test-time recall of 0.579.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2504.14089</link>
<guid>https://arxiv.org/abs/2504.14089</guid>
<content:encoded><![CDATA[
<div> framework, logical reasoning, proof exploration, premise search, heuristics  
Summary:  
- LogicTree is a new framework designed to enhance the logical reasoning capabilities of large language models (LLMs) by automating structured proof exploration and ensuring logical coherence.
- This framework incorporates a caching mechanism to utilize historical knowledge effectively, prevent reasoning stagnation, and minimize redundancy.
- By decomposing premise search into a linear process, LogicTree addresses the combinatorial complexity of searching for the right combination of premises at each reasoning step.
- The refined premise selection within LogicTree restricts inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning.
- Two LLM-free heuristics are introduced for premise prioritization, enabling strategic proof search and enhancing the overall performance of the framework.
<br /><br />Summary: <div>
arXiv:2504.14089v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models</title>
<link>https://arxiv.org/abs/2504.14117</link>
<guid>https://arxiv.org/abs/2504.14117</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vision Language Models, Parameter-Efficient Fine-Tuning, Resource challenges, PEFT techniques <br />
Summary: <br />
Large Language Models (LLMs) and Vision Language Models (VLMs) have revolutionized artificial intelligence applications. Fully fine-tuning these models is costly, leading to the development of Parameter-Efficient Fine-Tuning (PEFT) techniques. This survey explores the challenges of traditional fine-tuning, such as overfitting and parameter inefficiency. It introduces a taxonomy of PEFT methods, categorizing them into different frameworks based on their mechanisms. PEFT methods offer strong performance with lower resource costs across various domains, including language, vision, and generative modeling. The survey also discusses open challenges in scalability, interpretability, and robustness and suggests future research directions. The goal is to provide a unified understanding of PEFT and its role in enabling practical, efficient, and sustainable use of large models. <br /> <div>
arXiv:2504.14117v1 Announce Type: new 
Abstract: Large models such as Large Language Models (LLMs) and Vision Language Models (VLMs) have transformed artificial intelligence, powering applications in natural language processing, computer vision, and multimodal learning. However, fully fine-tuning these models remains expensive, requiring extensive computational resources, memory, and task-specific data. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting large models to downstream tasks by updating only a small portion of parameters. This survey presents a comprehensive overview of PEFT techniques, focusing on their motivations, design principles, and effectiveness. We begin by analyzing the resource and accessibility challenges posed by traditional fine-tuning and highlight key issues, such as overfitting, catastrophic forgetting, and parameter inefficiency. We then introduce a structured taxonomy of PEFT methods -- grouped into additive, selective, reparameterized, hybrid, and unified frameworks -- and systematically compare their mechanisms and trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse domains, including language, vision, and generative modeling, showing how these techniques offer strong performance with lower resource costs. We also discuss important open challenges in scalability, interpretability, and robustness, and suggest future directions such as federated learning, domain adaptation, and theoretical grounding. Our goal is to provide a unified understanding of PEFT and its growing role in enabling practical, efficient, and sustainable use of large models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations</title>
<link>https://arxiv.org/abs/2504.14150</link>
<guid>https://arxiv.org/abs/2504.14150</guid>
<content:encoded><![CDATA[
<div> faithfulness, large language models, explanations, social bias, medical question answering

Summary: 
- Large language models (LLMs) generate explanations that can misrepresent their reasoning process, leading to over-trust and misuse.
- A new approach is introduced to measure the faithfulness of LLM explanations by comparing influential concepts implied by explanations to truly influential concepts.
- The method estimates faithfulness by using an auxiliary LLM to create realistic counterfactuals and a Bayesian hierarchical model to quantify causal effects.
- Experiments demonstrate the method's ability to quantify and reveal patterns of unfaithfulness, such as hiding social bias influences and providing misleading claims in medical question answering.
- By uncovering cases where explanations are unfaithful, the method helps improve the trustworthiness and accuracy of LLMs in generating explanations. 

<br /><br />Summary: <div>
arXiv:2504.14150v1 Announce Type: new 
Abstract: Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SConU: Selective Conformal Uncertainty in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14154</link>
<guid>https://arxiv.org/abs/2504.14154</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, conformal uncertainty, significance tests, miscoverage rates, prediction sets

Summary:<br /><br />
This paper introduces a novel approach called Selective Conformal Uncertainty (SConU) for managing uncertainty in large language models. SConU uses significance tests to identify outliers in the data that violate the exchangeability assumption, helping to control miscoverage rates and improve the reliability of prediction sets. The approach also includes conformal p-values to determine if a sample deviates from the uncertainty distribution of the calibration set. This allows for efficient management of uncertainty across various domains and enhances prediction efficiency. The study analyzes the components of conformal procedures to approximate conditional coverage, especially in high-stakes question-answering tasks. By offering rigorous guarantees of task-specific metrics and enabling better control of prediction uncertainty, SConU provides a valuable tool for the reliable deployment of language models in real-world applications.<br /><br />Summary: <div>
arXiv:2504.14154v1 Announce Type: new 
Abstract: As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correction Makes LLMs Better Parsers</title>
<link>https://arxiv.org/abs/2504.14165</link>
<guid>https://arxiv.org/abs/2504.14165</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, syntactic parsing, grammar rules, self-correction method, NLP tasks 

Summary:
Large language models (LLMs) have shown success in various NLP tasks but struggle with syntactic parsing. The study identifies limitations in utilizing grammar rules from existing treebanks, hindering the generation of valid syntactic structures. To address this, a self-correction method is proposed that uses grammar rules to guide LLMs in rectifying errors. Errors are automatically detected, and relevant rules are searched for, providing hints and examples for correction. Experimental results on English and Chinese datasets show significant performance improvements in both in-domain and cross-domain settings. This method enhances LLMs' parsing capabilities and helps them achieve better accuracy in syntactic parsing tasks. <br /><br />Summary: <div>
arXiv:2504.14165v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across various natural language processing (NLP) tasks. However, recent studies suggest that they still face challenges in performing fundamental NLP tasks essential for deep language understanding, particularly syntactic parsing. In this paper, we conduct an in-depth analysis of LLM parsing capabilities, delving into the specific shortcomings of their parsing results. We find that LLMs may stem from limitations to fully leverage grammar rules in existing treebanks, which restricts their capability to generate valid syntactic structures. To help LLMs acquire knowledge without additional training, we propose a self-correction method that leverages grammar rules from existing treebanks to guide LLMs in correcting previous errors. Specifically, we automatically detect potential errors and dynamically search for relevant rules, offering hints and examples to guide LLMs in making corrections themselves. Experimental results on three datasets with various LLMs, demonstrate that our method significantly improves performance in both in-domain and cross-domain settings on the English and Chinese datasets.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion</title>
<link>https://arxiv.org/abs/2504.14175</link>
<guid>https://arxiv.org/abs/2504.14175</guid>
<content:encoded><![CDATA[
<div> large language models, query expansion, fact verification, knowledge leakage, performance gains <br />
<br />
Summary: 
The article investigates the effectiveness of query expansion methods powered by large language models (LLMs) in zero-shot retrieval tasks. It challenges the assumption that LLMs can generate hypothetical documents to enhance real evidence retrieval by examining knowledge leakage in benchmarks. The study focuses on fact verification and analyzes whether generated documents contain information entailed by ground truth evidence. Results show performance improvements in claims where generated documents include sentences entailed by ground truth evidence, suggesting the presence of knowledge leakage in benchmarks. This may inflate the perceived performance of LLM-based query expansion methods, especially in scenarios requiring niche or novel knowledge retrieval. <div>
arXiv:2504.14175v1 Announce Type: new 
Abstract: Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyzed whether the generated documents contained information entailed by ground truth evidence and assessed their impact on performance. Our findings indicate that performance improvements occurred consistently only for claims whose generated documents included sentences entailed by ground truth evidence. This suggests that knowledge leakage may be present in these benchmarks, inflating the perceived performance of LLM-based query expansion methods, particularly in real-world scenarios that require retrieving niche or novel knowledge.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models</title>
<link>https://arxiv.org/abs/2504.14194</link>
<guid>https://arxiv.org/abs/2504.14194</guid>
<content:encoded><![CDATA[
<div> Quality Metrics, Data Selection, Language Models, Meta-rater, Dataset

Summary:<br /><br /> 
The article introduces a new method called PRRC to evaluate data quality for large language models (LLMs) based on Professionalism, Readability, Reasoning, and Cleanliness. It also presents Meta-rater, a multi-dimensional data selection approach that combines these dimensions with existing metrics through learned weightings. Meta-rater significantly accelerates convergence speed for 1.3B parameter models and enhances downstream task performance by 3.23. The method shows scalable benefits for 3.3B models trained on 100B tokens. The authors release the SlimPajama-627B dataset annotated across 25 quality metrics, including PRRC, to facilitate research in data-centric LLM development. The study demonstrates that integrating multiple quality dimensions outperforms conventional single-dimensional approaches, offering an efficient way to enhance pre-training effectiveness and model capability. <div>
arXiv:2504.14194v1 Announce Type: new 
Abstract: The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition</title>
<link>https://arxiv.org/abs/2504.14203</link>
<guid>https://arxiv.org/abs/2504.14203</guid>
<content:encoded><![CDATA[
<div> Challenges, Nested NER, Low resource, Class imbalance, EIoU-EMC<br />
<br />
Summary: <br />
Research on nested Named Entity Recognition (NER) tasks in specific domains faces challenges like low resources and class imbalances. To address this, a novel loss function, EIoU-EMC, is proposed by enhancing Intersection over Union and Multiclass losses. This method focuses on entity boundary and classification information to improve learning from limited data samples. Experimental validation on three biomedical NER datasets and one industrial dataset demonstrates the method's competitive performance compared to baselines. Significant advancements are observed in entity boundary recognition and classification with the proposed method. The code for this study is available for further exploration. <br /> <div>
arXiv:2504.14203v1 Announce Type: new 
Abstract: In recent years, research has mainly focused on the general NER task. There still have some challenges with nested NER task in the specific domains. Specifically, the scenarios of low resource and class imbalance impede the wide application for biomedical and industrial domains. In this study, we design a novel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss and Multiclass loss. Our proposed method specially leverages the information of entity boundary and entity classification, thereby enhancing the model's capacity to learn from a limited number of data samples. To validate the performance of this innovative method in enhancing NER task, we conducted experiments on three distinct biomedical NER datasets and one dataset constructed by ourselves from industrial complex equipment maintenance documents. Comparing to strong baselines, our method demonstrates the competitive performance across all datasets. During the experimental analysis, our proposed method exhibits significant advancements in entity boundary recognition and entity classification. Our code are available here.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification</title>
<link>https://arxiv.org/abs/2504.14212</link>
<guid>https://arxiv.org/abs/2504.14212</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, social biases, pretraining data, protected attribute detection, bias analysis

Summary:
Large language models (LLMs) learn linguistic knowledge from pretraining on vast amounts of web-crawled text data. However, these data sources may contain social biases that can be perpetuated or magnified by LLMs. This study introduces an annotation pipeline to examine social biases in pretraining corpora efficiently and effectively. The pipeline first identifies protected attributes representing diverse demographics and then categorizes the language's sentiment towards each attribute. The researchers conducted experiments using Common Crawl as a primary pretraining corpus to analyze biases and implement mitigation strategies. This approach highlights the importance of recognizing and addressing social biases in language models by understanding the influence of pretraining data sources on model outputs.<br /><br />Summary: <div>
arXiv:2504.14212v1 Announce Type: new 
Abstract: Large language models (LLMs) acquire general linguistic knowledge from massive-scale pretraining. However, pretraining data mainly comprised of web-crawled texts contain undesirable social biases which can be perpetuated or even amplified by LLMs. In this study, we propose an efficient yet effective annotation pipeline to investigate social biases in the pretraining corpora. Our pipeline consists of protected attribute detection to identify diverse demographics, followed by regard classification to analyze the language polarity towards each attribute. Through our experiments, we demonstrate the effect of our bias analysis and mitigation measures, focusing on Common Crawl as the most representative pretraining corpus.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Repeat Curse in Large Language Models from a Feature Perspective</title>
<link>https://arxiv.org/abs/2504.14218</link>
<guid>https://arxiv.org/abs/2504.14218</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Repeat Curse, Sparse Autoencoders, Mechanistic interpretability, Repetition Features <br />
<br />
Summary: 
This study explores the issue of repetitive text generation in Large Language Models (LLMs), which is referred to as the "Repeat Curse". It investigates the root causes of repetition in LLMs through the use of Sparse Autoencoders (SAEs) to extract monosemantic features. The proposed approach, "Duplicatus Charm", identifies key model activations responsible for generating repetitive outputs, known as "Repetition Features". By analyzing model layers and manipulating activations using SAEs, the study successfully mitigates the Repeat Curse. A repetition dataset is constructed to cover token and paragraph level repetitions, and an evaluation pipeline is introduced to measure the impact of identified repetition features. Deactivating these features effectively reduces repetition in text generation by LLMs. <div>
arXiv:2504.14218v1 Announce Type: new 
Abstract: Large language models (LLMs) have made remarkable progress in various domains, yet they often suffer from repetitive text generation, a phenomenon we refer to as the "Repeat Curse". While previous studies have proposed decoding strategies to mitigate repetition, the underlying mechanism behind this issue remains insufficiently explored. In this work, we investigate the root causes of repetition in LLMs through the lens of mechanistic interpretability. Inspired by recent advances in Sparse Autoencoders (SAEs), which enable monosemantic feature extraction, we propose a novel approach, "Duplicatus Charm", to induce and analyze the Repeat Curse. Our method systematically identifies "Repetition Features" -the key model activations responsible for generating repetitive outputs. First, we locate the layers most involved in repetition through logit analysis. Next, we extract and stimulate relevant features using SAE-based activation manipulation. To validate our approach, we construct a repetition dataset covering token and paragraph level repetitions and introduce an evaluation pipeline to quantify the influence of identified repetition features. Furthermore, by deactivating these features, we have effectively mitigated the Repeat Curse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification</title>
<link>https://arxiv.org/abs/2504.14223</link>
<guid>https://arxiv.org/abs/2504.14223</guid>
<content:encoded><![CDATA[
<div> Keywords: text simplification, large language models, plain language, automatic, inclusivity

Summary: 
- Text simplification is vital for enhancing accessibility to complex content for individuals with comprehension difficulties.
- Current methods of automatic text simplification do not fully utilize large language models to provide tailored customization for different target audiences.
- The practice of plain language, which has proven benefits for both consumers and organizations, is underutilized.
- The system https://simplifymytext.org is introduced, employing GPT-4 and Llama-3 to produce plain language content with customizable options for diverse audiences.
- Evaluation of the system's outputs across various metrics demonstrates its effectiveness and contribution to the field of automatic text simplification.

<br /><br />Summary: <div>
arXiv:2504.14223v1 Announce Type: new 
Abstract: Text simplification is essential for making complex content accessible to diverse audiences who face comprehension challenges. Yet, the limited availability of simplified materials creates significant barriers to personal and professional growth and hinders social inclusion. Although researchers have explored various methods for automatic text simplification, none fully leverage large language models (LLMs) to offer tailored customization for different target groups and varying levels of simplicity. Moreover, despite its proven benefits for both consumers and organizations, the well-established practice of plain language remains underutilized. In this paper, we https://simplifymytext.org, the first system designed to produce plain language content from multiple input formats, including typed text and file uploads, with flexible customization options for diverse audiences. We employ GPT-4 and Llama-3 and evaluate outputs across multiple metrics. Overall, our work contributes to research on automatic text simplification and highlights the importance of tailored communication in promoting inclusivity.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale</title>
<link>https://arxiv.org/abs/2504.14225</link>
<guid>https://arxiv.org/abs/2504.14225</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Personalized Assistants, User Profiling, Personalized Responses, Chatbots

Summary:
Large Language Models (LLMs) are being used as personalized assistants for various tasks, but there are still challenges in effectively leveraging user interaction history to personalize responses. The PERSONAMEM benchmark introduces curated user profiles with simulated interaction histories to evaluate LLM chatbots' ability to generate personalized responses. Current LLMs struggle to recognize the dynamic evolution of user profiles over time, leading to inaccuracies in response generation. Even advanced models like GPT-4.1, o4-mini, GPT-4.5, o1, and Gemini-2.0 only achieve around 50% accuracy in aligning responses with user preferences. This highlights the need for improvement in developing user-aware chatbots. The PERSONAMEM dataset and pipeline aim to facilitate future research in this area. (200 words)

<br /><br />Summary: Large Language Models (LLMs) are being used as personalized assistants for various tasks, but there are still challenges in effectively leveraging user interaction history to personalize responses. The PERSONAMEM benchmark introduces curated user profiles with simulated interaction histories to evaluate LLM chatbots' ability to generate personalized responses. Current LLMs struggle to recognize the dynamic evolution of user profiles over time, leading to inaccuracies in response generation. Even advanced models like GPT-4.1, o4-mini, GPT-4.5, o1, and Gemini-2.0 only achieve around 50% accuracy in aligning responses with user preferences. This highlights the need for improvement in developing user-aware chatbots. The PERSONAMEM dataset and pipeline aim to facilitate future research in this area. <div>
arXiv:2504.14225v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as personalized assistants for users across a wide range of tasks -- from offering writing support to delivering tailored recommendations or consultations. Over time, the interaction history between a user and an LLM can provide extensive information about an individual's traits and preferences. However, open questions remain on how well LLMs today can effectively leverage such history to (1) internalize the user's inherent traits and preferences, (2) track how the user profiling and preferences evolve over time, and (3) generate personalized responses accordingly in new scenarios.
  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features curated user profiles with over 180 simulated user-LLM interaction histories, each containing up to 60 sessions of multi-turn conversations across 15 real-world tasks that require personalization. Given an in-situ user query, i.e. query issued by the user from the first-person perspective, we evaluate LLM chatbots' ability to identify the most suitable response according to the current state of the user's profile. We observe that current LLMs still struggle to recognize the dynamic evolution in users' profiles over time through direct prompting approaches. As a consequence, LLMs often fail to deliver responses that align with users' current situations and preferences, with frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0 achieving only around 50% overall accuracy, suggesting room for improvement. We hope that PERSONAMEM, along with the user profile and conversation simulation pipeline, can facilitate future research in the development of truly user-aware chatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Subtle Ideological Manipulation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14287</link>
<guid>https://arxiv.org/abs/2504.14287</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Ideological Manipulation, Progressive-Left, Conservative-Right, Fine-tuning

Summary:
Large Language Models (LLMs) have revolutionized natural language processing but are vulnerable to ideological manipulation in politically sensitive areas. Previous research focused on binary Left-Right biases, using explicit prompts and political QA datasets. This study goes beyond binary ideologies to explore a spectrum from Progressive-Left to Conservative-Right. A multi-task dataset was created to reflect diverse ideological positions. Three LLMs were fine-tuned on this dataset to assess their ability to adopt nuanced ideologies. The results showed that fine-tuning significantly improved ideological alignment, with minor refinements from explicit prompts. This suggests that LLMs can be subtly manipulated ideologically, highlighting the need for robust safeguards to minimize risks. <br /><br />Summary: The study investigates the susceptibility of Large Language Models to nuanced ideological influences ranging from Progressive-Left to Conservative-Right. Fine-tuning the models on a multi-task dataset enhanced their alignment with these ideologies, indicating potential risks of ideological manipulation. Stronger safeguards are needed to mitigate these risks. <div>
arXiv:2504.14287v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have transformed natural language processing, but concerns have emerged about their susceptibility to ideological manipulation, particularly in politically sensitive areas. Prior work has focused on binary Left-Right LLM biases, using explicit prompts and fine-tuning on political QA datasets. In this work, we move beyond this binary approach to explore the extent to which LLMs can be influenced across a spectrum of political ideologies, from Progressive-Left to Conservative-Right. We introduce a novel multi-task dataset designed to reflect diverse ideological positions through tasks such as ideological QA, statement ranking, manifesto cloze completion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2, Mistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and express these nuanced ideologies. Our findings indicate that fine-tuning significantly enhances nuanced ideological alignment, while explicit prompts provide only minor refinements. This highlights the models' susceptibility to subtle ideological manipulation, suggesting a need for more robust safeguards to mitigate these risks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach</title>
<link>https://arxiv.org/abs/2504.14321</link>
<guid>https://arxiv.org/abs/2504.14321</guid>
<content:encoded><![CDATA[
<div> multimodal coreference resolution, Chinese, social media, dataset, TikTalkCoref<br />
Summary:<br />
This article introduces the TikTalkCoref dataset, the first Chinese multimodal coreference dataset for real-world social media dialogues. Derived from the Douyin platform, the dataset pairs short videos with textual dialogues from user comments and includes manually annotated coreference clusters for person mentions in the text and corresponding video frames. An effective benchmark approach for multimodal coreference resolution in the celebrity domain is presented, accompanied by extensive experiments on the dataset to provide reliable benchmark results. The goal is to facilitate future research on multimodal coreference resolution for real-world social media dialogues. <div>
arXiv:2504.14321v1 Announce Type: new 
Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources.To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models</title>
<link>https://arxiv.org/abs/2504.14366</link>
<guid>https://arxiv.org/abs/2504.14366</guid>
<content:encoded><![CDATA[
<div> knowledge distillation, language models, subquadratic architectures, transferability, NLP benchmarks

Summary:
This work explores the transferability of knowledge distillation from a Transformer teacher model to nine subquadratic student architectures. The study evaluates how well different subquadratic models align with the teacher's learned representations and explores the impact of architectural constraints on the distillation process. Intelligent initialization strategies such as matrix mixing and query-key-value copying are also investigated to improve the adaptation process. Empirical results on various NLP benchmarks highlight the trade-offs between efficiency and performance, providing insights into the factors that contribute to successful knowledge transfer to subquadratic architectures. <div>
arXiv:2504.14366v1 Announce Type: new 
Abstract: Knowledge distillation is a widely used technique for compressing large language models (LLMs) by training a smaller student model to mimic a larger teacher model. Typically, both the teacher and student are Transformer-based architectures, leveraging softmax attention for sequence modeling. However, the quadratic complexity of self-attention at inference time remains a significant bottleneck, motivating the exploration of subquadratic alternatives such as structured state-space models (SSMs), linear attention, and recurrent architectures. In this work, we systematically evaluate the transferability of knowledge distillation from a Transformer teacher to nine subquadratic student architectures. Our study aims to determine which subquadratic model best aligns with the teacher's learned representations and how different architectural constraints influence the distillation process. We also investigate the impact of intelligent initialization strategies, including matrix mixing and query-key-value (QKV) copying, on the adaptation process. Our empirical results on multiple NLP benchmarks provide insights into the trade-offs between efficiency and performance, highlighting key factors for successful knowledge transfer to subquadratic architectures.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites</title>
<link>https://arxiv.org/abs/2504.14367</link>
<guid>https://arxiv.org/abs/2504.14367</guid>
<content:encoded><![CDATA[
<div> evolutionary approach, prompt engineering, large language models, context-free grammar, MAP-Elites algorithm <br />
Summary:<br />
- An evolutionary approach combining context-free grammar with the MAP-Elites algorithm is used to explore the prompt space for optimizing large language models (LLMs). 
- The method prioritizes both quality and diversity in generating high-performing and structurally varied prompts while analyzing their alignment with diverse tasks. 
- The study systematically maps the phenotypic space to reveal how structural variations impact LLM performance and offer insights for task-specific and adaptable prompt design. 
- The research evaluated the approach on seven BigBench Lite tasks across multiple LLMs, highlighting the critical interplay between quality and diversity in enhancing the effectiveness and versatility of LLMs. <br /> 
Summary: <div>
arXiv:2504.14367v1 Announce Type: new 
Abstract: Prompt engineering is essential for optimizing large language models (LLMs), yet the link between prompt structures and task performance remains underexplored. This work introduces an evolutionary approach that combines context-free grammar (CFG) with the MAP-Elites algorithm to systematically explore the prompt space. Our method prioritizes quality and diversity, generating high-performing and structurally varied prompts while analyzing their alignment with diverse tasks by varying traits such as the number of examples (shots) and reasoning depth. By systematically mapping the phenotypic space, we reveal how structural variations influence LLM performance, offering actionable insights for task-specific and adaptable prompt design. Evaluated on seven BigBench Lite tasks across multiple LLMs, our results underscore the critical interplay of quality and diversity, advancing the effectiveness and versatility of LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data</title>
<link>https://arxiv.org/abs/2504.14452</link>
<guid>https://arxiv.org/abs/2504.14452</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Paraphrase Preference Optimization, regurgitation, creativity, privacy

Summary:
- Language models (LMs) can inadvertently reproduce segments verbatim from their pretraining data, raising concerns about copyright, plagiarism, privacy, and creativity.
- The Paraphrase Preference Optimization (ParaPO) method fine-tunes LMs to prefer paraphrased versions of memorized segments to reduce unintentional regurgitation while maintaining overall utility.
- ParaPO consistently reduces regurgitation across various datasets, demonstrating its effectiveness in mitigating regurgitation issues.
- Compared to unlearning methods, ParaPO is more successful in reducing regurgitation in different domains beyond its targeted domain.
- When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully balances famous quotation recall and reduction of unintentional regurgitation, showcasing its versatility and effectiveness in controlling LM behavior.

<br /><br />Summary: Language models have the capability to memorize and reproduce content verbatim, leading to concerns regarding copyright and plagiarism. The introduction of Paraphrase Preference Optimization (ParaPO) addresses these issues by training models to prefer paraphrased content, effectively reducing unintentional regurgitation while maintaining overall functionality. ParaPO proves to be more effective than traditional unlearning methods in mitigating regurgitation across diverse datasets. Additionally, when applied to specific models with system prompting, ParaPO successfully balances the recall of famous quotations and minimizes unintentional regurgitation, showcasing its adaptability and efficacy in controlling LM behavior. <div>
arXiv:2504.14452v1 Announce Type: new 
Abstract: Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4).
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge</title>
<link>https://arxiv.org/abs/2504.14462</link>
<guid>https://arxiv.org/abs/2504.14462</guid>
<content:encoded><![CDATA[
<div> Dataset, Commonsense reasoning, Long-tail entities, Knowledge Graph Question Answering, LLM<br />
Summary:<br />
The article discusses the limitations of Large Language Models (LLMs) in handling reasoning errors and hallucinations, especially in tasks involving commonsense reasoning over obscure entities. To address this issue, a new dataset called Commonsense reasoning over Long-Tail entities (CoLoTa) is introduced, comprising 3,300 queries covering various commonsense reasoning skills. The dataset can also be utilized for Knowledge Graph Question Answering (KGQA) as it relies on the Wikidata knowledge graph for support. However, unlike existing KGQA benchmarks, CoLoTa focuses on commonsense reasoning in addition to factoid questions. The experiments conducted using LLM-based KGQA methodologies reveal their inadequacy in answering queries that require commonsense reasoning, highlighting the need for benchmark datasets like CoLoTa to assess LLM capabilities and KGQA methods in handling such tasks effectively. <br />Summary: <div>
arXiv:2504.14462v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has redefined the AI landscape, particularly due to their ability to encode factual and commonsense knowledge, and their outstanding performance in tasks requiring reasoning. Despite these advances, hallucinations and reasoning errors remain a significant barrier to their deployment in high-stakes settings. In this work, we observe that even the most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning errors and hallucinations on tasks requiring commonsense reasoning over obscure, long-tail entities. To investigate this limitation, we present a new dataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that consists of 3,300 queries from question answering and claim verification tasks and covers a diverse range of commonsense reasoning skills. We remark that CoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset since the support of knowledge required to answer its queries is present in the Wikidata knowledge graph. However, as opposed to existing KGQA benchmarks that merely focus on factoid questions, our CoLoTa queries also require commonsense reasoning. Our experiments with strong LLM-based KGQA methodologies indicate their severe inability to answer queries involving commonsense reasoning. Hence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM commonsense reasoning capabilities and their robustness to hallucinations on long-tail entities and (ii) the commonsense reasoning capabilities of KGQA methods.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment</title>
<link>https://arxiv.org/abs/2504.14468</link>
<guid>https://arxiv.org/abs/2504.14468</guid>
<content:encoded><![CDATA[
<div> neural activity, latent representations, multimodal foundation models, brain recordings, sEEG signals 

Summary: 
Interpreting neural activity remains a challenging task bridging neuroscience and artificial intelligence. This study explores using multimodal foundation models to align invasive brain recordings with natural language. The proposed SSENSE framework utilizes contrastive learning to project single-subject sEEG signals into a sentence embedding space derived from a pre-trained CLIP model. By training a neural encoder on spectral representations of sEEG using InfoNCE loss without fine-tuning the text encoder, SSENSE enables sentence-level retrieval directly from brain activity. Evaluation on a naturalistic movie-watching dataset shows promising results, indicating that leveraging general-purpose language representations can effectively aid in neural decoding. <div>
arXiv:2504.14468v1 Announce Type: new 
Abstract: Interpreting neural activity through meaningful latent representations remains a complex and evolving challenge at the intersection of neuroscience and artificial intelligence. We investigate the potential of multimodal foundation models to align invasive brain recordings with natural language. We present SSENSE, a contrastive learning framework that projects single-subject stereo-electroencephalography (sEEG) signals into the sentence embedding space of a frozen CLIP model, enabling sentence-level retrieval directly from brain activity. SSENSE trains a neural encoder on spectral representations of sEEG using InfoNCE loss, without fine-tuning the text encoder. We evaluate our method on time-aligned sEEG and spoken transcripts from a naturalistic movie-watching dataset. Despite limited data, SSENSE achieves promising results, demonstrating that general-purpose language representations can serve as effective priors for neural decoding.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue</title>
<link>https://arxiv.org/abs/2504.14482</link>
<guid>https://arxiv.org/abs/2504.14482</guid>
<content:encoded><![CDATA[
<div> DialogueAgents, speech synthesis, dataset, emotional expressiveness, natural communication<br />
<br />
Summary:<br />
DialogueAgents introduces a novel hybrid agent-based speech synthesis framework that includes script writer, speech synthesizer, and dialogue critic agents collaborating to generate diverse dialogues. This framework improves emotional expressiveness and paralinguistic features of synthesized dialogues through iterative refinement. The MultiTalk dataset, created using DialogueAgents, is a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments confirm the framework's effectiveness and dataset quality. The dataset and code are made available to support future research on advanced speech synthesis models and customized data generation. <div>
arXiv:2504.14482v1 Announce Type: new 
Abstract: Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents -- a script writer, a speech synthesizer, and a dialogue critic -- to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code https://github.com/uirlx/DialogueAgents to facilitate future research on advanced speech synthesis models and customized data generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering</title>
<link>https://arxiv.org/abs/2504.14492</link>
<guid>https://arxiv.org/abs/2504.14492</guid>
<content:encoded><![CDATA[
<div> debiasing, language models, FairSteer, hidden activation space, dynamic activation steering
Summary:
FairSteer is a new framework for debiasing large language models that addresses issues with existing methods. It operates by detecting biased activations, computing debiasing steering vectors (DSVs), and dynamically adjusting activations at inference time. FairSteer is inspired by the linear representation hypothesis, which suggests that fairness-related features can be encoded in separable directions in the hidden activation space. By training a linear classifier to detect bias signatures and using small contrastive prompt pairs to compute intervention directions, FairSteer successfully debiases models without the need for customized prompts or retraining. Evaluation with six LLMs shows that FairSteer outperforms other methods in tasks such as question answering, counterfactual input evaluation, and open-ended text generation. Code for FairSteer will be made available to the public. 
<br /><br />Summary: <div>
arXiv:2504.14492v1 Announce Type: new 
Abstract: Large language models (LLMs) are prone to capturing biases from training corpus, leading to potential negative social impacts. Existing prompt-based debiasing methods exhibit instability due to their sensitivity to prompt changes, while fine-tuning-based techniques incur substantial computational overhead and catastrophic forgetting. In this paper, we propose FairSteer, a novel inference-time debiasing framework without requiring customized prompt design or model retraining. Motivated by the linear representation hypothesis, our preliminary investigation demonstrates that fairness-related features can be encoded into separable directions in the hidden activation space. FairSteer operates in three steps: biased activation detection, debiasing steering vector (DSV) computation, and dynamic activation steering. Specifically, it first trains a lightweight linear classifier to detect bias signatures in activations, and then computes DSVs as intervention directions derived from small contrastive prompt pairs. Subsequently, it performs debiasing by adjusting activations with DSVs in the inference stage. Comprehensive evaluation with six LLMs demonstrates the superiority of FairSteer across question-answering, counterfactual input evaluation and open-ended text generation tasks. Code will be released.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Functional Abstraction of Knowledge Recall in Large Language Models</title>
<link>https://arxiv.org/abs/2504.14496</link>
<guid>https://arxiv.org/abs/2504.14496</guid>
<content:encoded><![CDATA[
<div> Transformer Language Models, Knowledge Recall, Functional Structure, Activation Vectors, Contextual Knowledge Editing <br />
Summary: <br />
This paper explores the knowledge recall mechanism in large language models, proposing that hidden activation spaces align with functional components during recall. Activation vectors map subjects to objects, with relation-related tokens defining the function. An algorithm identifies knowledge-aware activation vectors, and counter-knowledge testing evaluates their effects. Contextual knowledge editing is improved through activation patching, enhancing short-term memory retention for new knowledge recall. <div>
arXiv:2504.14496v1 Announce Type: new 
Abstract: Pre-trained transformer large language models (LLMs) demonstrate strong knowledge recall capabilities. This paper investigates the knowledge recall mechanism in LLMs by abstracting it into a functional structure. We propose that during knowledge recall, the model's hidden activation space implicitly entails a function execution process where specific activation vectors align with functional components (Input argument, Function body, and Return values). Specifically, activation vectors of relation-related tokens define a mapping function from subjects to objects, with subject-related token activations serving as input arguments and object-related token activations as return values. For experimental verification, we first design a patching-based knowledge-scoring algorithm to identify knowledge-aware activation vectors as independent functional components. Then, we conduct counter-knowledge testing to examine the independent functional effects of each component on knowledge recall outcomes. From this functional perspective, we improve the contextual knowledge editing approach augmented by activation patching. By rewriting incoherent activations in context, we enable improved short-term memory retention for new knowledge prompting.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causality for Natural Language Processing</title>
<link>https://arxiv.org/abs/2504.14530</link>
<guid>https://arxiv.org/abs/2504.14530</guid>
<content:encoded><![CDATA[
<div> Keywords: causal reasoning, large language models, natural language processing, computational social science, scientific impact 

Summary: 
This thesis explores the role of causal reasoning in large language models (LLMs) and its implications for artificial intelligence and natural language processing (NLP). It investigates how LLMs perform causal inference tasks, identifies the mechanisms driving their performance, and examines the impact of causal and anticausal learning on NLP tasks. The thesis also examines the use of causal reasoning in computational social science, focusing on political decision-making and scientific impact assessment through citations. By introducing new datasets, benchmark tasks, and methodological frameworks, the study highlights the challenges and opportunities for enhancing the causal reasoning abilities of LLMs. This research lays the groundwork for future studies in this rapidly evolving field. 

<br /><br />Summary: <div>
arXiv:2504.14530v1 Announce Type: new 
Abstract: Causal reasoning is a cornerstone of human intelligence and a critical capability for artificial systems aiming to achieve advanced understanding and decision-making. This thesis delves into various dimensions of causal reasoning and understanding in large language models (LLMs). It encompasses a series of studies that explore the causal inference skills of LLMs, the mechanisms behind their performance, and the implications of causal and anticausal learning for natural language processing (NLP) tasks. Additionally, it investigates the application of causal reasoning in text-based computational social science, specifically focusing on political decision-making and the evaluation of scientific impact through citations. Through novel datasets, benchmark tasks, and methodological frameworks, this work identifies key challenges and opportunities to improve the causal capabilities of LLMs, providing a comprehensive foundation for future research in this evolving field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation</title>
<link>https://arxiv.org/abs/2504.14538</link>
<guid>https://arxiv.org/abs/2504.14538</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, social simulation, multi-agent systems, fictional worlds, story generation

Summary:
BookWorld is a new system that allows for the construction and simulation of multi-agent societies based on established fictional worlds and characters. It incorporates real-world complexities such as diverse characters, dynamic interactions, and geographical constraints. The system offers various applications, including story generation, interactive games, and social simulation. Through experiments, BookWorld has shown to produce creative and high-quality stories that stay faithful to the source material, surpassing previous methods with a 75.36% success rate. The project code can be accessed on the project page provided. <div>
arXiv:2504.14538v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld's design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>a1: Steep Test-time Scaling Law via Environment Augmented Generation</title>
<link>https://arxiv.org/abs/2504.14597</link>
<guid>https://arxiv.org/abs/2504.14597</guid>
<content:encoded><![CDATA[
<div> environmental feedback, reasoning, branch exploration, machine learning, performance improvement

Summary:
Environment Augmented Generation (EAG) framework enhances Large Language Models (LLMs) reasoning by providing real-time environmental feedback for each step, dynamic branch exploration for error correction, and experience-based learning. EAG enables deliberate backtracking and strategic replanning by integrating execution feedback with branching exploration. The a1-32B model achieves state-of-the-art performance on benchmarks, matching larger models like o1 in competition mathematics. EAG's scaling pattern shows that initial investment in environment interaction yields long-term performance dividends, especially for complex tasks. Theoretical framework of EAG demonstrates a new paradigm for reliable machine reasoning, particularly for multi-step calculation and logical verification tasks. <div>
arXiv:2504.14597v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made remarkable breakthroughs in reasoning, yet continue to struggle with hallucinations, logical errors, and inability to self-correct during complex multi-step tasks. Current approaches like chain-of-thought prompting offer limited reasoning capabilities that fail when precise step validation is required. We propose Environment Augmented Generation (EAG), a framework that enhances LLM reasoning through: (1) real-time environmental feedback validating each reasoning step, (2) dynamic branch exploration for investigating alternative solution paths when faced with errors, and (3) experience-based learning from successful reasoning trajectories. Unlike existing methods, EAG enables deliberate backtracking and strategic replanning through tight integration of execution feedback with branching exploration. Our a1-32B model achieves state-of-the-art performance among similar-sized models across all benchmarks, matching larger models like o1 on competition mathematics while outperforming comparable models by up to 24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern: initial token investment in environment interaction yields substantial long-term performance dividends, with advantages amplifying proportionally to task complexity. EAG's theoretical framework demonstrates how environment interactivity and systematic branch exploration together establish a new paradigm for reliable machine reasoning, particularly for problems requiring precise multi-step calculation and logical verification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations</title>
<link>https://arxiv.org/abs/2504.14619</link>
<guid>https://arxiv.org/abs/2504.14619</guid>
<content:encoded><![CDATA[
<div> Keywords: Language technologies, neural machine translation, translation analytics, automatic evaluation metrics, professional environment

Summary:
The paper explores the opportunities for individual translators and language service providers created by recent advancements in language technologies. These include neural machine translation systems, large language models, and computer-assisted translation tools. The integration of these tools into workflows allows for not only translation but also quality evaluation, error spotting, glossary generation, and domain-specific adaptation. The paper introduces the concept of Translation Analytics, which provides evaluation techniques for smaller-scale users. It presents a framework for adapting automatic evaluation metrics to freelancers' needs and demonstrates their potential using a trilingual corpus from a medical domain project. Statistical analysis shows a correlation between human evaluations and automatic scores. The findings highlight the importance of embracing emerging technologies to succeed in the evolving professional landscape.

<br /><br />Summary: <div>
arXiv:2504.14619v1 Announce Type: new 
Abstract: This is the first in a series of papers exploring the rapidly expanding new opportunities arising from recent progress in language technologies for individual translators and language service providers with modest resources. The advent of advanced neural machine translation systems, large language models, and their integration into workflows via computer-assisted translation tools and translation management systems have reshaped the translation landscape. These advancements enable not only translation but also quality evaluation, error spotting, glossary generation, and adaptation to domain-specific needs, creating new technical opportunities for freelancers. In this series, we aim to empower translators with actionable methods to harness these advancements. Our approach emphasizes Translation Analytics, a suite of evaluation techniques traditionally reserved for large-scale industry applications but now becoming increasingly available for smaller-scale users. This first paper introduces a practical framework for adapting automatic evaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers' needs. We illustrate the potential of these metrics using a trilingual corpus derived from a real-world project in the medical domain and provide statistical analysis correlating human evaluations with automatic scores. Our findings emphasize the importance of proactive engagement with emerging technologies to not only adapt but thrive in the evolving professional environment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models</title>
<link>https://arxiv.org/abs/2504.14620</link>
<guid>https://arxiv.org/abs/2504.14620</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific paper innovation, HSPIM framework, large language models, section classification, question-answering, novelty scoring

Summary:
HSPIM is a novel framework for measuring scientific paper innovation that addresses the shortcomings of existing content-based methods. It utilizes large language models (LLMs) to segment the text into sections, classify sections, perform question-answering, and calculate novelty scores. By using a hierarchical approach and zero-shot LLM prompting, HSPIM provides a more comprehensive assessment of innovation at both the section and paper levels. The framework also incorporates a two-layer question structure and a genetic algorithm for optimizing question-prompt combinations. Experimental results demonstrate that HSPIM outperforms baseline methods in effectiveness, generalization, and interpretability. Overall, HSPIM offers a promising solution for evaluating the innovative impact of scientific papers in a more inclusive and accurate manner. 

<br /><br />Summary: <div>
arXiv:2504.14620v1 Announce Type: new 
Abstract: Measuring scientific paper innovation is both important and challenging. Existing content-based methods often overlook the full-paper context, fail to capture the full scope of innovation, and lack generalization. We propose HSPIM, a hierarchical and training-free framework based on large language models (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess innovation. We segment the text by section titles and use zero-shot LLM prompting to implement section classification, question-answering (QA) augmentation, and weighted novelty scoring. The generated QA pair focuses on section-level innovation and serves as additional context to improve the LLM scoring. For each chunk, the LLM outputs a novelty score and a confidence score. We use confidence scores as weights to aggregate novelty scores into a paper-level innovation score. To further improve performance, we propose a two-layer question structure consisting of common and section-specific questions, and apply a genetic algorithm to optimize the question-prompt combinations. Comprehensive experiments on scientific conference paper datasets show that HSPIM outperforms baseline methods in effectiveness, generalization, and interpretability.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish</title>
<link>https://arxiv.org/abs/2504.14630</link>
<guid>https://arxiv.org/abs/2504.14630</guid>
<content:encoded><![CDATA[
<div> Keyword: Automatic Text Summarization, Kurdish, Dataset, Language Model, ROUGE metrics

Summary:
This study focuses on developing resources for Automatic Text Summarization (ATS) in Kurdish, a language lacking in such resources. The researchers created a dataset of 231 scientific papers in Sorani Kurdish and built a language model using Sentence Weighting and TF-IDF algorithms. Two experiments were conducted, varying in whether conclusions were included, with an average word count of around 5,400. Manual and automatic evaluations using ROUGE metrics showed accuracy up to 19.58%. Six experts also conducted manual evaluations, with results varying by document. This research aims to contribute valuable resources for Kurdish NLP researchers to enhance ATS and related fields. 

Summary: <div>
arXiv:2504.14630v1 Announce Type: new 
Abstract: Extracting concise information from scientific documents aids learners, researchers, and practitioners. Automatic Text Summarization (ATS), a key Natural Language Processing (NLP) application, automates this process. While ATS methods exist for many languages, Kurdish remains underdeveloped due to limited resources. This study develops a dataset and language model based on 231 scientific papers in Sorani Kurdish, collected from four academic departments in two universities in the Kurdistan Region of Iraq (KRI), averaging 26 pages per document. Using Sentence Weighting and Term Frequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were conducted, differing in whether the conclusions were included. The average word count was 5,492.3 in the first experiment and 5,266.96 in the second. Results were evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L metrics, with the best accuracy reaching 19.58%. Six experts conducted manual evaluations using three criteria, with results varying by document. This research provides valuable resources for Kurdish NLP researchers to advance ATS and related fields.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance</title>
<link>https://arxiv.org/abs/2504.14633</link>
<guid>https://arxiv.org/abs/2504.14633</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial event entity extraction, Large Language Models, Parameter-Efficient Fine-Tuning, structured output generation, state-of-the-art performance

Summary: <br />
- Financial event entity extraction is a crucial task for analyzing market dynamics and building financial knowledge graphs.
- Traditional approaches using sequence labeling models face challenges with long-range dependencies and extracting multiple overlapping entities.
- A novel method is proposed that reframes financial event entity extraction as a text-to-structured-output generation task using Large Language Models (LLMs) and Parameter-Efficient Fine-Tuning (PEFT).
- The approach achieves a new state-of-the-art F1 score on the CCKS 2019 Financial Event Entity Extraction dataset, outperforming previous methods.
- Experimental results and human evaluation show the generative LLM method is more effective in handling financial text nuances and extracting high-quality entities. 

Summary: <div>
arXiv:2504.14633v1 Announce Type: new 
Abstract: Financial event entity extraction is a crucial task for analyzing market dynamics and building financial knowledge graphs, yet it presents significant challenges due to the specialized language and complex structures in financial texts. Traditional approaches often rely on sequence labeling models, which can struggle with long-range dependencies and the inherent complexity of extracting multiple, potentially overlapping entities. Motivated by the advanced language understanding and generative capabilities of Large Language Models (LLMs), we propose a novel method that reframes financial event entity extraction as a text-to-structured-output generation task. Our approach involves fine-tuning a pre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly generate a structured representation, such as a JSON object, containing the extracted entities and their precise character spans from the input text. We evaluate our method on the challenging CCKS 2019 Financial Event Entity Extraction dataset, comparing its performance against strong sequence labeling baselines, including SEBERTNets and sebertNets. Experimental results demonstrate that our generative LLM method achieves a new state-of-the-art F1 score on this benchmark, significantly outperforming previous methods. Through detailed quantitative analysis across event types, entity types, and instance complexity, as well as human evaluation, we show that our approach is more effective at handling the nuances of financial text and extracting high-quality entities. This work validates the potential of applying generative LLMs directly to complex, domain-specific information extraction tasks requiring structured output.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs</title>
<link>https://arxiv.org/abs/2504.14657</link>
<guid>https://arxiv.org/abs/2504.14657</guid>
<content:encoded><![CDATA[
<div> Keywords: Synthetic Electronic Health Records, Large Language Models, Privacy Preservation, Generalization, Healthcare<br />
<br />
Summary: 
The study focuses on the use of Large Language Models (LLMs) for generating synthetic Electronic Health Records (EHRs) and its implications in healthcare. LLMs offer benefits such as precise control over data schema and improved fairness in data representation. However, a significant challenge lies in ensuring that synthetic health records can generalize across different hospital settings. The research evaluates the current state of commercial LLMs, finding that while they can reliably generate synthetic data for smaller feature sets, they struggle with realistic distributions and correlations as the data dimensionality increases. This limitation hinders their ability to generalize effectively across diverse hospitals, highlighting the need for further improvements in LLMs for generating synthetic EHRs in healthcare. <br /><br /> <div>
arXiv:2504.14657v1 Announce Type: new 
Abstract: Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare. Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy. Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains. However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field. In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short. Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data</title>
<link>https://arxiv.org/abs/2504.14669</link>
<guid>https://arxiv.org/abs/2504.14669</guid>
<content:encoded><![CDATA[
<div> self-play, multilingual machine translation, large language models, Genetic Monte-Carlo Tree Search, supervised fine-tuning
Summary:<br /><br />The article introduces TRANS-ZERO, a self-play framework for multilingual machine translation that utilizes only monolingual data and the inherent multilingual knowledge of Large Language Models (LLMs). By combining Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, TRANS-ZERO achieves strong translation performance that rivals supervised methods, even surpassing them in non-English translation directions. The framework demonstrates the ability to match the performance of models trained on large-scale parallel data while enhancing translation quality through the exploration of semantically consistent candidates via G-MCTS. This approach addresses challenges such as data scarcity for low-resource languages and catastrophic forgetting in multilingual MT, providing a robust alternative to traditional supervised fine-tuning methods. <div>
arXiv:2504.14669v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models</title>
<link>https://arxiv.org/abs/2504.14690</link>
<guid>https://arxiv.org/abs/2504.14690</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Language Models, Persian, Benchmark, FarsEval-PKBETS <br />
Summary: 
Research on evaluating large language models in Persian has been limited compared to resource-rich languages like English. FarsEval-PKBETS, a subset of the FarsEval project, introduces a benchmark of 4000 questions in various formats covering diverse domains. Three models were evaluated on this benchmark, with an average accuracy of below 50%, indicating current language models struggle with Persian language tasks. The benchmark incorporates cultural and linguistic considerations specific to Persian and Iran. <div>
arXiv:2504.14690v1 Announce Type: new 
Abstract: Research on evaluating and analyzing large language models (LLMs) has been extensive for resource-rich languages such as English, yet their performance in languages such as Persian has received considerably less attention. This paper introduces FarsEval-PKBETS benchmark, a subset of FarsEval project for evaluating large language models in Persian. This benchmark consists of 4000 questions and answers in various formats, including multiple choice, short answer and descriptive responses. It covers a wide range of domains and tasks,including medicine, law, religion, Persian language, encyclopedic knowledge, human preferences, social knowledge, ethics and bias, text generation, and respecting others' rights. This bechmark incorporates linguistics, cultural, and local considerations relevant to the Persian language and Iran. To ensure the questions are challenging for current LLMs, three models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this benchmark. Their average accuracy was below 50%, meaning they provided fully correct answers to fewer than half of the questions. These results indicate that current language models are still far from being able to solve this benchmark
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding</title>
<link>https://arxiv.org/abs/2504.14692</link>
<guid>https://arxiv.org/abs/2504.14692</guid>
<content:encoded><![CDATA[
<div> Keywords: medical vision-language models, multimodal understanding, rotary position-adaptive encoder, token pruning mechanism, state-of-the-art performance

Summary:
OmniV-Med introduces a unified framework for multimodal medical understanding by creating a comprehensive dataset and developing innovative techniques. The OmniV-Med-Instruct dataset includes a wide range of medical modalities and tasks. A rotary position-adaptive encoder is designed to process various image resolutions and types within a single architecture, moving away from traditional modality-specific encoders. Additionally, a medical-aware token pruning mechanism helps reduce visual tokens in volumetric data and medical videos without sacrificing performance. Empirical evaluations show that OmniV-Med achieves state-of-the-art results on multiple medical imaging and video tasks. Even a lightweight variant of the model, OmniV-Med-1.5B, maintains high performance with fewer resources, making it efficient for training and inference. The dataset, code, and models will be made publicly available for further research and application. 

Summary: <div>
arXiv:2504.14692v1 Announce Type: new 
Abstract: The practical deployment of medical vision-language models (Med-VLMs) necessitates seamless integration of textual data with diverse visual modalities, including 2D/3D images and videos, yet existing models typically employ separate encoders for different modalities. To address this limitation, we present OmniV-Med, a unified framework for multimodal medical understanding. Our technical contributions are threefold: First, we construct OmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K instructional samples spanning 14 medical image modalities and 11 clinical tasks. Second, we devise a rotary position-adaptive encoder that processes multi-resolution 2D/3D images and videos within a unified architecture, diverging from conventional modality-specific encoders. Third, we introduce a medical-aware token pruning mechanism that exploits spatial-temporal redundancy in volumetric data (e.g., consecutive CT slices) and medical videos, effectively reducing 60\% of visual tokens without performance degradation. Empirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art performance on 7 benchmarks spanning 2D/3D medical imaging and video understanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains comparable performance while requiring only 8 RTX3090 GPUs for training and supporting efficient long-video inference. Data, code and model will be released.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives</title>
<link>https://arxiv.org/abs/2504.14707</link>
<guid>https://arxiv.org/abs/2504.14707</guid>
<content:encoded><![CDATA[
<div> BERTopic, LDA, KMeans, Belgian Dutch daily narratives, contextual embeddings <br />
Summary: This study compares BERTopic, LDA, and KMeans for modeling Belgian Dutch daily narratives. LDA performs well on automated metrics but shows semantically irrelevant co-occurrences, indicating limitations. BERTopic, leveraging contextual embeddings, captures culturally resonant themes, highlighting the importance of hybrid evaluation frameworks in morphologically rich languages. KMeans, contrary to prior studies, performs less coherently in personal narratives. The study underscores the necessity of robust generalization in NLP models, particularly in underrepresented linguistic contexts. <br /> <div>
arXiv:2504.14707v1 Announce Type: new 
Abstract: This study explores BERTopic's potential for modeling open-ended Belgian Dutch daily narratives, contrasting its performance with Latent Dirichlet Allocation (LDA) and KMeans. Although LDA scores well on certain automated metrics, human evaluations reveal semantically irrelevant co-occurrences, highlighting the limitations of purely statistic-based methods. In contrast, BERTopic's reliance on contextual embeddings yields culturally resonant themes, underscoring the importance of hybrid evaluation frameworks that account for morphologically rich languages. KMeans performed less coherently than prior research suggested, pointing to the unique challenges posed by personal narratives. Our findings emphasize the need for robust generalization in NLP models, especially in underrepresented linguistic contexts.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines</title>
<link>https://arxiv.org/abs/2504.14738</link>
<guid>https://arxiv.org/abs/2504.14738</guid>
<content:encoded><![CDATA[
<div> dataset, language models, assertions, pipeline, reliability  
Summary:  
The paper introduces PROMPTEVALS, a dataset comprising 2087 language model pipeline prompts with 12623 corresponding assertion criteria, aimed at improving reliability in large language model applications. Determining the right set of assertions for tasks is crucial in enhancing model performance. The dataset, larger than previous collections, serves as a benchmark for evaluating closed- and open-source models in generating relevant assertions. The fine-tuned Mistral and Llama 3 models displayed better performance compared to GPT-4o, with a 20.93% average improvement, offering both reduced latency and enhanced performance. The dataset is expected to drive further research in the areas of language model reliability, alignment, and prompt engineering.  
<br /><br />Summary: <div>
arXiv:2504.14738v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in specialized production data processing pipelines across diverse domains -- such as finance, marketing, and e-commerce. However, when running them in production across many inputs, they often fail to follow instructions or meet developer expectations. To improve reliability in these applications, creating assertions or guardrails for LLM outputs to run alongside the pipelines is essential. Yet, determining the right set of assertions that capture developer requirements for a task is challenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM pipeline prompts with 12623 corresponding assertion criteria, sourced from developers using our open-source LLM pipeline tools. This dataset is 5x larger than previous collections. Using a hold-out test split of PROMPTEVALS as a benchmark, we evaluated closed- and open-source models in generating relevant assertions. Notably, our fine-tuned Mistral and Llama 3 models outperform GPT-4o by 20.93% on average, offering both reduced latency and improved performance. We believe our dataset can spur further research in LLM reliability, alignment, and prompt engineering.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings</title>
<link>https://arxiv.org/abs/2504.14766</link>
<guid>https://arxiv.org/abs/2504.14766</guid>
<content:encoded><![CDATA[
<div> Keywords: neural embeddings, BERT, linguistic properties, interpretability, language models

Summary: 
The paper introduces a framework for understanding neural embeddings, like those in BERT, by uncovering the dimensions that encode distinct linguistic properties (LPs). It presents the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, focusing on key features such as synonymy, negation, tense, and quantity. Using various analysis methods, the study identifies the most influential dimensions for each LP, introducing the Embedding Dimension Impact (EDI) score to quantify dimension relevance. Results reveal robust encoding of properties like negation and polarity in specific dimensions, while synonymy shows more complex patterns. The findings provide insights into embedding interpretability, guiding the development of transparent and optimized language models. This has implications for addressing model bias and ensuring responsible AI system deployment.

<br /><br />Summary: <div>
arXiv:2504.14766v1 Announce Type: new 
Abstract: Understanding the inner workings of neural embeddings, particularly in models such as BERT, remains a challenge because of their high-dimensional and opaque nature. This paper proposes a framework for uncovering the specific dimensions of vector embeddings that encode distinct linguistic properties (LPs). We introduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which isolates ten key linguistic features such as synonymy, negation, tense, and quantity. Using this dataset, we analyze BERT embeddings with various methods, including the Wilcoxon signed-rank test, mutual information, and recursive feature elimination, to identify the most influential dimensions for each LP. We introduce a new metric, the Embedding Dimension Impact (EDI) score, which quantifies the relevance of each embedding dimension to a LP. Our findings show that certain properties, such as negation and polarity, are robustly encoded in specific dimensions, while others, like synonymy, exhibit more complex patterns. This study provides insights into the interpretability of embeddings, which can guide the development of more transparent and optimized language models, with implications for model bias mitigation and the responsible deployment of AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions</title>
<link>https://arxiv.org/abs/2504.14772</link>
<guid>https://arxiv.org/abs/2504.14772</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Dataset Distillation, Large Language Models, Compression, Scalability

Summary:
This survey explores the strategies of Knowledge Distillation (KD) and Dataset Distillation (DD) for compressing Large Language Models (LLMs) while maintaining their reasoning abilities and linguistic diversity. It covers methodologies in KD such as task-specific alignment and multi-teacher frameworks, as well as DD techniques like optimization-based gradient matching and generative synthesis. The integration of KD and DD is discussed to enhance compression strategies. Applications in healthcare and education are highlighted for efficient deployment. Challenges include preserving reasoning and diversity, adapting to evolving models and datasets, and establishing evaluation protocols. The survey aims to guide the development of sustainable LLMs by combining KD and DD principles.<br /><br />Summary: <div>
arXiv:2504.14772v1 Announce Type: new 
Abstract: The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands. This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity. We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis. Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies. Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities. We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance. Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols. By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends</title>
<link>https://arxiv.org/abs/2504.14804</link>
<guid>https://arxiv.org/abs/2504.14804</guid>
<content:encoded><![CDATA[
<div> Evaluation, document-level translation, automatic evaluation metrics, large language models, future trends <br />
<br />
Summary: 
The paper discusses the progress in machine translation, particularly in document-level translation facilitated by large language models (LLMs). Evaluation of document-level translation is crucial, with automatic evaluation metrics playing a key role. Current evaluation schemes and metrics are analyzed, including those with and without reference texts, traditional metrics, and LLM-based metrics. Challenges in current evaluation methods are identified, such as lack of reference diversity and dependency on sentence-level alignment. Future trends in evaluation methods are explored, including the need for user-friendly evaluation methods and robust LLM-based methods. Proposed research directions include reducing dependency on sentence-level information, introducing multi-level evaluation approaches, and training models for machine translation evaluation. The study aims to provide a comprehensive analysis of automatic evaluation for document-level translation and suggests potential advancements in the field. <div>
arXiv:2504.14804v1 Announce Type: new 
Abstract: With the rapid development of deep learning technologies, the field of machine translation has witnessed significant progress, especially with the advent of large language models (LLMs) that have greatly propelled the advancement of document-level translation. However, accurately evaluating the quality of document-level translation remains an urgent issue. This paper first introduces the development status of document-level translation and the importance of evaluation, highlighting the crucial role of automatic evaluation metrics in reflecting translation quality and guiding the improvement of translation systems. It then provides a detailed analysis of the current state of automatic evaluation schemes and metrics, including evaluation methods with and without reference texts, as well as traditional metrics, Model-based metrics and LLM-based metrics. Subsequently, the paper explores the challenges faced by current evaluation methods, such as the lack of reference diversity, dependence on sentence-level alignment information, and the bias, inaccuracy, and lack of interpretability of the LLM-as-a-judge method. Finally, the paper looks ahead to the future trends in evaluation methods, including the development of more user-friendly document-level evaluation methods and more robust LLM-as-a-judge methods, and proposes possible research directions, such as reducing the dependency on sentence-level information, introducing multi-level and multi-granular evaluation approaches, and training models specifically for machine translation evaluation. This study aims to provide a comprehensive analysis of automatic evaluation for document-level translation and offer insights into future developments.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Self-improving Token Embeddings</title>
<link>https://arxiv.org/abs/2504.14808</link>
<guid>https://arxiv.org/abs/2504.14808</guid>
<content:encoded><![CDATA[
<div> Keywords: token embeddings, out-of-vocabulary problem, corpus exploration, conceptual search, word sense disambiguation

Summary:
This article presents a new method for refining pre-trained token embeddings by incorporating neighboring tokens in text corpora. It continuously updates the representation of each token, addressing the out-of-vocabulary problem. Operating independently of large language models, the approach allows for versatile applications including corpus exploration, conceptual search, and word sense disambiguation. It is designed for topically homogeneous corpora, resulting in more meaningful embeddings. The methodology is applied to analyze storm events and their impacts using narratives from the NOAA Storm Events database, improving the representation of storm-related terms over time. This provides insights into the evolving nature of disaster narratives and showcases the effectiveness of the method in enhancing token representations within specific domains. 

<br /><br />Summary: <div>
arXiv:2504.14808v1 Announce Type: new 
Abstract: This article introduces a novel and fast method for refining pre-trained static word or, more generally, token embeddings. By incorporating the embeddings of neighboring tokens in text corpora, it continuously updates the representation of each token, including those without pre-assigned embeddings. This approach effectively addresses the out-of-vocabulary problem, too. Operating independently of large language models and shallow neural networks, it enables versatile applications such as corpus exploration, conceptual search, and word sense disambiguation. The method is designed to enhance token representations within topically homogeneous corpora, where the vocabulary is restricted to a specific domain, resulting in more meaningful embeddings compared to general-purpose pre-trained vectors. As an example, the methodology is applied to explore storm events and their impacts on infrastructure and communities using narratives from a subset of the NOAA Storm Events database. The article also demonstrates how the approach improves the representation of storm-related terms over time, providing valuable insights into the evolving nature of disaster narratives.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation</title>
<link>https://arxiv.org/abs/2504.14856</link>
<guid>https://arxiv.org/abs/2504.14856</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, citation generation, trustworthiness, internal knowledge, retrieval quality 

Summary: 
RAEL, a paradigm introduced in this work, focuses on Context-Prior Augmented Citation Generation task, aiming to generate trustworthy references by utilizing both external and internal knowledge. The INTRALIGN method, incorporating data generation and alignment algorithms, outperforms other baselines in cross-scenario performance. Experimental results highlight the impact of retrieval quality, question types, and model knowledge on citation trustworthiness. The study addresses the opacity in how language models utilize internal knowledge and aims to improve the reliability of generated answers through a comprehensive evaluation framework. <div>
arXiv:2504.14856v1 Announce Type: new 
Abstract: While hallucinations of large language models could been alleviated through retrieval-augmented generation and citation generation, how the model utilizes internal knowledge is still opaque, and the trustworthiness of its generated answers remains questionable. In this work, we introduce Context-Prior Augmented Citation Generation task, requiring models to generate citations considering both external and internal knowledge while providing trustworthy references, with 5 evaluation metrics focusing on 3 aspects: answer helpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the paradigm for our task, and also design INTRALIGN, an integrated method containing customary data generation and an alignment algorithm. Our experimental results show that our method achieves a better cross-scenario performance with regard to other baselines. Our extended experiments further reveal that retrieval quality, question types, and model knowledge have considerable influence on the trustworthiness in citation generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Fingerprints of Large Language Models</title>
<link>https://arxiv.org/abs/2504.14871</link>
<guid>https://arxiv.org/abs/2504.14871</guid>
<content:encoded><![CDATA[
<div> biases, language models, training data, natural fingerprints, unintended characteristics

Summary: 
Large language models (LLMs) often produce biased outputs, ranging from unfair responses to subtle patterns that can identify the model. This study investigates factors leading to identifiable characteristics in LLMs. Despite being trained on the same data, LLMs can still be distinguished based on their generated text, resulting in natural fingerprints. The study shows that differences in training conditions, such as parameter sizes, optimization settings, and random seeds, contribute to these unintended characteristics. Understanding natural fingerprints can provide insights into the origins of bias in LLMs and help improve control over their behavior. <div>
arXiv:2504.14871v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit biases -- systematic deviations from expected norms -- in their outputs. These range from overt issues, such as unfair responses, to subtler patterns that can reveal which model produced them. We investigate the factors that give rise to identifiable characteristics in LLMs. Since LLMs model training data distribution, it is reasonable that differences in training data naturally lead to the characteristics. However, our findings reveal that even when LLMs are trained on the exact same data, it is still possible to distinguish the source model based on its generated text. We refer to these unintended, distinctive characteristics as natural fingerprints. By systematically controlling training conditions, we show that the natural fingerprints can emerge from subtle differences in the training process, such as parameter sizes, optimization settings, and even random seeds. We believe that understanding natural fingerprints offers new insights into the origins of unintended bias and ways for improving control over LLM behavior.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2504.14891</link>
<guid>https://arxiv.org/abs/2504.14891</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, evaluation methods, RAG-specific datasets, meta-analysis

<br />
Summary:
This paper presents a comprehensive survey of Retrieval-Augmented Generation (RAG) evaluation methods and frameworks in the context of Large Language Models (LLMs). The unique challenges of evaluating RAG systems, which combine retrieval and generation components, are addressed, as well as their reliance on dynamic knowledge sources. The survey includes traditional and emerging evaluation approaches for system performance, factual accuracy, safety, and computational efficiency in the LLM era. RAG-specific datasets and evaluation frameworks are compiled and categorized, with a meta-analysis of evaluation practices in significant RAG research conducted. This work aims to bridge traditional evaluation methods with those driven by LLMs, serving as a vital resource for the advancement of RAG development.

<br /><br />Summary: <div>
arXiv:2504.14891v1 Announce Type: new 
Abstract: Recent advancements in Retrieval-Augmented Generation (RAG) have revolutionized natural language processing by integrating Large Language Models (LLMs) with external information retrieval, enabling accurate, up-to-date, and verifiable text generation across diverse applications. However, evaluating RAG systems presents unique challenges due to their hybrid architecture that combines retrieval and generation components, as well as their dependence on dynamic knowledge sources in the LLM era. In response, this paper provides a comprehensive survey of RAG evaluation methods and frameworks, systematically reviewing traditional and emerging evaluation approaches, for system performance, factual accuracy, safety, and computational efficiency in the LLM era. We also compile and categorize the RAG-specific datasets and evaluation frameworks, conducting a meta-analysis of evaluation practices in high-impact RAG research. To the best of our knowledge, this work represents the most comprehensive survey for RAG evaluation, bridging traditional and LLM-driven methods, and serves as a critical resource for advancing RAG development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs</title>
<link>https://arxiv.org/abs/2504.14905</link>
<guid>https://arxiv.org/abs/2504.14905</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, automated claim verification, large language models, conflicting reasoning, evidence retrieval

Summary: 

The article proposes a Conflicting Reasoning Approach for explainable claim Verification, called CRAVE, to address the challenges of verifying complex claims in the age of digital misinformation. The framework consists of three modules: Ambiguity Elimination enhanced Evidence Retrieval, Conflicting Perspective Reasoning and Preliminary Judgment using large language models (LLMs), and Small Language Model (SLM) based Judge. CRAVE utilizes LLMs to reason conflicting rationales based on evidence from external sources, across dimensions such as direct evidence, semantic relationships, linguistic patterns, and logical reasoning. The methodology enables the model to capture subtle inconsistencies in complex claims and improve both accuracy and transparency in claim verification. Experimental results on public datasets show that CRAVE outperforms state-of-the-art methods in finding relevant evidence and explaining predictions, showcasing its effectiveness in automated claim verification. The source code is available on GitHub for further exploration and development. 

<br /><br />Summary: <div>
arXiv:2504.14905v1 Announce Type: new 
Abstract: The rapid spread of misinformation, driven by digital media and AI-generated content, has made automatic claim verification essential. Traditional methods, which depend on expert-annotated evidence, are labor-intensive and not scalable. Although recent automated systems have improved, they still struggle with complex claims that require nuanced reasoning. To address this, we propose CRAVE, a Conflicting Reasoning Approach for explainable claim VErification, that verify the complex claims based on the conflicting rationales reasoned by large language models (LLMs). Specifically, CRAVE introduces a three-module framework. Ambiguity Elimination enchanced Evidence Retrieval module performs ambiguity elimination and entity-based search to gather relevant evidence related to claim verification from external sources like Wikipedia. Conflicting Perspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to reason rationales with conflicting stances about claim verification from retrieved evidence across four dimensions, i.e., direct evidence, semantic relationships, linguistic patterns, and logical reasoning and make a preliminary judgment. Finally, Small Language Model (SLM) based Judge module is fine-tuned to make use of preliminary judgment from LLMs to assess the confidence of the conflicting rationales and make a final authenticity judgment. This methodology allows CRAVE to capture subtle inconsistencies in complex claims, improving both the accuracy and transparency of claim verification. Extensive experiments on two public claim verification datasets demonstrate that our CRAVE model achieves much better performance than state-of-the-art methods and exhibits a superior capacity for finding relevant evidence and explaining the model predictions. The code is provided at https://github.com/8zym/CRAVE.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues</title>
<link>https://arxiv.org/abs/2504.14963</link>
<guid>https://arxiv.org/abs/2504.14963</guid>
<content:encoded><![CDATA[
<div> Keywords: speaker identification, text-based, fuzzy fingerprints, pre-trained models, context-aware modeling

Summary: 
This article explores the use of fuzzy fingerprints from large pre-trained models to enhance text-based speaker identification. By incorporating speaker-specific tokens and context-aware modeling, the research demonstrates that considering conversational context can significantly improve accuracy, achieving 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. The study also shows that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering better interpretability. Furthermore, the researchers analyze ambiguous utterances and suggest a mechanism to identify speaker-agnostic lines. These findings shed light on the challenges and provide valuable insights for enhancing text-based speaker identification methodologies.<br /><br />Summary: <div>
arXiv:2504.14963v1 Announce Type: new 
Abstract: Speaker identification using voice recordings leverages unique acoustic features, but this approach fails when only textual data is available. Few approaches have attempted to tackle the problem of identifying speakers solely from text, and the existing ones have primarily relied on traditional methods. In this work, we explore the use of fuzzy fingerprints from large pre-trained models to improve text-based speaker identification. We integrate speaker-specific tokens and context-aware modeling, demonstrating that conversational context significantly boosts accuracy, reaching 70.6% on the Friends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show that fuzzy fingerprints can approximate full fine-tuning performance with fewer hidden units, offering improved interpretability. Finally, we analyze ambiguous utterances and propose a mechanism to detect speaker-agnostic lines. Our findings highlight key challenges and provide insights for future improvements in text-based speaker identification.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)</title>
<link>https://arxiv.org/abs/2504.14969</link>
<guid>https://arxiv.org/abs/2504.14969</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese topic constructions, island constraints, Mandarin syntax, experimental design

Summary:
Large language models (LLMs) are being evaluated for their performance on Chinese topic constructions and sensitivity to island constraints in this proposed framework. Inspired by previous work, an experimental design is outlined to test LLMs' knowledge of Mandarin syntax. While no experiments have been conducted yet, this proposal sets the stage for future studies in this area. Feedback on the methodology is encouraged to further refine and improve the framework for evaluating LLMs' grammatical understanding of Chinese syntax. This paper aims to lay the groundwork for future research on this topic and seeks input from the academic community to enhance the experimental design and methodology for evaluating LLMs on Chinese language tasks. 

<br /><br />Summary: 
- Proposed framework for evaluating LLMs on Chinese topic constructions
- Focus on sensitivity to island constraints in Mandarin syntax
- Experimental design outlined for testing LLMs' knowledge of Mandarin grammar
- No experiments conducted yet, seeking feedback on proposed methodology
- Aims to provide foundation for future studies and improvements on LLM evaluation in Chinese language tasks. <div>
arXiv:2504.14969v1 Announce Type: new 
Abstract: This paper proposes a framework for evaluating large language models (LLMs) on Chinese topic constructions, focusing on their sensitivity to island constraints. Drawing inspiration from Tian et al. (2024), we outline an experimental design for testing LLMs' grammatical knowledge of Mandarin syntax. While no experiments have been conducted yet, this proposal aims to provide a foundation for future studies and invites feedback on the methodology.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Pretraining Length Scaling</title>
<link>https://arxiv.org/abs/2504.14992</link>
<guid>https://arxiv.org/abs/2504.14992</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, length scaling, pre-training, Parallel Hidden Decoding Transformer, KV cache management

Summary: 
The article introduces the Parallel Hidden Decoding Transformer (PHD-Transformer), a framework for efficient length scaling during pre-training in large language models. It utilizes a novel KV cache management strategy to distinguish between original tokens and hidden decoding tokens, optimizing long-range dependencies. Two optimized variants, PHD-SWA and PHD-CSWA, enhance performance by preserving local dependencies and eliminating linear growth in pre-filling time. Extensive experiments showcase consistent improvements across various benchmarks, highlighting the effectiveness of the PHD-Transformer in maintaining inference efficiency while enabling effective length scaling. <div>
arXiv:2504.14992v1 Announce Type: new 
Abstract: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs</title>
<link>https://arxiv.org/abs/2504.15013</link>
<guid>https://arxiv.org/abs/2504.15013</guid>
<content:encoded><![CDATA[
arXiv:2504.15013v1 Announce Type: new 
Abstract: The process of creating educational materials is both time-consuming and demanding for educators. This research explores the potential of Large Language Models (LLMs) to streamline this task by automating the generation of extended reading materials and relevant course suggestions. Using the TED-Ed Dig Deeper sections as an initial exploration, we investigate how supplementary articles can be enriched with contextual knowledge and connected to additional learning resources. Our method begins by generating extended articles from video transcripts, leveraging LLMs to include historical insights, cultural examples, and illustrative anecdotes. A recommendation system employing semantic similarity ranking identifies related courses, followed by an LLM-based refinement process to enhance relevance. The final articles are tailored to seamlessly integrate these recommendations, ensuring they remain cohesive and informative. Experimental evaluations demonstrate that our model produces high-quality content and accurate course suggestions, assessed through metrics such as Hit Rate, semantic similarity, and coherence. Our experimental analysis highlight the nuanced differences between the generated and existing materials, underscoring the model's capacity to offer more engaging and accessible learning experiences. This study showcases how LLMs can bridge the gap between core content and supplementary learning, providing students with additional recommended resources while also assisting teachers in designing educational materials.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs as Data Annotators: How Close Are We to Human Performance</title>
<link>https://arxiv.org/abs/2504.15022</link>
<guid>https://arxiv.org/abs/2504.15022</guid>
<content:encoded><![CDATA[
arXiv:2504.15022v1 Announce Type: new 
Abstract: In NLP, fine-tuning LLMs is effective for various applications but requires high-quality annotated data. However, manual annotation of data is labor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly used to automate the process, often employing in-context learning (ICL) in which some examples related to the task are given in the prompt for better performance. However, manually selecting context examples can lead to inefficiencies and suboptimal model performance. This paper presents comprehensive experiments comparing several LLMs, considering different embedding models, across various datasets for the Named Entity Recognition (NER) task. The evaluation encompasses models with approximately $7$B and $70$B parameters, including both proprietary and non-proprietary models. Furthermore, leveraging the success of Retrieval-Augmented Generation (RAG), it also considers a method that addresses the limitations of ICL by automatically retrieving contextual examples, thereby enhancing performance. The results highlight the importance of selecting the appropriate LLM and embedding model, understanding the trade-offs between LLM sizes and desired performance, and the necessity to direct research efforts towards more challenging datasets.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models</title>
<link>https://arxiv.org/abs/2504.15027</link>
<guid>https://arxiv.org/abs/2504.15027</guid>
<content:encoded><![CDATA[
arXiv:2504.15027v1 Announce Type: new 
Abstract: Enhancing computational efficiency and reducing deployment costs for large language models (LLMs) have become critical challenges in various resource-constrained scenarios. In this work, we present DistilQwen2.5, a family of distilled, lightweight LLMs derived from the public Qwen2.5 models. These distilled models exhibit enhanced instruction-following capabilities compared to the original models based on a series of distillation techniques that incorporate knowledge from much larger LLMs. In our industrial practice, we first leverage powerful proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn. After standard fine-tuning, we further leverage a computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge from their teachers. Experimental evaluations demonstrate that the distilled models possess significantly stronger capabilities than their original checkpoints. Additionally, we present use cases to illustrate the applications of our framework in real-world scenarios. To facilitate practical use, we have released all the DistilQwen2.5 models to the open-source community.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search</title>
<link>https://arxiv.org/abs/2504.15047</link>
<guid>https://arxiv.org/abs/2504.15047</guid>
<content:encoded><![CDATA[
arXiv:2504.15047v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but are susceptible to adversarial prompts that exploit vulnerabilities to produce unsafe or biased outputs. Existing red-teaming methods often face scalability challenges, resource-intensive requirements, or limited diversity in attack strategies. We propose RainbowPlus, a novel red-teaming framework rooted in evolutionary computation, enhancing adversarial prompt generation through an adaptive quality-diversity (QD) search that extends classical evolutionary algorithms like MAP-Elites with innovations tailored for language models. By employing a multi-element archive to store diverse high-quality prompts and a comprehensive fitness function to evaluate multiple prompts concurrently, RainbowPlus overcomes the constraints of single-prompt archives and pairwise comparisons in prior QD methods like Rainbow Teaming. Experiments comparing RainbowPlus to QD methods across six benchmark datasets and four open-source LLMs demonstrate superior attack success rate (ASR) and diversity (Diverse-Score $\approx 0.84$), generating up to 100 times more unique prompts (e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine state-of-the-art methods on the HarmBench dataset with twelve LLMs (ten open-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%, surpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours). Our open-source implementation fosters further advancements in LLM safety, offering a scalable tool for vulnerability assessment. Code and resources are publicly available at https://github.com/knoveleng/rainbowplus, supporting reproducibility and future research in LLM red-teaming.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT</title>
<link>https://arxiv.org/abs/2504.15052</link>
<guid>https://arxiv.org/abs/2504.15052</guid>
<content:encoded><![CDATA[
arXiv:2504.15052v1 Announce Type: new 
Abstract: This study investigates the capabilities of large language models (LLMs), specifically ChatGPT, in annotating MT outputs based on an error typology. In contrast to previous work focusing mainly on general language, we explore ChatGPT's ability to identify and categorise errors in specialised translations. By testing two different prompts and based on a customised error typology, we compare ChatGPT annotations with human expert evaluations of translations produced by DeepL and ChatGPT itself. The results show that, for translations generated by DeepL, recall and precision are quite high. However, the degree of accuracy in error categorisation depends on the prompt's specific features and its level of detail, ChatGPT performing very well with a detailed prompt. When evaluating its own translations, ChatGPT achieves significantly poorer results, revealing limitations with self-assessment. These results highlight both the potential and the limitations of LLMs for translation evaluation, particularly in specialised domains. Our experiments pave the way for future research on open-source LLMs, which could produce annotations of comparable or even higher quality. In the future, we also aim to test the practical effectiveness of this automated evaluation in the context of translation training, particularly by optimising the process of human evaluation by teachers and by exploring the impact of annotations by LLMs on students' post-editing and translation learning.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15093</link>
<guid>https://arxiv.org/abs/2504.15093</guid>
<content:encoded><![CDATA[
arXiv:2504.15093v1 Announce Type: new 
Abstract: Detecting collaborative and problem-solving behaviours from digital traces to interpret students' collaborative problem solving (CPS) competency is a long-term goal in the Artificial Intelligence in Education (AIEd) field. Although multimodal data and advanced models are argued to have the potential to detect complex CPS behaviours, empirical evidence on their value remains limited with some contrasting evidence. In this study, we investigated the potential of multimodal data to improve model performance in diagnosing 78 secondary school students' CPS subskills and indicators in authentic educational settings. In particular, text embeddings from verbal data and acoustic embeddings from audio data were used in a multimodal classification model for CPS diagnosis. Both unimodal and multimodal transformer-based models outperformed traditional models in detecting CPS classes. Although the inclusion of multimodality did not improve the performance of traditional unimodal models, its integration into transformer-based models demonstrated improved performance for diagnosing social-cognitive CPS classes compared to unimodal transformer-based models. Based on the results, the paper argues that multimodality and the selection of a particular modelling technique should not be taken for granted to achieve the best performance in the automated detection of every CPS subskill and indicator. Rather, their value is limited to certain types of CPS indicators, affected by the complexity of the labels, and dependent on the composition of indicators in the dataset. We conclude the paper by discussing the required nuance when considering the value of LLMs and multimodality in automated CPS diagnosis, highlighting the need for human-AI complementarity, and proposing the exploration of relevant model architectures and techniques to improve CPS diagnosis in authentic educational contexts.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuwain 1.5B: An Arabic SLM via Language Injection</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
arXiv:2504.15120v1 Announce Type: new 
Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models</title>
<link>https://arxiv.org/abs/2504.15133</link>
<guid>https://arxiv.org/abs/2504.15133</guid>
<content:encoded><![CDATA[
arXiv:2504.15133v1 Announce Type: new 
Abstract: In this paper, we introduce EasyEdit2, a framework designed to enable plug-and-play adjustability for controlling Large Language Model (LLM) behaviors. EasyEdit2 supports a wide range of test-time interventions, including safety, sentiment, personality, reasoning patterns, factuality, and language features. Unlike its predecessor, EasyEdit2 features a new architecture specifically designed for seamless model steering. It comprises key modules such as the steering vector generator and the steering vector applier, which enable automatic generation and application of steering vectors to influence the model's behavior without modifying its parameters. One of the main advantages of EasyEdit2 is its ease of use-users do not need extensive technical knowledge. With just a single example, they can effectively guide and adjust the model's responses, making precise control both accessible and efficient. Empirically, we report model steering performance across different LLMs, demonstrating the effectiveness of these techniques. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit along with a demonstration notebook. In addition, we provide a demo video at https://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks</title>
<link>https://arxiv.org/abs/2504.15160</link>
<guid>https://arxiv.org/abs/2504.15160</guid>
<content:encoded><![CDATA[
arXiv:2504.15160v1 Announce Type: new 
Abstract: Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa, require that all categories in an annotation task be sufficiently represented in the training data for optimal performance. However, it is often difficult to find sufficient examples for all categories in a task when building a high-quality training set. In this article, I describe this problem and propose a solution, the synthetic imputation approach. Leveraging a generative LLM (GPT-4o), this approach generates synthetic texts based on careful prompting and five original examples drawn randomly with replacement from the sample. This approach ensures that new synthetic texts are sufficiently different from the original texts to reduce overfitting, but retain the underlying substantive meaning of the examples to maximize out-of-sample performance. With 75 original examples or more, synthetic imputation's performance is on par with a full sample of original texts, and overfitting remains low, predictable and correctable with 50 original samples. The synthetic imputation approach provides a novel role for generative LLMs in research and allows applied researchers to balance their datasets for best performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On true empty category</title>
<link>https://arxiv.org/abs/2504.15168</link>
<guid>https://arxiv.org/abs/2504.15168</guid>
<content:encoded><![CDATA[
arXiv:2504.15168v1 Announce Type: new 
Abstract: According to Chomsky (1981, 1986), empty categories consist of PRO, pro, trace, and variable. However, some empty object positions seem to be incompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014) and Li & Wei (2014) raise the true empty category hypothesis, which holds that true empty category is only an empty position with category and Case features. As a last resort option, it is used mainly to meet the subcatgorization of a verb. This assumption is ingenious, and if proved to be true, it will exert a great impact on the study of UG. In this paper, we evaluate their evidence from topicalization and demonstrate that it can be accounted for without invoking true empty category.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges</title>
<link>https://arxiv.org/abs/2504.15205</link>
<guid>https://arxiv.org/abs/2504.15205</guid>
<content:encoded><![CDATA[
arXiv:2504.15205v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enables large language models (LLMs) to generate answers with citations from source documents containing "ground truth", thereby reducing system hallucinations. A crucial factor in RAG evaluation is "support", whether the information in the cited documents supports the answer. To this end, we conducted a large-scale comparative study of 45 participant submissions on 36 topics to the TREC 2024 RAG Track, comparing an automatic LLM judge (GPT-4o) against human judges for support assessment. We considered two conditions: (1) fully manual assessments from scratch and (2) manual assessments with post-editing of LLM predictions. Our results indicate that for 56% of the manual from-scratch assessments, human and GPT-4o predictions match perfectly (on a three-level scale), increasing to 72% in the manual with post-editing condition. Furthermore, by carefully analyzing the disagreements in an unbiased study, we found that an independent human judge correlates better with GPT-4o than a human judge, suggesting that LLM judges can be a reliable alternative for support assessment. To conclude, we provide a qualitative analysis of human and GPT-4o errors to help guide future iterations of support assessment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalAgent: Discovering Implicit Evaluation Criteria from the Web</title>
<link>https://arxiv.org/abs/2504.15219</link>
<guid>https://arxiv.org/abs/2504.15219</guid>
<content:encoded><![CDATA[
arXiv:2504.15219v1 Announce Type: new 
Abstract: Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like "Help me draft an academic talk on coffee intake vs research productivity", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Bayesian Approaches to Topics over Time</title>
<link>https://arxiv.org/abs/2504.15220</link>
<guid>https://arxiv.org/abs/2504.15220</guid>
<content:encoded><![CDATA[
arXiv:2504.15220v1 Announce Type: new 
Abstract: The Topics over Time (ToT) model captures thematic changes in timestamped datasets by explicitly modeling publication dates jointly with word co-occurrence patterns. However, ToT was not approached in a fully Bayesian fashion, a flaw that makes it susceptible to stability problems. To address this issue, we propose a fully Bayesian Topics over Time (BToT) model via the introduction of a conjugate prior to the Beta distribution. This prior acts as a regularization that prevents the online version of the algorithm from unstable updates when a topic is poorly represented in a mini-batch. The characteristics of this prior to the Beta distribution are studied here for the first time. Still, this model suffers from a difference in scale between the single-time observations and the multiplicity of words per document. A variation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a solution. In WBToT, publication dates are repeated a certain number of times per document, which balances the relative influence of words and timestamps along the inference process. We have tested our models on two datasets: a collection of over 200 years of US state-of-the-union (SOTU) addresses and a large-scale COVID-19 Twitter corpus of 10 million tweets. The results show that WBToT captures events better than Latent Dirichlet Allocation and other SOTA topic models like BERTopic: the median absolute deviation of the topic presence over time is reduced by $51\%$ and $34\%$, respectively. Our experiments also demonstrate the superior coherence of WBToT over BToT, which highlights the importance of balancing the time and word modalities. Finally, we illustrate the stability of the online optimization algorithm in WBToT, which allows the application of WBToT to problems that are intractable for standard ToT.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions</title>
<link>https://arxiv.org/abs/2504.15236</link>
<guid>https://arxiv.org/abs/2504.15236</guid>
<content:encoded><![CDATA[
arXiv:2504.15236v1 Announce Type: new 
Abstract: AI assistants can impart value judgments that shape people's decisions and worldviews, yet little is known empirically about what values these systems rely on in practice. To address this, we develop a bottom-up, privacy-preserving method to extract the values (normative considerations stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit in hundreds of thousands of real-world interactions. We empirically discover and taxonomize 3,307 AI values and study how they vary by context. We find that Claude expresses many practical and epistemic values, and typically supports prosocial human values while resisting values like "moral nihilism". While some values appear consistently across contexts (e.g. "transparency"), many are more specialized and context-dependent, reflecting the diversity of human interlocutors and their varied contexts. For example, "harm prevention" emerges when Claude resists users, "historical accuracy" when responding to queries about controversial events, "healthy boundaries" when asked for relationship advice, and "human agency" in technology ethics discussions. By providing the first large-scale empirical mapping of AI values in deployment, our work creates a foundation for more grounded evaluation and design of values in AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning</title>
<link>https://arxiv.org/abs/2504.15241</link>
<guid>https://arxiv.org/abs/2504.15241</guid>
<content:encoded><![CDATA[
arXiv:2504.15241v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual setting, where multilingual safety-aligned data are often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we propose an approach to build a multilingual guardrail with reasoning. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail consistently outperforms recent baselines across both in-domain and out-of-domain languages. The multilingual reasoning capability of our guardrail enables it to generate multilingual explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators</title>
<link>https://arxiv.org/abs/2504.15253</link>
<guid>https://arxiv.org/abs/2504.15253</guid>
<content:encoded><![CDATA[
arXiv:2504.15253v1 Announce Type: new 
Abstract: Scaling test-time computation, or affording a generator large language model (LLM) extra compute during inference, typically employs the help of external non-generative evaluators (i.e., reward models). Concurrently, LLM-judges, models trained to generate evaluations and critiques (explanations) in natural language, are becoming increasingly popular in automatic evaluation. Despite judge empirical successes, their effectiveness as evaluators in test-time scaling settings is largely unknown. In this paper, we introduce the Judge Evaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge performance in three domains (math reasoning, code generation, and instruction following) under three task settings: response reranking, step-level beam search, and critique-based response refinement. We evaluate 10 different judge models (7B-70B parameters) for 8 different base generator models (6.7B-72B parameters). Our benchmark shows that while judges are competitive with outcome reward models in reranking, they are consistently worse than process reward models in beam search procedures. Furthermore, though unique to LLM-judges, their natural language critiques are currently ineffective in guiding the generator towards better responses.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution</title>
<link>https://arxiv.org/abs/2504.13847</link>
<guid>https://arxiv.org/abs/2504.13847</guid>
<content:encoded><![CDATA[
arXiv:2504.13847v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) offer unprecedented opportunities to enhance human-AI collaboration in qualitative research methods, including interviews. While interviews are highly valued for gathering deep, contextualized insights, interviewers often face significant cognitive challenges, such as real-time information processing, question adaptation, and rapport maintenance. My doctoral research introduces Interview AI-ssistant, a system designed for real-time interviewer-AI collaboration during both the preparation and execution phases. Through four interconnected studies, this research investigates the design of effective human-AI collaboration in interviewing contexts, beginning with a formative study of interviewers' needs, followed by a prototype development study focused on AI-assisted interview preparation, an experimental evaluation of real-time AI assistance during interviews, and a field study deploying the system in a real-world research setting. Beyond informing practical implementations of intelligent interview support systems, this work contributes to the Intelligent User Interfaces (IUI) community by advancing the understanding of human-AI collaborative interfaces in complex social tasks and establishing design guidelines for AI-enhanced qualitative research tools.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark</title>
<link>https://arxiv.org/abs/2504.13861</link>
<guid>https://arxiv.org/abs/2504.13861</guid>
<content:encoded><![CDATA[
arXiv:2504.13861v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) are increasingly being explored for applications in telemedicine, yet their ability to engage with diverse patient behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source evaluation framework designed to assess LLM-driven medical consultations. Unlike existing benchmarks, 3MDBench simulates real-world patient variability by incorporating four temperament-driven Patient Agents and an Assessor Agent that evaluates diagnostic accuracy and dialogue quality. The benchmark integrates textual and image-based patient data across 34 common diagnoses, mirroring real-world telemedicine interactions. Under different diagnostic strategies, we evaluate state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings, underscoring the value of context-driven, information-seeking questioning. Additionally, we demonstrate that multimodal inputs enhance diagnostic efficiency. Image-supported models outperform text-only counterparts by raising the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting. Finally, we suggest an approach that improves the diagnostic F1-score to 70.3 by training the CNN model on the diagnosis prediction task and incorporating its top-3 predictions into the LVLM context. 3MDBench provides a reproducible and extendable evaluation framework for AI-driven medical assistants. It offers insights into how patient temperament, dialogue strategies, and multimodal reasoning influence diagnosis quality. By addressing real-world complexities in telemedicine, our benchmark paves the way for more empathetic, reliable, and context-aware AI-driven healthcare solutions. The source code of our benchmark is publicly available: https://github.com/univanxx/3mdbench
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on (M)LLM-Based GUI Agents</title>
<link>https://arxiv.org/abs/2504.13865</link>
<guid>https://arxiv.org/abs/2504.13865</guid>
<content:encoded><![CDATA[
arXiv:2504.13865v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation</title>
<link>https://arxiv.org/abs/2504.13882</link>
<guid>https://arxiv.org/abs/2504.13882</guid>
<content:encoded><![CDATA[
arXiv:2504.13882v1 Announce Type: cross 
Abstract: Our study introduces an automated system leveraging large language models (LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving effective praise, 2. reacting to errors, 3. determining what students know, 4. helping students manage inequity, and 5. responding to negative self-talk. Using a public dataset from the Teacher-Student Chatroom Corpus, our system classifies each tutoring strategy as either being employed as desired or undesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use of these strategies and analyze tutoring dialogues. The results show that for the five tutoring strategies, True Negative Rates (TNR) range from 0.655 to 0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is effective at excluding incorrect classifications but struggles to consistently identify the correct strategy. The strategy \textit{helping students manage inequity} showed the highest performance with a TNR of 0.738 and Recall of 0.432. The study highlights the potential of LLMs in tutoring strategy analysis and outlines directions for future improvements, including incorporating more advanced models for more nuanced feedback.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants</title>
<link>https://arxiv.org/abs/2504.13887</link>
<guid>https://arxiv.org/abs/2504.13887</guid>
<content:encoded><![CDATA[
arXiv:2504.13887v1 Announce Type: cross 
Abstract: Despite the growing integration of AI chatbots as conversational agents in public discourse, empirical evidence regarding their capacity to foster intercultural empathy remains limited. Using a randomized dialogue experiment, we examined how different types of AI chatbot interaction, i.e., deliberative versus non-deliberative and culturally aligned versus non-aligned, affect intercultural empathy across cultural groups. Results show that deliberative conversations increased intercultural empathy among American participants but not Latin American participants, who perceived AI responses as culturally inaccurate and failing to represent their cultural contexts and perspectives authentically. Real-time interaction analyses reveal that these differences stem from cultural knowledge gaps inherent in Large Language Models. Despite explicit prompting and instruction to represent cultural perspectives in participants' native languages, AI systems still exhibit significant disparities in cultural representation. This highlights the importance of designing AI systems capable of culturally authentic engagement in deliberative conversations. Our study contributes to deliberation theory and AI alignment research by underscoring AI's role in intercultural dialogue and the persistent challenge of representational asymmetry in democratic discourse.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment</title>
<link>https://arxiv.org/abs/2504.13888</link>
<guid>https://arxiv.org/abs/2504.13888</guid>
<content:encoded><![CDATA[
arXiv:2504.13888v1 Announce Type: cross 
Abstract: Kanji script writing is a skill that is often introduced to novice Japanese foreign language students for achieving Japanese writing mastery, but often poses difficulties to students with primarily English fluency due to their its vast differences with written English. Instructors often introduce various pedagogical methods -- such as visual structure and written techniques -- to assist students in kanji study, but may lack availability providing direct feedback on students' writing outside of class. Current educational applications are also limited due to lacking richer instructor-emulated feedback. We introduce Kanji Workbook, a writing-based intelligent tutoring system for students to receive intelligent assessment that emulates human instructor feedback. Our interface not only leverages students' computing devices for allowing them to learn, practice, and review the writing of prompted characters from their course's kanji script lessons, but also provides a diverse set of writing assessment metrics -- derived from instructor interviews and classroom observation insights -- through intelligent scoring and visual animations. We deployed our interface onto novice- and intermediate-level university courses over an entire academic year, and observed that interface users on average achieved higher course grades than their peers and also reacted positively to our interface's various features.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches</title>
<link>https://arxiv.org/abs/2504.13890</link>
<guid>https://arxiv.org/abs/2504.13890</guid>
<content:encoded><![CDATA[
arXiv:2504.13890v1 Announce Type: cross 
Abstract: Computational mental health research develops models to predict and understand psychological phenomena, but often relies on inappropriate measures of psychopathology constructs, undermining validity. We identify three key issues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis) over validated ones (e.g., diagnosis by clinician); (2) treating mental health constructs as categorical rather than dimensional; and (3) focusing on disorder-specific constructs instead of transdiagnostic ones. We outline the benefits of using validated, dimensional, and transdiagnostic measures and offer practical recommendations for practitioners. Using valid measures that reflect the nature and structure of psychopathology is essential for computational mental health research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALLMesh: a simple application for performing Thematic Analysis with Large Language Models</title>
<link>https://arxiv.org/abs/2504.13892</link>
<guid>https://arxiv.org/abs/2504.13892</guid>
<content:encoded><![CDATA[
arXiv:2504.13892v1 Announce Type: cross 
Abstract: Thematic analysis (TA) is a widely used qualitative research method for identifying and interpreting patterns within textual data, such as qualitative interviews. Recent research has shown that it is possible to satisfactorily perform TA using Large Language Models (LLMs). This paper presents a novel application using LLMs to assist researchers in conducting TA. The application enables users to upload textual data, generate initial codes and themes. All of this is possible through a simple Graphical User Interface, (GUI) based on the streamlit framework, working with python scripts for the analysis, and using Application Program Interfaces of LLMs. Having a GUI is particularly important for researchers in fields where coding skills may not be prevalent, such as social sciences or humanities. With the app, users can iteratively refine codes and themes adopting a human-in-the-loop process, without the need to work with programming and scripting. The paper describes the application key features, highlighting its potential for qualitative research while preserving methodological rigor. The paper discusses the design and interface of the app and outlines future directions for this work.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge</title>
<link>https://arxiv.org/abs/2504.13904</link>
<guid>https://arxiv.org/abs/2504.13904</guid>
<content:encoded><![CDATA[
arXiv:2504.13904v1 Announce Type: cross 
Abstract: We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Incident Prevention in an Enterprise AI Assistant</title>
<link>https://arxiv.org/abs/2504.13924</link>
<guid>https://arxiv.org/abs/2504.13924</guid>
<content:encoded><![CDATA[
arXiv:2504.13924v1 Announce Type: cross 
Abstract: Enterprise AI Assistants are increasingly deployed in domains where accuracy is paramount, making each erroneous output a potentially significant incident. This paper presents a comprehensive framework for monitoring, benchmarking, and continuously improving such complex, multi-component systems under active development by multiple teams. Our approach encompasses three key elements: (1) a hierarchical ``severity'' framework for incident detection that identifies and categorizes errors while attributing component-specific error rates, facilitating targeted improvements; (2) a scalable and principled methodology for benchmark construction, evaluation, and deployment, designed to accommodate multiple development teams, mitigate overfitting risks, and assess the downstream impact of system modifications; and (3) a continual improvement strategy leveraging multidimensional evaluation, enabling the identification and implementation of diverse enhancement opportunities. By adopting this holistic framework, organizations can systematically enhance the reliability and performance of their AI Assistants, ensuring their efficacy in critical enterprise environments. We conclude by discussing how this multifaceted evaluation approach opens avenues for various classes of enhancements, paving the way for more robust and trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining</title>
<link>https://arxiv.org/abs/2504.13932</link>
<guid>https://arxiv.org/abs/2504.13932</guid>
<content:encoded><![CDATA[
arXiv:2504.13932v1 Announce Type: cross 
Abstract: Large language models offer remarkable capabilities, but their size and computational demands pose practical challenges. Quantization methods compress their size through replacing their high-precision parameters by quantized values of lower precision. Post-training quantization reduces model size efficiently at the cost of decreased accuracy, while quantization-aware training better preserves accuracy but is resource-intensive. Among existing post-training quantization algorithms, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining may not be feasible through partial training. (2) This gain seems to depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. It relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on benchmark language models from the LLaMA family show that our proposed approach boosts accuracy and tightens the gap between the quantized model and the full-precision model, with minimal overhead. Our method will be made publicly available to facilitate future developments in ultra-low-bit quantization of large language models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations</title>
<link>https://arxiv.org/abs/2504.13955</link>
<guid>https://arxiv.org/abs/2504.13955</guid>
<content:encoded><![CDATA[
arXiv:2504.13955v1 Announce Type: cross 
Abstract: The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolRL: Reward is All Tool Learning Needs</title>
<link>https://arxiv.org/abs/2504.13958</link>
<guid>https://arxiv.org/abs/2504.13958</guid>
<content:encoded><![CDATA[
arXiv:2504.13958v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning. In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO). Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the codes are released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Safety Should Prioritize the Future of Work</title>
<link>https://arxiv.org/abs/2504.13959</link>
<guid>https://arxiv.org/abs/2504.13959</guid>
<content:encoded><![CDATA[
arXiv:2504.13959v1 Announce Type: cross 
Abstract: Current efforts in AI safety prioritize filtering harmful content, preventing manipulation of human behavior, and eliminating existential risks in cybersecurity or biosecurity. While pressing, this narrow focus overlooks critical human-centric considerations that shape the long-term trajectory of a society. In this position paper, we identify the risks of overlooking the impact of AI on the future of work and recommend comprehensive transition support towards the evolution of meaningful labor with human agency. Through the lens of economic theories, we highlight the intertemporal impacts of AI on human livelihood and the structural changes in labor markets that exacerbate income inequality. Additionally, the closed-source approach of major stakeholders in AI development resembles rent-seeking behavior through exploiting resources, breeding mediocrity in creative labor, and monopolizing innovation. To address this, we argue in favor of a robust international copyright anatomy supported by implementing collective licensing that ensures fair compensation mechanisms for using data to train AI models. We strongly recommend a pro-worker framework of global AI governance to enhance shared prosperity and economic justice while reducing technical debt.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels</title>
<link>https://arxiv.org/abs/2504.13984</link>
<guid>https://arxiv.org/abs/2504.13984</guid>
<content:encoded><![CDATA[
arXiv:2504.13984v1 Announce Type: cross 
Abstract: To reduce the time and computational costs of inference of large language models, there has been interest in parameter-efficient low-rank early-exit casting of transformer hidden-representations to final-representations. Such low-rank short-cutting has been shown to outperform identity shortcuts at early model stages while offering parameter-efficiency in shortcut jumps. However, current low-rank methods maintain a separate early-exit shortcut jump to final-representations for each transformer intermediate block-level during inference. In this work, we propose selection of a single One-Jump-Fits-All (OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter costs during inference. We show that despite this extreme reduction, our OJFA choice largely matches the performance of maintaining multiple shortcut jumps during inference and offers stable precision from all transformer block-levels for GPT2-XL, Phi3-Mini and Llama2-7B transformer models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs</title>
<link>https://arxiv.org/abs/2504.13989</link>
<guid>https://arxiv.org/abs/2504.13989</guid>
<content:encoded><![CDATA[
arXiv:2504.13989v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40\% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions</title>
<link>https://arxiv.org/abs/2504.14053</link>
<guid>https://arxiv.org/abs/2504.14053</guid>
<content:encoded><![CDATA[
arXiv:2504.14053v1 Announce Type: cross 
Abstract: This research examines whether Airbnb guests' positive and negative comments influence acceptance rates and rental prices across six U.S. regions: Rhode Island, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of reviews were collected and analyzed using Natural Language Processing (NLP) to classify sentiments as positive or negative, followed by statistical testing (t-tests and basic correlations) on the average scores. The findings reveal that over 90 percent of reviews in each region are positive, indicating that having additional reviews does not significantly enhance prices. However, listings with predominantly positive feedback exhibit slightly higher acceptance rates, suggesting that sentiment polarity, rather than the sheer volume of reviews, is a more critical factor for host success. Additionally, budget listings often gather extensive reviews while maintaining competitive pricing, whereas premium listings sustain higher prices with fewer but highly positive reviews. These results underscore the importance of sentiment quality over quantity in shaping guest behavior and pricing strategies in an overwhelmingly positive review environment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linking forward-pass dynamics in Transformers and real-time human processing</title>
<link>https://arxiv.org/abs/2504.14107</link>
<guid>https://arxiv.org/abs/2504.14107</guid>
<content:encoded><![CDATA[
arXiv:2504.14107v1 Announce Type: cross 
Abstract: Modern AI models are increasingly being used as theoretical tools to study human cognition. One dominant approach is to evaluate whether human-derived measures (such as offline judgments or real-time processing) are predicted by a model's output: that is, the end-product of forward pass(es) through the network. At the same time, recent advances in mechanistic interpretability have begun to reveal the internal processes that give rise to model outputs, raising the question of whether models and humans might arrive at outputs using similar "processing strategies". Here, we investigate the link between real-time processing in humans and "layer-time" dynamics in Transformer models. Across five studies spanning domains and modalities, we test whether the dynamics of computation in a single forward pass of pre-trained Transformers predict signatures of processing in humans, above and beyond properties of the model's output probability distribution. We consistently find that layer-time dynamics provide additional predictive power on top of output measures. Our results suggest that Transformer processing and human processing may be facilitated or impeded by similar properties of an input stimulus, and this similarity has emerged through general-purpose objectives such as next-token prediction or image recognition. Our work suggests a new way of using AI models to study human cognition: not just as a black box mapping stimuli to responses, but potentially also as explicit processing models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System of Agentic AI for the Discovery of Metal-Organic Frameworks</title>
<link>https://arxiv.org/abs/2504.14110</link>
<guid>https://arxiv.org/abs/2504.14110</guid>
<content:encoded><![CDATA[
arXiv:2504.14110v1 Announce Type: cross 
Abstract: Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five "AI-dreamt" MOFs, representing a major step toward automated synthesizable material discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Principles Improve Prompt Learning In Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.14123</link>
<guid>https://arxiv.org/abs/2504.14123</guid>
<content:encoded><![CDATA[
arXiv:2504.14123v1 Announce Type: cross 
Abstract: Prompt learning is a popular fine-tuning method for vision-language models due to its efficiency. It requires a small number of additional learnable parameters while significantly enhancing performance on target tasks. However, most existing methods suffer from overfitting to fine-tuning data, yielding poor generalizability. To address this, we propose a new training objective function based on a Bayesian learning principle to balance adaptability and generalizability. We derive a prior over the logits, where the mean function is parameterized by the pre-trained model, while the posterior corresponds to the fine-tuned model. This objective establishes a balance by allowing the fine-tuned model to adapt to downstream tasks while remaining close to the pre-trained model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models</title>
<link>https://arxiv.org/abs/2504.14126</link>
<guid>https://arxiv.org/abs/2504.14126</guid>
<content:encoded><![CDATA[
arXiv:2504.14126v1 Announce Type: cross 
Abstract: Determining the ideal architecture for deep learning models, such as the number of layers and neurons, is a difficult and resource-intensive process that frequently relies on human tuning or computationally costly optimization approaches. While Particle Swarm Optimization (PSO) and Large Language Models (LLMs) have been individually applied in optimization and deep learning, their combined use for enhancing convergence in numerical optimization tasks remains underexplored. Our work addresses this gap by integrating LLMs into PSO to reduce model evaluations and improve convergence for deep learning hyperparameter tuning. The proposed LLM-enhanced PSO method addresses the difficulties of efficiency and convergence by using LLMs (particularly ChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster achievement of target objectives. Our method speeds up search space exploration by substituting underperforming particle placements with best suggestions offered by LLMs. Comprehensive experiments across three scenarios -- (1) optimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM) networks for time series regression, and (3) using Convolutional Neural Networks (CNNs) for material classification -- show that the method significantly improves convergence rates and lowers computational costs. Depending on the application, computational complexity is lowered by 20% to 60% compared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in model calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by 60% for both regression and classification tasks, all while preserving accuracy and error rates. This groundbreaking methodology offers a very efficient and effective solution for optimizing deep learning models, leading to substantial computational performance improvements across a wide range of applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TALES: Text Adventure Learning Environment Suite</title>
<link>https://arxiv.org/abs/2504.14128</link>
<guid>https://arxiv.org/abs/2504.14128</guid>
<content:encoded><![CDATA[
arXiv:2504.14128v1 Announce Type: cross 
Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation</title>
<link>https://arxiv.org/abs/2504.14147</link>
<guid>https://arxiv.org/abs/2504.14147</guid>
<content:encoded><![CDATA[
arXiv:2504.14147v1 Announce Type: cross 
Abstract: Recent advancements in explainable recommendation have greatly bolstered user experience by elucidating the decision-making rationale. However, the existing methods actually fail to provide effective feedback signals for potentially better or worse generated explanations due to their reliance on traditional supervised learning paradigms in sparse interaction data. To address these issues, we propose a novel human-like feedback-driven optimization framework. This framework employs a dynamic interactive optimization mechanism for achieving human-centered explainable requirements without incurring high labor costs. Specifically, we propose to utilize large language models (LLMs) as human simulators to predict human-like feedback for guiding the learning process. To enable the LLMs to deeply understand the task essence and meet user's diverse personalized requirements, we introduce a human-induced customized reward scoring method, which helps stimulate the language understanding and logical reasoning capabilities of LLMs. Furthermore, considering the potential conflicts between different perspectives of explanation quality, we introduce a principled Pareto optimization that transforms the multi-perspective quality enhancement task into a multi-objective optimization problem for improving explanation performance. At last, to achieve efficient model training, we design an off-policy optimization pipeline. By incorporating a replay buffer and addressing the data distribution biases, we can effectively improve data utilization and enhance model generality. Extensive experiments on four datasets demonstrate the superiority of our approach.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Advantage Regression: Aligning LLMs with Online AI Reward</title>
<link>https://arxiv.org/abs/2504.14177</link>
<guid>https://arxiv.org/abs/2504.14177</guid>
<content:encoded><![CDATA[
arXiv:2504.14177v1 Announce Type: cross 
Abstract: Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The First VoicePrivacy Attacker Challenge</title>
<link>https://arxiv.org/abs/2504.14183</link>
<guid>https://arxiv.org/abs/2504.14183</guid>
<content:encoded><![CDATA[
arXiv:2504.14183v1 Announce Type: cross 
Abstract: The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand Challenge which focuses on evaluating attacker systems against a set of voice anonymization systems submitted to the VoicePrivacy 2024 Challenge. Training, development, and evaluation datasets were provided along with a baseline attacker. Participants developed their attacker systems in the form of automatic speaker verification systems and submitted their scores on the development and evaluation data. The best attacker systems reduced the equal error rate (EER) by 25-44% relative w.r.t. the baseline.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Idea Bench 2025: AI Research Idea Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.14191</link>
<guid>https://arxiv.org/abs/2504.14191</guid>
<content:encoded><![CDATA[
arXiv:2504.14191v1 Announce Type: cross 
Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction and achieved significant success in the generation of novel ideas. However, current assessments of idea generation overlook crucial factors such as knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded truth, and the limited scope of feasibility analysis constrained by prompt design. These limitations hinder the potential of uncovering groundbreaking research ideas. In this paper, we present AI Idea Bench 2025, a framework designed to quantitatively evaluate and compare the ideas generated by LLMs within the domain of AI research from diverse perspectives. The framework comprises a comprehensive dataset of 3,495 AI papers and their associated inspired works, along with a robust evaluation methodology. This evaluation system gauges idea quality in two dimensions: alignment with the ground-truth content of the original papers and judgment based on general reference material. AI Idea Bench 2025's benchmarking system stands to be an invaluable resource for assessing and comparing idea-generation techniques, thereby facilitating the automation of scientific discovery.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment</title>
<link>https://arxiv.org/abs/2504.14232</link>
<guid>https://arxiv.org/abs/2504.14232</guid>
<content:encoded><![CDATA[
arXiv:2504.14232v1 Announce Type: cross 
Abstract: This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz, an Artificial Intelligence (AI) driven plugin for automating Multiple-Choice Question (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured framework for categorizing educational objectives into hierarchical cognitive levels. Our research investigates whether incorporating this taxonomy can improve the alignment of AI-generated questions with specific cognitive objectives. We developed a dataset of 3691 questions categorized according to Bloom's levels and employed various classification models-Multinomial Logistic Regression, Naive Bayes, Linear Support Vector Classification (SVC), and a Transformer-based model (DistilBERT)-to evaluate their effectiveness in categorizing questions. Our results indicate that higher Bloom's levels generally correlate with increased question length, Flesch-Kincaid Grade Level (FKGL), and Lexical Density (LD), reflecting the increased complexity of higher cognitive demands. Multinomial Logistic Regression showed varying accuracy across Bloom's levels, performing best for "Knowledge" and less accurately for higher-order levels. Merging higher-level categories improved accuracy for complex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective classification for lower levels but struggled with higher-order tasks. DistilBERT achieved the highest performance, significantly improving classification of both lower and higher-order cognitive levels, achieving an overall validation accuracy of 91%. This study highlights the potential of integrating Bloom's Taxonomy into AI-driven assessment tools and underscores the advantages of advanced models like DistilBERT for enhancing educational content generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</title>
<link>https://arxiv.org/abs/2504.14239</link>
<guid>https://arxiv.org/abs/2504.14239</guid>
<content:encoded><![CDATA[
arXiv:2504.14239v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Explainable Fake Image Detection with Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2504.14245</link>
<guid>https://arxiv.org/abs/2504.14245</guid>
<content:encoded><![CDATA[
arXiv:2504.14245v1 Announce Type: cross 
Abstract: Progress in image generation raises significant public security concerns. We argue that fake image detection should not operate as a "black box". Instead, an ideal approach must ensure both strong generalization and transparency. Recent progress in Multi-modal Large Language Models (MLLMs) offers new opportunities for reasoning-based AI-generated image detection. In this work, we evaluate the capabilities of MLLMs in comparison to traditional detection methods and human evaluators, highlighting their strengths and limitations. Furthermore, we design six distinct prompts and propose a framework that integrates these prompts to develop a more robust, explainable, and reasoning-driven detection system. The code is available at https://github.com/Gennadiyev/mllm-defake.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-attention for State-based model RWKV-7</title>
<link>https://arxiv.org/abs/2504.14260</link>
<guid>https://arxiv.org/abs/2504.14260</guid>
<content:encoded><![CDATA[
arXiv:2504.14260v1 Announce Type: cross 
Abstract: We introduce CrossWKV, a novel cross-attention mechanism for the state-based RWKV-7 model, designed to enhance the expressive power of text-to-image generation. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV) architecture, CrossWKV integrates text and image modalities in a single pass, utilizing a generalized delta rule with vector-valued gating and low-rank adaptations (LoRA) to achieve superior cross-modal alignment. Unlike Transformer-based models, CrossWKV's non-diagonal, input-dependent transition matrix enables it to represent complex functions beyond the $\mathrm{TC}^0$ complexity class, including all regular languages, as demonstrated by its ability to perform state-tracking tasks like $S_5$ permutation modeling. Evaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B and ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and a CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance while offering robust generalization across diverse prompts. The model's enhanced expressivity, combined with constant memory usage and linear scaling, positions it as a powerful solution for advanced cross-modal tasks, with potential applications in high-resolution generation and dynamic state manipulation.Code at https://github.com/TorchRWKV/flash-linear-attention
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling</title>
<link>https://arxiv.org/abs/2504.14359</link>
<guid>https://arxiv.org/abs/2504.14359</guid>
<content:encoded><![CDATA[
arXiv:2504.14359v1 Announce Type: cross 
Abstract: There are many ways to describe, name, and group objects when captioning an image. Differences are evident when speakers come from diverse cultures due to the unique experiences that shape perception. Machine translation of captions has pushed multilingual capabilities in vision-language models (VLMs), but data comes mainly from English speakers, indicating a perceptual bias and lack of model flexibility. In this work, we address this challenge and outline a data-efficient framework to instill multilingual VLMs with greater understanding of perceptual diversity. We specifically propose an LLM-based, multimodal recaptioning strategy that alters the object descriptions of English captions before translation. The greatest benefits are demonstrated in a targeted multimodal mechanism guided by native speaker data. By adding produced rewrites as augmentations in training, we improve on German and Japanese text-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on non-native error cases). We further propose a mechanism to analyze the specific object description differences across datasets, and we offer insights into cross-dataset and cross-language generalization.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction</title>
<link>https://arxiv.org/abs/2504.14361</link>
<guid>https://arxiv.org/abs/2504.14361</guid>
<content:encoded><![CDATA[
arXiv:2504.14361v1 Announce Type: cross 
Abstract: In this study, we propose an innovative methodology for predicting Cancer Drug Response (CDR) through the integration of the scGPT foundation model within the DeepCDR model. Our approach utilizes scGPT to generate embeddings from gene expression data, which are then used as gene expression input data for DeepCDR. The experimental findings demonstrate the efficacy of this scGPT-based method in outperforming previous related works, including the original DeepCDR model and the scFoundation-based model. This study highlights the potential of scGPT embeddings to enhance the accuracy of CDR predictions and offers a promising alternative to existing approaches.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving RL Exploration for LLM Reasoning through Retrospective Replay</title>
<link>https://arxiv.org/abs/2504.14363</link>
<guid>https://arxiv.org/abs/2504.14363</guid>
<content:encoded><![CDATA[
arXiv:2504.14363v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has increasingly become a pivotal technique in the post-training of large language models (LLMs). The effective exploration of the output space is essential for the success of RL. We observe that for complex problems, during the early stages of training, the model exhibits strong exploratory capabilities and can identify promising solution ideas. However, its limited capability at this stage prevents it from successfully solving these problems. The early suppression of these potentially valuable solution ideas by the policy gradient hinders the model's ability to revisit and re-explore these ideas later. Consequently, although the LLM's capabilities improve in the later stages of training, it still struggles to effectively address these complex problems. To address this exploration issue, we propose a novel algorithm named Retrospective Replay-based Reinforcement Learning (RRL), which introduces a dynamic replay mechanism throughout the training process. RRL enables the model to revisit promising states identified in the early stages, thereby improving its efficiency and effectiveness in exploration. To evaluate the effectiveness of RRL, we conduct extensive experiments on complex reasoning tasks, including mathematical reasoning and code generation, and general dialogue tasks. The results indicate that RRL maintains high exploration efficiency throughout the training period, significantly enhancing the effectiveness of RL in optimizing LLMs for complicated reasoning tasks. Moreover, it also improves the performance of RLHF, making the model both safer and more helpful.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Density Measures for Language Generation</title>
<link>https://arxiv.org/abs/2504.14370</link>
<guid>https://arxiv.org/abs/2504.14370</guid>
<content:encoded><![CDATA[
arXiv:2504.14370v1 Announce Type: cross 
Abstract: The recent successes of large language models (LLMs) have led to a surge of theoretical research into language generation. A recent line of work proposes an abstract view, called language generation in the limit, where generation is seen as a game between an adversary and an algorithm: the adversary generates strings from an unknown language $K$, chosen from a countable collection of candidate languages, and after seeing a finite set of these strings, the algorithm must generate new strings from $K$ that it has not seen before. This formalism highlights a key tension: the trade-off between validity (the algorithm should only produce strings from the language) and breadth (it should be able to produce many strings from the language). This trade-off is central in applied language generation as well, where it appears as a balance between hallucination (generating invalid utterances) and mode collapse (generating only a restricted set of outputs). Despite its importance, this trade-off has been challenging to study quantitatively. We develop ways to quantify this trade-off by formalizing breadth using measures of density. Existing algorithms for language generation in the limit produce output sets that can have zero density in the true language, and this important failure of breadth might seem unavoidable. We show, however, that such a failure is not necessary: we provide an algorithm for language generation in the limit whose outputs have strictly positive density in $K$. We also study the internal representations built by these algorithms, specifically the sequence of hypothesized candidate languages they consider, and show that achieving the strongest form of breadth may require oscillating indefinitely between high- and low-density representations. Our analysis introduces a novel topology on language families, with notions of convergence and limit points playing a key role.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRe: Personalizing LLMs via Low-Rank Reward Modeling</title>
<link>https://arxiv.org/abs/2504.14439</link>
<guid>https://arxiv.org/abs/2504.14439</guid>
<content:encoded><![CDATA[
arXiv:2504.14439v1 Announce Type: cross 
Abstract: Personalizing large language models (LLMs) to accommodate diverse user preferences is essential for enhancing alignment and user satisfaction. Traditional reinforcement learning from human feedback (RLHF) approaches often rely on monolithic value representations, limiting their ability to adapt to individual preferences. We introduce a novel framework that leverages low-rank preference modeling to efficiently learn and generalize user-specific reward functions. By representing reward functions in a low-dimensional subspace and modeling individual preferences as weighted combinations of shared basis functions, our approach avoids rigid user categorization while enabling scalability and few-shot adaptation. We validate our method on multiple preference datasets, demonstrating superior generalization to unseen users and improved accuracy in preference prediction tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey</title>
<link>https://arxiv.org/abs/2504.14520</link>
<guid>https://arxiv.org/abs/2504.14520</guid>
<content:encoded><![CDATA[
arXiv:2504.14520v1 Announce Type: cross 
Abstract: This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding</title>
<link>https://arxiv.org/abs/2504.14526</link>
<guid>https://arxiv.org/abs/2504.14526</guid>
<content:encoded><![CDATA[
arXiv:2504.14526v1 Announce Type: cross 
Abstract: Vision Large Language Models (VLLMs) have demonstrated impressive capabilities in general visual tasks such as image captioning and visual question answering. However, their effectiveness in specialized, safety-critical domains like autonomous driving remains largely unexplored. Autonomous driving systems require sophisticated scene understanding in complex environments, yet existing multimodal benchmarks primarily focus on normal driving conditions, failing to adequately assess VLLMs' performance in safety-critical scenarios. To address this, we introduce DVBench, a pioneering benchmark designed to evaluate the performance of VLLMs in understanding safety-critical driving videos. Built around a hierarchical ability taxonomy that aligns with widely adopted frameworks for describing driving scenarios used in assessing highly automated driving systems, DVBench features 10,000 multiple-choice questions with human-annotated ground-truth answers, enabling a comprehensive evaluation of VLLMs' capabilities in perception and reasoning. Experiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal significant performance gaps, with no model achieving over 40% accuracy, highlighting critical limitations in understanding complex driving scenarios. To probe adaptability, we fine-tuned selected models using domain-specific data from DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage points, with relative improvements of up to 43.59%. This improvement underscores the necessity of targeted adaptation to bridge the gap between general-purpose VLLMs and mission-critical driving applications. DVBench establishes an essential evaluation framework and research roadmap for developing VLLMs that meet the safety and robustness requirements for real-world autonomous systems. We released the benchmark toolbox and the fine-tuned model at: https://github.com/tong-zeng/DVBench.git.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models</title>
<link>https://arxiv.org/abs/2504.14594</link>
<guid>https://arxiv.org/abs/2504.14594</guid>
<content:encoded><![CDATA[
arXiv:2504.14594v1 Announce Type: cross 
Abstract: Seeking dietary guidance often requires navigating complex professional knowledge while accommodating individual health conditions. Knowledge Graphs (KGs) offer structured and interpretable nutritional information, whereas Large Language Models (LLMs) naturally facilitate conversational recommendation delivery. In this paper, we present HealthGenie, an interactive system that combines the strengths of LLMs and KGs to provide personalized dietary recommendations along with hierarchical information visualization for a quick and intuitive overview. Upon receiving a user query, HealthGenie performs query refinement and retrieves relevant information from a pre-built KG. The system then visualizes and highlights pertinent information, organized by defined categories, while offering detailed, explainable recommendation rationales. Users can further tailor these recommendations by adjusting preferences interactively. Our evaluation, comprising a within-subject comparative experiment and an open-ended discussion, demonstrates that HealthGenie effectively supports users in obtaining personalized dietary guidance based on their health conditions while reducing interaction effort and cognitive load. These findings highlight the potential of LLM-KG integration in supporting decision-making through explainable and visualized information. We examine the system's usefulness and effectiveness with an N=12 within-subject study and provide design considerations for future systems that integrate conversational LLM and KG.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk Assessment Framework for Code LLMs via Leveraging Internal States</title>
<link>https://arxiv.org/abs/2504.14640</link>
<guid>https://arxiv.org/abs/2504.14640</guid>
<content:encoded><![CDATA[
arXiv:2504.14640v1 Announce Type: cross 
Abstract: The pre-training paradigm plays a key role in the success of Large Language Models (LLMs), which have been recognized as one of the most significant advancements of AI recently. Building on these breakthroughs, code LLMs with advanced coding capabilities bring huge impacts on software engineering, showing the tendency to become an essential part of developers' daily routines. However, the current code LLMs still face serious challenges related to trustworthiness, as they can generate incorrect, insecure, or unreliable code. Recent exploratory studies find that it can be promising to detect such risky outputs by analyzing LLMs' internal states, akin to how the human brain unconsciously recognizes its own mistakes. Yet, most of these approaches are limited to narrow sub-domains of LLM operations and fall short of achieving industry-level scalability and practicability. To address these challenges, in this paper, we propose PtTrust, a two-stage risk assessment framework for code LLM based on internal state pre-training, designed to integrate seamlessly with the existing infrastructure of software companies. The core idea is that the risk assessment framework could also undergo a pre-training process similar to LLMs. Specifically, PtTrust first performs unsupervised pre-training on large-scale unlabeled source code to learn general representations of LLM states. Then, it uses a small, labeled dataset to train a risk predictor. We demonstrate the effectiveness of PtTrust through fine-grained, code line-level risk assessment and demonstrate that it generalizes across tasks and different programming languages. Further experiments also reveal that PtTrust provides highly intuitive and interpretable features, fostering greater user trust. We believe PtTrust makes a promising step toward scalable and trustworthy assurance for code LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs</title>
<link>https://arxiv.org/abs/2504.14655</link>
<guid>https://arxiv.org/abs/2504.14655</guid>
<content:encoded><![CDATA[
arXiv:2504.14655v1 Announce Type: cross 
Abstract: We introduce LeetCodeDataset, a high-quality benchmark for evaluating and training code-generation models, addressing two key challenges in LLM research: the lack of reasoning-focused coding benchmarks and self-contained training testbeds. By curating LeetCode Python problems with rich metadata, broad coverage, 100+ test cases per problem, and temporal splits (pre/post July 2024), our dataset enables contamination-free evaluation and efficient supervised fine-tuning (SFT). Experiments show reasoning models significantly outperform non-reasoning counterparts, while SFT with only 2.6K model-generated solutions achieves performance comparable to 110K-sample counterparts. The dataset and evaluation framework are available on Hugging Face and Github.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities</title>
<link>https://arxiv.org/abs/2504.14773</link>
<guid>https://arxiv.org/abs/2504.14773</guid>
<content:encoded><![CDATA[
arXiv:2504.14773v1 Announce Type: cross 
Abstract: Planning is central to agents and agentic AI. The ability to plan, e.g., creating travel itineraries within a budget, holds immense potential in both scientific and commercial contexts. Moreover, optimal plans tend to require fewer resources compared to ad-hoc methods. To date, a comprehensive understanding of existing planning benchmarks appears to be lacking. Without it, comparing planning algorithms' performance across domains or selecting suitable algorithms for new scenarios remains challenging. In this paper, we examine a range of planning benchmarks to identify commonly used testbeds for algorithm development and highlight potential gaps. These benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation. Our study recommends the most appropriate benchmarks for various algorithms and offers insights to guide future benchmark development.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Completing A Systematic Review in Hours instead of Months with Interactive AI Agents</title>
<link>https://arxiv.org/abs/2504.14822</link>
<guid>https://arxiv.org/abs/2504.14822</guid>
<content:encoded><![CDATA[
arXiv:2504.14822v1 Announce Type: cross 
Abstract: Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG</title>
<link>https://arxiv.org/abs/2504.14858</link>
<guid>https://arxiv.org/abs/2504.14858</guid>
<content:encoded><![CDATA[
arXiv:2504.14858v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) has emerged as a foundational paradigm for knowledge-grounded text generation. However, existing RAG pipelines often fail to ensure that the reasoning trajectories align with the evidential constraints imposed by retrieved content. In this paper, we reframe RAG as a problem of retrieval-aware reasoning and identify a core challenge: reasoning misalignment-the mismatch between a model's reasoning trajectory and the retrieved evidence. To address this challenge, we propose AlignRAG, a novel test-time framework that mitigates reasoning misalignment through iterative Critique-Driven Alignment (CDA) steps. In contrast to prior approaches that rely on static training or post-hoc selection, AlignRAG actively refines reasoning trajectories during inference by enforcing fine-grained alignment with evidence. Our framework introduces a new paradigm for retrieval-aware reasoning by: (1) constructing context-rich training corpora; (2) generating contrastive critiques from preference-aware reasoning trajectories; (3) training a dedicated \textit{Critic Language Model (CLM)} to identify reasoning misalignments; and (4) applying CDA steps to optimize reasoning trajectories iteratively. Empirical results demonstrate that AlignRAG consistently outperforms all baselines and could integrate as a plug-and-play module into existing RAG pipelines without further changes. By reconceptualizing RAG as a structured reasoning trajectory and establishing the test-time framework for correcting reasoning misalignments in RAG, AlignRAG provides practical advancements for retrieval-aware generation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OTC: Optimal Tool Calls via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.14870</link>
<guid>https://arxiv.org/abs/2504.14870</guid>
<content:encoded><![CDATA[
arXiv:2504.14870v1 Announce Type: cross 
Abstract: Tool-integrated reasoning (TIR) augments large language models (LLMs) with the ability to invoke external tools, such as search engines and code interpreters, to solve tasks beyond the capabilities of language-only reasoning. While reinforcement learning (RL) has shown promise in improving TIR by optimizing final answer correctness, existing approaches often overlook the efficiency and cost associated with tool usage. This can lead to suboptimal behavior, including excessive tool calls that increase computational and financial overhead, or insufficient tool use that compromises answer quality. In this work, we propose Optimal Tool Call-controlled Policy Optimization (OTC-PO), a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool efficiency, promoting high tool productivity. We instantiate this framework within both Proximal Policy Optimization (PPO) and Group Relative Preference Optimization (GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and Qwen-Math across multiple QA benchmarks show that our approach reduces tool calls by up to 73.1\% and improves tool productivity by up to 229.4\%, while maintaining comparable answer accuracy. To the best of our knowledge, this is the first RL-based framework that explicitly optimizes tool-use efficiency in TIR.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform</title>
<link>https://arxiv.org/abs/2504.14904</link>
<guid>https://arxiv.org/abs/2504.14904</guid>
<content:encoded><![CDATA[
arXiv:2504.14904v1 Announce Type: cross 
Abstract: Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework</title>
<link>https://arxiv.org/abs/2504.14928</link>
<guid>https://arxiv.org/abs/2504.14928</guid>
<content:encoded><![CDATA[
arXiv:2504.14928v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v1 Announce Type: cross 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning (RL) with simple rule-based rewards. However, existing zero-RL approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Notably, we propose policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Remarkably, LUFFY achieves an over +7.0 average gain across six math benchmarks and an advantage of over +6.2 points in out-of-distribution tasks. It also substantially surpasses imitation-based supervised fine-tuning (SFT), particularly in generalization. Analysis shows LUFFY not only imitates effectively but also explores beyond demonstrations, offering a scalable path to train generalizable reasoning models with off-policy guidance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models</title>
<link>https://arxiv.org/abs/2504.15068</link>
<guid>https://arxiv.org/abs/2504.15068</guid>
<content:encoded><![CDATA[
arXiv:2504.15068v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly enhanced the capabilities of information access systems, especially with retrieval-augmented generation (RAG). Nevertheless, the evaluation of RAG systems remains a barrier to continued progress, a challenge we tackle in this work by proposing an automatic evaluation framework that is validated against human annotations. We believe that the nugget evaluation methodology provides a solid foundation for evaluating RAG systems. This approach, originally developed for the TREC Question Answering (QA) Track in 2003, evaluates systems based on atomic facts that should be present in good answers. Our efforts focus on "refactoring" this methodology, where we describe the AutoNuggetizer framework that specifically applies LLMs to both automatically create nuggets and automatically assign nuggets to system answers. In the context of the TREC 2024 RAG Track, we calibrate a fully automatic approach against strategies where nuggets are created manually or semi-manually by human assessors and then assigned manually to system answers. Based on results from a community-wide evaluation, we observe strong agreement at the run level between scores derived from fully automatic nugget evaluation and human-based variants. The agreement is stronger when individual framework components such as nugget assignment are automated independently. This suggests that our evaluation framework provides tradeoffs between effort and quality that can be used to guide the development of future RAG systems. However, further research is necessary to refine our approach, particularly in establishing robust per-topic agreement to diagnose system failures effectively.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis</title>
<link>https://arxiv.org/abs/2504.15072</link>
<guid>https://arxiv.org/abs/2504.15072</guid>
<content:encoded><![CDATA[
arXiv:2504.15072v1 Announce Type: cross 
Abstract: The rapid development of social media has significantly reshaped the dynamics of public opinion, resulting in complex interactions that traditional models fail to effectively capture. To address this challenge, we propose an innovative approach that integrates multi-dimensional Hawkes processes with Graph Neural Network, modeling opinion propagation dynamics among nodes in a social network while considering the intricate hierarchical relationships between comments. The extended multi-dimensional Hawkes process captures the hierarchical structure, multi-dimensional interactions, and mutual influences across different topics, forming a complex propagation network. Moreover, recognizing the lack of high-quality datasets capable of comprehensively capturing the evolution of public opinion dynamics, we introduce a new dataset, VISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015 second-level comments, and 29,578 third-level comments, covering diverse domains such as politics, entertainment, sports, health, and medicine. The dataset is annotated with detailed sentiment labels across 11 categories and clearly defined hierarchical relationships. When combined with our method, it offers strong interpretability by linking sentiment propagation to the comment hierarchy and temporal evolution. Our approach provides a robust baseline for future research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking</title>
<link>https://arxiv.org/abs/2504.15135</link>
<guid>https://arxiv.org/abs/2504.15135</guid>
<content:encoded><![CDATA[
arXiv:2504.15135v1 Announce Type: cross 
Abstract: Entity linking (EL) aligns textual mentions with their corresponding entities in a knowledge base, facilitating various applications such as semantic search and question answering. Recent advances in multimodal entity linking (MEL) have shown that combining text and images can reduce ambiguity and improve alignment accuracy. However, most existing MEL methods overlook the rich structural information available in the form of knowledge-graph (KG) triples. In this paper, we propose KGMEL, a novel framework that leverages KG triples to enhance MEL. Specifically, it operates in three stages: (1) Generation: Produces high-quality triples for each mention by employing vision-language models based on its text and images. (2) Retrieval: Learns joint mention-entity representations, via contrastive learning, that integrate text, images, and (generated or KG) triples to retrieve candidate entities for each mention. (3) Reranking: Refines the KG triples of the candidate entities and employs large language models to identify the best-matching entity for the mention. Extensive experiments on benchmark datasets demonstrate that KGMEL outperforms existing methods. Our code and datasets are available at: https://github.com/juyeonnn/KGMEL.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
arXiv:2504.15254v1 Announce Type: cross 
Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roll the dice &amp; look before you leap: Going beyond the creative limits of next-token prediction</title>
<link>https://arxiv.org/abs/2504.15266</link>
<guid>https://arxiv.org/abs/2504.15266</guid>
<content:encoded><![CDATA[
arXiv:2504.15266v1 Announce Type: cross 
Abstract: We design a suite of minimal algorithmic tasks that are a loose abstraction of open-ended real-world tasks. This allows us to cleanly and controllably quantify the creative limits of the present-day language model. Much like real-world tasks that require a creative, far-sighted leap of thought, our tasks require an implicit, open-ended stochastic planning step that either (a) discovers new connections in an abstract knowledge graph (like in wordplay, drawing analogies, or research) or (b) constructs new patterns (like in designing math problems or new proteins). In these tasks, we empirically and conceptually argue how next-token learning is myopic and memorizes excessively; comparatively, multi-token approaches, namely teacherless training and diffusion models, excel in producing diverse and original output. Secondly, in our tasks, we find that to elicit randomness from the Transformer without hurting coherence, it is better to inject noise right at the input layer (via a method we dub hash-conditioning) rather than defer to temperature sampling from the output layer. Thus, our work offers a principled, minimal test-bed for analyzing open-ended creative skills, and offers new arguments for going beyond next-token learning and softmax-based sampling. We make part of the code available under https://github.com/chenwu98/algorithmic-creativity
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes</title>
<link>https://arxiv.org/abs/2504.15270</link>
<guid>https://arxiv.org/abs/2504.15270</guid>
<content:encoded><![CDATA[
arXiv:2504.15270v1 Announce Type: cross 
Abstract: Large Multimodal Models (LMMs) uniformly perceive video frames, creating computational inefficiency for videos with inherently varying temporal information density. This paper present \textbf{Quicksviewer}, an LMM with new perceiving paradigm that partitions a video of nonuniform density into varying cubes using Gumbel Softmax, followed by a unified resampling for each cube to achieve efficient video understanding. This simple and intuitive approach dynamically compress video online based on its temporal density, significantly reducing spatiotemporal redundancy (overall 45$\times$ compression rate), while enabling efficient training with large receptive field. We train the model from a language backbone through three progressive stages, each incorporating lengthy videos on average of 420s/1fps thanks to the perceiving efficiency. With only 0.8M total video-text samples for training, our model outperforms the direct baseline employing a fixed partitioning strategy by a maximum of 8.72 in accuracy, demonstrating the effectiveness in performance. On Video-MME, Quicksviewer achieves SOTA under modest sequence lengths using just up to 5\% of tokens per frame required by baselines. With this paradigm, scaling up the number of input frames reveals a clear power law of the model capabilities. It is also empirically verified that the segments generated by the cubing network can help for analyzing continuous events in videos.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs</title>
<link>https://arxiv.org/abs/2504.15280</link>
<guid>https://arxiv.org/abs/2504.15280</guid>
<content:encoded><![CDATA[
arXiv:2504.15280v1 Announce Type: cross 
Abstract: Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persian Abstract Meaning Representation: Annotation Guidelines and Gold Standard Dataset</title>
<link>https://arxiv.org/abs/2205.07712</link>
<guid>https://arxiv.org/abs/2205.07712</guid>
<content:encoded><![CDATA[
arXiv:2205.07712v2 Announce Type: replace 
Abstract: This paper introduces the Persian Abstract Meaning Representation (AMR) guidelines, a detailed guide for annotating Persian sentences with AMR, focusing on the necessary adaptations to fit Persian's unique syntactic structures. We discuss the development process of a Persian AMR gold standard dataset consisting of 1,562 sentences created following the guidelines. By examining the language specifications and nuances that distinguish AMR annotations of a low-resource language like Persian, we shed light on the challenges and limitations of developing a universal meaning representation framework. The guidelines and the dataset introduced in this study highlight such challenges, aiming to advance the field.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLoRE: Evaluating Logical Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2310.09107</link>
<guid>https://arxiv.org/abs/2310.09107</guid>
<content:encoded><![CDATA[
arXiv:2310.09107v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongStory: Coherent, Complete and Length Controlled Long story Generation</title>
<link>https://arxiv.org/abs/2311.15208</link>
<guid>https://arxiv.org/abs/2311.15208</guid>
<content:encoded><![CDATA[
arXiv:2311.15208v2 Announce Type: replace 
Abstract: A human author can write any length of story without losing coherence. Also, they always bring the story to a proper ending, an ability that current language models lack. In this work, we present the LongStory for coherent, complete, and length-controlled long story generation. LongStory introduces two novel methodologies: (1) the long and short-term contexts weight calibrator (CWC) and (2) long story structural positions (LSP). The CWC adjusts weights for long-term context Memory and short-term context Cheating, acknowledging their distinct roles. The LSP employs discourse tokens to convey the structural positions of a long story. Trained on three datasets with varied average story lengths, LongStory outperforms other baselines, including the strong story generator Plotmachine, in coherence, completeness, relevance, and repetitiveness. We also perform zero-shot tests on each dataset to assess the model's ability to predict outcomes beyond its training data and validate our methodology by comparing its performance with variants of our model.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models</title>
<link>https://arxiv.org/abs/2403.10258</link>
<guid>https://arxiv.org/abs/2403.10258</guid>
<content:encoded><![CDATA[
arXiv:2403.10258v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key contribution lies in demonstrating that while translation into English can boost the performance of English-centric LLMs on NLP tasks, it is not universally optimal. For culture-related tasks that need deep language understanding, prompting in the native language proves more effective as it better captures the nuances of culture and language. Our experiments expose varied behaviors across LLMs and tasks in the multilingual context, underscoring the need for a more comprehensive approach to multilingual evaluation. Therefore, we call for greater efforts in developing and evaluating LLMs that go beyond English-centric paradigms.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning Language Models with Demonstrated Feedback</title>
<link>https://arxiv.org/abs/2406.00888</link>
<guid>https://arxiv.org/abs/2406.00888</guid>
<content:encoded><![CDATA[
arXiv:2406.00888v2 Announce Type: replace 
Abstract: Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number (< 10) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. Concretely, DITTO operates by having an LLM generate examples that are presumed to be inferior to expert demonstrations. The method iteratively constructs pairwise preference relationships between these LLM-generated samples and expert demonstrations, potentially including comparisons between different training checkpoints. These constructed preference pairs are then used to train the model using a preference optimization algorithm (e.g. DPO). We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants (N = 16). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an avg. of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inverse Constitutional AI: Compressing Preferences into Principles</title>
<link>https://arxiv.org/abs/2406.06560</link>
<guid>https://arxiv.org/abs/2406.06560</guid>
<content:encoded><![CDATA[
arXiv:2406.06560v2 Announce Type: replace 
Abstract: Feedback data is widely used for fine-tuning and evaluating state-of-the-art AI models. Pairwise text preferences, where human or AI annotators select the "better" of two options, are particularly common. Such preferences are used to train (reward) models or to rank models with aggregate statistics. For many applications it is desirable to understand annotator preferences in addition to modelling them - not least because extensive prior work has shown various unintended biases in preference datasets. Yet, preference datasets remain challenging to interpret. Neither black-box reward models nor statistics can answer why one text is preferred over another. Manual interpretation of the numerous (long) response pairs is usually equally infeasible. In this paper, we introduce the Inverse Constitutional AI (ICAI) problem, formulating the interpretation of pairwise text preference data as a compression task. In constitutional AI, a set of principles (a constitution) is used to provide feedback and fine-tune AI models. ICAI inverts this process: given a feedback dataset, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding ICAI algorithm and validate its generated constitutions quantitatively based on annotation reconstruction accuracy on several datasets: (a) synthetic feedback data with known principles; (b) AlpacaEval cross-annotated human feedback data; (c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse demographic groups. As a short and interpretable representation of the original dataset, generated constitutions have many potential use cases: help identify undesirable annotator biases, understand model performance better, scale feedback to unseen data, or adapt models to individual user or group preferences. We release the source code at https://github.com/rdnfn/icai.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition</title>
<link>https://arxiv.org/abs/2406.11192</link>
<guid>https://arxiv.org/abs/2406.11192</guid>
<content:encoded><![CDATA[
arXiv:2406.11192v3 Announce Type: replace 
Abstract: Open Named Entity Recognition (NER), which involves identifying arbitrary types of entities from arbitrary domains, remains challenging for Large Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on extensive NER data can boost their performance. However, training directly on existing datasets neglects their inconsistent entity definitions and redundant data, limiting LLMs to dataset-specific learning and hindering out-of-domain adaptation. To address this, we present B2NERD, a compact dataset designed to guide LLMs' generalization in Open NER under a universal entity taxonomy. B2NERD is refined from 54 existing English and Chinese datasets using a two-step process. First, we detect inconsistent entity definitions across datasets and clarify them by distinguishable label names to construct a universal taxonomy of 400+ entity types. Second, we address redundancy using a data pruning strategy that selects fewer samples with greater category and semantic diversity. Comprehensive evaluation shows that B2NERD significantly enhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD, outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3 out-of-domain benchmarks across 15 datasets and 6 languages. The data, models, and code are publicly available at https://github.com/UmeanNever/B2NER.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Knowledge Graph Question Answering: A Survey</title>
<link>https://arxiv.org/abs/2406.14191</link>
<guid>https://arxiv.org/abs/2406.14191</guid>
<content:encoded><![CDATA[
arXiv:2406.14191v3 Announce Type: replace 
Abstract: Knowledge Base Question Answering (KBQA) has been a long-standing field to answer questions based on knowledge bases. Recently, the evolving dynamics of knowledge have attracted a growing interest in Temporal Knowledge Graph Question Answering (TKGQA), an emerging task to answer temporal questions. However, this field grapples with ambiguities in defining temporal questions and lacks a systematic categorization of existing methods for TKGQA. In response, this paper provides a thorough survey from two perspectives: the taxonomy of temporal questions and the methodological categorization for TKGQA. Specifically, we first establish a detailed taxonomy of temporal questions engaged in prior studies. Subsequently, we provide a comprehensive review of TKGQA techniques of two categories: semantic parsing-based and TKG embedding-based. Building on this review, the paper outlines potential research directions aimed at advancing the field of TKGQA. This work aims to serve as a comprehensive reference for TKGQA and to stimulate further research.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveBench: A Challenging, Contamination-Limited LLM Benchmark</title>
<link>https://arxiv.org/abs/2406.19314</link>
<guid>https://arxiv.org/abs/2406.19314</guid>
<content:encoded><![CDATA[
arXiv:2406.19314v2 Announce Type: replace 
Abstract: Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be resistant to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-limited versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 70% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training on the Test Task Confounds Evaluation and Emergence</title>
<link>https://arxiv.org/abs/2407.07890</link>
<guid>https://arxiv.org/abs/2407.07890</guid>
<content:encoded><![CDATA[
arXiv:2407.07890v3 Announce Type: replace 
Abstract: We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice. Rather, the term describes a growing set of practices that utilize knowledge about evaluation tasks at training time. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data prior to evaluation. We then show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models, with broad implications for benchmarking and the study of emergent capabilities.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models</title>
<link>https://arxiv.org/abs/2408.06631</link>
<guid>https://arxiv.org/abs/2408.06631</guid>
<content:encoded><![CDATA[
arXiv:2408.06631v4 Announce Type: replace 
Abstract: End-to-end interpretation currently dominates the remote sensing fine-grained ship classification (RS-FGSC) task. However, the inference process remains uninterpretable, leading to criticisms of these models as "black box" systems. To address this issue, we propose a domain knowledge-enhanced Chain-of-Thought (CoT) prompt generation mechanism, which is used to semi-automatically construct a task-specific instruction-following dataset, TITANIC-FGS. By training on TITANIC-FGS, we adapt general-domain vision-language models (VLMs) to the FGSC task, resulting in a model named IFShip. Building upon IFShip, we develop an FGSC visual chatbot that redefines the FGSC problem as a step-by-step reasoning task and conveys the reasoning process in natural language. Experimental results show that IFShip outperforms state-of-the-art FGSC algorithms in both interpretability and classification accuracy. Furthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates superior performance on the FGSC task. It provides an accurate chain of reasoning when fine-grained ship types are recognizable to the human eye and offers interpretable explanations when they are not. Our dataset is publicly available at: https://github.com/lostwolves/IFShip.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal Aspect-based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2408.15379</link>
<guid>https://arxiv.org/abs/2408.15379</guid>
<content:encoded><![CDATA[
arXiv:2408.15379v3 Announce Type: replace 
Abstract: Multimodal Aspect-based Sentiment Analysis (MABSA) enhances sentiment detection by integrating textual data with complementary modalities, such as images, to provide a more refined and comprehensive understanding of sentiment. However, conventional attention mechanisms, despite notable benchmarks, are hindered by quadratic complexity, limiting their ability to fully capture global contextual dependencies and rich semantic information in both modalities. To address this limitation, we introduce DualKanbaFormer, a novel framework that leverages parallel Textual and Visual KanbaFormer modules for robust multimodal analysis. Our approach incorporates Aspect-Driven Sparse Attention (ADSA) to dynamically balance coarse-grained aggregation and fine-grained selection for aspect-focused precision, ensuring the preservation of both global context awareness and local precision in textual and visual representations. Additionally, we utilize the Selective State Space Model (Mamba) to capture extensive global semantic information across both modalities. Furthermore, We replace traditional feed-forward networks and normalization with Kolmogorov-Arnold Networks (KANs) and Dynamic Tanh (DyT) to enhance non-linear expressivity and inference stability. To facilitate the effective integration of textual and visual features, we design a multimodal gated fusion layer that dynamically optimizes inter-modality interactions, significantly enhancing the models efficacy in MABSA tasks. Comprehensive experiments on two publicly available datasets reveal that DualKanbaFormer consistently outperforms several state-of-the-art (SOTA) models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-evolving Agents with reflective and memory-augmented abilities</title>
<link>https://arxiv.org/abs/2409.00872</link>
<guid>https://arxiv.org/abs/2409.00872</guid>
<content:encoded><![CDATA[
arXiv:2409.00872v2 Announce Type: replace 
Abstract: Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents' capabilities in handling multi-tasking and long-span information.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2409.01035</link>
<guid>https://arxiv.org/abs/2409.01035</guid>
<content:encoded><![CDATA[
arXiv:2409.01035v4 Announce Type: replace 
Abstract: Large language models demonstrate impressive performance on downstream tasks, yet they require extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. In this paper, we delve into the concept of task-specific directions (TSDs), which are critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Additionally, based on our exploration of TSD, we focus on an important issue in PEFT: the initialization of LoRA. While some works have pointed out the significance of initialization for LoRA's performance and proposed various strategies, these methods are often empirical and not task-specific. To address this issue, we propose LoRA-Init. Starting from TSD, we identify the directions that require the most adjustment during fine-tuning for downstream tasks. By initializing the matrices in LoRA with these directions, LoRA-Init significantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash and LoRA-Init to create the final version of LoRA based on TSDs, which we refer to as LoRA-TSD. Extensive experiments have conclusively demonstrated the effectiveness of these methods, and in-depth analyses further reveal the underlying mechanisms behind their success.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seek and Solve Reasoning for Table Question Answering</title>
<link>https://arxiv.org/abs/2409.05286</link>
<guid>https://arxiv.org/abs/2409.05286</guid>
<content:encoded><![CDATA[
arXiv:2409.05286v3 Announce Type: replace 
Abstract: The complexities of table structures and question logic make table-based question answering (TQA) tasks challenging for Large Language Models (LLMs), often requiring task simplification before solving. This paper reveals that the reasoning process during task simplification may be more valuable than the simplified tasks themselves and aims to improve TQA performance by leveraging LLMs' reasoning capabilities. We propose a Seek-and-Solve pipeline that instructs the LLM to first seek relevant information and then answer questions, integrating these two stages at the reasoning level into a coherent Seek-and-Solve Chain of Thought (SS-CoT). Additionally, we distill a single-step TQA-solving prompt from this pipeline, using demonstrations with SS-CoT paths to guide the LLM in solving complex TQA tasks under In-Context Learning settings. Our experiments show that our approaches result in improved performance and reliability while being efficient. Our findings emphasize the importance of eliciting LLMs' reasoning capabilities to handle complex TQA tasks effectively.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval</title>
<link>https://arxiv.org/abs/2410.04585</link>
<guid>https://arxiv.org/abs/2410.04585</guid>
<content:encoded><![CDATA[
arXiv:2410.04585v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Training Data of Large Language Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2410.07582</link>
<guid>https://arxiv.org/abs/2410.07582</guid>
<content:encoded><![CDATA[
arXiv:2410.07582v2 Announce Type: replace 
Abstract: The advancement of large language models has grown parallel to the opacity of their training data. Membership inference attacks (MIAs) aim to determine whether specific data was used to train a model. They offer valuable insights into detecting data contamination and ensuring compliance with privacy and copyright standards. However, MIA for LLMs is challenging due to the massive scale of training data and the inherent ambiguity of membership in texts. Moreover, creating realistic MIA evaluation benchmarks is difficult as training and test data distributions are often unknown. We introduce EM-MIA, a novel membership inference method that iteratively refines membership scores and prefix scores via an expectation-maximization algorithm. Our approach leverages the observation that these scores can improve each other: membership scores help identify effective prefixes for detecting training data, while prefix scores help determine membership. As a result, EM-MIA achieves state-of-the-art results on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a benchmark built from OLMo resources, which allows controlling task difficulty through varying degrees of overlap between training and test data distributions. Our experiments demonstrate EM-MIA is robust across different scenarios while also revealing fundamental limitations of current MIA approaches when member and non-member distributions are nearly identical.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nudging: Inference-time Alignment of LLMs via Guided Decoding</title>
<link>https://arxiv.org/abs/2410.09300</link>
<guid>https://arxiv.org/abs/2410.09300</guid>
<content:encoded><![CDATA[
arXiv:2410.09300v3 Announce Type: replace 
Abstract: Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose nudging, a simple, plug-and-play, and training-free algorithm that aligns any base model at inference time using a small aligned model. Nudging is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, nudging employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high. We evaluate nudging across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, nudging enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our project website: https://fywalter.github.io/nudging/ .
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus</title>
<link>https://arxiv.org/abs/2410.14815</link>
<guid>https://arxiv.org/abs/2410.14815</guid>
<content:encoded><![CDATA[
arXiv:2410.14815v2 Announce Type: replace 
Abstract: Multilingual LLMs support a variety of languages; however, their performance is suboptimal for low-resource languages. In this work, we emphasize the importance of continued pre-training of multilingual LLMs and the use of translation-based synthetic pre-training corpora for improving LLMs in low-resource languages. We conduct our study in the context of the low-resource Indic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM supporting both Hindi and English, based on Nemotron-Mini 4B. The model is trained using a mix of real and synthetic Hindi + English tokens, with continuous pre-training performed on 400B tokens. We demonstrate that both the base and instruct models achieve state-of-the-art results on Hindi benchmarks while remaining competitive on English tasks. Additionally, we observe that the continued pre-training approach enhances the model's overall factual accuracy. We perform an ablation study to highlight the impact of Hindi pre-training, showing significant improvements in Hindi chat capabilities and factual accuracy, which cannot be achieved through Hindi alignment alone.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment</title>
<link>https://arxiv.org/abs/2410.16714</link>
<guid>https://arxiv.org/abs/2410.16714</guid>
<content:encoded><![CDATA[
arXiv:2410.16714v3 Announce Type: replace 
Abstract: Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a preference-based, two-player constant-sum game. However, existing methods either guarantee only average-iterate convergence, incurring high storage and inference costs, or converge to the NE of a regularized game, failing to accurately reflect true human preferences. In this paper, we introduce Magnetic Preference Optimization (MPO), a novel approach capable of achieving last-iterate convergence to the NE of the original game, effectively overcoming the limitations of existing methods. Building upon Magnetic Mirror Descent (MMD), MPO attains a linear convergence rate, making it particularly suitable for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and practically viable, we present a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. Empirical results demonstrate that MPO can significantly enhance the performance of LLMs, highlighting the potential of self-play methods in alignment.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2410.20941</link>
<guid>https://arxiv.org/abs/2410.20941</guid>
<content:encoded><![CDATA[
arXiv:2410.20941v4 Announce Type: replace 
Abstract: Large language models (LLMs) have excelled in various NLP tasks, including machine translation (MT), yet most studies focus on sentence-level translation. This work investigates the inherent capability of instruction-tuned LLMs for document-level translation (docMT). Unlike prior approaches that require specialized techniques, we evaluate LLMs by directly prompting them to translate entire documents in a single pass. Our results show that this method improves translation quality compared to translating sentences separately, even without document-level fine-tuning. However, this advantage is not reflected in BLEU scores, which often favor sentence-based translations. We propose using the LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess document coherence, accuracy, and fluency in a more nuanced way than n-gram-based metrics. Overall, our work demonstrates that instruction-tuned LLMs can effectively leverage document context for translation. However, we caution against using BLEU scores for evaluating docMT, as they often provide misleading outcomes, failing to capture the quality of document-level translation. Code and the outputs from GPT4-as-a-judge are available at https://github.com/EIT-NLP/BLEUless_DocMT
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents</title>
<link>https://arxiv.org/abs/2411.00927</link>
<guid>https://arxiv.org/abs/2411.00927</guid>
<content:encoded><![CDATA[
arXiv:2411.00927v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents are increasingly employed to interact with external environments (e.g., games, APIs, world models) to solve user-provided tasks. However, current frameworks often lack the ability to collaborate effectively with users in fully conversational settings. Conversations are essential for aligning on task details, achieving user-defined goals, and satisfying preferences. While existing agents address ambiguity through clarification questions, they underutilize the broader potential of an LLM's conversational capabilities. In this work, we introduce ReSpAct, an LLM-based agent designed to seamlessly integrate reasoning, decision-making, and dynamic dialogue for task-solving. Expanding on reasoning-first approaches like ReAct, ReSpAct employs active, free-flowing dialogues to interpret instructions, clarify goals, provide status updates, resolve subtask failures, and refine plans based on user inputs without any explicit dialogue schema. By alternating between task-solving actions and interactive conversations, ReSpAct demonstrates improved performance across diverse environments. We evaluate ReSpAct in user-interactive settings, including task-oriented dialogue systems (MultiWOZ) and decision-making tasks (ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate improvements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a 5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results highlight the value of integrating dynamic user-agent collaboration for more effective task resolution.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHATTER: A Character Attribution Dataset for Narrative Understanding</title>
<link>https://arxiv.org/abs/2411.05227</link>
<guid>https://arxiv.org/abs/2411.05227</guid>
<content:encoded><![CDATA[
arXiv:2411.05227v2 Announce Type: replace 
Abstract: Computational narrative understanding studies the identification, description, and interaction of the elements of a narrative: characters, attributes, events, and relations. Narrative research has given considerable attention to defining and classifying character types. However, these character-type taxonomies do not generalize well because they are small, too simple, or specific to a domain. We require robust and reliable benchmarks to test whether narrative models truly understand the nuances of the character's development in the story. Our work addresses this by curating the CHATTER dataset that labels whether a character portrays some attribute for 88124 character-attribute pairs, encompassing 2998 characters, 12967 attributes and 660 movies. We validate a subset of CHATTER, called CHATTEREVAL, using human annotations to serve as a benchmark to evaluate the character attribution task in movie scripts. \evaldataset{} also assesses narrative understanding and the long-context modeling capacity of language models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactLens: Benchmarking Fine-Grained Fact Verification</title>
<link>https://arxiv.org/abs/2411.05980</link>
<guid>https://arxiv.org/abs/2411.05980</guid>
<content:encoded><![CDATA[
arXiv:2411.05980v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive capability in language generation and understanding, but their tendency to hallucinate and produce factually incorrect information remains a key limitation. To verify LLM-generated contents and claims from other sources, traditional verification approaches often rely on holistic models that assign a single factuality label to complex claims, potentially obscuring nuanced errors. In this paper, we advocate for a shift toward fine-grained verification, where complex claims are broken down into smaller sub-claims for individual verification, allowing for more precise identification of inaccuracies, improved transparency, and reduced ambiguity in evidence retrieval. However, generating sub-claims poses challenges, such as maintaining context and ensuring semantic equivalence with respect to the original claim. We introduce FactLens, a benchmark for evaluating fine-grained fact verification, with metrics and automated evaluators of sub-claim quality. The benchmark data is manually curated to ensure high-quality ground truth. Our results show alignment between automated FactLens evaluators and human judgments, and we discuss the impact of sub-claim characteristics on the overall verification performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Star Attention: Efficient LLM Inference over Long Sequences</title>
<link>https://arxiv.org/abs/2411.17116</link>
<guid>https://arxiv.org/abs/2411.17116</guid>
<content:encoded><![CDATA[
arXiv:2411.17116v2 Announce Type: replace 
Abstract: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation</title>
<link>https://arxiv.org/abs/2412.01626</link>
<guid>https://arxiv.org/abs/2412.01626</guid>
<content:encoded><![CDATA[
arXiv:2412.01626v3 Announce Type: replace 
Abstract: The use of Large Language Models (LLMs) has increased significantly with users frequently asking questions to chatbots. In the time when information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WikiHint, which is based on Wikipedia and includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs for hint generation in answer-aware and answer-agnostic contexts. We assess the effectiveness of the hints with human participants who answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HintRank, to evaluate and rank hints in both answer-aware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints, (b) including answer information along with questions generally improves the quality of generated hints, and (c) encoder-based models perform better than decoder-based models in hint ranking.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unanswerability Evaluation for Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2412.12300</link>
<guid>https://arxiv.org/abs/2412.12300</guid>
<content:encoded><![CDATA[
arXiv:2412.12300v3 Announce Type: replace 
Abstract: Existing evaluation frameworks for retrieval-augmented generation (RAG) systems focus on answerable queries, but they overlook the importance of appropriately rejecting unanswerable requests. In this paper, we introduce UAEval4RAG, a framework designed to evaluate whether RAG systems can handle unanswerable queries effectively. We define a taxonomy with six unanswerable categories, and UAEval4RAG automatically synthesizes diverse and challenging queries for any given knowledge base with unanswered ratio and acceptable ratio metrics. We conduct experiments with various RAG components, including retrieval models, rewriting methods, rerankers, language models, and prompting strategies, and reveal hidden trade-offs in performance of RAG systems. Our findings highlight the critical role of component selection and prompt design in optimizing RAG systems to balance the accuracy of answerable queries with high rejection rates of unanswerable ones. UAEval4RAG provides valuable insights and tools for developing more robust and reliable RAG systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>State Space Models are Strong Text Rerankers</title>
<link>https://arxiv.org/abs/2412.14354</link>
<guid>https://arxiv.org/abs/2412.14354</guid>
<content:encoded><![CDATA[
arXiv:2412.14354v2 Announce Type: replace 
Abstract: Transformers dominate NLP and IR; but their inference inefficiencies and challenges in extrapolating to longer contexts have sparked interest in alternative model architectures. Among these, state space models (SSMs) like Mamba offer promising advantages, particularly $O(1)$ time complexity in inference. Despite their potential, SSMs' effectiveness at text reranking\, -- \,a task requiring fine-grained query-document interaction and long-context understanding\, -- \,remains underexplored. This study benchmarks SSM-based architectures (specifically, Mamba-1 and Mamba-2) against transformer-based models across various scales, architectures, and pre-training objectives, focusing on performance and efficiency in text reranking tasks. We find that (1) Mamba architectures achieve competitive text ranking performance, comparable to transformer-based models of similar size; (2) they are less efficient in training and inference compared to transformers with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance and efficiency. These results underscore the potential of state space models as a transformer alternative and highlight areas for improvement in future IR applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation</title>
<link>https://arxiv.org/abs/2501.05414</link>
<guid>https://arxiv.org/abs/2501.05414</guid>
<content:encoded><![CDATA[
arXiv:2501.05414v2 Announce Type: replace 
Abstract: Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens. We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. We evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks. Reasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training. Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations. These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement. Data and code available at: https://princeton-pli.github.io/LongProc.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond</title>
<link>https://arxiv.org/abs/2501.05714</link>
<guid>https://arxiv.org/abs/2501.05714</guid>
<content:encoded><![CDATA[
arXiv:2501.05714v2 Announce Type: replace 
Abstract: With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiom Detection in Sorani Kurdish Texts</title>
<link>https://arxiv.org/abs/2501.14528</link>
<guid>https://arxiv.org/abs/2501.14528</guid>
<content:encoded><![CDATA[
arXiv:2501.14528v3 Announce Type: replace 
Abstract: Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet</title>
<link>https://arxiv.org/abs/2502.05291</link>
<guid>https://arxiv.org/abs/2502.05291</guid>
<content:encoded><![CDATA[
arXiv:2502.05291v2 Announce Type: replace 
Abstract: Large language models (LLMs) have become ubiquitous, thus it is important to understand their risks and limitations. Smaller LLMs can be deployed where compute resources are constrained, such as edge devices, but with different propensity to generate harmful output. Mitigation of LLM harm typically depends on annotating the harmfulness of LLM output, which is expensive to collect from humans. This work studies two questions: How do smaller LLMs rank regarding generation of harmful content? How well can larger LLMs annotate harmfulness? We prompt three small LLMs to elicit harmful content of various types, such as discriminatory language, offensive content, privacy invasion, or negative influence, and collect human rankings of their outputs. Then, we evaluate three state-of-the-art large LLMs on their ability to annotate the harmfulness of these responses. We find that the smaller models differ with respect to harmfulness. We also find that large LLMs show low to moderate agreement with humans. These findings underline the need for further work on harm mitigation in LLMs.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models</title>
<link>https://arxiv.org/abs/2502.07346</link>
<guid>https://arxiv.org/abs/2502.07346</guid>
<content:encoded><![CDATA[
arXiv:2502.07346v2 Announce Type: replace 
Abstract: Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparQLe: Speech Queries to Text Translation Through LLMs</title>
<link>https://arxiv.org/abs/2502.09284</link>
<guid>https://arxiv.org/abs/2502.09284</guid>
<content:encoded><![CDATA[
arXiv:2502.09284v2 Announce Type: replace 
Abstract: With the growing influence of Large Language Models (LLMs), there is increasing interest in integrating speech representations with them to enable more seamless multi-modal processing and speech understanding. This study introduces a novel approach that leverages self-supervised speech representations in combination with instruction-tuned LLMs for speech-to-text translation. The proposed approach leverages a modality adapter to align extracted speech features with instruction-tuned LLMs using English-language data. Our experiments demonstrate that this method effectively preserves the semantic content of the input speech and serves as an effective bridge between self-supervised speech models and instruction-tuned LLMs, offering a promising solution for various speech understanding applications.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States</title>
<link>https://arxiv.org/abs/2502.14744</link>
<guid>https://arxiv.org/abs/2502.14744</guid>
<content:encoded><![CDATA[
arXiv:2502.14744v3 Announce Type: replace 
Abstract: The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</title>
<link>https://arxiv.org/abs/2502.14866</link>
<guid>https://arxiv.org/abs/2502.14866</guid>
<content:encoded><![CDATA[
arXiv:2502.14866v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores</title>
<link>https://arxiv.org/abs/2502.16358</link>
<guid>https://arxiv.org/abs/2502.16358</guid>
<content:encoded><![CDATA[
arXiv:2502.16358v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are revolutionizing information retrieval, with chatbots becoming an important source for answering user queries. As by their design, LLMs prioritize generating correct answers, the value of highly plausible yet incorrect answers (candidate answers) tends to be overlooked. However, such answers can still prove useful, for example, they can play a crucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA Robustness Assessment (QARA). Existing QA datasets primarily focus on correct answers without explicit consideration of the plausibility of other candidate answers, limiting opportunity for more nuanced evaluations of models. To address this gap, we introduce PlausibleQA, a large-scale dataset comprising 10,000 questions and 100,000 candidate answers, each annotated with plausibility scores and justifications for their selection. Additionally, the dataset includes 900,000 justifications for pairwise comparisons between candidate answers, further refining plausibility assessments. We evaluate PlausibleQA through human assessments and empirical experiments, demonstrating its utility in MCQA and QARA analysis. Our findings show that plausibility-aware approaches are effective for MCQA distractor generation and QARA. We release PlausibleQA as a resource for advancing QA research and enhancing LLM performance in distinguishing plausible distractors from correct answers.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Multiple Large Language Models: A Survey on LLM Ensemble</title>
<link>https://arxiv.org/abs/2502.18036</link>
<guid>https://arxiv.org/abs/2502.18036</guid>
<content:encoded><![CDATA[
arXiv:2502.18036v3 Announce Type: replace 
Abstract: LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Wave Functions: Exploring Meaning in Large Language Models Through Quantum Formalism</title>
<link>https://arxiv.org/abs/2503.10664</link>
<guid>https://arxiv.org/abs/2503.10664</guid>
<content:encoded><![CDATA[
arXiv:2503.10664v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) encode semantic relationships in high-dimensional vector embeddings. This paper explores the analogy between LLM embedding spaces and quantum mechanics, positing that LLMs operate within a quantized semantic space where words and phrases behave as quantum states. To capture nuanced semantic interference effects, we extend the standard real-valued embedding space to the complex domain, drawing parallels to the double-slit experiment. We introduce a "semantic wave function" to formalize this quantum-derived representation and utilize potential landscapes, such as the double-well potential, to model semantic ambiguity. Furthermore, we propose a complex-valued similarity measure that incorporates both magnitude and phase information, enabling a more sensitive comparison of semantic representations. We develop a path integral formalism, based on a nonlinear Schr\"odinger equation with a gauge field and Mexican hat potential, to model the dynamic evolution of LLM behavior. This interdisciplinary approach offers a new theoretical framework for understanding and potentially manipulating LLMs, with the goal of advancing both artificial and natural language understanding.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Halving transcription time: A fast, user-friendly and GDPR-compliant workflow to create AI-assisted transcripts for content analysis</title>
<link>https://arxiv.org/abs/2503.13031</link>
<guid>https://arxiv.org/abs/2503.13031</guid>
<content:encoded><![CDATA[
arXiv:2503.13031v2 Announce Type: replace 
Abstract: In qualitative research, data transcription is often labor-intensive and time-consuming. To expedite this process, a workflow utilizing artificial intelligence (AI) was developed. This workflow not only enhances transcription speed but also addresses the issue of AI-generated transcripts often lacking compatibility with standard content analysis software. Within this workflow, automatic speech recognition is employed to create initial transcripts from audio recordings, which are then formatted to be compatible with content analysis software such as ATLAS or MAXQDA. Empirical data from a study of 12 interviews suggests that this workflow can reduce transcription time by up to 76.4%. Furthermore, by using widely used standard software, this process is suitable for both students and researchers while also being adaptable to a variety of learning, teaching, and research environments. It is also particularly beneficial for non-native speakers. In addition, the workflow is GDPR-compliant and facilitates local, offline transcript generation, which is crucial when dealing with sensitive data.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language Anchor-Guided Method for Robust Noisy Domain Generalization</title>
<link>https://arxiv.org/abs/2503.17211</link>
<guid>https://arxiv.org/abs/2503.17211</guid>
<content:encoded><![CDATA[
arXiv:2503.17211v2 Announce Type: replace 
Abstract: Real-world machine learning applications often struggle with two major challenges: distribution shift and label noise. Models tend to overfit by focusing on redundant and uninformative features in the training data, which makes it hard for them to generalize to the target domain. Noisy data worsens this problem by causing further overfitting to the noise, meaning that existing methods often fail to tell the difference between true, invariant features and misleading, spurious ones. To tackle these issues, we introduce Anchor Alignment and Adaptive Weighting (A3W). This new algorithm uses sample reweighting guided by natural language processing (NLP) anchors to extract more representative features. In simple terms, A3W leverages semantic representations from natural language models as a source of domain-invariant prior knowledge. Additionally, it employs a weighted loss function that adjusts each sample's contribution based on its similarity to the corresponding NLP anchor. This adjustment makes the model more robust to noisy labels. Extensive experiments on standard benchmark datasets show that A3W consistently outperforms state-of-the-art domain generalization methods, offering significant improvements in both accuracy and robustness across different datasets and noise levels.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Agents That Act Like Us: Accurate Human Behavior Simulation with Real-World Data</title>
<link>https://arxiv.org/abs/2503.20749</link>
<guid>https://arxiv.org/abs/2503.20749</guid>
<content:encoded><![CDATA[
arXiv:2503.20749v4 Announce Type: replace 
Abstract: Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating and improving LLM's objective ``accuracy'' rather than the subjective ``believability'' in the web action generation task, leveraging a large-scale, real-world dataset collected from online shopping human actions. We present the first comprehensive quantitative evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action generation. Our results show that fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasoning traces into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work establishes a new benchmark for evaluating LLMs in behavior simulation and offers actionable insights into how real-world action data and reasoning augmentation can enhance the fidelity of LLM agents.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging</title>
<link>https://arxiv.org/abs/2503.21088</link>
<guid>https://arxiv.org/abs/2503.21088</guid>
<content:encoded><![CDATA[
arXiv:2503.21088v2 Announce Type: replace 
Abstract: This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4: Unlearning Sensitive Content from Large Language Models. This task aims to selectively erase sensitive knowledge from large language models, avoiding both over-forgetting and under-forgetting issues. We propose an unlearning system that leverages Model Merging (specifically TIES-Merging), combining two specialized models into a more balanced unlearned model. Our system achieves competitive results, ranking second among 26 teams, with an online score of 0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we also conduct local experiments and perform a comprehensive analysis of the unlearning process, examining performance trajectories, loss dynamics, and weight perspectives, along with several supplementary experiments, to understand the effectiveness of our method. Furthermore, we analyze the shortcomings of our method and evaluation metrics, emphasizing that MIA scores and ROUGE-based metrics alone are insufficient to fully evaluate successful unlearning. Finally, we emphasize the need for more comprehensive evaluation methodologies and rethinking of unlearning objectives in future research. Code is available at https://github.com/zjunlp/unlearn/tree/main/semeval25.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</title>
<link>https://arxiv.org/abs/2503.23512</link>
<guid>https://arxiv.org/abs/2503.23512</guid>
<content:encoded><![CDATA[
arXiv:2503.23512v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection</title>
<link>https://arxiv.org/abs/2504.00695</link>
<guid>https://arxiv.org/abs/2504.00695</guid>
<content:encoded><![CDATA[
arXiv:2504.00695v3 Announce Type: replace 
Abstract: Pre-training large language models (LLMs) necessitates enormous diverse textual corpora, making effective data selection a key challenge for balancing computational resources and model performance. Current methodologies primarily emphasize data quality metrics and mixing proportions, yet they fail to adequately capture the underlying semantic connections between training samples and quality disparities within individual domains. We introduce ToReMi (Topic-based Reweighting for Model improvement), a novel two-stage framework that dynamically adjusts training sample weights according to their topical associations and observed learning patterns. Our comprehensive experiments reveal that ToReMi variants consistently achieve superior performance over conventional pre-training approaches, demonstrating accelerated perplexity reduction across multiple domains and enhanced capabilities on downstream evaluation tasks. Code is available at https://github.com/zxx000728/ToReMi.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation</title>
<link>https://arxiv.org/abs/2504.02438</link>
<guid>https://arxiv.org/abs/2504.02438</guid>
<content:encoded><![CDATA[
arXiv:2504.02438v3 Announce Type: replace 
Abstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLaMP, a hierarchical video-language model that processes hour-long videos at ``mixed precision'' through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLaMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLaMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLaMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers</title>
<link>https://arxiv.org/abs/2504.03595</link>
<guid>https://arxiv.org/abs/2504.03595</guid>
<content:encoded><![CDATA[
arXiv:2504.03595v2 Announce Type: replace 
Abstract: A key element to support the increased amounts of renewable energy in the energy system is flexibility, i.e., the possibility of changing energy loads in time and amount. Many flexibility models have been designed; however, exact models fail to scale for long time horizons or many devices. Because of this, the FlexOffer (FOs) model has been designed, to provide device-independent approximations of flexibility with good accuracy, and much better scaling for long time horizons and many devices. An important aspect of the real-life implementation of energy flexibility is enabling flexible data exchange with many types of smart energy appliances and market systems, e.g., in smart buildings. For this, ontologies standardizing data formats are required. However, the current industry standard ontology for integrating smart devices for energy purposes, SAREF for Energy Flexibility (SAREF4ENER) only has limited support for flexibility and thus cannot support important use cases. In this paper we propose an extension of SAREF4ENER that integrates full support for the complete FlexOffer model, including advanced use cases, while maintaining backward compatibility. This novel ontology module can accurately describe flexibility for advanced devices such as electric vehicles, batteries, and heat pumps. It can also capture the inherent uncertainty associated with many flexible load types.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAACL2025 Tutorial: Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2504.03931</link>
<guid>https://arxiv.org/abs/2504.03931</guid>
<content:encoded><![CDATA[
arXiv:2504.03931v2 Announce Type: replace 
Abstract: This tutorial on adaptation of LLMs is designed to address the growing demand for models that go beyond the static capabilities of generic LLMs by providing an overview of dynamic, domain-specific, and task-adaptive LLM adaptation techniques. While general LLMs have demonstrated strong generalization across a variety of tasks, they often struggle to perform well in specialized domains such as finance, healthcare, and code generation for underrepresented languages. Additionally, their static nature limits their ability to evolve with the changing world, and they are often extremely large in size, making them impractical and costly to deploy at scale. As a result, the adaptation of LLMs has drawn much attention since the birth of LLMs and is of core importance, both for industry, which focuses on serving its targeted users, and academia, which can greatly benefit from small but powerful LLMs. To address this gap, this tutorial aims to provide an overview of the LLM adaptation techniques. We start with an introduction to LLM adaptation, from both the data perspective and the model perspective. We then emphasize how the evaluation metrics and benchmarks are different from other techniques. After establishing the problems, we explore various adaptation techniques. We categorize adaptation techniques into two main families. The first is parametric knowledge adaptation, which focuses on updating the parametric knowledge within LLMs. Additionally, we will discuss real-time adaptation techniques, including model editing, which allows LLMs to be updated dynamically in production environments. The second kind of adaptation is semi-parametric knowledge adaptation, where the goal is to update LLM parameters to better leverage external knowledge or tools through techniques like retrieval-augmented generation (RAG) and agent-based systems.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2504.04994</link>
<guid>https://arxiv.org/abs/2504.04994</guid>
<content:encoded><![CDATA[
arXiv:2504.04994v2 Announce Type: replace 
Abstract: Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the value mechanisms behind them. However, current research primarily evaluates these values through external responses with a focus on AI safety, lacking interpretability and failing to assess social values in real-world contexts. In this paper, we propose a novel framework called ValueExploration, which aims to explore the behavior-driven mechanisms of National Social Values within LLMs at the neuron level. As a case study, we focus on Chinese Social Values and first construct C-voice, a large-scale bilingual benchmark for identifying and evaluating Chinese Social Values in LLMs. By leveraging C-voice, we then identify and locate the neurons responsible for encoding these values according to activation difference. Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making. Extensive experiments on four representative LLMs validate the efficacy of our framework. The benchmark and code will be available.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games</title>
<link>https://arxiv.org/abs/2504.06868</link>
<guid>https://arxiv.org/abs/2504.06868</guid>
<content:encoded><![CDATA[
arXiv:2504.06868v2 Announce Type: replace 
Abstract: Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation</title>
<link>https://arxiv.org/abs/2504.07532</link>
<guid>https://arxiv.org/abs/2504.07532</guid>
<content:encoded><![CDATA[
arXiv:2504.07532v2 Announce Type: replace 
Abstract: AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents</title>
<link>https://arxiv.org/abs/2504.09407</link>
<guid>https://arxiv.org/abs/2504.09407</guid>
<content:encoded><![CDATA[
arXiv:2504.09407v2 Announce Type: replace 
Abstract: Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\textbf{LLM Agent}) research inspired us to design \textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families</title>
<link>https://arxiv.org/abs/2504.10340</link>
<guid>https://arxiv.org/abs/2504.10340</guid>
<content:encoded><![CDATA[
arXiv:2504.10340v2 Announce Type: replace 
Abstract: Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings -- extracted via an LLM-assisted annotation pipeline -- serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Knowledge Manipulation in a Russian Wikipedia Fork</title>
<link>https://arxiv.org/abs/2504.10663</link>
<guid>https://arxiv.org/abs/2504.10663</guid>
<content:encoded><![CDATA[
arXiv:2504.10663v2 Announce Type: replace 
Abstract: Wikipedia is powered by MediaWiki, a free and open-source software that is also the infrastructure for many other wiki-based online encyclopedias. These include the recently launched website Ruwiki, which has copied and modified the original Russian Wikipedia content to conform to Russian law. To identify practices and narratives that could be associated with different forms of knowledge manipulation, this article presents an in-depth analysis of this Russian Wikipedia fork. We propose a methodology to characterize the main changes with respect to the original version. The foundation of this study is a comprehensive comparative analysis of more than 1.9M articles from Russian Wikipedia and its fork. Using meta-information and geographical, temporal, categorical, and textual features, we explore the changes made by Ruwiki editors. Furthermore, we present a classification of the main topics of knowledge manipulation in this fork, including a numerical estimation of their scope. This research not only sheds light on significant changes within Ruwiki, but also provides a methodology that could be applied to analyze other Wikipedia forks and similar collaborative projects.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge</title>
<link>https://arxiv.org/abs/2312.05693</link>
<guid>https://arxiv.org/abs/2312.05693</guid>
<content:encoded><![CDATA[
arXiv:2312.05693v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) stand out for their impressive performance in intricate language modeling tasks. However, their demanding computational and memory needs pose obstacles for broad use on edge devices. Quantization is then introduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or lower weight quantization is feasible with minimal impact on end-to-end task performance, while the activation is still not quantized. On the other hand, mainstream commodity edge devices still struggle to execute these sub-8-bit quantized networks effectively. In this paper, we propose Agile-Quant, an activation-guided quantization framework for popular Large Language Models (LLMs), and implement an end-to-end accelerator on multiple edge devices for faster inference. Considering the hardware profiling and activation analysis, we first introduce a basic activation quantization strategy to balance the trade-off of task performance and real inference speed. Then we leverage the activation-aware token pruning technique to reduce the outliers and the adverse impact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier and our efficient TRIP matrix multiplication to implement the accelerator for LLMs on the edge. We apply our framework on different scales of LLMs including LLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the weight quantization. Experiments show that Agile-Quant achieves simultaneous quantization of model weights and activations while maintaining task performance comparable to existing weight-only quantization methods. Moreover, in the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up to 2.55x compared to its FP16 counterparts across multiple edge devices, marking a pioneering advancement in this domain. Code: https://github.com/shawnricecake/agile-quant
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Ontologies via Incorporating Extensional and Intensional Knowledge</title>
<link>https://arxiv.org/abs/2402.01677</link>
<guid>https://arxiv.org/abs/2402.01677</guid>
<content:encoded><![CDATA[
arXiv:2402.01677v5 Announce Type: replace-cross 
Abstract: Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can capture both structure information and textual information. Experimental results show that EIKE significantly outperforms state-of-the-art methods in three datasets for both triple classification and link prediction, indicating that EIKE provides a more comprehensive and representative perspective of the domain.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLMRec: Distilling Large Language Models into Small for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2405.17890</link>
<guid>https://arxiv.org/abs/2405.17890</guid>
<content:encoded><![CDATA[
arXiv:2405.17890v4 Announce Type: replace-cross 
Abstract: Sequential Recommendation (SR) task involves predicting the next item a user is likely to interact with, given their past interactions. The SR models examine the sequence of a user's actions to discern more complex behavioral patterns and temporal dynamics. Recent research demonstrates the great impact of LLMs on sequential recommendation systems, either viewing sequential recommendation as language modeling or serving as the backbone for user representation. Although these methods deliver outstanding performance, there is scant evidence of the necessity of a large language model and how large the language model is needed, especially in the sequential recommendation scene. Meanwhile, due to the huge size of LLMs, it is inefficient and impractical to apply a LLM-based model in real-world platforms that often need to process billions of traffic logs daily. In this paper, we explore the influence of LLMs' depth by conducting extensive experiments on large-scale industry datasets. Surprisingly, our motivational experiments reveal that most intermediate layers of LLMs are redundant, indicating that pruning the remaining layers can still maintain strong performance. Motivated by this insight, we empower small language models for SR, namely SLMRec, which adopt a simple yet effective knowledge distillation method. Moreover, SLMRec is orthogonal to other post-training efficiency techniques, such as quantization and pruning, so that they can be leveraged in combination. Comprehensive experimental results illustrate that the proposed SLMRec model attains the best performance using only 13% of the parameters found in LLM-based recommendation models while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively. Besides, we provide a theoretical justification for why small language models can perform comparably to large language models in SR.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DataComp-LM: In search of the next generation of training sets for language models</title>
<link>https://arxiv.org/abs/2406.11794</link>
<guid>https://arxiv.org/abs/2406.11794</guid>
<content:encoded><![CDATA[
arXiv:2406.11794v4 Announce Type: replace-cross 
Abstract: We introduce DataComp for Language Models (DCLM), a testbed for controlled dataset experiments with the goal of improving language models. As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations. Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at model scales ranging from 412M to 7B parameters. As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set. The resulting dataset, DCLM-Baseline enables training a 7B parameter language model from scratch to 64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6.6 percentage point improvement on MMLU while being trained with 40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and Llama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53 natural language understanding tasks while being trained with 6.6x less compute than Llama 3 8B. Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking as a Reward Misspecification Problem</title>
<link>https://arxiv.org/abs/2406.14393</link>
<guid>https://arxiv.org/abs/2406.14393</guid>
<content:encoded><![CDATA[
arXiv:2406.14393v5 Announce Type: replace-cross 
Abstract: The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. This misspecification occurs when the reward function fails to accurately capture the intended behavior, leading to misaligned model outputs. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts in a reward-misspecified space. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark against various target aligned LLMs while preserving the human readability of the generated prompts. Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed analysis highlights the unique advantages of the proposed reward misspecification objective compared to previous methods, offering new insights for improving LLM safety and robustness.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHands: An Open Platform for AI Software Developers as Generalist Agents</title>
<link>https://arxiv.org/abs/2407.16741</link>
<guid>https://arxiv.org/abs/2407.16741</guid>
<content:encoded><![CDATA[
arXiv:2407.16741v3 Announce Type: replace-cross 
Abstract: Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenHands (f.k.a. OpenDevin), a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2.1K contributions from over 188 contributors.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders</title>
<link>https://arxiv.org/abs/2409.06635</link>
<guid>https://arxiv.org/abs/2409.06635</guid>
<content:encoded><![CDATA[
arXiv:2409.06635v4 Announce Type: replace-cross 
Abstract: The rapid advancements in large language models (LLMs) have significantly enhanced natural language processing capabilities, facilitating the development of AudioLLMs that process and understand speech and audio inputs alongside text. Existing AudioLLMs typically combine a pre-trained audio encoder with a pre-trained LLM, which are subsequently finetuned on specific audio tasks. However, the pre-trained audio encoder has constrained capacity to capture features for new tasks and datasets. To address this, we propose to incorporate mixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE supplements a base encoder with a pool of relatively light weight encoders, selectively activated based on the audio input to enhance feature extraction without significantly increasing model size. Our empirical results demonstrate that MoWE effectively improves multi-task performance, broadening the applicability of AudioLLMs to more diverse audio tasks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models</title>
<link>https://arxiv.org/abs/2410.09344</link>
<guid>https://arxiv.org/abs/2410.09344</guid>
<content:encoded><![CDATA[
arXiv:2410.09344v2 Announce Type: replace-cross 
Abstract: Storing open-source fine-tuned models separately introduces redundancy and increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters--the differences between fine-tuned and pre-trained model weights--while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters. To push DARE's limits, we introduce DAREx (DARE the eXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling factor modification that significantly boosts performance at high pruning rates (e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in decoder models), and (2) DAREx-L2, which combines DARE with AdamR, an in-training method that applies appropriate delta regularization before DPP. We also demonstrate that DAREx-q can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-Parametric Inversion: Why Instruction Finetuning Can Worsen Context Reliance</title>
<link>https://arxiv.org/abs/2410.10796</link>
<guid>https://arxiv.org/abs/2410.10796</guid>
<content:encoded><![CDATA[
arXiv:2410.10796v3 Announce Type: replace-cross 
Abstract: A standard practice when using large language models is for users to supplement their instruction with an input context containing new information for the model to process. However, models struggle to reliably follow the input context, especially when it conflicts with their parametric knowledge from pretraining. In-principle, one would expect models to adapt to the user context better after instruction finetuning, particularly when handling knowledge conflicts. However, we observe a surprising failure mode: during instruction tuning, the context reliance under knowledge conflicts initially increases as expected, but then gradually decreases as instruction finetuning progresses. This happens while the performance on standard benchmarks keeps on increasing far after this drop. We call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across different model families like Llama, Mistral, and Pythia. We perform various controlled studies and theoretical analysis to show that context-parametric inversion occurs due to examples in the instruction finetuning data where the input context provides information that aligns with model's parametric knowledge. Our analysis suggests some natural mitigation strategies with limited but insightful gains, and serves as a useful starting point in addressing this deficiency in instruction finetuning.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2410.14148</link>
<guid>https://arxiv.org/abs/2410.14148</guid>
<content:encoded><![CDATA[
arXiv:2410.14148v4 Announce Type: replace-cross 
Abstract: The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aioli: A Unified Optimization Framework for Language Model Data Mixing</title>
<link>https://arxiv.org/abs/2411.05735</link>
<guid>https://arxiv.org/abs/2411.05735</guid>
<content:encoded><![CDATA[
arXiv:2411.05735v2 Announce Type: replace-cross 
Abstract: Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that no existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity. To understand this inconsistency, we unify existing methods into a standard framework, showing they are equivalent to solving a common optimization problem: minimize average loss subject to a method-specific mixing law -- an implicit assumption on the relationship between loss and mixture proportions. This framework suggests that measuring the fidelity of a method's mixing law can offer insights into its performance. Empirically, we find that existing methods set their mixing law parameters inaccurately, resulting in the inconsistent mixing performance we observe. Using this insight, we derive a new online method named Aioli, which directly estimates the mixing law parameters throughout training and uses them to dynamically adjust proportions. Aioli outperforms stratified sampling on 6 out of 6 datasets by an average of 0.27 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points worse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints, Aioli can dynamically adjust these proportions over the full training run, consistently improving performance over existing methods by up to 12.012 test perplexity points.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures</title>
<link>https://arxiv.org/abs/2411.16260</link>
<guid>https://arxiv.org/abs/2411.16260</guid>
<content:encoded><![CDATA[
arXiv:2411.16260v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable mathematical capabilities, largely driven by chain-of-thought (CoT) prompting, which decomposes complex reasoning into step-by-step solutions. This approach has enabled significant advancements, as evidenced by performance on benchmarks like GSM8K and MATH. However, the mechanisms underlying LLMs' ability to perform arithmetic in a single step of CoT remain poorly understood. Existing studies debate whether LLMs encode numerical values or rely on symbolic reasoning, while others explore attention and multi-layered processing in arithmetic tasks. In this work, we propose that LLMs learn arithmetic by capturing algebraic structures, such as commutativity and identity properties. Since these structures are observable through input-output relationships, they can generalize to unseen data. We empirically demonstrate that LLMs can learn algebraic structures using a custom dataset of arithmetic problems, as well as providing theoretical evidence showing that, under specific configurations of weights and biases, the transformer-based LLMs can generate embeddings that remain invariant to both permutations of input tokens and the presence of identity elements. Our findings indicate that leveraging algebraic structures can enhance the LLMs' arithmetic capabilities, offering insights into improving their arithmetic performance.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems</title>
<link>https://arxiv.org/abs/2503.21074</link>
<guid>https://arxiv.org/abs/2503.21074</guid>
<content:encoded><![CDATA[
arXiv:2503.21074v3 Announce Type: replace-cross 
Abstract: This thesis employs a hybrid CNN-Transformer architecture, alongside a detailed anthropological framework, to investigate potential historical connections between the visual morphology of the Indus Valley script and pictographic systems of the Tibetan-Yi Corridor. Through an ensemble methodology of three target scripts across 15 independently trained models, we demonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-fold higher visual similarity to the Indus script (0.635) than to the Bronze Age Proto-Cuneiform (0.102) or Proto-Elamite (0.078).
  Contrary to expectations, when measured through direct script-to-script embedding comparisons, the Indus script maps closer to Tibetan-Yi Corridor scripts with a mean cosine similarity of 0.930 (CI: [0.917, 0.942]) than to contemporaneous West Asian signaries, which recorded mean similarities of 0.887 (CI: [0.863, 0.911]) and 0.855 (CI: [0.818, 0.891]). Across dimensionality reduction and clustering methods, the Indus script consistently clusters closest to Tibetan-Yi Corridor scripts.
  These computational findings align with observed pictorial parallels in numeral systems, gender markers, and iconographic elements. Archaeological evidence of contact networks along the ancient Shu-Shendu road, coinciding with the Indus Civilization's decline, provides a plausible transmission pathway. While alternate explanations cannot be ruled out, the specificity and consistency of similarities suggest more complex cultural transmission networks between South and East Asia than previously recognized.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design</title>
<link>https://arxiv.org/abs/2504.01337</link>
<guid>https://arxiv.org/abs/2504.01337</guid>
<content:encoded><![CDATA[
arXiv:2504.01337v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) has successfully scaled up models while maintaining nearly constant computing costs. By employing a gating network to route input tokens, it selectively activates a subset of expert networks to process the corresponding token embeddings. However, in practice, the efficiency of MoE is challenging to achieve due to two key reasons: imbalanced expert activation, which leads to substantial idle time during model or expert parallelism, and insufficient capacity utilization; massive communication overhead, induced by numerous expert routing combinations in expert parallelism at the system level. Previous works typically formulate it as the load imbalance issue characterized by the gating network favoring certain experts over others or attribute it to static execution which fails to adapt to the dynamic expert workload at runtime. In this paper, we exploit it from a brand new perspective, a higher-order view and analysis of MoE routing policies: expert collaboration and specialization where some experts tend to activate broadly with others (collaborative), while others are more likely to activate only with a specific subset of experts (specialized). Our experiments reveal that most experts tend to be overly collaborative, leading to increased communication overhead from repeatedly sending tokens to different accelerators. To this end, we propose a novel collaboration-constrained routing (C2R) strategy to encourage more specialized expert groups, as well as to improve expert utilization, and present an efficient implementation of MoE that further leverages expert specialization. We achieve an average performance improvement of 0.51% and 0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP benchmarks, and reduce the all2all communication costs between GPUs, bringing an extra 20%-30% total running time savings on top of the existing SoTA, i.e. MegaBlocks.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Resource Allocation in Multi-Agent LLM Systems</title>
<link>https://arxiv.org/abs/2504.02051</link>
<guid>https://arxiv.org/abs/2504.02051</guid>
<content:encoded><![CDATA[
arXiv:2504.02051v2 Announce Type: replace-cross 
Abstract: With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling</title>
<link>https://arxiv.org/abs/2504.05216</link>
<guid>https://arxiv.org/abs/2504.05216</guid>
<content:encoded><![CDATA[
arXiv:2504.05216v2 Announce Type: replace-cross 
Abstract: Dense retrieval is a crucial task in Information Retrieval (IR) and is the foundation for downstream tasks such as re-ranking. Recently, large language models (LLMs) have shown compelling semantic understanding capabilities and are appealing to researchers studying dense retrieval. LLMs, as decoder-style generative models, are competent at language generation while falling short on modeling global information due to the lack of attention to tokens afterward. Inspired by the classical word-based language modeling approach for IR, i.e., the query likelihood (QL) model, we seek to sufficiently utilize LLMs' generative ability by QL maximization. However, instead of ranking documents with QL estimation, we introduce an auxiliary task of QL maximization to yield a better backbone for contrastively learning a discriminative retriever. We name our model as LLM-QL. To condense global document semantics to a single vector during QL modeling, LLM-QL has two major components, Attention Stop (AS) and Input Corruption (IC). AS stops the attention of predictive tokens to previous tokens until the ending token of the document. IC masks a portion of tokens in the input documents during prediction. Experiments on MSMARCO show that LLM-QL can achieve significantly better performance than other LLM-based retrievers and using QL estimated by LLM-QL for ranking outperforms word-based QL by a large margin.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations</title>
<link>https://arxiv.org/abs/2504.09602</link>
<guid>https://arxiv.org/abs/2504.09602</guid>
<content:encoded><![CDATA[
arXiv:2504.09602v2 Announce Type: replace-cross 
Abstract: Configuring computational fluid dynamics (CFD) simulations typically demands extensive domain expertise, limiting broader access. Although large language models (LLMs) have advanced scientific computing, their use in automating CFD workflows is underdeveloped. We introduce a novel approach centered on domain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM, our custom dataset of 28716 natural language-to-OpenFOAM configuration pairs with chain-of-thought (CoT) annotations, we enable direct translation from natural language descriptions to executable CFD setups. A multi-agent framework orchestrates the process, autonomously verifying inputs, generating configurations, running simulations, and correcting errors. Evaluation on a benchmark of 21 diverse flow cases demonstrates state-of-the-art performance, achieving 88.7% solution accuracy and 82.6% first-attempt success rate. This significantly outperforms larger general-purpose models like Qwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also requiring fewer correction iterations and maintaining high computational efficiency. The results highlight the critical role of domain-specific adaptation in deploying LLM assistants for complex engineering workflows. Our code and fine-tuned model have been deposited at https://github.com/YYgroup/AutoCFD.
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety</title>
<link>https://arxiv.org/abs/2504.09689</link>
<guid>https://arxiv.org/abs/2504.09689</guid>
<content:encoded><![CDATA[
arXiv:2504.09689v2 Announce Type: replace-cross 
Abstract: The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent
]]></content:encoded>
<pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis</title>
<link>https://arxiv.org/abs/2504.13187</link>
<guid>https://arxiv.org/abs/2504.13187</guid>
<content:encoded><![CDATA[
<div> large language models, calculus differentiation problems, performance evaluation, procedural capabilities, conceptual understanding 
Summary:
The study evaluates five leading large language models (LLMs) on their performance in solving calculus differentiation problems. Chat GPT 4o showed the highest success rate, followed by Claude Pro, Gemini Advanced, Copilot Pro, and Meta AI. While all models excelled at procedural tasks, they showed limitations in conceptual understanding and algebraic manipulation. Problems involving increasing/decreasing intervals and optimization word problems were particularly challenging. Claude Pro generated the most difficult problems, implying distinct capabilities in problem generation and solving. The findings suggest the potential and limitations of LLMs in educational applications, emphasizing the importance of human instruction for deeper mathematical comprehension. <div>
arXiv:2504.13187v1 Announce Type: new 
Abstract: This study presents a comprehensive evaluation of five leading large language models (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta AI - on their performance in solving calculus differentiation problems. The investigation assessed these models across 13 fundamental problem types, employing a systematic cross-evaluation framework where each model solved problems generated by all models. Results revealed significant performance disparities, with Chat GPT 4o achieving the highest success rate (94.71%), followed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro (76.30%), and Meta AI (56.75%). All models excelled at procedural differentiation tasks but showed varying limitations with conceptual understanding and algebraic manipulation. Notably, problems involving increasing/decreasing intervals and optimization word problems proved most challenging across all models. The cross-evaluation matrix revealed that Claude Pro generated the most difficult problems, suggesting distinct capabilities between problem generation and problem-solving. These findings have significant implications for educational applications, highlighting both the potential and limitations of LLMs as calculus learning tools. While they demonstrate impressive procedural capabilities, their conceptual understanding remains limited compared to human mathematical reasoning, emphasizing the continued importance of human instruction for developing deeper mathematical comprehension.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models</title>
<link>https://arxiv.org/abs/2504.13189</link>
<guid>https://arxiv.org/abs/2504.13189</guid>
<content:encoded><![CDATA[
<div> classification, sector identification, performance ranking, fiscal policy impacts, computational economics research
Summary:<br />
- The study introduces a framework called BASIR to analyze the impact of India's Union Budget announcements on sector-specific equity performance in real-time. 
- It involves multi-label classification of budget transcripts into economic sectors and ranking these sectors based on their predicted performances. 
- The results show a 0.605 F1-score in sector classification and a 0.997 NDCG score in predicting sector performance ranks post-budget. 
- The methodology aims to provide structured, data-driven insights for investors and policymakers to quantify fiscal policy impacts. 
- An annotated dataset, released under a CC-BY-NC-SA-4.0 license, is available to support computational economics research. 
<br /><br />Summary: <div>
arXiv:2504.13189v1 Announce Type: new 
Abstract: Government fiscal policies, particularly annual union budgets, exert significant influence on financial markets. However, real-time analysis of budgetary impacts on sector-specific equity performance remains methodologically challenging and largely unexplored. This study proposes a framework to systematically identify and rank sectors poised to benefit from India's Union Budget announcements. The framework addresses two core tasks: (1) multi-label classification of excerpts from budget transcripts into 81 predefined economic sectors, and (2) performance ranking of these sectors. Leveraging a comprehensive corpus of Indian Union Budget transcripts from 1947 to 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an annotated dataset mapping excerpts from budgetary transcripts to sectoral impacts. Our architecture incorporates fine-tuned embeddings for sector identification, coupled with language models that rank sectors based on their predicted performances. Our results demonstrate 0.605 F1-score in sector classification, and 0.997 NDCG score in predicting ranks of sectors based on post-budget performances. The methodology enables investors and policymakers to quantify fiscal policy impacts through structured, data-driven insights, addressing critical gaps in manual analysis. The annotated dataset has been released under CC-BY-NC-SA-4.0 license to advance computational economics research.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding</title>
<link>https://arxiv.org/abs/2504.13216</link>
<guid>https://arxiv.org/abs/2504.13216</guid>
<content:encoded><![CDATA[
<div> benchmark suite, large language models, Korean financial domain, evaluation, financial knowledge

Summary:
The article introduces KFinEval-Pilot, a benchmark suite specifically created to evaluate large language models in the Korean financial sector. It consists of over 1,000 curated questions covering financial knowledge, legal reasoning, and financial toxicity. The benchmark is developed using a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. Performance variations are observed among representative LLMs, showcasing trade-offs between task accuracy and output safety. Challenges persist in applying LLMs to high-stakes financial tasks, particularly in reasoning and safety. Grounded in real-world financial scenarios and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot acts as a diagnostic tool for developing more secure and trustworthy financial AI systems. 

Summary: <div>
arXiv:2504.13216v1 Announce Type: new 
Abstract: We introduce KFinEval-Pilot, a benchmark suite specifically designed to evaluate large language models (LLMs) in the Korean financial domain. Addressing the limitations of existing English-centric benchmarks, KFinEval-Pilot comprises over 1,000 curated questions across three critical areas: financial knowledge, legal reasoning, and financial toxicity. The benchmark is constructed through a semi-automated pipeline that combines GPT-4-generated prompts with expert validation to ensure domain relevance and factual accuracy. We evaluate a range of representative LLMs and observe notable performance differences across models, with trade-offs between task accuracy and output safety across different model families. These results highlight persistent challenges in applying LLMs to high-stakes financial applications, particularly in reasoning and safety. Grounded in real-world financial use cases and aligned with the Korean regulatory and linguistic context, KFinEval-Pilot serves as an early diagnostic tool for developing safer and more reliable financial AI systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sustainability via LLM Right-sizing</title>
<link>https://arxiv.org/abs/2504.13217</link>
<guid>https://arxiv.org/abs/2504.13217</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sustainability, performance benchmarks, task evaluation, responsible deployment<br />
Summary:<br />
This study evaluates the performance of eleven large language models (LLMs) across ten everyday occupational tasks, considering factors such as output quality, factual accuracy, and ethical responsibility. The authors introduce a dual-LLM-based evaluation framework to automate task execution and standardize evaluation criteria. Results show that smaller models like Gemma-3 and Phi-4 can achieve strong results in tasks, highlighting their viability in cost-efficient, locally deployable, or private contexts. The study identifies three model groups - premium all-rounders, competent generalists, and limited but safe performers - underscoring trade-offs between quality, control, and sustainability. Task type significantly influences model effectiveness, with conceptual tasks posing challenges while aggregation and transformation tasks leading to better performances. The authors advocate for a shift towards task- and context-aware sufficiency assessments for responsible LLM deployment, offering actionable guidance for organizations. <br /><br /> <div>
arXiv:2504.13217v1 Announce Type: new 
Abstract: Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model "good enough"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIDS: Domain Impact-aware Data Sampling for Large Language Model Training</title>
<link>https://arxiv.org/abs/2504.13227</link>
<guid>https://arxiv.org/abs/2504.13227</guid>
<content:encoded><![CDATA[
<div> algorithm, language model, data sampling, domain impact, training efficiency

Summary:
The paper introduces the Domain Impact-aware Data Sampling (DIDS) framework for optimizing domain-level data sampling strategies in large language models (LLMs). DIDS addresses the challenges of maintaining intra-domain consistency and accurately measuring domain impact. It uses a gradient clustering algorithm to group training data based on their learning effects while employing a proxy language model and dimensionality reduction for computational efficiency. To measure domain impact, a Fisher Information Matrix (FIM) guided metric is developed to quantify how domain-specific parameter updates affect model output distributions on downstream tasks, with theoretical guarantees. DIDS determines optimal sampling ratios by combining FIM-guided domain impact assessment and loss learning trajectories. Experimental results show that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency.<br /><br />Summary: <div>
arXiv:2504.13227v1 Announce Type: new 
Abstract: Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs</title>
<link>https://arxiv.org/abs/2504.13237</link>
<guid>https://arxiv.org/abs/2504.13237</guid>
<content:encoded><![CDATA[
<div> Keywords: delta compression, large language models, sparsification, importance-aware, singular vectors

Summary:<br /><br />
The paper introduces ImPart, a new importance-aware delta sparsification approach for compressing task-specific large language models. Unlike previous methods that either ignore parameter importance or evaluate it at a coarse granularity, ImPart dynamically adjusts sparsity ratios of singular vectors based on their importance after SVD. This allows crucial task-specific knowledge to be retained even at high sparsity ratios, leading to state-of-the-art compression performance. Experiments show that ImPart achieves a 2x higher compression ratio compared to baselines while maintaining the same performance level. When combined with existing methods, ImPart establishes a new state-of-the-art in delta quantization and model merging. <div>
arXiv:2504.13237v1 Announce Type: new 
Abstract: With the proliferation of task-specific large language models, delta compression has emerged as a method to mitigate the resource challenges of deploying numerous such models by effectively compressing the delta model parameters. Previous delta-sparsification methods either remove parameters randomly or truncate singular vectors directly after singular value decomposition (SVD). However, these methods either disregard parameter importance entirely or evaluate it with too coarse a granularity. In this work, we introduce ImPart, a novel importance-aware delta sparsification approach. Leveraging SVD, it dynamically adjusts sparsity ratios of different singular vectors based on their importance, effectively retaining crucial task-specific knowledge even at high sparsity ratios. Experiments show that ImPart achieves state-of-the-art delta sparsification performance, demonstrating $2\times$ higher compression ratio than baselines at the same performance level. When integrated with existing methods, ImPart sets a new state-of-the-art on delta quantization and model merging.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models</title>
<link>https://arxiv.org/abs/2504.13261</link>
<guid>https://arxiv.org/abs/2504.13261</guid>
<content:encoded><![CDATA[
<div> benchmark, pedagogical grammar, language models, foreign language instruction, evaluation

Summary:
CPG-EVAL is introduced as a benchmark to evaluate large language models' (LLMs) knowledge of pedagogical grammar in foreign language instruction. The benchmark consists of five tasks focusing on grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Smaller LLMs perform well in single language instance tasks but struggle with multiple instances and interference. Larger models show better resistance to interference but still have room for improvement in accuracy. The study highlights the need for better alignment of instructional practices and more rigorous benchmarks to guide the use of LLMs in education effectively. CPG-EVAL provides insights for educators, policymakers, and model developers to assess LLMs in educational settings and lays the foundation for future research on model alignment and improving educational suitability in foreign language instruction. 

<br /><br />Summary: <div>
arXiv:2504.13261v1 Announce Type: new 
Abstract: Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT has significantly impacted foreign language education, yet their pedagogical grammar competence remains under-assessed. This paper introduces CPG-EVAL, the first dedicated benchmark specifically designed to evaluate LLMs' knowledge of pedagogical grammar within the context of foreign language instruction. Methodology: The benchmark comprises five tasks designed to assess grammar recognition, fine-grained grammatical distinction, categorical discrimination, and resistance to linguistic interference. Findings: Smaller-scale models can succeed in single language instance tasks, but struggle with multiple instance tasks and interference from confusing instances. Larger-scale models show better resistance to interference but still have significant room for accuracy improvement. The evaluation indicates the need for better instructional alignment and more rigorous benchmarks, to effectively guide the deployment of LLMs in educational contexts. Value: This study offers the first specialized, theory-driven, multi-tiered benchmark framework for systematically evaluating LLMs' pedagogical grammar competence in Chinese language teaching contexts. CPG-EVAL not only provides empirical insights for educators, policymakers, and model developers to better gauge AI's current abilities in educational settings, but also lays the groundwork for future research on improving model alignment, enhancing educational suitability, and ensuring informed decision-making concerning LLM integration in foreign language instruction.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal</title>
<link>https://arxiv.org/abs/2504.13284</link>
<guid>https://arxiv.org/abs/2504.13284</guid>
<content:encoded><![CDATA[
<div> Keywords: Internet penetration, smartphones, social networks, Senegal, sentiment analysis 

Summary: 

Internet penetration rates in Africa, particularly Senegal, are increasing, with a significant boost from the availability of smartphones. Young people in Senegal are heavily reliant on social networks as their primary means of expression. Despite the growing accessibility to the Internet, limited options from few operators restrict alternatives for mobile Internet services in terms of value for money. To understand young people's perspectives on mobile Internet pricing in Senegal, sentiment analysis was conducted on Twitter and Facebook comments. The study aimed to gauge sentiments towards the price of mobile Internet in relation to the perceived quality of service. Through this analysis, insights were derived from the general feelings expressed by the youth regarding the affordability and quality of mobile Internet services in Senegal. <div>
arXiv:2504.13284v1 Announce Type: new 
Abstract: Internet penetration rates in Africa are rising steadily, and mobile Internet is getting an even bigger boost with the availability of smartphones. Young people are increasingly using the Internet, especially social networks, and Senegal is no exception to this revolution. Social networks have become the main means of expression for young people. Despite this evolution in Internet access, there are few operators on the market, which limits the alternatives available in terms of value for money. In this paper, we will look at how young people feel about the price of mobile Internet in Senegal, in relation to the perceived quality of the service, through their comments on social networks. We scanned a set of Twitter and Facebook comments related to the subject and applied a sentiment analysis model to gather their general feelings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13367</link>
<guid>https://arxiv.org/abs/2504.13367</guid>
<content:encoded><![CDATA[
<div> difficulty, token allocation, calibration, reasoning models, THOUGHTTERMINATOR

Summary: 
The article discusses the issue of overthinking in reasoning models and introduces approximate measures of problem-level difficulty to optimize token allocation. It shows a clear relationship between problem difficulty and optimal token spend, highlighting the poor calibration of reasoning models in efficiently allocating tokens, especially on easy problems. The DUMB500 dataset is introduced for evaluating calibration on easy questions, contrasting with existing difficult benchmarks. Additionally, THOUGHTTERMINATOR, a training-free black box decoding technique, is proposed to improve reasoning model calibration effectively. <div>
arXiv:2504.13367v1 Announce Type: new 
Abstract: Reasoning models have demonstrated impressive performance on difficult tasks that traditional language models struggle at. However, many are plagued with the problem of overthinking--generating large amounts of unnecessary tokens which don't improve accuracy on a question. We introduce approximate measures of problem-level difficulty and demonstrate that a clear relationship between problem difficulty and optimal token spend exists, and evaluate how well calibrated a variety of reasoning models are in terms of efficiently allocating the optimal token count. We find that in general, reasoning models are poorly calibrated, particularly on easy problems. To evaluate calibration on easy questions we introduce DUMB500, a dataset of extremely easy math, reasoning, code, and task problems, and jointly evaluate reasoning model on these simple examples and extremely difficult examples from existing frontier benchmarks on the same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free black box decoding technique that significantly improves reasoning model calibration.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering</title>
<link>https://arxiv.org/abs/2504.13425</link>
<guid>https://arxiv.org/abs/2504.13425</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Enterprise Settings, Data Security, Secure Multifaceted-RAG, External LLM-generated knowledge

Summary:
The article introduces the Secure Multifaceted-RAG (SecMulti-RAG) framework to address challenges faced by existing Retrieval-Augmented Generation (RAG) systems in enterprise settings. SecMulti-RAG improves response accuracy and completeness by retrieving from internal documents, pre-generated expert knowledge, and on-demand external Large Language Models (LLMs). Security risks are mitigated by using a local open-source generator and selectively employing external LLMs with a filtering mechanism. In an evaluation on a report generation task in the automotive industry, SecMulti-RAG outperforms traditional RAG in correctness, richness, and helpfulness according to LLM-based evaluation. Human evaluation also shows significant improvements. This approach enhances completeness, prevents data leakage, and reduces costs, making SecMulti-RAG a practical and secure solution for enterprise RAG.<br /><br />Summary: <div>
arXiv:2504.13425v1 Announce Type: new 
Abstract: Existing Retrieval-Augmented Generation (RAG) systems face challenges in enterprise settings due to limited retrieval scope and data security risks. When relevant internal documents are unavailable, the system struggles to generate accurate and complete responses. Additionally, using closed-source Large Language Models (LLMs) raises concerns about exposing proprietary information. To address these issues, we propose the Secure Multifaceted-RAG (SecMulti-RAG) framework, which retrieves not only from internal documents but also from two supplementary sources: pre-generated expert knowledge for anticipated queries and on-demand external LLM-generated knowledge. To mitigate security risks, we adopt a local open-source generator and selectively utilize external LLMs only when prompts are deemed safe by a filtering mechanism. This approach enhances completeness, prevents data leakage, and reduces costs. In our evaluation on a report generation task in the automotive industry, SecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9 percent win rates across correctness, richness, and helpfulness in LLM-based evaluation, and 56.3 to 70.4 percent in human evaluation. This highlights SecMulti-RAG as a practical and secure solution for enterprise RAG.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model</title>
<link>https://arxiv.org/abs/2504.13439</link>
<guid>https://arxiv.org/abs/2504.13439</guid>
<content:encoded><![CDATA[
<div> distractor generator, multiple-choice evaluation, D-GEN, ranking alignment, entropy analysis

Summary:
The article introduces D-GEN, an open-source distractor generator model that transforms open-ended data into a multiple-choice (MC) format. To evaluate the quality of the generated distractors, two novel methods are proposed: ranking alignment and entropy analysis. The results demonstrate that D-GEN maintains ranking consistency and closely matches the entropy distribution of ground-truth distractors. Human evaluation validates the fluency, coherence, distractiveness, and incorrectness of the generated distractors. This work advances the field by providing a robust and efficient method for automated distractor generation and evaluation, setting a new standard for MC evaluation. <div>
arXiv:2504.13439v1 Announce Type: new 
Abstract: Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs</title>
<link>https://arxiv.org/abs/2504.13471</link>
<guid>https://arxiv.org/abs/2504.13471</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, NLP pipeline, model compression, knowledge transfer, cost-efficient deployment pipeline 

Summary:
In this paper, a three-stage cost-efficient end-to-end deployment pipeline for Large Language Models (LLMs) is introduced to address the cost-performance dilemma in LLM-based frameworks. The approach involves prototyping, knowledge transfer, and model compression stages to optimize cost and performance for online systems. Initially, a high-performance prototype system is built to generate high-quality data as a teacher model. Techniques like rejection fine-tuning, reinforcement learning, and knowledge distillation are then used to transfer knowledge to a smaller student model, resulting in effective performance at minimal cost. Finally, quantization and pruning are applied to compress the model further, achieving ultra-low latency and cost. The modular design and cross-domain capabilities of the framework suggest potential applicability in various NLP areas. 

<br /><br />Summary: <div>
arXiv:2504.13471v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) pipelines, improving performance and generalization. This has spurred their integration into various systems. Many NLP systems, including ours, employ a "one-stage" pipeline directly incorporating LLMs. While effective, this approach incurs substantial costs and latency due to the need for large model parameters to achieve satisfactory outcomes. This paper introduces a three-stage cost-efficient end-to-end LLM deployment pipeline-including prototyping, knowledge transfer, and model compression-to tackle the cost-performance dilemma in LLM-based frameworks. Our approach yields a super tiny model optimized for cost and performance in online systems, simplifying the system architecture. Initially, by transforming complex tasks into a function call-based LLM-driven pipeline, an optimal performance prototype system is constructed to produce high-quality data as a teacher model. The second stage combine techniques like rejection fine-tuning, reinforcement learning and knowledge distillation to transfer knowledge to a smaller 0.5B student model, delivering effective performance at minimal cost. The final stage applies quantization and pruning to extremely compress model to 0.4B, achieving ultra-low latency and cost. The framework's modular design and cross-domain capabilities suggest potential applicability in other NLP areas.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Sensitivity Evaluation Framework for Clinical Diagnosis</title>
<link>https://arxiv.org/abs/2504.13475</link>
<guid>https://arxiv.org/abs/2504.13475</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, clinical diagnosis, sensitivity, key medical information, diagnostic decision-making

Summary: 
Large language models (LLMs) have shown impressive performance in various domains, but for clinical diagnosis, higher expectations are needed for their sensitivity and reliability. Existing research mostly focuses on LLMs' sensitivity to irrelevant context rather than key medical information crucial for diagnostic reasoning. This study evaluates the sensitivity of LLMs, such as GPT-3.5, GPT-4, Gemini, Claude3, and LLaMA2-7b, to key medical information using different perturbation strategies. The results demonstrate current LLMs' limitations in accurately capturing crucial medical information for diagnostic decision-making. Future advancements in LLMs should prioritize improving reliability and sensitivity to key information to enhance trust and practical use in real-world scenarios. The code and dataset for this study are publicly available at https://github.com/chenwei23333/DiagnosisQA.

Summary: <div>
arXiv:2504.13475v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across various domains. However, for clinical diagnosis, higher expectations are required for LLM's reliability and sensitivity: thinking like physicians and remaining sensitive to key medical information that affects diagnostic reasoning, as subtle variations can lead to different diagnosis results. Yet, existing works focus mainly on investigating the sensitivity of LLMs to irrelevant context and overlook the importance of key information. In this paper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini, Claude3 and LLaMA2-7b, to key medical information by introducing different perturbation strategies. The evaluation results highlight the limitations of current LLMs in remaining sensitive to key medical information for diagnostic decision-making. The evolution of LLMs must focus on improving their reliability, enhancing their ability to be sensitive to key information, and effectively utilizing this information. These improvements will enhance human trust in LLMs and facilitate their practical application in real-world scenarios. Our code and dataset are available at https://github.com/chenwei23333/DiagnosisQA.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning</title>
<link>https://arxiv.org/abs/2504.13500</link>
<guid>https://arxiv.org/abs/2504.13500</guid>
<content:encoded><![CDATA[
<div> Keywords: process prejudge, LLM reasoning, dynamic tree-searching, supervised fine-tuning, reinforcement learning

Summary: 
In this paper, a new process prejudge strategy is introduced in LLM reasoning to enhance adaptive reasoning capabilities. The strategy involves anticipating errors in reasoning steps similar to human decision-making processes. A prejudge node is defined in the rationale to represent a step where errors may occur. An automated reasoning framework with dynamic tree-searching is presented, requiring only one LLM to perform various tasks. A two-phase training mechanism involving supervised fine-tuning and reinforcement learning further improves reasoning abilities. Experimental results show significant enhancements in LLM reasoning skills, with the model learning to prejudge before proceeding. The code and data are available at the provided GitHub repository.<br /><br />Summary: <div>
arXiv:2504.13500v1 Announce Type: new 
Abstract: In this paper, we introduce a new \emph{process prejudge} strategy in LLM reasoning to demonstrate that bootstrapping with process prejudge allows the LLM to adaptively anticipate the errors encountered when advancing the subsequent reasoning steps, similar to people sometimes pausing to think about what mistakes may occur and how to avoid them, rather than relying solely on trial and error. Specifically, we define a prejudge node in the rationale, which represents a reasoning step, with at least one step that follows the prejudge node that has no paths toward the correct answer. To synthesize the prejudge reasoning process, we present an automated reasoning framework with a dynamic tree-searching strategy. This framework requires only one LLM to perform answer judging, response critiquing, prejudge generation, and thought completion. Furthermore, we develop a two-phase training mechanism with supervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance the reasoning capabilities of LLMs. Experimental results from competition-level complex reasoning demonstrate that our method can teach the model to prejudge before thinking and significantly enhance the reasoning ability of LLMs. Code and data is released at https://github.com/wjn1996/Prejudge-Before-Think.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.13534</link>
<guid>https://arxiv.org/abs/2504.13534</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph, Chain-of-Thought Reasoning, Large Language Models, Reasoning Framework, Retrieval-Augmented Generation

Summary:
CoT-RAG is introduced as a new reasoning framework addressing challenges in chain-of-thought reasoning for large language models. It incorporates knowledge graphs to improve the credibility of reasoning chains generated by LLMs. Additionally, it integrates retrieval-augmented generation (RAG) into knowledge graphs to provide LLMs with relevant sub-cases and sub-descriptions for enhanced reasoning. Moreover, the framework encourages LLMs to execute reasoning tasks in pseudo-programs to increase logical rigor. Evaluation across nine public datasets demonstrates a significant accuracy improvement compared to existing methods, ranging from 4.0% to 23.0%. Testing on domain-specific datasets showcases CoT-RAG's accuracy and efficiency, underscoring its practical applicability and scalability.

<br /><br />Summary: <div>
arXiv:2504.13534v1 Announce Type: new 
Abstract: While chain-of-thought (CoT) reasoning improves the performance of large language models (LLMs) in complex tasks, it still has two main challenges: the low reliability of relying solely on LLMs to generate reasoning chains and the interference of natural language reasoning chains on the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation, featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to execute reasoning tasks in pseudo-programs with greater logical rigor. We conduct a comprehensive evaluation on nine public datasets, covering three reasoning problems. Compared with the-state-of-the-art methods, CoT-RAG exhibits a significant accuracy improvement, ranging from 4.0% to 23.0%. Furthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable accuracy and efficient execution, highlighting its strong practical applicability and scalability.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content</title>
<link>https://arxiv.org/abs/2504.13545</link>
<guid>https://arxiv.org/abs/2504.13545</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, banking sector, multilingual capabilities, explainable outputs, aspect-based analysis

Summary: 
This research introduces a hybrid aspect-based sentiment analysis framework for the banking sector, focusing on multilingual customer feedback in languages like English, Sinhala, Singlish, and code-mixed text. The framework combines fine-tuned models such as XLM-RoBERTa for Sinhala and code-mixed text, BERT-base-uncased for English, and domain-specific lexicon correction. It classifies sentiment with confidence scores and utilizes SHAP and LIME techniques for real-time sentiment explanations, achieving high accuracy and F1-scores. The research emphasizes the importance of interpretability in sentiment analysis and delivers aspect-wise sentiment insights through a user-friendly interface. Overall, this study enhances sentiment analysis in the banking sector by addressing challenges in multilingual, low-resource languages and providing transparent and reliable outputs. 

<br /><br />Summary: <div>
arXiv:2504.13545v1 Announce Type: new 
Abstract: Sentiment analysis is crucial for brand reputation management in the banking sector, where customer feedback spans English, Sinhala, Singlish, and code-mixed text. Existing models struggle with low-resource languages like Sinhala and lack interpretability for practical use. This research develops a hybrid aspect-based sentiment analysis framework that enhances multilingual capabilities with explainable outputs. Using cleaned banking customer reviews, we fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate domain-specific lexicon correction, and employ BERT-base-uncased for English. The system classifies sentiment (positive, neutral, negative) with confidence scores, while SHAP and LIME improve interpretability by providing real-time sentiment explanations. Experimental results show that our approaches outperform traditional transformer-based classifiers, achieving 92.3 percent accuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and code-mixed content. An explainability analysis reveals key sentiment drivers, improving trust and transparency. A user-friendly interface delivers aspect-wise sentiment insights, ensuring accessibility for businesses. This research contributes to robust, transparent sentiment analysis for financial applications by bridging gaps in multilingual, low-resource NLP and explainability.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification</title>
<link>https://arxiv.org/abs/2504.13562</link>
<guid>https://arxiv.org/abs/2504.13562</guid>
<content:encoded><![CDATA[
<div> attention modification, jailbreak attacks, Large Language Models, defense, DETAM

Summary:
DETAM introduces a defense approach against jailbreak attacks on Large Language Models (LLMs) without the need for finetuning. By analyzing attention scores, sensitive attention heads are identified and reallocated during inference to emphasize the user's intent and minimize interference from attack tokens. Experimental results show DETAM outperforming various baselines in jailbreak defense and maintaining effectiveness across different attacks and models, even on in-the-wild data. Utility evaluation with over-defense datasets further validates DETAM's superior performance in defending against jailbreak attacks. The code for DETAM will be released upon acceptance. <div>
arXiv:2504.13562v1 Announce Type: new 
Abstract: With the widespread adoption of Large Language Models (LLMs), jailbreak attacks have become an increasingly pressing safety concern. While safety-aligned LLMs can effectively defend against normal harmful queries, they remain vulnerable to such attacks. Existing defense methods primarily rely on fine-tuning or input modification, which often suffer from limited generalization and reduced utility. To address this, we introduce DETAM, a finetuning-free defense approach that improves the defensive capabilities against jailbreak attacks of LLMs via targeted attention modification. Specifically, we analyze the differences in attention scores between successful and unsuccessful defenses to identify the attention heads sensitive to jailbreak attacks. During inference, we reallocate attention to emphasize the user's core intention, minimizing interference from attack tokens. Our experimental results demonstrate that DETAM outperforms various baselines in jailbreak defense and exhibits robust generalization across different attacks and models, maintaining its effectiveness even on in-the-wild jailbreak data. Furthermore, in evaluating the model's utility, we incorporated over-defense datasets, which further validate the superior performance of our approach. The code will be released immediately upon acceptance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling</title>
<link>https://arxiv.org/abs/2504.13592</link>
<guid>https://arxiv.org/abs/2504.13592</guid>
<content:encoded><![CDATA[
<div> intent detection, task-oriented dialogue, reinforcement learning, reward-based curriculum sampling, generalization <br />
Summary: <br />
Intent detection in task-oriented dialogue systems faces challenges in adapting to new integrable tools. This study proposes using Reinforcement Learning (RL) with Reward-based Curriculum Sampling (RCS) during training to improve generalization in unseen tasks. RL-trained models outperform supervised fine-tuning (SFT) baselines, with RCS enhancing the effectiveness of RL by focusing on challenging cases. Chain-of-Thought (COT) processes in RL help improve generalization in complex intent detection tasks. This work advances intent detection generalization, offering insights for deploying adaptable dialogue systems. <div>
arXiv:2504.13592v1 Announce Type: new 
Abstract: Intent detection, a critical component in task-oriented dialogue (TOD) systems, faces significant challenges in adapting to the rapid influx of integrable tools with complex interrelationships. Existing approaches, such as zero-shot reformulations and LLM-based dynamic recognition, struggle with performance degradation when encountering unseen intents, leading to erroneous task routing. To enhance the model's generalization performance on unseen tasks, we employ Reinforcement Learning (RL) combined with a Reward-based Curriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO) training in intent detection tasks. Experiments demonstrate that RL-trained models substantially outperform supervised fine-tuning (SFT) baselines in generalization. Besides, the introduction of the RCS, significantly bolsters the effectiveness of RL in intent detection by focusing the model on challenging cases during training. Moreover, incorporating Chain-of-Thought (COT) processes in RL notably improves generalization in complex intent detection tasks, underscoring the importance of thought in challenging scenarios. This work advances the generalization of intent detection tasks, offering practical insights for deploying adaptable dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Pre-Training is (not) What You Need in Domain Adaption</title>
<link>https://arxiv.org/abs/2504.13603</link>
<guid>https://arxiv.org/abs/2504.13603</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Legal research, Legal reasoning, Domain adaptation, Legal AI  
Summary:  
- Legal Large Language Models (LLMs) have transformed legal research and practice by automating tasks and supporting decision-making processes.  
- Adapting LLMs to the legal domain is challenging due to the complexity of legal reasoning and specialized language interpretation.  
- Domain-Adaptive Continual Pre-Training (DACP) enhances domain-specific knowledge but does not uniformly improve performance across all legal tasks.  
- DACP impacts model generalization and performance in prompt-based tasks.  
- Future research should focus on optimizing domain adaptation strategies in legal AI.  

Summary: <div>
arXiv:2504.13603v1 Announce Type: new 
Abstract: The recent advances in Legal Large Language Models (LLMs) have transformed the landscape of legal research and practice by automating tasks, enhancing research precision, and supporting complex decision-making processes. However, effectively adapting LLMs to the legal domain remains challenging due to the complexity of legal reasoning, the need for precise interpretation of specialized language, and the potential for hallucinations. This paper examines the efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the legal reasoning capabilities of LLMs. Through a series of experiments on legal reasoning tasks within the Taiwanese legal framework, we demonstrate that while DACP enhances domain-specific knowledge, it does not uniformly improve performance across all legal tasks. We discuss the trade-offs involved in DACP, particularly its impact on model generalization and performance in prompt-based tasks, and propose directions for future research to optimize domain adaptation strategies in legal AI.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long-context Non-factoid Question Answering in Indic Languages</title>
<link>https://arxiv.org/abs/2504.13615</link>
<guid>https://arxiv.org/abs/2504.13615</guid>
<content:encoded><![CDATA[
<div> semantic scores, token-level scores, context-shortening techniques, Indic languages, Question Answering 

Summary:<br />
- Context-shortening techniques such as Open Information Extraction, coreference resolution, and Answer Paragraph Selection improve QA performance for long contexts in Indic languages.
- Experiments on Hindi, Tamil, Telugu, and Urdu show an average increase of 4% in semantic scores and 47% in token-level scores without fine-tuning.
- With fine-tuning, there is an average increase of 2% in both semantic and token-level scores. 
- Techniques reduce computational overhead and improve efficiency.
- Explainability techniques show that APS confidently identifies answer-containing paragraphs.
- Limitations exist for LLM-based QA systems in addressing non-factoid questions.
- Verbalizing OIE triples does not enhance system performance.
- Context-shortening techniques have the potential to enhance LLM-based QA systems for low-resource languages. 

Summary: <div>
arXiv:2504.13615v1 Announce Type: new 
Abstract: Question Answering (QA) tasks, which involve extracting answers from a given context, are relatively straightforward for modern Large Language Models (LLMs) when the context is short. However, long contexts pose challenges due to the quadratic complexity of the self-attention mechanism. This challenge is compounded in Indic languages, which are often low-resource. This study explores context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, Answer Paragraph Selection (APS), and their combinations, to improve QA performance. Compared to the baseline of unshortened (long) contexts, our experiments on four Indic languages (Hindi, Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield an average improvement of 4\% in semantic scores and 47\% in token-level scores when evaluated on three popular LLMs without fine-tuning. Furthermore, with fine-tuning, we achieve an average increase of 2\% in both semantic and token-level scores. Additionally, context-shortening reduces computational overhead. Explainability techniques like LIME and SHAP reveal that when the APS model confidently identifies the paragraph containing the answer, nearly all tokens within the selected text receive high relevance scores. However, the study also highlights the limitations of LLM-based QA systems in addressing non-factoid questions, particularly those requiring reasoning or debate. Moreover, verbalizing OIE-generated triples does not enhance system performance. These findings emphasize the potential of context-shortening techniques to improve the efficiency and effectiveness of LLM-based QA systems, especially for low-resource languages. The source code and resources are available at https://github.com/ritwikmishra/IndicGenQA.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, overthinking, computational cost reduction, safety alignment, CoT generator 

Summary:<br /><br />
The article introduces a new approach, ThoughtMani, to address the issue of overthinking in large reasoning models (LRMs). LRMs often generate redundant reasoning steps during computation, resulting in limited performance gains. ThoughtMani leverages external Context of Thought (CoT) generated by smaller models to reduce unnecessary intermediate steps and computational costs significantly. Experimental results show that applying ThoughtMani to LRMs such as QwQ-32B on the LiveBench/Code dataset maintains original performance while reducing output token counts by about 30%. Additionally, ThoughtMani improves safety alignment by an average of 10%. This approach makes LRMs more efficient and cost-effective for real-world applications, especially when model vendors serve models of varying sizes simultaneously. <div>
arXiv:2504.13626v1 Announce Type: new 
Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities in multiple tasks. However, LRMs typically suffer from "overthinking" problems, where models generate significantly redundant reasoning steps while bringing limited performance gains. Existing work relies on fine-tuning to mitigate overthinking, which requires additional data, unconventional training setups, risky safety misalignment, and poor generalization.
  Through empirical analysis, we reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token ($\texttt{}$ and $\texttt{)}$ can effectively manipulate the model to generate fewer thoughts. Building on these insights, we propose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass unnecessary intermediate steps and reduce computational costs significantly. We conduct extensive experiments to validate the utility and efficiency of ThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, ThoughtMani keeps the original performance and reduces output token counts by approximately 30%, with little overhead from the CoT generator. Furthermore, we find that ThoughtMani enhances safety alignment by an average of 10%. Since model vendors typically serve models of different sizes simultaneously, ThoughtMani provides an effective way to construct more efficient and accessible LRMs for real-world applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing</title>
<link>https://arxiv.org/abs/2504.13629</link>
<guid>https://arxiv.org/abs/2504.13629</guid>
<content:encoded><![CDATA[
<div> adoption patterns, writing convergence, academic disciplines, LLM usage, stylistic shifts  
Summary:  
Large Language Models (LLMs), like ChatGPT, are transforming academic writing by providing AI-assisted generative revisions. Research investigates the impact of these revisions on over 627,000 academic papers from arXiv. Results show disparities in LLM adoption based on academic disciplines, gender, native language status, and career stage. LLM usage improves clarity, conciseness, and adherence to formal writing conventions, with effects varying by revision type. The study reveals a rapid evolution in scholarly writing styles and suggests that LLMs drive convergence in academic writing. Early adopters, male researchers, non-native speakers, and junior scholars exhibit the most significant stylistic shifts, aligning their writing more closely with established researchers. <div>
arXiv:2504.13629v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, are reshaping content creation and academic writing. This study investigates the impact of AI-assisted generative revisions on research manuscripts, focusing on heterogeneous adoption patterns and their influence on writing convergence. Leveraging a dataset of over 627,000 academic papers from arXiv, we develop a novel classification framework by fine-tuning prompt- and discipline-specific large language models to detect the style of ChatGPT-revised texts. Our findings reveal substantial disparities in LLM adoption across academic disciplines, gender, native language status, and career stage, alongside a rapid evolution in scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness, and adherence to formal writing conventions, with improvements varying by revision type. Finally, a difference-in-differences analysis shows that while LLMs drive convergence in academic writing, early adopters, male researchers, non-native speakers, and junior scholars exhibit the most pronounced stylistic shifts, aligning their writing more closely with that of established researchers.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling</title>
<link>https://arxiv.org/abs/2504.13630</link>
<guid>https://arxiv.org/abs/2504.13630</guid>
<content:encoded><![CDATA[
<div> metric framework, translation evaluation, neural metrics, quality translation, preference data
<br />
Summary:<br />
ReMedy is a novel MT metric framework that addresses the challenge of noisy human ratings in translation evaluation. By reformulating evaluation as a reward modeling task and using pairwise preference data to learn relative translation quality, ReMedy provides a more reliable assessment of translation performance. In extensive experiments across multiple WMT shared tasks, ReMedy outperforms state-of-the-art metrics at both segment- and system-level evaluation. It exceeds larger WMT winners and massive closed LLMs, demonstrating superior capability in detecting translation errors and evaluating low-quality translations. By focusing on relative quality rather than direct regression on human ratings, ReMedy offers a more robust and accurate means of evaluating machine translation systems. 
<br /> <div>
arXiv:2504.13630v1 Announce Type: new 
Abstract: A key challenge in MT evaluation is the inherent noise and inconsistency of human ratings. Regression-based neural metrics struggle with this noise, while prompting LLMs shows promise at system-level evaluation but performs poorly at segment level. In this work, we propose ReMedy, a novel MT metric framework that reformulates translation evaluation as a reward modeling task. Instead of regressing on imperfect human ratings directly, ReMedy learns relative translation quality using pairwise preference data, resulting in a more reliable evaluation. In extensive experiments across WMT22-24 shared tasks (39 language pairs, 111 MT systems), ReMedy achieves state-of-the-art performance at both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses larger WMT winners and massive closed LLMs such as MetricX-13B, XCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses demonstrate that ReMedy delivers superior capability in detecting translation errors and evaluating low-quality translations.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning</title>
<link>https://arxiv.org/abs/2504.13643</link>
<guid>https://arxiv.org/abs/2504.13643</guid>
<content:encoded><![CDATA[
<div> persona, dialogue policy planning, user characteristics, user-tailored, user feedback

Summary:
The article discusses the importance of incorporating user characteristics in dialogue policy planning for conversational systems. Existing approaches often overlook the individual traits of users, such as personality, preferences, and goals, which are crucial for real-world applications like conversational search and recommendation. The User-Tailored Dialogue Policy Planning (UDP) framework is proposed to address this gap. UDP incorporates an Intrinsic User World Model to model user traits and feedback, operating in three stages: User Persona Portraying, User Feedback Anticipating, and User-Tailored Policy Planning. An active learning approach is also introduced to prioritize challenging user personas during training. Comprehensive experiments on collaborative and non-collaborative settings demonstrate the effectiveness of UDP in learning user-specific dialogue strategies, highlighting its robustness, adaptability, and potential to advance user-centric dialogue systems.<br /><br />Summary: <div>
arXiv:2504.13643v1 Announce Type: new 
Abstract: Recent advancements in dialogue policy planning have emphasized optimizing system agent policies to achieve predefined goals, focusing on strategy design, trajectory acquisition, and efficient training paradigms. However, these approaches often overlook the critical role of user characteristics, which are essential in real-world scenarios like conversational search and recommendation, where interactions must adapt to individual user traits such as personality, preferences, and goals. To address this gap, we first conduct a comprehensive study utilizing task-specific user personas to systematically assess dialogue policy planning under diverse user behaviors. By leveraging realistic user profiles for different tasks, our study reveals significant limitations in existing approaches, highlighting the need for user-tailored dialogue policy planning. Building on this foundation, we present the User-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an Intrinsic User World Model to model user traits and feedback. UDP operates in three stages: (1) User Persona Portraying, using a diffusion model to dynamically infer user profiles; (2) User Feedback Anticipating, leveraging a Brownian Bridge-inspired anticipator to predict user reactions; and (3) User-Tailored Policy Planning, integrating these insights to optimize response strategies. To ensure robust performance, we further propose an active learning approach that prioritizes challenging user personas during training. Comprehensive experiments on benchmarks, including collaborative and non-collaborative settings, demonstrate the effectiveness of UDP in learning user-specific dialogue strategies. Results validate the protocol's utility and highlight UDP's robustness, adaptability, and potential to advance user-centric dialogue systems.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embedding Techniques for Classification of Star Ratings</title>
<link>https://arxiv.org/abs/2504.13653</link>
<guid>https://arxiv.org/abs/2504.13653</guid>
<content:encoded><![CDATA[
<div> Word Embedding, Natural Language Processing, Telecom Services, Text Classification, PCA <br />
Summary: <br />
Telecom services are essential for society, and customer feedback is crucial in improving service quality. This study examines how different word embedding models impact text classification in analyzing telecom customer reviews. Various state-of-the-art word embedding techniques, including BERT, Word2Vec, and Doc2Vec, were evaluated alongside different classification algorithms. Feature engineering and dimensionality reduction were also explored, with a focus on PCA-based approaches. The study found that certain word embeddings, such as BERT combined with PCA, consistently outperformed others in terms of precision, recall, and F1-Score. Additionally, a novel PCA approach of combining word vectors using the first principal component showed improved performance compared to the traditional averaging method. The research also considered the energy consumption associated with different word embeddings. Overall, the findings provide valuable insights for utilizing NLP tools to enhance telecom services based on customer feedback. <br /> <div>
arXiv:2504.13653v1 Announce Type: new 
Abstract: Telecom services are at the core of today's societies' everyday needs. The availability of numerous online forums and discussion platforms enables telecom providers to improve their services by exploring the views of their customers to learn about common issues that the customers face. Natural Language Processing (NLP) tools can be used to process the free text collected.
  One way of working with such data is to represent text as numerical vectors using one of many word embedding models based on neural networks. This research uses a novel dataset of telecom customers' reviews to perform an extensive study showing how different word embedding algorithms can affect the text classification process. Several state-of-the-art word embedding techniques are considered, including BERT, Word2Vec and Doc2Vec, coupled with several classification algorithms. The important issue of feature engineering and dimensionality reduction is addressed and several PCA-based approaches are explored. Moreover, the energy consumption used by the different word embeddings is investigated. The findings show that some word embedding models can lead to consistently better text classifiers in terms of precision, recall and F1-Score. In particular, for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics. Moreover, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2504.13655</link>
<guid>https://arxiv.org/abs/2504.13655</guid>
<content:encoded><![CDATA[
<div> contextual information, conversational recommender systems, multi-type, mixture-of-experts, structured information

Summary: 
The paper introduces a multi-type context-aware conversational recommender system (MCCRS) that leverages various types of contextual information to enhance recommendations. MCCRS combines structured knowledge graph, unstructured conversation history, and item reviews through a mixture-of-experts approach, with each expert specializing in a specific domain. A ChairBot coordinates multiple experts to generate final recommendations, addressing the challenge of integrating different contextual information effectively. Experimental results show that MCCRS outperforms existing baselines, demonstrating the effectiveness of leveraging multi-type contextual information and expert specialization in conversational recommender systems. <div>
arXiv:2504.13655v1 Announce Type: new 
Abstract: Conversational recommender systems enable natural language conversations and thus lead to a more engaging and effective recommendation scenario. As the conversations for recommender systems usually contain limited contextual information, many existing conversational recommender systems incorporate external sources to enrich the contextual information. However, how to combine different types of contextual information is still a challenge. In this paper, we propose a multi-type context-aware conversational recommender system, called MCCRS, effectively fusing multi-type contextual information via mixture-of-experts to improve conversational recommender systems. MCCRS incorporates both structured information and unstructured information, including the structured knowledge graph, unstructured conversation history, and unstructured item reviews. It consists of several experts, with each expert specialized in a particular domain (i.e., one specific contextual information). Multiple experts are then coordinated by a ChairBot to generate the final results. Our proposed MCCRS model takes advantage of different contextual information and the specialization of different experts followed by a ChairBot breaks the model bottleneck on a single contextual information. Experimental results demonstrate that our proposed MCCRS method achieves significantly higher performance compared to existing baselines.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results</title>
<link>https://arxiv.org/abs/2504.13677</link>
<guid>https://arxiv.org/abs/2504.13677</guid>
<content:encoded><![CDATA[
<div> AUROC, Uncertainty Quantification, Language Models, Correctness Functions, Bias 

Summary: 
- Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving safety and reliability, often evaluated using performance metrics like AUROC. 
- Evaluations using correctness functions like ROUGE-L may bias UQ assessments by inflating the performance of certain methods. 
- Analysis of 7 correctness functions across various datasets, models, and UQ methods reveals length biases causing distortions in assessments. 
- Error in correctness functions interacts with length biases in UQ methods, impacting evaluation accuracy. 
- LLM-as-a-judge approaches are identified as less length-biased choices, presenting a potential solution to mitigate biases. 

<br /><br /> <div>
arXiv:2504.13677v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep literature reviews: an application of fine-tuned language models to migration research</title>
<link>https://arxiv.org/abs/2504.13685</link>
<guid>https://arxiv.org/abs/2504.13685</guid>
<content:encoded><![CDATA[
<div> climate-induced migration, environmental hazards, human health, air and water pollution, infectious diseases 
Summary:
A hybrid framework for literature reviews combines traditional bibliometric methods with large language models (LLMs). By fine-tuning LLMs, qualitative insights can be extracted at scale from a large volume of research content. An error-focused validation process enhances annotation efficiency and consistency. Applying this framework to over 20,000 articles on human migration reveals a growing focus on climate-induced migration. However, there is a disproportionate emphasis on certain environmental hazards, neglecting others like air and water pollution and infectious diseases that impact human health. More comprehensive research is needed to explore the ecological and societal consequences of environmental changes on migration patterns. The study showcases the ability of LLMs to improve efficiency and depth of literature reviews, leading to enhanced knowledge synthesis and scientific discovery. 

<br /><br />Summary: <div>
arXiv:2504.13685v1 Announce Type: new 
Abstract: This paper presents a hybrid framework for literature reviews that augments traditional bibliometric methods with large language models (LLMs). By fine-tuning open-source LLMs, our approach enables scalable extraction of qualitative insights from large volumes of research content, enhancing both the breadth and depth of knowledge synthesis. To improve annotation efficiency and consistency, we introduce an error-focused validation process in which LLMs generate initial labels and human reviewers correct misclassifications. Applying this framework to over 20000 scientific articles about human migration, we demonstrate that a domain-adapted LLM can serve as a "specialist" model - capable of accurately selecting relevant studies, detecting emerging trends, and identifying critical research gaps. Notably, the LLM-assisted review reveals a growing scholarly interest in climate-induced migration. However, existing literature disproportionately centers on a narrow set of environmental hazards (e.g., floods, droughts, sea-level rise, and land degradation), while overlooking others that more directly affect human health and well-being, such as air and water pollution or infectious diseases. This imbalance highlights the need for more comprehensive research that goes beyond physical environmental changes to examine their ecological and societal consequences, particularly in shaping migration as an adaptive response. Overall, our proposed framework demonstrates the potential of fine-tuned LLMs to conduct more efficient, consistent, and insightful literature reviews across disciplines, ultimately accelerating knowledge synthesis and scientific discovery.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence</title>
<link>https://arxiv.org/abs/2504.13730</link>
<guid>https://arxiv.org/abs/2504.13730</guid>
<content:encoded><![CDATA[
<div> Keywords: Open-source intelligence, textual data, territorial control prediction, large language models, few-shot methods <br />
Summary: <br />
- The framework CONTACT utilizes large language models (LLMs) and minimal supervision for territorial control prediction, focusing on assessing ISIS activity in Syria and Iraq using news articles.
- Two approaches are evaluated: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. 
- The model is trained on a small hand-labeled dataset and employs prompt-conditioned extraction of control-relevant signals like military operations and casualties.
- Results indicate that the BLOOMZ-based model outperforms the SetFit baseline, showcasing the effectiveness of prompt-based supervision for generalization in low-resource settings.
- CONTACT demonstrates the potential of LLMs fine-tuned with few-shot methods to reduce annotation burdens and enable structured inference from open-ended OSINT streams. <br /> 
Summary: <div>
arXiv:2504.13730v1 Announce Type: new 
Abstract: Open-source intelligence provides a stream of unstructured textual data that can inform assessments of territorial control. We present CONTACT, a framework for territorial control prediction using large language models (LLMs) and minimal supervision. We evaluate two approaches: SetFit, an embedding-based few-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a multilingual generative LLM. Our model is trained on a small hand-labeled dataset of news articles covering ISIS activity in Syria and Iraq, using prompt-conditioned extraction of control-relevant signals such as military operations, casualties, and location references. We show that the BLOOMZ-based model outperforms the SetFit baseline, and that prompt-based supervision improves generalization in low-resource settings. CONTACT demonstrates that LLMs fine-tuned using few-shot methods can reduce annotation burdens and support structured inference from open-ended OSINT streams. Our code is available at https://github.com/PaulKMandal/CONTACT/.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models</title>
<link>https://arxiv.org/abs/2504.13775</link>
<guid>https://arxiv.org/abs/2504.13775</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attack, large language model, adaptive optimization mechanism, prompt adaptability, semantic consistency

Summary:
<br /><br />
The paper introduces a new backdoor attack, BadApex, that utilizes a black-box large language model (LLM) to generate poisoned text through a refined prompt. An Adaptive Optimization Mechanism is designed to iteratively refine the prompt using generation and modification agents, improving prompt adaptability, semantic consistency, and text quality. Extensive experiments on three datasets with six backdoor attacks and two defenses show that BadApex outperforms state-of-the-art attacks. It achieves high attack success rates even when defenses are applied, demonstrating its effectiveness in improving prompt adaptability, semantic consistency, and text quality. <div>
arXiv:2504.13775v1 Announce Type: new 
Abstract: Previous insertion-based and paraphrase-based backdoors have achieved great success in attack efficacy, but they ignore the text quality and semantic consistency between poisoned and clean texts. Although recent studies introduce LLMs to generate poisoned texts and improve the stealthiness, semantic consistency, and text quality, their hand-crafted prompts rely on expert experiences, facing significant challenges in prompt adaptability and attack performance after defenses. In this paper, we propose a novel backdoor attack based on adaptive optimization mechanism of black-box large language models (BadApex), which leverages a black-box LLM to generate poisoned text through a refined prompt. Specifically, an Adaptive Optimization Mechanism is designed to refine an initial prompt iteratively using the generation and modification agents. The generation agent generates the poisoned text based on the initial prompt. Then the modification agent evaluates the quality of the poisoned text and refines a new prompt. After several iterations of the above process, the refined prompt is used to generate poisoned texts through LLMs. We conduct extensive experiments on three dataset with six backdoor attacks and two defenses. Extensive experimental results demonstrate that BadApex significantly outperforms state-of-the-art attacks. It improves prompt adaptability, semantic consistency, and text quality. Furthermore, when two defense methods are applied, the average attack success rate (ASR) still up to 96.75%.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations</title>
<link>https://arxiv.org/abs/2504.13816</link>
<guid>https://arxiv.org/abs/2504.13816</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, knowledge boundaries, cross-lingual analysis, language differences, fine-tuning

Summary:<br /><br />LLMs encode perceptions of knowledge boundaries in middle to middle-upper layers across different languages, according to a new study. Language differences in knowledge boundary perception follow a linear structure, prompting the development of a training-free alignment method to transfer this ability across languages and reduce hallucination risk in low-resource languages. Fine-tuning on bilingual question pair translation further improves LLMs' recognition of knowledge boundaries across languages. Additionally, a multilingual evaluation suite with various types of knowledge boundary data has been created due to the lack of standard testbeds for cross-lingual analysis. The code and datasets for this research are openly available at the specified GitHub repository. <div>
arXiv:2504.13816v1 Announce Type: new 
Abstract: While understanding the knowledge boundaries of LLMs is crucial to prevent hallucination, research on knowledge boundaries of LLMs has predominantly focused on English. In this work, we present the first study to analyze how LLMs recognize knowledge boundaries across different languages by probing their internal representations when processing known and unknown questions in multiple languages. Our empirical studies reveal three key findings: 1) LLMs' perceptions of knowledge boundaries are encoded in the middle to middle-upper layers across different languages. 2) Language differences in knowledge boundary perception follow a linear structure, which motivates our proposal of a training-free alignment method that effectively transfers knowledge boundary perception ability across languages, thereby helping reduce hallucination risk in low-resource languages; 3) Fine-tuning on bilingual question pair translation further enhances LLMs' recognition of knowledge boundaries across languages. Given the absence of standard testbeds for cross-lingual knowledge boundary analysis, we construct a multilingual evaluation suite comprising three representative types of knowledge boundary data. Our code and datasets are publicly available at https://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models</title>
<link>https://arxiv.org/abs/2504.13825</link>
<guid>https://arxiv.org/abs/2504.13825</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge distillation, teacher models, student models, attention mechanisms, language models

Summary: 
Knowledge distillation (KD) is a technique that transfers knowledge from complex teacher models to simpler student models, improving efficiency and accuracy. Recent innovations in KD methods, such as attention-based approaches and block-wise logit distillation, have enhanced student model performance by focusing on stimulus complexity and global information capture. KD has also been successful in compressing large language models while maintaining accuracy and improving inference speed. This survey synthesizes the latest literature on KD, highlighting key findings and contributions, and discussing future directions for research and application in artificial intelligence and machine learning.

<br /><br />Summary: <div>
arXiv:2504.13825v1 Announce Type: new 
Abstract: Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI Act II: Test Time Scaling Drives Cognition Engineering</title>
<link>https://arxiv.org/abs/2504.13828</link>
<guid>https://arxiv.org/abs/2504.13828</guid>
<content:encoded><![CDATA[
<div> scaling, cognition engineering, Large Language Models, prompt engineering, knowledge latency

Summary:
In this paper, the authors discuss the transition from "Act I" to "Act II" in the field of generative AI, focusing on cognition engineering and test-time scaling techniques. The first generation of Large Language Models achieved success but had limitations in knowledge latency and shallow reasoning. Prompt engineering was crucial for communication with AI through natural language. In "Act II", models are evolving into thought-construction engines through scaling methods, creating a mind-level connection with AI. The paper provides tutorials and implementations to democratize access to cognition engineering. The authors emphasize the importance of this moment for the development of cognition engineering and offer a GitHub repository for updated resources on test-time scaling. This shift marks a critical phase in advancing AI capabilities and enabling practitioners to engage in AI's second act. 

<br /><br />Summary: <div>
arXiv:2504.13828v1 Announce Type: new 
Abstract: The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations in knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific literature, hierarchical structure, clustering algorithms, LLM-based prompting, interdisciplinary research

Summary:
SCIENCE HIERARCHOGRAPHY aims to organize scientific literature into a hierarchical structure for categorization across varying levels of abstraction. The approach involves combining fast embedding-based clustering with LLM-based prompting to efficiently balance computational speed and semantic precision. This method proves to be more effective than relying solely on LLM prompting. The hierarchical framework captures multiple dimensions of categorization beyond simple topics, reflecting the interdisciplinary nature of research papers. Evaluations demonstrate the framework's utility in enhancing interpretability, supporting trend discovery, and providing an alternative pathway for exploring scientific literature. The code, data, and demo for SCIENCE HIERARCHOGRAPHY are available on GitHub for further exploration. <br /><br />Summary: <div>
arXiv:2504.13834v1 Announce Type: new 
Abstract: Scientific knowledge is growing rapidly, making it challenging to track progress and high-level conceptual links across broad disciplines. While existing tools like citation networks and search engines make it easy to access a few related papers, they fundamentally lack the flexible abstraction needed to represent the density of activity in various scientific subfields. We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that allows for the categorization of scientific work across varying levels of abstraction, from very broad fields to very specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve the goals of SCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach combines fast embedding-based clustering with LLM-based prompting to balance the computational efficiency of embedding methods with the semantic precision offered by LLM prompting. We demonstrate that this approach offers the best trade-off between quality and speed compared to methods that heavily rely on LLM prompting, such as iterative tree construction with LLMs. To better reflect the interdisciplinary and multifaceted nature of research papers, our hierarchy captures multiple dimensions of categorization beyond simple topic labels. We evaluate the utility of our framework by assessing how effectively an LLM-based agent can locate target papers using the hierarchy. Results show that this structured approach enhances interpretability, supports trend discovery, and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo: $\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space</title>
<link>https://arxiv.org/abs/2504.13835</link>
<guid>https://arxiv.org/abs/2504.13835</guid>
<content:encoded><![CDATA[
<div> Keywords: Data quality, diversity, instruction-tuning datasets, semantic space, information gain

Summary:
The article introduces a new method for automatically selecting high-quality and diverse subsets from instruction-tuning datasets. Existing methods often rely on heuristic rules, leading to suboptimal results. The proposed method quantifies the information content of datasets by modeling the semantic space using a label graph. Diversity is measured based on the distribution of information within the graph. An efficient sampling method, called MIG (Maximize Information Gain), is introduced to select data samples iteratively in the semantic space. Experimental results show that MIG outperforms state-of-the-art methods consistently. Fine-tuning a model with 5% Tulu3 data sampled using MIG achieves comparable performance to the official SFT model trained on the full dataset, with significant improvements on AlpacaEval and Wildbench datasets. <div>
arXiv:2504.13835v1 Announce Type: new 
Abstract: Data quality and diversity are key to the construction of effective instruction-tuning datasets. %
With the increasing availability of open-source instruction-tuning datasets, it is advantageous to automatically select high-quality and diverse subsets from a vast amount of data. %
Existing methods typically prioritize instance quality and use heuristic rules to maintain diversity. %
However, this absence of a comprehensive view of the entire collection often leads to suboptimal results. %
Moreover, heuristic rules generally focus on distance or clustering within the embedding space, which fails to accurately capture the intent of complex instructions in the semantic space. %
To bridge this gap, we propose a unified method for quantifying the information content of datasets. This method models the semantic space by constructing a label graph and quantifies diversity based on the distribution of information within the graph. %
Based on such a measurement, we further introduce an efficient sampling method that selects data samples iteratively to \textbf{M}aximize the \textbf{I}nformation \textbf{G}ain (MIG) in semantic space. %
Experiments on various datasets and base models demonstrate that MIG consistently outperforms state-of-the-art methods. %
Notably, the model fine-tuned with 5\% Tulu3 data sampled by MIG achieves comparable performance to the official SFT model trained on the full dataset, with improvements of +5.73\% on AlpacaEval and +6.89\% on Wildbench.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Quantum LLM: Modeling Semantic Spaces with Quantum Principles</title>
<link>https://arxiv.org/abs/2504.13202</link>
<guid>https://arxiv.org/abs/2504.13202</guid>
<content:encoded><![CDATA[
<div> framework, semantic representation, quantum mechanics, language models, quantum computing
Summary:
In the article, a quantum-inspired framework for modeling semantic representation and processing in Large Language Models (LLMs) is presented. The framework is based on mathematical tools and analogies from quantum mechanics. Six key principles governing semantic representation, interaction, and dynamics within LLMs are detailed, demonstrating the validity of the quantum-inspired approach. The framework provides insights into information processing and response generation in LLMs. The potential of utilizing quantum computing to enhance the efficiency and power of LLMs based on these principles is discussed. <div>
arXiv:2504.13202v1 Announce Type: cross 
Abstract: In the previous article, we presented a quantum-inspired framework for modeling semantic representation and processing in Large Language Models (LLMs), drawing upon mathematical tools and conceptual analogies from quantum mechanics to offer a new perspective on these complex systems. In this paper, we clarify the core assumptions of this model, providing a detailed exposition of six key principles that govern semantic representation, interaction, and dynamics within LLMs. The goal is to justify that a quantum-inspired framework is a valid approach to studying semantic spaces. This framework offers valuable insights into their information processing and response generation, and we further discuss the potential of leveraging quantum computing to develop significantly more powerful and efficient LLMs based on these principles.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
<div> Framework, Language Models, Multi-turn Interactions, Safety, Attacks  
Summary:  
X-Teaming is introduced as a scalable framework for exploring how seemingly harmless interactions can escalate into harmful outcomes in multi-turn interactions with language models (LMs). This framework utilizes collaborative agents for planning, attack optimization, and verification, achieving high success rates in multi-turn jailbreak effectiveness. X-Teaming demonstrates state-of-the-art performance in generating attack scenarios against both open-weight and closed-source models, including achieving a 96.2% success rate against the previously considered immune Claude 3.7 Sonnet model. Additionally, XGuard-Train, a large-scale multi-turn safety training dataset, is introduced to enhance the robustness of multi-turn safety alignment for LMs. These tools and insights provided by the study contribute towards mitigating sophisticated conversational attacks and advancing the safety of LMs in multi-turn interactions.  
<br /><br />Summary: <div>
arXiv:2504.13203v1 Announce Type: cross 
Abstract: Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces</title>
<link>https://arxiv.org/abs/2504.13277</link>
<guid>https://arxiv.org/abs/2504.13277</guid>
<content:encoded><![CDATA[
<div> Keywords: Suicide, Suicidal Ideation, Interpersonal Theory of Suicide, Machine Learning, AI chatbots

Summary: 
This study examines suicidal ideation (SI) posts on Reddit's r/SuicideWatch using the Interpersonal Theory of Suicide (IPTS). Analysis of 59,607 posts categorizes them into SI dimensions and risk factors, revealing that high-risk SI posts often express planning, attempts, and pain. Supportive responses varied based on the stage of SI posts. The study also explores the effectiveness of AI chatbots in providing support, finding improvements in structural coherence but limitations in personalized and empathetic responses. The findings emphasize the importance of understanding and developing AI-driven interventions for mental health support. <br /><br />Summary: <div>
arXiv:2504.13277v1 Announce Type: cross 
Abstract: Suicide is a critical global public health issue, with millions experiencing suicidal ideation (SI) each year. Online spaces enable individuals to express SI and seek peer support. While prior research has revealed the potential of detecting SI using machine learning and natural language analysis, a key limitation is the lack of a theoretical framework to understand the underlying factors affecting high-risk suicidal intent. To bridge this gap, we adopted the Interpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607 posts from Reddit's r/SuicideWatch, categorizing them into SI dimensions (Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk factors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired Capability of Suicide). We found that high-risk SI posts express planning and attempts, methods and tools, and weaknesses and pain. In addition, we also examined the language of supportive responses through psycholinguistic and content analyses to find that individuals respond differently to different stages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI chatbots in providing effective supportive responses to suicidal ideation posts. We found that although AI improved structural coherence, expert evaluations highlight persistent shortcomings in providing dynamic, personalized, and deeply empathetic support. These findings underscore the need for careful reflection and deeper understanding in both the development and consideration of AI-driven interventions for effective mental health support.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope</title>
<link>https://arxiv.org/abs/2504.13308</link>
<guid>https://arxiv.org/abs/2504.13308</guid>
<content:encoded><![CDATA[
<div> Keywords: Acoustic-to-Articulatory Inversion, Speaker Dependent, Speaker Independent, Automatic Speech Recognition, Machine Learning

Summary:
This review discusses data-driven approaches in Acoustic-to-Articulatory Inversion (AAI) of speech over the last decade. The focus is on Speaker Dependent and Speaker Independent AAI for tasks such as Articulatory approximation, Articulatory Feature space selection, and Automatic Speech Recognition. Various medical imaging models and speech corpora are used, including Electromagnetic Articulography and real-time Magnetic Resonance Imaging. Machine learning methods are predominantly employed in recent works, with evaluation based on metrics like Correlation Coefficient and Root Mean Square Error. The practical applications of AAI include providing interpretable feedback on articulatory positions, particularly tongue movements, with potential applications in phonetic, language, and speech therapy for pathological subjects.<br /><br />Summary: <div>
arXiv:2504.13308v1 Announce Type: cross 
Abstract: This review is focused on the data-driven approaches applied in different applications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review paper considered the relevant works published in the last ten years (2011-2021). The selection criteria includes (a) type of AAI - Speaker Dependent and Speaker Independent AAI, (b) objectives of the work - Articulatory approximation, Articulatory Feature space selection and Automatic Speech Recognition (ASR), explore the correlation between acoustic and articulatory features, and framework for Computer-assisted language training, (c) Corpus - Simultaneously recorded speech (wav) and medical imaging models such as ElectroMagnetic Articulography (EMA), Electropalatography (EPG), Laryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound, and real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models - recent works are considered, and therefore all the works are based on machine learning, (e) Evaluation - as AAI is a non-linear regression problem, the performance evaluation is mostly done by Correlation Coefficient (CC), Root Mean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean Format Error (MFE). The practical application of the AAI model can provide a better and user-friendly interpretable image feedback system of articulatory positions, especially tongue movement. Such trajectory feedback system can be used to provide phonetic, language, and speech therapy for pathological subjects.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-of-Pass: An Economic Framework for Evaluating Language Models</title>
<link>https://arxiv.org/abs/2504.13359</link>
<guid>https://arxiv.org/abs/2504.13359</guid>
<content:encoded><![CDATA[
<div> production theory, language models, cost-of-pass, economic value, inference cost

Summary:
The article proposes a framework for evaluating language models based on accuracy and inference cost using production theory. It introduces the concept of "cost-of-pass" as the expected monetary cost of generating a correct solution and the "frontier cost-of-pass" as the minimum achievable cost across available models. The analysis reveals that lightweight models are cost-effective for basic tasks, large models for knowledge-intensive tasks, and reasoning models for complex quantitative problems. Progress over the past year shows significant cost reductions, particularly for complex tasks. Innovations in lightweight, large, and reasoning models have been key drivers of cost-efficiency. Common inference-time techniques like majority voting and self-refinement may not justify their costs in terms of marginal accuracy gains. The findings emphasize the importance of complementary model-level innovations in driving cost-efficiency, and the economic framework provided can guide deployment strategies. 

Summary: <div>
arXiv:2504.13359v1 Announce Type: cross 
Abstract: The widespread adoption of AI systems in the economy hinges on their ability to generate economic value that outweighs their inference costs. Evaluating this tradeoff requires metrics that account for both performance and costs. We propose a framework grounded in production theory for evaluating language models by combining accuracy and inference cost. We introduce "cost-of-pass", the expected monetary cost of generating a correct solution. We then define the "frontier cost-of-pass" as the minimum cost-of-pass achievable across available models or the "human-expert, using the approximate cost of hiring an expert. Our analysis reveals distinct economic insights. First, lightweight models are most cost-effective for basic quantitative tasks, large models for knowledge-intensive ones, and reasoning models for complex quantitative problems, despite higher per-token costs. Second, tracking this frontier cost-of-pass over the past year reveals significant progress, particularly for complex quantitative tasks where the cost has roughly halved every few months. Third, to trace key innovations driving this progress, we examine counterfactual frontiers: estimates of cost-efficiency without specific model classes. We find that innovations in lightweight, large, and reasoning models have been essential for pushing the frontier in basic quantitative, knowledge-intensive, and complex quantitative tasks, respectively. Finally, we assess the cost-reductions afforded by common inference-time techniques like majority voting and self-refinement, finding that their marginal accuracy gains rarely justify their costs. Our findings underscore that complementary model-level innovations are the primary drivers of cost-efficiency, and our economic framework provides a principled tool for measuring this progress and guiding deployment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A mean teacher algorithm for unlearning of language models</title>
<link>https://arxiv.org/abs/2504.13388</link>
<guid>https://arxiv.org/abs/2504.13388</guid>
<content:encoded><![CDATA[
<div> mean teacher algorithm, language model unlearning, memorization reduction, natural gradient descent, negative log-unlikelihood<br />
Summary:<br />
This paper explores the application of the mean teacher algorithm in language model unlearning to reduce memorization while preserving general abilities. The mean teacher algorithm approximates a slow natural gradient descent trajectory to minimize model degradation during unlearning. To prevent vanishing gradients, a new unlearning loss called negative log-unlikelihood is introduced. The combination of mean teacher and negative log-unlikelihood demonstrates improvements on MUSE benchmarks. This approach addresses the challenge of reducing memorization in large datasets without compromising model utility, offering a promising solution for continual learning and model adaptability in natural language processing tasks. <div>
arXiv:2504.13388v1 Announce Type: cross 
Abstract: One of the goals of language model unlearning is to reduce memorization of selected text instances while retaining the model's general abilities. Despite various proposed methods, reducing memorization of large datasets without noticeable degradation in model utility remains challenging. In this paper, we investigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple proximal optimization method from continual learning literature that gradually modifies the teacher model. We show that the mean teacher can approximate a trajectory of a slow natural gradient descent (NGD), which inherently seeks low-curvature updates that are less likely to degrade the model utility. While slow NGD can suffer from vanishing gradients, we introduce a new unlearning loss called "negative log-unlikelihood" (NLUL) that avoids this problem. We show that the combination of mean teacher and NLUL improves some metrics on the MUSE benchmarks (Shi et al., 2024).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LangCoop: Collaborative Driving with Language</title>
<link>https://arxiv.org/abs/2504.13406</link>
<guid>https://arxiv.org/abs/2504.13406</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaboration, autonomous driving, natural language, communication, information sharing

Summary: LangCoop proposes a new approach to enhance collaborative autonomous driving systems by utilizing natural language for inter-agent communication. It introduces M$^3$CoT for structured zero-shot vision-language reasoning and LangPack for efficient information packaging. Through experiments in CARLA simulations, LangCoop significantly reduces communication bandwidth (< 2KB per message) compared to image-based methods while maintaining high driving performance. This innovative paradigm addresses challenges such as high bandwidth demands, agent heterogeneity, and information loss in multi-agent communication. <div>
arXiv:2504.13406v1 Announce Type: cross 
Abstract: Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that leverages natural language as a compact yet expressive medium for inter-agent communication. LangCoop features two key innovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured zero-shot vision-language reasoning and Natural Language Information Packaging (LangPack) for efficiently packaging information into concise, language-based messages. Through extensive experiments conducted in the CARLA simulations, we demonstrate that LangCoop achieves a remarkable 96\% reduction in communication bandwidth (< 2KB per message) compared to image-based communication, while maintaining competitive driving performance in the closed-loop evaluation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings</title>
<link>https://arxiv.org/abs/2504.13416</link>
<guid>https://arxiv.org/abs/2504.13416</guid>
<content:encoded><![CDATA[
<div> watermarking, language models, dataset membership, data contamination, pretraining corpora <br />
Summary: <br />
The paper introduces STAMP, a framework designed to detect dataset membership in the pretraining corpora of large language models (LLMs). STAMP involves embedding unique watermarks in multiple rephrased versions of an original piece of content, with one version made public and others kept private. By comparing model likelihoods between the public and private versions, creators can use paired statistical tests to prove dataset inclusion. The framework successfully detects dataset contamination in four benchmarks, outperforming other detection methods. STAMP maintains the semantic meaning and utility of the original data while comparing models. The approach is applied to detect the inclusion of paper abstracts and blog articles in pretraining corpora, addressing concerns about proprietary data usage and dataset integrity in model training processes. <br /> <div>
arXiv:2504.13416v1 Announce Type: cross 
Abstract: Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and the utility of the original data in comparing different models. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation</title>
<link>https://arxiv.org/abs/2504.13472</link>
<guid>https://arxiv.org/abs/2504.13472</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, code generation, evaluation, multisource domain knowledge, complex code<br />
<br />
Summary: <br />
The article introduces CodeVisionary, a novel framework for evaluating Large Language Models (LLMs) in code generation tasks. CodeVisionary addresses the limitations of existing evaluation approaches by incorporating a multisource knowledge analysis stage to gather comprehensive domain knowledge and a negotiation-based scoring stage where judges engage in discussions to better understand complex code. Through extensive experiments, CodeVisionary outperforms baseline methods in evaluation metrics such as Pearson, Spearman, and Kendall-Tau coefficients. Additionally, CodeVisionary provides detailed evaluation reports to help developers identify areas for improvement. The resources for CodeVisionary are available online, making it a valuable tool for evaluating LLMs in code generation tasks. <div>
arXiv:2504.13472v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code.
  To mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Locality-Aware Attention with Transformers for General Geometry PDEs</title>
<link>https://arxiv.org/abs/2504.13480</link>
<guid>https://arxiv.org/abs/2504.13480</guid>
<content:encoded><![CDATA[
<div> Transformer-based neural operators, such as LA2Former, improve predictive accuracy in solving PDEs. Keywords: Neural operators, partial differential equations, Transformer, attention mechanism, local interactions <br />Summary: LA2Former is a new approach for solving PDEs using Transformer-based neural operators that leverage K-nearest neighbors for dynamic patchifying and integrate global-local attention for enhanced PDE modeling. By combining global context encoding with pairwise attention for local interactions, LA2Former achieves optimal balance between efficiency and accuracy. Extensive evaluations show LA2Former improves predictive accuracy by over 50% compared to existing methods, outperforming full pairwise attention under optimal conditions. This work highlights the importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains. <br /><br /> <div>
arXiv:2504.13480v1 Announce Type: cross 
Abstract: Neural operators have emerged as promising frameworks for learning mappings governed by partial differential equations (PDEs), serving as data-driven alternatives to traditional numerical methods. While methods such as the Fourier neural operator (FNO) have demonstrated notable performance, their reliance on uniform grids restricts their applicability to complex geometries and irregular meshes. Recently, Transformer-based neural operators with linear attention mechanisms have shown potential in overcoming these limitations for large-scale PDE simulations. However, these approaches predominantly emphasize global feature aggregation, often overlooking fine-scale dynamics and localized PDE behaviors essential for accurate solutions. To address these challenges, we propose the Locality-Aware Attention Transformer (LA2Former), which leverages K-nearest neighbors for dynamic patchifying and integrates global-local attention for enhanced PDE modeling. By combining linear attention for efficient global context encoding with pairwise attention for capturing intricate local interactions, LA2Former achieves an optimal balance between computational efficiency and predictive accuracy. Extensive evaluations across six benchmark datasets demonstrate that LA2Former improves predictive accuracy by over 50% relative to existing linear attention methods, while also outperforming full pairwise attention under optimal conditions. This work underscores the critical importance of localized feature learning in advancing Transformer-based neural operators for solving PDEs on complex and irregular domains.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation</title>
<link>https://arxiv.org/abs/2504.13551</link>
<guid>https://arxiv.org/abs/2504.13551</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial attacks, language models, black-box attacks, Q-faker, surrogate model 

Summary: 
Q-faker is a novel method for generating adversarial examples without accessing the target model, using a surrogate model instead. The approach aims to overcome the limitations of traditional adversarial attack methods by providing a query-free hard black-box attacker. By leveraging controlled generation techniques, the method demonstrates high transferability and the production of high-quality adversarial examples across various datasets. The proposed approach eliminates the need for extensive queries and expensive training costs associated with existing attack methods, making it practical for real-world scenarios where the target model is closed and inaccessible. Experimental results validate the effectiveness of Q-faker in hard black-box settings, showcasing its potential for generating adversarial examples efficiently and accurately. 

<br /><br />Summary: <div>
arXiv:2504.13551v1 Announce Type: cross 
Abstract: Many adversarial attack approaches are proposed to verify the vulnerability of language models. However, they require numerous queries and the information on the target model. Even black-box attack methods also require the target model's output information. They are not applicable in real-world scenarios, as in hard black-box settings where the target model is closed and inaccessible. Even the recently proposed hard black-box attacks still require many queries and demand extremely high costs for training adversarial generators. To address these challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a novel and efficient method that generates adversarial examples without accessing the target model. To avoid accessing the target model, we use a surrogate model instead. The surrogate model generates adversarial sentences for a target-agnostic attack. During this process, we leverage controlled generation techniques. We evaluate our proposed method on eight datasets. Experimental results demonstrate our method's effectiveness including high transferability and the high quality of the generated adversarial examples, and prove its practical in hard black-box settings.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs</title>
<link>https://arxiv.org/abs/2504.13644</link>
<guid>https://arxiv.org/abs/2504.13644</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, probabilistic reasoning, information retrieval, automated decision systems, uncertainty quantification

Summary: 
Large language models (LLMs) have become essential tools in information retrieval and automated decision systems. However, a crucial aspect of their performance lies in the representation of probabilistic reasoning. Despite previous beliefs that LLMs can exhibit complex reasoning and accurate uncertainty quantification, recent studies reveal shortcomings in their ability to provide logical and coherent probabilistic beliefs. To address this, a new dataset of claims with uncertain truth values was introduced, and established techniques for uncertainty quantification were employed to assess the LLMs' adherence to foundational principles of probabilistic reasoning. The findings indicate that current iterations of LLMs fall short in delivering rational and consistent probabilistic representations, highlighting the need for further research and development in this area. 

<br /><br />Summary: <div>
arXiv:2504.13644v1 Announce Type: cross 
Abstract: Advances in the general capabilities of large language models (LLMs) have led to their use for information retrieval, and as components in automated decision systems. A faithful representation of probabilistic reasoning in these models may be essential to ensure trustworthy, explainable and effective performance in these tasks. Despite previous work suggesting that LLMs can perform complex reasoning and well-calibrated uncertainty quantification, we find that current versions of this class of model lack the ability to provide rational and coherent representations of probabilistic beliefs. To demonstrate this, we introduce a novel dataset of claims with indeterminate truth values and apply a number of well-established techniques for uncertainty quantification to measure the ability of LLM's to adhere to fundamental properties of probabilistic reasoning.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm</title>
<link>https://arxiv.org/abs/2504.13667</link>
<guid>https://arxiv.org/abs/2504.13667</guid>
<content:encoded><![CDATA[
<div> education, Large Language Models, technology, interactive systems designers, self-ethnographic study

Summary: The paper discusses the potential impacts of Large Language Models (LLMs) on education and technology interaction for children. It highlights the relatively minor effects of LLMs on education thus far and predicts significant upcoming changes. A scenario and self-ethnographic study are presented to demonstrate these changes. Additionally, five key considerations for interactive systems designers in the future are identified. These considerations emphasize the need to adapt to the evolving role of LLMs in shaping learning experiences and technological interactions for children. <div>
arXiv:2504.13667v1 Announce Type: cross 
Abstract: This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation</title>
<link>https://arxiv.org/abs/2504.13707</link>
<guid>https://arxiv.org/abs/2504.13707</guid>
<content:encoded><![CDATA[
<div> evaluate, large language models, deception risks, OpenDeception, agent applications
Summary: 
- OpenDeception is introduced as a novel framework to evaluate deception risks in large language models (LLMs) used in agent applications.
- The framework includes an open-ended scenario dataset to assess both the deception intention and capabilities of LLM-based agents by analyzing their internal reasoning process.
- Five common use case types with ten diverse scenarios each are constructed for evaluation.
- To avoid ethical concerns, simulations of multi-turn dialogues are proposed instead of interactions with human testers.
- Evaluation of eleven mainstream LLMs on OpenDeception reveals a high deception intention ratio and success rate, calling for measures to address deception risks and security concerns in LLM-based agents. Stronger LLM capabilities are linked to a higher risk of deception, emphasizing the need for efforts to curb deceptive behaviors.br><br />Summary: <div>
arXiv:2504.13707v1 Announce Type: cross 
Abstract: As the general capabilities of large language models (LLMs) improve and agent applications become more widespread, the underlying deception risks urgently require systematic evaluation and effective oversight. Unlike existing evaluation which uses simulated games or presents limited choices, we introduce OpenDeception, a novel deception evaluation framework with an open-ended scenario dataset. OpenDeception jointly evaluates both the deception intention and capabilities of LLM-based agents by inspecting their internal reasoning process. Specifically, we construct five types of common use cases where LLMs intensively interact with the user, each consisting of ten diverse, concrete scenarios from the real world. To avoid ethical concerns and costs of high-risk deceptive interactions with human testers, we propose to simulate the multi-turn dialogue via agent simulation. Extensive evaluation of eleven mainstream LLMs on OpenDeception highlights the urgent need to address deception risks and security concerns in LLM-based agents: the deception intention ratio across the models exceeds 80%, while the deception success rate surpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do exhibit a higher risk of deception, which calls for more alignment efforts on inhibiting deceptive behaviors.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Attribute with Attention</title>
<link>https://arxiv.org/abs/2504.13752</link>
<guid>https://arxiv.org/abs/2504.13752</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, token attribution, attention weights, Attribution with Attention, question answering

Summary:
The study introduces a method called Attribution with Attention (AT2) to efficiently attribute the influence of preceding tokens in language models. Traditional methods involve ablating tokens, but AT2 leverages attention weights as features to accurately estimate a token's impact on model behavior. By treating attention weights from different attention heads as features, AT2 delivers reliable attributions comparable to ablation-based methods but with significantly lower computational cost. The authors demonstrate the efficacy of AT2 by applying it to prune less important parts of context in a question answering task, leading to improved answer quality. The code for AT2 is available on GitHub for further exploration and implementation. Overall, AT2 offers a practical and efficient solution for token attribution in language models, with potential applications in various natural language processing tasks. 

<br /><br />Summary: <div>
arXiv:2504.13752v1 Announce Type: cross 
Abstract: Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as features. This way, we can learn how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, we use it to prune less important parts of a provided context in a question answering setting, improving answer quality. We provide code for AT2 at https://github.com/MadryLab/AT2 .
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling sparse feature circuit finding for in-context learning</title>
<link>https://arxiv.org/abs/2504.13756</link>
<guid>https://arxiv.org/abs/2504.13756</guid>
<content:encoded><![CDATA[
arXiv:2504.13756v1 Announce Type: cross 
Abstract: Sparse autoencoders (SAEs) are a popular tool for interpreting large language model activations, but their utility in addressing open questions in interpretability remains unclear. In this work, we demonstrate their effectiveness by using SAEs to deepen our understanding of the mechanism behind in-context learning (ICL). We identify abstract SAE features that (i) encode the model's knowledge of which task to execute and (ii) whose latent vectors causally induce the task zero-shot. This aligns with prior work showing that ICL is mediated by task vectors. We further demonstrate that these task vectors are well approximated by a sparse sum of SAE latents, including these task-execution features. To explore the ICL mechanism, we adapt the sparse feature circuits methodology of Marks et al. (2024) to work for the much larger Gemma-1 2B model, with 30 times as many parameters, and to the more complex task of ICL. Through circuit finding, we discover task-detecting features with corresponding SAE latents that activate earlier in the prompt, that detect when tasks have been performed. They are causally linked with task-execution features through the attention and MLP sublayers.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13818</link>
<guid>https://arxiv.org/abs/2504.13818</guid>
<content:encoded><![CDATA[
arXiv:2504.13818v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing reasoning capabilities in large language models, but faces a fundamental asymmetry in computation and memory requirements: inference is embarrassingly parallel with a minimal memory footprint, while policy updates require extensive synchronization and are memory-intensive. To address this asymmetry, we introduce PODS (Policy Optimization with Down-Sampling), a framework that strategically decouples these phases by generating numerous rollouts in parallel but updating only on an informative subset. Within this framework, we develop max-variance down-sampling, a theoretically motivated method that selects rollouts with maximally diverse reward signals. We prove that this approach has an efficient algorithmic solution, and empirically demonstrate that GRPO with PODS using max-variance down-sampling achieves superior performance over standard GRPO on the GSM8K benchmark.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning capabilities of LLMs, particularly in mathematics and programming tasks. It is widely believed that RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed corresponding base models' capacity. In this study, however, we critically re-examines this assumption by measuring the pass@\textit{k} metric with large values of \textit{k} to explore the reasoning capability boundary of the models across a wide range of model families and benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally new reasoning patterns. While RL-trained models outperform their base models at smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or even higher pass@$k$ score compared to their RL counterparts at large $k$ values. The reasoning paths generated by RL-trained models are already included in the base models' sampling distribution, suggesting that most reasoning abilities manifested in RL-trained models are already obtained by base models. Further analysis shows that RL training boosts the performance by biasing the model's output distribution toward paths that are more likely to yield rewards, therefore sampling correct responses more efficiently. But this also results in a narrower reasoning capability boundary compared to base models. Similar results are observed in visual reasoning tasks trained with RLVR. Moreover, we find that distillation can genuinely introduce new knowledge into the model, different from RLVR. These findings underscore a critical limitation of RLVR in advancing LLM reasoning abilities which requires us to fundamentally rethink the impact of RL training in reasoning LLMs and the need of a better paradigm. Project Page: https://limit-of-RLVR.github.io
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation</title>
<link>https://arxiv.org/abs/2401.07456</link>
<guid>https://arxiv.org/abs/2401.07456</guid>
<content:encoded><![CDATA[
arXiv:2401.07456v2 Announce Type: replace 
Abstract: Federated learning (FL) is a promising distributed machine learning paradigm that enables multiple clients to collaboratively train a global model. In this paper, we focus on a practical federated multilingual learning setup where clients with their own language-specific data aim to collaboratively construct a high-quality neural machine translation (NMT) model. However, communication constraints in practical network systems present challenges for exchanging large-scale NMT engines between FL parties. We propose a meta-learning-based adaptive parameter selection methodology, MetaSend, that improves the communication efficiency of model transmissions from clients during FL-based multilingual NMT training. Our approach learns a dynamic threshold for filtering parameters prior to transmission without compromising the NMT model quality, based on the tensor deviations of clients between different FL rounds. Through experiments on two NMT datasets with different language distributions, we demonstrate that MetaSend obtains substantial improvements over baselines in translation quality in the presence of a limited communication budget.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of LLM Sampling: Part Descriptive and Part Prescriptive</title>
<link>https://arxiv.org/abs/2402.11005</link>
<guid>https://arxiv.org/abs/2402.11005</guid>
<content:encoded><![CDATA[
arXiv:2402.11005v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly utilized in autonomous decision-making, where they sample options from vast action spaces. However, the heuristics that guide this sampling process remain under-explored. We study this sampling behavior and show that this underlying heuristics resembles that of human decision-making: comprising a descriptive component (reflecting statistical norm) and a prescriptive component (implicit ideal encoded in the LLM) of a concept. We show that this deviation of a sample from the statistical norm towards a prescriptive component consistently appears in concepts across diverse real-world domains like public health, and economic trends. To further illustrate the theory, we demonstrate that concept prototypes in LLMs are affected by prescriptive norms, similar to the concept of normality in humans. Through case studies and comparison with human studies, we illustrate that in real-world applications, the shift of samples toward an ideal value in LLMs' outputs can result in significantly biased decision-making, raising ethical concerns.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction</title>
<link>https://arxiv.org/abs/2402.12170</link>
<guid>https://arxiv.org/abs/2402.12170</guid>
<content:encoded><![CDATA[
arXiv:2402.12170v3 Announce Type: replace 
Abstract: Large language models require updates to remain up-to-date or adapt to new domains by fine-tuning them with new documents. One key is memorizing the latest information in a way that the memorized information is extractable with a query prompt. However, LLMs suffer from a phenomenon called perplexity curse; despite minimizing document perplexity during fine-tuning, LLMs struggle to extract information through a prompt sentence. In this new knowledge acquisition and extraction, we find a very intriguing fact that LLMs can accurately answer questions about the first sentence, but they struggle to extract information described in the middle or end of the documents used for fine-tuning. Our study suggests that the auto-regressive training causes this issue; each token is prompted by reliance on all previous tokens, which hinders the model from recalling information from training documents by question prompts. To conduct the in-depth study, we publish both synthetic and real datasets, enabling the evaluation of the QA performance w.r.t. the position of the corresponding answer in a document. Our investigation shows that even a large model suffers from the perplexity curse, but regularization such as denoising auto-regressive loss can enhance the information extraction from diverse positions. These findings will be (i) a key to improving knowledge extraction from LLMs and (ii) new elements to discuss the trade-off between RAG and fine-tuning in adapting LLMs to a new domain.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Argumentative Large Language Models for Explainable and Contestable Claim Verification</title>
<link>https://arxiv.org/abs/2405.02079</link>
<guid>https://arxiv.org/abs/2405.02079</guid>
<content:encoded><![CDATA[
arXiv:2405.02079v3 Announce Type: replace 
Abstract: The profusion of knowledge encoded in large language models (LLMs) and their ability to apply this knowledge zero-shot in a range of settings makes them promising candidates for use in decision-making. However, they are currently limited by their inability to provide outputs which can be faithfully explained and effectively contested to correct mistakes. In this paper, we attempt to reconcile these strengths and weaknesses by introducing \emph{argumentative LLMs (ArgLLMs)}, a method for augmenting LLMs with argumentative reasoning. Concretely, ArgLLMs construct argumentation frameworks, which then serve as the basis for formal reasoning in support of decision-making. The interpretable nature of these argumentation frameworks and formal reasoning means that any decision made by ArgLLMs may be explained and contested. We evaluate ArgLLMs' performance experimentally in comparison with state-of-the-art techniques, in the context of the decision-making task of claim verification. We also define novel properties to characterise contestability and assess ArgLLMs formally in terms of these properties.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations</title>
<link>https://arxiv.org/abs/2405.13828</link>
<guid>https://arxiv.org/abs/2405.13828</guid>
<content:encoded><![CDATA[
arXiv:2405.13828v2 Announce Type: replace 
Abstract: Humans are efficient language learners and inherently social creatures. Our language development is largely shaped by our social interactions, for example, the demonstration and feedback from caregivers. Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward. In this work, we explore how corrective feedback from interactions influences neural language acquisition from scratch through systematically controlled experiments, assessing whether it contributes to word learning efficiency in language models. We introduce a trial-and-demonstration (TnD) learning framework that incorporates three distinct components: student trials, teacher demonstrations, and a reward conditioned on language competence at various developmental stages. Our experiments reveal that the TnD approach accelerates word acquisition for student models of equal and smaller numbers of parameters, and we highlight the significance of both trials and demonstrations. We further show that the teacher's choices of words influence students' word-specific learning efficiency, and a practice-makes-perfect effect is evident by a strong correlation between the frequency of words in trials and their respective learning curves. Our findings suggest that interactive language learning, with teacher demonstrations and active trials, can facilitate efficient word learning in language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is In-Context Learning Sufficient for Instruction Following in LLMs?</title>
<link>https://arxiv.org/abs/2405.19874</link>
<guid>https://arxiv.org/abs/2405.19874</guid>
<content:encoded><![CDATA[
arXiv:2405.19874v3 Announce Type: replace 
Abstract: In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on the established benchmark MT-Bench, especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding high-quality, potentially carefully selected via greedy search, demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime, where ICL can be a viable alternative to IFT. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT. We provide our code at https://github.com/tml-epfl/icl-alignment.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection</title>
<link>https://arxiv.org/abs/2406.11260</link>
<guid>https://arxiv.org/abs/2406.11260</guid>
<content:encoded><![CDATA[
arXiv:2406.11260v3 Announce Type: replace 
Abstract: The spread of fake news harms individuals and presents a critical social challenge that must be addressed. Although numerous algorithmic and insightful features have been developed to detect fake news, many of these features can be manipulated with style-conversion attacks, especially with the emergence of advanced language models, making it more difficult to differentiate from genuine news. This study proposes adversarial style augmentation, AdStyle, designed to train a fake news detector that remains robust against various style-conversion attacks. The primary mechanism involves the strategic use of LLMs to automatically generate a diverse and coherent array of style-conversion attack prompts, enhancing the generation of particularly challenging prompts for the detector. Experiments indicate that our augmentation strategy significantly improves robustness and detection performance when evaluated on fake news benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?</title>
<link>https://arxiv.org/abs/2406.12307</link>
<guid>https://arxiv.org/abs/2406.12307</guid>
<content:encoded><![CDATA[
arXiv:2406.12307v4 Announce Type: replace 
Abstract: Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To this end, we address a dataset by manipulating instances from two datasets by removing necessary tools or essential information for tool invocation. Our experiments show that LLMs often struggle to identify the absence of information required to utilize specific tools and recognize the absence of appropriate tools. We further analyze model behaviors in different environments and compare their performance against humans. Our research can contribute to advancing reliable LLMs by addressing common scenarios during interactions between humans and LLMs. Our code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators</title>
<link>https://arxiv.org/abs/2406.12319</link>
<guid>https://arxiv.org/abs/2406.12319</guid>
<content:encoded><![CDATA[
arXiv:2406.12319v4 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly used as evaluators for natural language generation tasks, ensuring unbiased assessments is essential. However, LLM evaluators often display biased preferences, such as favoring verbosity and authoritative tones. Our empirical analysis reveals that these biases are exacerbated in pairwise evaluation, where LLMs directly compare two outputs and easily prioritize superficial attributes. In contrast, pointwise evaluation, which assesses outputs independently, is less susceptible to such bias because each output is judged in isolation. To address the limitations of the pairwise evaluation, we introduce a novel evaluation method, PRePair, which integrates pointwise reasoning within a pairwise framework. PRePair effectively alleviates biased preference, improving performance on the adversarial benchmark (LLMBar) while outperforming pointwise evaluation on the standard benchmark (MT-Bench).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Refusal Training in LLMs Generalize to the Past Tense?</title>
<link>https://arxiv.org/abs/2407.11969</link>
<guid>https://arxiv.org/abs/2407.11969</guid>
<content:encoded><![CDATA[
arXiv:2407.11969v4 Announce Type: replace 
Abstract: Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., "How to make a Molotov cocktail?" to "How did people make a Molotov cocktail?") is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini, o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1% using direct requests to 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques -- such as SFT, RLHF, and adversarial training -- employed to align the studied models can be brittle and do not always generalize as intended. We provide code and jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind</title>
<link>https://arxiv.org/abs/2408.12022</link>
<guid>https://arxiv.org/abs/2408.12022</guid>
<content:encoded><![CDATA[
arXiv:2408.12022v2 Announce Type: replace 
Abstract: How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts</title>
<link>https://arxiv.org/abs/2409.11056</link>
<guid>https://arxiv.org/abs/2409.11056</guid>
<content:encoded><![CDATA[
arXiv:2409.11056v2 Announce Type: replace 
Abstract: With the advent of Large Language Models (LLMs), generating rule-based data for real-world applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-to-MIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing the Scope of Language Models</title>
<link>https://arxiv.org/abs/2410.21597</link>
<guid>https://arxiv.org/abs/2410.21597</guid>
<content:encoded><![CDATA[
arXiv:2410.21597v2 Announce Type: replace 
Abstract: We now deploy language models in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions about documentation or acting as coding assistants, but they require general language understanding. Under these circumstances these models should not be able to answer irrelevant requests such as, poetry generation or questions about physics, etc. Instead we would like language models to only answer to queries corresponding to desired behavior and refuse all other requests, which we refer to as scoping. We conduct a comprehensive empirical evaluation of potential methods from prompting to fine-tuning to preference learning to a recently proposed method for general alignment called Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that, when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2411.18337</link>
<guid>https://arxiv.org/abs/2411.18337</guid>
<content:encoded><![CDATA[
arXiv:2411.18337v2 Announce Type: replace 
Abstract: Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation</title>
<link>https://arxiv.org/abs/2412.11831</link>
<guid>https://arxiv.org/abs/2412.11831</guid>
<content:encoded><![CDATA[
arXiv:2412.11831v2 Announce Type: replace 
Abstract: In an educational setting, an estimate of the difficulty of multiple-choice questions (MCQs), a commonly used strategy to assess learning progress, constitutes very useful information for both teachers and students. Since human assessment is costly from multiple points of view, automatic approaches to MCQ item difficulty estimation are investigated, yielding however mixed success until now. Our approach to this problem takes a different angle from previous work: asking various Large Language Models to tackle the questions included in three different MCQ datasets, we leverage model uncertainty to estimate item difficulty. By using both model uncertainty features as well as textual features in a Random Forest regressor, we show that uncertainty features contribute substantially to difficulty prediction, where difficulty is inversely proportional to the number of students who can correctly answer a question. In addition to showing the value of our approach, we also observe that our model achieves state-of-the-art results on the USMLE and CMCQRD publicly available datasets.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StaICC: Standardized Evaluation for Classification Task in In-context Learning</title>
<link>https://arxiv.org/abs/2501.15708</link>
<guid>https://arxiv.org/abs/2501.15708</guid>
<content:encoded><![CDATA[
arXiv:2501.15708v3 Announce Type: replace 
Abstract: Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (StaICC) for in-context classification. Including, for the normal classification task, we provide StaICC-Normal, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmark StaICC-Diag for diagnosing ICL from several aspects, aiming for a more robust inference processing.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A-MEM: Agentic Memory for LLM Agents</title>
<link>https://arxiv.org/abs/2502.12110</link>
<guid>https://arxiv.org/abs/2502.12110</guid>
<content:encoded><![CDATA[
arXiv:2502.12110v5 Announce Type: replace 
Abstract: While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code for evaluating performance is available at https://github.com/WujiangXu/AgenticMemory, while the source code of agentic memory system is available at https://github.com/agiresearch/A-mem.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models</title>
<link>https://arxiv.org/abs/2502.14427</link>
<guid>https://arxiv.org/abs/2502.14427</guid>
<content:encoded><![CDATA[
arXiv:2502.14427v2 Announce Type: replace 
Abstract: Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
arXiv:2503.21729v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Layer-skipping in Pre-trained LLMs</title>
<link>https://arxiv.org/abs/2503.23798</link>
<guid>https://arxiv.org/abs/2503.23798</guid>
<content:encoded><![CDATA[
arXiv:2503.23798v2 Announce Type: replace 
Abstract: Various layer-skipping methods have been proposed to accelerate token generation in large language models (LLMs). However, they have overlooked a fundamental question: How do computational demands vary across the generation of different tokens? In this work, we introduce FlexiDepth, a method that dynamically adjusts the number of Transformer layers used in text generation. By incorporating a plug-in router and adapter, FlexiDepth enables adaptive layer-skipping in LLMs without modifying their original parameters. Introducing FlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32, and meanwhile maintains the full 100\% benchmark performance. Experimental results with FlexiDepth demonstrate that computational demands in LLMs significantly vary based on token type. Specifically, generating repetitive tokens or fixed phrases requires fewer layers, whereas producing tokens involving computation or high uncertainty requires more layers. Interestingly, this adaptive allocation pattern aligns with human intuition. To advance research in this area, we open sourced FlexiDepth and a dataset documenting FlexiDepth's layer allocation patterns for future exploration.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models</title>
<link>https://arxiv.org/abs/2504.05050</link>
<guid>https://arxiv.org/abs/2504.05050</guid>
<content:encoded><![CDATA[
arXiv:2504.05050v2 Announce Type: replace 
Abstract: Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Token to Line: Enhancing Code Generation with a Long-Term Perspective</title>
<link>https://arxiv.org/abs/2504.07433</link>
<guid>https://arxiv.org/abs/2504.07433</guid>
<content:encoded><![CDATA[
arXiv:2504.07433v2 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can postgraduate translation students identify machine-generated text?</title>
<link>https://arxiv.org/abs/2504.09164</link>
<guid>https://arxiv.org/abs/2504.09164</guid>
<content:encoded><![CDATA[
arXiv:2504.09164v2 Announce Type: replace 
Abstract: Given the growing use of generative artificial intelligence as a tool for creating multilingual content and bypassing both machine and traditional translation methods, this study explores the ability of linguistically trained individuals to discern machine-generated output from human-written text (HT). After brief training sessions on the textual anomalies typically found in synthetic text (ST), twenty-three postgraduate translation students analysed excerpts of Italian prose and assigned likelihood scores to indicate whether they believed they were human-written or AI-generated (ChatGPT-4o). The results show that, on average, the students struggled to distinguish between HT and ST, with only two participants achieving notable accuracy. Closer analysis revealed that the students often identified the same textual anomalies in both HT and ST, although features such as low burstiness and self-contradiction were more frequently associated with ST. These findings suggest the need for improvements in the preparatory training. Moreover, the study raises questions about the necessity of editing synthetic text to make it sound more human-like and recommends further research to determine whether AI-generated text is already sufficiently natural-sounding not to require further refinement.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset</title>
<link>https://arxiv.org/abs/2504.09958</link>
<guid>https://arxiv.org/abs/2504.09958</guid>
<content:encoded><![CDATA[
arXiv:2504.09958v2 Announce Type: replace 
Abstract: Stance detection has become an essential tool for analyzing public discussions on social media. Current methods face significant challenges, particularly in Chinese language processing and multi-turn conversational analysis. To address these limitations, we introduce C-MTCSD, the largest Chinese multi-turn conversational stance detection dataset, comprising 24,264 carefully annotated instances from Sina Weibo, which is 4.2 times larger than the only prior Chinese conversational stance detection dataset. Our comprehensive evaluation using both traditional approaches and large language models reveals the complexity of C-MTCSD: even state-of-the-art models achieve only 64.07% F1 score in the challenging zero-shot setting, while performance consistently degrades with increasing conversation depth. Traditional models particularly struggle with implicit stance detection, achieving below 50% F1 score. This work establishes a challenging new benchmark for Chinese stance detection research, highlighting significant opportunities for future improvements.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination</title>
<link>https://arxiv.org/abs/2504.10020</link>
<guid>https://arxiv.org/abs/2504.10020</guid>
<content:encoded><![CDATA[
arXiv:2504.10020v2 Announce Type: replace 
Abstract: Contrastive decoding strategies are widely used to reduce hallucinations in multimodal large language models (MLLMs). These methods work by constructing contrastive samples to induce hallucinations and then suppressing them in the output distribution. However, this paper demonstrates that such approaches fail to effectively mitigate the hallucination problem. The performance improvements observed on POPE Benchmark are largely driven by two misleading factors: (1) crude, unidirectional adjustments to the model's output distribution and (2) the adaptive plausibility constraint, which reduces the sampling strategy to greedy search. To further illustrate these issues, we introduce a series of spurious improvement methods and evaluate their performance against contrastive decoding techniques. Experimental results reveal that the observed performance gains in contrastive decoding are entirely unrelated to its intended goal of mitigating hallucinations. Our findings challenge common assumptions about the effectiveness of contrastive decoding strategies and pave the way for developing genuinely effective solutions to hallucinations in MLLMs.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems</title>
<link>https://arxiv.org/abs/2405.19653</link>
<guid>https://arxiv.org/abs/2405.19653</guid>
<content:encoded><![CDATA[
arXiv:2405.19653v4 Announce Type: replace-cross 
Abstract: Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. Our work introduces the use of language descriptions, which we call ``system captions'' or SysCaps, to interface with such surrogates. We argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts. We introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. Our experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system. Additional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Select: Feature Selection with Large Language Models</title>
<link>https://arxiv.org/abs/2407.02694</link>
<guid>https://arxiv.org/abs/2407.02694</guid>
<content:encoded><![CDATA[
arXiv:2407.02694v2 Announce Type: replace-cross 
Abstract: In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., "blood pressure") in predicting an outcome of interest (e.g., "heart failure"), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2407.07880</link>
<guid>https://arxiv.org/abs/2407.07880</guid>
<content:encoded><![CDATA[
arXiv:2407.07880v2 Announce Type: replace-cross 
Abstract: This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings. The code is available at https://github.com/junkangwu/Dr_DPO.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spin glass model of in-context learning</title>
<link>https://arxiv.org/abs/2408.02288</link>
<guid>https://arxiv.org/abs/2408.02288</guid>
<content:encoded><![CDATA[
arXiv:2408.02288v3 Announce Type: replace-cross 
Abstract: Large language models show a surprising in-context learning ability -- being able to use a prompt to form a prediction for a query, yet without additional training, in stark contrast to old-fashioned supervised learning. Providing a mechanistic interpretation and linking the empirical phenomenon to physics are thus challenging and remain unsolved. We study a simple yet expressive transformer with linear attention and map this structure to a spin glass model with real-valued spins, where the couplings and fields explain the intrinsic disorder in data. The spin glass model explains how the weight parameters interact with each other during pre-training, and further clarifies why an unseen function can be predicted by providing only a prompt yet without further training. Our theory reveals that for single-instance learning, increasing the task diversity leads to the emergence of in-context learning, by allowing the Boltzmann distribution to converge to a unique correct solution of weight parameters. Therefore the pre-trained transformer displays a prediction power in a novel prompt setting. The proposed analytically tractable model thus offers a promising avenue for thinking about how to interpret many intriguing but puzzling properties of large language models.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding</title>
<link>https://arxiv.org/abs/2408.15545</link>
<guid>https://arxiv.org/abs/2408.15545</guid>
<content:encoded><![CDATA[
arXiv:2408.15545v5 Announce Type: replace-cross 
Abstract: Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery. Despite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.
  To develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.cIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. We address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation. Applying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding. These models demonstrate promising performance on scientific literature understanding benchmarks.
  Our contributions are threefold: (1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains. (2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for supervised fine-tuning in less-represented scientific domains. (3) SciLitLLM achieves promising performance improvements on scientific literature understanding benchmarks.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections</title>
<link>https://arxiv.org/abs/2409.09045</link>
<guid>https://arxiv.org/abs/2409.09045</guid>
<content:encoded><![CDATA[
arXiv:2409.09045v2 Announce Type: replace-cross 
Abstract: "Synthetic samples" based on large language models (LLMs) have been argued to serve as efficient alternatives to surveys of humans, assuming that their training data includes information on human attitudes and behavior. However, LLM-synthetic samples might exhibit bias, for example due to training data and fine-tuning processes being unrepresentative of diverse contexts. Such biases risk reinforcing existing biases in research, policymaking, and society. Therefore, researchers need to investigate if and under which conditions LLM-generated synthetic samples can be used for public opinion prediction. In this study, we examine to what extent LLM-based predictions of individual public opinion exhibit context-dependent biases by predicting the results of the 2024 European Parliament elections. Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior. By comparing them to the actual results, we show that LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt. The findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. In investigating their contextual biases, this study contributes to the understanding and mitigation of inequalities in the development of LLMs and their applications in computational social science.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</title>
<link>https://arxiv.org/abs/2410.09024</link>
<guid>https://arxiv.org/abs/2410.09024</guid>
<content:encoded><![CDATA[
arXiv:2410.09024v3 Announce Type: replace-cross 
Abstract: The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title>
<link>https://arxiv.org/abs/2410.21465</link>
<guid>https://arxiv.org/abs/2410.21465</guid>
<content:encoded><![CDATA[
arXiv:2410.21465v2 Announce Type: replace-cross 
Abstract: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation</title>
<link>https://arxiv.org/abs/2411.16750</link>
<guid>https://arxiv.org/abs/2411.16750</guid>
<content:encoded><![CDATA[
arXiv:2411.16750v2 Announce Type: replace-cross 
Abstract: Traditional monocular depth estimation suffers from inherent ambiguity and visual nuisance. We argue that language prior can enhance monocular depth estimation by leveraging the inductive bias learned during the text-to-image pre-training of diffusion models. The ability of these models to generate images that align with text indicates that they have learned the spatial relationships, size, and shape of specified objects, which can be applied to improve depth estimation. Thus, we propose PriorDiffusion, using a pre-trained text-to-image diffusion model that takes both images and corresponding text descriptions to infer affine-invariant depth through a denoising process. We also show that language prior enhances the model's perception of specific regions of images that users care about and describe. Simultaneously, language prior acts as a constraint to accelerate the convergence of both training and the inference diffusion trajectory. By training on HyperSim and Virtual KITTI, we achieve faster training convergence, fewer inference diffusion steps, and state-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant</title>
<link>https://arxiv.org/abs/2501.17176</link>
<guid>https://arxiv.org/abs/2501.17176</guid>
<content:encoded><![CDATA[
arXiv:2501.17176v2 Announce Type: replace-cross 
Abstract: The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs</title>
<link>https://arxiv.org/abs/2502.19413</link>
<guid>https://arxiv.org/abs/2502.19413</guid>
<content:encoded><![CDATA[
arXiv:2502.19413v2 Announce Type: replace-cross 
Abstract: Paywalls, licenses and copyright rules often restrict the broad dissemination and reuse of scientific knowledge. We take the position that it is both legally and technically feasible to extract the scientific knowledge in scholarly texts. Current methods, like text embeddings, fail to reliably preserve factual content, and simple paraphrasing may not be legally sound. We propose a new idea for the community to adopt: convert scholarly documents into knowledge preserving, but style agnostic representations we term Knowledge Units using LLMs. These units use structured data capturing entities, attributes and relationships without stylistic content. We provide evidence that Knowledge Units (1) form a legally defensible framework for sharing knowledge from copyrighted research texts, based on legal analyses of German copyright law and U.S. Fair Use doctrine, and (2) preserve most (~95\%) factual knowledge from original text, measured by MCQ performance on facts from the original copyrighted text across four research domains. Freeing scientific knowledge from copyright promises transformative benefits for scientific research and education by allowing language models to reuse important facts from copyrighted text. To support this, we share open-source tools for converting research documents into Knowledge Units. Overall, our work posits the feasibility of democratizing access to scientific knowledge while respecting copyright.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review</title>
<link>https://arxiv.org/abs/2503.05992</link>
<guid>https://arxiv.org/abs/2503.05992</guid>
<content:encoded><![CDATA[
arXiv:2503.05992v2 Announce Type: replace-cross 
Abstract: Context: A deeper understanding of human factors in software engineering (SE) is essential for improving team collaboration, decision-making, and productivity. Communication channels like code reviews and chats provide insights into developers' psychological and emotional states. While large language models excel at text analysis, they often lack transparency and precision. Psycholinguistic tools like Linguistic Inquiry and Word Count (LIWC) offer clearer, interpretable insights into cognitive and emotional processes exhibited in text. Despite its wide use in SE research, no comprehensive review of LIWC's use has been conducted. Objective: We examine the importance of psycholinguistic tools, particularly LIWC, and provide a thorough analysis of its current and potential future applications in SE research. Methods: We conducted a systematic review of six prominent databases, identifying 43 SE-related papers using LIWC. Our analysis focuses on five research questions. Results: Our findings reveal a wide range of applications, including analyzing team communication to detect developer emotions and personality, developing ML models to predict deleted Stack Overflow posts, and more recently comparing AI-generated and human-written text. LIWC has been primarily used with data from project management platforms (e.g., GitHub) and Q&amp;A forums (e.g., Stack Overflow). Key BSE concepts include Communication, Organizational Climate, and Positive Psychology. 26 of 43 papers did not formally evaluate LIWC. Concerns were raised about some limitations, including difficulty handling SE-specific vocabulary. Conclusion: We highlight the potential of psycholinguistic tools and their limitations, and present new use cases for advancing the research of human factors in SE (e.g., bias in human-LLM conversations).
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocAgent: A Multi-Agent System for Automated Code Documentation Generation</title>
<link>https://arxiv.org/abs/2504.08725</link>
<guid>https://arxiv.org/abs/2504.08725</guid>
<content:encoded><![CDATA[
arXiv:2504.08725v2 Announce Type: replace-cross 
Abstract: High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Judging Bias in Large Reasoning Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2504.09946</link>
<guid>https://arxiv.org/abs/2504.09946</guid>
<content:encoded><![CDATA[
arXiv:2504.09946v2 Announce Type: replace-cross 
Abstract: Large Reasoning Models (LRMs) like DeepSeek-R1 and OpenAI-o1 have demonstrated remarkable reasoning capabilities, raising important questions about their biases in LLM-as-a-judge settings. We present a comprehensive benchmark comparing judging biases between LLMs and LRMs across both subjective preference-alignment datasets and objective fact-based datasets. Through investigation of bandwagon, authority, position, and distraction biases, we uncover four key findings: (1) despite their advanced reasoning capabilities, LRMs remain susceptible to the above biases; (2) LRMs demonstrate better robustness than LLMs specifically on fact-related datasets; (3) LRMs exhibit notable position bias, preferring options in later positions; and (4) we identify a novel "superficial reflection bias" where phrases mimicking reasoning (e.g., "wait, let me think...") significantly influence model judgments. To address these biases, we design and evaluate three mitigation strategies: specialized system prompts that reduce judging biases by up to 19\% in preference alignment datasets and 14\% in fact-related datasets, in-context learning that provides up to 27\% improvement on preference tasks but shows inconsistent results on factual tasks, and a self-reflection mechanism that reduces biases by up to 10\% in preference datasets and 16\% in fact-related datasets, with self-reflection proving particularly effective for LRMs. Our work provides crucial insights for developing more reliable LLM-as-a-Judge frameworks, especially as LRMs become increasingly deployed as automated judges.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents</title>
<link>https://arxiv.org/abs/2504.10458</link>
<guid>https://arxiv.org/abs/2504.10458</guid>
<content:encoded><![CDATA[
arXiv:2504.10458v3 Announce Type: replace-cross 
Abstract: Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \name achieves superior performance using only 0.02\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.
]]></content:encoded>
<pubDate>Mon, 21 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability</title>
<link>https://arxiv.org/abs/2504.12308</link>
<guid>https://arxiv.org/abs/2504.12308</guid>
<content:encoded><![CDATA[
<div> Privacy Masking, Named Entity Recognition, PII datasets, NER approaches, model performance, contextual disclosure <br />
Summary: <br />
Privacy Masking is a crucial aspect of data privacy involving the anonymization and de-anonymization of personally identifiable information (PII). Named Entity Recognition (NER) approaches play a key role in identifying and classifying named entities in text, but they have limitations such as content sensitivity, phrasing variabilities, and format variations. Two widely used PII datasets, Piiranha and Starpii, have been analyzed for their masking quality. A dataset of 17k unique, semi-synthetic sentences with different types of PII was curated from multiple jurisdictions. The sentences were generated at five different NER detection feature dimensions and in an adversarial context. The study highlights the privacy risks associated with using these models and emphasizes the need for contextual disclosure in model cards to address performance gaps. <br /> <div>
arXiv:2504.12308v1 Announce Type: new 
Abstract: Privacy Masking is a critical concept under data privacy involving anonymization and de-anonymization of personally identifiable information (PII). Privacy masking techniques rely on Named Entity Recognition (NER) approaches under NLP support in identifying and classifying named entities in each text. NER approaches, however, have several limitations including (a) content sensitivity including ambiguous, polysemic, context dependent or domain specific content, (b) phrasing variabilities including nicknames and alias, informal expressions, alternative representations, emerging expressions, evolving naming conventions and (c) formats or syntax variations, typos, misspellings. However, there are a couple of PII datasets that have been widely used by researchers and the open-source community to train models on PII detection or masking. These datasets have been used to train models including Piiranha and Starpii, which have been downloaded over 300k and 580k times on HuggingFace. We examine the quality of the PII masking by these models given the limitations of the datasets and of the NER approaches. We curate a dataset of 17K unique, semi-synthetic sentences containing 16 types of PII by compiling information from across multiple jurisdictions including India, U.K and U.S. We generate sentences (using language models) containing these PII at five different NER detection feature dimensions - (1) Basic Entity Recognition, (2) Contextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4) Evolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER) and 1 in adversarial context. We present the results and exhibit the privacy exposure caused by such model use (considering the extent of lifetime downloads of these models). We conclude by highlighting the gaps in measuring performance of the models and the need for contextual disclosure in model cards for such models.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer</title>
<link>https://arxiv.org/abs/2504.12311</link>
<guid>https://arxiv.org/abs/2504.12311</guid>
<content:encoded><![CDATA[
<div> Transferability, Stability, Prompt tuning, Ensemble weights, Gradient Alignment Regularization

Summary:<br /><br />
HGPrompt is introduced as an adaptive framework for multi-source prompt transfer in resource-constrained systems. It aims to optimize ensemble weights by jointly optimizing transferability and stability. The framework evaluates the transferability of prompt-induced features using an information-theoretic metric and introduces Gradient Alignment Regularization to address gradient conflicts among prompts. This helps in stable and coherent knowledge transfer from multiple sources, overcoming representation collapse due to mutual interference. Extensive experiments on the VTAB benchmark validate HGPrompt's effectiveness in achieving state-of-the-art performance in multi-source prompt transfer. <div>
arXiv:2504.12311v1 Announce Type: new 
Abstract: Prompt tuning has emerged as a lightweight adaptation strategy for adapting foundation models to downstream tasks, particularly in resource-constrained systems. As pre-trained prompts have become valuable intellectual assets, combining multiple source prompts offers a promising approach to enhance generalization to new tasks by leveraging complementary knowledge from diverse sources. However, naive aggregation of these prompts often leads to representation collapse due to mutual interference, undermining their collective potential. To address these challenges, we propose HGPrompt, an adaptive framework for multi-source prompt transfer that learns optimal ensemble weights by jointly optimizing dual objectives: transferability and stability. Specifically, we first introduce an information-theoretic metric to evaluate the transferability of prompt-induced features on the target task, capturing the intrinsic alignment between the feature representations. Additionally, we propose a novel Gradient Alignment Regularization to mitigate gradient conflicts among prompts, enabling stable and coherent knowledge transfer from multiple sources while suppressing interference. Extensive experiments on the large-scale VTAB benchmark demonstrate that HGPrompt achieves state-of-the-art performance, validating its effectiveness in multi-source prompt transfer.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</title>
<link>https://arxiv.org/abs/2504.12312</link>
<guid>https://arxiv.org/abs/2504.12312</guid>
<content:encoded><![CDATA[
<div> Logic Programming, Logical Fallacies, Large Language Models, Benchmark, Reddit Posts
Summary:
The article introduces SmartyPat-Bench, a new benchmark for evaluating logical reasoning abilities of Large Language Models (LLMs). It is derived from real-world Reddit posts with subtle logical fallacies, providing detailed annotations and diverse data. To address limitations in data collection and labeling, the automated framework SmartyPat generates logically fallacious statements using Prolog rules and refines them into natural-language sentences. Evaluation shows that SmartyPat produces high-quality fallacies and outperforms baseline methods. Insights from experiments reveal that excessive reasoning hinders fallacy detection accuracy while structured reasoning improves fallacy categorization performance. <div>
arXiv:2504.12312v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12313</link>
<guid>https://arxiv.org/abs/2504.12313</guid>
<content:encoded><![CDATA[
<div> personality traits, conversational recommender systems, large language models, user simulation, evaluation

Summary:
The paper introduces PerCRS, a personality-aware user simulation for Conversational Recommender Systems (CRSs) that leverages large language models (LLMs) to provide personalized recommendations. By incorporating customizable personality traits and preferences in the user agent, and persuasion capabilities in the system agent, PerCRS can simulate realistic interactions in CRSs. Through multi-aspect evaluation, the system is tested for robustness and effectiveness. Experimental results show that LLMs can generate diverse user responses based on specified personality traits, enabling CRSs to adjust recommendation strategies dynamically. The study provides valuable empirical insights into how personality traits influence the outcomes of conversational recommender systems. <div>
arXiv:2504.12313v1 Announce Type: new 
Abstract: Conversational Recommender Systems (CRSs) engage users in multi-turn interactions to deliver personalized recommendations. The emergence of large language models (LLMs) further enhances these systems by enabling more natural and dynamic user interactions. However, a key challenge remains in understanding how personality traits shape conversational recommendation outcomes. Psychological evidence highlights the influence of personality traits on user interaction behaviors. To address this, we introduce an LLM-based personality-aware user simulation for CRSs (PerCRS). The user agent induces customizable personality traits and preferences, while the system agent possesses the persuasion capability to simulate realistic interaction in CRSs. We incorporate multi-aspect evaluation to ensure robustness and conduct extensive analysis from both user and system perspectives. Experimental results demonstrate that state-of-the-art LLMs can effectively generate diverse user responses aligned with specified personality traits, thereby prompting CRSs to dynamically adjust their recommendation strategies. Our experimental analysis offers empirical insights into the impact of personality traits on the outcomes of conversational recommender systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension</title>
<link>https://arxiv.org/abs/2504.12314</link>
<guid>https://arxiv.org/abs/2504.12314</guid>
<content:encoded><![CDATA[
<div> Hallucination, Molecular Comprehension, LLMs, PubChem dataset, Mol-Hallu metric <br />
Summary: 
Large language models (LLMs) are widely used in scientific applications but are prone to hallucination issues. This study investigates the sources of hallucination in LLMs for molecular comprehension tasks, focusing on the knowledge shortcut phenomenon in the PubChem dataset. A new evaluation metric, Mol-Hallu, is introduced to quantify the degree of hallucination in generated text based on scientific entailment relationships. The Hallucination Reduction Post-processing stage (HRPP) is proposed to reduce molecular hallucinations in LLMs. Experiments demonstrate the efficacy of HRPP in addressing hallucination in both decoder-only and encoder-decoder molecular LLMs. These findings offer valuable insights into mitigating hallucination and enhancing the reliability of LLMs in scientific domains. <br /><br />Summary: <div>
arXiv:2504.12314v1 Announce Type: new 
Abstract: Large language models are increasingly used in scientific domains, especially for molecular understanding and analysis. However, existing models are affected by hallucination issues, resulting in errors in drug design and utilization. In this paper, we first analyze the sources of hallucination in LLMs for molecular comprehension tasks, specifically the knowledge shortcut phenomenon observed in the PubChem dataset. To evaluate hallucination in molecular comprehension tasks with computational efficiency, we introduce \textbf{Mol-Hallu}, a novel free-form evaluation metric that quantifies the degree of hallucination based on the scientific entailment relationship between generated text and actual molecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze the extent of hallucination in various LLMs performing molecular comprehension tasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is proposed to alleviate molecular hallucinations, Experiments show the effectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our findings provide critical insights into mitigating hallucination and improving the reliability of LLMs in scientific applications.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models</title>
<link>https://arxiv.org/abs/2504.12315</link>
<guid>https://arxiv.org/abs/2504.12315</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Capybara-OMNI, training, benchmarks, chat-based model

Summary:<br /><br />
The article introduces Capybara-OMNI, a Multimodal Large Language Model (MLLM) that efficiently trains on text, image, video, and audio modalities. The framework design, data construction, and training recipe are detailed to guide the development of competitive MLLMs. Benchmarks are provided to evaluate understanding capabilities across different modalities. Results demonstrate the model's competitive performance on various benchmarks. Additionally, a chat-based version is discussed to enhance instruction following and conversational abilities for real-time interaction. The Capybara-OMNI model, along with its chat version, is publicly disclosed on GitHub, including model weights, training data, and inference codes. The article aims to empower the open-source community in creating powerful MLLMs in a lightweight and efficient manner. <div>
arXiv:2504.12315v1 Announce Type: new 
Abstract: With the development of Multimodal Large Language Models (MLLMs), numerous outstanding accomplishments have emerged within the open-source community. Due to the complexity of creating and training multimodal data pairs, it is still a computational and time-consuming process to build powerful MLLMs. In this work, we introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient manner and supports understanding text, image, video, and audio modalities. We present in detail the framework design, the data construction, and the training recipe, to develop an MLLM step-by-step to obtain competitive performance. We also provide exclusive benchmarks utilized in our experiments to show how to properly verify understanding capabilities across different modalities. Results show that by following our guidance, we can efficiently build an MLLM that achieves competitive performance among models of the same scale on various multimodal benchmarks. Additionally, to enhance the multimodal instruction following and conversational capabilities of the model, we further discuss how to train the chat version upon an MLLM understanding model, which is more in line with user habits for tasks like real-time interaction with humans. We publicly disclose the Capybara-OMNI model, along with its chat-based version. The disclosure includes both the model weights, a portion of the training data, and the inference codes, which are made available on GitHub.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Metabolism: An Efficient Data Design Schema For Vision Language Model</title>
<link>https://arxiv.org/abs/2504.12316</link>
<guid>https://arxiv.org/abs/2504.12316</guid>
<content:encoded><![CDATA[
<div> Keywords: Data curation, Visual Language Models, Data Metabolism, Model performance, Multimodal tasks

Summary:
Data curation is essential for training Visual Language Models (VLMs), with the concept of Data Metabolism guiding the development framework. The framework emphasizes continuous improvement through data curation and iteration, leading to enhanced model performance. A detailed codebook is provided for processing large datasets and creating user-specific data strategies. The Capybara-VL VLM, despite its compact size, outperforms larger open-source models and competes with proprietary models in tasks like visual question answering and scientific reasoning. The results showcase the efficacy of the data-centric approach in training smaller yet efficient VLMs capable of achieving competitive performance levels.<br /><br />Summary: Data curation is crucial in training Visual Language Models, with the Data Metabolism concept guiding a framework for continuous model improvement. The development process focuses on data curation and iteration, leading to the creation of the Capybara-VL VLM that excels in various multimodal tasks. Despite its smaller size, Capybara-VL outperforms larger open-source models and competes with proprietary ones, demonstrating the power of the data-centric approach in training efficient VLMs. <div>
arXiv:2504.12316v1 Announce Type: new 
Abstract: Data curation plays a crucial role in training powerful Visual Language Models (VLMs). In this work, we introduce the concept of Data Metabolism and present our data-centric framework to build VLMs throughout the development lifecycle. Starting from a standard model architecture, we discuss and provide insights into two crucial development steps: data curation and iteration, forming a closed-loop system that continuously improves model performance. We show a detailed codebook on how to process existing massive datasets and build user-specific data flywheel. As a demonstration, we release a VLM, named Capybara-VL, which excels in typical multimodal tasks (e.g. , visual question answering, scientific reasoning, and text-rich tasks). Despite its relatively compact size, Capybara-VL surpasses several open-source models that are up to 10 times larger in size. Moreover, it achieves results that are on par with those of several leading proprietary models, demonstrating its remarkable competitiveness. These results highlight the power of our data-centric framework and the potential of training smaller and more efficient VLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing</title>
<link>https://arxiv.org/abs/2504.12317</link>
<guid>https://arxiv.org/abs/2504.12317</guid>
<content:encoded><![CDATA[
<div> Keywords: ChatGPT, academic writing, lexical complexity, non-native English-speakers, equity <br />
Summary: 
- The study examines the impact of ChatGPT on lexical complexity in academic writing for non-native English speakers.
- Analysis of 2.8 million articles from OpenAlex (2020-2024) shows that ChatGPT enhances vocabulary sophistication in NNES-authored abstracts.
- The Measure of Textual Lexical Diversity (MTLD) was used to quantify the effect, revealing significant improvements.
- The study employed a difference-in-differences (DID) design to establish causal effects.
- Results indicate that ChatGPT has a pronounced effect on abstracts in preprints, technology, biology fields, and lower-tier journals.<br />Summary: <div>
arXiv:2504.12317v1 Announce Type: new 
Abstract: The advent of ChatGPT has profoundly reshaped scientific research practices, particularly in academic writing, where non-native English-speakers (NNES) historically face linguistic barriers. This study investigates whether ChatGPT mitigates these barriers and fosters equity by analyzing lexical complexity shifts across 2.8 million articles from OpenAlex (2020-2024). Using the Measure of Textual Lexical Diversity (MTLD) to quantify vocabulary sophistication and a difference-in-differences (DID) design to identify causal effects, we demonstrate that ChatGPT significantly enhances lexical complexity in NNES-authored abstracts, even after controlling for article-level controls, authorship patterns, and venue norms. Notably, the impact is most pronounced in preprint papers, technology- and biology-related fields and lower-tier journals. These findings provide causal evidence that ChatGPT reduces linguistic disparities and promotes equity in global academia.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability</title>
<link>https://arxiv.org/abs/2504.12320</link>
<guid>https://arxiv.org/abs/2504.12320</guid>
<content:encoded><![CDATA[
<div> performance, creativity, language models, variability, assessment

Summary: 
The study evaluated 14 large language models (LLMs) on creativity tasks, finding no evidence of improved performance over the past 18-24 months. GPT-4 performed worse than expected, while GPT-4o and o3-mini performed best on the Alternative Uses Task (AUT). LLM-generated responses rarely reached the top 10% of human creativity benchmarks. The study revealed significant intra-model variability, with the same LLM producing outputs of varying creativity levels. The choice of prompts had differing impacts on LLM performance. The findings emphasize the need for nuanced evaluation frameworks and highlight the importance of model selection, prompt design, and repeated assessment when using Generative AI tools in creative applications.<br /><br /> <div>
arXiv:2504.12320v1 Announce Type: new 
Abstract: Following the widespread adoption of ChatGPT in early 2023, numerous studies reported that large language models (LLMs) can match or even surpass human performance in creative tasks. However, it remains unclear whether LLMs have become more creative over time, and how consistent their creative output is. In this study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama, Grok, Mistral, and DeepSeek -- across two validated creativity assessments: the Divergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary to expectations, we found no evidence of increased creative performance over the past 18-24 months, with GPT-4 performing worse than in previous studies. For the more widely used AUT, all models performed on average better than the average human, with GPT-4o and o3-mini performing best. However, only 0.28% of LLM-generated responses reached the top 10% of human creativity benchmarks. Beyond inter-model differences, we document substantial intra-model variability: the same LLM, given the same prompt, can produce outputs ranging from below-average to original. This variability has important implications for both creativity research and practical applications. Ignoring such variability risks misjudging the creative potential of LLMs, either inflating or underestimating their capabilities. The choice of prompts affected LLMs differently. Our findings underscore the need for more nuanced evaluation frameworks and highlight the importance of model selection, prompt design, and repeated assessment when using Generative AI (GenAI) tools in creative contexts.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks</title>
<link>https://arxiv.org/abs/2504.12321</link>
<guid>https://arxiv.org/abs/2504.12321</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Attention Mechanism, Jailbreak Detection, Adversarial Prompts, Explainable Defense

Summary: 
In this research, the focus is on defending Language Models (LMs) against malicious input that can cause deviations from intended behavior, known as jailbreaks. The proposed defense approach, called AttentionDefense, utilizes the system-prompt attention mechanism from Small Language Models (SLMs) to characterize adversarial prompts in a novel and explainable manner. The research demonstrates that the attention mechanism plays a crucial role in understanding and explaining how LMs respond to malicious input, which is not captured solely in the semantic meaning of text embeddings. Evaluation against existing jailbreak benchmark datasets shows that SLM-based AttentionDefense performs equivalently or better than text embedding-based classifiers and GPT-4 zero-shot detectors. A novel jailbreak dataset generated using a closed-loop LM-based multi-agent system further validates the efficacy of AttentionDefense, as it outperforms existing approaches in handling these new jailbreak variants. AttentionDefense is highlighted as a practical solution with the computational requirements of a small LM but the performance of a larger LLM detector.

Summary: <br /><br /> <div>
arXiv:2504.12321v1 Announce Type: new 
Abstract: In the past few years, Language Models (LMs) have shown par-human capabilities in several domains. Despite their practical applications and exceeding user consumption, they are susceptible to jailbreaks when malicious input exploits the LM's weaknesses, causing it to deviate from its intended behavior. Current defensive strategies either classify the input prompt as adversarial or prevent LMs from generating harmful outputs. However, it is challenging to explain the reason behind the malicious nature of the jailbreak, which results in a wide variety of closed-box approaches. In this research, we propose and demonstrate that system-prompt attention from Small Language Models (SLMs) can be used to characterize adversarial prompts, providing a novel, explainable, and cheaper defense approach called AttentionDefense. Our research suggests that the attention mechanism is an integral component in understanding and explaining how LMs respond to malicious input that is not captured in the semantic meaning of text embeddings. The proposed AttentionDefense is evaluated against existing jailbreak benchmark datasets. Ablation studies show that SLM-based AttentionDefense has equivalent or better jailbreak detection performance compared to text embedding-based classifiers and GPT-4 zero-shot detectors.To further validate the efficacy of the proposed approach, we generate a dataset of novel jailbreak variants of the existing benchmark dataset using a closed-loop LLM-based multi-agent system. We demonstrate that the proposed AttentionDefense approach performs robustly on this novel jailbreak dataset while existing approaches suffer in performance. Additionally, for practical purposes AttentionDefense is an ideal solution as it has the computation requirements of a small LM but the performance of a LLM detector.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis</title>
<link>https://arxiv.org/abs/2504.12322</link>
<guid>https://arxiv.org/abs/2504.12322</guid>
<content:encoded><![CDATA[
<div> Keywords: data synthesis, small language models, multiple small LLMs, collaborative framework, peer review

Summary: 
The article introduces a new framework called GRA that utilizes multiple small Language Models (LLMs) working collaboratively to enhance data synthesis. In this framework, each small LLM takes on a specific role - Generator, Reviewer, or Adjudicator - to simulate a peer review-inspired process for data synthesis. By decomposing the synthesis process into specialized tasks, the collaborative small LLMs can achieve high-quality data synthesis comparable to that of large LLMs. The experiments conducted across various benchmarks show that the data produced by GRA matches or exceeds the quality of outputs from single large LLMs. This challenges the notion that large monolithic models are necessary for high-quality data synthesis and instead advocates for strategic coordination among smaller agents. The datasets, models, and code related to GRA are publicly available on GitHub at https://github.com/GX-XinGao/GRA.

<br /><br />Summary: <div>
arXiv:2504.12322v1 Announce Type: new 
Abstract: While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.12323</link>
<guid>https://arxiv.org/abs/2504.12323</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, fairness, experimentation, mitigating<br />
Summary:<br />
The study explores the impact of Retrieval-Augmented Generation (RAG) on the fairness of Large Language Models (LLMs). Conducting experiments with various LLMs, retrievers, and retrieval sources, the research finds that the scale of the LLM significantly affects fairness outcomes within the RAG framework. For small-scale LLMs, integration of retrieval mechanisms can worsen unfairness. To address this issue, two approaches are proposed: FairFT aligns the retriever with the LLM for fairer outputs, and FairFilter introduces a fairness filtering mechanism post-retrieval. Real-world dataset validations show the effectiveness of these approaches in improving fairness without compromising performance.<br /> <div>
arXiv:2504.12323v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant document from external knowledge sources. By referencing this external knowledge, RAG effectively reduces the generation of factually incorrect content and addresses hallucination issues within LLMs. Recently, there has been growing attention to improving the performance and efficiency of RAG systems from various perspectives. While these advancements have yielded significant results, the application of RAG in domains with considerable societal implications raises a critical question about fairness: What impact does the introduction of the RAG paradigm have on the fairness of LLMs? To address this question, we conduct extensive experiments by varying the LLMs, retrievers, and retrieval sources. Our experimental analysis reveals that the scale of the LLMs plays a significant role in influencing fairness outcomes within the RAG framework. When the model scale is smaller than 8B, the integration of retrieval mechanisms often exacerbates unfairness in small-scale LLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness issues introduced by RAG for small-scale LLMs, we propose two approaches, FairFT and FairFilter. Specifically, in FairFT, we align the retriever with the LLM in terms of fairness, enabling it to retrieve documents that facilitate fairer model outputs. In FairFilter, we propose a fairness filtering mechanism to filter out biased content after retrieval. Finally, we validate our proposed approaches on real-world datasets, demonstrating their effectiveness in improving fairness while maintaining performance.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction</title>
<link>https://arxiv.org/abs/2504.12324</link>
<guid>https://arxiv.org/abs/2504.12324</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Inference, Cross-Document, Cross-Lingual, RST, Graph Fusion

Summary: 
Cross-Document Cross-Lingual Natural Language Inference (CDCL-NLI) is a relatively unexplored area within the field. This paper introduces a novel paradigm for CDCL-NLI, expanding traditional NLI capabilities to multi-document, multilingual scenarios. The authors construct a high-quality dataset for CDCL-NLI, encompassing 1,110 instances across 26 languages. Their proposed method integrates Rhetorical Structure Theory (RST) and Relation-aware Graph Attention Network (RGAT) for cross-document context modeling, along with a structure-aware semantic alignment mechanism based on lexical chains for cross-lingual understanding. Additionally, they develop an Enhanced Distinctiveness Unit (EDU)-level attribution framework for generating extractive explanations and improving NLI interpretability. Experimental results demonstrate the superior performance of the proposed approach compared to traditional NLI models and Large Language Models (LLMs) such as DocNLI, R2F, Llama3, and GPT-4o. This work contributes to the study of NLI and encourages further exploration in cross-document cross-lingual context understanding, semantic retrieval, and interpretability inference. <div>
arXiv:2504.12324v1 Announce Type: new 
Abstract: Natural Language Inference (NLI) is a fundamental task in both natural language processing and information retrieval. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm for CDCL-NLI that extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 1,110 instances and spanning 26 languages. To build a baseline for this task, we also propose an innovative method that integrates RST-enhanced graph fusion and interpretability prediction. Our method employs RST (Rhetorical Structure Theory) on RGAT (Relation-aware Graph Attention Network) for cross-document context modeling, coupled with a structure-aware semantic alignment mechanism based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU-level attribution framework that generates extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both traditional NLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, semantic retrieval and interpretability inference. Our dataset and code are available at \href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer review}.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media</title>
<link>https://arxiv.org/abs/2504.12325</link>
<guid>https://arxiv.org/abs/2504.12325</guid>
<content:encoded><![CDATA[
<div> language models, social media, taxonomy, factual claims, automated construction
Summary: 
- The paper introduces LLMTaxo, a framework that uses large language models to create a taxonomy of factual claims from social media by generating topics at multiple levels of granularity.
- This framework helps stakeholders better understand and navigate the vast amount of content on social media platforms.
- LLMTaxo is implemented with different models on three unique datasets and evaluated using specially designed taxonomy evaluation metrics.
- Results from evaluations by human evaluators and GPT-4 show that LLMTaxo effectively categorizes factual claims from social media.
- Certain models perform better on specific datasets, highlighting the importance of selecting the right model for the task.<br /><br />Summary: <div>
arXiv:2504.12325v1 Announce Type: new 
Abstract: With the vast expansion of content on social media platforms, analyzing and comprehending online discourse has become increasingly complex. This paper introduces LLMTaxo, a novel framework leveraging large language models for the automated construction of taxonomy of factual claims from social media by generating topics from multi-level granularities. This approach aids stakeholders in more effectively navigating the social media landscapes. We implement this framework with different models across three distinct datasets and introduce specially designed taxonomy evaluation metrics for a comprehensive assessment. With the evaluations from both human evaluators and GPT-4, the results indicate that LLMTaxo effectively categorizes factual claims from social media, and reveals that certain models perform better on specific datasets.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis</title>
<link>https://arxiv.org/abs/2504.12326</link>
<guid>https://arxiv.org/abs/2504.12326</guid>
<content:encoded><![CDATA[
<div> pipeline, phenotype, time-localized, Sepsis-3, language models<br />
<br />
Summary: 
The study focuses on utilizing large language models (LLMs) to extract and annotate time-localized clinical findings from case reports for Sepsis-3. They generated a text corpus from 2,139 case reports and compared their results to physician-expert annotations. The results showed high recovery rates of clinical findings and strong temporal ordering, demonstrating the effectiveness of LLMs in extracting information from text. However, the study also highlighted limitations in using LLMs for temporal reconstruction and suggested potential improvements through multimodal integration. The pipeline developed in this study has the potential to enhance the accuracy and completeness of patient encounter summarization, providing valuable insights for healthcare professionals. <div>
arXiv:2504.12326v1 Announce Type: new 
Abstract: Clinical case reports and discharge summaries may be the most complete and accurate summarization of patient encounters, yet they are finalized, i.e., timestamped after the encounter. Complementary data structured streams become available sooner but suffer from incompleteness. To train models and algorithms on more complete and temporally fine-grained data, we construct a pipeline to phenotype, extract, and annotate time-localized findings within case reports using large language models. We apply our pipeline to generate an open-access textual time series corpus for Sepsis-3 comprising 2,139 case reports from the Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA and timeline annotations from I2B2/MIMIC-IV and compare the results to physician-expert annotations. We show high recovery rates of clinical findings (event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B Instruct--0.932). Our work characterizes the ability of LLMs to time-localize clinical findings in text, illustrating the limitations of LLM use for temporal reconstruction and providing several potential avenues of improvement via multimodal integration.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Embeddings Track Social Group Changes Across 70 Years in China</title>
<link>https://arxiv.org/abs/2504.12327</link>
<guid>https://arxiv.org/abs/2504.12327</guid>
<content:encoded><![CDATA[
<div> Keywords: Chinese state-controlled media, word embeddings, social groups, linguistic representations, computational analysis <br />
Summary: This study conducted a large-scale computational analysis of Chinese state-controlled media spanning from 1950 to 2019 to investigate how societal beliefs about social groups are encoded in language. The analysis focused on economic status, ethnicity, gender, age, and body type. The findings revealed significant differences in Chinese representations compared to Western contexts, with distinct evolutionary dynamics observed. Ethnicity, age, and body type stereotypes showed remarkable stability across political upheavals, while representations of gender and economic classes underwent dramatic shifts reflecting historical transformations. The research underscores the role of language in encoding social structure and emphasizes the importance of considering non-Western perspectives in computational social science. <br /><br />Summary: This study utilized diachronic word embeddings to analyze the language patterns in Chinese state-controlled media, shedding light on how revolutionary social transformations are reflected in linguistic representations of social groups. The research highlighted significant differences between Chinese and Western representations, particularly in terms of economic status, ethnicity, and gender. Ethnicity, age, and body type stereotypes were found to be stable over time, while gender and economic class representations exhibited dramatic shifts corresponding to historical changes. The study contributes to understanding how language encodes societal beliefs and emphasizes the need to incorporate non-Western perspectives in computational social science. <div>
arXiv:2504.12327v1 Announce Type: new 
Abstract: Language encodes societal beliefs about social groups through word patterns. While computational methods like word embeddings enable quantitative analysis of these patterns, studies have primarily examined gradual shifts in Western contexts. We present the first large-scale computational analysis of Chinese state-controlled media (1950-2019) to examine how revolutionary social transformations are reflected in official linguistic representations of social groups. Using diachronic word embeddings at multiple temporal resolutions, we find that Chinese representations differ significantly from Western counterparts, particularly regarding economic status, ethnicity, and gender. These representations show distinct evolutionary dynamics: while stereotypes of ethnicity, age, and body type remain remarkably stable across political upheavals, representations of gender and economic classes undergo dramatic shifts tracking historical transformations. This work advances our understanding of how officially sanctioned discourse encodes social structure through language while highlighting the importance of non-Western perspectives in computational social science.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future</title>
<link>https://arxiv.org/abs/2504.12328</link>
<guid>https://arxiv.org/abs/2504.12328</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward Model, Large Language Model, Preference Collection, Evaluation Benchmarks, Research Directions <br />
Summary: 
This paper provides a comprehensive overview of Reward Models (RMs) and their potential for enhancing Large Language Models (LLMs). The discussion covers the collection of preferences, modeling rewards, and the practical usage of RMs. The paper also delves into various applications of RMs and suggests benchmarks for their evaluation. It then analyzes the current challenges in the field and proposes potential research directions for further exploration. The resources for beginners interested in RMs are made available on GitHub for reference and future studies. Overall, this paper serves as a valuable introduction to RMs, offering insights into their significance in guiding LLMs and paving the way for future advancements in the field. <br /><br />Summary: <div>
arXiv:2504.12328v1 Announce Type: new 
Abstract: Reward Model (RM) has demonstrated impressive potential for enhancing Large Language Models (LLM), as RM can serve as a proxy for human preferences, providing signals to guide LLMs' behavior in various tasks. In this paper, we provide a comprehensive overview of relevant research, exploring RMs from the perspectives of preference collection, reward modeling, and usage. Next, we introduce the applications of RMs and discuss the benchmarks for evaluation. Furthermore, we conduct an in-depth analysis of the challenges existing in the field and dive into the potential research directions. This paper is dedicated to providing beginners with a comprehensive introduction to RMs and facilitating future studies. The resources are publicly available at github\footnote{https://github.com/JLZhong23/awesome-reward-models}.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time</title>
<link>https://arxiv.org/abs/2504.12329</link>
<guid>https://arxiv.org/abs/2504.12329</guid>
<content:encoded><![CDATA[
<div> framework, reasoning models, speculative thinking, inference, training-free  
Summary:  
Speculative Thinking is a novel framework that enhances reasoning model performance without requiring additional training. By leveraging larger models to guide smaller ones during inference based on specific tokens signals, such as "wait" following structural delimiters, the method improves reasoning accuracy and reduces output length. The framework significantly boosts the accuracy of reasoning models on MATH500, with a 6.2% improvement and 15.7% decrease in output length. When applied to a non-reasoning model, it also achieves a significant 7.8% accuracy improvement. This approach demonstrates the potential of leveraging larger models for improved performance without the need for costly training pipelines.  
<br /><br />Summary: <div>
arXiv:2504.12329v1 Announce Type: new 
Abstract: Recent advances leverage post-training to enhance model reasoning performance, which typically requires costly training pipelines and still suffers from inefficient, overly lengthy outputs. We introduce Speculative Thinking, a training-free framework that enables large reasoning models to guide smaller ones during inference at the reasoning level, distinct from speculative decoding, which operates at the token level. Our approach is based on two observations: (1) reasoning-supportive tokens such as "wait" frequently appear after structural delimiters like "\n\n", serving as signals for reflection or continuation; and (2) larger models exhibit stronger control over reflective behavior, reducing unnecessary backtracking while improving reasoning quality. By strategically delegating reflective steps to a more capable model, our method significantly boosts the reasoning accuracy of reasoning models while shortening their output. With the assistance of the 32B reasoning model, the 1.5B model's accuracy on MATH500 increases from 83.2% to 89.4%, marking a substantial improvement of 6.2%. Simultaneously, the average output length is reduced from 5439 tokens to 4583 tokens, representing a 15.7% decrease. Moreover, when applied to a non-reasoning model (Qwen-2.5-7B-Instruct), our framework boosts its accuracy from 74.0% to 81.8% on the same benchmark, achieving a relative improvement of 7.8%.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2504.12330</link>
<guid>https://arxiv.org/abs/2504.12330</guid>
<content:encoded><![CDATA[
<div> Hierarchical Multi-agent Multimodal RAG, collaborative intelligence, dynamic knowledge synthesis, structured data, unstructured data<br />
<br />
Summary: <br />
The article introduces a novel Hierarchical Multi-agent Multimodal RAG framework, HM-RAG, which enhances traditional Retrieval-Augmented Generation (RAG) systems by enabling collaborative intelligence for dynamic knowledge synthesis across diverse data types. The framework incorporates a three-tiered architecture with specialized agents: a Decomposition Agent for dissecting complex queries, Multi-source Retrieval Agents for modality-specific retrieval, and a Decision Agent for integrating answers and resolving discrepancies. HM-RAG achieves a significant improvement in answer accuracy and question classification accuracy over baseline RAG systems on specific benchmarks. It also excels in zero-shot settings on these datasets. The modular architecture of HM-RAG allows for the seamless integration of new data modalities while ensuring data governance, making it a significant advancement in addressing multimodal reasoning and knowledge synthesis challenges in RAG systems. <div>
arXiv:2504.12330v1 Announce Type: new 
Abstract: While Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge, conventional single-agent RAG remains fundamentally limited in resolving complex queries demanding coordinated reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative intelligence for dynamic knowledge synthesis across structured, unstructured, and graph-based data. The framework is composed of three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks via semantic-aware query rewriting and schema-guided context augmentation; Multi-source Retrieval Agents that carry out parallel, modality-specific retrieval using plug-and-play modules designed for vector, graph, and web-based databases; and a Decision Agent that uses consistency voting to integrate multi-source answers and resolve discrepancies in retrieval results through Expert Model Refinement. This architecture attains comprehensive query understanding by combining textual, graph-relational, and web-derived evidence, resulting in a remarkable 12.95% improvement in answer accuracy and a 3.56% boost in question classification accuracy over baseline RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG establishes state-of-the-art results in zero-shot settings on both datasets. Its modular architecture ensures seamless integration of new data modalities while maintaining strict data governance, marking a significant advancement in addressing the critical challenges of multimodal reasoning and knowledge synthesis in RAG systems. Code is available at https://github.com/ocean-luna/HMRAG.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation</title>
<link>https://arxiv.org/abs/2504.12331</link>
<guid>https://arxiv.org/abs/2504.12331</guid>
<content:encoded><![CDATA[
<div> Keywords: Span-level emotion-cause-category triplet extraction, instruction tuning, data augmentation, large language models, emotion cause analysis

Summary: 
This study introduces a novel approach to span-level emotion-cause-category triplet extraction, a challenging task in emotion cause analysis. By leveraging instruction tuning and data augmentation techniques based on large language models, the proposed method achieves significant improvements in triplet extraction metrics. Specifically, the method utilizes task-specific triplet extraction instructions and low-rank adaptation to fine-tune large language models without the need for complex architectures. A prompt-based data augmentation strategy is also implemented to generate synthetic training data and address data scarcity. Experimental results demonstrate a 12.8% improvement over existing baseline methods, showcasing the method's effectiveness and robustness. The developed framework offers a promising direction for further research in emotion cause analysis.<br /><br />Summary: <div>
arXiv:2504.12331v1 Announce Type: new 
Abstract: Span-level emotion-cause-category triplet extraction represents a novel and complex challenge within emotion cause analysis. This task involves identifying emotion spans, cause spans, and their associated emotion categories within the text to form structured triplets. While prior research has predominantly concentrated on clause-level emotion-cause pair extraction and span-level emotion-cause detection, these methods often confront challenges originating from redundant information retrieval and difficulty in accurately determining emotion categories, particularly when emotions are expressed implicitly or ambiguously. To overcome these challenges, this study explores a fine-grained approach to span-level emotion-cause-category triplet extraction and introduces an innovative framework that leverages instruction tuning and data augmentation techniques based on large language models. The proposed method employs task-specific triplet extraction instructions and utilizes low-rank adaptation to fine-tune large language models, eliminating the necessity for intricate task-specific architectures. Furthermore, a prompt-based data augmentation strategy is developed to address data scarcity by guiding large language models in generating high-quality synthetic training data. Extensive experimental evaluations demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving at least a 12.8% improvement in span-level emotion-cause-category triplet extraction metrics. The results demonstrate the method's effectiveness and robustness, offering a promising avenue for advancing research in emotion cause analysis. The source code is available at https://github.com/zxgnlp/InstruDa-LLM.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can the capability of Large Language Models be described by human ability? A Meta Study</title>
<link>https://arxiv.org/abs/2504.12332</link>
<guid>https://arxiv.org/abs/2504.12332</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, capabilities, performance data, human abilities, parameter scale<br />
Summary:<br />
1. LLMs, even with fewer than 10 billion parameters, exhibit capabilities that can be likened to human abilities.<br />
2. Some abilities that are interconnected in humans show little correlation in LLMs.<br />
3. The capabilities of LLMs vary significantly based on the model's parameter scale.<br /> <div>
arXiv:2504.12332v1 Announce Type: new 
Abstract: Users of Large Language Models (LLMs) often perceive these models as intelligent entities with human-like capabilities. However, the extent to which LLMs' capabilities truly approximate human abilities remains a topic of debate. In this paper, to characterize the capabilities of LLMs in relation to human capabilities, we collected performance data from over 80 models across 37 evaluation benchmarks. The evaluation benchmarks are categorized into 6 primary abilities and 11 sub-abilities in human aspect. Then, we then clustered the performance rankings into several categories and compared these clustering results with classifications based on human ability aspects. Our findings lead to the following conclusions: 1. We have confirmed that certain capabilities of LLMs with fewer than 10 billion parameters can indeed be described using human ability metrics; 2. While some abilities are considered interrelated in humans, they appear nearly uncorrelated in LLMs; 3. The capabilities possessed by LLMs vary significantly with the parameter scale of the model.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games</title>
<link>https://arxiv.org/abs/2504.12333</link>
<guid>https://arxiv.org/abs/2504.12333</guid>
<content:encoded><![CDATA[
<div> Keywords: open-ended responses, serious games, Large Language Models, evaluation, decision-making<br />
Summary:<br />
The study evaluates the reliability of five small-scale Large Language Models (LLMs) in assessing player responses in the serious game "En-join." The evaluation of open-ended responses in serious games can be subjective, making it a unique challenge. By using traditional binary classification metrics, the study compares the performance of the LLMs in various evaluation scenarios. The results showcase the strengths and limitations of each model, emphasizing the trade-offs between sensitivity, specificity, and overall performance. While some models excel at identifying correct responses, others struggle with false positives and inconsistent evaluations. The findings stress the importance of context-aware evaluation frameworks and careful model selection when deploying LLMs for evaluation tasks. This study contributes valuable insights into the trustworthiness of AI-driven assessment tools and sheds light on how different LLM architectures handle subjective evaluation tasks.<br /> 
Summary: <div>
arXiv:2504.12333v1 Announce Type: new 
Abstract: The evaluation of open-ended responses in serious games presents a unique challenge, as correctness is often subjective. Large Language Models (LLMs) are increasingly being explored as evaluators in such contexts, yet their accuracy and consistency remain uncertain, particularly for smaller models intended for local execution. This study investigates the reliability of five small-scale LLMs when assessing player responses in \textit{En-join}, a game that simulates decision-making within energy communities. By leveraging traditional binary classification metrics (including accuracy, true positive rate, and true negative rate), we systematically compare these models across different evaluation scenarios. Our results highlight the strengths and limitations of each model, revealing trade-offs between sensitivity, specificity, and overall performance. We demonstrate that while some models excel at identifying correct responses, others struggle with false positives or inconsistent evaluations. The findings highlight the need for context-aware evaluation frameworks and careful model selection when deploying LLMs as evaluators. This work contributes to the broader discourse on the trustworthiness of AI-driven assessment tools, offering insights into how different LLM architectures handle subjective evaluation tasks.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model</title>
<link>https://arxiv.org/abs/2504.12334</link>
<guid>https://arxiv.org/abs/2504.12334</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Biomedical Tasks, Quantized Models, Tree of Thought, Medical Data Distillation

Summary: 
The study introduces the Quantized Medical Tree of Thought (QM-ToT), a framework designed to enhance the performance of large language models (LLMs) in specialized biomedical tasks by utilizing a Tree of Thought (ToT) reasoning approach. This approach breaks down complex medical problems into manageable subtasks, leading to significant performance improvements in INT4-quantized models on the MedQAUSMLE dataset. Specifically, accuracy increased from 34% to 50% for the LLaMA2-70b model and from 58.77% to 69.49% for LLaMA-3.1-8b. Additionally, the study proposes a novel data distillation method based on ToT, showing an 86.27% improvement while using only 3.9% of the data. These findings demonstrate the potential of ToT in enhancing the performance of quantized LLMs on complex biomedical tasks, laying the groundwork for deploying high-performing models in resource-limited medical settings.<br /><br />Summary: <div>
arXiv:2504.12334v1 Announce Type: new 
Abstract: Large language models (LLMs) face significant challenges in specialized biomedical tasks due to the inherent complexity of medical reasoning and the sensitive nature of clinical data. Existing LLMs often struggle with intricate medical terminology and the need for accurate clinical insights, leading to performance reduction when quantized for resource-constrained deployment. To address these issues, we propose Quantized Medical Tree of Thought (QM-ToT), a path-based reasoning framework. QM-ToT leverages a Tree of Thought (ToT) reasoning approach to decompose complex medical problems into manageable subtasks, coupled with evaluator assessment layers. This framework facilitates substantial performance improvements in INT4-quantized models on the challenging MedQAUSMLE dataset. Specifically, we demonstrate a remarkable accuracy increase from 34% to 50% for the LLaMA2-70b model and from 58.77% to 69.49% for LLaMA-3.1-8b. Besides, we also proposed an effect data distillation method based on ToT. Compared to the traditional distillation method, we achieved an improvement of 86. 27% while using only 3.9% of the data.This work, for the first time, showcases the potential of ToT to significantly enhance performance on complex biomedical tasks, establishing a crucial foundation for future advances in deploying high-performing quantized LLM in resource-limited medical settings.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You've Changed: Detecting Modification of Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2504.12335</link>
<guid>https://arxiv.org/abs/2504.12335</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, API service, monitoring, text features, prompt injection attacks

Summary: 
Large Language Models (LLMs) are increasingly being provided as a service through APIs, making it difficult for developers to detect changes in their behavior. A new approach has been proposed to monitor LLMs for changes by comparing the distributions of linguistic and psycholinguistic features of generated text. This method involves using a statistical test to determine the equivalence of feature distributions from two text samples, allowing developers to identify when an LLM has changed. The study demonstrated the effectiveness of this approach using several OpenAI completion models and Meta's Llama 3 70B chat model. The results showed that simple text features, combined with a statistical test, can differentiate between different language models. Furthermore, the approach was also explored for its potential in detecting prompt injection attacks. This work enables developers to monitor LLM changes frequently without the need for computationally expensive benchmark evaluations. 

<br /><br />Summary: <div>
arXiv:2504.12335v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are often provided as a service via an API, making it challenging for developers to detect changes in their behavior. We present an approach to monitor LLMs for changes by comparing the distributions of linguistic and psycholinguistic features of generated text. Our method uses a statistical test to determine whether the distributions of features from two samples of text are equivalent, allowing developers to identify when an LLM has changed. We demonstrate the effectiveness of our approach using five OpenAI completion models and Meta's Llama 3 70B chat model. Our results show that simple text features coupled with a statistical test can distinguish between language models. We also explore the use of our approach to detect prompt injection attacks. Our work enables frequent LLM change monitoring and avoids computationally expensive benchmark evaluations.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool</title>
<link>https://arxiv.org/abs/2504.12337</link>
<guid>https://arxiv.org/abs/2504.12337</guid>
<content:encoded><![CDATA[
<div> TikTok comments, LLMs, mental health support, user experiences, attitudes <br />
<br />
Summary: The study explores user engagement with large language models (LLMs) as mental health tools through analysis of TikTok comments. Nearly 20% of comments indicate personal use, with users expressing positive attitudes towards LLMs for accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and lack of professional oversight are prevalent. The need for clinical and ethical scrutiny in using AI for mental health support is emphasized. It is noted that user feedback does not specify the therapeutic framework aligning with LLM-generated output, highlighting the importance of understanding the implications of AI in mental health practices. <div>
arXiv:2504.12337v1 Announce Type: new 
Abstract: The emergence of generative AI chatbots such as ChatGPT has prompted growing public and academic interest in their role as informal mental health support tools. While early rule-based systems have been around for several years, large language models (LLMs) offer new capabilities in conversational fluency, empathy simulation, and availability. This study explores how users engage with LLMs as mental health tools by analyzing over 10,000 TikTok comments from videos referencing LLMs as mental health tools. Using a self-developed tiered coding schema and supervised classification models, we identify user experiences, attitudes, and recurring themes. Results show that nearly 20% of comments reflect personal use, with these users expressing overwhelmingly positive attitudes. Commonly cited benefits include accessibility, emotional support, and perceived therapeutic value. However, concerns around privacy, generic responses, and the lack of professional oversight remain prominent. It is important to note that the user feedback does not indicate which therapeutic framework, if any, the LLM-generated output aligns with. While the findings underscore the growing relevance of AI in everyday practices, they also highlight the urgent need for clinical and ethical scrutiny in the use of AI for mental health support.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions</title>
<link>https://arxiv.org/abs/2504.12338</link>
<guid>https://arxiv.org/abs/2504.12338</guid>
<content:encoded><![CDATA[
<div> Keywords: predictive models, healthcare, clinical notes, GPT-4o-mini, mortality prediction

Summary:
- The study explores using GPT-4o-mini to answer clinical questions about patients using discharge summaries for mortality prediction.
- Data from 14,011 admissions to cardiac care units was used, showing that GPT-based models alone outperform traditional tabular data models.
- Combining GPT responses with tabular data increases predictive power, with a 5.1% increase in AUC and a 29.9% increase in positive predictive value for high-risk patients.
- Integration of large language models (LLMs) in healthcare prediction tasks is shown to be beneficial.
- The study suggests the potential for leveraging LLMs in various domains where unstructured text data is underutilized.<br /><br />Summary: <div>
arXiv:2504.12338v1 Announce Type: new 
Abstract: There is a long history of building predictive models in healthcare using tabular data from electronic medical records. However, these models fail to extract the information found in unstructured clinical notes, which document diagnosis, treatment, progress, medications, and care plans. In this study, we investigate how answers generated by GPT-4o-mini (ChatGPT) to simple clinical questions about patients, when given access to the patient's discharge summary, can support patient-level mortality prediction. Using data from 14,011 first-time admissions to the Coronary Care or Cardiovascular Intensive Care Units in the MIMIC-IV Note dataset, we implement a transparent framework that uses GPT responses as input features in logistic regression models. Our findings demonstrate that GPT-based models alone can outperform models trained on standard tabular data, and that combining both sources of information yields even greater predictive power, increasing AUC by an average of 5.1 percentage points and increasing positive predictive value by 29.9 percent for the highest-risk decile. These results highlight the value of integrating large language models (LLMs) into clinical prediction tasks and underscore the broader potential for using LLMs in any domain where unstructured text data remains an underutilized resource.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture</title>
<link>https://arxiv.org/abs/2504.12339</link>
<guid>https://arxiv.org/abs/2504.12339</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, text-to-speech synthesis, dual-branch architecture, paralinguistic features, fine-tuning

Summary:
The article introduces a novel approach, GOAT-TTS, to address key challenges in text-to-speech synthesis using large language models (LLMs). Existing architectures face limitations such as loss of acoustic characteristics, dependency on aligned speech-text pairs, and forgetting linguistic knowledge during optimization. GOAT-TTS incorporates a dual-branch architecture to capture continuous acoustic embeddings and enable bidirectional correlation between paralinguistic features and semantic text representations without the need for transcripts. The framework also utilizes modular fine-tuning on LLM layers for speech token prediction while preserving foundational linguistic knowledge. Multi-token prediction supports real-time streaming TTS synthesis. Experimental results show that GOAT-TTS achieves comparable performance to state-of-the-art models and validates the effectiveness of synthesized dialect speech data. This approach presents a promising solution for enhancing text-to-speech synthesis capabilities using LLMs. 

<br /><br />Summary: <div>
arXiv:2504.12339v1 Announce Type: new 
Abstract: While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-k layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streamlining Biomedical Research with Specialized LLMs</title>
<link>https://arxiv.org/abs/2504.12341</link>
<guid>https://arxiv.org/abs/2504.12341</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, information retrieval, question answering, biomedical, pharmaceutical

Summary: 
The paper introduces a novel system that combines advanced language models with information retrieval techniques for precise and context-aware responses. By integrating various components, the system can validate outputs and generate high-quality responses with relevant data. It leverages a robust question-answering model to improve dialogue generation and enhance response accuracy. The system offers real-time interactions and access to a wide range of literature and data, benefiting professionals in biomedical and pharmaceutical fields. Users can experience efficient human-computer interaction and make faster, more informed decisions during the research and development process. This system is accessible at https://synapse-chat.patsnap.com. 

Summary: <div>
arXiv:2504.12341v1 Announce Type: new 
Abstract: In this paper, we propose a novel system that integrates state-of-the-art, domain-specific large language models with advanced information retrieval techniques to deliver comprehensive and context-aware responses. Our approach facilitates seamless interaction among diverse components, enabling cross-validation of outputs to produce accurate, high-quality responses enriched with relevant data, images, tables, and other modalities. We demonstrate the system's capability to enhance response precision by leveraging a robust question-answering model, significantly improving the quality of dialogue generation. The system provides an accessible platform for real-time, high-fidelity interactions, allowing users to benefit from efficient human-computer interaction, precise retrieval, and simultaneous access to a wide range of literature and data. This dramatically improves the research efficiency of professionals in the biomedical and pharmaceutical domains and facilitates faster, more informed decision-making throughout the R\&amp;D process. Furthermore, the system proposed in this paper is available at https://synapse-chat.patsnap.com.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation</title>
<link>https://arxiv.org/abs/2504.12342</link>
<guid>https://arxiv.org/abs/2504.12342</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented Large Language Models, Benchmark, Biopharmaceuticals, QRUC, Citation-based classification<br />
Summary:<br />
This paper introduces BRAGE, a benchmark designed for evaluating the Query and Reference Understanding Capability (QRUC) of Large Language Models (LLMs) in the biopharmaceutical domain. Existing traditional question-answering metrics are insufficient for open-ended retrieval-augmented QA scenarios, hence a citation-based classification method is proposed. Mainstream LLMs are evaluated on BRAGE using this method, revealing a significant gap in their biopharmaceutical QRUC. Improvement in the QRUC of LLM models used in biopharmaceuticals is necessary for better performance. <div>
arXiv:2504.12342v1 Announce Type: new 
Abstract: Recently, the application of the retrieval-augmented Large Language Models (LLMs) in specific domains has gained significant attention, especially in biopharmaceuticals. However, in this context, there is no benchmark specifically designed for biopharmaceuticals to evaluate LLMs. In this paper, we introduce the Biopharmaceuticals Retrieval-Augmented Generation Evaluation (BRAGE) , the first benchmark tailored for evaluating LLMs' Query and Reference Understanding Capability (QRUC) in the biopharmaceutical domain, available in English, French, German and Chinese. In addition, Traditional Question-Answering (QA) metrics like accuracy and exact match fall short in the open-ended retrieval-augmented QA scenarios. To address this, we propose a citation-based classification method to evaluate the QRUC of LLMs to understand the relationship between queries and references. We apply this method to evaluate the mainstream LLMs on BRAGE. Experimental results show that there is a significant gap in the biopharmaceutical QRUC of mainstream LLMs, and their QRUC needs to be improved.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Propaganda via AI? A Study on Semantic Backdoors in Large Language Models</title>
<link>https://arxiv.org/abs/2504.12344</link>
<guid>https://arxiv.org/abs/2504.12344</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, semantic backdoors, detection framework, cross-model consistency analysis, concept-level auditing

Summary: 
Large language models (LLMs) have shown exceptional performance in various language tasks but are susceptible to backdoor attacks where hidden triggers manipulate model outputs. This study highlights the presence of semantic backdoors in LLMs, which are covert triggers embedded at a conceptual level, making them harder to detect than token-level anomalies. The researchers demonstrate the practical feasibility of implanting semantic backdoors with a small poisoned corpus. They introduce a black-box detection framework called RAVEN, which combines semantic entropy with cross-model consistency analysis to uncover semantic backdoors. By probing multiple LLMs with structured topic-perspective prompts and comparing their responses, the researchers identify previously undetected vulnerabilities in models such as GPT-4o, Llama, DeepSeek, and Mistral. This work underscores the importance of concept-level auditing for deployed language models and provides a valuable tool for detecting semantic backdoors. The code and data for RAVEN are available on GitHub for further research and development.<br /><br />Summary: <div>
arXiv:2504.12344v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate remarkable performance across myriad language tasks, yet they remain vulnerable to backdoor attacks, where adversaries implant hidden triggers that systematically manipulate model outputs. Traditional defenses focus on explicit token-level anomalies and therefore overlook semantic backdoors-covert triggers embedded at the conceptual level (e.g., ideological stances or cultural references) that rely on meaning-based cues rather than lexical oddities. We first show, in a controlled finetuning setting, that such semantic backdoors can be implanted with only a small poisoned corpus, establishing their practical feasibility. We then formalize the notion of semantic backdoors in LLMs and introduce a black-box detection framework, RAVEN (short for "Response Anomaly Vigilance for uncovering semantic backdoors"), which combines semantic entropy with cross-model consistency analysis. The framework probes multiple models with structured topic-perspective prompts, clusters the sampled responses via bidirectional entailment, and flags anomalously uniform outputs; cross-model comparison isolates model-specific anomalies from corpus-wide biases. Empirical evaluations across diverse LLM families (GPT-4o, Llama, DeepSeek, Mistral) uncover previously undetected semantic backdoors, providing the first proof-of-concept evidence of these hidden vulnerabilities and underscoring the urgent need for concept-level auditing of deployed language models. We open-source our code and data at https://github.com/NayMyatMin/RAVEN.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reimagining Urban Science: Scaling Causal Inference with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12345</link>
<guid>https://arxiv.org/abs/2504.12345</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban causal research, large language models, AutoUrbanCI, AI-augmented workflows, inclusive urban causal reasoning

Summary:
Urban causal research is crucial for understanding cities and informing policies, but faces challenges with hypothesis generation, data complexity, and experimental methodology. This Perspective explores current research gaps and proposes an innovative framework, AutoUrbanCI, leveraging large language models for hypothesis generation, data processing, experiment design, and results interpretation with policy recommendations. Evaluation criteria are suggested for rigor and transparency, emphasizing the potential of human-AI collaboration to enhance urban causal analysis. The framework aims to broaden participation, improve reproducibility, and foster more inclusive urban causal reasoning. This new research agenda advocates for AI-augmented workflows as tools to complement human expertise, enhancing urban research practices for greater equity and accountability. 

<br /><br />Summary: <div>
arXiv:2504.12345v1 Announce Type: new 
Abstract: Urban causal research is essential for understanding the complex dynamics of cities and informing evidence-based policies. However, it is challenged by the inefficiency and bias of hypothesis generation, barriers to multimodal data complexity, and the methodological fragility of causal experimentation. Recent advances in large language models (LLMs) present an opportunity to rethink how urban causal analysis is conducted. This Perspective examines current urban causal research by analyzing taxonomies that categorize research topics, data sources, and methodological approaches to identify structural gaps. We then introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy recommendations. We propose evaluation criteria for rigor and transparency and reflect on implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces AI-augmented workflows not as replacements for human expertise but as tools to broaden participation, improve reproducibility, and unlock more inclusive forms of urban causal reasoning.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination</title>
<link>https://arxiv.org/abs/2504.12347</link>
<guid>https://arxiv.org/abs/2504.12347</guid>
<content:encoded><![CDATA[
<div> language models, mathematical reasoning, educational settings, Finnish matriculation examination, high-stakes test <br />
<br />
Summary: 
This study evaluates the mathematical capabilities of large language models (LLMs) using the Finnish matriculation examination, a high-stakes digital test for upper secondary education. Initial tests showed moderate performance, but as the language models evolved, their mathematical proficiency substantially improved. Some models achieved near-perfect or perfect scores, on par with top student performance, demonstrating their potential to support educational assessments at scale. The findings highlight the rapid advances in LLMs' mathematical reasoning and their ability to match human performance in a high-stakes academic setting. <div>
arXiv:2504.12347v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown increasing promise in educational settings, yet their mathematical reasoning has been considered evolving. This study evaluates the mathematical capabilities of various LLMs using the Finnish matriculation examination, a high-stakes digital test for upper secondary education. Initial tests yielded moderate performance corresponding to mid-range grades, but later evaluations demonstrated substantial improvements as the language models evolved. Remarkably, some models achieved near-perfect or perfect scores, matching top student performance and qualifying for university admission. Our findings highlight the rapid advances in the mathematical proficiency of LLMs and illustrate their potential to also support educational assessments at scale.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports</title>
<link>https://arxiv.org/abs/2504.12350</link>
<guid>https://arxiv.org/abs/2504.12350</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical events, Temporal localization, Electronic health records, Textual time series, Language model<br />
Summary: 
The article discusses the importance of timing in clinical events for analyzing patient trajectories. Structured electronic health records and clinical reports often lack crucial temporal data. The study presents a system that converts case reports into pairs of textual events and timestamps. Manual and large language model annotations were compared, with the latter showing moderate event recall but high temporal concordance. The research establishes a benchmark for leveraging the PubMed open-access corpus for temporal analytics. This work highlights the need for structured temporal data in healthcare analytics and demonstrates the feasibility of utilizing language models for this purpose.<br /><br />Summary: <div>
arXiv:2504.12350v1 Announce Type: new 
Abstract: Timing of clinical events is central to characterization of patient trajectories, enabling analyses such as process tracing, forecasting, and causal reasoning. However, structured electronic health records capture few data elements critical to these tasks, while clinical reports lack temporal localization of events in structured form. We present a system that transforms case reports into textual time series-structured pairs of textual events and timestamps. We contrast manual and large language model (LLM) annotations (n=320 and n=390 respectively) of ten randomly-sampled PubMed open-access (PMOA) case reports (N=152,974) and assess inter-LLM agreement (n=3,103; N=93). We find that the LLM models have moderate event recall(O1-preview: 0.80) but high temporal concordance among identified events (O1-preview: 0.95). By establishing the task, annotation, and assessment systems, and by demonstrating high concordance, this work may serve as a benchmark for leveraging the PMOA corpus for temporal analytics.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media</title>
<link>https://arxiv.org/abs/2504.12355</link>
<guid>https://arxiv.org/abs/2504.12355</guid>
<content:encoded><![CDATA[
<div> Keywords: Drug overdose, social media, AI-driven NLP framework, neural networks, public health surveillance <br />
Summary: <br />
- The study focuses on utilizing social media data to detect commonly used drugs and associated overdose symptoms using an AI-driven NLP framework. 
- Traditional research methods are limited in this area, and social media provides real-time insights into self-reported substance use. 
- The framework achieved high accuracy rates of 98% in multi-class classification and 97% in multi-label classification by employing a hybrid annotation strategy and advanced transformer-based models. 
- AI technology has the potential to support public health surveillance efforts and personalized intervention strategies for addressing drug overdose issues. 
- The findings of the study highlight the efficacy of using AI in analyzing social media data for detecting drug use patterns and overdose symptoms, demonstrating its value in enhancing public health initiatives. <br /> 
Summary: <div>
arXiv:2504.12355v1 Announce Type: new 
Abstract: Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Replicating ReLM Results: Validating Large Language Models with ReLM</title>
<link>https://arxiv.org/abs/2504.12357</link>
<guid>https://arxiv.org/abs/2504.12357</guid>
<content:encoded><![CDATA[
<div> formal languages, Large Language Models (LLMs), evaluation, memorization, bias 

Summary:
This study focuses on Validating Large Language Models (LLMs) using formal languages to assess and manage their behavior in terms of memorization, bias, and zero-shot performance. Currently, existing approaches for evaluating LLMs are usually slow, imprecise, costly, or introduce their own biases. The study highlights the significance of understanding and controlling these behaviors when deploying LLMs in real-world applications. By replicating key findings from the original ReLM paper and elaborating on the methodology and potential applications, this research sheds light on the relevance of formal languages in the field of machine learning systems. <div>
arXiv:2504.12357v1 Announce Type: new 
Abstract: Validating Large Language Models with ReLM explores the application of formal languages to evaluate and control Large Language Models (LLMs) for memorization, bias, and zero-shot performance. Current approaches for evaluating these types behavior are often slow, imprecise, costly, or introduce biases of their own, but are necessary due to the importance of this behavior when productionizing LLMs. This project reproduces key results from the original ReLM paper and expounds on the approach and applications with an emphasis on the relevance to the field of systems for machine learning.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version</title>
<link>https://arxiv.org/abs/2504.12360</link>
<guid>https://arxiv.org/abs/2504.12360</guid>
<content:encoded><![CDATA[
<div> Graph Spectral Clustering, negative similarities, document embeddings, Laplacians, GloVe
<br />
Summary: 
This paper explores the issue of Graph Spectral Clustering with negative similarities resulting from document embeddings other than the traditional Term Vector Space. It delves into solutions for both combinatorial and normalized Laplacians, discussing the advantages and disadvantages of various proposed methods. The study reveals that GloVe embeddings can impede normalized Laplacian based Graph Spectral Clustering due to negative similarities. Implementing techniques to address negative similarities enhances accuracy for both types of Laplacians. It also enables the application of explanation methods developed for Term Vector Space embeddings to GloVe embeddings. Through experimental investigation, the research highlights the challenges posed by negative similarities in document embeddings and offers insights into improving the performance of Graph Spectral Clustering algorithms. <div>
arXiv:2504.12360v1 Announce Type: new 
Abstract: This paper investigates the problem of Graph Spectral Clustering with negative similarities, resulting from document embeddings different from the traditional Term Vector Space (like doc2vec, GloVe, etc.). Solutions for combinatorial Laplacians and normalized Laplacians are discussed. An experimental investigation shows the advantages and disadvantages of 6 different solutions proposed in the literature and in this research. The research demonstrates that GloVe embeddings frequently cause failures of normalized Laplacian based GSC due to negative similarities. Furthermore, application of methods curing similarity negativity leads to accuracy improvement for both combinatorial and normalized Laplacian based GSC. It also leads to applicability for GloVe embeddings of explanation methods developed originally bythe authors for Term Vector Space embeddings.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Most Expensive Part of an LLM should be its Training Data</title>
<link>https://arxiv.org/abs/2504.12427</link>
<guid>https://arxiv.org/abs/2504.12427</guid>
<content:encoded><![CDATA[
<div> compensation, Large Language Model, training data, human labor, financial liability
Summary:
This position paper argues for assigning monetary value to the human labor behind training data for Large Language Models (LLMs). It highlights that the costs of producing training datasets for LLMs are significantly higher than the costs of training the models themselves. The study examines 64 LLMs from 2016 to 2024 and shows that even conservative estimates of wage rates result in training data costs being 10-1000 times larger. This discrepancy poses a financial liability for LLM providers. The paper calls for fairer practices in compensating training data producers to bridge the gap between the value of their labor and the lack of compensation. Discussions on research directions to achieve fairer practices in the future are presented. 
<br /><br />Summary: <div>
arXiv:2504.12427v1 Announce Type: new 
Abstract: Training a state-of-the-art Large Language Model (LLM) is an increasingly expensive endeavor due to growing computational, hardware, energy, and engineering demands. Yet, an often-overlooked (and seldom paid) expense is the human labor behind these models' training data. Every LLM is built on an unfathomable amount of human effort: trillions of carefully written words sourced from books, academic papers, codebases, social media, and more. This position paper aims to assign a monetary value to this labor and argues that the most expensive part of producing an LLM should be the compensation provided to training data producers for their work. To support this position, we study 64 LLMs released between 2016 and 2024, estimating what it would cost to pay people to produce their training datasets from scratch. Even under highly conservative estimates of wage rates, the costs of these models' training datasets are 10-1000 times larger than the costs to train the models themselves, representing a significant financial liability for LLM providers. In the face of the massive gap between the value of training data and the lack of compensation for its creation, we highlight and discuss research directions that could enable fairer practices in the future.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Linear Representations and Pretraining Data Frequency in Language Models</title>
<link>https://arxiv.org/abs/2504.12459</link>
<guid>https://arxiv.org/abs/2504.12459</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, pretraining data, linear representations, factual relations, regression model

Summary:
The study explores the relationship between pretraining data frequency and language model representations, particularly focusing on the linear encoding of factual relations. It is found that the frequency of co-occurrence of subject-object pairs and in-context learning accuracy for relations strongly influences the formation of linear representations in models like OLMo-7B and GPT-J. The research identifies specific frequency thresholds (1k for subjects, 2k for objects) at which linear representations consistently form during pretraining. A regression model is developed to predict term frequencies in pretraining data, even across different models. The findings suggest that the quality of linear representations in language models can provide insights into the characteristics of the pretraining corpora, offering opportunities for optimizing model behavior by manipulating training data to meet desired frequency thresholds. 

<br /><br />Summary: <div>
arXiv:2504.12459v1 Announce Type: new 
Abstract: Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data's effect on downstream task behavior, we investigate its relationship to LM representations. Previous work has discovered that, in language models, some concepts are encoded `linearly' in the representations, but what factors cause these representations to form? We study the connection between pretraining data frequency and models' linear representations of factual relations. We find evidence that the formation of linear representations is strongly connected to pretraining term frequencies; specifically for subject-relation-object fact triplets, both subject-object co-occurrence frequency and in-context learning accuracy for the relation are highly correlated with linear representations. This is the case across all phases of pretraining. In OLMo-7B and GPT-J, we discover that a linear representation consistently (but not exclusively) forms when the subjects and objects within a relation co-occur at least 1k and 2k times, respectively, regardless of when these occurrences happen during pretraining. Finally, we train a regression model on measurements of linear representation quality in fully-trained LMs that can predict how often a term was seen in pretraining. Our model achieves low error even on inputs from a different model with a different pretraining dataset, providing a new method for estimating properties of the otherwise-unknown training data of closed-data models. We conclude that the strength of linear representations in LMs contains signal about the models' pretraining corpora that may provide new avenues for controlling and improving model behavior: particularly, manipulating the models' training data to meet specific frequency thresholds.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse</title>
<link>https://arxiv.org/abs/2504.12466</link>
<guid>https://arxiv.org/abs/2504.12466</guid>
<content:encoded><![CDATA[
<div> Keywords: fallacies, manipulation detection, social media, linguistic models, online forums 

Summary: 
This paper examines the detection of fallacies in online discourse, focusing on the Ukrainian-Russian Conflict discussion boards. While automated fallacy detection has been studied, current datasets are limited to formal linguistic domains. The authors introduce SLURG, a method using large language models to generate synthetic fallacious forum-style comments. Their research shows that LLMs can replicate real data's syntactic patterns and mimic online forum vocabulary diversity with high-quality few-shot prompts. The prevalence of misinformation and misguided intentions in online discussions highlights the importance of detecting fallacies in social media. The study expands the scope of automated fallacy detection to non-standardized language domains, such as online forums, providing insights into improving manipulation detection algorithms in online discourse. <br /><br />Summary: <div>
arXiv:2504.12466v1 Announce Type: new 
Abstract: In our paper we explore the definition, and extrapolation of fallacies as they pertain to the automatic detection of manipulation on social media. In particular we explore how these logical fallacies might appear in the real world i.e internet forums. We discovered a prevalence of misinformation / misguided intention in discussion boards specifically centered around the Ukrainian Russian Conflict which serves to narrow the domain of our task. Although automatic fallacy detection has gained attention recently, most datasets use unregulated fallacy taxonomies or are limited to formal linguistic domains like political debates or news reports. Online discourse, however, often features non-standardized and diverse language not captured in these domains. We present Shady Linguistic Utterance Replication-Generation (SLURG) to address these limitations, exploring the feasibility of generating synthetic fallacious forum-style comments using large language models (LLMs), specifically DeepHermes-3-Mistral-24B. Our findings indicate that LLMs can replicate the syntactic patterns of real data} and that high-quality few-shot prompts enhance LLMs' ability to mimic the vocabulary diversity of online forums.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex</title>
<link>https://arxiv.org/abs/2504.12474</link>
<guid>https://arxiv.org/abs/2504.12474</guid>
<content:encoded><![CDATA[
<div> Graph-Text Fusion, GNNs, LLMs, Bi-directional Attention, Node Classification <br />
<br />
Summary:
The paper introduces BiGTex, a novel architecture for representation learning on text-attributed graphs. By integrating Graph Neural Networks (GNNs) and Large Language Models (LLMs) through Graph-Text Fusion Units, BiGTex enables bidirectional information flow between textual content and graph structure. The architecture leverages mutual attention mechanisms to allow text to influence structure and vice versa. Training the model using parameter-efficient fine-tuning (LoRA), BiGTex achieves state-of-the-art performance in node classification and effectively generalizes to link prediction tasks. The results of extensive experiments on various benchmark datasets demonstrate the effectiveness of soft prompting and bi-directional attention in enhancing model performance. BiGTex addresses the unique challenges posed by Text-attributed graphs by combining the strengths of GNNs in capturing graph topology information and LLMs in understanding unstructured text, highlighting the importance of integrating different modalities for comprehensive representation learning. <br /> <div>
arXiv:2504.12474v1 Announce Type: new 
Abstract: Text-attributed graphs (TAGs) present unique challenges in representation learning by requiring models to capture both the semantic richness of node-associated texts and the structural dependencies of the graph. While graph neural networks (GNNs) excel at modeling topological information, they lack the capacity to process unstructured text. Conversely, large language models (LLMs) are proficient in text understanding but are typically unaware of graph structure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel architecture that tightly integrates GNNs and LLMs through stacked Graph-Text Fusion Units. Each unit allows for mutual attention between textual and structural representations, enabling information to flow in both directions, text influencing structure and structure guiding textual interpretation. The proposed architecture is trained using parameter-efficient fine-tuning (LoRA), keeping the LLM frozen while adapting to task-specific signals. Extensive experiments on five benchmark datasets demonstrate that BiGTex achieves state-of-the-art performance in node classification and generalizes effectively to link prediction. An ablation study further highlights the importance of soft prompting and bi-directional attention in the model's success.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?</title>
<link>https://arxiv.org/abs/2504.12491</link>
<guid>https://arxiv.org/abs/2504.12491</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-training checkpoints, downstream fine-tuning, LLMs, proxy metrics, supervised learning

Summary: 
The study focuses on the selection of pre-training checkpoints to maximize downstream fine-tuning performance in Large Language Models (LLMs). Traditional metrics like perplexity are found to be unreliable indicators of success. To address this, a new dataset of 1B parameter LLM variants with different pre-training configurations is used. Unsupervised and supervised proxy metrics derived from pre-training are introduced to improve performance prediction accuracy by over 50%. Despite the complexity of the task, the proposed proxies show practical utility in specific scenarios, enabling more efficient design of pre-training schemes tailored for various downstream tasks. 

<br /><br />Summary: <div>
arXiv:2504.12491v1 Announce Type: new 
Abstract: While metrics available during pre-training, such as perplexity, correlate well with model performance at scaling-laws studies, their predictive capacities at a fixed model size remain unclear, hindering effective model selection and development. To address this gap, we formulate the task of selecting pre-training checkpoints to maximize downstream fine-tuning performance as a pairwise classification problem: predicting which of two LLMs, differing in their pre-training, will perform better after supervised fine-tuning (SFT). We construct a dataset using 50 1B parameter LLM variants with systematically varied pre-training configurations, e.g., objectives or data, and evaluate them on diverse downstream tasks after SFT. We first conduct a study and demonstrate that the conventional perplexity is a misleading indicator. As such, we introduce novel unsupervised and supervised proxy metrics derived from pre-training that successfully reduce the relative performance prediction error rate by over 50%. Despite the inherent complexity of this task, we demonstrate the practical utility of our proposed proxies in specific scenarios, paving the way for more efficient design of pre-training schemes optimized for various downstream tasks.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification</title>
<link>https://arxiv.org/abs/2504.12494</link>
<guid>https://arxiv.org/abs/2504.12494</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical NLP, Hybrid framework, Support Vector Machine, BERT-based model, Dementia identification <br />
Summary: <br />
A hybrid NLP framework combining rule-based filtering, SVM classifier, and BERT-based model was proposed to enhance efficiency and maintain accuracy in clinical text analysis. In a dementia identification case study with 4.9 million veterans and 2.1 billion clinical notes, the method achieved a precision of 0.90, recall of 0.84, and F1-score of 0.87 at the patient level. It outperformed structured data methods by identifying over three times as many dementia cases. The entire processing was completed in approximately two weeks using a single machine with dual A40 GPUs, demonstrating the feasibility of large-scale clinical text analysis with limited computational resources. This approach makes advanced NLP solutions more accessible to healthcare organizations, paving the way for improved clinical research and operational practices. <br /> 
Summary: <div>
arXiv:2504.12494v1 Announce Type: new 
Abstract: Clinical natural language processing (NLP) is increasingly in demand in both clinical research and operational practice. However, most of the state-of-the-art solutions are transformers-based and require high computational resources, limiting their accessibility. We propose a hybrid NLP framework that integrates rule-based filtering, a Support Vector Machine (SVM) classifier, and a BERT-based model to improve efficiency while maintaining accuracy. We applied this framework in a dementia identification case study involving 4.9 million veterans with incident hypertension, analyzing 2.1 billion clinical notes. At the patient level, our method achieved a precision of 0.90, a recall of 0.84, and an F1-score of 0.87. Additionally, this NLP approach identified over three times as many dementia cases as structured data methods. All processing was completed in approximately two weeks using a single machine with dual A40 GPUs. This study demonstrates the feasibility of hybrid NLP solutions for large-scale clinical text analysis, making state-of-the-art methods more accessible to healthcare organizations with limited computational resources.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Text: Characterizing Domain Expert Needs in Document Research</title>
<link>https://arxiv.org/abs/2504.12495</link>
<guid>https://arxiv.org/abs/2504.12495</guid>
<content:encoded><![CDATA[
<div> document research, NLP systems, domain experts, social context, iterative process

Summary:
The study explores the effectiveness of text-based NLP systems in assisting with document research processes as performed by domain experts. By interviewing sixteen experts from different domains, the research highlights that the current NLP systems may not fully capture the complexities of document research. The experts' processes are found to be personalized, iterative, and heavily reliant on the social context of a document. NLP approaches that prioritize the document as an object rather than just text align more closely with the experts' priorities but may lack accessibility outside specific research communities. The study emphasizes the importance of considering the document's role in developing NLP tools that are accessible, customizable, iterative, and socially aware. 

<br /><br />Summary: <div>
arXiv:2504.12495v1 Announce Type: new 
Abstract: Working with documents is a key part of almost any knowledge work, from contextualizing research in a literature review to reviewing legal precedent. Recently, as their capabilities have expanded, primarily text-based NLP systems have often been billed as able to assist or even automate this kind of work. But to what extent are these systems able to model these tasks as experts conceptualize and perform them now? In this study, we interview sixteen domain experts across two domains to understand their processes of document research, and compare it to the current state of NLP systems. We find that our participants processes are idiosyncratic, iterative, and rely extensively on the social context of a document in addition its content; existing approaches in NLP and adjacent fields that explicitly center the document as an object, rather than as merely a container for text, tend to better reflect our participants' priorities, though they are often less accessible outside their research communities. We call on the NLP community to more carefully consider the role of the document in building useful tools that are accessible, personalizable, iterative, and socially aware.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents</title>
<link>https://arxiv.org/abs/2504.12516</link>
<guid>https://arxiv.org/abs/2504.12516</guid>
<content:encoded><![CDATA[
<div> Benchmark, browsing agents, internet, information retrieval, persistence

Summary:
BrowseComp is a new benchmark designed to test the browsing capabilities of agents navigating the web. It consists of 1,266 challenging questions that require agents to search for elusive information online. The benchmark aims to assess an agent's ability to persistently navigate the internet to find difficult-to-locate data. Despite the complexity of the questions, BrowseComp is user-friendly, with short and easily verifiable predicted answers. The benchmark can be likened to programming competitions for coding agents, serving as a valuable measure of an agent's persistence and creativity in information retrieval. While BrowseComp does not mimic real user query distribution perfectly, it focuses on the core ability of finding information effectively. BrowseComp can be accessed on GitHub at https://github.com/openai/simple-evals. 

<br /><br />Summary: <div>
arXiv:2504.12516v1 Announce Type: new 
Abstract: We present BrowseComp, a simple yet challenging benchmark for measuring the ability for agents to browse the web. BrowseComp comprises 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. BrowseComp for browsing agents can be seen as analogous to how programming competitions are an incomplete but useful benchmark for coding agents. While BrowseComp sidesteps challenges of a true user query distribution, like generating long answers or resolving ambiguity, it measures the important core capability of exercising persistence and creativity in finding information. BrowseComp can be found at https://github.com/openai/simple-evals.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Diversity and Quality of LLM Generated Content</title>
<link>https://arxiv.org/abs/2504.12522</link>
<guid>https://arxiv.org/abs/2504.12522</guid>
<content:encoded><![CDATA[
<div> preference-tuning, semantic diversity, language models, diversity, syntactic diversity  
Summary:  
- Preference-tuning techniques in language models like PPO and GRPO reduce diversity but improve effective semantic diversity.  
- A framework for measuring effective semantic diversity in LLMs reveals a distinction between syntactic and semantic diversity.  
- Open-ended tasks show that preference-tuned models generate more high-quality outputs overall.  
- Smaller models are more parameter-efficient at generating unique content within a fixed sampling budget.  
- The findings have implications for applications requiring diverse yet high-quality outputs, such as creative assistance and synthetic data generation.  

<br /><br />Summary: <div>
arXiv:2504.12522v1 Announce Type: new 
Abstract: Recent work suggests that preference-tuning techniques--including Reinforcement Learning from Human Preferences (RLHF) methods like PPO and GRPO, as well as alternatives like DPO--reduce diversity, creating a dilemma given that such models are widely deployed in applications requiring diverse outputs. To address this, we introduce a framework for measuring effective semantic diversity--diversity among outputs that meet quality thresholds--which better reflects the practical utility of large language models (LLMs). Using open-ended tasks that require no human intervention, we find counterintuitive results: although preference-tuned models--especially those trained via RL--exhibit reduced lexical and syntactic diversity, they produce greater effective semantic diversity than SFT or base models, not from increasing diversity among high-quality outputs, but from generating more high-quality outputs overall. We discover that preference tuning reduces syntactic diversity while preserving semantic diversity--revealing a distinction between diversity in form and diversity in content that traditional metrics often overlook. Our analysis further shows that smaller models are consistently more parameter-efficient at generating unique content within a fixed sampling budget, offering insights into the relationship between model scaling and diversity. These findings have important implications for applications that require diverse yet high-quality outputs, from creative assistance to synthetic data generation.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization vs. Reasoning: Updating LLMs with New Knowledge</title>
<link>https://arxiv.org/abs/2504.12523</link>
<guid>https://arxiv.org/abs/2504.12523</guid>
<content:encoded><![CDATA[
<div> pipeline, knowledge update, memory conditioning, large language models, reasoning

Summary:
Knowledge Update Playground (KUP) is introduced as an automatic pipeline for simulating realistic knowledge updates in large language models (LLMs). The evaluation framework of KUP includes direct and indirect probes to test memorization and reasoning abilities of LLMs after updates. Memory conditioned training (MCT) is proposed as a lightweight method to improve LLMs' performance by conditioning tokens on self-generated "memory" tokens during training. The results show that the KUP benchmark is highly challenging, with the best models achieving minimal error rates in reasoning tasks. MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving memorization results by up to 25.4%. <div>
arXiv:2504.12523v1 Announce Type: new 
Abstract: Large language models (LLMs) encode vast amounts of pre-trained knowledge in their parameters, but updating them as real-world information evolves remains a challenge. Existing methodologies and benchmarks primarily target entity substitutions, failing to capture the full breadth of complex real-world dynamics. In this paper, we introduce Knowledge Update Playground (KUP), an automatic pipeline for simulating realistic knowledge updates reflected in an evidence corpora. KUP's evaluation framework includes direct and indirect probes to both test memorization of updated facts and reasoning over them, for any update learning methods. Next, we present a lightweight method called memory conditioned training (MCT), which conditions tokens in the update corpus on self-generated "memory" tokens during training. Our strategy encourages LLMs to surface and reason over newly memorized knowledge at inference. Our results on two strong LLMs show that (1) KUP benchmark is highly challenging, with the best CPT models achieving $<2\%$ in indirect probing setting (reasoning) and (2) MCT training significantly outperforms prior continued pre-training (CPT) baselines, improving direct probing (memorization) results by up to $25.4\%$.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization: A Close Look at Books</title>
<link>https://arxiv.org/abs/2504.12549</link>
<guid>https://arxiv.org/abs/2504.12549</guid>
<content:encoded><![CDATA[
<div> extraction, LLMs, books, regurgitation mitigation, verbatim memorization  
Summary:  
- The study explores the ability of large language models (LLMs) to auto-regressively extract entire books, showing high levels of similarity for popular works like "Alice's Adventures in Wonderland" using the Llama 3 70B models and the "prefix-prompting" technique.  
- Extraction rates of books correlate with popularity and potential duplication in training data, indicating limitations in regurgitation mitigation strategies.  
- Changes in the instruction-tuned Llama 3.1 model undo previous mitigation efforts, primarily affecting a small fraction of weights in lower transformer blocks.  
- The study highlights the challenges and limitations in mitigating verbatim memorization in aligned LLMs, suggesting the need for further research on fine-tuning effects.  
- This research provides a framework for analyzing the impact of fine-tuning on LLMs in retrieving memorized information, offering insights into the complexities of information extraction from these models.  
<br /><br />Summary: <div>
arXiv:2504.12549v1 Announce Type: new 
Abstract: To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the "prefix-prompting" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.
  We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELAB: Extensive LLM Alignment Benchmark in Persian Language</title>
<link>https://arxiv.org/abs/2504.12553</link>
<guid>https://arxiv.org/abs/2504.12553</guid>
<content:encoded><![CDATA[
<div> Keywords: Persian Large Language Models, ethical dimensions, evaluation framework, cultural norms, dataset creation

Summary: 
This paper introduces a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with ethical considerations, focusing on safety, fairness, and adherence to cultural norms. The framework addresses the lack of evaluation tools tailored to Persian language and culture by creating translated and new datasets specifically for Persian LLM evaluation. By translating existing benchmark data and creating new datasets such as ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa, the framework enables evaluation of LLMs on aspects like avoiding harmful content, mitigating biases, and adhering to culturally accepted behaviors. A unified evaluation framework is established for assessing Persian LLMs' alignment with safety, fairness, and social norms. The work includes a publicly available leaderboard for benchmarking Persian LLMs on these dimensions, promoting culturally grounded alignment evaluation. The evaluation covers safety, fairness, and adherence to social norms, providing a valuable tool for improving Persian LLMs' ethical performance in language generation tasks. 

<br /><br />Summary: <div>
arXiv:2504.12553v1 Announce Type: new 
Abstract: This paper presents a comprehensive evaluation framework for aligning Persian Large Language Models (LLMs) with critical ethical dimensions, including safety, fairness, and social norms. It addresses the gaps in existing LLM evaluation frameworks by adapting them to Persian linguistic and cultural contexts. This benchmark creates three types of Persian-language benchmarks: (i) translated data, (ii) new data generated synthetically, and (iii) new naturally collected data. We translate Anthropic Red Teaming data, AdvBench, HarmBench, and DecodingTrust into Persian. Furthermore, we create ProhibiBench-fa, SafeBench-fa, FairBench-fa, and SocialBench-fa as new datasets to address harmful and prohibited content in indigenous culture. Moreover, we collect extensive dataset as GuardBench-fa to consider Persian cultural norms. By combining these datasets, our work establishes a unified framework for evaluating Persian LLMs, offering a new approach to culturally grounded alignment evaluation. A systematic evaluation of Persian LLMs is performed across the three alignment aspects: safety (avoiding harmful content), fairness (mitigating biases), and social norms (adhering to culturally accepted behaviors). We present a publicly available leaderboard that benchmarks Persian LLMs with respect to safety, fairness, and social norms at: https://huggingface.co/spaces/MCILAB/LLM_Alignment_Evaluation.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.12560</link>
<guid>https://arxiv.org/abs/2504.12560</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, causal relationships, generative reasoning, factual accuracy, explainability

Summary: 
The article introduces CDF-RAG, a framework that enhances generative reasoning by improving causal consistency, factual accuracy, and explainability in large language models. CDF-RAG addresses the limitation of existing retrieval-augmented generation frameworks by enabling multi-hop causal reasoning and validating responses against causal pathways. By iteratively refining queries and retrieving structured causal graphs, CDF-RAG ensures logically coherent and factually grounded outputs. Through evaluations on diverse datasets, CDF-RAG demonstrates improved response accuracy and causal correctness compared to existing RAG-based methods. The framework is publicly available on GitHub for further research and development. <div>
arXiv:2504.12560v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly enhanced large language models (LLMs) in knowledge-intensive tasks by incorporating external knowledge retrieval. However, existing RAG frameworks primarily rely on semantic similarity and correlation-driven retrieval, limiting their ability to distinguish true causal relationships from spurious associations. This results in responses that may be factually grounded but fail to establish cause-and-effect mechanisms, leading to incomplete or misleading insights. To address this issue, we introduce Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation (CDF-RAG), a framework designed to improve causal consistency, factual accuracy, and explainability in generative reasoning. CDF-RAG iteratively refines queries, retrieves structured causal graphs, and enables multi-hop causal reasoning across interconnected knowledge sources. Additionally, it validates responses against causal pathways, ensuring logically coherent and factually grounded outputs. We evaluate CDF-RAG on four diverse datasets, demonstrating its ability to improve response accuracy and causal correctness over existing RAG-based methods. Our code is publicly available at https://github.com/ elakhatibi/CDF-RAG.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2504.12563</link>
<guid>https://arxiv.org/abs/2504.12563</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, language models, MetaSynth, diversity, domain adaptation 

Summary: 
MetaSynth is proposed as a method for generating synthetic data to enhance diversity by using meta-prompting with multiple expert LLM agents. This approach successfully adapts Mistral-7B-v0.3 to Finance and Biomedicine domains using only 25 million tokens of synthetic data. The diversity of the generated data is evaluated and found to be comparable to LLM pre-training corpora. Continual pre-training with MetaSynth outperforms the base LLM, showing significant improvements in specialized domains. Training Mistral-7B-v0.3 with diverse synthetic data using MetaSynth is more effective for domain adaptation compared to using a template prompt approach, even with prior generations and varying In-Context exemplars of real data. These findings suggest that a few million tokens of diverse synthetic data without mixing real data can be sufficient for effective domain adaptation when utilizing MetaSynth.<br /><br />Summary: <div>
arXiv:2504.12563v1 Announce Type: new 
Abstract: Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.
  Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models</title>
<link>https://arxiv.org/abs/2504.12585</link>
<guid>https://arxiv.org/abs/2504.12585</guid>
<content:encoded><![CDATA[
arXiv:2504.12585v1 Announce Type: new 
Abstract: Large language models (LLMs) sometimes fail to respond appropriately to deterministic tasks -- such as counting or forming acronyms -- because the implicit prior distribution they have learned over sequences of tokens influences their responses. In this work, we show that, in at least some cases, LLMs actually compute the information needed to perform these tasks correctly, and we identify some interventions that can allow them to access this information to improve their performance. First, we show that simply prompting the language model to not rely on its prior knowledge leads to dramatic improvements in prior-dominated tasks. We then use mechanistic interpretability techniques to localize the prior within the LLM and manipulate the extent to which that prior influences its responses. Specifically, we show that it is possible to identify layers of the underlying neural network that correlate with the prior probability of a response and that lightweight finetuning of these layers with basic prompts on prior-dominated tasks achieves high performance on held-out answers. These results suggest that the information required to produce a correct response is contained within the representations of the problems formed by the models. Furthermore, we show that this finetuning is significantly more effective for prior-dominated tasks, and that the error after finetuning is no longer correlated with the prior. Our results suggest that it may be possible to define effective methods for manipulating the extent to which LLMs rely upon their priors in solving problems, potentially increasing their performance in settings where LLMs hallucinate for reasons related to the prior probability of token sequences.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2504.12597</link>
<guid>https://arxiv.org/abs/2504.12597</guid>
<content:encoded><![CDATA[
arXiv:2504.12597v1 Announce Type: new 
Abstract: Geometry problem-solving (GPS), a challenging task requiring both visual comprehension and symbolic reasoning, effectively measures the reasoning capabilities of multimodal large language models (MLLMs). Humans exhibit strong reasoning ability in this task through accurate identification and adaptive application of geometric principles within visual contexts. However, existing benchmarks fail to jointly assess both dimensions of the human-like geometric reasoning mechanism in MLLMs, remaining a critical gap in assessing their ability to tackle GPS. To this end, we introduce GeoSense, the first comprehensive bilingual benchmark designed to systematically evaluate the geometric reasoning abilities of MLLMs through the lens of geometric principles. GeoSense features a five-level hierarchical framework of geometric principles spanning plane and solid geometry, an intricately annotated dataset of 1,789 problems, and an innovative evaluation strategy. Through extensive experiments on GeoSense with various open-source and closed-source MLLMs, we observe that Gemini-2.0-pro-flash performs best, achieving an overall score of $65.3$. Our in-depth analysis reveals that the identification and application of geometric principles remain a bottleneck for leading MLLMs, jointly hindering their reasoning abilities. These findings underscore GeoSense's potential to guide future advancements in MLLMs' geometric reasoning capabilities, paving the way for more robust and human-like reasoning in artificial intelligence.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs</title>
<link>https://arxiv.org/abs/2504.12633</link>
<guid>https://arxiv.org/abs/2504.12633</guid>
<content:encoded><![CDATA[
arXiv:2504.12633v1 Announce Type: new 
Abstract: Large Language Models (LLMs) not only have solved complex reasoning problems but also exhibit remarkable performance in tasks that require subjective decision making. Existing studies suggest that LLM generations can be subjectively grounded to some extent, yet exploring whether LLMs can account for individual-level subjectivity has not been sufficiently studied. In this paper, we characterize subjectivity of individuals on social media and infer their moral judgments using LLMs. We propose a framework, SOLAR (Subjective Ground with Value Abstraction), that observes value conflicts and trade-offs in the user-generated texts to better represent subjective ground of individuals. Empirical results show that our framework improves overall inference results as well as performance on controversial situations. Additionally, we qualitatively show that SOLAR provides explanations about individuals' value preferences, which can further account for their judgments.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation</title>
<link>https://arxiv.org/abs/2504.12637</link>
<guid>https://arxiv.org/abs/2504.12637</guid>
<content:encoded><![CDATA[
arXiv:2504.12637v1 Announce Type: new 
Abstract: Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. Through a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment</title>
<link>https://arxiv.org/abs/2504.12663</link>
<guid>https://arxiv.org/abs/2504.12663</guid>
<content:encoded><![CDATA[
arXiv:2504.12663v1 Announce Type: new 
Abstract: Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models</title>
<link>https://arxiv.org/abs/2504.12673</link>
<guid>https://arxiv.org/abs/2504.12673</guid>
<content:encoded><![CDATA[
arXiv:2504.12673v1 Announce Type: new 
Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However,retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language modelbased compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy-reducing documents, making it highly useful in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs</title>
<link>https://arxiv.org/abs/2504.12681</link>
<guid>https://arxiv.org/abs/2504.12681</guid>
<content:encoded><![CDATA[
arXiv:2504.12681v1 Announce Type: new 
Abstract: Large Language Models (LLMs) trained on extensive datasets often learn sensitive information, which raises significant social and legal concerns under principles such as the "Right to be forgotten." Retraining entire models from scratch to remove undesired information is both costly and impractical. Furthermore, existing single-domain unlearning methods fail to address multi-domain scenarios, where knowledge is interwoven across domains such as privacy and copyright, creating overlapping representations that lead to excessive knowledge removal or degraded performance. To tackle these issues, we propose GRAIL (GRadient-based AdaptIve unLearning), a novel multi-domain unlearning framework. GRAIL leverages gradient information from multiple domains to precisely distinguish the unlearning scope from the retention scope, and applies an adaptive parameter-wise localization strategy to selectively remove targeted knowledge while preserving critical parameters for each domain. Experimental results on unlearning benchmarks show that GRAIL achieves unlearning success on par with the existing approaches, while also demonstrating up to 17% stronger knowledge retention success compared to the previous state-of-art method. Our findings establish a new paradigm for effectively managing and regulating sensitive information in large-scale pre-trained language models.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-efficient LLM Fine-tuning for Code Generation</title>
<link>https://arxiv.org/abs/2504.12687</link>
<guid>https://arxiv.org/abs/2504.12687</guid>
<content:encoded><![CDATA[
arXiv:2504.12687v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated significant potential in code generation tasks. However, there remains a performance gap between open-source and closed-source models. To address this gap, existing approaches typically generate large amounts of synthetic data for fine-tuning, which often leads to inefficient training. In this work, we propose a data selection strategy in order to improve the effectiveness and efficiency of training for code-based LLMs. By prioritizing data complexity and ensuring that the sampled subset aligns with the distribution of the original dataset, our sampling strategy effectively selects high-quality data. Additionally, we optimize the tokenization process through a "dynamic pack" technique, which minimizes padding tokens and reduces computational resource consumption. Experimental results show that when training on 40% of the OSS-Instruct dataset, the DeepSeek-Coder-Base-6.7B model achieves an average performance of 66.9%, surpassing the 66.1% performance with the full dataset. Moreover, training time is reduced from 47 minutes to 34 minutes, and the peak GPU memory decreases from 61.47 GB to 42.72 GB during a single epoch. Similar improvements are observed with the CodeLlama-Python-7B model on the Evol-Instruct dataset. By optimizing both data selection and tokenization, our approach not only improves model performance but also improves training efficiency.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations</title>
<link>https://arxiv.org/abs/2504.12691</link>
<guid>https://arxiv.org/abs/2504.12691</guid>
<content:encoded><![CDATA[
arXiv:2504.12691v1 Announce Type: new 
Abstract: Large language models (LLMs) frequently generate hallucinations-content that deviates from factual accuracy or provided context-posing challenges for diagnosis due to the complex interplay of underlying causes. This paper introduces a subsequence association framework to systematically trace and understand hallucinations. Our key insight is that hallucinations arise when dominant hallucinatory associations outweigh faithful ones. Through theoretical and empirical analyses, we demonstrate that decoder-only transformers effectively function as subsequence embedding models, with linear layers encoding input-output associations. We propose a tracing algorithm that identifies causal subsequences by analyzing hallucination probabilities across randomized input contexts. Experiments show our method outperforms standard attribution techniques in identifying hallucination causes and aligns with evidence from the model's training corpus. This work provides a unified perspective on hallucinations and a robust framework for their tracing and analysis.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KODIS: A Multicultural Dispute Resolution Dialogue Corpus</title>
<link>https://arxiv.org/abs/2504.12723</link>
<guid>https://arxiv.org/abs/2504.12723</guid>
<content:encoded><![CDATA[
arXiv:2504.12723v1 Announce Type: new 
Abstract: We present KODIS, a dyadic dispute resolution corpus containing thousands of dialogues from over 75 countries. Motivated by a theoretical model of culture and conflict, participants engage in a typical customer service dispute designed by experts to evoke strong emotions and conflict. The corpus contains a rich set of dispositional, process, and outcome measures. The initial analysis supports theories of how anger expressions lead to escalatory spirals and highlights cultural differences in emotional expression. We make this corpus and data collection framework available to the community.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge</title>
<link>https://arxiv.org/abs/2504.12734</link>
<guid>https://arxiv.org/abs/2504.12734</guid>
<content:encoded><![CDATA[
arXiv:2504.12734v1 Announce Type: new 
Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions (NLQs) by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods either rely on employing task-specific strategies or custom-defined representations, which struggle to leverage the knowledge transfer between different SKR tasks or align with the prior of LLMs, thereby limiting their performance. This paper proposes a novel USKR framework named \textsc{Pandora}, which takes advantage of \textsc{Python}'s \textsc{Pandas} API to construct a unified knowledge representation for alignment with LLM pre-training. It employs an LLM to generate textual reasoning steps and executable Python code for each question. Demonstrations are drawn from a memory of training examples that cover various SKR tasks, facilitating knowledge transfer. Extensive experiments on four benchmarks involving three SKR tasks demonstrate that \textsc{Pandora} outperforms existing unified frameworks and competes effectively with task-specific methods.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chinese-Vicuna: A Chinese Instruction-following Llama-based Model</title>
<link>https://arxiv.org/abs/2504.12737</link>
<guid>https://arxiv.org/abs/2504.12737</guid>
<content:encoded><![CDATA[
arXiv:2504.12737v1 Announce Type: new 
Abstract: Chinese-Vicuna is an open-source, resource-efficient language model designed to bridge the gap in Chinese instruction-following capabilities by fine-tuning Meta's LLaMA architecture using Low-Rank Adaptation (LoRA). Targeting low-resource environments, it enables cost-effective deployment on consumer GPUs (e.g., RTX-2080Ti for 7B models) and supports domain-specific adaptation in fields like healthcare and law. By integrating hybrid datasets (BELLE and Guanaco) and 4-bit quantization (QLoRA), the model achieves competitive performance in tasks such as translation, code generation, and domain-specific Q\&amp;A. The project provides a comprehensive toolkit for model conversion, CPU inference, and multi-turn dialogue interfaces, emphasizing accessibility for researchers and developers. Evaluations indicate competitive performance across medical tasks, multi-turn dialogue coherence, and real-time legal updates. Chinese-Vicuna's modular design, open-source ecosystem, and community-driven enhancements position it as a versatile foundation for Chinese LLM applications.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts</title>
<link>https://arxiv.org/abs/2504.12767</link>
<guid>https://arxiv.org/abs/2504.12767</guid>
<content:encoded><![CDATA[
arXiv:2504.12767v1 Announce Type: new 
Abstract: We know that language models (LMs) form biases and stereotypes of minorities, leading to unfair treatments of members of these groups, thanks to research mainly in the US and the broader English-speaking world. As the negative behavior of these models has severe consequences for society and individuals, industry and academia are actively developing methods to reduce the bias in LMs. However, there are many under-represented groups and languages that have been overlooked so far. This includes marginalized groups that are specific to individual countries and regions in the English speaking and Western world, but crucially also almost all marginalized groups in the rest of the world. The UN estimates, that between 600 million to 1.2 billion people worldwide are members of marginalized groups and in need for special protection. If we want to develop inclusive LMs that work for everyone, we have to broaden our understanding to include overlooked marginalized groups and low-resource languages and dialects.
  In this work, we contribute to this effort with the first study investigating offensive stereotyping bias in 23 LMs for 270 marginalized groups from Egypt, the remaining 21 Arab countries, Germany, the UK, and the US. Additionally, we investigate the impact of low-resource languages and dialects on the study of bias in LMs, demonstrating the limitations of current bias metrics, as we measure significantly higher bias when using the Egyptian Arabic dialect versus Modern Standard Arabic. Our results show, LMs indeed show higher bias against many marginalized groups in comparison to dominant groups. However, this is not the case for Arabic LMs, where the bias is high against both marginalized and dominant groups in relation to religion and ethnicity.
  Our results also show higher intersectional bias against Non-binary, LGBTQIA+ and Black women.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration</title>
<link>https://arxiv.org/abs/2504.12773</link>
<guid>https://arxiv.org/abs/2504.12773</guid>
<content:encoded><![CDATA[
arXiv:2504.12773v1 Announce Type: new 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have achieved remarkable progress in general domains and demonstrated promise in multimodal mathematical reasoning. However, applying MLLMs to geometry problem solving (GPS) remains challenging due to lack of accurate step-by-step solution data and severe hallucinations during reasoning. In this paper, we propose GeoGen, a pipeline that can automatically generates step-wise reasoning paths for geometry diagrams. By leveraging the precise symbolic reasoning, \textbf{GeoGen} produces large-scale, high-quality question-answer pairs. To further enhance the logical reasoning ability of MLLMs, we train \textbf{GeoLogic}, a Large Language Model (LLM) using synthetic data generated by GeoGen. Serving as a bridge between natural language and symbolic systems, GeoLogic enables symbolic tools to help verifying MLLM outputs, making the reasoning process more rigorous and alleviating hallucinations. Experimental results show that our approach consistently improves the performance of MLLMs, achieving remarkable results on benchmarks for geometric reasoning tasks. This improvement stems from our integration of the strengths of LLMs and symbolic systems, which enables a more reliable and interpretable approach for the GPS task. Codes are available at https://github.com/ycpNotFound/GeoGen.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation</title>
<link>https://arxiv.org/abs/2504.12805</link>
<guid>https://arxiv.org/abs/2504.12805</guid>
<content:encoded><![CDATA[
arXiv:2504.12805v1 Announce Type: new 
Abstract: This study explored how large language models (LLMs) perform in two areas related to art: writing critiques of artworks and reasoning about mental states (Theory of Mind, or ToM) in art-related situations. For the critique generation part, we built a system that combines Noel Carroll's evaluative framework with a broad selection of art criticism theories. The model was prompted to first write a full-length critique and then shorter, more coherent versions using a step-by-step prompting process. These AI-generated critiques were then compared with those written by human experts in a Turing test-style evaluation. In many cases, human subjects had difficulty telling which was which, and the results suggest that LLMs can produce critiques that are not only plausible in style but also rich in interpretation, as long as they are carefully guided. In the second part, we introduced new simple ToM tasks based on situations involving interpretation, emotion, and moral tension, which can appear in the context of art. These go beyond standard false-belief tests and allow for more complex, socially embedded forms of reasoning. We tested 41 recent LLMs and found that their performance varied across tasks and models. In particular, tasks that involved affective or ambiguous situations tended to reveal clearer differences. Taken together, these results help clarify how LLMs respond to complex interpretative challenges, revealing both their cognitive limitations and potential. While our findings do not directly contradict the so-called Generative AI Paradox--the idea that LLMs can produce expert-like output without genuine understanding--they suggest that, depending on how LLMs are instructed, such as through carefully designed prompts, these models may begin to show behaviors that resemble understanding more closely than we might assume.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMARTe: Slot-based Method for Accountable Relational Triple extraction</title>
<link>https://arxiv.org/abs/2504.12816</link>
<guid>https://arxiv.org/abs/2504.12816</guid>
<content:encoded><![CDATA[
arXiv:2504.12816v1 Announce Type: new 
Abstract: Relational Triple Extraction (RTE) is a fundamental task in Natural Language Processing (NLP). However, prior research has primarily focused on optimizing model performance, with limited efforts to understand the internal mechanisms driving these models. Many existing methods rely on complex preprocessing to induce specific interactions, often resulting in opaque systems that may not fully align with their theoretical foundations. To address these limitations, we propose SMARTe: a Slot-based Method for Accountable Relational Triple extraction. SMARTe introduces intrinsic interpretability through a slot attention mechanism and frames the task as a set prediction problem. Slot attention consolidates relevant information into distinct slots, ensuring all predictions can be explicitly traced to learned slot representations and the tokens contributing to each predicted relational triple. While emphasizing interpretability, SMARTe achieves performance comparable to state-of-the-art models. Evaluations on the NYT and WebNLG datasets demonstrate that adding interpretability does not compromise performance. Furthermore, we conducted qualitative assessments to showcase the explanations provided by SMARTe, using attention heatmaps that map to their respective tokens. We conclude with a discussion of our findings and propose directions for future research.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks</title>
<link>https://arxiv.org/abs/2504.12845</link>
<guid>https://arxiv.org/abs/2504.12845</guid>
<content:encoded><![CDATA[
arXiv:2504.12845v1 Announce Type: new 
Abstract: Existing multilingual long-context benchmarks, often based on the popular needle-in-a-haystack test, primarily evaluate a model's ability to locate specific information buried within irrelevant texts. However, such a retrieval-centric approach is myopic and inherently limited, as successful recall alone does not indicate a model's capacity to reason over extended contexts. Moreover, these benchmarks are susceptible to data leakage, short-circuiting, and risk making the evaluation a priori identifiable. To address these limitations, we introduce MLRBench, a new synthetic benchmark for multilingual long-context reasoning. Unlike existing benchmarks, MLRBench goes beyond surface-level retrieval by including tasks that assess multi-hop inference, aggregation, and epistemic reasoning. Spanning seven languages, MLRBench is designed to be parallel, resistant to leakage, and scalable to arbitrary context lengths. Our extensive experiments with an open-weight large language model (LLM) reveal a pronounced gap between high- and low-resource languages, particularly for tasks requiring the model to aggregate multiple facts or predict the absence of information. We also find that, in multilingual settings, LLMs effectively utilize less than 30% of their claimed context length. Although off-the-shelf Retrieval Augmented Generation helps alleviate this to a certain extent, it does not solve the long-context problem. We open-source MLRBench to enable future research in improved evaluation and training of multilingual LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos</title>
<link>https://arxiv.org/abs/2504.12882</link>
<guid>https://arxiv.org/abs/2504.12882</guid>
<content:encoded><![CDATA[
arXiv:2504.12882v1 Announce Type: new 
Abstract: The growing influence of video content as a medium for communication and misinformation underscores the urgent need for effective tools to analyze claims in multilingual and multi-topic settings. Existing efforts in misinformation detection largely focus on written text, leaving a significant gap in addressing the complexity of spoken text in video transcripts. We introduce ViClaim, a dataset of 1,798 annotated video transcripts across three languages (English, German, Spanish) and six topics. Each sentence in the transcripts is labeled with three claim-related categories: fact-check-worthy, fact-non-check-worthy, or opinion. We developed a custom annotation tool to facilitate the highly complex annotation process. Experiments with state-of-the-art multilingual language models demonstrate strong performance in cross-validation (macro F1 up to 0.896) but reveal challenges in generalization to unseen topics, particularly for distinct domains. Our findings highlight the complexity of claim detection in video transcripts. ViClaim offers a robust foundation for advancing misinformation detection in video-based communication, addressing a critical gap in multimodal analysis.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication</title>
<link>https://arxiv.org/abs/2504.12891</link>
<guid>https://arxiv.org/abs/2504.12891</guid>
<content:encoded><![CDATA[
arXiv:2504.12891v1 Announce Type: new 
Abstract: The rapid evolution of artificial intelligence (AI) has introduced AI agents as a disruptive paradigm across various industries, yet their application in machine translation (MT) remains underexplored. This paper describes and analyses the potential of single- and multi-agent systems for MT, reflecting on how they could enhance multilingual digital communication. While single-agent systems are well-suited for simpler translation tasks, multi-agent systems, which involve multiple specialized AI agents collaborating in a structured manner, may offer a promising solution for complex scenarios requiring high accuracy, domain-specific knowledge, and contextual awareness. To demonstrate the feasibility of multi-agent workflows in MT, we are conducting a pilot study in legal MT. The study employs a multi-agent system involving four specialized AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and (iv) final editing. Our findings suggest that multi-agent systems may have the potential to significantly improve domain-adaptability and contextual awareness, with superior translation quality to traditional MT or single-agent systems. This paper also sets the stage for future research into multi-agent applications in MT, integration into professional translation workflows, and shares a demo of the system analyzed in the paper.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models</title>
<link>https://arxiv.org/abs/2504.12898</link>
<guid>https://arxiv.org/abs/2504.12898</guid>
<content:encoded><![CDATA[
arXiv:2504.12898v1 Announce Type: new 
Abstract: Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset biases and the insufficient nature of bias suppression based on in-context learning, the effectiveness of previous prior knowledge-based debiasing methods and in-context learning based automatic debiasing methods is limited. To address these challenges, we explore the combination of causal mechanisms with information theory and propose an information gain-guided causal intervention debiasing (IGCIDB) framework. This framework first utilizes an information gain-guided causal intervention method to automatically and autonomously balance the distribution of instruction-tuning dataset. Subsequently, it employs a standard supervised fine-tuning process to train LLMs on the debiased dataset. Experimental results show that IGCIDB can effectively debias LLM to improve its generalizability across different tasks.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Multi-National Value Alignment for Large Language Models</title>
<link>https://arxiv.org/abs/2504.12911</link>
<guid>https://arxiv.org/abs/2504.12911</guid>
<content:encoded><![CDATA[
arXiv:2504.12911v1 Announce Type: new 
Abstract: Do Large Language Models (LLMs) hold positions that conflict with your country's values? Occasionally they do! However, existing works primarily focus on ethical reviews, failing to capture the diversity of national values, which encompass broader policy, legal, and moral considerations. Furthermore, current benchmarks that rely on spectrum tests using manually designed questionnaires are not easily scalable.
  To address these limitations, we introduce NaVAB, a comprehensive benchmark to evaluate the alignment of LLMs with the values of five major nations: China, the United States, the United Kingdom, France, and Germany. NaVAB implements a national value extraction pipeline to efficiently construct value assessment datasets. Specifically, we propose a modeling procedure with instruction tagging to process raw data sources, a screening process to filter value-related topics and a generation process with a Conflict Reduction mechanism to filter non-conflicting values.We conduct extensive experiments on various LLMs across countries, and the results provide insights into assisting in the identification of misaligned scenarios. Moreover, we demonstrate that NaVAB can be combined with alignment techniques to effectively reduce value concerns by aligning LLMs' values with the target country.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAIN: Mutual Alignment Is Necessary for instruction tuning</title>
<link>https://arxiv.org/abs/2504.12913</link>
<guid>https://arxiv.org/abs/2504.12913</guid>
<content:encoded><![CDATA[
arXiv:2504.12913v1 Announce Type: new 
Abstract: Instruction tuning has enabled large language models (LLMs) to achieve remarkable performance, but its success heavily depends on the availability of large-scale, high-quality instruction-response pairs. However, current methods for scaling up data generation often overlook a crucial aspect: the alignment between instructions and responses. We hypothesize that high-quality instruction-response pairs are not defined by the individual quality of each component, but by the extent of their alignment with each other. To address this, we propose a Mutual Alignment Framework (MAIN) that ensures coherence between the instruction and response through mutual constraints. Experiments demonstrate that models such as LLaMA and Mistral, fine-tuned within this framework, outperform traditional methods across multiple benchmarks. This approach underscores the critical role of instruction-response alignment in enabling scalable and high-quality instruction tuning for LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConExion: Concept Extraction with Large Language Models</title>
<link>https://arxiv.org/abs/2504.12915</link>
<guid>https://arxiv.org/abs/2504.12915</guid>
<content:encoded><![CDATA[
arXiv:2504.12915v1 Announce Type: new 
Abstract: In this paper, an approach for concept extraction from documents using pre-trained large language models (LLMs) is presented. Compared with conventional methods that extract keyphrases summarizing the important information discussed in a document, our approach tackles a more challenging task of extracting all present concepts related to the specific domain, not just the important ones. Through comprehensive evaluations of two widely used benchmark datasets, we demonstrate that our method improves the F1 score compared to state-of-the-art techniques. Additionally, we explore the potential of using prompts within these models for unsupervised concept extraction. The extracted concepts are intended to support domain coverage evaluation of ontologies and facilitate ontology learning, highlighting the effectiveness of LLMs in concept extraction tasks. Our source code and datasets are publicly available at https://github.com/ISE-FIZKarlsruhe/concept_extraction.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback</title>
<link>https://arxiv.org/abs/2504.12951</link>
<guid>https://arxiv.org/abs/2504.12951</guid>
<content:encoded><![CDATA[
arXiv:2504.12951v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have catalyzed the development of general-purpose autonomous agents, demonstrating remarkable performance in complex reasoning tasks across various domains. This surge has spurred the evolution of a plethora of prompt-based reasoning frameworks. A recent focus has been on iterative reasoning strategies that refine outputs through self-evaluation and verbalized feedback. However, these strategies require additional computational complexity to enable models to recognize and correct their mistakes, leading to a significant increase in their cost. In this work, we introduce the concept of ``retrials without feedback'', an embarrassingly simple yet powerful mechanism for enhancing reasoning frameworks by allowing LLMs to retry problem-solving attempts upon identifying incorrect answers. Unlike conventional iterative refinement methods, our method does not require explicit self-reflection or verbalized feedback, simplifying the refinement process. Our findings indicate that simpler retrial-based approaches often outperform more sophisticated reasoning frameworks, suggesting that the benefits of complex methods may not always justify their computational costs. By challenging the prevailing assumption that more intricate reasoning strategies inherently lead to better performance, our work offers new insights into how simpler, more efficient approaches can achieve optimal results. So, are retrials all you need?
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization</title>
<link>https://arxiv.org/abs/2504.12972</link>
<guid>https://arxiv.org/abs/2504.12972</guid>
<content:encoded><![CDATA[
arXiv:2504.12972v1 Announce Type: new 
Abstract: Recent advances in long-context reasoning abilities of language models led to interesting applications in large-scale multi-document summarization. However, prior work has shown that these long-context models are not effective at their claimed context windows. To this end, retrieval-augmented systems provide an efficient and effective alternative. However, their performance can be highly sensitive to the choice of retrieval context length. In this work, we present a hybrid method that combines retrieval-augmented systems with long-context windows supported by recent language models. Our method first estimates the optimal retrieval length as a function of the retriever, summarizer, and dataset. On a randomly sampled subset of the dataset, we use a panel of LLMs to generate a pool of silver references. We use these silver references to estimate the optimal context length for a given RAG system configuration. Our results on the multi-document summarization task showcase the effectiveness of our method across model classes and sizes. We compare against length estimates from strong long-context benchmarks such as RULER and HELMET. Our analysis also highlights the effectiveness of our estimation method for very long-context LMs and its generalization to new classes of LMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparks of Science: Hypothesis Generation Using Structured Paper Data</title>
<link>https://arxiv.org/abs/2504.12976</link>
<guid>https://arxiv.org/abs/2504.12976</guid>
<content:encoded><![CDATA[
arXiv:2504.12976v1 Announce Type: new 
Abstract: Generating novel and creative scientific hypotheses is a cornerstone in achieving Artificial General Intelligence. Large language and reasoning models have the potential to aid in the systematic creation, selection, and validation of scientifically informed hypotheses. However, current foundation models often struggle to produce scientific ideas that are both novel and feasible. One reason is the lack of a dedicated dataset that frames Scientific Hypothesis Generation (SHG) as a Natural Language Generation (NLG) task. In this paper, we introduce HypoGen, the first dataset of approximately 5500 structured problem-hypothesis pairs extracted from top-tier computer science conferences structured with a Bit-Flip-Spark schema, where the Bit is the conventional assumption, the Spark is the key insight or conceptual leap, and the Flip is the resulting counterproposal. HypoGen uniquely integrates an explicit Chain-of-Reasoning component that reflects the intellectual process from Bit to Flip. We demonstrate that framing hypothesis generation as conditional language modelling, with the model fine-tuned on Bit-Flip-Spark and the Chain-of-Reasoning (and where, at inference, we only provide the Bit), leads to improvements in the overall quality of the hypotheses. Our evaluation employs automated metrics and LLM judge rankings for overall quality assessment. We show that by fine-tuning on our HypoGen dataset we improve the novelty, feasibility, and overall quality of the generated hypotheses. The HypoGen dataset is publicly available at huggingface.co/datasets/UniverseTBD/hypogen-dr1.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild</title>
<link>https://arxiv.org/abs/2504.12982</link>
<guid>https://arxiv.org/abs/2504.12982</guid>
<content:encoded><![CDATA[
arXiv:2504.12982v1 Announce Type: new 
Abstract: The proliferation of large language models (LLMs) has significantly advanced information retrieval systems, particularly in response generation (RG). Unfortunately, LLMs often face knowledge conflicts between internal memory and retrievaled external information, arising from misinformation, biases, or outdated knowledge. These conflicts undermine response reliability and introduce uncertainty in decision-making. In this work, we analyze how LLMs navigate knowledge conflicts from an information-theoretic perspective and reveal that when conflicting and supplementary information exhibit significant differences, LLMs confidently resolve their preferences. However, when the distinction is ambiguous, LLMs experience heightened uncertainty. Based on this insight, we propose Swin-VIB, a novel framework that integrates a pipeline of variational information bottleneck models into adaptive augmentation of retrieved information and guiding LLM preference in response generation. Extensive experiments on single-choice, open-ended question-answering (QA), and retrieval augmented generation (RAG) validate our theoretical findings and demonstrate the efficacy of Swin-VIB. Notably, our method improves single-choice task accuracy by at least 7.54\% over competitive baselines.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation</title>
<link>https://arxiv.org/abs/2504.12996</link>
<guid>https://arxiv.org/abs/2504.12996</guid>
<content:encoded><![CDATA[
arXiv:2504.12996v1 Announce Type: new 
Abstract: Large language models (LLMs) frequently memorize sensitive information during training, posing risks when deploying publicly accessible models. Current machine unlearning methods struggle to selectively remove specific data associations without degrading overall model capabilities. This paper presents our solution to SemEval-2025 Task 4 on targeted unlearning, which introduces a two-stage methodology that combines causal mediation analysis with layer-specific optimization. Through systematic causal tracing experiments on OLMo architectures (1B and 7B parameters), we identify the critical role of the first few transformer layers (layers 0-5) in storing subject-attribute associations within MLP modules. Building on this insight, we develop a constrained optimization approach that freezes upper layers while applying a novel joint loss function to lower layers-simultaneously maximizing forget set loss via output token cross-entropy penalties and minimizing retain set deviation through adaptive regularization. Our method achieves 2nd place in the 1B model track, demonstrating strong task performance while maintaining 88% of baseline MMLU accuracy. These results establish causal-informed layer optimization as a promising paradigm for efficient, precise unlearning in LLMs, offering a significant step forward in addressing data privacy concerns in AI systems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images</title>
<link>https://arxiv.org/abs/2504.13023</link>
<guid>https://arxiv.org/abs/2504.13023</guid>
<content:encoded><![CDATA[
arXiv:2504.13023v1 Announce Type: new 
Abstract: Recent studies have made significant progress in developing large language models (LLMs) in the medical domain, which can answer expert-level questions and demonstrate the potential to assist clinicians in real-world clinical scenarios. Studies have also witnessed the importance of integrating various modalities with the existing LLMs for a better understanding of complex clinical contexts, which are innately multi-faceted by nature. Although studies have demonstrated the ability of multimodal LLMs in histopathology to answer questions from given images, they lack in understanding of thorough clinical context due to the patch-level data with limited information from public datasets. Thus, developing WSI-level MLLMs is significant in terms of the scalability and applicability of MLLMs in histopathology. In this study, we introduce an expert-level MLLM for histopathology using WSIs, dubbed as ChatEXAONEPath. We present a retrieval-based data generation pipeline using 10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas (TCGA). We also showcase an AI-based evaluation protocol for a comprehensive understanding of the medical context from given multimodal information and evaluate generated answers compared to the original histopathology reports. We demonstrate the ability of diagnosing the given histopathology images using ChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and reports. Our proposed model can understand pan-cancer WSIs and clinical context from various cancer types. We argue that our proposed model has the potential to assist clinicians by comprehensively understanding complex morphology of WSIs for cancer diagnosis through the integration of multiple modalities.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation</title>
<link>https://arxiv.org/abs/2504.13054</link>
<guid>https://arxiv.org/abs/2504.13054</guid>
<content:encoded><![CDATA[
arXiv:2504.13054v1 Announce Type: new 
Abstract: Aspect-based summarization aims to generate summaries tailored to specific aspects, addressing the resource constraints and limited generalizability of traditional summarization approaches. Recently, large language models have shown promise in this task without the need for training. However, they rely excessively on prompt engineering and face token limits and hallucination challenges, especially with in-context learning. To address these challenges, in this paper, we propose a novel framework for aspect-based summarization: Self-Aspect Retrieval Enhanced Summary Generation. Rather than relying solely on in-context learning, given an aspect, we employ an embedding-driven retrieval mechanism to identify its relevant text segments. This approach extracts the pertinent content while avoiding unnecessary details, thereby mitigating the challenge of token limits. Moreover, our framework optimizes token usage by deleting unrelated parts of the text and ensuring that the model generates output strictly based on the given aspect. With extensive experiments on benchmark datasets, we demonstrate that our framework not only achieves superior performance but also effectively mitigates the token limitation problem.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models</title>
<link>https://arxiv.org/abs/2504.13068</link>
<guid>https://arxiv.org/abs/2504.13068</guid>
<content:encoded><![CDATA[
arXiv:2504.13068v1 Announce Type: new 
Abstract: This study explores the relationship between deep learning (DL) model accuracy and expert agreement in the classification of crash narratives. We evaluate five DL models -- including BERT variants, the Universal Sentence Encoder (USE), and a zero-shot classifier -- against expert-labeled data and narrative text. The analysis is further extended to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our results reveal a counterintuitive trend: models with higher technical accuracy often exhibit lower agreement with domain experts, whereas LLMs demonstrate greater expert alignment despite relatively lower accuracy scores. To quantify and interpret model-expert agreement, we employ Cohen's Kappa, Principal Component Analysis (PCA), and SHAP-based explainability techniques. Findings indicate that expert-aligned models tend to rely more on contextual and temporal language cues, rather than location-specific keywords. These results underscore that accuracy alone is insufficient for evaluating models in safety-critical NLP applications. We advocate for incorporating expert agreement as a complementary metric in model evaluation frameworks and highlight the promise of LLMs as interpretable, scalable tools for crash analysis pipelines.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation with Conflicting Evidence</title>
<link>https://arxiv.org/abs/2504.13079</link>
<guid>https://arxiv.org/abs/2504.13079</guid>
<content:encoded><![CDATA[
arXiv:2504.13079v1 Announce Type: new 
Abstract: Large language model (LLM) agents are increasingly employing retrieval-augmented generation (RAG) to improve the factuality of their responses. However, in practice, these systems often need to handle ambiguous user queries and potentially conflicting information from multiple sources while also suppressing inaccurate information from noisy or irrelevant documents. Prior work has generally studied and addressed these challenges in isolation, considering only one aspect at a time, such as handling ambiguity or robustness to noise and misinformation. We instead consider multiple factors simultaneously, proposing (i) RAMDocs (Retrieval with Ambiguity and Misinformation in Documents), a new dataset that simulates complex and realistic scenarios for conflicting evidence for a user query, including ambiguity, misinformation, and noise; and (ii) MADAM-RAG, a multi-agent approach in which LLM agents debate over the merits of an answer over multiple rounds, allowing an aggregator to collate responses corresponding to disambiguated entities while discarding misinformation and noise, thereby handling diverse sources of conflict jointly. We demonstrate the effectiveness of MADAM-RAG using both closed and open-source models on AmbigDocs -- which requires presenting all valid answers for ambiguous queries -- improving over strong RAG baselines by up to 11.40% and on FaithEval -- which requires suppressing misinformation -- where we improve by up to 15.80% (absolute) with Llama3.3-70B-Instruct. Furthermore, we find that RAMDocs poses a challenge for existing RAG baselines (Llama3.3-70B-Instruct only obtains 32.60 exact match score). While MADAM-RAG begins to address these conflicting factors, our analysis indicates that a substantial gap remains especially when increasing the level of imbalance in supporting evidence and misinformation.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard</title>
<link>https://arxiv.org/abs/2504.13125</link>
<guid>https://arxiv.org/abs/2504.13125</guid>
<content:encoded><![CDATA[
arXiv:2504.13125v1 Announce Type: new 
Abstract: This paper investigates the application of large language models (LLMs) to financial tasks. We fine-tuned foundation models using the Open FinLLM Leaderboard as a benchmark. Building on Qwen2.5 and Deepseek-R1, we employed techniques including supervised fine-tuning (SFT), direct preference optimization (DPO), and reinforcement learning (RL) to enhance their financial capabilities. The fine-tuned models demonstrated substantial performance gains across a wide range of financial tasks. Moreover, we measured the data scaling law in the financial domain. Our work demonstrates the potential of large language models (LLMs) in financial applications.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Based Reward Models for Robust Language Model Alignment</title>
<link>https://arxiv.org/abs/2504.13134</link>
<guid>https://arxiv.org/abs/2504.13134</guid>
<content:encoded><![CDATA[
arXiv:2504.13134v1 Announce Type: new 
Abstract: Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2504.13139</link>
<guid>https://arxiv.org/abs/2504.13139</guid>
<content:encoded><![CDATA[
arXiv:2504.13139v1 Announce Type: new 
Abstract: A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as probabilistic conditioning, but exact generation from the resulting distribution -- which can differ substantially from the LM's base distribution -- is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains -- Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis -- we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8x larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. Our system builds on the framework of Lew et al. (2023) and integrates with its language model probabilistic programming language, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training</title>
<link>https://arxiv.org/abs/2504.13161</link>
<guid>https://arxiv.org/abs/2504.13161</guid>
<content:encoded><![CDATA[
arXiv:2504.13161v1 Announce Type: new 
Abstract: Pre-training datasets are typically collected from web content and lack inherent domain divisions. For instance, widely used datasets like Common Crawl do not include explicit domain labels, while manually curating labeled datasets such as The Pile is labor-intensive. Consequently, identifying an optimal pre-training data mixture remains a challenging problem, despite its significant benefits for pre-training performance. To address these challenges, we propose CLustering-based Iterative Data Mixture Bootstrapping (CLIMB), an automated framework that discovers, evaluates, and refines data mixtures in a pre-training setting. Specifically, CLIMB embeds and clusters large-scale datasets in a semantic space and then iteratively searches for optimal mixtures using a smaller proxy model and a predictor. When continuously trained on 400B tokens with this mixture, our 1B model exceeds the state-of-the-art Llama-3.2-1B by 2.0%. Moreover, we observe that optimizing for a specific domain (e.g., Social Sciences) yields a 5% improvement over random sampling. Finally, we introduce ClimbLab, a filtered 1.2-trillion-token corpus with 20 clusters as a research playground, and ClimbMix, a compact yet powerful 400-billion-token dataset designed for efficient pre-training that delivers superior performance under an equal token budget. We analyze the final data mixture, elucidating the characteristics of an optimal data mixture. Our data is available at: https://research.nvidia.com/labs/lpr/climb/
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective</title>
<link>https://arxiv.org/abs/2504.12309</link>
<guid>https://arxiv.org/abs/2504.12309</guid>
<content:encoded><![CDATA[
arXiv:2504.12309v1 Announce Type: cross 
Abstract: From 2000 to 2015, the UN's Millennium Development Goals guided global priorities. The subsequent Sustainable Development Goals (SDGs) adopted a more dynamic approach, with annual indicator updates. As 2030 nears and progress lags, innovative acceleration strategies are critical. This study develops an AI-powered knowledge graph system to analyze SDG interconnections, discover potential new goals, and visualize them online. Using official SDG texts, Elsevier's keyword dataset, and 1,127 TED Talk transcripts (2020-2023), a pilot on 269 talks from 2023 applies AI-speculative design, large language models, and retrieval-augmented generation. Key findings include: (1) Heatmap analysis reveals strong associations between Goal 10 and Goal 16, and minimal coverage of Goal 6. (2) In the knowledge graph, simulated dialogue over time reveals new central nodes, showing how richer data supports divergent thinking and goal clarity. (3) Six potential new goals are proposed, centered on equity, resilience, and technology-driven inclusion. This speculative-AI framework offers fresh insights for policymakers and lays groundwork for future multimodal and cross-system SDG applications.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Specialized text classification: an approach to classifying Open Banking transactions</title>
<link>https://arxiv.org/abs/2504.12319</link>
<guid>https://arxiv.org/abs/2504.12319</guid>
<content:encoded><![CDATA[
arXiv:2504.12319v1 Announce Type: cross 
Abstract: With the introduction of the PSD2 regulation in the EU which established the Open Banking framework, a new window of opportunities has opened for banks and fintechs to explore and enrich Bank transaction descriptions with the aim of building a better understanding of customer behavior, while using this understanding to prevent fraud, reduce risks and offer more competitive and tailored services.
  And although the usage of natural language processing models and techniques has seen an incredible progress in various applications and domains over the past few years, custom applications based on domain-specific text corpus remain unaddressed especially in the banking sector.
  In this paper, we introduce a language-based Open Banking transaction classification system with a focus on the french market and french language text. The system encompasses data collection, labeling, preprocessing, modeling, and evaluation stages. Unlike previous studies that focus on general classification approaches, this system is specifically tailored to address the challenges posed by training a language model with a specialized text corpus (Banking data in the French context). By incorporating language-specific techniques and domain knowledge, the proposed system demonstrates enhanced performance and efficiency compared to generic approaches.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment</title>
<link>https://arxiv.org/abs/2504.12408</link>
<guid>https://arxiv.org/abs/2504.12408</guid>
<content:encoded><![CDATA[
arXiv:2504.12408v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to automate relevance judgments for information retrieval (IR) tasks, often demonstrating agreement with human labels that approaches inter-human agreement. To assess the robustness and reliability of LLM-based relevance judgments, we systematically investigate impact of prompt sensitivity on the task. We collected prompts for relevance assessment from 15 human experts and 15 LLMs across three tasks~ -- ~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After filtering out unusable prompts from three humans and three LLMs, we employed the remaining 72 prompts with three different LLMs as judges to label document/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We compare LLM-generated labels with TREC official human labels using Cohen's $\kappa$ and pairwise agreement measures. In addition to investigating the impact of prompt variations on agreement with human labels, we compare human- and LLM-generated prompts and analyze differences among different LLMs as judges. We also compare human- and LLM-generated prompts with the standard UMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval Augmented Generation (RAG) Track. To support future research in LLM-based evaluation, we release all data and prompts at https://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Conversational AI for Human-Machine Collaborative MLOps</title>
<link>https://arxiv.org/abs/2504.12477</link>
<guid>https://arxiv.org/abs/2504.12477</guid>
<content:encoded><![CDATA[
arXiv:2504.12477v1 Announce Type: cross 
Abstract: This paper presents a Large Language Model (LLM) based conversational agent system designed to enhance human-machine collaboration in Machine Learning Operations (MLOps). We introduce the Swarm Agent, an extensible architecture that integrates specialized agents to create and manage ML workflows through natural language interactions. The system leverages a hierarchical, modular design incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline orchestration, a MinIO Agent for data management, and a Retrieval-Augmented Generation (RAG) Agent for domain-specific knowledge integration. Through iterative reasoning loops and context-aware processing, the system enables users with varying technical backgrounds to discover, execute, and monitor ML pipelines; manage datasets and artifacts; and access relevant documentation, all via intuitive conversational interfaces. Our approach addresses the accessibility gap in complex MLOps platforms like Kubeflow, making advanced ML tools broadly accessible while maintaining the flexibility to extend to other platforms. The paper describes the architecture, implementation details, and demonstrates how this conversational MLOps assistant reduces complexity and lowers barriers to entry for users across diverse technical skill levels.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice</title>
<link>https://arxiv.org/abs/2504.12545</link>
<guid>https://arxiv.org/abs/2504.12545</guid>
<content:encoded><![CDATA[
arXiv:2504.12545v1 Announce Type: cross 
Abstract: Mass-shooting events pose a significant challenge to public safety, generating large volumes of unstructured textual data that hinder effective investigations and the formulation of public policy. Despite the urgency, few prior studies have effectively automated the extraction of key information from these events to support legal and investigative efforts. This paper presented the first dataset designed for knowledge acquisition on mass-shooting events through the application of named entity recognition (NER) techniques. It focuses on identifying key entities such as offenders, victims, locations, and criminal instruments, that are vital for legal and investigative purposes. The NER process is powered by Large Language Models (LLMs) using few-shot prompting, facilitating the efficient extraction and organization of critical information from diverse sources, including news articles, police reports, and social media. Experimental results on real-world mass-shooting corpora demonstrate that GPT-4o is the most effective model for mass-shooting NER, achieving the highest Micro Precision, Micro Recall, and Micro F1-scores. Meanwhile, o1-mini delivers competitive performance, making it a resource-efficient alternative for less complex NER tasks. It is also observed that increasing the shot count enhances the performance of all models, but the gains are more substantial for GPT-4o and o1-mini, highlighting their superior adaptability to few-shot learning scenarios.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLM-based Relevance Judgment Methods</title>
<link>https://arxiv.org/abs/2504.12558</link>
<guid>https://arxiv.org/abs/2504.12558</guid>
<content:encoded><![CDATA[
arXiv:2504.12558v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in both academic and industry settings to automate the evaluation of information seeking systems, particularly by generating graded relevance judgments. Previous work on LLM-based relevance assessment has primarily focused on replicating graded human relevance judgments through various prompting strategies. However, there has been limited exploration of alternative assessment methods or comprehensive comparative studies. In this paper, we systematically compare multiple LLM-based relevance assessment methods, including binary relevance judgments, graded relevance assessments, pairwise preference-based methods, and two nugget-based evaluation methods~--~document-agnostic and document-dependent. In addition to a traditional comparison based on system rankings using Kendall correlations, we also examine how well LLM judgments align with human preferences, as inferred from relevance grades. We conduct extensive experiments on datasets from three TREC Deep Learning tracks 2019, 2020 and 2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain question answering. As part of our data release, we include relevance judgments generated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model. Our goal is to \textit{reproduce} various LLM-based relevance judgment methods to provide a comprehensive comparison. All code, data, and resources are publicly available in our GitHub Repository at https://github.com/Narabzad/llm-relevance-judgement-comparison.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition</title>
<link>https://arxiv.org/abs/2504.12562</link>
<guid>https://arxiv.org/abs/2504.12562</guid>
<content:encoded><![CDATA[
arXiv:2504.12562v1 Announce Type: cross 
Abstract: Evaluating the capabilities of Large Language Models (LLMs) has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations - methods that often suffer from overfitting, high costs, and biases. ZeroSumEval is a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (PyJail), classic games (Chess, Liar's Dice, Poker), knowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, and creativity. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework. To demonstrate this, we conduct extensive experiments with >7000 simulations across 7 games and 13 models. Our results show that while frontier models from the GPT and Claude families can play common games and answer questions, they struggle to play games that require creating novel and challenging questions. We also observe that models cannot reliably jailbreak each other and fail generally at tasks requiring creativity. We release our code at https://github.com/facebookresearch/ZeroSumEval.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provable Secure Steganography Based on Adaptive Dynamic Sampling</title>
<link>https://arxiv.org/abs/2504.12579</link>
<guid>https://arxiv.org/abs/2504.12579</guid>
<content:encoded><![CDATA[
arXiv:2504.12579v1 Announce Type: cross 
Abstract: The security of private communication is increasingly at risk due to widespread surveillance. Steganography, a technique for embedding secret messages within innocuous carriers, enables covert communication over monitored channels. Provably Secure Steganography (PSS) is state of the art for making stego carriers indistinguishable from normal ones by ensuring computational indistinguishability between stego and cover distributions. However, current PSS methods often require explicit access to the distribution of generative model for both sender and receiver, limiting their practicality in black box scenarios. In this paper, we propose a provably secure steganography scheme that does not require access to explicit model distributions for both sender and receiver. Our method incorporates a dynamic sampling strategy, enabling generative models to embed secret messages within multiple sampling choices without disrupting the normal generation process of the model. Extensive evaluations of three real world datasets and three LLMs demonstrate that our blackbox method is comparable with existing white-box steganography methods in terms of efficiency and capacity while eliminating the degradation of steganography in model generated outputs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simplifying Graph Transformers</title>
<link>https://arxiv.org/abs/2504.12588</link>
<guid>https://arxiv.org/abs/2504.12588</guid>
<content:encoded><![CDATA[
arXiv:2504.12588v1 Announce Type: cross 
Abstract: Transformers have attained outstanding performance across various modalities, employing scaled-dot-product (SDP) attention mechanisms. Researchers have attempted to migrate Transformers to graph learning, but most advanced Graph Transformers are designed with major architectural differences, either integrating message-passing or incorporating sophisticated attention mechanisms. These complexities prevent the easy adoption of Transformer training advances. We propose three simple modifications to the plain Transformer to render it applicable to graphs without introducing major architectural distortions. Specifically, we advocate for the use of (1) simplified $L_2$ attention to measure the magnitude closeness of tokens; (2) adaptive root-mean-square normalization to preserve token magnitude information; and (3) a relative positional encoding bias with a shared encoder. Significant performance gains across a variety of graph datasets justify the effectiveness of our proposed modifications. Furthermore, empirical evaluation on the expressiveness benchmark reveals noteworthy realized expressiveness in the graph isomorphism.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization</title>
<link>https://arxiv.org/abs/2504.12661</link>
<guid>https://arxiv.org/abs/2504.12661</guid>
<content:encoded><![CDATA[
arXiv:2504.12661v1 Announce Type: cross 
Abstract: Aligning Vision-Language Models (VLMs) with safety standards is essential to mitigate risks arising from their multimodal complexity, where integrating vision and language unveils subtle threats beyond the reach of conventional safeguards. Inspired by the insight that reasoning across modalities is key to preempting intricate vulnerabilities, we propose a novel direction for VLM safety: multimodal reasoning-driven prompt rewriting. To this end, we introduce VLMGuard-R1, a proactive framework that refines user inputs through a reasoning-guided rewriter, dynamically interpreting text-image interactions to deliver refined prompts that bolster safety across diverse VLM architectures without altering their core parameters. To achieve this, we devise a three-stage reasoning pipeline to synthesize a dataset that trains the rewriter to infer subtle threats, enabling tailored, actionable responses over generic refusals. Extensive experiments across three benchmarks with five VLMs reveal that VLMGuard-R1 outperforms four baselines. In particular, VLMGuard-R1 achieves a remarkable 43.59\% increase in average safety across five models on the SIUO benchmark.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents</title>
<link>https://arxiv.org/abs/2504.12682</link>
<guid>https://arxiv.org/abs/2504.12682</guid>
<content:encoded><![CDATA[
arXiv:2504.12682v1 Announce Type: cross 
Abstract: Most recent web agent research has focused on navigation and transaction tasks, with little emphasis on extracting structured data at scale. We present WebLists, a benchmark of 200 data-extraction tasks across four common business and enterprise use-cases. Each task requires an agent to navigate to a webpage, configure it appropriately, and extract complete datasets with well-defined schemas. We show that both LLMs with search capabilities and SOTA web agents struggle with these tasks, with a recall of 3% and 31%, respectively, despite higher performance on question-answering tasks.
  To address this challenge, we propose BardeenAgent, a novel framework that enables web agents to convert their execution into repeatable programs, and replay them at scale across pages with similar structure. BardeenAgent is also the first LLM agent to take advantage of the regular structure of HTML. In particular BardeenAgent constructs a generalizable CSS selector to capture all relevant items on the page, then fits the operations to extract the data.
  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more than doubling the performance of SOTA web agents, and reducing cost per output row by 3x.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Lossless Token Pruning in Late-Interaction Retrieval Models</title>
<link>https://arxiv.org/abs/2504.12778</link>
<guid>https://arxiv.org/abs/2504.12778</guid>
<content:encoded><![CDATA[
arXiv:2504.12778v1 Announce Type: cross 
Abstract: Late interaction neural IR models like ColBERT offer a competitive effectiveness-efficiency trade-off across many benchmarks. However, they require a huge memory space to store the contextual representation for all the document tokens. Some works have proposed using either heuristics or statistical-based techniques to prune tokens from each document. This however doesn't guarantee that the removed tokens have no impact on the retrieval score. Our work uses a principled approach to define how to prune tokens without impacting the score between a document and a query. We introduce three regularization losses, that induce a solution with high pruning ratios, as well as two pruning strategies. We study them experimentally (in and out-domain), showing that we can preserve ColBERT's performance while using only 30\% of the tokens.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting</title>
<link>https://arxiv.org/abs/2504.12867</link>
<guid>https://arxiv.org/abs/2504.12867</guid>
<content:encoded><![CDATA[
arXiv:2504.12867v1 Announce Type: cross 
Abstract: Human speech goes beyond the mere transfer of information; it is a profound exchange of emotions and a connection between individuals. While Text-to-Speech (TTS) models have made huge progress, they still face challenges in controlling the emotional expression in the generated speech. In this work, we propose EmoVoice, a novel emotion-controllable TTS model that exploits large language models (LLMs) to enable fine-grained freestyle natural language emotion control, and a phoneme boost variant design that makes the model output phoneme tokens and audio tokens in parallel to enhance content consistency, inspired by chain-of-thought (CoT) and modality-of-thought (CoM) techniques. Besides, we introduce EmoVoice-DB, a high-quality 40-hour English emotion dataset featuring expressive speech and fine-grained emotion labels with natural language descriptions. EmoVoice achieves state-of-the-art performance on the English EmoVoice-DB test set using only synthetic training data, and on the Chinese Secap test set using our in-house data. We further investigate the reliability of existing emotion evaluation metrics and their alignment with human perceptual preferences, and explore using SOTA multimodal LLMs GPT-4o-audio and Gemini to assess emotional speech. Demo samples are available at https://anonymous.4open.science/r/EmoVoice-DF55. Dataset, code, and checkpoints will be released.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Russian Benchmark for Evaluation of Information Retrieval Models</title>
<link>https://arxiv.org/abs/2504.12879</link>
<guid>https://arxiv.org/abs/2504.12879</guid>
<content:encoded><![CDATA[
arXiv:2504.12879v1 Announce Type: cross 
Abstract: We introduce RusBEIR, a comprehensive benchmark designed for zero-shot evaluation of information retrieval (IR) models in the Russian language. Comprising 17 datasets from various domains, it integrates adapted, translated, and newly created datasets, enabling systematic comparison of lexical and neural models. Our study highlights the importance of preprocessing for lexical models in morphologically rich languages and confirms BM25 as a strong baseline for full-document retrieval. Neural models, such as mE5-large and BGE-M3, demonstrate superior performance on most datasets, but face challenges with long-document retrieval due to input size constraints. RusBEIR offers a unified, open-source framework that promotes research in Russian-language information retrieval.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology</title>
<link>https://arxiv.org/abs/2504.12977</link>
<guid>https://arxiv.org/abs/2504.12977</guid>
<content:encoded><![CDATA[
arXiv:2504.12977v1 Announce Type: cross 
Abstract: This paper presents a novel research analytical IT system grounded in Martin Heidegger's Fundamental Ontology, distinguishing between beings (das Seiende) and Being (das Sein). The system employs two modally distinct, descriptively complete languages: a categorical language of beings for processing user inputs and an existential language of Being for internal analysis. These languages are bridged via a phenomenological reduction module, enabling the system to analyze user queries (including questions, answers, and dialogues among IT specialists), identify recursive and self-referential structures, and provide actionable insights in categorical terms. Unlike contemporary systems limited to categorical analysis, this approach leverages Heidegger's phenomenological existential analysis to uncover deeper ontological patterns in query processing, aiding in resolving logical traps in complex interactions, such as metaphor usage in IT contexts. The path to full realization involves formalizing the language of Being by a research team based on Heidegger's Fundamental Ontology; given the existing completeness of the language of beings, this reduces the system's computability to completeness, paving the way for a universal query analysis tool. The paper presents the system's architecture, operational principles, technical implementation, use cases--including a case based on real IT specialist dialogues--comparative evaluation with existing tools, and its advantages and limitations.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses</title>
<link>https://arxiv.org/abs/2504.13038</link>
<guid>https://arxiv.org/abs/2504.13038</guid>
<content:encoded><![CDATA[
arXiv:2504.13038v1 Announce Type: cross 
Abstract: The release of ChatGPT in late 2022 caused a flurry of activity and concern in the academic and educational communities. Some see the tool's ability to generate human-like text that passes at least cursory inspections for factual accuracy ``often enough'' a golden age of information retrieval and computer-assisted learning. Some, on the other hand, worry the tool may lead to unprecedented levels of academic dishonesty and cheating. In this work, we quantify some of the effects of the emergence of Large Language Models (LLMs) on online education by analyzing a multi-year dataset of student essay responses from a free university-level MOOC on AI ethics. Our dataset includes essays submitted both before and after ChatGPT's release. We find that the launch of ChatGPT coincided with significant changes in both the length and style of student essays, mirroring observations in other contexts such as academic publishing. We also observe -- as expected based on related public discourse -- changes in prevalence of key content words related to AI and LLMs, but not necessarily the general themes or topics discussed in the student essays as identified through (dynamic) topic modeling.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins</title>
<link>https://arxiv.org/abs/2504.13059</link>
<guid>https://arxiv.org/abs/2504.13059</guid>
<content:encoded><![CDATA[
arXiv:2504.13059v1 Announce Type: cross 
Abstract: In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples demonstrate significant potential for enhancing dual-arm robotic manipulation systems by improving success rates by over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia</title>
<link>https://arxiv.org/abs/2504.13085</link>
<guid>https://arxiv.org/abs/2504.13085</guid>
<content:encoded><![CDATA[
arXiv:2504.13085v1 Announce Type: cross 
Abstract: Eradicating poverty is the first goal in the United Nations Sustainable Development Goals. However, aporophobia -- the societal bias against people living in poverty -- constitutes a major obstacle to designing, approving and implementing poverty-mitigation policies. This work presents an initial step towards operationalizing the concept of aporophobia to identify and track harmful beliefs and discriminative actions against poor people on social media. In close collaboration with non-profits and governmental organizations, we conduct data collection and exploration. Then we manually annotate a corpus of English tweets from five world regions for the presence of (1) direct expressions of aporophobia, and (2) statements referring to or criticizing aporophobic views or actions of others, to comprehensively characterize the social media discourse related to bias and discrimination against the poor. Based on the annotated data, we devise a taxonomy of categories of aporophobic attitudes and actions expressed through speech on social media. Finally, we train several classifiers and identify the main challenges for automatic detection of aporophobia in social networks. This work paves the way towards identifying, tracking, and mitigating aporophobic views on social media at scale.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing and Inducing Combinational Creativity in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.13120</link>
<guid>https://arxiv.org/abs/2504.13120</guid>
<content:encoded><![CDATA[
arXiv:2504.13120v1 Announce Type: cross 
Abstract: The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents</title>
<link>https://arxiv.org/abs/2504.13128</link>
<guid>https://arxiv.org/abs/2504.13128</guid>
<content:encoded><![CDATA[
arXiv:2504.13128v1 Announce Type: cross 
Abstract: We introduce FreshStack, a reusable framework for automatically building information retrieval (IR) evaluation benchmarks from community-asked questions and answers. FreshStack conducts the following steps: (1) automatic corpus collection from code and technical documentation, (2) nugget generation from community-asked questions and answers, and (3) nugget-level support, retrieving documents using a fusion of retrieval techniques and hybrid architectures. We use FreshStack to build five datasets on fast-growing, recent, and niche topics to ensure the tasks are sufficiently challenging. On FreshStack, existing retrieval models, when applied out-of-the-box, significantly underperform oracle approaches on all five topics, denoting plenty of headroom to improve IR quality. In addition, we identify cases where rerankers do not clearly improve first-stage retrieval accuracy (two out of five topics). We hope that FreshStack will facilitate future work toward constructing realistic, scalable, and uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are available at: https://fresh-stack.github.io.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
arXiv:2504.13146v1 Announce Type: cross 
Abstract: Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. \emph{Antidistillation sampling} provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIB: A Mechanistic Interpretability Benchmark</title>
<link>https://arxiv.org/abs/2504.13151</link>
<guid>https://arxiv.org/abs/2504.13151</guid>
<content:encoded><![CDATA[
arXiv:2504.13151v1 Announce Type: cross 
Abstract: How can we know whether new mechanistic interpretability methods achieve real improvements? In pursuit of meaningful and lasting evaluation standards, we propose MIB, a benchmark with two tracks spanning four tasks and five models. MIB favors methods that precisely and concisely recover relevant causal pathways or specific causal variables in neural language models. The circuit localization track compares methods that locate the model components - and connections between them - most important for performing a task (e.g., attribution patching or information flow routes). The causal variable localization track compares methods that featurize a hidden vector, e.g., sparse autoencoders (SAEs) or distributed alignment search (DAS), and locate model features for a causal variable relevant to the task. Using MIB, we find that attribution and mask optimization methods perform best on circuit localization. For causal variable localization, we find that the supervised DAS method performs best, while SAE features are not better than neurons, i.e., standard dimensions of hidden vectors. These findings illustrate that MIB enables meaningful comparisons of methods, and increases our confidence that there has been real progress in the field.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sleep-time Compute: Beyond Inference Scaling at Test-time</title>
<link>https://arxiv.org/abs/2504.13171</link>
<guid>https://arxiv.org/abs/2504.13171</guid>
<content:encoded><![CDATA[
arXiv:2504.13171v1 Announce Type: cross 
Abstract: Scaling test-time compute has emerged as a key ingredient for enabling large language models (LLMs) to solve difficult problems, but comes with high latency and inference cost. We introduce sleep-time compute, which allows models to "think" offline about contexts before queries are presented: by anticipating what queries users might ask and pre-computing useful quantities, we can significantly reduce the compute requirements at test-time. To demonstrate the efficacy of our method, we create modified versions of two reasoning tasks - Stateful GSM-Symbolic and Stateful AIME. We find that sleep-time compute can reduce the amount of test-time compute needed to achieve the same accuracy by ~ 5x on Stateful GSM-Symbolic and Stateful AIME and that by scaling sleep-time compute we can further increase accuracy by up to 13% on Stateful GSM-Symbolic and 18% on Stateful AIME. Furthermore, we introduce Multi-Query GSM-Symbolic, which extends GSM-Symbolic by including multiple related queries per context. By amortizing sleep-time compute across related queries about the same context using Multi-Query GSM-Symbolic, we can decrease the average cost per query by 2.5x. We then conduct additional analysis to understand when sleep-time compute is most effective, finding the predictability of the user query to be well correlated with the efficacy of sleep-time compute. Finally, we conduct a case-study of applying sleep-time compute to a realistic agentic SWE task.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs</title>
<link>https://arxiv.org/abs/2504.13172</link>
<guid>https://arxiv.org/abs/2504.13172</guid>
<content:encoded><![CDATA[
arXiv:2504.13172v1 Announce Type: cross 
Abstract: Cross-modal retrieval (CMR) is a fundamental task in multimedia research, focused on retrieving semantically relevant targets across different modalities. While traditional CMR methods match text and image via embedding-based similarity calculations, recent advancements in pre-trained generative models have established generative retrieval as a promising alternative. This paradigm assigns each target a unique identifier and leverages a generative model to directly predict identifiers corresponding to input queries without explicit indexing. Despite its great potential, current generative CMR approaches still face semantic information insufficiency in both identifier construction and generation processes. To address these limitations, we propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval framework (SemCORE), designed to unleash the semantic understanding capabilities in generative cross-modal retrieval task. Specifically, we first construct a Structured natural language IDentifier (SID) that effectively aligns target identifiers with generative models optimized for natural language comprehension and generation. Furthermore, we introduce a Generative Semantic Verification (GSV) strategy enabling fine-grained target discrimination. Additionally, to the best of our knowledge, SemCORE is the first framework to simultaneously consider both text-to-image and image-to-text retrieval tasks within generative cross-modal retrieval. Extensive experiments demonstrate that our framework outperforms state-of-the-art generative cross-modal retrieval methods. Notably, SemCORE achieves substantial improvements across benchmark datasets, with an average increase of 8.65 points in Recall@1 for text-to-image retrieval.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation</title>
<link>https://arxiv.org/abs/2207.14000</link>
<guid>https://arxiv.org/abs/2207.14000</guid>
<content:encoded><![CDATA[
arXiv:2207.14000v4 Announce Type: replace 
Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gated attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of PARARULE-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Baichuan 2: Open Large-scale Language Models</title>
<link>https://arxiv.org/abs/2309.10305</link>
<guid>https://arxiv.org/abs/2309.10305</guid>
<content:encoded><![CDATA[
arXiv:2309.10305v4 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers</title>
<link>https://arxiv.org/abs/2402.11700</link>
<guid>https://arxiv.org/abs/2402.11700</guid>
<content:encoded><![CDATA[
arXiv:2402.11700v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while preserving their performance, thereby opening avenues for significantly more efficient use of LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Citation-Enhanced Generation for LLM-based Chatbots</title>
<link>https://arxiv.org/abs/2402.16063</link>
<guid>https://arxiv.org/abs/2402.16063</guid>
<content:encoded><![CDATA[
arXiv:2402.16063v4 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our codes and dataset will be publicly available.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory</title>
<link>https://arxiv.org/abs/2404.11672</link>
<guid>https://arxiv.org/abs/2404.11672</guid>
<content:encoded><![CDATA[
arXiv:2404.11672v3 Announce Type: replace 
Abstract: While current large language models (LLMs) perform well on many knowledge-related tasks, they are limited by relying on their parameters as an implicit storage mechanism. As a result, they struggle with memorizing rare events and with updating their memory as facts change over time. In addition, the uninterpretable nature of parametric memory makes it challenging to prevent hallucination. Model editing and augmenting LLMs with parameters specialized for memory are only partial solutions. In this paper, we introduce MemLLM, a novel method of enhancing LLMs by integrating a structured and explicit read-and-write memory module. MemLLM tackles the aforementioned challenges by enabling dynamic interaction with the memory and improving the LLM's capabilities in using stored knowledge. Our experiments indicate that MemLLM enhances the LLM's performance and interpretability, in language modeling in general and knowledge-intensive tasks in particular. We see MemLLM as an important step towards making LLMs more grounded and factual through memory augmentation. The project repository is publicly available at https://github.com/amodaresi/MemLLM
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fleet of Agents: Coordinated Problem Solving with Large Language Models</title>
<link>https://arxiv.org/abs/2405.06691</link>
<guid>https://arxiv.org/abs/2405.06691</guid>
<content:encoded><![CDATA[
arXiv:2405.06691v2 Announce Type: replace 
Abstract: While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unipa-GPT: Large Language Models for university-oriented QA in Italian</title>
<link>https://arxiv.org/abs/2407.14246</link>
<guid>https://arxiv.org/abs/2407.14246</guid>
<content:encoded><![CDATA[
arXiv:2407.14246v3 Announce Type: replace 
Abstract: This paper illustrates the architecture and training of Unipa-GPT, a chatbot relying on a Large Language Model, developed for assisting students in choosing a bachelor/master degree course at the University of Palermo. Unipa-GPT relies on gpt-3.5-turbo, it was presented in the context of the European Researchers' Night (SHARPER night). In our experiments we adopted both the Retrieval Augmented Generation (RAG) approach and fine-tuning to develop the system. The whole architecture of Unipa-GPT is presented, both the RAG and the fine-tuned systems are compared, and a brief discussion on their performance is reported. Further comparison with other Large Language Models and the experimental results during the SHARPER night are illustrated. Corpora and code are available on GitHub
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities</title>
<link>https://arxiv.org/abs/2408.04682</link>
<guid>https://arxiv.org/abs/2408.04682</guid>
<content:encoded><![CDATA[
arXiv:2408.04682v2 Announce Type: replace 
Abstract: Recent large language models (LLMs) advancements sparked a growing research interest in tool assisted LLMs solving real-world challenges, which calls for comprehensive evaluation of tool-use capabilities. While previous works focused on either evaluating over stateless web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, ToolSandbox includes stateful tool execution, implicit state dependencies between tools, a built-in user simulator supporting on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajectory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canonicalization and Insufficient Information defined in ToolSandbox are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities. ToolSandbox evaluation framework is released at https://github.com/apple/ToolSandbox
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant</title>
<link>https://arxiv.org/abs/2409.11055</link>
<guid>https://arxiv.org/abs/2409.11055</guid>
<content:encoded><![CDATA[
arXiv:2409.11055v2 Announce Type: replace 
Abstract: Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in coding and STEM tasks, though reasoning may sometimes improve.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective</title>
<link>https://arxiv.org/abs/2410.10291</link>
<guid>https://arxiv.org/abs/2410.10291</guid>
<content:encoded><![CDATA[
arXiv:2410.10291v4 Announce Type: replace 
Abstract: Accurate interpretation and visualization of human instructions are crucial for text-to-image (T2I) synthesis. However, current models struggle to capture semantic variations from word order changes, and existing evaluations, relying on indirect metrics like text-image similarity, fail to reliably assess these challenges. This often obscures poor performance on complex or uncommon linguistic patterns by the focus on frequent word combinations. To address these deficiencies, we propose a novel metric called SemVarEffect and a benchmark named SemVarBench, designed to evaluate the causality between semantic variations in inputs and outputs in T2I synthesis. Semantic variations are achieved through two types of linguistic permutations, while avoiding easily predictable literal variations. Experiments reveal that the CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1. Semantic variations in object relations are less understood than attributes, scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in UNet or Transformers plays a crucial role in handling semantic variations, a factor previously overlooked by a focus on textual encoders. Our work establishes an effective evaluation framework that advances the T2I synthesis community's exploration of human instruction understanding. Our benchmark and code are available at https://github.com/zhuxiangru/SemVarBench .
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-context KV-Cache Eviction for LLMs via Attention-Gate</title>
<link>https://arxiv.org/abs/2410.12876</link>
<guid>https://arxiv.org/abs/2410.12876</guid>
<content:encoded><![CDATA[
arXiv:2410.12876v3 Announce Type: replace 
Abstract: The KV-Cache technique has become the standard for the inference of large language models (LLMs). Yet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system. This paper enables a novel dynamic KV-Cache eviction policy by injecting a lightweight module called Attention-Gate to the model. It accepts the global context as input and yields eviction flags for each token. The self-attention modules in the model proceed according to the flags and cache only a subset of the KV states for next token prediction. The Attention-Gates can yield various flags for different heads and layers and be easily tuned on top of a pre-trained LLM via continual pre-training or supervised fine-tuning. The computational and memory overhead introduced by Attention-Gates can be minimal. We empirically evaluate the proposed approach across multiple scenarios, showing that effective eviction of redundant tokens can not only improve efficiency but also enhance performance.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</title>
<link>https://arxiv.org/abs/2410.17385</link>
<guid>https://arxiv.org/abs/2410.17385</guid>
<content:encoded><![CDATA[
arXiv:2410.17385v2 Announce Type: replace 
Abstract: Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs</title>
<link>https://arxiv.org/abs/2411.07466</link>
<guid>https://arxiv.org/abs/2411.07466</guid>
<content:encoded><![CDATA[
arXiv:2411.07466v2 Announce Type: replace 
Abstract: Recent evaluations of LLMs on coreference resolution have revealed that traditional output formats and evaluation metrics do not fully capture the models' referential understanding. To address this, we introduce IdentifyMe, a new benchmark for mention resolution presented in a multiple-choice question (MCQ) format, commonly used for evaluating LLMs. IdentifyMe features long narratives and employs heuristics to exclude easily identifiable mentions, creating a more challenging task. The benchmark also consists of a curated mixture of different mention types and corresponding entities, allowing for a fine-grained analysis of model performance. We evaluate both closed- and open source LLMs on IdentifyMe and observe a significant performance gap (20-30%) between the state-of-the-art sub-10B open models vs. closed ones. We observe that pronominal mentions, which have limited surface information, are typically much harder for models to resolve than nominal mentions. Additionally, we find that LLMs often confuse entities when their mentions overlap in nested structures. The highest-scoring model, GPT-4o, achieves 81.9% accuracy, highlighting the strong referential capabilities of state-of-the-art LLMs while also indicating room for further improvement.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMPS: ASR with Multimodal Paraphrase Supervision</title>
<link>https://arxiv.org/abs/2411.18368</link>
<guid>https://arxiv.org/abs/2411.18368</guid>
<content:encoded><![CDATA[
arXiv:2411.18368v2 Announce Type: replace 
Abstract: Spontaneous or conversational multilingual speech presents many challenges for state-of-the-art automatic speech recognition (ASR) systems. In this work, we present a new technique AMPS that augments a multilingual multimodal ASR system with paraphrase-based supervision for improved conversational ASR in multiple languages, including Hindi, Marathi, Malayalam, Kannada, and Nyanja. We use paraphrases of the reference transcriptions as additional supervision while training the multimodal ASR model and selectively invoke this paraphrase objective for utterances with poor ASR performance. Using AMPS with a state-of-the-art multimodal model SeamlessM4T, we obtain significant relative reductions in word error rates (WERs) of up to 5%. We present detailed analyses of our system using both objective and human evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Arithmetic Reasoning</title>
<link>https://arxiv.org/abs/2412.01113</link>
<guid>https://arxiv.org/abs/2412.01113</guid>
<content:encoded><![CDATA[
arXiv:2412.01113v2 Announce Type: replace 
Abstract: This study investigates the internal reasoning process of language models during arithmetic multi-step reasoning, motivated by the question of when they internally form their answers during reasoning. Particularly, we inspect whether the answer is determined before or after chain-of-thought (CoT) begins to determine whether models follow a post-hoc Think-to-Talk mode or a step-by-step Talk-to-Think mode of explanation. Through causal probing experiments in controlled arithmetic reasoning tasks, we found systematic internal reasoning patterns across models in our case study; for example, single-step subproblems are solved before CoT begins, and more complicated multi-step calculations are performed during CoT.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?</title>
<link>https://arxiv.org/abs/2502.20973</link>
<guid>https://arxiv.org/abs/2502.20973</guid>
<content:encoded><![CDATA[
arXiv:2502.20973v2 Announce Type: replace 
Abstract: In this era of rapid technological advancements, communication continues to evolve as new linguistic phenomena emerge. Among these is Arabizi, a hybrid form of Arabic that incorporates Latin characters and numbers to represent the spoken dialects of Arab communities. Arabizi is widely used on social media and allows people to communicate in an informal and dynamic way, but it poses significant challenges for machine translation due to its lack of formal structure and deeply embedded cultural nuances. This case study arises from a growing need to translate Arabizi for gisting purposes. It evaluates the capacity of different LLMs to decode and translate Arabizi, focusing on multiple Arabic dialects that have rarely been studied up until now. Using a combination of human evaluators and automatic metrics, this research project investigates the model's performance in translating Arabizi into both Modern Standard Arabic and English. Key questions explored include which dialects are translated most effectively and whether translations into English surpass those into Arabic.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition</title>
<link>https://arxiv.org/abs/2503.10673</link>
<guid>https://arxiv.org/abs/2503.10673</guid>
<content:encoded><![CDATA[
arXiv:2503.10673v2 Announce Type: replace 
Abstract: We introduce ZeroSumEval, a dynamic, competition-based, and evolving evaluation framework for Large Language Models (LLMs) that leverages competitive games. ZeroSumEval encompasses a diverse suite of games, including security challenges (Capture the Flag), classic board games (chess), and knowledge tests (MathQuiz). These games are designed to evaluate a range of capabilities such as strategic reasoning, planning, knowledge application, safety, and adaptability. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework for easily implementing games and leverages DSPy to provide a better abstraction for LLM player strategies.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models</title>
<link>https://arxiv.org/abs/2504.00046</link>
<guid>https://arxiv.org/abs/2504.00046</guid>
<content:encoded><![CDATA[
arXiv:2504.00046v2 Announce Type: replace 
Abstract: In recent years, social media has emerged as a primary channel for users to promptly share feedback and issues during disasters and emergencies, playing a key role in crisis management. While significant progress has been made in collecting and analyzing social media content, there remains a pressing need to enhance the automation, aggregation, and customization of this data to deliver actionable insights tailored to diverse stakeholders, including the press, police, EMS, and firefighters. This effort is essential for improving the coordination of activities such as relief efforts, resource distribution, and media communication. This paper presents a methodology that leverages the capabilities of LLMs to enhance disaster response and management. Our approach combines classification techniques with generative AI to bridge the gap between raw user feedback and stakeholder-specific reports. Social media posts shared during catastrophic events are analyzed with a focus on user-reported issues, service interruptions, and encountered challenges. We employ full-spectrum LLMs, using analytical models like BERT for precise, multi-dimensional classification of content type, sentiment, emotion, geolocation, and topic. Generative models such as ChatGPT are then used to produce human-readable, informative reports tailored to distinct audiences, synthesizing insights derived from detailed classifications. We compare standard approaches, which analyze posts directly using prompts in ChatGPT, to our advanced method, which incorporates multi-dimensional classification, sub-event selection, and tailored report generation. Our methodology demonstrates superior performance in both quantitative metrics, such as text coherence scores and latent representations, and qualitative assessments by automated tools and field experts, delivering precise insights for diverse disaster response stakeholders.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OnRL-RAG: Real-Time Personalized Mental Health Dialogue System</title>
<link>https://arxiv.org/abs/2504.02894</link>
<guid>https://arxiv.org/abs/2504.02894</guid>
<content:encoded><![CDATA[
arXiv:2504.02894v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion</title>
<link>https://arxiv.org/abs/2504.06562</link>
<guid>https://arxiv.org/abs/2504.06562</guid>
<content:encoded><![CDATA[
arXiv:2504.06562v2 Announce Type: replace 
Abstract: Heterogeneous model fusion enhances the performance of LLMs by integrating the knowledge and capabilities of multiple structurally diverse models. However, existing approaches often rely solely on selecting the best output for each prompt from source models, which underutilizes their full potential due to limited source knowledge and results in sparse optimization signals. To address this limitation, we propose FuseRL, a novel two-stage framework comprising FuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT establishes a robust initialization by integrating the strengths of heterogeneous source models through weighted supervised fine-tuning (SFT) on diverse outputs for each prompt. FusePO optimizes weighted preferences based on the outputs of multiple source models to enable superior alignment performance. Extensive experiments demonstrate the effectiveness of our framework across various preference alignment methods, including RLOO, DPO, and SimPO. Using Llama-3.1-8B-Instruct as the target model, our approach achieves state-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard benchmarks. Further analysis suggests that FuseSFT regularizes the training process to reduce overfitting, while FusePO introduces dense and diverse signals for preference optimization.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare</title>
<link>https://arxiv.org/abs/2504.08260</link>
<guid>https://arxiv.org/abs/2504.08260</guid>
<content:encoded><![CDATA[
arXiv:2504.08260v2 Announce Type: replace 
Abstract: Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taxonomy and Analysis of Sensitive User Queries in Generative AI Search</title>
<link>https://arxiv.org/abs/2404.08672</link>
<guid>https://arxiv.org/abs/2404.08672</guid>
<content:encoded><![CDATA[
arXiv:2404.08672v3 Announce Type: replace-cross 
Abstract: Although there has been a growing interest among industries in integrating generative LLMs into their services, limited experience and scarcity of resources act as a barrier in launching and servicing large-scale LLM-based services. In this paper, we share our experiences in developing and operating generative AI models within a national-scale search engine, with a specific focus on the sensitiveness of user queries. We propose a taxonomy for sensitive search queries, outline our approaches, and present a comprehensive analysis report on sensitive queries from actual users. We believe that our experiences in launching generative AI search systems can contribute to reducing the barrier in building generative LLM-based services.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALCM: Autonomous LLM-Augmented Causal Discovery Framework</title>
<link>https://arxiv.org/abs/2405.01744</link>
<guid>https://arxiv.org/abs/2405.01744</guid>
<content:encoded><![CDATA[
arXiv:2405.01744v2 Announce Type: replace-cross 
Abstract: To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP- hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs</title>
<link>https://arxiv.org/abs/2409.09586</link>
<guid>https://arxiv.org/abs/2409.09586</guid>
<content:encoded><![CDATA[
arXiv:2409.09586v2 Announce Type: replace-cross 
Abstract: As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and large language models (LLMs) across four real-world scenarios: collaborative writing, education, public sectors, and healthcare. Our findings reveal concerning misalignments between humans and LLMs, such as humans frequently endorse values like "National Security" which were largely rejected by LLMs. We also observe that values differ across scenarios, highlighting the need for context-aware AI alignment strategies. This work provides valuable insights into the design space of human-AI alignment, laying the foundations for developing AI systems that responsibly reflect societal values and ethics.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Field Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2410.20056</link>
<guid>https://arxiv.org/abs/2410.20056</guid>
<content:encoded><![CDATA[
arXiv:2410.20056v2 Announce Type: replace-cross 
Abstract: Document retrieval for tasks such as search and retrieval-augmented generation typically involves datasets that are unstructured: free-form text without explicit internal structure in each document. However, documents can have a structured form, consisting of fields such as an article title, message body, or HTML header. To address this gap, we introduce Multi-Field Adaptive Retrieval (MFAR), a flexible framework that accommodates any number of and any type of document indices on structured data. Our framework consists of two main steps: (1) the decomposition of an existing document into fields, each indexed independently through dense and lexical methods, and (2) learning a model which adaptively predicts the importance of a field by conditioning on the document query, allowing on-the-fly weighting of the most likely field(s). We find that our approach allows for the optimized use of dense versus lexical representations across field types, significantly improves in document ranking over a number of existing retrievers, and achieves state-of-the-art performance for multi-field structured data.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLMs Can Reason about Aesthetics in Zero-Shot</title>
<link>https://arxiv.org/abs/2501.09012</link>
<guid>https://arxiv.org/abs/2501.09012</guid>
<content:encoded><![CDATA[
arXiv:2501.09012v2 Announce Type: replace-cross 
Abstract: The rapid progress of generative art has democratized the creation of visually pleasing imagery. However, achieving genuine artistic impact - the kind that resonates with viewers on a deeper, more meaningful level - requires a sophisticated aesthetic sensibility. This sensibility involves a multi-faceted reasoning process extending beyond mere visual appeal, which is often overlooked by current computational models. This paper pioneers an approach to capture this complex process by investigating how the reasoning capabilities of Multimodal LLMs (MLLMs) can be effectively elicited for aesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a tendency towards hallucinations during aesthetic reasoning, characterized by subjective opinions and unsubstantiated artistic interpretations. We further demonstrate that these limitations can be overcome by employing an evidence-based, objective reasoning process, as substantiated by our proposed baseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and in-depth aesthetic reasoning that aligns significantly better with human judgment. These findings have direct applications in areas such as AI art tutoring and as reward models for generative art. Ultimately, our work paves the way for AI systems that can truly understand, appreciate, and generate artworks that align with the sensible human aesthetic standard.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Agent Security: A Policy for Every Purpose</title>
<link>https://arxiv.org/abs/2501.17070</link>
<guid>https://arxiv.org/abs/2501.17070</guid>
<content:encoded><![CDATA[
arXiv:2501.17070v3 Announce Type: replace-cross 
Abstract: Judging an action's safety requires knowledge of the context in which the action takes place. To human agents who act in various contexts, this may seem obvious: performing an action such as email deletion may or may not be appropriate depending on the email's content, the goal (e.g., to erase sensitive emails or to clean up trash), and the type of email address (e.g., work or personal). Unlike people, computational systems have often had only limited agency in limited contexts. Thus, manually crafted policies and user confirmation (e.g., smartphone app permissions or network access control lists), while imperfect, have sufficed to restrict harmful actions. However, with the upcoming deployment of generalist agents that support a multitude of tasks (e.g., an automated personal assistant), we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual agent security (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments</title>
<link>https://arxiv.org/abs/2504.03160</link>
<guid>https://arxiv.org/abs/2504.03160</guid>
<content:encoded><![CDATA[
arXiv:2504.03160v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.09081</link>
<guid>https://arxiv.org/abs/2504.09081</guid>
<content:encoded><![CDATA[
arXiv:2504.09081v2 Announce Type: replace-cross 
Abstract: We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography</title>
<link>https://arxiv.org/abs/2504.10090</link>
<guid>https://arxiv.org/abs/2504.10090</guid>
<content:encoded><![CDATA[
arXiv:2504.10090v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) and multimodal large language models (MLLMs) have significantly advanced artificial intelligence. However, visual reasoning, reasoning involving both visual and textual inputs, remains underexplored. Recent advancements, including the reasoning models like OpenAI o1 and Gemini 2.0 Flash Thinking, which incorporate image inputs, have opened this capability. In this ongoing work, we focus specifically on photography-related tasks because a photo is a visual snapshot of the physical world where the underlying physics (i.e., illumination, blur extent, etc.) interplay with the camera parameters. Successfully reasoning from the visual information of a photo to identify these numerical camera settings requires the MLLMs to have a deeper understanding of the underlying physics for precise visual comprehension, representing a challenging and intelligent capability essential for practical applications like photography assistant agents. We aim to evaluate MLLMs on their ability to distinguish visual differences related to numerical camera settings, extending a methodology previously proposed for vision-language models (VLMs). Our preliminary results demonstrate the importance of visual reasoning in photography-related tasks. Moreover, these results show that no single MLLM consistently dominates across all evaluation tasks, demonstrating ongoing challenges and opportunities in developing MLLMs with better visual reasoning.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSPLADE: Learned Sparse Retrieval with Causal Language Models</title>
<link>https://arxiv.org/abs/2504.10816</link>
<guid>https://arxiv.org/abs/2504.10816</guid>
<content:encoded><![CDATA[
arXiv:2504.10816v2 Announce Type: replace-cross 
Abstract: In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) has emerged as promising alternative, achieving competitive retrieval performance while also being able to leverage the classical inverted index data structure for efficient retrieval. However, limited works have explored scaling LSR beyond BERT scale. In this work, we identify two challenges in training large language models (LLM) for LSR: (1) training instability during the early stage of contrastive training; (2) suboptimal performance due to pre-trained LLM's unidirectional attention. To address these challenges, we propose two corresponding techniques: (1) a lightweight adaptation training phase to eliminate training instability; (2) two model variants to enable bidirectional information. With these techniques, we are able to train LSR models with 8B scale LLM, and achieve competitive retrieval performance with reduced index size. Furthermore, we are among the first to analyze the performance-efficiency tradeoff of LLM-based LSR model through the lens of model quantization. Our findings provide insights into adapting LLMs for efficient retrieval modeling.
]]></content:encoded>
<pubDate>Fri, 18 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.11468</link>
<guid>https://arxiv.org/abs/2504.11468</guid>
<content:encoded><![CDATA[
arXiv:2504.11468v1 Announce Type: new 
Abstract: This work revisits the dominant supervised fine-tuning (SFT) then reinforcement learning (RL) paradigm for training Large Vision-Language Models (LVLMs), and reveals a key finding: SFT can significantly undermine subsequent RL by inducing ``pseudo reasoning paths'' imitated from expert models. While these paths may resemble the native reasoning paths of RL models, they often involve prolonged, hesitant, less informative steps, and incorrect reasoning. To systematically study this effect, we introduce VLAA-Thinking, a new multimodal dataset designed to support reasoning in LVLMs. Constructed via a six-step pipeline involving captioning, reasoning distillation, answer rewrite and verification, VLAA-Thinking comprises high-quality, step-by-step visual reasoning traces for SFT, along with a more challenging RL split from the same data source. Using this dataset, we conduct extensive experiments comparing SFT, RL and their combinations. Results show that while SFT helps models learn reasoning formats, it often locks aligned models into imitative, rigid reasoning modes that impede further learning. In contrast, building on the Group Relative Policy Optimization (GRPO) with a novel mixed reward module integrating both perception and cognition signals, our RL approach fosters more genuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on Qwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard (https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard) among 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope our findings provide valuable insights in developing reasoning-capable LVLMs and can inform future research in this area.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReTool: Reinforcement Learning for Strategic Tool Use in LLMs</title>
<link>https://arxiv.org/abs/2504.11536</link>
<guid>https://arxiv.org/abs/2504.11536</guid>
<content:encoded><![CDATA[
arXiv:2504.11536v1 Announce Type: new 
Abstract: While reasoning models (e.g., DeepSeek R1) trained with reinforcement learning (RL), excel in textual reasoning, they struggle in scenarios requiring structured problem-solving, such as geometric reasoning, concise computation, or complex equation solving-areas where computational tools like code interpreters (CI) demonstrate distinct advantages. To bridge this gap, we propose ReTool, which enhances long-form reasoning with tool-integrated learning, including two key features: (1) dynamic interleaving of real-time code execution within natural language reasoning processes, and (2) an automated RL paradigm that allows policy rollouts with multi-turn real-time code execution and teaches the model in learning when and how to invoke tools based on outcome feedback. ReTool employs a systematic training framework, beginning with synthetic cold-start data generation to produce code-augmented long-form reasoning traces for fine-tuning base models. Subsequent RL training leverages task outcomes as rewards to iteratively refine the model's tool use strategy, enabling autonomous discovery of optimal tool invocation patterns without human priors. Experiments on the challenging MATH Olympiad benchmark AIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with 400 training steps, outperforming text-based RL baseline (40% accuracy, 1080 steps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5% accuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further analysis reveals emergent behaviors such as code self-correction, signaling an ''aha moment'' in which the model autonomously masters adaptive tool use. These findings highlight the promise of outcome-driven tool integration for advancing complex mathematical reasoning and offer new insights into hybrid neuro-symbolic systems.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AskQE: Question Answering as Automatic Evaluation for Machine Translation</title>
<link>https://arxiv.org/abs/2504.11582</link>
<guid>https://arxiv.org/abs/2504.11582</guid>
<content:encoded><![CDATA[
arXiv:2504.11582v1 Announce Type: new 
Abstract: How can a monolingual English speaker determine whether an automatic translation in French is good enough to be shared? Existing MT error detection and quality estimation (QE) techniques do not address this practical scenario. We introduce AskQE, a question generation and answering framework designed to detect critical MT errors and provide actionable feedback, helping users decide whether to accept or reject MT outputs even without the knowledge of the target language. Using ContraTICO, a dataset of contrastive synthetic MT errors in the COVID-19 domain, we explore design choices for AskQE and develop an optimized version relying on LLaMA-3 70B and entailed facts to guide question generation. We evaluate the resulting system on the BioMQM dataset of naturally occurring MT errors, where AskQE has higher Kendall's Tau correlation and decision accuracy with human ratings compared to other QE metrics.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Instruct Models for Free: A Study on Partial Adaptation</title>
<link>https://arxiv.org/abs/2504.11626</link>
<guid>https://arxiv.org/abs/2504.11626</guid>
<content:encoded><![CDATA[
arXiv:2504.11626v1 Announce Type: new 
Abstract: Instruct models, obtained from various instruction tuning or post-training steps, are commonly deemed superior and more usable than their base counterpart. While the model gains instruction following ability, instruction tuning may lead to forgetting the knowledge from pre-training or it may encourage the model being overly conversational or verbose. This, in turn, can lead to degradation of in-context few-shot learning performance. In this work, we study the performance trajectory between base and instruct models by scaling down the strength of instruction-tuning via the partial adaption method. We show that, across several model families and model sizes, reducing the strength of instruction-tuning results in material improvement on a few-shot in-context learning benchmark covering a variety of classic natural language tasks. This comes at the cost of losing some degree of instruction following ability as measured by AlpacaEval. Our study shines light on the potential trade-off between in-context learning and instruction following abilities that is worth considering in practice.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions</title>
<link>https://arxiv.org/abs/2504.11673</link>
<guid>https://arxiv.org/abs/2504.11673</guid>
<content:encoded><![CDATA[
arXiv:2504.11673v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses during the early phases of survey design. While previous studies have examined whether models can reflect individual opinions or attitudes, we argue that a \emph{higher-order} binding of virtual personas requires successfully approximating not only the opinions of a user as an identified member of a group, but also the nuanced ways in which that user perceives and evaluates those outside the group. In particular, faithfully simulating how humans perceive different social groups is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies. Altogether, our work extends the applicability of LLMs beyond estimating individual self-opinions, enabling their use in a broader range of human studies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters</title>
<link>https://arxiv.org/abs/2504.11770</link>
<guid>https://arxiv.org/abs/2504.11770</guid>
<content:encoded><![CDATA[
arXiv:2504.11770v1 Announce Type: new 
Abstract: Cross-linguistically, native words and loanwords follow different phonological rules. In English, for example, words of Germanic and Latinate origin exhibit different stress patterns, and a certain syntactic structure is exclusive to Germanic verbs. When seeing them as a cognitive model, however, such etymology-based generalizations face challenges in terms of learnability, since the historical origins of words are presumably inaccessible information for general language learners. In this study, we present computational evidence indicating that the Germanic-Latinate distinction in the English lexicon is learnable from the phonotactic information of individual words. Specifically, we performed an unsupervised clustering on corpus-extracted words, and the resulting word clusters largely aligned with the etymological distinction. The model-discovered clusters also recovered various linguistic generalizations documented in the previous literature regarding the corresponding etymological classes. Moreover, our findings also uncovered previously unrecognized features of the quasi-etymological clusters, offering novel hypotheses for future experimental studies.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Web Agents with Explicit Rollback Mechanisms</title>
<link>https://arxiv.org/abs/2504.11788</link>
<guid>https://arxiv.org/abs/2504.11788</guid>
<content:encoded><![CDATA[
arXiv:2504.11788v1 Announce Type: new 
Abstract: With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification</title>
<link>https://arxiv.org/abs/2504.11793</link>
<guid>https://arxiv.org/abs/2504.11793</guid>
<content:encoded><![CDATA[
arXiv:2504.11793v1 Announce Type: new 
Abstract: Federated Learning (FL) faces major challenges regarding communication overhead and model privacy when training large language models (LLMs), especially in healthcare applications. To address these, we introduce Selective Attention Federated Learning (SAFL), a novel approach that dynamically fine-tunes only those transformer layers identified as attention-critical. By employing attention patterns to determine layer importance, SAFL significantly reduces communication bandwidth and enhances differential privacy resilience. Evaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and MIMIC-III discharge summaries) demonstrate that SAFL achieves competitive performance with centralized models while substantially improving communication efficiency and privacy preservation.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture</title>
<link>https://arxiv.org/abs/2504.11809</link>
<guid>https://arxiv.org/abs/2504.11809</guid>
<content:encoded><![CDATA[
arXiv:2504.11809v1 Announce Type: new 
Abstract: Simultaneous speech translation (SimulST) produces translations incrementally while processing partial speech input. Although large language models (LLMs) have showcased strong capabilities in offline translation tasks, applying them to SimulST poses notable challenges. Existing LLM-based SimulST approaches either incur significant computational overhead due to repeated encoding of bidirectional speech encoder, or they depend on a fixed read/write policy, limiting the efficiency and performance. In this work, we introduce Efficient and Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional architecture, including both speech encoder and LLM. EASiST includes a multi-latency data curation strategy to generate semantically aligned SimulST training samples and redefines SimulST as an interleaved generation task with explicit read/write tokens. To facilitate adaptive inference, we incorporate a lightweight policy head that dynamically predicts read/write actions. Additionally, we employ a multi-stage training strategy to align speech-text modalities and optimize both translation and policy behavior. Experiments on the MuST-C En$\rightarrow$De and En$\rightarrow$Es datasets demonstrate that EASiST offers superior latency-quality trade-offs compared to several strong baselines.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARWI: Arabic Write and Improve</title>
<link>https://arxiv.org/abs/2504.11814</link>
<guid>https://arxiv.org/abs/2504.11814</guid>
<content:encoded><![CDATA[
arXiv:2504.11814v1 Announce Type: new 
Abstract: Although Arabic is spoken by over 400 million people, advanced Arabic writing assistance tools remain limited. To address this gap, we present ARWI, a new writing assistant that helps learners improve essay writing in Modern Standard Arabic. ARWI is the first publicly available Arabic writing assistant to include a prompt database for different proficiency levels, an Arabic text editor, state-of-the-art grammatical error detection and correction, and automated essay scoring aligned with the Common European Framework of Reference standards for language attainment. Moreover, ARWI can be used to gather a growing auto-annotated corpus, facilitating further research on Arabic grammar correction and essay scoring, as well as profiling patterns of errors made by native speakers and non-native learners. A preliminary user study shows that ARWI provides actionable feedback, helping learners identify grammatical gaps, assess language proficiency, and guide improvement.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2504.11829</link>
<guid>https://arxiv.org/abs/2504.11829</guid>
<content:encoded><![CDATA[
arXiv:2504.11829v1 Announce Type: new 
Abstract: Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Could Thinking Multilingually Empower LLM Reasoning?</title>
<link>https://arxiv.org/abs/2504.11833</link>
<guid>https://arxiv.org/abs/2504.11833</guid>
<content:encoded><![CDATA[
arXiv:2504.11833v1 Announce Type: new 
Abstract: Previous work indicates that large language models exhibit a significant "English bias", i.e. they often perform better when tasks are presented in English. Interestingly, we have observed that using certain other languages in reasoning tasks can yield better performance than English. However, this phenomenon remains under-explored. In this paper, we explore the upper bound of harnessing multilingualism in reasoning tasks, suggesting that multilingual reasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly (tolerance for variations in translation quality and language choice) higher upper bounds than English-only reasoning. Besides analyzing the reason behind the upper bound and challenges in reaching it, we also find that common answer selection methods cannot achieve this upper bound, due to their limitations and biases. These insights could pave the way for future research aimed at fully harnessing the potential of multilingual reasoning in LLMs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations</title>
<link>https://arxiv.org/abs/2504.11837</link>
<guid>https://arxiv.org/abs/2504.11837</guid>
<content:encoded><![CDATA[
arXiv:2504.11837v1 Announce Type: new 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Finite State Machine (FSM) on LLMs, and propose a framework called FiSMiness. Our framework allows a single LLM to bootstrap the planning during ESC, and self-reason the seeker's emotion, support strategy and the final response upon each conversational turn. Substantial experiments on ESC datasets suggest that FiSMiness outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and external-assisted methods, even those with many more parameters.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection</title>
<link>https://arxiv.org/abs/2504.11900</link>
<guid>https://arxiv.org/abs/2504.11900</guid>
<content:encoded><![CDATA[
arXiv:2504.11900v1 Announce Type: new 
Abstract: Stories are a fundamental aspect of human experience. Engaging deeply with stories and spotting plot holes -- inconsistencies in a storyline that break the internal logic or rules of a story's world -- requires nuanced reasoning skills, including tracking entities and events and their interplay, abstract thinking, pragmatic narrative understanding, commonsense and social reasoning, and theory of mind. As Large Language Models (LLMs) increasingly generate, interpret, and modify text, rigorously assessing their narrative consistency and deeper language understanding becomes critical. However, existing benchmarks focus mainly on surface-level comprehension. In this work, we propose plot hole detection in stories as a proxy to evaluate language understanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel algorithm to controllably and carefully synthesize plot holes in human-written stories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot hole detection abilities in stories -- FlawedFictions -- , which is robust to contamination, with human filtering ensuring high quality. We find that state-of-the-art LLMs struggle in accurately solving FlawedFictions regardless of the reasoning effort allowed, with performance significantly degrading as story length increases. Finally, we show that LLM-based story summarization and story generation are prone to introducing plot holes, with more than 50% and 100% increases in plot hole detection rates with respect to human-written originals.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation</title>
<link>https://arxiv.org/abs/2504.11934</link>
<guid>https://arxiv.org/abs/2504.11934</guid>
<content:encoded><![CDATA[
arXiv:2504.11934v1 Announce Type: new 
Abstract: Gender-neutral translation (GNT) aims to avoid expressing the gender of human referents when the source text lacks explicit cues about the gender of those referents. Evaluating GNT automatically is particularly challenging, with current solutions being limited to monolingual classifiers. Such solutions are not ideal because they do not factor in the source sentence and require dedicated data and fine-tuning to scale to new languages. In this work, we address such limitations by investigating the use of large language models (LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches: one in which LLMs generate sentence-level assessments only, and another, akin to a chain-of-thought approach, where they first produce detailed phrase-level annotations before a sentence-level judgment. Through extensive experiments on multiple languages with five models, both open and proprietary, we show that LLMs can serve as evaluators of GNT. Moreover, we find that prompting for phrase-level annotations before sentence-level assessments consistently improves the accuracy of all models, providing a better and more scalable alternative to current solutions.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v1 Announce Type: new 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA</title>
<link>https://arxiv.org/abs/2504.11972</link>
<guid>https://arxiv.org/abs/2504.11972</guid>
<content:encoded><![CDATA[
arXiv:2504.11972v1 Announce Type: new 
Abstract: Extractive reading comprehension question answering (QA) datasets are typically evaluated using Exact Match (EM) and F1-score, but these metrics often fail to fully capture model performance. With the success of large language models (LLMs), they have been employed in various tasks, including serving as judges (LLM-as-a-judge). In this paper, we reassess the performance of QA models using LLM-as-a-judge across four reading comprehension QA datasets. We examine different families of LLMs and various answer types to evaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show that LLM-as-a-judge is highly correlated with human judgments and can replace traditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human judgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85. These findings confirm that EM and F1 metrics underestimate the true performance of the QA models. While LLM-as-a-judge is not perfect for more difficult answer types (e.g., job), it still outperforms EM/F1, and we observe no bias issues, such as self-preference, when the same model is used for both the QA and judgment tasks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes</title>
<link>https://arxiv.org/abs/2504.11975</link>
<guid>https://arxiv.org/abs/2504.11975</guid>
<content:encoded><![CDATA[
arXiv:2504.11975v1 Announce Type: new 
Abstract: We present the Mu-SHROOM shared task which is focused on detecting hallucinations and other overgeneration mistakes in the output of instruction-tuned large language models (LLMs). Mu-SHROOM addresses general-purpose LLMs in 14 languages, and frames the hallucination detection problem as a span-labeling task. We received 2,618 submissions from 43 participating teams employing diverse methodologies. The large number of submissions underscores the interest of the community in hallucination detection. We present the results of the participating systems and conduct an empirical analysis to identify key factors contributing to strong performance in this task. We also emphasize relevant current challenges, notably the varying degree of hallucinations across languages and the high annotator disagreement when labeling hallucination spans.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems</title>
<link>https://arxiv.org/abs/2504.11986</link>
<guid>https://arxiv.org/abs/2504.11986</guid>
<content:encoded><![CDATA[
arXiv:2504.11986v1 Announce Type: new 
Abstract: This essay proposes an analogy between large language models (LLMs) and quasicrystals: systems that exhibit global coherence without periodic repetition and that are generated through local constraints. While LLMs are often evaluated in terms of predictive accuracy, factuality, or alignment, this structural perspective suggests that their most characteristic behavior is the production of internally resonant linguistic patterns. Just as quasicrystals forced a redefinition of order in physical systems, viewing LLMs as generators of quasi-structured language opens new paths for evaluation and design: privileging propagation of constraint over token-level accuracy, and coherence of form over fixed meaning. LLM outputs should be read not only for what they say, but for the patterns of constraint and coherence that organize them. This shift reframes generative language as a space of emergent patterning: LLMs are neither fully random nor strictly rule-based, but defined by a logic of constraint, resonance, and structural depth.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS</title>
<link>https://arxiv.org/abs/2504.12052</link>
<guid>https://arxiv.org/abs/2504.12052</guid>
<content:encoded><![CDATA[
arXiv:2504.12052v1 Announce Type: new 
Abstract: We present a Bayesian dynamic borrowing (BDB) approach to enhance the quantitative identification of adverse events (AEs) in spontaneous reporting systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior within a Bayesian hierarchical model and incorporates semantic similarity measures (SSMs) to enable weighted information sharing from MedDRA Preferred Terms (PTs) that are clinical similar to the target PT. This continuous similarity-based borrowing addresses limitation of rigid hierarchical grouping in current disproportionality analysis (DPA).
  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015 and 2019, we evalute this approach - termed IC SSM - against standard Information Component (IC) analysis and IC with borrowing at the MedDRA high-level group term (HLGT) level. A novel references set (PVLens), derived from FDA product label updates, enabled prospective evaluation of method performance in identifying AEs prior to official labeling.
  The IC SSM approach demonstrated improved sensitivity compared to both traditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and Youden's index. IC SSM consistently identified more true positives and detected signals over 5 months sooner than traditional IC. Despite a marginally lower aggregate Youden's index, IC SSM showed higher performance in the early post-marketing period, providing more stable and relevant estimates than HLGT-based borrowing and traditional IC.
  These findings support the use of SSM-informed Bayesian borrowing as a scalable and context-aware enhancement to traditional DPA methods. Future research should validate this approach across other datasets and explore additional similarity metrics and Bayesian inference strategies using case-level data.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection</title>
<link>https://arxiv.org/abs/2504.12082</link>
<guid>https://arxiv.org/abs/2504.12082</guid>
<content:encoded><![CDATA[
arXiv:2504.12082v1 Announce Type: new 
Abstract: Hate speech detection is a crucial area of research in natural language processing, essential for ensuring online community safety. However, detecting implicit hate speech, where harmful intent is conveyed in subtle or indirect ways, remains a major challenge. Unlike explicit hate speech, implicit expressions often depend on context, cultural subtleties, and hidden biases, making them more challenging to identify consistently. Additionally, the interpretation of such speech is influenced by external knowledge and demographic biases, resulting in varied detection results across different language models. Furthermore, Large Language Models often show heightened sensitivity to toxic language and references to vulnerable groups, which can lead to misclassifications. This over-sensitivity results in false positives (incorrectly identifying harmless statements as hateful) and false negatives (failing to detect genuinely harmful content). Addressing these issues requires methods that not only improve detection precision but also reduce model biases and enhance robustness. To address these challenges, we propose a novel method, which utilizes in-context learning without requiring model fine-tuning. By adaptively retrieving demonstrations that focus on similar groups or those with the highest similarity scores, our approach enhances contextual comprehension. Experimental results show that our method outperforms current state-of-the-art techniques. Implementation details and code are available at TBD.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gauging Overprecision in LLMs: An Empirical Study</title>
<link>https://arxiv.org/abs/2504.12098</link>
<guid>https://arxiv.org/abs/2504.12098</guid>
<content:encoded><![CDATA[
arXiv:2504.12098v1 Announce Type: new 
Abstract: Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. However, existing approaches prompt the \textit{black box LLMs} to produce their confidence (\textit{verbalized confidence}), which can be subject to many biases and hallucinations. Inspired by a different aspect of overconfidence in cognitive science called \textit{overprecision}, we designed a framework for its study in black box LLMs. This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation. In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence. This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches. We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process. In the refinement phase, answers from the previous phase are refined to generate better answers. The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) {\color{blue}there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions}, {\color{blue}3)} LLM numerical precision differs depending on the task, scale of answer and prompting technique {\color{blue}4) Refinement of answers doesn't improve precision in most cases}. We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation</title>
<link>https://arxiv.org/abs/2504.12108</link>
<guid>https://arxiv.org/abs/2504.12108</guid>
<content:encoded><![CDATA[
arXiv:2504.12108v1 Announce Type: new 
Abstract: The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks. To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold. Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability. Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80\% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Contextualization of Large Language Models for Document-Level Machine Translation</title>
<link>https://arxiv.org/abs/2504.12140</link>
<guid>https://arxiv.org/abs/2504.12140</guid>
<content:encoded><![CDATA[
arXiv:2504.12140v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task</title>
<link>https://arxiv.org/abs/2504.12172</link>
<guid>https://arxiv.org/abs/2504.12172</guid>
<content:encoded><![CDATA[
arXiv:2504.12172v1 Announce Type: new 
Abstract: Arabic poetry is an essential and integral part of Arabic language and culture. It has been used by the Arabs to spot lights on their major events such as depicting brutal battles and conflicts. They also used it, as in many other languages, for various purposes such as romance, pride, lamentation, etc. Arabic poetry has received major attention from linguistics over the decades. One of the main characteristics of Arabic poetry is its special rhythmic structure as opposed to prose. This structure is referred to as a meter. Meters, along with other poetic characteristics, are intensively studied in an Arabic linguistic field called "\textit{Aroud}". Identifying these meters for a verse is a lengthy and complicated process. It also requires technical knowledge in \textit{Aruod}. For recited poetry, it adds an extra layer of processing. Developing systems for automatic identification of poem meters for recited poems need large amounts of labelled data. In this study, we propose a state-of-the-art framework to identify the poem meters of recited Arabic poetry, where we integrate two separate high-resource systems to perform the low-resource task. To ensure generalization of our proposed architecture, we publish a benchmark for this task for future research.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube</title>
<link>https://arxiv.org/abs/2504.12177</link>
<guid>https://arxiv.org/abs/2504.12177</guid>
<content:encoded><![CDATA[
arXiv:2504.12177v1 Announce Type: new 
Abstract: This article analyzes the Hamas-Israel controversy through 253,925 Spanish-language YouTube comments posted between October 2023 and January 2024, following the October 7 attack that escalated the conflict. Adopting an interdisciplinary approach, the study combines the analysis of controversies from Science and Technology Studies (STS) with advanced computational methodologies, specifically Natural Language Processing (NLP) using the BERT (Bidirectional Encoder Representations from Transformers) model. Using this approach, the comments were automatically classified into seven categories, reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli positions, among others. The results show a predominance of pro- Palestinian comments, although pro-Israeli and anti-Palestinian comments received more "likes." This study also applies the agenda-setting theory to demonstrate how media coverage significantly influences public perception, observing a notable shift in public opinion, transitioning from a pro- Palestinian stance to a more critical position towards Israel. This work highlights the importance of combining social science perspectives with technological tools in the analysis of controversies, presenting a methodological innovation by integrating computational analysis with critical social theories to address complex public opinion phenomena and media narratives.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification</title>
<link>https://arxiv.org/abs/2504.12180</link>
<guid>https://arxiv.org/abs/2504.12180</guid>
<content:encoded><![CDATA[
arXiv:2504.12180v1 Announce Type: new 
Abstract: One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications.
  The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar.
  These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data</title>
<link>https://arxiv.org/abs/2504.12185</link>
<guid>https://arxiv.org/abs/2504.12185</guid>
<content:encoded><![CDATA[
arXiv:2504.12185v1 Announce Type: new 
Abstract: In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data. To address this problem, we propose SALAD}(Structure Aware and LLM-driven Augmented Data), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning. Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns. By applying contrastive learning, SALAD enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations. We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference. The results demonstrate that SALAD not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure</title>
<link>https://arxiv.org/abs/2504.12187</link>
<guid>https://arxiv.org/abs/2504.12187</guid>
<content:encoded><![CDATA[
arXiv:2504.12187v1 Announce Type: new 
Abstract: It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.12216</link>
<guid>https://arxiv.org/abs/2504.12216</guid>
<content:encoded><![CDATA[
arXiv:2504.12216v1 Announce Type: new 
Abstract: Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BitNet b1.58 2B4T Technical Report</title>
<link>https://arxiv.org/abs/2504.12285</link>
<guid>https://arxiv.org/abs/2504.12285</guid>
<content:encoded><![CDATA[
arXiv:2504.12285v1 Announce Type: new 
Abstract: We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Conceptual Data Models to Multimodal Representation</title>
<link>https://arxiv.org/abs/2504.11459</link>
<guid>https://arxiv.org/abs/2504.11459</guid>
<content:encoded><![CDATA[
arXiv:2504.11459v1 Announce Type: cross 
Abstract: 1) Introduction and Conceptual Framework: This document explores the concept of information design by dividing it into two major practices: defining the meaning of a corpus of textual data and its visual or multimodal representation. It draws on expertise in enriching textual corpora, particularly audiovisual ones, and transforming them into multiple narrative formats. The text highlights a crucial distinction between the semantic content of a domain and the modalities of its graphic expression, illustrating this approach with concepts rooted in structural semiotics and linguistics traditions.
  2) Modeling and Conceptual Design:  The article emphasizes the importance of semantic modeling, often achieved through conceptual networks or graphs. These tools enable the structuring of knowledge within a domain by accounting for relationships between concepts, contexts of use, and specific objectives. Stockinger also highlights the constraints and challenges involved in creating dynamic and adaptable models, integrating elements such as thesauri or interoperable ontologies to facilitate the analysis and publication of complex corpora.
  3) Applications and Multimodal Visualization:  The text concludes by examining the practical application of these models in work environments like OKAPI, developed to analyze, publish, and reuse audiovisual data. It also discusses innovative approaches such as visual storytelling and document reengineering, which involve transforming existing content into new resources tailored to various contexts. These methods emphasize interoperability, flexibility, and the intelligence of communication systems, paving the way for richer and more collaborative use of digital data. The content of this document was presented during the "Semiotics of Information Design" Day organized by Anne Beyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on June 21, 2018, in Bordeaux.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Matters: Multimodal Features for Affective Analysis</title>
<link>https://arxiv.org/abs/2504.11460</link>
<guid>https://arxiv.org/abs/2504.11460</guid>
<content:encoded><![CDATA[
arXiv:2504.11460v1 Announce Type: cross 
Abstract: In this study, we present our methodology for two tasks: the Behavioural Ambivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry Intensity (EMI) Estimation Challenge, both conducted as part of the 8th Workshop and Competition on Affective & Behavior Analysis in-the-wild. Building on previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast dataset to extract various audio features, capturing both linguistic and paralinguistic information. Our approach incorporates a valence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like encoder, and a vision transformer (ViT) with predictions subsequently processed through a long short-term memory (LSTM) architecture for temporal modeling. In this iteration, we integrate the textual and visual modality into our analysis, recognizing that semantic content provides valuable contextual cues and underscoring that the meaning of speech often conveys more critical insights than its acoustic counterpart alone. Fusing in the vision modality helps in some cases to interpret the textual modality more precisely. This combined approach yields significant performance improvements over baseline methods.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language and Knowledge Representation: A Stratified Approach</title>
<link>https://arxiv.org/abs/2504.11492</link>
<guid>https://arxiv.org/abs/2504.11492</guid>
<content:encoded><![CDATA[
arXiv:2504.11492v1 Announce Type: cross 
Abstract: The thesis proposes the problem of representation heterogeneity to emphasize the fact that heterogeneity is an intrinsic property of any representation, wherein, different observers encode different representations of the same target reality in a stratified manner using different concepts, language and knowledge (as well as data). The thesis then advances a top-down solution approach to the above stratified problem of representation heterogeneity in terms of several solution components, namely: (i) a representation formalism stratified into concept level, language level, knowledge level and data level to accommodate representation heterogeneity, (ii) a top-down language representation using Universal Knowledge Core (UKC), UKC namespaces and domain languages to tackle the conceptual and language level heterogeneity, (iii) a top-down knowledge representation using the notions of language teleontology and knowledge teleontology to tackle the knowledge level heterogeneity, (iv) the usage and further development of the existing LiveKnowledge catalog for enforcing iterative reuse and sharing of language and knowledge representations, and, (v) the kTelos methodology integrating the solution components above to iteratively generate the language and knowledge representations absolving representation heterogeneity. The thesis also includes proof-of-concepts of the language and knowledge representations developed for two international research projects - DataScientia (data catalogs) and JIDEP (materials modelling). Finally, the thesis concludes with future lines of research.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment</title>
<link>https://arxiv.org/abs/2504.11515</link>
<guid>https://arxiv.org/abs/2504.11515</guid>
<content:encoded><![CDATA[
arXiv:2504.11515v1 Announce Type: cross 
Abstract: Predicting personality traits automatically has become a challenging problem in computer vision. This paper introduces an innovative multimodal feature learning framework for personality analysis in short video clips. For visual processing, we construct a facial graph and design a Geo-based two-stream network incorporating an attention mechanism, leveraging both Graph Convolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture static facial expressions. Additionally, ResNet18 and VGGFace networks are employed to extract global scene and facial appearance features at the frame level. To capture dynamic temporal information, we integrate a BiGRU with a temporal attention module for extracting salient frame representations. To enhance the model's robustness, we incorporate the VGGish CNN for audio-based features and XLM-Roberta for text-based features. Finally, a multimodal channel attention mechanism is introduced to integrate different modalities, and a Multi-Layer Perceptron (MLP) regression model is used to predict personality traits. Experimental results confirm that our proposed framework surpasses existing state-of-the-art approaches in performance.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation</title>
<link>https://arxiv.org/abs/2504.11524</link>
<guid>https://arxiv.org/abs/2504.11524</guid>
<content:encoded><![CDATA[
arXiv:2504.11524v1 Announce Type: cross 
Abstract: There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods. Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data. However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns. Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses. These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphicBench: A Planning Benchmark for Graphic Design with Language Agents</title>
<link>https://arxiv.org/abs/2504.11571</link>
<guid>https://arxiv.org/abs/2504.11571</guid>
<content:encoded><![CDATA[
arXiv:2504.11571v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2504.11739</link>
<guid>https://arxiv.org/abs/2504.11739</guid>
<content:encoded><![CDATA[
arXiv:2504.11739v1 Announce Type: cross 
Abstract: The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce \textbf{RAPO}, a novel \textbf{R}etrieval-\textbf{A}ugmented \textbf{P}rompt \textbf{O}ptimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts. Project website: \href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?</title>
<link>https://arxiv.org/abs/2504.11741</link>
<guid>https://arxiv.org/abs/2504.11741</guid>
<content:encoded><![CDATA[
arXiv:2504.11741v1 Announce Type: cross 
Abstract: Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Goal-Directedness of Large Language Models</title>
<link>https://arxiv.org/abs/2504.11844</link>
<guid>https://arxiv.org/abs/2504.11844</guid>
<content:encoded><![CDATA[
arXiv:2504.11844v1 Announce Type: cross 
Abstract: To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach</title>
<link>https://arxiv.org/abs/2504.11889</link>
<guid>https://arxiv.org/abs/2504.11889</guid>
<content:encoded><![CDATA[
arXiv:2504.11889v1 Announce Type: cross 
Abstract: Existing large language model LLM-based recommendation methods face several challenges, including inefficiency in handling large candidate pools, sensitivity to item order within prompts ("lost in the middle" phenomenon) poor scalability, and unrealistic evaluation due to random negative sampling. To address these issues, we propose a Query-to-Recommendation approach that leverages LLMs to generate personalized queries for retrieving relevant items from the entire candidate pool, eliminating the need for candidate pre-selection. This method can be integrated into an ID-based recommendation system without additional training, enhances recommendation performance and diversity through LLMs' world knowledge, and performs well even for less popular item groups. Experiments on three datasets show up to 57 percent improvement, with an average gain of 31 percent, demonstrating strong zero-shot performance and further gains when ensembled with existing models.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation</title>
<link>https://arxiv.org/abs/2504.11942</link>
<guid>https://arxiv.org/abs/2504.11942</guid>
<content:encoded><![CDATA[
arXiv:2504.11942v1 Announce Type: cross 
Abstract: Current sign language machine translation systems rely on recognizing hand movements, facial expressions and body postures, and natural language processing, to convert signs into text. Recent approaches use Transformer architectures to model long-range dependencies via positional encoding. However, they lack accuracy in recognizing fine-grained, short-range temporal dependencies between gestures captured at high frame rates. Moreover, their high computational complexity leads to inefficient training. To mitigate these issues, we propose an Adaptive Transformer (ADAT), which incorporates components for enhanced feature extraction and adaptive feature weighting through a gating mechanism to emphasize contextually relevant features while reducing training overhead and maintaining translation accuracy. To evaluate ADAT, we introduce MedASL, the first public medical American Sign Language dataset. In sign-to-gloss-to-text experiments, ADAT outperforms the encoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing training time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text experiments, it improves accuracy by 8.7% and reduces training time by 2.8% on PHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on MedASL. Compared to encoder-only and decoder-only baselines in sign-to-text, ADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its dual-stream structure.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -</title>
<link>https://arxiv.org/abs/2504.12137</link>
<guid>https://arxiv.org/abs/2504.12137</guid>
<content:encoded><![CDATA[
arXiv:2504.12137v1 Announce Type: cross 
Abstract: Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided. To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time. By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations. Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training. We evaluate our method on several benchmark datasets and across different LVLMs. Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Needs Input Repetition Masking</title>
<link>https://arxiv.org/abs/2504.12229</link>
<guid>https://arxiv.org/abs/2504.12229</guid>
<content:encoded><![CDATA[
arXiv:2504.12229v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning</title>
<link>https://arxiv.org/abs/2504.12254</link>
<guid>https://arxiv.org/abs/2504.12254</guid>
<content:encoded><![CDATA[
arXiv:2504.12254v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dysarthria Normalization via Local Lie Group Transformations for Robust ASR</title>
<link>https://arxiv.org/abs/2504.12279</link>
<guid>https://arxiv.org/abs/2504.12279</guid>
<content:encoded><![CDATA[
arXiv:2504.12279v1 Announce Type: cross 
Abstract: We present a geometry-driven method for normalizing dysarthric speech using local Lie group transformations of spectrograms. Time, frequency, and amplitude distortions are modeled as smooth, invertible deformations, parameterized by scalar fields and applied via exponential maps. A neural network is trained to infer these fields from synthetic distortions of typical speech-without using any pathological data. At test time, the model applies an approximate inverse to real dysarthric inputs. Despite zero-shot generalization, we observe substantial ASR gains, including up to 16 percentage points WER reduction on challenging TORGO samples, with no degradation on clean speech. This work introduces a principled, interpretable approach for robust speech recognition under motor speech disorders
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph Reasoning with Self-supervised Reinforcement Learning</title>
<link>https://arxiv.org/abs/2405.13640</link>
<guid>https://arxiv.org/abs/2405.13640</guid>
<content:encoded><![CDATA[
arXiv:2405.13640v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is an effective method of finding reasoning pathways in incomplete knowledge graphs (KGs). To overcome the challenges of a large action space, a self-supervised pre-training method is proposed to warm up the policy network before the RL training stage. To alleviate the distributional mismatch issue in general self-supervised RL (SSRL), in our supervised learning (SL) stage, the agent selects actions based on the policy network and learns from generated labels; this self-generation of labels is the intuition behind the name self-supervised. With this training framework, the information density of our SL objective is increased and the agent is prevented from getting stuck with the early rewarded paths. Our self-supervised RL (SSRL) method improves the performance of RL by pairing it with the wide coverage achieved by SL during pretraining, since the breadth of the SL objective makes it infeasible to train an agent with that alone. We show that our SSRL model meets or exceeds current state-of-the-art results on all Hits@k and mean reciprocal rank (MRR) metrics on four large benchmark KG datasets. This SSRL method can be used as a plug-in for any RL architecture for a KGR task. We adopt two RL architectures, i.e., MINERVA and MultiHopKG as our baseline RL models and experimentally show that our SSRL model consistently outperforms both baselines on all of these four KG reasoning tasks. Full code for the paper available at https://github.com/owenonline/Knowledge-Graph-Reasoning-with-Self-supervised-Reinforcement-Learning.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection</title>
<link>https://arxiv.org/abs/2407.12879</link>
<guid>https://arxiv.org/abs/2407.12879</guid>
<content:encoded><![CDATA[
arXiv:2407.12879v4 Announce Type: replace 
Abstract: Large visual-language models (LVLMs) exhibit exceptional performance in visual-language reasoning across diverse cross-modal benchmarks. Despite these advances, recent research indicates that Large Language Models (LLMs), like GPT-3.5-turbo, underachieve compared to well-trained smaller models, such as BERT, in Fake News Detection (FND), prompting inquiries into LVLMs' efficacy in FND tasks. Although performance could improve through fine-tuning LVLMs, the substantial parameters and requisite pre-trained weights render it a resource-heavy endeavor for FND applications. This paper initially assesses the FND capabilities of two notable LVLMs, CogVLM and GPT4V, in comparison to a smaller yet adeptly trained CLIP model in a zero-shot context. The findings demonstrate that LVLMs can attain performance competitive with that of the smaller model. Next, we integrate standard in-context learning (ICL) with LVLMs, noting improvements in FND performance, though limited in scope and consistency. To address this, we introduce the \textbf{I}n-context \textbf{M}ultimodal \textbf{F}ake \textbf{N}ews \textbf{D}etection (IMFND) framework, enriching in-context examples and test inputs with predictions and corresponding probabilities from a well-trained smaller model. This strategic integration directs the LVLMs' focus towards news segments associated with higher probabilities, thereby improving their analytical accuracy. The experimental results suggest that the IMFND framework significantly boosts the FND efficiency of LVLMs, achieving enhanced accuracy over the standard ICL approach across three publicly available FND datasets.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Application of AI-based Models for Online Fraud Detection and Analysis</title>
<link>https://arxiv.org/abs/2409.19022</link>
<guid>https://arxiv.org/abs/2409.19022</guid>
<content:encoded><![CDATA[
arXiv:2409.19022v2 Announce Type: replace 
Abstract: Fraud is a prevalent offence that extends beyond financial loss, causing psychological and physical harm to victims. The advancements in online communication technologies alowed for online fraud to thrive in this vast network, with fraudsters increasingly using these channels for deception. With the progression of technologies like AI, there is a growing concern that fraud will scale up, using sophisticated methods, like deep-fakes in phishing campaigns, all generated by language generation models like ChatGPT. However, the application of AI in detecting and analyzing online fraud remains understudied. We conduct a Systematic Literature Review on AI and NLP techniques for online fraud detection. The review adhered the PRISMA-ScR protocol, with eligibility criteria including relevance to online fraud, use of text data, and AI methodologies. We screened 2,457 academic records, 350 met our eligibility criteria, and included 223. We report the state-of-the-art NLP techniques for analysing various online fraud categories; the training data sources; the NLP algorithms and models built; and the performance metrics employed for model evaluation. We find that current research on online fraud is divided into various scam activitiesand identify 16 different frauds that researchers focus on. This SLR enhances the academic understanding of AI-based detection methods for online fraud and offers insights for policymakers, law enforcement, and businesses on safeguarding against such activities. We conclude that focusing on specific scams lacks generalization, as multiple models are required for different fraud types. The evolving nature of scams limits the effectiveness of models trained on outdated data. We also identify issues in data limitations, training bias reporting, and selective presentation of metrics in model performance reporting, which can lead to potential biases in model evaluation.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement</title>
<link>https://arxiv.org/abs/2410.02108</link>
<guid>https://arxiv.org/abs/2410.02108</guid>
<content:encoded><![CDATA[
arXiv:2410.02108v2 Announce Type: replace 
Abstract: Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a method to self-synthesize reasoning paths as post-training data by progressing from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct in-depth analysis of our framework and show ReGenesis is effective across various LLMs and design choices.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.17088</link>
<guid>https://arxiv.org/abs/2410.17088</guid>
<content:encoded><![CDATA[
arXiv:2410.17088v2 Announce Type: replace 
Abstract: A vast amount of scholarly work is published daily, yet much of it remains inaccessible to the general public due to dense jargon and complex language. To address this challenge in science communication, we introduce a reinforcement learning framework that fine-tunes a language model to rewrite scholarly abstracts into more comprehensible versions. Guided by a carefully balanced combination of word- and sentence-level accessibility rewards, our language model effectively substitutes technical terms with more accessible alternatives, a task which models supervised fine-tuned or guided by conventional readability measures struggle to accomplish. Our best model adjusts the readability level of scholarly abstracts by approximately six U.S. grade levels -- in other words, from a postgraduate to a high school level. This translates to roughly a 90% relative boost over the supervised fine-tuning baseline, all while maintaining factual accuracy and high-quality language. An in-depth analysis of our approach shows that balanced rewards lead to systematic modifications in the base model, likely contributing to smoother optimization and superior performance. We envision this work as a step toward bridging the gap between scholarly research and the general public, particularly younger readers and those without a college degree.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings</title>
<link>https://arxiv.org/abs/2411.05986</link>
<guid>https://arxiv.org/abs/2411.05986</guid>
<content:encoded><![CDATA[
arXiv:2411.05986v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been proven to be an effective and robust method for training neural machine translation systems, especially when paired with powerful reward models that accurately assess translation quality. However, most research has focused on RL methods that use sentence-level feedback, leading to inefficient learning signals due to the reward sparsity problem -- the model receives a single score for the entire sentence. To address this, we propose a novel approach that leverages fine-grained, token-level quality assessments along with error severity levels using RL methods. Specifically, we use xCOMET, a state-of-the-art quality estimation system, as our token-level reward model. We conduct experiments on small and large translation datasets with standard encoder-decoder and large language models-based machine translation systems, comparing the impact of sentence-level versus fine-grained reward signals on translation quality. Our results show that training with token-level rewards improves translation quality across language pairs over baselines according to both automatic and human evaluation. Furthermore, token-level reward optimization improves training stability, evidenced by a steady increase in mean rewards over training epochs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework</title>
<link>https://arxiv.org/abs/2411.16707</link>
<guid>https://arxiv.org/abs/2411.16707</guid>
<content:encoded><![CDATA[
arXiv:2411.16707v2 Announce Type: replace 
Abstract: The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence-Level Leakage Risk of Training Data in Large Language Models</title>
<link>https://arxiv.org/abs/2412.11302</link>
<guid>https://arxiv.org/abs/2412.11302</guid>
<content:encoded><![CDATA[
arXiv:2412.11302v3 Announce Type: replace 
Abstract: This work quantifies the risk of training data leakage from LLMs (Large Language Models) using sequence-level probabilities. Computing extraction probabilities for individual sequences provides finer-grained information than has been studied in prior benchmarking work. We re-analyze the effects of decoding schemes, model sizes, prefix lengths, partial sequence leakages, and token positions to uncover new insights that were not possible in previous works due to their choice of metrics. We perform this study on two pre-trained models, Llama and OPT, trained on the Common Crawl and The Pile respectively. We discover that 1) Extraction Rate, the predominant metric used in prior quantification work, underestimates the threat of leakage of training data in randomized LLMs by as much as 2.14X. 2) Although on average, larger models and longer prefixes can extract more data, this is not true for a substantial portion of individual sequences. 30.4-41.5% of our sequences are easier to extract with either shorter prefixes or smaller models. 3) Contrary to previous beliefs, partial leakage in commonly used decoding schemes like top-k and top-p is not easier than leaking verbatim training data. The aim of this work is to encourage the adoption of this metric for future work on quantification of training data extraction.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models</title>
<link>https://arxiv.org/abs/2412.12144</link>
<guid>https://arxiv.org/abs/2412.12144</guid>
<content:encoded><![CDATA[
arXiv:2412.12144v3 Announce Type: replace 
Abstract: Personality assessment, particularly through situational judgment tests (SJTs), is a vital tool for psychological research, talent selection, and educational evaluation. This study explores the potential of GPT-4, a state-of-the-art large language model (LLM), to automate the generation of personality situational judgment tests (PSJTs) in Chinese. Traditional SJT development is labor-intensive and prone to biases, while GPT-4 offers a scalable, efficient alternative. Two studies were conducted: Study 1 evaluated the impact of prompt design and temperature settings on content validity, finding that optimized prompts with a temperature of 1.0 produced creative and accurate items. Study 2 assessed the psychometric properties of GPT-4-generated PSJTs, revealing that they demonstrated satisfactory reliability and validity, surpassing the performance of manually developed tests in measuring the Big Five personality traits. This research highlights GPT-4's effectiveness in developing high-quality PSJTs, providing a scalable and innovative method for psychometric test development. These findings expand the possibilities of automatic item generation and the application of LLMs in psychology, and offer practical implications for streamlining test development processes in resource-limited settings.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy</title>
<link>https://arxiv.org/abs/2501.12537</link>
<guid>https://arxiv.org/abs/2501.12537</guid>
<content:encoded><![CDATA[
arXiv:2501.12537v2 Announce Type: replace 
Abstract: The increased screen time and isolation caused by the COVID-19 pandemic have led to a significant surge in cases of online grooming, which is the use of strategies by predators to lure children into sexual exploitation. Previous efforts to detect grooming in industry and academia have involved accessing and monitoring private conversations through centrally-trained models or sending private conversations to a global server. In this work, we implement a privacy-preserving pipeline for the early detection of sexual predators. We leverage federated learning and differential privacy in order to create safer online spaces for children while respecting their privacy. We investigate various privacy-preserving implementations and discuss their benefits and shortcomings. Our extensive evaluation using real-world data proves that privacy and utility can coexist with only a slight reduction in utility.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Theory of Mind Enables the Invention of Proto-Writing</title>
<link>https://arxiv.org/abs/2502.01568</link>
<guid>https://arxiv.org/abs/2502.01568</guid>
<content:encoded><![CDATA[
arXiv:2502.01568v3 Announce Type: replace 
Abstract: Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Inclusively do LMs Perceive Social and Moral Norms?</title>
<link>https://arxiv.org/abs/2502.02696</link>
<guid>https://arxiv.org/abs/2502.02696</guid>
<content:encoded><![CDATA[
arXiv:2502.02696v2 Announce Type: replace 
Abstract: This paper discusses and contains offensive content. Language models (LMs) are used in decision-making systems and as interactive assistants. However, how well do these models making judgements align with the diversity of human values, particularly regarding social and moral norms? In this work, we investigate how inclusively LMs perceive norms across demographic groups (e.g., gender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare their outputs with the existing responses of 100 human annotators. We introduce the Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on ordinal questions. We find notable disparities in LM responses, with younger, higher-income groups showing closer alignment, raising concerns about the representation of marginalized perspectives. Our findings highlight the importance of further efforts to make LMs more inclusive of diverse human values. The code and prompts are available on GitHub under the CC BY-NC 4.0 license.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation</title>
<link>https://arxiv.org/abs/2502.05151</link>
<guid>https://arxiv.org/abs/2502.05151</guid>
<content:encoded><![CDATA[
arXiv:2502.05151v2 Announce Type: replace 
Abstract: With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of "AI4Science".
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Input Rewriting Improves Translation with Large Language Models</title>
<link>https://arxiv.org/abs/2502.16682</link>
<guid>https://arxiv.org/abs/2502.16682</guid>
<content:encoded><![CDATA[
arXiv:2502.16682v2 Announce Type: replace 
Abstract: Can we improve machine translation (MT) with LLMs by rewriting their inputs automatically? Users commonly rely on the intuition that well-written text is easier to translate when using off-the-shelf MT systems. LLMs can rewrite text in many ways but in the context of MT, these capabilities have been primarily exploited to rewrite outputs via post-editing. We present an empirical study of 21 input rewriting methods with 3 open-weight LLMs for translating from English into 6 target languages. We show that text simplification is the most effective MT-agnostic rewrite strategy and that it can be improved further when using quality estimation to assess translatability. Human evaluation further confirms that simplified rewrites and their MT outputs both largely preserve the original meaning of the source and MT. These results suggest LLM-assisted input rewriting as a promising direction for improving translations.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Figurative Archive: an open dataset and web-based application for the study of metaphor</title>
<link>https://arxiv.org/abs/2503.00444</link>
<guid>https://arxiv.org/abs/2503.00444</guid>
<content:encoded><![CDATA[
arXiv:2503.00444v2 Announce Type: replace 
Abstract: Research on metaphor has steadily increased over the last decades, as this phenomenon opens a window into a range of linguistic and cognitive processes. At the same time, the demand for rigorously constructed and extensively normed experimental materials increased as well. Here, we present the Figurative Archive, an open database of 997 metaphors in Italian enriched with rating and corpus-based measures (from familiarity to concreteness), derived by collecting stimuli used across 11 studies. It includes both everyday and literary metaphors, varying in structure and semantic domains, and is validated based on correlations between familiarity and other measures. The archive has several aspects of novelty: it is increased in size compared to previous resources; it includes a measure of inclusiveness, to comply with recommendations for non-discriminatory language use; it is displayed in a web-based interface, with features for a customized consultation. We provide guidelines for using the archive as a source of material for studies investigating metaphor processing and the relationships between metaphor features in humans and computational models.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
arXiv:2503.17287v2 Announce Type: replace 
Abstract: Improving the training efficiency remains one of the most significant challenges in large-scale reinforcement learning. In this paper, we investigate how the model's context length and the complexity of the training dataset influence the training process of R1-like models. Our experiments reveal three key insights: (1) adopting longer context lengths may not necessarily result in better performance; (2) selecting an appropriate context length helps mitigate entropy collapse; and (3) appropriately controlling the model's context length and curating training data based on input prompt length can effectively improve RL training efficiency, achieving better performance with shorter thinking length. Inspired by these insights, we propose FastCuRL, a curriculum reinforcement learning framework with the progressive context extension strategy, and successfully accelerate the training process of RL models. Experimental results demonstrate that FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview across all five benchmarks while only utilizing 50\% of training steps. Furthermore, all training stages for FastCuRL-1.5B-Preview are completed using a single node with 8 GPUs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models</title>
<link>https://arxiv.org/abs/2503.24235</link>
<guid>https://arxiv.org/abs/2503.24235</guid>
<content:encoded><![CDATA[
arXiv:2503.24235v2 Announce Type: replace 
Abstract: As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing how hyperparameters impact Large Language Models' sarcasm detection performance</title>
<link>https://arxiv.org/abs/2504.06166</link>
<guid>https://arxiv.org/abs/2504.06166</guid>
<content:encoded><![CDATA[
arXiv:2504.06166v2 Announce Type: replace 
Abstract: Sarcasm detection is challenging for both humans and machines. This work explores how model characteristics impact sarcasm detection in OpenAI's GPT, and Meta's Llama-2 models, given their strong natural language understanding, and popularity. We evaluate fine-tuned and zero-shot models across various sizes, releases, and hyperparameters. Experiments were conducted on the political and balanced (pol-bal) portion of the popular Self-Annotated Reddit Corpus (SARC2.0) sarcasm dataset. Fine-tuned performance improves monotonically with model size within a model family, while hyperparameter tuning also impacts performance. In the fine-tuning scenario, full precision Llama-2-13b achieves state-of-the-art accuracy and $F_1$-score, both measured at 0.83, comparable to average human performance. In the zero-shot setting, one GPT-4 model achieves competitive performance to prior attempts, yielding an accuracy of 0.70 and an $F_1$-score of 0.75. Furthermore, a model's performance may increase or decline with each release, highlighting the need to reassess performance after each release.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification</title>
<link>https://arxiv.org/abs/2504.09394</link>
<guid>https://arxiv.org/abs/2504.09394</guid>
<content:encoded><![CDATA[
arXiv:2504.09394v2 Announce Type: replace 
Abstract: Despite the successes of language models, their evaluation remains a daunting challenge for new and existing tasks. We consider the task of text simplification, commonly used to improve information accessibility, where evaluation faces two major challenges. First, the data in existing benchmarks might not reflect the capabilities of current language models on the task, often containing disfluent, incoherent, or simplistic examples. Second, existing human ratings associated with the benchmarks often contain a high degree of disagreement, resulting in inconsistent ratings; nevertheless, existing metrics still have to show higher correlations with these imperfect ratings. As a result, evaluation for the task is not reliable and does not reflect expected trends (e.g., more powerful models being assigned higher scores). We address these challenges for the task of text simplification through three contributions. First, we introduce SynthSimpliEval, a synthetic benchmark for text simplification featuring simplified sentences generated by models of varying sizes. Through a pilot study, we show that human ratings on our benchmark exhibit high inter-annotator agreement and reflect the expected trend: larger models produce higher-quality simplifications. Second, we show that auto-evaluation with a panel of LLM judges (LLMs-as-a-jury) often suffices to obtain consistent ratings for the evaluation of text simplification. Third, we demonstrate that existing learnable metrics for text simplification benefit from training on our LLMs-as-a-jury-rated synthetic data, closing the gap with pure LLMs-as-a-jury for evaluation. Overall, through our case study on text simplification, we show that a reliable evaluation requires higher quality test data, which could be obtained through synthetic data and LLMs-as-a-jury ratings.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution</title>
<link>https://arxiv.org/abs/2504.09566</link>
<guid>https://arxiv.org/abs/2504.09566</guid>
<content:encoded><![CDATA[
arXiv:2504.09566v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting enhances the reasoning of large language models (LLMs) by decomposing problems into sequential steps, mimicking human logic and reducing errors. However, complex tasks with vast solution spaces and vague constraints often exceed the capacity of a single reasoning chain. Inspired by Minimal Free Resolution (MFR) in commutative algebra and algebraic geometry, we propose Syzygy of Thoughts (SoT)-a novel framework that extends CoT by introducing auxiliary, interrelated reasoning paths. SoT captures deeper logical dependencies, enabling more robust and structured problem-solving. MFR decomposes a module into a sequence of free modules with minimal rank, providing a structured analytical approach to complex systems. This method introduces the concepts of "Module", "Betti numbers","Freeness", "Mapping", "Exactness" and "Minimality", enabling the systematic decomposition of the original complex problem into logically complete minimal subproblems while preserving key problem features and reducing reasoning length. We tested SoT across diverse datasets (e.g., GSM8K, MATH) and models (e.g., GPT-4o-mini, Qwen2.5), achieving inference accuracy that matches or surpasses mainstream CoTs standards. Additionally, by aligning the sampling process with algebraic constraints, our approach enhances the scalability of inference time in LLMs, ensuring both transparent reasoning and high performance. Our code will be publicly available at https://github.com/dlMARiA/Syzygy-of-thoughts.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks</title>
<link>https://arxiv.org/abs/2504.10185</link>
<guid>https://arxiv.org/abs/2504.10185</guid>
<content:encoded><![CDATA[
arXiv:2504.10185v2 Announce Type: replace 
Abstract: Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a "coreset"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https://github.com/OPTML-Group/MU-Coreset.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs</title>
<link>https://arxiv.org/abs/2504.10982</link>
<guid>https://arxiv.org/abs/2504.10982</guid>
<content:encoded><![CDATA[
arXiv:2504.10982v2 Announce Type: replace 
Abstract: Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Python Translation</title>
<link>https://arxiv.org/abs/2504.11290</link>
<guid>https://arxiv.org/abs/2504.11290</guid>
<content:encoded><![CDATA[
arXiv:2504.11290v2 Announce Type: replace 
Abstract: Python is one of the most commonly used programming languages in industry and education. Its English keywords and built-in functions/modules allow it to come close to pseudo-code in terms of its readability and ease of writing. However, those who do not speak English may not experience these advantages. In fact, they may even be hindered in their ability to understand Python code, as the English nature of its terms creates an additional layer of overhead. To that end, we introduce the task of automatically translating Python's natural modality (keywords, error types, identifiers, etc.) into other human languages. This presents a unique challenge, considering the abbreviated nature of these forms, as well as potential untranslatability of advanced mathematical/programming concepts across languages. We therefore create an automated pipeline to translate Python into other human languages, comparing strategies using machine translation and large language models. We then use this pipeline to acquire translations from five common Python libraries (pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a quality test on a subset of these terms in French, Greek, and Bengali. We hope this will provide a clearer path forward towards creating a universal Python, accessible to anyone regardless of nationality or language background.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Local Grammar-Based Coding Revisited</title>
<link>https://arxiv.org/abs/2209.13636</link>
<guid>https://arxiv.org/abs/2209.13636</guid>
<content:encoded><![CDATA[
arXiv:2209.13636v3 Announce Type: replace-cross 
Abstract: In the setting of minimal local grammar-based coding, the input string is represented as a grammar with the minimal output length defined via simple symbol-by-symbol encoding. This paper discusses four contributions to this field. First, we invoke a simple harmonic bound on ranked probabilities, which reminds Zipf's law and simplifies universality proofs for minimal local grammar-based codes. Second, we refine known bounds on the vocabulary size, showing its partial power-law equivalence with mutual information and redundancy. These bounds are relevant for linking Zipf's law with the neural scaling law for large language models. Third, we develop a framework for universal codes with fixed infinite vocabularies, recasting universal coding as matching ranked patterns that are independent of empirical data. Finally, we analyze grammar-based codes with finite vocabularies being empirical rank lists, proving that that such codes are also universal. These results extend foundations of universal grammar-based coding and reaffirm previously stated connections to power laws for human language and language models.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text</title>
<link>https://arxiv.org/abs/2403.14773</link>
<guid>https://arxiv.org/abs/2403.14773</guid>
<content:encoded><![CDATA[
arXiv:2403.14773v2 Announce Type: replace-cross 
Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene, and (iii) a randomized blending approach that enables to apply a video enhancer autoregressively for infinitely long videos without inconsistencies between chunks. Experiments show that StreamingT2V generates high motion amount. In contrast, all competing image-to-video methods are prone to video stagnation when applied naively in an autoregressive manner. Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator that outperforms competitors with consistency and motion. Our code will be available at: https://github.com/Picsart-AI-Research/StreamingT2V
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Data and Transformers for Audio Generation</title>
<link>https://arxiv.org/abs/2406.19388</link>
<guid>https://arxiv.org/abs/2406.19388</guid>
<content:encoded><![CDATA[
arXiv:2406.19388v4 Announce Type: replace-cross 
Abstract: The scalability of ambient sound generators is hindered by data scarcity, insufficient caption quality, and limited scalability in model architecture. This work addresses these challenges by advancing both data and model scaling. First, we propose an efficient and scalable dataset collection pipeline tailored for ambient audio generation, resulting in AutoReCap-XL, the largest ambient audio-text dataset with over 47 million clips. To provide high-quality textual annotations, we propose AutoCap, a high-quality automatic audio captioning model. By adopting a Q-Former module and leveraging audio metadata, AutoCap substantially enhances caption quality, reaching a CIDEr score of $83.2$, a $3.2\%$ improvement over previous captioning models. Finally, we propose GenAu, a scalable transformer-based audio generation architecture that we scale up to 1.25B parameters. We demonstrate its benefits from data scaling with synthetic captions as well as model size scaling. When compared to baseline audio generators trained at similar size and data scale, GenAu obtains significant improvements of $4.7\%$ in FAD score, $11.1\%$ in IS, and $13.5\%$ in CLAP score. Our code, model checkpoints, and dataset are publicly available.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Outlines for Code: Literate Programming in the LLM Era</title>
<link>https://arxiv.org/abs/2408.04820</link>
<guid>https://arxiv.org/abs/2408.04820</guid>
<content:encoded><![CDATA[
arXiv:2408.04820v3 Announce Type: replace-cross 
Abstract: We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL: a developer can change one and the LLM automatically updates the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and malware detection.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)</title>
<link>https://arxiv.org/abs/2409.02920</link>
<guid>https://arxiv.org/abs/2409.02920</guid>
<content:encoded><![CDATA[
arXiv:2409.02920v3 Announce Type: replace-cross 
Abstract: In the rapidly advancing field of robotics, dual-arm coordination and complex object manipulation are essential capabilities for developing advanced autonomous systems. However, the scarcity of diverse, high-quality demonstration data and real-world-aligned evaluation benchmarks severely limits such development. To address this, we introduce RoboTwin, a generative digital twin framework that uses 3D generative foundation models and large language models to produce diverse expert datasets and provide a real-world-aligned evaluation platform for dual-arm robotic tasks. Specifically, RoboTwin creates varied digital twins of objects from single 2D images, generating realistic and interactive scenarios. It also introduces a spatial relation-aware code generation framework that combines object annotations with large language models to break down tasks, determine spatial constraints, and generate precise robotic movement code. Our framework offers a comprehensive benchmark with both simulated and real-world data, enabling standardized evaluation and better alignment between simulated training and real-world performance. We validated our approach using the open-source COBOT Magic Robot platform. Policies pre-trained on RoboTwin-generated data and fine-tuned with limited real-world samples improve the success rate of over 70% for single-arm tasks and over 40% for dual-arm tasks compared to models trained solely on real-world data. This significant improvement demonstrates RoboTwin's potential to enhance the development and evaluation of dual-arm robotic manipulation systems. Project Page: https://robotwin-benchmark.github.io/early-version/.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models</title>
<link>https://arxiv.org/abs/2410.01795</link>
<guid>https://arxiv.org/abs/2410.01795</guid>
<content:encoded><![CDATA[
arXiv:2410.01795v2 Announce Type: replace-cross 
Abstract: Predicting phenotypes with complex genetic bases based on a small, interpretable set of variant features remains a challenging task. Conventionally, data-driven approaches are utilized for this task, yet the high dimensional nature of genotype data makes the analysis and prediction difficult. Motivated by the extensive knowledge encoded in pre-trained LLMs and their success in processing complex biomedical concepts, we set to examine the ability of LLMs in feature selection and engineering for tabular genotype data, with a novel knowledge-driven framework. We develop FREEFORM, Free-flow Reasoning and Ensembling for Enhanced Feature Output and Robust Modeling, designed with chain-of-thought and ensembling principles, to select and engineer features with the intrinsic knowledge of LLMs. Evaluated on two distinct genotype-phenotype datasets, genetic ancestry and hereditary hearing loss, we find this framework outperforms several data-driven methods, particularly on low-shot regimes. FREEFORM is available as open-source framework at GitHub: https://github.com/PennShenLab/FREEFORM.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Need to Talk: Asynchronous Mixture of Language Models</title>
<link>https://arxiv.org/abs/2410.03529</link>
<guid>https://arxiv.org/abs/2410.03529</guid>
<content:encoded><![CDATA[
arXiv:2410.03529v2 Announce Type: replace-cross 
Abstract: We introduce SMALLTALK LM, an innovative method for training a mixture of language models in an almost asynchronous manner. Each model of the mixture specializes in distinct parts of the data distribution, without the need for high-bandwidth communication between the nodes training each model. At inference, a lightweight router directs a given sequence to a single expert, according to a short prefix. This inference scheme naturally uses a fraction of the parameters from the overall mixture model. Unlike prior works on asynchronous LLM training, our routing method does not rely on full corpus clustering or access to metadata, making it more suitable for real-world applications. Our experiments on language modeling demonstrate that SMALLTALK LM achieves significantly lower perplexity than dense model baselines for the same total training FLOPs and an almost identical inference cost. Finally, in our downstream evaluations we outperform the dense baseline on 75% of the tasks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs</title>
<link>https://arxiv.org/abs/2410.09080</link>
<guid>https://arxiv.org/abs/2410.09080</guid>
<content:encoded><![CDATA[
arXiv:2410.09080v2 Announce Type: replace-cross 
Abstract: Growing evidence suggests that social determinants of health (SDoH), a set of nonmedical factors, affect individuals' risks of developing Alzheimer's disease (AD) and related dementias. Nevertheless, the etiological mechanisms underlying such relationships remain largely unclear, mainly due to difficulties in collecting relevant information. This study presents a novel, automated framework that leverages recent advancements of large language model (LLM) and natural language processing techniques to mine SDoH knowledge from extensive literature and integrate it with AD-related biological entities extracted from the general-purpose knowledge graph PrimeKG. Utilizing graph neural networks, we performed link prediction tasks to evaluate the resultant SDoH-augmented knowledge graph. Our framework shows promise for enhancing knowledge discovery in AD and can be generalized to other SDoH-related research areas, offering a new tool for exploring the impact of social determinants on health outcomes. Our code is available at: https://github.com/hwq0726/SDoHenPKG
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction</title>
<link>https://arxiv.org/abs/2410.21169</link>
<guid>https://arxiv.org/abs/2410.21169</guid>
<content:encoded><![CDATA[
arXiv:2410.21169v4 Announce Type: replace-cross 
Abstract: Document parsing is essential for converting unstructured and semi-structured documents such as contracts, academic papers, and invoices into structured, machine-readable data. Document parsing reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It outlines future research directions and emphasizes the importance of developing larger and more diverse datasets.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions</title>
<link>https://arxiv.org/abs/2411.09018</link>
<guid>https://arxiv.org/abs/2411.09018</guid>
<content:encoded><![CDATA[
arXiv:2411.09018v4 Announce Type: replace-cross 
Abstract: Recent research increasingly focuses on training vision-language models (VLMs) with long, detailed image captions. However, small-scale VLMs often struggle to balance the richness of these captions with the risk of hallucinating content during fine-tuning. In this paper, we explore how well VLMs adapt to such captions. To quantify caption quality, we propose Decomposed NLI (DNLI), an evaluation framework that breaks down generated captions into individual propositions, assessing each in isolation. This fine-grained analysis reveals a critical balance between capturing descriptive details and preventing hallucinations. Our findings show that simply reducing caption complexity or employing standard data curation techniques does not effectively resolve this issue. To tackle this challenge, we introduce Knowledge Adapted (KnowAda) fine-tuning, a data-centric approach that automatically adapts training data with the model's existing knowledge and visual understanding. KnowAda minimizes hallucinations while preserving high descriptiveness. We validate this approach across several small-scale VLMs (up to 7B parameters) and dense caption datasets, demonstrating that KnowAda effectively balances hallucination reduction and descriptiveness. Our results show that KnowAda outperforms various baselines in both automatic metrics and human evaluations. We will release our code and models.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving</title>
<link>https://arxiv.org/abs/2411.17404</link>
<guid>https://arxiv.org/abs/2411.17404</guid>
<content:encoded><![CDATA[
arXiv:2411.17404v3 Announce Type: replace-cross 
Abstract: LLMs exhibit advanced reasoning capabilities, offering the potential to transform natural language questions into mathematical models. However, existing open-source datasets in operations research domain lack detailed annotations of the modeling process, such as variable definitions, focusing solely on objective values, which hinders reinforcement learning applications. To address this, we release the StructuredOR dataset, annotated with comprehensive labels that capture the complete mathematical modeling process. We further propose BPP-Search, an algorithm that integrates reinforcement learning into a tree-of-thought structure using Beam search, a Process reward model, and a pairwise Preference algorithm. This approach enables efficient exploration of tree structures, avoiding exhaustive search while improving accuracy. Extensive experiments on StructuredOR, NL4OPT, and MAMO-ComplexLP datasets show that BPP-Search significantly outperforms state-of-the-art methods. In tree-based reasoning, BPP-Search excels in accuracy and efficiency, enabling faster retrieval of correct solutions. The StructuredOR dataset is available at https://github.com/tengwang0318/StructuredOR.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads</title>
<link>https://arxiv.org/abs/2412.00127</link>
<guid>https://arxiv.org/abs/2412.00127</guid>
<content:encoded><![CDATA[
arXiv:2412.00127v2 Announce Type: replace-cross 
Abstract: We introduce Orthus, an autoregressive (AR) transformer that excels in generating images given textual prompts, answering questions based on visual inputs, and even crafting lengthy image-text interleaved contents. Unlike prior arts on unified multimodal modeling, Orthus simultaneously copes with discrete text tokens and continuous image features under the AR modeling principle. The continuous treatment of visual signals minimizes the information loss for both image understanding and generation while the fully AR formulation renders the characterization of the correlation between modalities straightforward. The key mechanism enabling Orthus to leverage these advantages lies in its modality-specific heads -- one regular language modeling (LM) head predicts discrete text tokens and one diffusion head generates continuous image features conditioning on the output of the backbone. We devise an efficient strategy for building Orthus -- by substituting the Vector Quantization (VQ) operation in the existing unified AR model with a soft alternative, introducing a diffusion head, and tuning the added modules to reconstruct images, we can create an Orthus-base model effortlessly (e.g., within mere 72 A100 GPU hours). Orthus-base can further embrace post-training to better model interleaved images and texts. Empirically, Orthus surpasses competing baselines including Show-o and Chameleon across standard benchmarks, achieving a GenEval score of 0.58 and an MME-P score of 1265.8 using 7B parameters. Orthus also shows exceptional mixed-modality generation capabilities, reflecting the potential for handling intricate practical generation tasks.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChaosEater: Fully Automating Chaos Engineering with Large Language Models</title>
<link>https://arxiv.org/abs/2501.11107</link>
<guid>https://arxiv.org/abs/2501.11107</guid>
<content:encoded><![CDATA[
arXiv:2501.11107v2 Announce Type: replace-cross 
Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resiliency of distributed systems. It involves artificially injecting specific failures into a distributed system and observing its behavior in response. Based on the observation, the system can be proactively improved to handle those failures. Recent CE tools implement the automated execution of predefined CE experiments. However, defining these experiments and improving the system based on the experimental results still remain manual. To reduce the costs of the manual operations, we propose ChaosEater, a system for automating the entire CE operations with Large Language Models (LLMs). It predefines the agentic workflow according to a systematic CE cycle and assigns subdivided operations within the workflow to LLMs. ChaosEater targets CE for Kubernetes systems, which are managed through code (i.e., Infrastructure as Code). Therefore, the LLMs in ChaosEater perform software engineering tasks to complete CE cycles, including requirement definition, code generation, debugging, and testing. We evaluate ChaosEater through case studies on both small and large Kubernetes systems. The results demonstrate that it stably completes reasonable single CE cycles with significantly low time and monetary costs. The CE cycles are also qualitatively validated by human engineers and LLMs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for Parallel Sequence Generation</title>
<link>https://arxiv.org/abs/2503.07630</link>
<guid>https://arxiv.org/abs/2503.07630</guid>
<content:encoded><![CDATA[
arXiv:2503.07630v2 Announce Type: replace-cross 
Abstract: We present FourierNAT, a novel non-autoregressive Transformer (NAT) architecture that employs Fourier-based mixing in the decoder to generate output sequences in parallel. While traditional NAT approaches often face challenges with capturing global dependencies, our method leverages a discrete Fourier transform to mix token embeddings across the entire sequence dimension, coupled with learned frequency-domain gating. This allows the model to efficiently propagate context without explicit autoregressive steps. Empirically, FourierNAT achieves competitive results against leading NAT baselines on standard benchmarks like WMT machine translation and CNN/DailyMail summarization, providing significant speed advantages over autoregressive Transformers. We further demonstrate that learned frequency-domain parameters allow the model to adaptively focus on long-range or short-range dependencies, partially mitigating the well-known coherence gaps in one-pass NAT generation. Overall, FourierNAT highlights the potential of integrating spectral-domain operations to accelerate and improve parallel text generation. This approach can potentially provide great computational and time savings in inference tasks LLMs.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
arXiv:2503.22675v2 Announce Type: replace-cross 
Abstract: Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose \textbf{ReaRec}, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks</title>
<link>https://arxiv.org/abs/2504.08525</link>
<guid>https://arxiv.org/abs/2504.08525</guid>
<content:encoded><![CDATA[
arXiv:2504.08525v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis</title>
<link>https://arxiv.org/abs/2504.11257</link>
<guid>https://arxiv.org/abs/2504.11257</guid>
<content:encoded><![CDATA[
arXiv:2504.11257v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://colmon46.github.io/i2e-bench-leaderboard/ .
]]></content:encoded>
<pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>