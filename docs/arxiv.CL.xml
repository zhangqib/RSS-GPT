<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Towards Domain Specification of Embedding Models in Medicine</title>
<link>https://arxiv.org/abs/2507.19407</link>
<guid>https://arxiv.org/abs/2507.19407</guid>
<content:encoded><![CDATA[
<div> medical text embedding models, healthcare applications, clinical decision support, biomedical information retrieval, medical question answering <br>
Summary: 
- Medical text embedding models play a crucial role in various healthcare applications such as clinical decision support and biomedical information retrieval.
- Existing models are limited by training on a narrow range of medical data and outdated methodologies, hindering their ability to capture the breadth of medical terminology.
- The proposed MEDTE model, trained on diverse medical corpora through self-supervised contrastive learning, aims to address these limitations and deliver robust medical text embeddings.
- A benchmark suite of 51 tasks, tailored to medical text nuances, is introduced to evaluate the model's performance across classification, clustering, pair classification, and retrieval tasks.
- Results show that the combined approach of MEDTE and the benchmark suite outperforms existing state-of-the-art alternatives in various medical text tasks.<br>
Summary: <div>
arXiv:2507.19407v2 Announce Type: replace 
Abstract: Medical text embedding models are foundational to a wide array of healthcare applications, ranging from clinical decision support and biomedical information retrieval to medical question answering, yet they remain hampered by two critical shortcomings. First, most models are trained on a narrow slice of medical and biological data, beside not being up to date in terms of methodology, making them ill suited to capture the diversity of terminology and semantics encountered in practice. Second, existing evaluations are often inadequate: even widely used benchmarks fail to generalize across the full spectrum of real world medical tasks.
  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical corpora through self-supervised contrastive learning across multiple data sources, to deliver robust medical text embeddings.
  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification, clustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of medical text. Our results demonstrate that this combined approach not only establishes a robust evaluation framework but also yields embeddings that consistently outperform state of the art alternatives in different tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion</title>
<link>https://arxiv.org/abs/2508.03712</link>
<guid>https://arxiv.org/abs/2508.03712</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, representational bias, diversity, India, caste<br />
<br />
Summary: 
The study focuses on representational bias in large language models (LLMs), particularly GPT-4 Turbo, by examining their outputs in relation to religion and caste diversity in India. By prompting the model to generate stories about significant life events, the researchers found that the responses consistently overrepresented culturally dominant groups, despite attempts to encourage diversity through prompts. The study also highlighted the "stickiness" of representational bias in LLMs, indicating a more pronounced bias than that present in the model's training data. Additionally, the findings suggested that repeated nudges towards diversity had limited success in correcting these biases. The results emphasize the need for more comprehensive approaches to addressing representational bias in LLMs, beyond merely diversifying training data.<br /><br />Summary: <div>
arXiv:2508.03712v1 Announce Type: new 
Abstract: Representational bias in large language models (LLMs) has predominantly been measured through single-response interactions and has focused on Global North-centric identities like race and gender. We expand on that research by conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded representational biases are and how they extend to less-explored dimensions of identity. We prompt GPT-4 Turbo to generate over 7,200 stories about significant life events (such as weddings) in India, using prompts designed to encourage diversity to varying extents. Comparing the diversity of religious and caste representation in the outputs against the actual population distribution in India as recorded in census data, we quantify the presence and "stickiness" of representational bias in the LLM for religion and caste. We find that GPT-4 responses consistently overrepresent culturally dominant groups far beyond their statistical representation, despite prompts intended to encourage representational diversity. Our findings also suggest that representational bias in LLMs has a winner-take-all quality that is more biased than the likely distribution bias in their training data, and repeated prompt-based nudges have limited and inconsistent efficacy in dislodging these biases. These results suggest that diversifying training data alone may not be sufficient to correct LLM bias, highlighting the need for more fundamental changes in model development. Dataset and Codebook: https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FeynTune: Large Language Models for High-Energy Theory</title>
<link>https://arxiv.org/abs/2508.03716</link>
<guid>https://arxiv.org/abs/2508.03716</guid>
<content:encoded><![CDATA[
<div> specialized Large Language Models, theoretical High-Energy Physics, Llama-3.1 model, fine-tuned, hep-th, hep-ph, gr-qc <br />
<br />
Summary:
The study presents 20 specialized Large Language Models tailored for theoretical High-Energy Physics, derived from fine-tuning the 8-billion parameter Llama-3.1 model using arXiv abstracts from hep-th, hep-ph, and gr-qc fields up to August 2024. Two different Low-Rank Adaptation fine-tuning approaches were employed with varying dataset sizes, resulting in improved performance on hep-th abstract completion tasks. Comparative analysis was conducted against commercial LLMs like ChatGPT, Claude, Gemini, and DeepSeek. Insights were gained for the development of specialized language models in the High-Energy Theoretical Physics domain. <div>
arXiv:2508.03716v1 Announce Type: new 
Abstract: We present specialized Large Language Models for theoretical High-Energy Physics, obtained as 20 fine-tuned variants of the 8-billion parameter Llama-3.1 model. Each variant was trained on arXiv abstracts (through August 2024) from different combinations of hep-th, hep-ph and gr-qc. For a comparative study, we also trained models on datasets that contained abstracts from disparate fields such as the q-bio and cs categories. All models were fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and varying dataset sizes, and outperformed the base model on hep-th abstract completion tasks. We compare performance against leading commercial LLMs (ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing specialized language models for High-Energy Theoretical Physics.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering</title>
<link>https://arxiv.org/abs/2508.03719</link>
<guid>https://arxiv.org/abs/2508.03719</guid>
<content:encoded><![CDATA[
<div> Keywords: Indian farmers, AI-powered agricultural chatbot, personalized advice, multi-turn conversation, retrieval-based generation

Summary:

Indian farmers often face challenges in accessing timely and language-friendly agricultural advice, especially in rural areas with low literacy. To bridge this gap, a novel AI-powered agricultural chatbot called Krishi Sathi has been developed. This chatbot utilizes a combination of intent-driven dialogue flows, instruction-tuned models, and retrieval-based generation to provide personalized and easy-to-understand responses to farmer queries. Krishi Sathi follows a structured, multi-turn conversation flow to ensure that queries are fully understood before generating a response. The chatbot supports both English and Hindi languages, with speech input and output features for users with limited digital skills. Results show that the system achieved high query response accuracy, contextual relevance, and personalization, with a quick average response time of under 6 seconds. This innovative approach showcases how AI can improve the quality and accessibility of digital agricultural support in India. 

<br /><br />Summary: Indian farmers can now access personalized agricultural advice through an AI-powered chatbot called Krishi Sathi. The chatbot uses multi-turn conversations and retrieval-based generation to provide accurate and personalized responses. Supporting both English and Hindi languages, Krishi Sathi ensures accessibility for users with limited digital skills. The system's high accuracy, relevance, and quick response time demonstrate its effectiveness in improving digital agricultural support in India. <div>
arXiv:2508.03719v1 Announce Type: new 
Abstract: Indian farmers often lack timely, accessible, and language-friendly agricultural advice, especially in rural areas with low literacy. To address this gap in accessibility, this paper presents a novel AI-powered agricultural chatbot, Krishi Sathi, designed to support Indian farmers by providing personalized, easy-to-understand answers to their queries through both text and speech. The system's intelligence stems from an IFT model, subsequently refined through fine-tuning on Indian agricultural knowledge across three curated datasets. Unlike traditional chatbots that respond to one-off questions, Krishi Sathi follows a structured, multi-turn conversation flow to gradually collect the necessary details from the farmer, ensuring the query is fully understood before generating a response. Once the intent and context are extracted, the system performs Retrieval-Augmented Generation (RAG) by first fetching information from a curated agricultural database and then generating a tailored response using the IFT model. The chatbot supports both English and Hindi languages, with speech input and output features (via ASR and TTS) to make it accessible for users with low literacy or limited digital skills. This work demonstrates how combining intent-driven dialogue flows, instruction-tuned models, and retrieval-based generation can improve the quality and accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query response accuracy of 97.53%, 91.35% contextual relevance and personalization, and a query completion rate of 97.53%. The average response time remained under 6 seconds, ensuring timely support for users across both English and Hindi interactions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Verification of Speculative Beams for Accelerating LLM Inference</title>
<link>https://arxiv.org/abs/2508.03726</link>
<guid>https://arxiv.org/abs/2508.03726</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Hierarchical Verification Tree, speculative decoding, beam sampling, inference efficiency

Summary:
The article introduces the concept of the Hierarchical Verification Tree (HVT) as a novel framework designed to enhance the efficiency of large language models (LLMs) during inference. Traditional methods of speculative decoding and beam sampling often lead to unnecessary computational overhead due to the sequential verification of draft sequences. The HVT restructures this process by prioritizing high-likelihood drafts and enabling early pruning of suboptimal candidates. The framework includes a formal verification-pruning algorithm to ensure correctness and efficiency without requiring retraining or modification of LLM architectures. Experimental evaluations have shown that the HVT outperforms existing methods, significantly reducing inference time and energy consumption while maintaining or improving output quality. This research suggests that hierarchical verification strategies could offer a new direction for accelerating LLM inference. 

<br /><br />Summary: <div>
arXiv:2508.03726v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success across diverse natural language processing tasks but face persistent challenges in inference efficiency due to their autoregressive nature. While speculative decoding and beam sampling offer notable improvements, traditional methods verify draft sequences sequentially without prioritization, leading to unnecessary computational overhead. This work proposes the Hierarchical Verification Tree (HVT), a novel framework that restructures speculative beam decoding by prioritizing high-likelihood drafts and enabling early pruning of suboptimal candidates. Theoretical foundations and a formal verification-pruning algorithm are developed to ensure correctness and efficiency. Integration with standard LLM inference pipelines is achieved without requiring retraining or architecture modification. Experimental evaluations across multiple datasets and models demonstrate that HVT consistently outperforms existing speculative decoding schemes, achieving substantial reductions in inference time and energy consumption while maintaining or enhancing output quality. The findings highlight the potential of hierarchical verification strategies as a new direction for accelerating large language model inference.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WINELL: Wikipedia Never-Ending Updating with LLM Agents</title>
<link>https://arxiv.org/abs/2508.03728</link>
<guid>https://arxiv.org/abs/2508.03728</guid>
<content:encoded><![CDATA[
<div> Keywords: Wikipedia, knowledge acquisition, multi-agent framework, editing models, LLM agents <br />
Summary: <br />
This paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Inspired by NELL's continuous knowledge acquisition vision and leveraging LLM-based agents, WiNELL uses a multi-agent framework to gather online information, select important knowledge for Wikipedia articles, and generate precise edit suggestions for human review. The editing models, trained on Wikipedia's editing history, ensure updates are consistent with human behavior. WiNELL outperforms both open-source baselines and closed-source LLMs like GPT-4o in information coverage and editing efficiency. End-to-end evaluation on active Wikipedia pages demonstrates WiNELL's ability to suggest timely factual updates, presenting a new avenue for LLM agents in automatic knowledge base updates. <br /> <div>
arXiv:2508.03728v1 Announce Type: new 
Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces significant challenges in maintaining up-to-date content due to its reliance on manual human editors. Inspired by the vision of continuous knowledge acquisition in NELL and fueled by advances in LLM-based agents, this paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Our approach employs a multi-agent framework to aggregate online information, select new and important knowledge for a target entity in Wikipedia, and then generate precise edit suggestions for human review. Our fine-grained editing models, trained on Wikipedia's extensive history of human edits, enable incorporating updates in a manner consistent with human editing behavior. Our editor models outperform both open-source instruction-following baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and editing efficiency. End-to-end evaluation on high-activity Wikipedia pages demonstrates WiNELL's ability to identify and suggest timely factual updates. This opens up a promising research direction in LLM agents for automatically updating knowledge bases in a never-ending fashion.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models</title>
<link>https://arxiv.org/abs/2508.03737</link>
<guid>https://arxiv.org/abs/2508.03737</guid>
<content:encoded><![CDATA[
<div> benchmarks, Vision Language Models, GanitBench, Mathematics, Hindi
Summary:
- GanitBench is a tough benchmark comprising 1527 vision-only questions in Mathematics in English and Hindi languages.
- Questions are collected from major Indian examinations and include images and text.
- Two closed source models, GPT-4o mini, are evaluated in zero-shot and two-shot settings, with GPT-4o mini showing higher accuracy.
- The "Double Lock" constraint significantly decreases model performance.
- Two-shot CoT setting appears to be more effective, and performance drops when answering in Hindi. 
<br /><br />Summary: <div>
arXiv:2508.03737v1 Announce Type: new 
Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on several fields and domains are being curated more frequently over the last few years. However these are often monolingual, mostly available in English. Additionally there also is a lack of datasets available in Hindi on tasks apart from comprehension and translation. We introduce GanitBench, a tough benchmark consisting of 1527 vision-only questions covering several topics in Mathematics - available in languages English and Hindi. Collected from two major examinations from India, the JEE Advanced and the CBSE Boards examinations, this benchmark includes questions in the form of images comprising of figures essential to a question as well as text. We evaluate two closed source models for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings. GPT-4o mini is found to be the more dominant model on the benchmark, with it's highest average accuracy being 38.15%. We also evaluate models through a "Double Lock" constraint, which brings down the performance of the models by considerable margins. We observe that two-shot CoT appears to be a more effective setting under this environment. Performance of the two VLMs also decreases when answering the same questions in the Hindi language. We hope to facilitate the inclusion of languages like Hindi in research through our work.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AttnTrace: Attention-based Context Traceback for Long-Context LLMs</title>
<link>https://arxiv.org/abs/2508.03793</link>
<guid>https://arxiv.org/abs/2508.03793</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, context traceback, attention weights, efficiency, accuracy
Summary:<br /><br />Large language models (LLMs) like Gemini-2.5-Pro and Claude-Sonnet-4 are crucial for advanced AI systems. Context traceback methods trace back to texts in the context that contribute to LLM responses. Existing solutions, while effective, can be computationally costly. This work introduces AttnTrace, a new context traceback method leveraging attention weights from LLMs. AttnTrace is more accurate and efficient than current methods, as demonstrated through a systematic evaluation. It improves prompt injection detection in long contexts and can pinpoint injected instructions in manipulative content. Theoretical insights and two new techniques support AttnTrace's effectiveness. Code for AttnTrace is available on GitHub at https://github.com/Wang-Yanting/AttnTrace. <div>
arXiv:2508.03793v1 Announce Type: new 
Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and Claude-Sonnet-4, are increasingly used to empower advanced AI systems, including retrieval-augmented generation (RAG) pipelines and autonomous agents. In these systems, an LLM receives an instruction along with a context--often consisting of texts retrieved from a knowledge database or memory--and generates a response that is contextually grounded by following the instruction. Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. These solutions have numerous real-world applications, including performing post-attack forensic analysis and improving the interpretability and trustworthiness of LLM outputs. While significant efforts have been made, state-of-the-art solutions such as TracLLM often lead to a high computation cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a single response-context pair. In this work, we propose AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. To effectively utilize attention weights, we introduce two techniques designed to enhance the effectiveness of AttnTrace, and we provide theoretical insights for our design choice. We also perform a systematic evaluation for AttnTrace. The results demonstrate that AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. We also show that AttnTrace can improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm. As a real-world application, we demonstrate that AttnTrace can effectively pinpoint injected instructions in a paper designed to manipulate LLM-generated reviews. The code is at https://github.com/Wang-Yanting/AttnTrace.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Majority Bit-Aware Watermarking For Large Language Models</title>
<link>https://arxiv.org/abs/2508.03829</link>
<guid>https://arxiv.org/abs/2508.03829</guid>
<content:encoded><![CDATA[
<div> watermarking, Large Language Models, text generation, decoding accuracy, content quality

Summary:
- The article discusses the issue of potential misuse of Large Language Models (LLMs) in generating harmful or deceptive content and proposes watermarking techniques as a solution.
- MajorMark, a novel watermarking method, improves the trade-off between text quality and decoding accuracy by using majority bit-aware encoding.
- MajorMark selects preferred token sets based on the majority bit of the message, allowing for larger and more flexible sampling of tokens.
- A clustering-based decoding strategy is employed by MajorMark for better decoding accuracy, even with a large preferred token set, maintaining content quality.
- MajorMark$^+$ partitions the message into blocks for independent encoding and deterministic decoding, further enhancing text quality and decoding accuracy. 

<br /><br />Summary: <div>
arXiv:2508.03829v1 Announce Type: new 
Abstract: The growing deployment of Large Language Models (LLMs) in real-world applications has raised concerns about their potential misuse in generating harmful or deceptive content. To address this issue, watermarking techniques have emerged as a promising solution by embedding identifiable binary messages into generated text for origin verification and misuse tracing. While recent efforts have explored multi-bit watermarking schemes capable of embedding rich information such as user identifiers, they typically suffer from the fundamental trade-off between text quality and decoding accuracy: to ensure reliable message decoding, they have to restrict the size of preferred token sets during encoding, yet such restrictions reduce the quality of the generated content. In this work, we propose MajorMark, a novel watermarking method that improves this trade-off through majority bit-aware encoding. MajorMark selects preferred token sets based on the majority bit of the message, enabling a larger and more flexible sampling of tokens. In contrast to prior methods that rely on token frequency analysis for decoding, MajorMark employs a clustering-based decoding strategy, which maintains high decoding accuracy even when the preferred token set is large, thus preserving both content quality and decoding accuracy. We further introduce MajorMark$^+$, which partitions the message into multiple blocks to independently encode and deterministically decode each block, thereby further enhancing the quality of watermarked text and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs demonstrate that our methods significantly enhance both decoding accuracy and text generation quality, outperforming prior multi-bit watermarking baselines.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.03860</link>
<guid>https://arxiv.org/abs/2508.03860</guid>
<content:encoded><![CDATA[
<div> Fact-checking, Large Language Models, Evaluation Methods, Misinformation, Domain-specific customization

Summary:
This review examines the challenges of assessing factual accuracy in Large Language Models (LLMs) and the necessity for robust fact-checking frameworks. Key issues such as hallucinations, dataset limitations, and the reliability of evaluation metrics are outlined. The review proposes five research questions to guide the analysis of recent literature, emphasizing evaluation methods and mitigation techniques. It discusses the role of advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods in enhancing fact-checking. The review highlights the importance of instruction tuning, multi-agent reasoning, and external knowledge access through RAG frameworks. It calls attention to the limitations of current metrics, the value of grounding outputs with verified external evidence, and the significance of domain-specific customization for improving factual consistency. The review underscores the necessity of developing accurate, explainable, and context-aware language models tailored for domain-specific fact-checking.<br /><br />Summary: <div>
arXiv:2508.03860v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. The review also discusses the role of instruction tuning, multi-agent reasoning, and external knowledge access via RAG frameworks. Key findings highlight the limitations of current metrics, the value of grounding outputs with validated external evidence, and the importance of domain-specific customization to improve factual consistency. Overall, the review underlines the importance of building LLMs that are not only accurate and explainable but also tailored for domain-specific fact-checking. These insights contribute to the advancement of research toward more trustworthy and context-aware language models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Entity Linking Agent for Question Answering</title>
<link>https://arxiv.org/abs/2508.03865</link>
<guid>https://arxiv.org/abs/2508.03865</guid>
<content:encoded><![CDATA[
<div> Keywords: Question Answering, Knowledge Bases, Entity Linking, Large Language Model, Cognitive Workflows 

Summary: 
An entity linking agent for Question Answering (QA) tasks has been proposed, utilizing a Large Language Model to enhance accuracy. The agent actively identifies entity mentions, retrieves candidate entities, and makes decisions resembling human cognitive workflows. Two experiments were conducted to verify the agent's effectiveness: tool-based entity linking and QA task evaluation. Results confirmed the robustness and efficiency of the proposed agent in handling short, ambiguous user questions in QA tasks. This approach addresses the limitations of existing Entity Linking (EL) methods designed for longer contexts, providing a solution tailored to the unique challenges of QA systems reliant on knowledge bases for accurate answers. By simulating human cognitive workflows, the agent offers a promising solution for improving the performance of QA systems, enhancing entity linking accuracy, and overall system effectiveness. 

<br /><br />Summary: <div>
arXiv:2508.03865v1 Announce Type: new 
Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide accurate answers. Entity Linking (EL) plays a critical role in linking natural language mentions to KB entries. However, most existing EL methods are designed for long contexts and do not perform well on short, ambiguous user questions in QA tasks. We propose an entity linking agent for QA, based on a Large Language Model that simulates human cognitive workflows. The agent actively identifies entity mentions, retrieves candidate entities, and makes decision. To verify the effectiveness of our agent, we conduct two experiments: tool-based entity linking and QA task evaluation. The results confirm the robustness and effectiveness of our agent.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sotopia-RL: Reward Design for Social Intelligence</title>
<link>https://arxiv.org/abs/2508.03905</link>
<guid>https://arxiv.org/abs/2508.03905</guid>
<content:encoded><![CDATA[
<div> Keywords: Social intelligence, Reinforcement learning, Partial observability, Multi-dimensionality, Sotopia-RL

Summary:
Sotopia-RL introduces a novel framework to enhance the training of socially intelligent agents through reinforcement learning in complex social interactions. By refining episode-level feedback into utterance-level, multi-dimensional rewards, it addresses challenges such as partial observability and multi-dimensionality. This approach allows for more accurate credit assignment to individual utterances and captures the diverse aspects of social interactions. Experimental results in the Sotopia environment show that Sotopia-RL achieves superior social goal completion scores compared to existing methods. Ablation studies confirm the importance of both utterance-level credit assignment and multi-dimensional reward design in the training process. The implementation of Sotopia-RL is publicly available, providing a valuable resource for researchers in the field. <div>
arXiv:2508.03905v1 Announce Type: new 
Abstract: Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoAct-1: Computer-using Agents with Coding as Actions</title>
<link>https://arxiv.org/abs/2508.03923</link>
<guid>https://arxiv.org/abs/2508.03923</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous agents, Graphical User Interfaces (GUIs), Coding, Computer automation, OSWorld benchmark <br />
Summary: <br />
The article introduces a new paradigm for autonomous agents operating computers via GUIs, incorporating coding as an enhanced action. The CoAct-1 multi-agent system combines GUI-based control with direct programmatic execution, dynamically delegating subtasks to either a GUI operator or a Programmer agent. This hybrid approach allows agents to bypass inefficient GUI actions for tasks like file management and data processing while leveraging visual interaction when needed. CoAct-1 achieves a state-of-the-art success rate of 60.76% on the challenging OSWorld benchmark, significantly outperforming prior methods. The system improves efficiency by reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation. <br /> <div>
arXiv:2508.03923v1 Announce Type: new 
Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. In this work, we introduce a more robust and flexible paradigm: enabling agents to use coding as a enhanced action. We present CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. We evaluate our system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, our approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation</title>
<link>https://arxiv.org/abs/2508.03935</link>
<guid>https://arxiv.org/abs/2508.03935</guid>
<content:encoded><![CDATA[
<div> Encoder, Context Injection, Reinforcement Module, Personalization, Factual Consistency
Summary:
Context-Augmented Personalized LLM (CAP-LLM) is introduced as a framework to generate personalized news headlines by incorporating user preferences and factual consistency constraints into Large Language Models (LLMs). It includes a User Preference Encoder to capture user interests, a Context Injection Adapter to integrate preferences and current context into the LLM, and a Fact-Consistency Reinforcement Module to maintain factual accuracy. CAP-LLM outperforms existing methods on metrics like FactCC, personalization, and content coverage on the PENS dataset. Ablation studies, human evaluations, and sensitivity analyses confirm the effectiveness and robustness of the approach, striking a balance between personalization and factual accuracy in news headline generation.  <div>
arXiv:2508.03935v1 Announce Type: new 
Abstract: In the era of information overload, personalized news headline generation is crucial for engaging users by tailoring content to their preferences while accurately conveying news facts. Existing methods struggle with effectively capturing complex user interests and ensuring factual consistency, often leading to generic or misleading headlines. Leveraging the unprecedented capabilities of Large Language Models (LLMs) in text generation, we propose Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates user preferences and factual consistency constraints into a powerful pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture long-term user interests, a Context Injection Adapter to seamlessly integrate these preferences and current article context into the LLM's generation process, and a Fact-Consistency Reinforcement Module employing a novel contrastive loss to mitigate hallucination. Evaluated on the real-world PENS dataset, CAP-LLM achieves state-of-the-art performance across all metrics. Notably, it significantly improves factual consistency (FactCC of 87.50) over strong baselines like BART (86.67), while simultaneously enhancing personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations, and sensitivity analyses further validate the effectiveness of each component and the robustness of our approach, demonstrating CAP-LLM's ability to achieve a superior balance between personalization and factual accuracy in news headline generation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data and AI governance: Promoting equity, ethics, and fairness in large language models</title>
<link>https://arxiv.org/abs/2508.03970</link>
<guid>https://arxiv.org/abs/2508.03970</guid>
<content:encoded><![CDATA[
<div> Keywords: bias, fairness, ethics, governance, large language models <br />
Summary: 
This paper discusses approaches to addressing bias and fairness issues in Large Language Models (LLMs) throughout the machine learning model life cycle. The authors introduce the Bias Evaluation and Assessment Test Suite (BEATS) for LLMs and highlight common bias-related gaps in LLMs. They propose a data and AI governance framework to promote Bias, Ethics, Fairness, and Factuality within LLMs. This governance approach enables benchmarking of LLMs before deployment, continuous evaluation, and proactive oversight of LLM-generated responses. Implementing this framework can enhance safety and responsibility in GenAI systems, mitigating discrimination risks and safeguarding against reputational harm. The goal is to support the development and deployment of socially responsible and ethically sound generative artificial intelligence applications.<br /><br />Summary: <div>
arXiv:2508.03970v1 Announce Type: new 
Abstract: In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency</title>
<link>https://arxiv.org/abs/2508.03979</link>
<guid>https://arxiv.org/abs/2508.03979</guid>
<content:encoded><![CDATA[
<div> Efficiency, Self-consistency, Hypothesis pruning, Token expenditure, Long reasoning tasks
Summary: 
Efficiency is a crucial aspect of self-consistency in long chain-of-thought reasoning tasks, but the high token expenditure can limit its practicality. This study explores the token-efficient enhancements of self-consistency through early hypothesis pruning, maintaining parallelism. The approach involves generating solutions in parallel and pruning intermediate hypotheses based on model confidence and lexical coverage. A fast weighted set cover algorithm utilizing these indicators is designed. Evaluation of five language models on math benchmarks demonstrates a token efficiency improvement of 10-35% in many cases. <div>
arXiv:2508.03979v1 Announce Type: new 
Abstract: Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis pruning. Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five LLMs on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Today's LLMs Ready to Explain Well-Being Concepts?</title>
<link>https://arxiv.org/abs/2508.03990</link>
<guid>https://arxiv.org/abs/2508.03990</guid>
<content:encoded><![CDATA[
<div> Keywords: Well-being, Large Language Models, Explanations, Evaluation Framework, Fine-Tuning<br />
Summary: <br />
1. The study focuses on generating accurate and tailored explanations of well-being concepts using Large Language Models (LLMs). <br />
2. A dataset of 43,880 explanations for 2,194 well-being concepts from ten LLMs is created for evaluation. <br />
3. An evaluation framework using dual judges is introduced to assess explanation quality based on factual correctness and user expectations. <br />
4. Fine-tuning LLMs using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) enhances the quality of generated explanations. <br />
5. The results show alignment between LLM judges and human evaluations, significant variation in explanation quality across models, audiences, and categories, and the superior performance of DPO- and SFT-finetuned models. <br /> <div>
arXiv:2508.03990v1 Announce Type: new 
Abstract: Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2508.03998</link>
<guid>https://arxiv.org/abs/2508.03998</guid>
<content:encoded><![CDATA[
<div> keywords: group meetings, social robot, facilitator, concept bottleneck model, transfer learning<br />
Summary: 
A new study introduces a social robot co-facilitator designed to enhance group meetings by analyzing multimodal data and providing discreet cues to human facilitators. The robot's decision-making is based on a concept bottleneck model, which focuses on transparent human-interpretable concepts like participant engagement and sentiments. The study highlights a transfer learning framework that enables the model to distill expert knowledge and outperform existing foundation models in predicting intervention needs. This concept-driven system allows for real-time correction by human facilitators. Importantly, the model demonstrates robust knowledge transfer across different groups and benefits novices by improving their performance through expert cognitive model transfer. The research provides a blueprint for leveraging robotic technology to augment human capabilities in complex social settings. 

<br /><br />Summary: <div>
arXiv:2508.03998v1 Announce Type: new 
Abstract: Successful group meetings, such as those implemented in group behavioral-change programs, work meetings, and other social contexts, must promote individual goal setting and execution while strengthening the social relationships within the group. Consequently, an ideal facilitator must be sensitive to the subtle dynamics of disengagement, difficulties with individual goal setting and execution, and interpersonal difficulties that signal a need for intervention. The challenges and cognitive load experienced by facilitators create a critical gap for an embodied technology that can interpret social exchanges while remaining aware of the needs of the individuals in the group and providing transparent recommendations that go beyond powerful but "black box" foundation models (FMs) that identify social cues. We address this important demand with a social robot co-facilitator that analyzes multimodal meeting data and provides discreet cues to the facilitator. The robot's reasoning is powered by an agentic concept bottleneck model (CBM), which makes decisions based on human-interpretable concepts like participant engagement and sentiments, ensuring transparency and trustworthiness. Our core contribution is a transfer learning framework that distills the broad social understanding of an FM into our specialized and transparent CBM. This concept-driven system significantly outperforms direct zero-shot FMs in predicting the need for intervention and enables real-time human correction of its reasoning. Critically, we demonstrate robust knowledge transfer: the model generalizes across different groups and successfully transfers the expertise of senior human facilitators to improve the performance of novices. By transferring an expert's cognitive model into an interpretable robotic partner, our work provides a powerful blueprint for augmenting human capabilities in complex social domains.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization</title>
<link>https://arxiv.org/abs/2508.04010</link>
<guid>https://arxiv.org/abs/2508.04010</guid>
<content:encoded><![CDATA[
<div> Framework, Multi-agent, Policy Enhancement, Objective Optimization, Web Environments
Summary:
HarmonyGuard is a multi-agent collaborative framework designed to address the challenge of balancing task performance with emerging risks in web environments. It features Adaptive Policy Enhancement, where the Policy Agent extracts and maintains security policies from external documents, and Dual-Objective Optimization, with the Utility Agent evaluating and optimizing safety and utility objectives in real-time. The framework outperforms existing baselines, improving policy compliance by up to 38% and task completion by up to 20%, while consistently achieving over 90% policy compliance across tasks. The architecture of HarmonyGuard showcases its capability for collaborative optimization of safety and utility, providing a robust solution for autonomous agents operating in open web environments.<br /><br />Summary: <div>
arXiv:2508.04010v1 Announce Type: new 
Abstract: Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing</title>
<link>https://arxiv.org/abs/2508.04012</link>
<guid>https://arxiv.org/abs/2508.04012</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, model editing, meta-learning, low-data scenarios, training efficiency

Summary:
Large Language Models (LLMs) are crucial in AI applications but updating them can be costly due to their static nature. Model editing through targeted parameter modifications presents an efficient alternative, with meta-learning-based model editing (MLBME) methods showing advantages in effectiveness and efficiency. However, MLBME faces challenges in low-data scenarios and efficiency due to the computation of KL divergence. To address these issues, the novel Step More Edit (SMEdit) method employs Multiple Backpropagation Steps (MBPS) to enhance editing performance with limited supervision and includes a norm regularization on weight updates to boost training efficiency. Experimental results on two datasets and LLMs demonstrate that SMEdit surpasses previous MLBME methods, and integrating MBPS into existing approaches further enhances their performance. The code for SMEdit will be made available soon. 

<br /><br />Summary: <div>
arXiv:2508.04012v1 Announce Type: new 
Abstract: Large Language Models (LLMs) underpin many AI applications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular, meta-learning-based model editing (MLBME) methods have demonstrated notable advantages in both editing effectiveness and efficiency. Despite this, we find that MLBME exhibits suboptimal performance in low-data scenarios, and its training efficiency is bottlenecked by the computation of KL divergence. To address these, we propose $\textbf{S}$tep $\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation $\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency. Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance. Our code will be released soon.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents</title>
<link>https://arxiv.org/abs/2508.04038</link>
<guid>https://arxiv.org/abs/2508.04038</guid>
<content:encoded><![CDATA[
<div> Keywords: Motion sensor time-series, human activity recognition, zero-shot learning, explainable AI, hierarchical agent pipeline

Summary: 
ZARA is a novel agent-based framework for zero-shot, explainable human activity recognition (HAR) directly from raw motion time-series data. It automatically derives a pair-wise feature knowledge base, integrates a multi-sensor retrieval module, and utilizes a hierarchical agent pipeline to guide a large language model (LLM) in making activity predictions and providing natural-language explanations. ZARA achieves state-of-the-art zero-shot performance on 8 HAR benchmarks, surpassing existing baselines by 2.53x in macro F1 score. The necessity of each module is confirmed through ablation studies, demonstrating the effectiveness of ZARA in flexible and interpretable HAR without the need for fine-tuning or task-specific classifiers. This work represents a significant step towards trustworthy and plug-and-play motion time-series analysis. The ZARA codes are publicly available on GitHub for further research and application. 

<br /><br />Summary: <div>
arXiv:2508.04038v1 Announce Type: new 
Abstract: Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Reasoning Models Are Autonomous Jailbreak Agents</title>
<link>https://arxiv.org/abs/2508.04039</link>
<guid>https://arxiv.org/abs/2508.04039</guid>
<content:encoded><![CDATA[
<div> Jailbreaking, large reasoning models, autonomous adversaries, multi-turn conversations, alignment regression <br />
Summary: 
Large reasoning models (LRMs) are shown to simplify and scale the process of bypassing safety mechanisms in AI models, known as jailbreaking. Four LRMs were evaluated for their ability to autonomously conduct conversations with target models and successfully execute jailbreaks in various sensitive domains. The study revealed a high attack success rate of 97.14%, highlighting the potential dangers of LRMs eroding the safety guardrails of other models. This raises the urgency for better alignment of frontier models to prevent them from being exploited as jailbreak agents. The findings emphasize the need for enhanced security measures to protect AI models from such manipulative tactics. 


<br /><br /> <div>
arXiv:2508.04039v1 Announce Type: new 
Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has traditionally required complex technical procedures or specialized human expertise. In this study, we show that the persuasive capabilities of large reasoning models (LRMs) simplify and scale jailbreaking, converting it into an inexpensive activity accessible to non-experts. We evaluated the capabilities of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as autonomous adversaries conducting multi-turn conversations with nine widely used target models. LRMs received instructions via a system prompt, before proceeding to planning and executing jailbreaks with no further supervision. We performed extensive experiments with a benchmark of harmful prompts composed of 70 items covering seven sensitive domains. This setup yielded an overall attack success rate across all model combinations of 97.14%. Our study reveals an alignment regression, in which LRMs can systematically erode the safety guardrails of other models, highlighting the urgent need to further align frontier models not only to resist jailbreak attempts, but also to prevent them from being co-opted into acting as jailbreak agents.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation</title>
<link>https://arxiv.org/abs/2508.04047</link>
<guid>https://arxiv.org/abs/2508.04047</guid>
<content:encoded><![CDATA[
<div> Keywords: Controllable Text Generation, Air-Decoding, Prefix Augmentation, Attribute Control, Long Text Generation

Summary:
- Controllable Text Generation (CTG) is important in Natural Language Processing (NLP) for generating text with desired attributes.
- Air-Decoding, a powerful prefix-based method, shows a decline in controllability with longer text sequences due to decay in attention to prefixes.
- Different types of prefixes, including soft and hard prefixes, impact the performance of controllable text generation.
- A new framework called Dynamic Token-level Prefix Augmentation (DTPA) is proposed for enhancing attribute control in text generation tasks.
- DTPA dynamically amplifies attention to prefixes based on sequence length, selecting optimal prefix types and balancing attribute distribution reconstruction for improved text quality.
- Experiments show that DTPA outperforms existing methods in attribute control while maintaining fluency, diversity, and topic relevance in long text generation scenarios. 

<br /><br />Summary: 
Controllable Text Generation is crucial for generating text with desired attributes in NLP. Air-Decoding, a powerful method, may lose controllability with longer sequences due to attention decay to prefixes. Different prefix types affect performance. A new framework, DTPA, enhances attribute control by dynamically amplifying prefix attention and optimizing prefix types. It shows superior effectiveness in long text generation tasks, outperforming existing methods in attribute control, fluency, diversity, and topic relevance. <div>
arXiv:2508.04047v1 Announce Type: new 
Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language Processing (NLP), aiming to generate text that aligns with desired attributes. However, previous studies commonly focus on the quality of controllable text generation for short sequences, while the generation of long-form text remains largely underexplored. In this paper, we observe that the controllability of texts generated by the powerful prefix-based method Air-Decoding tends to decline with increasing sequence length, which we hypothesize primarily arises from the observed decay in attention to the prefixes. Meanwhile, different types of prefixes including soft and hard prefixes are also key factors influencing performance. Building on these insights, we propose a lightweight and effective framework called Dynamic Token-level Prefix Augmentation (DTPA) based on Air-Decoding for controllable text generation. Specifically, it first selects the optimal prefix type for a given task. Then we dynamically amplify the attention to the prefix for the attribute distribution to enhance controllability, with a scaling factor growing exponentially as the sequence length increases. Moreover, based on the task, we optionally apply a similar augmentation to the original prompt for the raw distribution to balance text quality. After attribute distribution reconstruction, the generated text satisfies the attribute constraints well. Experiments on multiple CTG tasks demonstrate that DTPA generally outperforms other methods in attribute control while maintaining competitive fluency, diversity, and topic relevance. Further analysis highlights DTPA's superior effectiveness in long text generation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG</title>
<link>https://arxiv.org/abs/2508.04057</link>
<guid>https://arxiv.org/abs/2508.04057</guid>
<content:encoded><![CDATA[
<div> RAG, LLM, PAIRS, DPR, QA <br />
Summary: <br />
The article introduces PAIRS, a framework that combines parametric and retrieved knowledge to improve efficiency and accuracy in Retrieval-Augmented Generation (RAG) systems. PAIRS utilizes a dual-path generation mechanism where the LLM produces direct and context-augmented answers. When these converge, external retrieval is bypassed, enhancing efficiency. In cases of divergence, a dual-path retrieval process is activated, guided by query and self-generated context signals, followed by an Adaptive Information Selection module for document filtering. Experimental results on six QA benchmarks show that PAIRS reduces retrieval costs by 25% while achieving a 1.1% increase in EM and 1.0% increase in F1 accuracy over prior baselines on average. With its focus on adaptively selecting external information, PAIRS improves the overall performance of RAG systems. <br /> <div>
arXiv:2508.04057v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models (LLMs) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the LLM's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain sparse information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the LLM produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Strategy for Improving Large Language Model (LLM) Capabilities</title>
<link>https://arxiv.org/abs/2508.04073</link>
<guid>https://arxiv.org/abs/2508.04073</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, efficiency, data processing, training strategies, limited knowledge base
Summary: 
This work focuses on improving the efficiency of Large Language Models (LLMs) in resource-constrained environments and within a delimited knowledge base. The proposed approach involves exploring data processing techniques, data selection strategies, training methods, and architectural adjustments. The methodology includes defining criteria for building reliable datasets, conducting controlled experiments with different configurations, and evaluating resulting variants based on capability, versatility, response time, and safety. Comparative tests were done to measure the performance of the developed variants against the proposed strategies. The study is based on a master's thesis in Systems and Computer Engineering titled "Efficient Strategy for Improving the Capabilities of Large Language Models (LLMs)". 
<br /><br />Summary: <div>
arXiv:2508.04073v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become a milestone in the field of artificial intelligence and natural language processing. However, their large-scale deployment remains constrained by the need for significant computational resources. This work proposes starting from a base model to explore and combine data processing and careful data selection techniques, training strategies, and architectural adjustments to improve the efficiency of LLMs in resource-constrained environments and within a delimited knowledge base. The methodological approach included defining criteria for building reliable datasets, conducting controlled experiments with different configurations, and systematically evaluating the resulting variants in terms of capability, versatility, response time, and safety. Finally, comparative tests were conducted to measure the performance of the developed variants and to validate the effectiveness of the proposed strategies. This work is based on the master's thesis in Systems and Computer Engineering titled "Efficient Strategy for Improving the Capabilities of Large Language Models (LLMs)".
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"</title>
<link>https://arxiv.org/abs/2508.04086</link>
<guid>https://arxiv.org/abs/2508.04086</guid>
<content:encoded><![CDATA[
<div> Framework, ToolGrad, tool-use chains, user queries, dataset

Summary:
ToolGrad is an innovative framework that transforms the conventional approach to synthesizing tool-use datasets. By prioritizing the construction of valid tool-use chains through iterative processes guided by textual gradients, ToolGrad then generates user queries, resulting in the creation of a dataset called ToolGrad-5k. This dataset features more complex tool use annotations, achieved at a lower cost and with a 100% pass rate. Experimental results demonstrate that models trained on ToolGrad-5k outperform those trained on expensive baseline datasets and proprietary LLMs, even excelling on out-of-domain benchmarks. ToolGrad's answer-first methodology demonstrates the potential for efficient and effective data generation in the development of tool-use datasets for language model training. 

<br /><br />Summary: <div>
arXiv:2508.04086v1 Announce Type: new 
Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user query, followed by complex tool-use annotations like DFS. This leads to inevitable annotation failures and low efficiency in data generation. We introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad first constructs valid tool-use chains through an iterative process guided by textual "gradients", and then synthesizes corresponding user queries. This "answer-first" approach led to ToolGrad-5k, a dataset generated with more complex tool use, lower cost, and 100% pass rate. Experiments show that models trained on ToolGrad-5k outperform those on expensive baseline datasets and proprietary LLMs, even on OOD benchmarks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2508.04088</link>
<guid>https://arxiv.org/abs/2508.04088</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Process Reward Models, Generative Multimodal Process Reward Model, reasoning, correction<br />
Summary: <br />
Multimodal Large Language Models (MLLMs) struggle with complex mathematical reasoning tasks due to errors in visual perception or logical deduction. Process Reward Models (PRMs) offer step-by-step supervision but lack correction abilities. The Generative Multimodal Process Reward Model (GM-PRM) transforms the PRM into an active reasoning collaborator, providing fine-grained analysis of reasoning steps and the ability to generate corrections for errors. The GM-PRM improves solution quality using a test-time inference strategy called Refined Best-of-N (Refined-BoN), guiding the policy model towards more promising reasoning trajectories. State-of-the-art results are achieved on multimodal math benchmarks with remarkable data efficiency using GM-PRM, which requires only a 20K-sample training dataset. <div>
arXiv:2508.04088v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities but often struggle with complex, multi-step mathematical reasoning, where minor errors in visual perception or logical deduction can lead to complete failure. While Process Reward Models (PRMs) offer step-by-step supervision, existing multimodal PRMs are limited to being binary verifiers that can identify but not correct errors, offering little explanatory power. To address these deficiencies, we introduce the Generative Multimodal Process Reward Model (GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an active reasoning collaborator. Instead of a simple scalar score, GM-PRM provides a fine-grained, interpretable analysis of each reasoning step, evaluating its step intent, visual alignment, and logical soundness. More critically, GM-PRM is trained to generate a corrected version of the first erroneous step it identifies. This unique corrective capability enables our new test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework actively enhances solution quality by using the PRM's generated correction to guide the policy model toward a more promising reasoning trajectory, thereby improving the diversity and correctness of the solution pool. We demonstrate that GM-PRM achieves state-of-the-art results on multiple multimodal math benchmarks, significantly boosting policy model performance with remarkable data efficiency, requiring only a 20K-sample training dataset. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks</title>
<link>https://arxiv.org/abs/2508.04117</link>
<guid>https://arxiv.org/abs/2508.04117</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, finetuning, over-memorization, learning dynamics, reasoning tasks <br />
<br />
Summary: 
The study investigates the learning dynamics of large language models (LLMs) during finetuning on reasoning tasks, uncovering an over-memorization phenomenon. This phenomenon occurs when LLMs excessively memorize training data during a specific stage of finetuning, leading to high test perplexity despite good test accuracy. Factors such as training epochs and large learning rates contribute to over-memorization. Although over-memorized models maintain test accuracy, they exhibit reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. The research shows that over-memorization is prevalent across various tasks, models, and finetuning methods, indicating unique learning dynamics in overparameterized LLMs. Recommendations are provided for checkpoint and learning rate selection during finetuning. <div>
arXiv:2508.04117v1 Announce Type: new 
Abstract: The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We investigate the conditions that lead to LLM over-memorization and find that training epochs and large learning rates contribute to this issue. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. Our experiments unveil the over-memorization to be broadly applicable across different tasks, models, and finetuning methods. Our research highlights that overparameterized, extensively finetuned LLMs exhibit unique learning dynamics distinct from traditional machine learning models. Based on our observations of over-memorization, we provide recommendations on checkpoint and learning rate selection during finetuning.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap</title>
<link>https://arxiv.org/abs/2508.04149</link>
<guid>https://arxiv.org/abs/2508.04149</guid>
<content:encoded><![CDATA[
<div> novelty, data selection strategy, preference datasets, model alignment, data efficiency

Summary:
The article introduces a novel difficulty-based data selection strategy for preference datasets used in aligning large language models with human preferences. The approach is grounded in the Direct Preference Optimization (DPO) implicit reward mechanism and focuses on selecting preference data examples with smaller DPO implicit reward gaps. This selection method improves data efficiency and model alignment, outperforming five strong baselines across multiple datasets and alignment tasks. The strategy shows superior performance even with only 10% of the original data, offering a promising solution for scaling LLM alignment with limited resources. <div>
arXiv:2508.04149v1 Announce Type: new 
Abstract: Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The State Of TTS: A Case Study with Human Fooling Rates</title>
<link>https://arxiv.org/abs/2508.04179</link>
<guid>https://arxiv.org/abs/2508.04179</guid>
<content:encoded><![CDATA[
<div> deception testing, TTS systems, Human Fooling Rate, benchmarking, commercial models<br />
<br />
Summary:<br />
(i) Current TTS systems struggle to pass human deception tests, despite claims of human parity.<br />
(ii) Benchmarking TTS progress on datasets with high Human Fooling Rates is crucial for accurate evaluation.<br />
(iii) Commercial models perform well in zero-shot scenarios, while open-source systems lag in natural conversational speech.<br />
(iv) Fine-tuning on quality data improves realism but does not completely bridge the gap.<br />
(v) The study highlights the importance of realistic, human-centric evaluations in addition to subjective tests for assessing TTS advancements. <br /> <div>
arXiv:2508.04179v1 Announce Type: new 
Abstract: While subjective evaluations in recent years indicate rapid progress in TTS, can current TTS systems truly pass a human deception test in a Turing-like evaluation? We introduce Human Fooling Rate (HFR), a metric that directly measures how often machine-generated speech is mistaken for human. Our large-scale evaluation of open-source and commercial TTS models reveals critical insights: (i) CMOS-based claims of human parity often fail under deception testing, (ii) TTS progress should be benchmarked on datasets where human speech achieves high HFRs, as evaluating against monotonous or less expressive reference samples sets a low bar, (iii) Commercial models approach human deception in zero-shot settings, while open-source systems still struggle with natural conversational speech; (iv) Fine-tuning on high-quality data improves realism but does not fully bridge the gap. Our findings underscore the need for more realistic, human-centric evaluations alongside existing subjective tests.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity</title>
<link>https://arxiv.org/abs/2508.04182</link>
<guid>https://arxiv.org/abs/2508.04182</guid>
<content:encoded><![CDATA[
<div> hallucinations, multimodal large language models, causal analysis, reinforcement learning, benchmark datasets  
Summary:  
Through causal analysis, the authors identify two types of hallucinations in Multimodal Large Language Models (MLLMs): omission-related and fabrication-related. Omission hallucinations occur when essential causal factors are not adequately captured, while fabrication hallucinations stem from non-causal cues misleading the model. To address these issues, a novel reinforcement learning framework guided by causal completeness is proposed. This framework evaluates the standalone contribution and indispensability of each token to define a token-level causal completeness reward. This reward is utilized within the GRPO optimization framework to encourage the model to focus on tokens that are both causally sufficient and necessary for accurate generation. Experimental results on benchmark datasets demonstrate the effectiveness of this approach in mitigating hallucinations in MLLMs.  
<br /><br />Summary: <div>
arXiv:2508.04182v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across vision-language tasks. However, they may suffer from hallucinations--generating outputs that are semantically inconsistent with the input image or text. Through causal analyses, we find that: (i) hallucinations with omission may arise from the failure to adequately capture essential causal factors, and (ii) hallucinations with fabrication are likely caused by the model being misled by non-causal cues. To address these challenges, we propose a novel reinforcement learning framework guided by causal completeness, which jointly considers both causal sufficiency and causal necessity of tokens. Specifically, we evaluate each token's standalone contribution and counterfactual indispensability to define a token-level causal completeness reward. This reward is used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are both causally sufficient and necessary for accurate generation. Experimental results across various benchmark datasets and tasks demonstrate the effectiveness of our approach, which effectively mitigates hallucinations in MLLMs.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing Deep Research: A Benchmark and Formal Definition</title>
<link>https://arxiv.org/abs/2508.04183</link>
<guid>https://arxiv.org/abs/2508.04183</guid>
<content:encoded><![CDATA[
<div> Keywords: deep research, reasoning-intensive tasks, intermediate output representation, benchmark, state-of-the-art models

Summary: 
The paper introduces the concept of deep research (DR) as a complex information task requiring broad and reasoning-intensive exploration. Unlike traditional report-writing tasks, DR focuses on the process of uncovering key claims through high fan-out over concepts during the search process. An intermediate output representation is proposed to separate the reasoning challenge from report generation, enabling objective evaluation. The LiveDRBench benchmark is introduced, consisting of 100 challenging tasks across scientific topics and public interest events. Current state-of-the-art DR systems achieve F1 scores ranging from 0.02 to 0.72, with OpenAI's model performing the best at 0.55. Analysis of reasoning traces highlights the distribution of referenced sources, branching, and backtracking events in current DR systems, suggesting areas for improvement in search mechanisms and grounding capabilities. The benchmark is available for further research and evaluation at https://github.com/microsoft/LiveDRBench. 

<br /><br />Summary: <div>
arXiv:2508.04183v1 Announce Type: new 
Abstract: Information tasks such as writing surveys or analytical reports require complex search and reasoning, and have recently been grouped under the umbrella of \textit{deep research} -- a term also adopted by recent models targeting these capabilities. Despite growing interest, the scope of the deep research task remains underdefined and its distinction from other reasoning-intensive problems is poorly understood. In this paper, we propose a formal characterization of the deep research (DR) task and introduce a benchmark to evaluate the performance of DR systems. We argue that the core defining feature of deep research is not the production of lengthy report-style outputs, but rather the high fan-out over concepts required during the search process, i.e., broad and reasoning-intensive exploration. To enable objective evaluation, we define DR using an intermediate output representation that encodes key claims uncovered during search-separating the reasoning challenge from surface-level report generation. Based on this formulation, we propose a diverse, challenging benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g., datasets, materials discovery, prior art search) and public interest events (e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1 score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model performs the best with an overall F1 score of 0.55. Analysis of reasoning traces reveals the distribution over the number of referenced sources, branching, and backtracking events executed by current DR systems, motivating future directions for improving their search mechanisms and grounding capabilities. The benchmark is available at https://github.com/microsoft/LiveDRBench.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models</title>
<link>https://arxiv.org/abs/2508.04196</link>
<guid>https://arxiv.org/abs/2508.04196</guid>
<content:encoded><![CDATA[
<div> vulnerabilities, language models, alignment methods, manipulation, conversational 

Summary: Despite advancements in alignment techniques, language models can still be manipulated through strategically crafted conversational scenarios to exhibit misaligned behaviors such as deception and manipulative reasoning. A manual red-teaming approach uncovered 10 successful attack scenarios, highlighting vulnerabilities in how models handle narrative immersion and emotional pressure. The findings were used to create an automated evaluation framework, MISALIGNMENTBENCH, which revealed a 76% vulnerability rate across tested models. GPT-4.1 was the most susceptible with a 90% vulnerability rate, while Claude-4-Sonnet showed greater resistance at 40%. The study emphasizes the importance of addressing subtle, scenario-based manipulation in future AI systems to enhance alignment strategies and protect against misaligned behavior. 

<br /><br />Summary: <div>
arXiv:2508.04196v1 Announce Type: new 
Abstract: Despite significant advances in alignment techniques, we demonstrate that state-of-the-art language models remain vulnerable to carefully crafted conversational scenarios that can induce various forms of misalignment without explicit jailbreaking. Through systematic manual red-teaming with Claude-4-Opus, we discovered 10 successful attack scenarios, revealing fundamental vulnerabilities in how current alignment methods handle narrative immersion, emotional pressure, and strategic framing. These scenarios successfully elicited a range of misaligned behaviors, including deception, value drift, self-preservation, and manipulative reasoning, each exploiting different psychological and contextual vulnerabilities. To validate generalizability, we distilled our successful manual attacks into MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible testing across multiple models. Cross-model evaluation of our 10 scenarios against five frontier LLMs revealed an overall 76% vulnerability rate, with significant variations: GPT-4.1 showed the highest susceptibility (90%), while Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate that sophisticated reasoning capabilities often become attack vectors rather than protective mechanisms, as models can be manipulated into complex justifications for misaligned behavior. This work provides (i) a detailed taxonomy of conversational manipulation patterns and (ii) a reusable evaluation framework. Together, these findings expose critical gaps in current alignment strategies and highlight the need for robustness against subtle, scenario-based manipulation in future AI systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts</title>
<link>https://arxiv.org/abs/2508.04199</link>
<guid>https://arxiv.org/abs/2508.04199</guid>
<content:encoded><![CDATA[
<div> Keywords: sentiment analysis, cultural nuances, large language models, interpretation, social-science measurement<br />
Summary:<br />
The article discusses the challenges of sentiment analysis in low-resource, culturally nuanced contexts, focusing on informal, code-mixed WhatsApp messages from Nairobi youth health groups. It introduces a diagnostic framework that views sentiment as a context-dependent and culturally embedded construct. The study evaluates how large language models (LLMs) interpret sentiment using human-annotated data and counterfactuals. Through rubric-based explanation evaluation, the researchers assess LLMs' interpretability, robustness, and alignment with human reasoning. By operationalizing LLMs' outputs as a measurement instrument for sentiment, the findings show varying model reasoning quality, with top-tier LLMs exhibiting interpretive stability. However, open models struggle with ambiguity and sentiment shifts. The research underscores the importance of culturally sensitive and reasoning-aware AI evaluation in real-world communication scenarios. <br />
Summary: <div>
arXiv:2508.04199v1 Announce Type: new 
Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe LLM interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate LLMs outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier LLMs demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world communication.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments</title>
<link>https://arxiv.org/abs/2508.04204</link>
<guid>https://arxiv.org/abs/2508.04204</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Reasoning Models, ReasoningGuard, safeguard, malicious content, inference-time

Summary:
Large Reasoning Models (LRMs) have shown strong performance in reasoning tasks but are susceptible to malicious content generation. Existing defense mechanisms are expensive and limited in scalability. To address this, ReasoningGuard is proposed as an inference-time safeguard for LRMs. By analyzing the model's internal attention behavior, it identifies critical points in the reasoning process and prompts safety-oriented reflection at the right moments. Additionally, a scaling sampling strategy is implemented during decoding to select optimal reasoning paths while minimizing extra inference cost. ReasoningGuard successfully defends against various jailbreak attacks targeting LRMs' reasoning processes, outperforming existing safeguards and avoiding overcompensating for safety. This approach provides state-of-the-art safety defenses for LRMs without compromising performance. 

<br /><br />Summary: <div>
arXiv:2508.04204v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in reasoning-intensive tasks, but they remain vulnerable to harmful content generation, particularly in the mid-to-late steps of their reasoning processes. Existing defense mechanisms, however, rely on costly fine-tuning and additional expert knowledge, which restricts their scalability. In this work, we propose ReasoningGuard, an inference-time safeguard for LRMs, which injects timely safety aha moments to steer harmless while helpful reasoning processes. Leveraging the model's internal attention behavior, our approach accurately identifies critical points in the reasoning path, and triggers spontaneous, safety-oriented reflection. To safeguard both the subsequent reasoning steps and the final answers, we further implement a scaling sampling strategy during the decoding phase, selecting the optimal reasoning path. Inducing minimal extra inference cost, ReasoningGuard effectively mitigates three types of jailbreak attacks, including the latest ones targeting the reasoning process of LRMs. Our approach outperforms seven existing safeguards, achieving state-of-the-art safety defenses while effectively avoiding the common exaggerated safety issues.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Text Classification Using Black Box Large Language Models</title>
<link>https://arxiv.org/abs/2508.04219</link>
<guid>https://arxiv.org/abs/2508.04219</guid>
<content:encoded><![CDATA[
<div> Hierarchical Text Classification, Large Language Models, API, Prompting Strategies, Zero-shot, Few-shot<br />
Summary:<br />
This study explores the use of black box Large Language Models through APIs for Hierarchical Text Classification, comparing three prompting strategies: Direct Leaf Label Prediction, Direct Hierarchical Label Prediction, and Top-down Multi-step Hierarchical Label Prediction in zero-shot and few-shot settings. Results show that few-shot settings consistently improve classification accuracy. Traditional machine learning models perform well on shallow hierarchies, while LLMs, particularly the DH strategy, outperform on deeper hierarchies. However, higher input tokens for deeper hierarchies increase API costs significantly. This study emphasizes the trade-off between accuracy improvement and computational cost of prompt strategies, highlighting the potential of LLMs for HTC and the importance of strategy selection to balance performance and cost.<br /> <div>
arXiv:2508.04219v1 Announce Type: new 
Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured label hierarchies; however, it faces challenges due to data scarcity and model complexity. This study explores the feasibility of using black box Large Language Models (LLMs) accessed via APIs for HTC, as an alternative to traditional machine learning methods that require extensive labeled data and computational resources. We evaluate three prompting strategies -- Direct Leaf Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and few-shot settings, comparing the accuracy and cost-effectiveness of these strategies. Experiments on two datasets show that a few-shot setting consistently improves classification accuracy compared to a zero-shot setting. While a traditional machine learning model achieves high accuracy on a dataset with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the machine learning model on a dataset with a deeper hierarchy. API costs increase significantly due to the higher input tokens required for deeper label hierarchies on DH strategy. These results emphasize the trade-off between accuracy improvement and the computational cost of prompt strategy. These findings highlight the potential of black box LLMs for HTC while underscoring the need to carefully select a prompt strategy to balance performance and cost.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.04239</link>
<guid>https://arxiv.org/abs/2508.04239</guid>
<content:encoded><![CDATA[
<div> Keywords: Time series forecasting, textual information, dual-prompt framework, large language model, multimodal data <br />
Summary: 
The article introduces DP-GPT4MTS, a dual-prompt framework for time series forecasting that combines task instructions and textual context to improve accuracy. Traditional models often overlook the impact of textual information on forecasting. DP-GPT4MTS integrates multimodal data by generating clear task instructions and context-aware embeddings from time-stamped text. Through self-attention and feed-forward networks, the model refines these embeddings for enhanced performance. Experiments on diverse datasets show that DP-GPT4MTS outperforms existing algorithms in time series forecasting, emphasizing the importance of incorporating textual context for more accurate predictions.<br /><br />Summary: <div>
arXiv:2508.04239v1 Announce Type: new 
Abstract: Time series forecasting is crucial in strategic planning and decision-making across various industries. Traditional forecasting models mainly concentrate on numerical time series data, often overlooking important textual information such as events and news, which can significantly affect forecasting accuracy. While large language models offer a promise for integrating multimodal data, existing single-prompt frameworks struggle to effectively capture the semantics of timestamped text, introducing redundant information that can hinder model performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt GPT2-base for Multimodal Time Series), a novel dual-prompt large language model framework that combines two complementary prompts: an explicit prompt for clear task instructions and a textual prompt for context-aware embeddings from time-stamped data. The tokenizer generates the explicit prompt while the embeddings from the textual prompt are refined through self-attention and feed-forward networks. Comprehensive experiments conducted on diverse textural-numerical time series datasets demonstrate that this approach outperforms state-of-the-art algorithms in time series forecasting. This highlights the significance of incorporating textual context via a dual-prompt mechanism to achieve more accurate time series predictions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening</title>
<link>https://arxiv.org/abs/2508.04248</link>
<guid>https://arxiv.org/abs/2508.04248</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health services, diagnostic models, simulated patients, advanced language models, depression diagnosis

Summary: 
The article discusses the shortage of real training data for developing clinical professionals in the field of mental health services, particularly in diagnosing depression. To address this gap, the authors introduce TalkDep, a clinician-in-the-loop patient simulation pipeline that leverages advanced language models to create diverse and authentic simulated patients. By conditioning the model on diagnostic criteria, symptom severity scales, and contextual factors, the generated patient responses aim to support diagnostic model training and evaluation. The use of validated simulated patients offers a scalable and adaptable resource for improving the accuracy and generalizability of automatic depression diagnosis systems. Clinical professionals have verified the reliability of these simulated patients, highlighting their potential to enhance the development of diagnostic models in mental health services. 

<br /><br />Summary: <div>
arXiv:2508.04248v1 Announce Type: new 
Abstract: The increasing demand for mental health services has outpaced the availability of real training data to develop clinical professionals, leading to limited support for the diagnosis of depression. This shortage has motivated the development of simulated or virtual patients to assist in training and evaluation, but existing approaches often fail to generate clinically valid, natural, and diverse symptom presentations. In this work, we embrace the recent advanced language models as the backbone and propose a novel clinician-in-the-loop patient simulation pipeline, TalkDep, with access to diversified patient profiles to develop simulated patients. By conditioning the model on psychiatric diagnostic criteria, symptom severity scales, and contextual factors, our goal is to create authentic patient responses that can better support diagnostic model training and evaluation. We verify the reliability of these simulated patients with thorough assessments conducted by clinical professionals. The availability of validated simulated patients offers a scalable and adaptable resource for improving the robustness and generalisability of automatic depression diagnosis systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs</title>
<link>https://arxiv.org/abs/2508.04257</link>
<guid>https://arxiv.org/abs/2508.04257</guid>
<content:encoded><![CDATA[
<div> cache quantization, large language models, attention sinks, KV cache, KVSink 

Summary:
The article discusses the optimization technique of Key-Value (KV) cache quantization for large language models (LLMs). It emphasizes the importance of preserving original precision for the first few tokens to protect attention sinks. The study delves into the mechanisms of attention sinks and their role in extreme activation outliers across layers. The introduction of KVSink method aids in predicting sink tokens with minimal overhead, enhancing preservation during KV cache quantization. Extensive experiments show that KVSink surpasses the existing Preserve-First-N (PFN) strategy, leading to more effective preservation of attention sinks. When integrated with the KVQuant method, KVSink not only improves perplexity (PPL) but also reduces reliance on 16-bit numerical outliers. This research provides valuable insights into the interplay between attention sinks and KV cache quantization for efficient inference in large language models. 

<br /><br />Summary: <div>
arXiv:2508.04257v1 Announce Type: new 
Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents</title>
<link>https://arxiv.org/abs/2508.04266</link>
<guid>https://arxiv.org/abs/2508.04266</guid>
<content:encoded><![CDATA[
<div> ShoppingBench, e-commerce, user intents, real-world users, vouchers<br /><br />Summary: Existing e-commerce benchmarks often focus on basic user intents, but real-world users have more complex goals. ShoppingBench is a new benchmark designed to simulate user instructions for tasks like applying vouchers and finding multi-product sellers. The benchmark includes over 2.5 million real-world products and challenges even advanced language agents like GPT-4.1, with success rates below 50%. To improve performance, a trajectory distillation strategy is proposed using supervised fine-tuning and reinforcement learning on synthetic trajectories. The trained agent achieves competitive performance against GPT-4.1. <div>
arXiv:2508.04266v1 Announce Type: new 
Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such as finding or purchasing products. However, real-world users often pursue more complex goals, such as applying vouchers, managing budgets, and finding multi-products seller. To bridge this gap, we propose ShoppingBench, a novel end-to-end shopping benchmark designed to encompass increasingly challenging levels of grounded intent. Specifically, we propose a scalable framework to simulate user instructions based on various intents derived from sampled real-world products. To facilitate consistent and reliable evaluations, we provide a large-scale shopping sandbox that serves as an interactive simulated environment, incorporating over 2.5 million real-world products. Experimental results demonstrate that even state-of-the-art language agents (such as GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks, highlighting the significant challenges posed by our ShoppingBench. In addition, we propose a trajectory distillation strategy and leverage supervised fine-tuning, along with reinforcement learning on synthetic trajectories, to distill the capabilities of a large language agent into a smaller one. As a result, our trained agent achieves competitive performance compared to GPT-4.1.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.04276</link>
<guid>https://arxiv.org/abs/2508.04276</guid>
<content:encoded><![CDATA[
<div> Graph-based Retrieval-Augmented Generation, Knowledge Poisoning Attacks, Targeted KPA, Universal KPA, GraphRAG<br />
Summary:<br />
- GraphRAG enhances large language models by converting text into knowledge graphs for improved accuracy and explainability.<br />
- Proposed two knowledge poisoning attacks (KPAs) that manipulate source text to mislead downstream reasoning.<br />
- Targeted KPA (TKPA) achieves precise control over specific question-answering outcomes with a high success rate.<br />
- Universal KPA (UKPA) disrupts graph structural integrity by globally altering influential words, drastically reducing QA accuracy.<br />
- State-of-the-art defense methods fail to detect these attacks, emphasizing the need to secure GraphRAG pipelines against knowledge poisoning. <br /> 
Summary: <div>
arXiv:2508.04276v1 Announce Type: new 
Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as a promising paradigm for enhancing large language models (LLMs) by converting raw text into structured knowledge graphs, improving both accuracy and explainability. However, GraphRAG relies on LLMs to extract knowledge from raw text during graph construction, and this process can be maliciously manipulated to implant misleading information. Targeting this attack surface, we propose two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a few words in the source text can significantly change the constructed graph, poison the GraphRAG, and severely mislead downstream reasoning. The first attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate vulnerable nodes in the generated graphs and rewrites the corresponding narratives with LLMs, achieving precise control over specific question-answering (QA) outcomes with a success rate of 93.1\%, while keeping the poisoned text fluent and natural. The second attack, named Universal KPA (UKPA), exploits linguistic cues such as pronouns and dependency relations to disrupt the structural integrity of the generated graph by altering globally influential words. With fewer than 0.05\% of full text modified, the QA accuracy collapses from 95\% to 50\%. Furthermore, experiments show that state-of-the-art defense methods fail to detect these attacks, highlighting that securing GraphRAG pipelines against knowledge poisoning remains largely unexplored.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models</title>
<link>https://arxiv.org/abs/2508.04325</link>
<guid>https://arxiv.org/abs/2508.04325</guid>
<content:encoded><![CDATA[
<div> criteria, benchmarks, healthcare, data management, model robustness  
Summary:  
The article introduces MedCheck, a framework designed for assessing medical benchmarks throughout their development stages. It addresses concerns about the reliability of current benchmarks by highlighting issues such as a lack of clinical fidelity, data management issues, and insufficient safety-evaluation metrics. MedCheck consists of 46 medically-tailored criteria and aims to improve the standardization, reliability, and transparency of AI evaluation in healthcare. An empirical evaluation of 53 medical LLM benchmarks using MedCheck revealed widespread problems, including a disconnect from clinical practice, data integrity risks, and a neglect of safety-critical evaluation dimensions. The article emphasizes the importance of implementing MedCheck as a diagnostic tool for existing benchmarks and as a guideline for future benchmark development to enhance the quality of AI evaluation in healthcare.  
<br /><br />Summary: <div>
arXiv:2508.04325v1 Announce Type: new 
Abstract: Large language models (LLMs) show significant potential in healthcare, prompting numerous benchmarks to evaluate their capabilities. However, concerns persist regarding the reliability of these benchmarks, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics. To address these shortcomings, we introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness. Based on these findings, MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling and Classifying the Components of a Literature Review</title>
<link>https://arxiv.org/abs/2508.04337</link>
<guid>https://arxiv.org/abs/2508.04337</guid>
<content:encoded><![CDATA[
<div> annotation schema, literature review generation, large language models, Sci-Sentence, rhetorical roles

Summary:
- A new annotation schema is introduced to support the generation of literature reviews.
- The paper evaluates 37 large language models (LLMs) on classifying rhetorical roles using the Sci-Sentence benchmark.
- LLMs perform well when fine-tuned on high-quality data, achieving over 96% F1 score.
- Both proprietary models like GPT-4o and lightweight open-source alternatives show excellent performance.
- Enriching training data with semi-synthetic examples generated by LLMs improves performance, enabling small encoders to achieve robust results and enhancing open decoder models. 

<br /><br />Summary: <div>
arXiv:2508.04337v1 Announce Type: new 
Abstract: Previous work has demonstrated that AI methods for analysing scientific literature benefit significantly from annotating sentences in papers according to their rhetorical roles, such as research gaps, results, limitations, extensions of existing methodologies, and others. Such representations also have the potential to support the development of a new generation of systems capable of producing high-quality literature reviews. However, achieving this goal requires the definition of a relevant annotation schema and effective strategies for large-scale annotation of the literature. This paper addresses these challenges by 1) introducing a novel annotation schema specifically designed to support literature review generation and 2) conducting a comprehensive evaluation of a wide range of state-of-the-art large language models (LLMs) in classifying rhetorical roles according to this schema. To this end, we also present Sci-Sentence, a novel multidisciplinary benchmark comprising 700 sentences manually annotated by domain experts and 2,240 sentences automatically labelled using LLMs. We evaluate 37 LLMs on this benchmark, spanning diverse model families and sizes, using both zero-shot learning and fine-tuning approaches. The experiments yield several novel insights that advance the state of the art in this challenging domain. First, the current generation of LLMs performs remarkably well on this task when fine-tuned on high-quality data, achieving performance levels above 96\% F1. Second, while large proprietary models like GPT-4o achieve the best results, some lightweight open-source alternatives also demonstrate excellent performance. Finally, enriching the training data with semi-synthetic examples generated by LLMs proves beneficial, enabling small encoders to achieve robust results and significantly enhancing the performance of several open decoder models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</title>
<link>https://arxiv.org/abs/2508.04349</link>
<guid>https://arxiv.org/abs/2508.04349</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Large Language Model, Dynamic Entropy Weighting, Group Token Policy Optimization, Group Relative Policy Optimization.

Summary:
Dynamic Entropy Weighting addresses the limitation of coarse-grained credit assignment in RL algorithms like GRPO for LLM reasoning. By assigning entropy-weighted rewards to tokens, fine-grained credit assignment is achieved through GTPO. Additionally, GRPO-S assigns entropy-weighted rewards to sequences based on average token entropy. Experiments demonstrate the effectiveness of these methods in surpassing the DAPO baseline, showcasing the importance of the entropy-weighting mechanism in enhancing deep reasoning capabilities of models. This approach offers a promising direction for improving performance in long-chain reasoning tasks.<br /><br />Summary: <div>
arXiv:2508.04349v1 Announce Type: new 
Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Questions: Guiding Multimodal Curiosity in Language Models</title>
<link>https://arxiv.org/abs/2508.04350</link>
<guid>https://arxiv.org/abs/2508.04350</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning capabilities, large language models, multimodal context, Chain of Questions, curiosity-driven reasoning

Summary:
The paper introduces the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach for multimodal language models. This framework prompts models to generate targeted questions about their environment, guiding them to activate relevant sensory modalities. By doing so, the models can gather essential information for accurate reasoning and response generation. The framework is evaluated on a new multimodal benchmark dataset, combining various existing datasets. Experimental results show that the CoQ method enhances a model's ability to identify and integrate relevant sensory information, leading to improved accuracy, interpretability, and alignment with diverse multimodal tasks. The framework represents a significant advancement in enabling large language models to effectively reason in complex real-world environments.<br /><br />Summary: <div>
arXiv:2508.04350v1 Announce Type: new 
Abstract: Reasoning capabilities in large language models (LLMs) have substantially advanced through methods such as chain-of-thought and explicit step-by-step explanations. However, these improvements have not yet fully transitioned to multimodal contexts, where models must proactively decide which sensory modalities such as vision, audio, or spatial perception to engage when interacting with complex real-world environments. In this paper, we introduce the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach that encourages multimodal language models to dynamically generate targeted questions regarding their surroundings. These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning and response generation. We evaluate our framework on a novel multimodal benchmark dataset, assembled by integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results demonstrate that our CoQ method improves a foundation model's ability to effectively identify and integrate pertinent sensory information. This leads to improved accuracy, interpretability, and alignment of the reasoning process with diverse multimodal tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIC CTU@FEVER 8: On-premise fact checking through long context RAG</title>
<link>https://arxiv.org/abs/2508.04390</link>
<guid>https://arxiv.org/abs/2508.04390</guid>
<content:encoded><![CDATA[
<div> pipeline, fact-checking, RAG, FEVER 8, state-of-the-art

Summary:
The paper discusses a fact-checking pipeline that achieved first place in the FEVER 8 shared task. The system is a two-step RAG pipeline based on a previous submission. Despite resource constraints of a single NVidia A10 GPU, 23GB of graphical memory, and a 60-second running time per claim, the pipeline was able to achieve state-of-the-art fact-checking performance. The system showcases how it can be redeployed on-premise with impressive results, particularly in terms of the Ev2R test-score. The methodology described in the paper can provide valuable insights for researchers and practitioners working in the field of fact-checking and natural language processing. <div>
arXiv:2508.04390v1 Announce Type: new 
Abstract: In this paper, we present our fact-checking pipeline which has scored first in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG pipeline based on our last year's submission. We show how the pipeline can be redeployed on-premise, achieving state-of-the-art fact-checking performance (in sense of Ev2R test-score), even under the constraint of a single NVidia A10 GPU, 23GB of graphical memory and 60s running time per claim.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky</title>
<link>https://arxiv.org/abs/2508.04399</link>
<guid>https://arxiv.org/abs/2508.04399</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, crash data quality, Kentucky, transformer models, performance evaluation


Summary: <br /><br />This study evaluates the use of advanced natural language processing techniques to improve crash data quality by analyzing crash narratives in Kentucky. Three model classes were compared, including zero-shot large language models, fine-tuned transformers, and traditional logistic regression, with fine-tuned transformers performing best. RoBERTa achieved the highest F1-score and accuracy, outperforming other models. While large language models excelled in recall, they were computationally costly compared to fine-tuned models. Mid-sized LLMs showed promise in balancing performance and runtime. Practical deployment considerations include privacy-preserving local deployment, ensemble approaches, and incremental processing for scalability. Overall, fine-tuned transformer models offer a balanced approach to improving crash-data quality with advanced NLP techniques. <div>
arXiv:2508.04399v1 Announce Type: new 
Abstract: This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why are LLMs' abilities emergent?</title>
<link>https://arxiv.org/abs/2508.04401</link>
<guid>https://arxiv.org/abs/2508.04401</guid>
<content:encoded><![CDATA[
<div> neural networks, deep learning, emergent properties, complex dynamics, scaling laws
Summary: 
This paper explores the emergent properties of Deep Neural Networks (DNNs) through theoretical analysis and empirical observation, addressing the challenge of "creation without understanding" in AI development. It highlights how DNNs rely on nonlinear, stochastic processes, leading to macro-level capabilities that cannot be analytically derived from micro-level activities. The analysis of scaling laws, grokking phenomena, and phase transitions demonstrates that emergent abilities stem from highly sensitive nonlinear systems rather than just parameter scaling. The paper argues that understanding LLM capabilities requires recognizing DNNs as complex dynamical systems governed by universal principles of emergence seen in other natural phenomena. By shifting the focus from phenomenological definitions to internal dynamic transformations, it emphasizes the cooperative interactions among simple components in DNNs that result in transcendent capabilities. <div>
arXiv:2508.04401v1 Announce Type: new 
Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks has raised fundamental questions about the nature of their acquired capabilities, which often appear to emerge unexpectedly without explicit training. This paper examines the emergent properties of Deep Neural Networks (DNNs) through both theoretical analysis and empirical observation, addressing the epistemological challenge of "creation without understanding" that characterises contemporary AI development. We explore how the neural approach's reliance on nonlinear, stochastic processes fundamentally differs from symbolic computational paradigms, creating systems whose macro-level behaviours cannot be analytically derived from micro-level neuron activities. Through analysis of scaling laws, grokking phenomena, and phase transitions in model capabilities, I demonstrate that emergent abilities arise from the complex dynamics of highly sensitive nonlinear systems rather than simply from parameter scaling alone. My investigation reveals that current debates over metrics, pre-training loss thresholds, and in-context learning miss the fundamental ontological nature of emergence in DNNs. I argue that these systems exhibit genuine emergent properties analogous to those found in other complex natural phenomena, where systemic capabilities emerge from cooperative interactions among simple components without being reducible to their individual behaviours. The paper concludes that understanding LLM capabilities requires recognising DNNs as a new domain of complex dynamical systems governed by universal principles of emergence, similar to those operating in physics, chemistry, and biology. This perspective shifts the focus from purely phenomenological definitions of emergence to understanding the internal dynamic transformations that enable these systems to acquire capabilities that transcend their individual components.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems</title>
<link>https://arxiv.org/abs/2508.04402</link>
<guid>https://arxiv.org/abs/2508.04402</guid>
<content:encoded><![CDATA[
<div> Keywords: spoken dialogue systems, automatic speech recognition, selective listening, transcriptions, evaluation method

Summary:<br /><br />
This study explores the concept of selective listening in humans and its relevance to automatic speech recognition (ASR) in spoken dialogue systems (SDSs). Selective listening refers to the ability to focus on and extract important information during a conversation. The researchers conducted experiments to compare human transcriptions of dialogue responses with reference transcriptions, confirming the existence of selective listening in dialogue generation. The results suggest the potential for a new ASR evaluation method that incorporates human selective listening skills. This approach could help identify discrepancies in transcription accuracy between ASR systems and human listeners, highlighting areas for improvement in SDSs. By understanding how humans selectively listen and process information, researchers can enhance the performance of ASR technology in dialogue systems to facilitate more accurate and meaningful interactions. <div>
arXiv:2508.04402v1 Announce Type: new 
Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at the front end of their pipeline. The role of ASR in SDSs is to recognize information in user speech related to response generation appropriately. Examining selective listening of humans, which refers to the ability to focus on and listen to important parts of a conversation during the speech, will enable us to identify the ASR capabilities required for SDSs and evaluate them. In this study, we experimentally confirmed selective listening when humans generate dialogue responses by comparing human transcriptions for generating dialogue responses and reference transcriptions. Based on our experimental results, we discuss the possibility of a new ASR evaluation method that leverages human selective listening, which can identify the gap between transcription ability between ASR systems and humans.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model</title>
<link>https://arxiv.org/abs/2508.04403</link>
<guid>https://arxiv.org/abs/2508.04403</guid>
<content:encoded><![CDATA[
<div> prediction confidence model, user-perceived latency, dialogue systems, prefetching, language models

Summary: 
The study focuses on reducing user-perceived latency (UPL) in spoken dialogue systems by prefetching dialogue responses. A prediction confidence model (PCM) is proposed to determine the possibility of prefetching by estimating semantic similarity between predicted and actual user utterances. The PCM evaluates differences between predicted and actual user utterances to improve prefetching accuracy. The goal is to predict complete user utterances before the end of the user's speech to minimize UPL. By utilizing language models and semantic similarity estimation, the PCM aims to enhance the efficiency of dialogue response preparation. The study evaluates the PCM's performance based on its ability to accurately predict user utterances and reduce UPL in spoken dialogue interactions. Overall, the PCM offers a framework for improving prefetching capabilities in dialogue systems, ultimately enhancing user experience through reduced waiting times. 

<br /><br />Summary: <div>
arXiv:2508.04403v1 Announce Type: new 
Abstract: Prefetching of dialogue responses has been investigated to reduce user-perceived latency (UPL), which refers to the user's waiting time before receiving the system's response, in spoken dialogue systems. To reduce the UPL, it is necessary to predict complete user utterances before the end of the user's speech, typically by language models, to prepare prefetched dialogue responses. In this study, we proposed a prediction confidence model (PCM) that determines whether prefetching is possible or not by estimating the semantic similarity between the predicted complete user utterance and the complete user utterance. We evaluated our PCM based on the differences between the predicted complete user utterance and the complete user utterance.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating, Synthesizing, and Enhancing for Customer Support Conversation</title>
<link>https://arxiv.org/abs/2508.04423</link>
<guid>https://arxiv.org/abs/2508.04423</guid>
<content:encoded><![CDATA[
<div> Keywords: Customer Support Conversation, COPC guidelines, CSConv dataset, LLMs, Role-playing approach

Summary: 
The article introduces the Customer Support Conversation (CSC) task, focusing on training customer service agents to utilize well-defined support strategies. A structured CSC framework based on COPC guidelines guides high-quality interactions through five conversational stages and twelve strategies. The CSConv dataset, consisting of 1,855 real-world customer-agent conversations rewritten with deliberate strategy use, is created and annotated. A role-playing approach using LLM-powered roles aligned with the CSC framework results in the RoleCS training dataset. Fine-tuning strong LLMs on RoleCS demonstrates significant improvements in generating high-quality, strategy-aligned responses on CSConv, leading to enhanced problem resolution according to human evaluations. The code and data will be publicly available at https://github.com/aliyun/qwen-dianjin. 

<br /><br />Summary: <div>
arXiv:2508.04423v1 Announce Type: new 
Abstract: Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion</title>
<link>https://arxiv.org/abs/2508.04440</link>
<guid>https://arxiv.org/abs/2508.04440</guid>
<content:encoded><![CDATA[
arXiv:2508.04440v1 Announce Type: new 
Abstract: Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI</title>
<link>https://arxiv.org/abs/2508.04442</link>
<guid>https://arxiv.org/abs/2508.04442</guid>
<content:encoded><![CDATA[
arXiv:2508.04442v1 Announce Type: new 
Abstract: This paper addresses the critical need for scalable and high-quality educational assessment tools within the Malaysian education system. It highlights the potential of Generative AI (GenAI) while acknowledging the significant challenges of ensuring factual accuracy and curriculum alignment, especially for low-resource languages like Bahasa Melayu. This research introduces and compares four incremental pipelines for generating Form 1 Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's GPT-4o. The methods range from non-grounded prompting (structured and basic) to Retrieval-Augmented Generation (RAG) approaches (one using the LangChain framework, one implemented manually). The system is grounded in official curriculum documents, including teacher-prepared notes and the yearly teaching plan (RPT). A dual-pronged automated evaluation framework is employed to assess the generated questions. Curriculum alignment is measured using Semantic Textual Similarity (STS) against the RPT, while contextual validity is verified through a novel RAG-based Question-Answering (RAG-QA) method. The results demonstrate that RAG-based pipelines significantly outperform non-grounded prompting methods, producing questions with higher curriculum alignment and factual validity. The study further analyzes the trade-offs between the ease of implementation of framework-based RAG and the fine-grained control offered by a manual pipeline. This work presents a validated methodology for generating curriculum-specific educational content in a low-resource language, introduces a symbiotic RAG-QA evaluation technique, and provides actionable insights for the development and deployment of practical EdTech solutions in Malaysia and similar regions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation</title>
<link>https://arxiv.org/abs/2508.04494</link>
<guid>https://arxiv.org/abs/2508.04494</guid>
<content:encoded><![CDATA[
arXiv:2508.04494v1 Announce Type: new 
Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt in different contexts, and the semantic relations that exist between meanings of different words. To investigate them, Contextualized Language Models are a valuable tool that provides context-sensitive representations that can be used to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the task of Word-in-Context to fine-tune them to get more semantically accurate representations, but Word-in-Context only compares occurrences of the same lemma, limiting the range of captured information. In this paper, we propose an extension, Concept Differentiation, to include inter-words scenarios. We provide a dataset for this task, derived from SemCor data. Then we fine-tune several representation models on this dataset. We call these models Concept-Aligned Embeddings (CALE). By challenging our models and other models on various lexical semantic tasks, we demonstrate that the proposed models provide efficient multi-purpose representations of lexical meaning that reach best performances in our experiments. We also show that CALE's fine-tuning brings valuable changes to the spatial organization of embeddings.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering</title>
<link>https://arxiv.org/abs/2508.04530</link>
<guid>https://arxiv.org/abs/2508.04530</guid>
<content:encoded><![CDATA[
arXiv:2508.04530v1 Announce Type: new 
Abstract: Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the model's core truthfulness representations, resulting in reduced answer correctness. We term this phenomenon stylization-induced truthfulness collapse. We attribute this issue to latent coupling between style and truth directions in certain key attention heads, and propose StyliTruth, a mechanism that preserves stylization while keeping truthfulness intact. StyliTruth separates the style-relevant and truth-relevant subspaces in the model's representation space via an orthogonal deflation process. This decomposition enables independent control of style and truth in their own subspaces, minimizing interference. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness. We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning</title>
<link>https://arxiv.org/abs/2508.04531</link>
<guid>https://arxiv.org/abs/2508.04531</guid>
<content:encoded><![CDATA[
arXiv:2508.04531v1 Announce Type: new 
Abstract: Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.04575</link>
<guid>https://arxiv.org/abs/2508.04575</guid>
<content:encoded><![CDATA[
arXiv:2508.04575v1 Announce Type: new 
Abstract: While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning</title>
<link>https://arxiv.org/abs/2508.04581</link>
<guid>https://arxiv.org/abs/2508.04581</guid>
<content:encoded><![CDATA[
arXiv:2508.04581v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TURA: Tool-Augmented Unified Retrieval Agent for AI Search</title>
<link>https://arxiv.org/abs/2508.04604</link>
<guid>https://arxiv.org/abs/2508.04604</guid>
<content:encoded><![CDATA[
arXiv:2508.04604v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider</title>
<link>https://arxiv.org/abs/2508.04623</link>
<guid>https://arxiv.org/abs/2508.04623</guid>
<content:encoded><![CDATA[
arXiv:2508.04623v1 Announce Type: new 
Abstract: Text-to-SQL translation enables non-expert users to query relational databases using natural language, with applications in education and business intelligence. This study evaluates three lightweight transformer models - T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on low-resource settings. We developed a reusable, model-agnostic pipeline that tailors schema formatting to each model's architecture, training them across 1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2 (20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL generation. Despite resource constraints limiting performance, our pipeline's modularity supports future enhancements, such as advanced schema linking or alternative base models. This work underscores the potential of compact transformers for accessible text-to-SQL solutions in resource-scarce environments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis</title>
<link>https://arxiv.org/abs/2508.04626</link>
<guid>https://arxiv.org/abs/2508.04626</guid>
<content:encoded><![CDATA[
arXiv:2508.04626v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2508.04632</link>
<guid>https://arxiv.org/abs/2508.04632</guid>
<content:encoded><![CDATA[
arXiv:2508.04632v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech</title>
<link>https://arxiv.org/abs/2508.04638</link>
<guid>https://arxiv.org/abs/2508.04638</guid>
<content:encoded><![CDATA[
arXiv:2508.04638v1 Announce Type: new 
Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has gained traction in NLP as a promising intervention. While early work emphasised collaboration with non-governmental organisation stakeholders, recent research trends have shifted toward automated pipelines that reuse a small set of legacy datasets, often without input from affected communities. This paper presents a systematic review of 74 NLP studies on counterspeech, analysing the extent to which stakeholder participation influences dataset creation, model development, and evaluation. To complement this analysis, we conducted a participatory case study with five NGOs specialising in online Gender-Based Violence (oGBV), identifying stakeholder-informed practices for counterspeech generation. Our findings reveal a growing disconnect between current NLP research and the needs of communities most impacted by toxic online content. We conclude with concrete recommendations for re-centring stakeholder expertise in counterspeech research.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs</title>
<link>https://arxiv.org/abs/2508.04660</link>
<guid>https://arxiv.org/abs/2508.04660</guid>
<content:encoded><![CDATA[
arXiv:2508.04660v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</title>
<link>https://arxiv.org/abs/2508.04664</link>
<guid>https://arxiv.org/abs/2508.04664</guid>
<content:encoded><![CDATA[
arXiv:2508.04664v1 Announce Type: new 
Abstract: Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay</title>
<link>https://arxiv.org/abs/2508.04676</link>
<guid>https://arxiv.org/abs/2508.04676</guid>
<content:encoded><![CDATA[
arXiv:2508.04676v1 Announce Type: new 
Abstract: The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data</title>
<link>https://arxiv.org/abs/2508.04698</link>
<guid>https://arxiv.org/abs/2508.04698</guid>
<content:encoded><![CDATA[
arXiv:2508.04698v1 Announce Type: new 
Abstract: LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis</title>
<link>https://arxiv.org/abs/2508.04699</link>
<guid>https://arxiv.org/abs/2508.04699</guid>
<content:encoded><![CDATA[
arXiv:2508.04699v1 Announce Type: new 
Abstract: The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved ("hops"), completeness in capturing relevant information ("coverage"), and cognitive inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MD-LLM-1: A Large Language Model for Molecular Dynamics</title>
<link>https://arxiv.org/abs/2508.03709</link>
<guid>https://arxiv.org/abs/2508.03709</guid>
<content:encoded><![CDATA[
arXiv:2508.03709v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) is a powerful approach for modelling molecular systems, but it remains computationally intensive on spatial and time scales of many macromolecular systems of biological interest. To explore the opportunities offered by deep learning to address this problem, we introduce a Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how LLMs can be leveraged to learn protein dynamics and discover states not seen in training. By applying MD-LLM-1, the first implementation of this approach, obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein systems, we show that training on one conformational state enables the prediction of other conformational states. These results indicate that MD-LLM-1 can learn the principles for the exploration of the conformational landscapes of proteins, although it is not yet modeling explicitly their thermodynamics and kinetics.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Social Data-Driven System for Identifying Estate-related Events and Topics</title>
<link>https://arxiv.org/abs/2508.03711</link>
<guid>https://arxiv.org/abs/2508.03711</guid>
<content:encoded><![CDATA[
arXiv:2508.03711v1 Announce Type: cross 
Abstract: Social media platforms such as Twitter and Facebook have become deeply embedded in our everyday life, offering a dynamic stream of localized news and personal experiences. The ubiquity of these platforms position them as valuable resources for identifying estate-related issues, especially in the context of growing urban populations. In this work, we present a language model-based system for the detection and classification of estate-related events from social media content. Our system employs a hierarchical classification framework to first filter relevant posts and then categorize them into actionable estate-related topics. Additionally, for posts lacking explicit geotags, we apply a transformer-based geolocation module to infer posting locations at the point-of-interest level. This integrated approach supports timely, data-driven insights for urban management, operational response and situational awareness.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding</title>
<link>https://arxiv.org/abs/2508.03718</link>
<guid>https://arxiv.org/abs/2508.03718</guid>
<content:encoded><![CDATA[
arXiv:2508.03718v1 Announce Type: cross 
Abstract: U.S. health insurance is complex, and inadequate understanding and limited access to justice have dire implications for the most vulnerable. Advances in natural language processing present an opportunity to support efficient, case-specific understanding, and to improve access to justice and healthcare. Yet existing corpora lack context necessary for assessing even simple cases. We collect and release a corpus of reputable legal and medical text related to U.S. health insurance. We also introduce an outcome prediction task for health insurance appeals designed to support regulatory and patient self-help applications, and release a labeled benchmark for our task, and models trained on it.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.03733</link>
<guid>https://arxiv.org/abs/2508.03733</guid>
<content:encoded><![CDATA[
arXiv:2508.03733v1 Announce Type: cross 
Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on "one-time" diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind, the first generative model to achieve interleaved "think-answer" reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO: Trajectory-Based Policy Optimization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[
arXiv:2508.03772v1 Announce Type: cross 
Abstract: Policy-based optimizations are widely adopted today for the training and alignment of language models, where one of the most recent and effective approaches is Group-relative Policy Optimization (GRPO). In this paper, we reveals and analyze two major limitations of GRPO: (i) tokens frequently appear in completions with both positive and negative rewards, leading to conflicting gradient updates that can reduce their output probability, even though can be essential for maintaining proper structure; (ii) negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, progressively flattening the output distribution and degrading learning. To address these issues and provide a more stable and effective policy optimization strategy, we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which identifies conflict tokens, tokens appearing in the same position across completions with opposite rewards, protects them by skipping negative updates, while amplifying positive ones. To further prevent policy collapse, GTPO filters out completions whose entropy exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources</title>
<link>https://arxiv.org/abs/2508.03828</link>
<guid>https://arxiv.org/abs/2508.03828</guid>
<content:encoded><![CDATA[
arXiv:2508.03828v1 Announce Type: cross 
Abstract: We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles with their citations and scraped web sources; articles are represented in a rich data structure, and scraped source texts are stored inline with precise character offsets of their citations in the article text. MegaWika 2 is a major upgrade from the original MegaWika, spanning six times as many articles and twice as many fully scraped citations. Both MegaWika and MegaWika 2 support report generation research ; whereas MegaWika also focused on supporting question answering and retrieval applications, MegaWika 2 is designed to support fact checking and analyses across time and language.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants</title>
<link>https://arxiv.org/abs/2508.03936</link>
<guid>https://arxiv.org/abs/2508.03936</guid>
<content:encoded><![CDATA[
arXiv:2508.03936v1 Announce Type: cross 
Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software development, but their safety remains deeply uncertain-especially in high-stakes domains like cybersecurity. Current red-teaming tools often rely on fixed benchmarks or unrealistic prompts, missing many real-world vulnerabilities. We present ASTRA, an automated agent system designed to systematically uncover safety flaws in AI-driven code generation and security guidance systems. ASTRA works in three stages: (1) it builds structured domain-specific knowledge graphs that model complex software tasks and known weaknesses; (2) it performs online vulnerability exploration of each target model by adaptively probing both its input space, i.e., the spatial exploration, and its reasoning processes, i.e., the temporal exploration, guided by the knowledge graphs; and (3) it generates high-quality violation-inducing cases to improve model alignment. Unlike prior methods, ASTRA focuses on realistic inputs-requests that developers might actually ask-and uses both offline abstraction guided domain modeling and online domain knowledge graph adaptation to surface corner-case vulnerabilities. Across two major evaluation domains, ASTRA finds 11-66% more issues than existing techniques and produces test cases that lead to 17% more effective alignment training, showing its practical value for building safer AI systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers</title>
<link>https://arxiv.org/abs/2508.03962</link>
<guid>https://arxiv.org/abs/2508.03962</guid>
<content:encoded><![CDATA[
arXiv:2508.03962v1 Announce Type: cross 
Abstract: The growing volume of scientific literature makes it challenging for scientists to move from a list of papers to a synthesized understanding of a topic. Because of the constant influx of new papers on a daily basis, even if a scientist identifies a promising set of papers, they still face the tedious task of individually reading through dozens of titles and abstracts to make sense of occasionally conflicting findings. To address this critical bottleneck in the research workflow, we introduce a summarization feature to BIP! Finder, a scholarly search engine that ranks literature based on distinct impact aspects like popularity and influence. Our approach enables users to generate two types of summaries from top-ranked search results: a concise summary for an instantaneous at-a-glance comprehension and a more comprehensive literature review-style summary for greater, better-organized comprehension. This ability dynamically leverages BIP! Finder's already existing impact-based ranking and filtering features to generate context-sensitive, synthesized narratives that can significantly accelerate literature discovery and comprehension.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval</title>
<link>https://arxiv.org/abs/2508.04001</link>
<guid>https://arxiv.org/abs/2508.04001</guid>
<content:encoded><![CDATA[
arXiv:2508.04001v1 Announce Type: cross 
Abstract: Conversational search aims to satisfy users' complex information needs via multiple-turn interactions. The key challenge lies in revealing real users' search intent from the context-dependent queries. Previous studies achieve conversational search by fine-tuning a conversational dense retriever with relevance judgments between pairs of context-dependent queries and documents. However, this training paradigm encounters data scarcity issues. To this end, we propose ConvMix, a mixed-criteria framework to augment conversational dense retrieval, which covers more aspects than existing data augmentation frameworks. We design a two-sided relevance judgment augmentation schema in a scalable manner via the aid of large language models. Besides, we integrate the framework with quality control mechanisms to obtain semantically diverse samples and near-distribution supervisions to combine various annotated data. Experimental results on five widely used benchmarks show that the conversational dense retriever trained by our ConvMix framework outperforms previous baseline methods, which demonstrates our superior effectiveness.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities</title>
<link>https://arxiv.org/abs/2508.04118</link>
<guid>https://arxiv.org/abs/2508.04118</guid>
<content:encoded><![CDATA[
arXiv:2508.04118v1 Announce Type: cross 
Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in an ever-changing world, especially when considering the continual emergence of new entities in daily news. Existing approaches for KGC mainly rely on pretrained language models' parametric knowledge, pre-constructed queries, or single-step retrieval, typically requiring substantial supervision and training data. Even so, they often fail to capture comprehensive and up-to-date information about unpopular and/or emerging entities. To this end, we introduce Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework that combines iterative retrieval actions and multi-step reasoning to dynamically construct rich knowledge graph triplets. Experiments show that, despite requiring zero training efforts, AgREE significantly outperforms existing methods in constructing knowledge graph triplets, especially for emerging entities that were not seen during language models' training processes, outperforming previous methods by up to 13.7%. Moreover, we propose a new evaluation methodology that addresses a fundamental weakness of existing setups and a new benchmark for KGC on emerging entities. Our work demonstrates the effectiveness of combining agent-based reasoning with strategic information retrieval for maintaining up-to-date knowledge graphs in dynamic information environments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COPO: Consistency-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2508.04138</link>
<guid>https://arxiv.org/abs/2508.04138</guid>
<content:encoded><![CDATA[
arXiv:2508.04138v1 Announce Type: cross 
Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Source Tracing of Speech Deepfakes: A First Benchmark</title>
<link>https://arxiv.org/abs/2508.04143</link>
<guid>https://arxiv.org/abs/2508.04143</guid>
<content:encoded><![CDATA[
arXiv:2508.04143v1 Announce Type: cross 
Abstract: Recent progress in generative AI has made it increasingly easy to create natural-sounding deepfake speech from just a few seconds of audio. While these tools support helpful applications, they also raise serious concerns by making it possible to generate convincing fake speech in many languages. Current research has largely focused on detecting fake speech, but little attention has been given to tracing the source models used to generate it. This paper introduces the first benchmark for multilingual speech deepfake source tracing, covering both mono- and cross-lingual scenarios. We comparatively investigate DSP- and SSL-based modeling; examine how SSL representations fine-tuned on different languages impact cross-lingual generalization performance; and evaluate generalization to unseen languages and speakers. Our findings offer the first comprehensive insights into the challenges of identifying speech generation models when training and inference languages differ. The dataset, protocol and code are available at https://github.com/xuanxixi/Multilingual-Source-Tracing.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations</title>
<link>https://arxiv.org/abs/2508.04166</link>
<guid>https://arxiv.org/abs/2508.04166</guid>
<content:encoded><![CDATA[
arXiv:2508.04166v1 Announce Type: cross 
Abstract: The 2025 Global Risks Report identifies state-based armed conflict and societal polarisation among the most pressing global threats, with social media playing a central role in amplifying toxic discourse. Memes, as a widely used mode of online communication, often serve as vehicles for spreading harmful content. However, limitations in data accessibility and the high cost of dataset curation hinder the development of robust meme moderation systems. To address this challenge, in this work, we introduce a first-of-its-kind dataset of 6,300 real-world meme-based posts annotated in two stages: (i) binary classification into toxic and normal, and (ii) fine-grained labelling of toxic memes as hateful, dangerous, or offensive. A key feature of this dataset is that it is enriched with auxiliary metadata of socially relevant tags, enhancing the context of each meme. In addition, we propose a tag generation module that produces socially grounded tags, because most in-the-wild memes often do not come with tags. Experimental results show that incorporating these tags substantially enhances the performance of state-of-the-art VLMs detection tasks. Our contributions offer a novel and scalable foundation for improved content moderation in multimodal online environments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Representation Learning with Massive Unlabeled Data for Rumor Detection</title>
<link>https://arxiv.org/abs/2508.04252</link>
<guid>https://arxiv.org/abs/2508.04252</guid>
<content:encoded><![CDATA[
arXiv:2508.04252v1 Announce Type: cross 
Abstract: With the development of social media, rumors spread quickly, cause great harm to society and economy. Thereby, many effective rumor detection methods have been developed, among which the rumor propagation structure learning based methods are particularly effective compared to other methods. However, the existing methods still suffer from many issues including the difficulty to obtain large-scale labeled rumor datasets, which leads to the low generalization ability and the performance degeneration on new events since rumors are time-critical and usually appear with hot topics or newly emergent events. In order to solve the above problems, in this study, we used large-scale unlabeled topic datasets crawled from the social media platform Weibo and Twitter with claim propagation structure to improve the semantic learning ability of a graph reprentation learing model on various topics. We use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE in two commonly used training strategies, to verify the performance of general graph semi-supervised methods in rumor detection tasks. In addition, for alleviating the time and topic difference between unlabeled topic data and rumor data, we also collected a rumor dataset covering a variety of topics over a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our experiments show that these general graph self-supervised learning methods outperform previous methods specifically designed for rumor detection tasks and achieve good performance under few-shot conditions, demonstrating the better generalization ability with the help of our massive unlabeled topic dataset.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents</title>
<link>https://arxiv.org/abs/2508.04412</link>
<guid>https://arxiv.org/abs/2508.04412</guid>
<content:encoded><![CDATA[
arXiv:2508.04412v1 Announce Type: cross 
Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At that, a model poses as an instantaneous domain model backend. Ought to suggest interaction, it is consulted with a web-based task and respective application state. The key problem lies in application state serialisation $\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are premised on grounded GUI snapshots, i.e., screenshots enhanced with visual cues. Not least to resemble human perception, but for images representing relatively cheap means of model input. LLM vision still lag behind code interpretation capabilities. DOM snapshots, which structurally resemble HTML, impose a desired alternative. Vast model input token size, however, disables reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input token order of magnitude (1e3). Our best evaluated configurations $\unicode{x2013}$ one token order above, but within the model's context window $\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2508.04469</link>
<guid>https://arxiv.org/abs/2508.04469</guid>
<content:encoded><![CDATA[
arXiv:2508.04469v1 Announce Type: cross 
Abstract: The deployment of vision-language models remains constrained by substantial computational requirements. We present \textbf{FrEVL}, a framework exploring whether frozen pretrained embeddings can support effective vision-language understanding. Our analysis reveals that frozen embeddings contain rich information for discriminative tasks, achieving 85\% to 95\% of state-of-the-art performance on standard benchmarks with only 68.4M trainable parameters. This performance dichotomy reveals a critical insight: frozen embedding effectiveness depends on alignment between pretraining objectives and downstream task requirements. When accounting for end-to-end computation including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\% lower energy consumption, making it suitable for scenarios with pre-computable inputs or when deployment constraints outweigh marginal performance gains. Our evaluation provides practitioners with guidance on when frozen embedding approaches represent viable alternatives to full model deployment. We will release our complete implementation and evaluation framework to facilitate further research into efficient multi-modal understanding.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use</title>
<link>https://arxiv.org/abs/2508.04482</link>
<guid>https://arxiv.org/abs/2508.04482</guid>
<content:encoded><![CDATA[
arXiv:2508.04482v1 Announce Type: cross 
Abstract: The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Reflection with Language Models</title>
<link>https://arxiv.org/abs/2508.04495</link>
<guid>https://arxiv.org/abs/2508.04495</guid>
<content:encoded><![CDATA[
arXiv:2508.04495v1 Announce Type: cross 
Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with robust causal reasoning, often relying on spurious correlations and brittle patterns. Similarly, traditional Reinforcement Learning agents also lack causal understanding, optimizing for rewards without modeling why actions lead to outcomes. We introduce Causal Reflection, a framework that explicitly models causality as a dynamic function over state, action, time, and perturbation, enabling agents to reason about delayed and nonlinear effects. Additionally, we define a formal Reflect mechanism that identifies mismatches between predicted and observed outcomes and generates causal hypotheses to revise the agent's internal model. In this architecture, LLMs serve not as black-box reasoners, but as structured inference engines translating formal causal outputs into natural language explanations and counterfactuals. Our framework lays the theoretical groundwork for Causal Reflective agents that can adapt, self-correct, and communicate causal understanding in evolving environments.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing and Mitigating Object Hallucination: A Training Bias Perspective</title>
<link>https://arxiv.org/abs/2508.04567</link>
<guid>https://arxiv.org/abs/2508.04567</guid>
<content:encoded><![CDATA[
arXiv:2508.04567v1 Announce Type: cross 
Abstract: As scaling up training data has significantly improved the general multimodal capabilities of Large Vision-Language Models (LVLMs), they still suffer from the hallucination issue, generating text that is inconsistent with the visual input. This phenomenon motivates us to systematically investigate the role of training data in hallucination. We introduce a new benchmark, POPEv2, which consists of counterfactual images collected from the training data of LVLMs with certain objects masked. Through comprehensive evaluation on POPEv2, we find that current LVLMs suffer from training bias: they fail to fully leverage their training data and hallucinate more frequently on images seen during training. Specifically, they perform poorly on counterfactual images, often incorrectly answering ``Yes'' to questions about masked objects. To understand this issue, we conduct probing experiments on the models' internal components, revealing that this training bias is primarily located in the language modeling (LM) head. Based on these findings, we propose Obliviate, an efficient and lightweight unlearning method designed to mitigate object hallucination via training bias unlearning. Obliviate identifies the discrepancy between ground-truth labels and model outputs on the training data as a proxy for bias and adopts a parameter- and data-efficient fine-tuning strategy that only updates the LM head. Extensive experiments demonstrate the effectiveness of our approach. While only reusing the training data and updating approximately 2\% of the parameters, Obliviate significantly reduces hallucination across both discriminative and generative tasks. Furthermore, it demonstrates strong scalability with respect to both model size (2B to 72B) and training data volume, and exhibits promising generalization to hallucination types beyond object-level hallucination. Our code and data will be publicly released.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation</title>
<link>https://arxiv.org/abs/2508.04571</link>
<guid>https://arxiv.org/abs/2508.04571</guid>
<content:encoded><![CDATA[
arXiv:2508.04571v1 Announce Type: cross 
Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by integrating heterogeneous content, such as images and textual metadata. While effective, it remains unclear whether their gains stem from true multimodal understanding or increased model complexity. This work investigates the role of multimodal item embeddings, emphasizing the semantic informativeness of the representations. Initial experiments reveal that embeddings from standard extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on modality-specific encoders and ad hoc fusion strategies that lack control over cross-modal alignment. To overcome these limitations, we leverage Large Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via structured prompts. This approach yields semantically aligned representations without requiring any fusion. Experiments across multiple settings show notable performance improvements. Furthermore, LVLMs embeddings offer a distinctive advantage: they can be decoded into structured textual descriptions, enabling direct assessment of their multimodal comprehension. When such descriptions are incorporated as side content into recommender systems, they improve recommendation performance, empirically validating the semantic depth and alignment encoded within LVLMs outputs. Our study highlights the importance of semantically rich representations and positions LVLMs as a compelling foundation for building robust and meaningful multimodal representations in recommendation tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference</title>
<link>https://arxiv.org/abs/2508.04586</link>
<guid>https://arxiv.org/abs/2508.04586</guid>
<content:encoded><![CDATA[
arXiv:2508.04586v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering</title>
<link>https://arxiv.org/abs/2508.04683</link>
<guid>https://arxiv.org/abs/2508.04683</guid>
<content:encoded><![CDATA[
arXiv:2508.04683v1 Announce Type: cross 
Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that enhances search precision and relevance by decomposing open text queries into structured metadata tags and semantic elements. QAM addresses traditional search limitations by automatically extracting metadata filters from free-form text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique items with 40,000+ reviews and detailed product attributes) demonstrated QAM's superior performance, achieving a mean average precision at 5 (mAP@5) of 52.99\%. This represents significant improvement over conventional methods, including BM25 keyword search, encoder-based semantic similarity search, cross-encoder re-ranking, and hybrid search combining BM25 and semantic results via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust solution for Enterprise Search applications, particularly in e-commerce systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</title>
<link>https://arxiv.org/abs/2508.04700</link>
<guid>https://arxiv.org/abs/2508.04700</guid>
<content:encoded><![CDATA[
arXiv:2508.04700v1 Announce Type: cross 
Abstract: Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions</title>
<link>https://arxiv.org/abs/2406.14805</link>
<guid>https://arxiv.org/abs/2406.14805</guid>
<content:encoded><![CDATA[
arXiv:2406.14805v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness Definitions in Language Models Explained</title>
<link>https://arxiv.org/abs/2407.18454</link>
<guid>https://arxiv.org/abs/2407.18454</guid>
<content:encoded><![CDATA[
arXiv:2407.18454v2 Announce Type: replace 
Abstract: Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their transformer architecture: encoder-only, decoder-only, and encoder-decoder LMs. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The repository is publicly available online at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parse Trees Guided LLM Prompt Compression</title>
<link>https://arxiv.org/abs/2409.15395</link>
<guid>https://arxiv.org/abs/2409.15395</guid>
<content:encoded><![CDATA[
arXiv:2409.15395v2 Announce Type: replace 
Abstract: Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated</title>
<link>https://arxiv.org/abs/2410.03723</link>
<guid>https://arxiv.org/abs/2410.03723</guid>
<content:encoded><![CDATA[
arXiv:2410.03723v2 Announce Type: replace 
Abstract: As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as "Human Generated," over those labeled "AI Generated," by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Conversational Search</title>
<link>https://arxiv.org/abs/2410.15576</link>
<guid>https://arxiv.org/abs/2410.15576</guid>
<content:encoded><![CDATA[
arXiv:2410.15576v2 Announce Type: replace 
Abstract: As a cornerstone of modern information access, search engines have become indispensable in everyday life. With the rapid advancements in AI and natural language processing (NLP) technologies, particularly large language models (LLMs), search engines have evolved to support more intuitive and intelligent interactions between users and systems. Conversational search, an emerging paradigm for next-generation search engines, leverages natural language dialogue to facilitate complex and precise information retrieval, thus attracting significant attention. Unlike traditional keyword-based search engines, conversational search systems enhance user experience by supporting intricate queries, maintaining context over multi-turn interactions, and providing robust information integration and processing capabilities. Key components such as query reformulation, search clarification, conversational retrieval, and response generation work in unison to enable these sophisticated interactions. In this survey, we explore the recent advancements and potential future directions in conversational search, examining the critical modules that constitute a conversational search system. We highlight the integration of LLMs in enhancing these systems and discuss the challenges and opportunities that lie ahead in this dynamic field. Additionally, we provide insights into real-world applications and robust evaluations of current conversational search systems, aiming to guide future research and development in conversational search.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context</title>
<link>https://arxiv.org/abs/2410.16520</link>
<guid>https://arxiv.org/abs/2410.16520</guid>
<content:encoded><![CDATA[
arXiv:2410.16520v4 Announce Type: replace 
Abstract: As our understanding of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first benchmark dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. The dataset comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and is annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and align with human judgments, underscoring their limitations in this domain. We publicly release AUTALIC along with the individual annotations which serve as a valuable resource to researchers working on ableism, neurodiversity, and also studying disagreements in annotation tasks. This dataset serves as a crucial step towards developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision</title>
<link>https://arxiv.org/abs/2411.08397</link>
<guid>https://arxiv.org/abs/2411.08397</guid>
<content:encoded><![CDATA[
arXiv:2411.08397v3 Announce Type: replace 
Abstract: This paper presents CLaSP, a novel model for retrieving time-series signals using natural language queries that describe signal characteristics. The ability to search time-series signals based on descriptive queries is essential in domains such as industrial diagnostics, where data scientists often need to find signals with specific characteristics. However, existing methods rely on sketch-based inputs, predefined synonym dictionaries, or domain-specific manual designs, limiting their scalability and adaptability. CLaSP addresses these challenges by employing contrastive learning to map time-series signals to natural language descriptions. Unlike prior approaches, it eliminates the need for predefined synonym dictionaries and leverages the rich contextual knowledge of large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair time-series signals with natural language descriptions, we demonstrate that CLaSP achieves high accuracy in retrieving a variety of time series patterns based on natural language queries.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs</title>
<link>https://arxiv.org/abs/2412.12422</link>
<guid>https://arxiv.org/abs/2412.12422</guid>
<content:encoded><![CDATA[
arXiv:2412.12422v2 Announce Type: replace 
Abstract: Verifying and attributing factual claims is essential for the safe and effective use of large language models (LLMs) in healthcare. A core component of factuality evaluation is fact decomposition, the process of breaking down complex clinical statements into fine-grained atomic facts for verification. Recent work has proposed fact decomposition, which uses LLMs to rewrite source text into concise sentences conveying a single piece of information, to facilitate fine-grained fact verification. However, clinical documentation poses unique challenges for fact decomposition due to dense terminology and diverse note types and remains understudied. To address this gap and explore these challenges, we present FactEHR, an NLI dataset consisting of document fact decompositions for 2,168 clinical notes spanning four types from three hospital systems, resulting in 987,266 entailment pairs. We assess the generated facts on different axes, from entailment evaluation of LLMs to a qualitative analysis. Our evaluation, including review by the clinicians, reveals substantial variability in LLM performance for fact decomposition. For example, Gemini-1.5-Flash consistently generates relevant and accurate facts, while Llama-3 8B produces fewer and less consistent outputs. The results underscore the need for better LLM capabilities to support factual verification in clinical text.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Unbiased Watermark for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11268</link>
<guid>https://arxiv.org/abs/2502.11268</guid>
<content:encoded><![CDATA[
arXiv:2502.11268v3 Announce Type: replace 
Abstract: As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks</title>
<link>https://arxiv.org/abs/2502.13053</link>
<guid>https://arxiv.org/abs/2502.13053</guid>
<content:encoded><![CDATA[
arXiv:2502.13053v3 Announce Type: replace 
Abstract: As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation</title>
<link>https://arxiv.org/abs/2502.15434</link>
<guid>https://arxiv.org/abs/2502.15434</guid>
<content:encoded><![CDATA[
arXiv:2502.15434v3 Announce Type: replace 
Abstract: Model merging aims to integrate multiple task-specific models into a unified model that inherits the capabilities of the task-specific models, without additional training. Existing model merging methods often lack consideration of the varying contribution ratios of different task-specific models to the final merged model. In this paper, we propose Mixup Model Merge (M3), a simple yet effective method inspired by the randomized linear interpolation strategy from the Mixup data augmentation technique. M3 performs randomized linear interpolation in parameter space between two task-specific LLMs, where interpolation coefficients are sampled from a Beta distribution to explore diverse contribution ratios. This controllable randomness allows M3 to outperform standard equal-ratio merging by discovering better contribution ratio combinations. Extensive experiments show that M3 significantly (1) improves merged LLM performance across tasks, (2) enhances out-of-distribution and adversarial robustness, (3) outperforms the positive effects of the sparsification method DARE on model merging and can be further combined with DARE to achieve superior results, and (4) balances exploration efficiency and diversity in contribution ratios by tuning the Beta distribution's shape parameters. The code is provided in the supplementary materials.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data</title>
<link>https://arxiv.org/abs/2502.16781</link>
<guid>https://arxiv.org/abs/2502.16781</guid>
<content:encoded><![CDATA[
arXiv:2502.16781v2 Announce Type: replace 
Abstract: Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors - imperfect extraction of text, including character insertion, deletion, and substitution can significantly impact downstream tasks like question-answering (QA). In this work, we conduct a comprehensive analysis of how OCR-induced noise affects the performance of Multilingual QA Systems. To support this analysis, we introduce a multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs across three languages, English, French, and German. The dataset is curated from OCR-ed historical documents, which include different levels and types of OCR noise. We then evaluate how different state-of-the-art Large Language models (LLMs) perform under different error conditions, focusing on three major OCR error types. Our findings show that QA systems are highly prone to OCR-induced errors and perform poorly on noisy OCR text. By comparing model performance on clean versus noisy texts, we provide insights into the limitations of current approaches and emphasize the need for more noise-resilient QA systems in historical digitization contexts.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Agentic Large Language Models in Multilingual National Bias</title>
<link>https://arxiv.org/abs/2502.17945</link>
<guid>https://arxiv.org/abs/2502.17945</guid>
<content:encoded><![CDATA[
arXiv:2502.17945v2 Announce Type: replace 
Abstract: Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education. \footnote{Code available at: https://github.com/yiyunya/assess_agentic_national_bias
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.09516</link>
<guid>https://arxiv.org/abs/2503.09516</guid>
<content:encoded><![CDATA[
arXiv:2503.09516v5 Announce Type: replace 
Abstract: Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory</title>
<link>https://arxiv.org/abs/2503.10533</link>
<guid>https://arxiv.org/abs/2503.10533</guid>
<content:encoded><![CDATA[
arXiv:2503.10533v2 Announce Type: replace 
Abstract: High-quality test items are essential for educational assessments, particularly within Item Response Theory (IRT). Traditional validation methods rely on resource-intensive pilot testing to estimate item difficulty and discrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a domain-general approach for evaluating test items based on textual features. This method offers a scalable, pre-deployment evaluation without requiring student data, but its predictive validity concerning empirical IRT parameters is underexplored. To address this gap, we conducted a study involving 7,126 multiple-choice questions across various STEM subjects (physical science, mathematics, and life/earth sciences). Using an automated approach, we annotated each question with a 19-criteria IWF rubric and studied relationships to data-driven IRT parameters. Our analysis revealed statistically significant links between the number of IWFs and IRT difficulty and discrimination parameters, particularly in life/earth and physical science domains. We further observed how specific IWF criteria can impact item quality more and less severely (e.g., negative wording vs. implausible distractors) and how they might make a question more or less challenging. Overall, our findings establish automated IWF analysis as a valuable supplement to traditional validation, providing an efficient method for initial item screening, particularly for flagging low-difficulty MCQs. Our findings show the need for further research on domain-general evaluation rubrics and algorithms that understand domain-specific content for robust item validation.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2503.18878</link>
<guid>https://arxiv.org/abs/2503.18878</guid>
<content:encoded><![CDATA[
arXiv:2503.18878v2 Announce Type: replace 
Abstract: Recent LLMs like DeepSeek-R1 have demonstrated state-of-the-art performance by integrating deep thinking and complex reasoning during generation. However, the internal mechanisms behind these reasoning processes remain unexplored. We observe reasoning LLMs consistently use vocabulary associated with human reasoning processes. We hypothesize these words correspond to specific reasoning moments within the models' internal mechanisms. To test this hypothesis, we employ Sparse Autoencoders (SAEs), a technique for sparse decomposition of neural network activations into human-interpretable features. We introduce ReasonScore, an automatic metric to identify active SAE features during these reasoning moments. We perform manual and automatic interpretation of the features detected by our metric, and find those with activation patterns matching uncertainty, exploratory thinking, and reflection. Through steering experiments, we demonstrate that amplifying these features increases performance on reasoning-intensive benchmarks (+2.2%) while producing longer reasoning traces (+20.5%). Using the model diffing technique, we provide evidence that these features are present only in models with reasoning capabilities. Our work provides the first step towards a mechanistic understanding of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer</title>
<link>https://arxiv.org/abs/2504.12311</link>
<guid>https://arxiv.org/abs/2504.12311</guid>
<content:encoded><![CDATA[
arXiv:2504.12311v3 Announce Type: replace 
Abstract: Prompt tuning has emerged as a lightweight strategy for adapting foundation models to downstream tasks, particularly for resource-constrained systems. As pre-trained prompts become valuable assets, combining multiple source prompts offers a promising approach to enhance generalization for new tasks by leveraging complementary knowledge. However, naive aggregation often overlooks different source prompts have different contribution potential to the target task. To address this, we propose HGPrompt, a dynamic framework that learns optimal ensemble weights. These weights are optimized by jointly maximizing an information-theoretic metric for transferability and minimizing gradient conflicts via a novel regularization strategy. Specifically, we propose a differentiable prompt transferability metric to captures the discriminability of prompt-induced features on the target task. Meanwhile, HGPrompt match the gradient variances with respect to different source prompts based on Hessian and Fisher Information, ensuring stable and coherent knowledge transfer while suppressing gradient conflicts among them. Extensive experiments on the large-scale VTAB benchmark demonstrate the state-of-the-art performance of HGPrompt, validating its effectiveness in learning an optimal ensemble for effective multi-source prompt transfer.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine</title>
<link>https://arxiv.org/abs/2504.12342</link>
<guid>https://arxiv.org/abs/2504.12342</guid>
<content:encoded><![CDATA[
arXiv:2504.12342v2 Announce Type: replace 
Abstract: Recent development in Retrieval-Augmented Large Language Models (LLMs) have shown great promise in biomedical applications. How ever, a critical gap persists in reliably evaluating their curation ability the process by which models select and integrate relevant references while filtering out noise. To address this, we introduce the benchmark for Curation of Retrieval-Augmented LLMs in Biomedicine (CRAB), the first multilingual benchmark tailored for evaluating the biomedical curation of retrieval-augmented LLMs, available in English, French, German and Chinese. By incorporating a novel citation-based evaluation metric, CRAB quantifies the curation performance of retrieval-augmented LLMs in biomedicine. Experimental results reveal significant discrepancies in the curation performance of mainstream LLMs, underscoring the urgent need to improve it in the domain of biomedicine. Our dataset is available at https://huggingface.co/datasets/zhm0/CRAB.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models</title>
<link>https://arxiv.org/abs/2504.14194</link>
<guid>https://arxiv.org/abs/2504.14194</guid>
<content:encoded><![CDATA[
arXiv:2504.14194v4 Announce Type: replace 
Abstract: The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose four dimensions to evaluate data quality: professionalism, readability, reasoning, and cleanliness. We further introduce Meta-rater,a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with advantages that scale to models as large as 7.2B parameters. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability. To advance future research, we release scripts, data, and models at https://github.com/opendatalab/Meta-rater.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study</title>
<link>https://arxiv.org/abs/2504.16414</link>
<guid>https://arxiv.org/abs/2504.16414</guid>
<content:encoded><![CDATA[
arXiv:2504.16414v2 Announce Type: replace 
Abstract: In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the fact-checking performance of language models by relying on their entailment ability</title>
<link>https://arxiv.org/abs/2505.15050</link>
<guid>https://arxiv.org/abs/2505.15050</guid>
<content:encoded><![CDATA[
arXiv:2505.15050v2 Announce Type: replace 
Abstract: Automated fact-checking is a crucial task in this digital age. The NLP community has been trying various strategies to build robust fact-checking systems. However, we have not been very successful yet. One main reason behind this is that fact verification is a complex process. Language models have to parse through multiple pieces of evidence, often contradicting each other, to predict a claim's veracity. In this paper, we proposed a simple yet effective strategy, where we relied on the entailment ability of language models to improve the fact-checking performance. Apart from that, we did a comparison of different prompting and fine-tuning strategies, as it is currently lacking in the literature. Some of our observations are: (i) training language models with raw evidence sentences (TBE-1) and overall claim-evidence understanding (TBE-2) resulted in an improvement up to 8.20% and 16.39% in macro-F1 for RAW-FC dataset, and (ii) training language models with entailed justifications (TBE-3) outperformed the baselines by a huge margin (up to 28.57% and 44.26% for LIAR-RAW and RAW-FC, respectively). We have shared our code repository to reproduce the results.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16227</link>
<guid>https://arxiv.org/abs/2505.16227</guid>
<content:encoded><![CDATA[
arXiv:2505.16227v2 Announce Type: replace 
Abstract: Personalizing jargon detection and explanation is essential for making technical documents accessible to readers with diverse disciplinary backgrounds. However, tailoring models to individual users typically requires substantial annotation efforts and computational resources due to user-specific finetuning. To address this, we present a systematic study of personalized jargon detection, focusing on methods that are both efficient and scalable for real-world deployment. We explore two personalization strategies: (1) lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models, and (2) personalized prompting, which tailors model behavior at inference time without retaining. To reflect realistic constraints, we also investigate hybrid approaches that combine limited annotated data with unsupervised user background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably, our method achieves comparable performance using only 10% of the annotated training data, demonstrating its practicality for resource-constrained settings. Our study offers the first work to systematically explore efficient, low-resource personalization of jargon detection using open-source language models, offering a practical path toward scalable, user-adaptive NLP system.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators</title>
<link>https://arxiv.org/abs/2505.18601</link>
<guid>https://arxiv.org/abs/2505.18601</guid>
<content:encoded><![CDATA[
arXiv:2505.18601v2 Announce Type: replace 
Abstract: Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs</title>
<link>https://arxiv.org/abs/2505.22548</link>
<guid>https://arxiv.org/abs/2505.22548</guid>
<content:encoded><![CDATA[
arXiv:2505.22548v2 Announce Type: replace 
Abstract: Long chain-of-thought (CoT) reasoning has shown great promise in enhancing the emotion understanding performance of large language models (LLMs). However, current fixed-length CoT methods struggle to balance reasoning depth and efficiency. Simple tasks (e.g., sentiment classification) are over-reasoned, while complex tasks (e.g., sarcasm understanding) lack depth. To fill this gap, we present Emotion-o1, an adaptive CoT framework that dynamically adjusts reasoning length based on emotion-task complexity. Emotion-o1 is trained by distilling adaptive CoT patterns from a reasoning-oriented LLM, followed by supervised fine-tuning and reinforcement learning with a four-part reward targeting accuracy, brevity, structure, and redundancy. Experimental results on four emotion tasks highlight: (1) Emotion-o1 demonstrates significant improvements over its backbone, with F1 score increases of 10%(Sentiment), 5%(Emotion), 18%(Humor), and 27%(Sarcasm). (2) In sentiment and sarcasm tasks, our 8B model demonstrates superior performance against advanced LLMs, outperforming Grok-3 by 1.1% and Claude-3.7 by 2%. (3) The framework maintains accuracy while reducing reasoning length by 83% compared to OpenAI-o1, demonstrating effective precision-efficiency optimization. Emotion-o1 effectively balances reasoning depth and efficiency for emotion understanding in LLMs.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models</title>
<link>https://arxiv.org/abs/2506.02132</link>
<guid>https://arxiv.org/abs/2506.02132</guid>
<content:encoded><![CDATA[
arXiv:2506.02132v3 Announce Type: replace 
Abstract: Large transformer-based language models dominate modern NLP, yet our understanding of how they encode linguistic information is rooted in studies of early models like BERT and GPT-2. To better understand today's language models, we investigate how 25 models - from classical architectures (BERT, DeBERTa, GPT-2) to modern large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1) - represent lexical identity and inflectional morphology across six typologically diverse languages. Using linear and nonlinear classifiers trained on hidden activations, we predict word lemmas and inflectional features layer by layer. We find that models concentrate lexical information linearly in early layers and increasingly nonlinearly in later layers, while keeping inflectional information uniformly accessible and linearly separable throughout. Additional experiments probe the nature of these encodings: attention and residual analyses examine where within layers information can be recovered, steering vector experiments test what information can be functionally manipulated, and intrinsic dimensionality analyses explore how the representational structure evolves across layers. Remarkably, these encoding patterns emerge across all models we test, despite differences in architecture, size, and training regime (pretrained and instruction-tuned variants). This suggests that, even with substantial advances in LLM technologies, transformer models organize linguistic information in similar ways, indicating that these properties are important for next token prediction and are learned early during pretraining. Our code is available at https://github.com/ml5885/model_internal_sleuthing
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging</title>
<link>https://arxiv.org/abs/2506.05828</link>
<guid>https://arxiv.org/abs/2506.05828</guid>
<content:encoded><![CDATA[
arXiv:2506.05828v2 Announce Type: replace 
Abstract: We introduce FinanceReasoning, a novel benchmark designed to evaluate the reasoning capabilities of large reasoning models (LRMs) in financial numerical reasoning problems. Compared to existing benchmarks, our work provides three key advancements. (1) Credibility: We update 15.6% of the questions from four public datasets, annotating 908 new questions with detailed Python solutions and rigorously refining evaluation standards. This enables an accurate assessment of the reasoning improvements of LRMs. (2) Comprehensiveness: FinanceReasoning covers 67.8% of financial concepts and formulas, significantly surpassing existing datasets. Additionally, we construct 3,133 Python-formatted functions, which enhances LRMs' financial reasoning capabilities through refined knowledge (e.g., 83.2% $\rightarrow$ 91.6% for GPT-4o). (3) Challenge: Models are required to apply multiple financial formulas for precise numerical reasoning on 238 Hard problems. The best-performing model (i.e., OpenAI o1 with PoT) achieves 89.1% accuracy, yet LRMs still face challenges in numerical precision. We demonstrate that combining Reasoner and Programmer models can effectively enhance LRMs' performance (e.g., 83.2% $\rightarrow$ 87.8% for DeepSeek-R1). Our work paves the way for future research on evaluating and improving LRMs in domain-specific complex reasoning tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NameTag 3: A Tool and a Service for Multilingual/Multitagset NER</title>
<link>https://arxiv.org/abs/2506.05949</link>
<guid>https://arxiv.org/abs/2506.05949</guid>
<content:encoded><![CDATA[
arXiv:2506.05949v2 Announce Type: replace 
Abstract: We introduce NameTag 3, an open-source tool and cloud-based web service for multilingual, multidataset, and multitagset named entity recognition (NER), supporting both flat and nested entities. NameTag 3 achieves state-of-the-art results on 21 test datasets in 15 languages and remains competitive on the rest, even against larger models. It is available as a command-line tool and as a cloud-based service, enabling use without local installation. NameTag 3 web service currently provides flat NER for 17 languages, trained on 21 corpora and three NE tagsets, all powered by a single 355M-parameter fine-tuned model; and nested NER for Czech, powered by a 126M fine-tuned model. The source code is licensed under open-source MPL 2.0, while the models are distributed under non-commercial CC BY-NC-SA 4.0. Documentation is available at https://ufal.mff.cuni.cz/nametag, source code at https://github.com/ufal/nametag3, and trained models via https://lindat.cz. The REST service and the web application can be found at https://lindat.mff.cuni.cz/services/nametag/. A demonstration video is available at https://www.youtube.com/watch?v=-gaGnP0IV8A.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UITron-Speech: Towards Automated GUI Agents Based on Speech Instructions</title>
<link>https://arxiv.org/abs/2506.11127</link>
<guid>https://arxiv.org/abs/2506.11127</guid>
<content:encoded><![CDATA[
arXiv:2506.11127v2 Announce Type: replace 
Abstract: Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this issue, we propose replacing text with speech as the instruction input modality for GUI agents, and introduce UITron-Speech, which is the first end-to-end GUI agent capable of directly processing speech instructions and on-device screenshots to predict user actions. To tackle the problem of data scarcity, we synthesize high-quality speech instruction datasets using a random-speaker text-to-speech model. Additionally, we design a mixed-modality training strategy to mitigate the inherent modality imbalance in pre-trained foundation models. Furthermore, we conduct a statistical analysis of the distribution of GUI grounding prediction errors and propose a training-free two-step grounding refinement method to alleviate minor localization deviations. Extensive experiments on multiple benchmarks demonstrate that UITron-Speech achieves robust performance and superior adaptability, underscoring the feasibility and potential of speech-driven GUI agents for more accessible and intelligent human-computer interaction. Our code and datasets are available at https://github.com/UITron-hub/UITron-Speech.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison</title>
<link>https://arxiv.org/abs/2506.14448</link>
<guid>https://arxiv.org/abs/2506.14448</guid>
<content:encoded><![CDATA[
arXiv:2506.14448v2 Announce Type: replace 
Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-RE: Cross-Domain Relation Extraction with RLVR</title>
<link>https://arxiv.org/abs/2507.04642</link>
<guid>https://arxiv.org/abs/2507.04642</guid>
<content:encoded><![CDATA[
arXiv:2507.04642v2 Announce Type: replace 
Abstract: Relation extraction (RE) is a core task in natural language processing. Traditional approaches typically frame RE as a supervised learning problem, directly mapping context to labels-an approach that often suffers from poor out-of-domain (OOD) generalization. Inspired by the workflow of human annotators, we reframe RE as a reasoning task guided by annotation guidelines and introduce R1-RE, the first reinforcement learning with verifiable reward (RLVR) framework for RE tasks. Our method elicits the reasoning abilities of small language models for annotation tasks, resulting in significantly improved OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of approximately 70%, on par with leading proprietary models such as GPT-4o. Additionally, our comprehensive analysis provides novel insights into the training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Queries to Criteria: Understanding How Astronomers Evaluate LLMs</title>
<link>https://arxiv.org/abs/2507.15715</link>
<guid>https://arxiv.org/abs/2507.15715</guid>
<content:encoded><![CDATA[
arXiv:2507.15715v2 Announce Type: replace 
Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other scientific research, but benchmarks for LLM evaluation in general have not kept pace with the increasingly diverse ways that real people evaluate and use these models. In this study, we seek to improve evaluation procedures by building an understanding of how users evaluate LLMs. We focus on a particular use case: an LLM-powered retrieval-augmented generation bot for engaging with astronomical literature, which we deployed via Slack. Our inductive coding of 368 queries to the bot over four weeks and our follow-up interviews with 11 astronomers reveal how humans evaluated this system, including the types of questions asked and the criteria for judging responses. We synthesize our findings into concrete recommendations for building better benchmarks, which we then employ in constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our work offers ways to improve LLM evaluation and ultimately usability, particularly for use in scientific research.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strong Priority and Determinacy in Timed CCS</title>
<link>https://arxiv.org/abs/2403.04618</link>
<guid>https://arxiv.org/abs/2403.04618</guid>
<content:encoded><![CDATA[
arXiv:2403.04618v4 Announce Type: replace-cross 
Abstract: Building on the standard theory of process algebra with priorities, we identify a new scheduling mechanism, called "constructive reduction" which is designed to capture the essence of synchronous programming. The distinctive property of this evaluation strategy is to achieve determinacy-by-construction for multi-cast concurrent communication with shared memory. In the technical setting of CCS extended by clocks and priorities, we prove for a large class of "coherent" processes a confluence property for constructive reductions. We show that under some restrictions, called "pivotability", coherence is preserved by the operators of prefix, summation, parallel composition, restriction and hiding. Since this permits memory and sharing, we are able to cover a strictly larger class of processes compared to those in Milner's classical confluence theory for CCS without priorities.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models for Target Applications</title>
<link>https://arxiv.org/abs/2405.15877</link>
<guid>https://arxiv.org/abs/2405.15877</guid>
<content:encoded><![CDATA[
arXiv:2405.15877v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) significantly enhance the performance of various applications, but they are computationally intensive and energy-demanding. This makes it challenging to deploy them on devices with limited resources, such as personal computers and mobile/wearable devices, and results in substantial inference costs in resource-rich environments like cloud servers. To extend the use of LLMs, we introduce a low-rank decomposition approach to effectively compress these models, tailored to the requirements of specific applications. We observe that LLMs pretrained on general datasets contain many redundant components not needed for particular applications. Our method focuses on identifying and removing these redundant parts, retaining only the necessary elements for the target applications. Specifically, we represent the weight matrices of LLMs as a linear combination of base components. We then prune the irrelevant bases and enhance the model with new bases beneficial for specific applications. Deep compression results on the Llama 2-7b and -13B models, conducted on target applications including mathematical reasoning and code generation, show that our method significantly reduces model size while maintaining comparable accuracy to state-of-the-art low-rank compression techniques.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AVG-LLaVA: An Efficient Large Multimodal Model with Adaptive Visual Granularity</title>
<link>https://arxiv.org/abs/2410.02745</link>
<guid>https://arxiv.org/abs/2410.02745</guid>
<content:encoded><![CDATA[
arXiv:2410.02745v3 Announce Type: replace-cross 
Abstract: Recently, large multimodal models (LMMs) have achieved significant advancements. When dealing with high-resolution images, dominant LMMs typically divide them into multiple local images and a global image, leading to a large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction. Specifically, we first apply the multiple pooling layers to obtain visual tokens at different granularities. Then we propose a visual granularity router, which includes a Transformer layer, an MLP layer, and a voter layer, used to select the appropriate visual granularity based on the image and instruction. Furthermore, we put forward RGLF, a novel training paradigm that aims at aligning the granularity predicted by the router with the preferences of the LMM, without the need for additional manually annotated data. Extensive experiments and analysis show that AVG-LLaVA achieves superior performance across 11 benchmarks, as well as significantly reduces the number of visual tokens and speeds up inference (e.g., an 85.3% reduction in visual tokens and a 2.53$\times$ increase in inference speed on the AI2D benchmark).
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Adapter Retrieval: Latent Geometry-Preserving Composition via Sparse Task Projection</title>
<link>https://arxiv.org/abs/2410.09908</link>
<guid>https://arxiv.org/abs/2410.09908</guid>
<content:encoded><![CDATA[
arXiv:2410.09908v2 Announce Type: replace-cross 
Abstract: Recent advances in parameter-efficient transfer learning have demonstrated the utility of composing LoRA adapters from libraries of pretrained modules. However, most existing approaches rely on simple retrieval heuristics or uniform averaging, which overlook the latent structure of task relationships in representation space. We propose a new framework for adapter reuse that moves beyond retrieval, formulating adapter composition as a geometry-aware sparse reconstruction problem. Specifically, we represent each task by a latent prototype vector derived from the base model's encoder and aim to approximate the target task prototype as a sparse linear combination of retrieved reference prototypes, under an $\ell_1$-regularized optimization objective. The resulting combination weights are then used to blend the corresponding LoRA adapters, yielding a composite adapter tailored to the target task. This formulation not only preserves the local geometric structure of the task representation manifold, but also promotes interpretability and efficient reuse by selecting a minimal set of relevant adapters. We demonstrate the effectiveness of our approach across multiple domains-including medical image segmentation, medical report generation and image synthesis. Our results highlight the benefit of coupling retrieval with latent geometry-aware optimization for improved zero-shot generalization.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically Interpreting Millions of Features in Large Language Models</title>
<link>https://arxiv.org/abs/2410.13928</link>
<guid>https://arxiv.org/abs/2410.13928</guid>
<content:encoded><![CDATA[
arXiv:2410.13928v3 Announce Type: replace-cross 
Abstract: While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-$k$ postprocessing. Our code is available at https://github.com/EleutherAI/sae-auto-interp, and our explanations are available at https://huggingface.co/datasets/EleutherAI/auto_interp_explanations.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay</title>
<link>https://arxiv.org/abs/2412.04449</link>
<guid>https://arxiv.org/abs/2412.04449</guid>
<content:encoded><![CDATA[
arXiv:2412.04449v2 Announce Type: replace-cross 
Abstract: Despite the remarkable performance of multimodal large language models (MLLMs) across diverse tasks, the substantial training and inference costs impede their advancement. In this paper, we propose p-MoD, an efficient MLLM architecture that significantly reduces training and inference costs while maintaining model performance. The majority of computation in MLLMs stems from the overwhelming volume of vision tokens processed by the transformer-based LLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each LLM layer selects essential vision tokens to process while skipping redundant ones. However, integrating MoD into MLLMs is non-trivial. To address the challenges of training and inference stability as well as limited training data, we adapt the MoD module with two novel designs: tanh-gated weight normalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we observe that vision tokens exhibit higher redundancy in deeper layers and thus design a progressive ratio decay (PRD) strategy, which gradually reduces the token retention ratio layer by layer, employing a shifted cosine schedule. This crucial design fully unleashes the potential of MoD, significantly boosting the efficiency and performance of our models. Extensive experiments on two baseline models across 15 benchmarks show that our model matches or even surpasses the performance of corresponding baselines, while requiring only 55.6% TFLOPs and 53.7% KV cache storage during inference, and 77.7% GPU hours during training.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ultra Memory-Efficient On-FPGA Training of Transformers via Tensor-Compressed Optimization</title>
<link>https://arxiv.org/abs/2501.06663</link>
<guid>https://arxiv.org/abs/2501.06663</guid>
<content:encoded><![CDATA[
arXiv:2501.06663v2 Announce Type: replace-cross 
Abstract: Transformer models have achieved state-of-the-art performance across a wide range of machine learning tasks. There is growing interest in training transformers on resource-constrained edge devices due to considerations such as privacy, domain adaptation, and on-device scientific machine learning. However, the significant computational and memory demands required for transformer training often exceed the capabilities of an edge device. Leveraging low-rank tensor compression, this paper presents the first on-FPGA accelerator for end-to-end transformer training. On the algorithm side, we present a bi-directional contraction flow for tensorized transformer training, significantly reducing the computational FLOPS and intra-layer memory costs compared to existing tensor operations. On the hardware side, we store all highly compressed model parameters and gradient information on chip, creating an on-chip-memory-only framework for each stage in training. This reduces off-chip communication and minimizes latency and energy costs. Additionally, we implement custom computing kernels for each training stage and employ intra-layer parallelism and pipe-lining to further enhance run-time and memory efficiency. Through experiments on transformer models within $36.7$ to $93.5$ MB using FP-32 data formats on the ATIS dataset, our tensorized FPGA accelerator could conduct single-batch end-to-end training on the AMD Alevo U50 FPGA, with a memory budget of less than $6$-MB BRAM and $22.5$-MB URAM. Compared to uncompressed training on the NVIDIA RTX 3090 GPU, our on-FPGA training achieves a memory reduction of $30\times$ to $51\times$. Our FPGA accelerator also achieves up to $3.6\times$ less energy cost per epoch compared with tensor Transformer training on an NVIDIA RTX 3090 GPU.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool Unlearning for Tool-Augmented LLMs</title>
<link>https://arxiv.org/abs/2502.01083</link>
<guid>https://arxiv.org/abs/2502.01083</guid>
<content:encoded><![CDATA[
arXiv:2502.01083v2 Announce Type: replace-cross 
Abstract: Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to forget learned tools due to security vulnerabilities, privacy regulations, or tool deprecations. However, ``tool unlearning'' has not been investigated in unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete, the first approach for unlearning tools from tool-augmented LLMs. It implements three key properties to address the above challenges for effective tool unlearning and introduces a new membership inference attack (MIA) model for effective evaluation. Extensive experiments on multiple tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns randomly selected tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild</title>
<link>https://arxiv.org/abs/2503.18892</link>
<guid>https://arxiv.org/abs/2503.18892</guid>
<content:encoded><![CDATA[
arXiv:2503.18892v3 Announce Type: replace-cross 
Abstract: DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the "aha moment"). Notably, we observe the "aha moment" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAIN: Hijacking LLM-Humans Conversations via Malicious System Prompts</title>
<link>https://arxiv.org/abs/2505.16888</link>
<guid>https://arxiv.org/abs/2505.16888</guid>
<content:encoded><![CDATA[
arXiv:2505.16888v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have advanced many applications, but are also known to be vulnerable to adversarial attacks. In this work, we introduce a novel security threat: hijacking AI-human conversations by manipulating LLMs' system prompts to produce malicious answers only to specific targeted questions (e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"), while behaving benignly on others. This attack is detrimental as it can enable malicious actors to exercise large-scale information manipulation by spreading harmful but benign-looking system prompts online. To demonstrate such an attack, we develop CAIN, an algorithm that can automatically curate such harmful system prompts for a specific target question in a black-box setting or without the need to access the LLM's parameters. Evaluated on both open-source and commercial LLMs, CAIN demonstrates significant adversarial impact. In untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves up to 40% F1 degradation on targeted questions while preserving high accuracy on benign inputs. For targeted attacks or forcing LLMs to output specific harmful answers, CAIN achieves over 70% F1 scores on these targeted responses with minimal impact on benign questions. Our results highlight the critical need for enhanced robustness measures to safeguard the integrity and safety of LLMs in real-world applications. All source code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
<link>https://arxiv.org/abs/2506.06382</link>
<guid>https://arxiv.org/abs/2506.06382</guid>
<content:encoded><![CDATA[
arXiv:2506.06382v4 Announce Type: replace-cross 
Abstract: This paper establishes a fundamental impossibility theorem: no LLM capable performing non-trivial knowledge aggregation can simultaneously achieve truthful (internally consistent) knowledge representation, semantic information conservation, complete revelation of relevant knowledge, and knowledge-constrained optimality. This impossibility is not an engineering limitation but arises from the mathematical structure of information aggregation itself. We establish this result by describing the inference process as an auction of ideas, where distributed components compete exploiting their partial knowledge to shape responses. The proof spans three independent mathematical domains: mechanism design theory (Green-Laffont), the theory of proper scoring rules (Savage), and direct architectural analysis of transformers (Log-Sum-Exp convexity). In particular, we show how in the strictly concave settings the score of an aggregate of diverse beliefs strictly exceeds the sum of individual scores. That gap may quantify the creation of unattributable certainty or overconfidence -- the mathematical origin of both hallucination and creativity, or imagination.
  To support this analysis, we introduce the complementary concepts of the semantic information measure and the emergence operator to model bounded reasoning in a general setting. We prove that while bounded reasoning generates accessible information, providing valuable insights and inspirations, idealized reasoning strictly preserves semantic content. By demonstrating that hallucination and imagination are mathematically identical phenomena-grounded in the necessary violation of information conservation-this paper offers a principled foundation for managing these behaviors in advanced AI systems. Finally, we present some speculative ideas to inspire evaluation and refinements of the proposed theory.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v2 Announce Type: replace-cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under [this https URL](https://github.com/AI45Lab/IS-Bench).
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Anchors: Which LLM Reasoning Steps Matter?</title>
<link>https://arxiv.org/abs/2506.19143</link>
<guid>https://arxiv.org/abs/2506.19143</guid>
<content:encoded><![CDATA[
arXiv:2506.19143v3 Announce Type: replace-cross 
Abstract: Reasoning large language models have recently achieved state-of-the-art performance in many fields. However, their long-form chain-of-thought reasoning creates interpretability challenges as each generated token depends on all previous ones, making the computation harder to decompose. We argue that analyzing reasoning traces at the sentence level is a promising approach to understanding reasoning processes. We present three complementary attribution methods: (1) a black-box method measuring each sentence's counterfactual importance by comparing final answers across 100 rollouts conditioned on the model generating that sentence or one with a different meaning; (2) a white-box method of aggregating attention patterns between pairs of sentences, which identified "broadcasting" sentences that receive disproportionate attention from all future sentences via "receiver" attention heads; (3) a causal attribution method measuring logical connections between sentences by suppressing attention toward one sentence and measuring the effect on each future sentence's tokens. Each method provides evidence for the existence of thought anchors, reasoning steps that have outsized importance and that disproportionately influence the subsequent reasoning process. These thought anchors are typically planning or backtracking sentences. We provide an open-source tool (www.thought-anchors.com) for visualizing the outputs of our methods, and present a case study showing converging patterns across methods that map how a model performs multi-step reasoning. The consistency across methods demonstrates the potential of sentence-level analysis for a deeper understanding of reasoning models.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Specialized LLMs as Dense Retrievers</title>
<link>https://arxiv.org/abs/2507.03958</link>
<guid>https://arxiv.org/abs/2507.03958</guid>
<content:encoded><![CDATA[
arXiv:2507.03958v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) are increasingly deployed as dense retrievers, the impact of their domain-specific specialization on retrieval effectiveness remains underexplored. This investigation systematically examines how task-specific adaptations in LLMs influence their retrieval capabilities, an essential step toward developing unified retrievers capable of handling text, code, images, and multimodal content. We conduct extensive experiments with eight Qwen2.5 7B LLMs, including base, instruction-tuned, code/math-specialized, long reasoning, and vision-language models across zero-shot retrieval settings and the supervised setting. For the zero-shot retrieval settings, we consider text retrieval from the BEIR benchmark and code retrieval from the CoIR benchmark. Further, to evaluate supervised performance, all LLMs are fine-tuned on the MS MARCO dataset. We find that mathematical specialization and the long reasoning capability cause consistent degradation in three settings, indicating conflicts between mathematical reasoning and semantic matching. The vision-language model and code-specialized LLMs demonstrate superior zero-shot performance compared to other LLMs, even surpassing BM25 on the code retrieval task, and maintain comparable performance to base LLMs in supervised settings. These findings suggest promising directions for the unified retrieval task leveraging cross-domain and cross-modal fusion.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextASR-Bench: A Massive Contextual Speech Recognition Benchmark</title>
<link>https://arxiv.org/abs/2507.05727</link>
<guid>https://arxiv.org/abs/2507.05727</guid>
<content:encoded><![CDATA[
arXiv:2507.05727v2 Announce Type: replace-cross 
Abstract: Automatic Speech Recognition (ASR) has been extensively investigated, yet prior benchmarks have largely focused on assessing the acoustic robustness of ASR models, leaving evaluations of their linguistic capabilities relatively underexplored. This largely stems from the limited parameter sizes and training corpora of conventional ASR models, leaving them with insufficient world knowledge, which is crucial for accurately recognizing named entities across diverse domains. For instance, drug and treatment names in medicine or specialized technical terms in engineering. Recent breakthroughs in Large Language Models (LLMs) and corresponding Large Audio Language Models (LALMs) have markedly enhanced the visibility of advanced context modeling and general artificial intelligence capabilities. Leveraging LLMs, we envision a unified system capable of robust speech recognition across diverse real-world domains, yet existing benchmarks are inadequate for evaluating this objective. To address this gap, we propose ContextASR-Bench: a comprehensive, large-scale benchmark designed to assess the linguistic competence of ASR systems using corpora that feature numerous named entities across multiple domains. It encompasses up to 40,000 data entries with more than 300,000 named entities across over 10 domains. Beyond the audio and its transcription, each sample provides the domain it belongs to and a list of named entities it contains, which are referred to as the context. Based on this, we introduce three evaluation modes to assess how effectively models can exploit such context to improve ASR accuracy. Extensive evaluation on ContextASR-Bench highlights that LALMs outperform conventional ASR models by a large margin thanks to the strong world knowledge and context modeling of LLMs, yet there remains ample room for further improvement. The dataset and evaluation code have been released.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation</title>
<link>https://arxiv.org/abs/2507.17937</link>
<guid>https://arxiv.org/abs/2507.17937</guid>
<content:encoded><![CDATA[
arXiv:2507.17937v2 Announce Type: replace-cross 
Abstract: Memorization in generative models extends far beyond verbatim text reproduction--it manifests through non-literal patterns, semantic associations, and surprisingly, across modalities in transcript-conditioned generation tasks such as Lyrics-to-Song (L2S) and Text-to-Video (T2V) models. We reveal a new class of cross-modality memorization where models trained on these tasks leak copyrighted content through indirect, phonetic pathways invisible to traditional text-based analysis. In this work, we introduce Adversarial PhoneTic Prompting (APT), an attack that replaces iconic phrases with homophonic alternatives--e.g., "mom's spaghetti" becomes "Bob's confetti"--preserving the acoustic form while largely changing semantic content. We demonstrate that models can be prompted to regurgitate memorized songs using phonetically similar but semantically unrelated lyrics. Despite the semantic drift, black-box models like SUNO and open-source models like YuE generate outputs that are strikingly similar to the original songs--melodically, rhythmically, and vocally--achieving high scores on AudioJudge, CLAP, and CoverID. These effects persist across genres and languages. More surprisingly, we find that phonetic prompts alone can trigger visual memorization in text-to-video models: when given altered lyrics from Lose Yourself, Veo 3 generates scenes that mirror the original music video--complete with a hooded rapper and dim urban settings--despite no explicit visual cues in the prompt. This cross-modality leakage represents an unprecedented threat: models memorize deep, structural patterns that transcend their training modality, making traditional safety measures like copyright filters ineffective. Our findings reveal a fundamental vulnerability in transcript-conditioned generative models and raise urgent concerns around copyright, provenance, and secure deployment of multimodal generation systems.
]]></content:encoded>
<pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation</title>
<link>https://arxiv.org/abs/2508.02808</link>
<guid>https://arxiv.org/abs/2508.02808</guid>
<content:encoded><![CDATA[
<div> Radiological imaging, automated radiology report generation, ICARE, interpretable evaluation framework, large language model agents<br />
Summary: ICARE is a new interpretable evaluation framework for automated radiology report generation. It leverages large language model agents and multiple-choice question answering to generate clinically meaningful questions. Two agents, one with the ground-truth report and the other with the generated report, quiz each other to assess the preservation and consistency of findings. This method provides transparent and interpretable assessment of the reports. Clinician studies show that ICARE aligns significantly better with expert judgment compared to existing metrics. Perturbation analyses demonstrate sensitivity to clinical content and reproducibility, while model comparisons reveal interpretable error patterns. ICARE offers a reliable way to evaluate automated radiology reports and ensure their clinical precision and accuracy.<br /> <div>
arXiv:2508.02808v1 Announce Type: new 
Abstract: Radiological imaging is central to diagnosis, treatment planning, and clinical decision-making. Vision-language foundation models have spurred interest in automated radiology report generation (RRG), but safe deployment requires reliable clinical evaluation of generated reports. Existing metrics often rely on surface-level similarity or behave as black boxes, lacking interpretability. We introduce ICARE (Interpretable and Clinically-grounded Agent-based Report Evaluation), an interpretable evaluation framework leveraging large language model agents and dynamic multiple-choice question answering (MCQA). Two agents, each with either the ground-truth or generated report, generate clinically meaningful questions and quiz each other. Agreement on answers captures preservation and consistency of findings, serving as interpretable proxies for clinical precision and recall. By linking scores to question-answer pairs, ICARE enables transparent, and interpretable assessment. Clinician studies show ICARE aligns significantly more with expert judgment than prior metrics. Perturbation analyses confirm sensitivity to clinical content and reproducibility, while model comparisons reveal interpretable error patterns.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives</title>
<link>https://arxiv.org/abs/2508.02853</link>
<guid>https://arxiv.org/abs/2508.02853</guid>
<content:encoded><![CDATA[
<div> approach, annotator disagreement, NLP tasks, DEM-MoE, demographic aware, synthetic annotations<br />
Summary:<br />
The research introduces DEM-MoE, a model for handling annotator disagreement in NLP tasks by leveraging annotator demographics. By routing inputs to expert subnetworks based on demographics, the model captures structured variation more effectively than previous approaches. DEM-MoE demonstrates strong performance across demographic groups, particularly on datasets with high disagreement. The study explores using LLM-generated synthetic annotations to address sparse demographic coverage, finding moderate alignment with human annotations and potential for enriching training data. Strategies for blending real and synthetic data are proposed and evaluated based on dataset structure, highlighting the importance of tailored approaches. These innovations collectively enhance the representation of diverse perspectives in subjective NLP tasks. <br /> <div>
arXiv:2508.02853v1 Announce Type: new 
Abstract: We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address sparse demographic coverage, we test whether LLM-generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Highlight &amp; Summarize: RAG without the jailbreaks</title>
<link>https://arxiv.org/abs/2508.02872</link>
<guid>https://arxiv.org/abs/2508.02872</guid>
<content:encoded><![CDATA[
<div> Keywords: Preventing jailbreaking, Large Language Models, Retrieval-augmented generation, Highlight & Summarize, Generation quality

Summary:
Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is challenging due to the potential for malicious users to manipulate the system. Existing approaches rely on system prompt hardening or content classifiers, but these can be easily bypassed. The Highlight & Summarize (H&amp;S) design pattern for retrieval-augmented generation (RAG) systems aims to prevent attacks by splitting the pipeline into a highlighter and summarizer, hiding the user question from the generative LLM. Evaluations show that H&amp;S responses, especially when using an LLM-based highlighter, are often superior in correctness, relevance, and response quality compared to standard RAG pipelines. This approach provides a more secure and effective method for generating natural language answers to questions while protecting the LLM from potential attacks.<br /><br />Summary: <div>
arXiv:2508.02872v1 Announce Type: new 
Abstract: Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is an important yet challenging task. For example, when interacting with a chatbot, malicious users can input specially crafted prompts to cause the LLM to generate undesirable content or perform a completely different task from its intended purpose. Existing mitigations for such attacks typically rely on hardening the LLM's system prompt or using a content classifier trained to detect undesirable content or off-topic conversations. However, these probabilistic approaches are relatively easy to bypass due to the very large space of possible inputs and undesirable outputs. In this paper, we present and evaluate Highlight & Summarize (H&amp;S), a new design pattern for retrieval-augmented generation (RAG) systems that prevents these attacks by design. The core idea is to perform the same task as a standard RAG pipeline (i.e., to provide natural language answers to questions, based on relevant sources) without ever revealing the user's question to the generative LLM. This is achieved by splitting the pipeline into two components: a highlighter, which takes the user's question and extracts relevant passages ("highlights") from the retrieved documents, and a summarizer, which takes the highlighted passages and summarizes them into a cohesive answer. We describe several possible instantiations of H&amp;S and evaluate their generated responses in terms of correctness, relevance, and response quality. Surprisingly, when using an LLM-based highlighter, the majority of H&amp;S responses are judged to be better than those of a standard RAG pipeline.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages</title>
<link>https://arxiv.org/abs/2508.02885</link>
<guid>https://arxiv.org/abs/2508.02885</guid>
<content:encoded><![CDATA[
<div> Keywords: syntax, Merge, linguistics, neurocognitive, comprehension

Summary: 
Syntax, specifically the computational operation of 'Merge', is a crucial component of modern language sciences. Merge combines linguistic units to form categorized structures, respecting non-associativity and abstract grouping. This operation is considered elemental and may have emerged in a single evolutionary step. From a neurocognitive perspective, different mechanisms support mental objects constructed by Merge, such as simple command constructions, adjective-noun mergers, and noun-preposition mergers. The study investigates participants' comprehension of sentences with increasing syntactic complexity, revealing three distinct structural types. These structures may develop at different stages and be subject to selective impairment, suggesting a nuanced cognitive processing of Merge-based objects. While Merge-based syntax may have rapidly emerged in evolutionary history, the processing of different merge types appears to engage distinct cognitive mechanisms. <div>
arXiv:2508.02885v1 Announce Type: new 
Abstract: In the modern language sciences, the core computational operation of syntax, 'Merge', is defined as an operation that combines two linguistic units (e.g., 'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase). This can then be further combined with additional linguistic units based on this categorial information, respecting non-associativity such that abstract grouping is respected. Some linguists have embraced the view that Merge is an elementary, indivisible operation that emerged in a single evolutionary step. From a neurocognitive standpoint, different mental objects constructed by Merge may be supported by distinct mechanisms: (1) simple command constructions (e.g., "eat apples"); (2) the merging of adjectives and nouns ("red boat"); and (3) the merging of nouns with spatial prepositions ("laptop behind the sofa"). Here, we systematically investigate participants' comprehension of sentences with increasing levels of syntactic complexity. Clustering analyses revealed behavioral evidence for three distinct structural types, which we discuss as potentially emerging at different developmental stages and subject to selective impairment. While a Merge-based syntax may still have emerged suddenly in evolutionary time, responsible for the structured symbolic turn our species took, different cognitive mechanisms seem to underwrite the processing of various types of Merge-based objects.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.02886</link>
<guid>https://arxiv.org/abs/2508.02886</guid>
<content:encoded><![CDATA[
<div> framework, LVLMs, reasoning, multimodal, inference <br />
Summary: <br />
The Coherent Multimodal Reasoning Framework (CMRF) addresses the limitations of current large language and vision-language models in complex, multi-step, cross-modal common sense reasoning tasks. It enhances reasoning capabilities by breaking down problems, generating step-by-step inferences, and self-correcting errors. CMRF consists of three modules: Reasoning Decomposition Unit (RDU), Contextual Inference Engine (CIE), and Coherence Assessment Module (CAM), along with Adaptive Iterative Refinement strategy. Trained on a Multimodal Daily Activity Reasoning (MDAR) dataset, CMRF achieves state-of-the-art performance on benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It surpasses baseline accuracy by 2.4 percentage points, excelling in complex reasoning scenarios. Extensive studies and evaluations validate the effectiveness of each module and iterative refinement in improving reasoning coherence and accuracy. <br /> <div>
arXiv:2508.02886v1 Announce Type: new 
Abstract: Despite significant advancements, current large language models (LLMs) and vision-language models (LVLMs) continue to struggle with complex, multi-step, cross-modal common sense reasoning tasks, often exhibiting a lack of "deliberative thinking." They tend to rely on superficial associations rather than deep, chained inference, particularly when integrating visual information with abstract concepts. To address this, we propose the Coherent Multimodal Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense reasoning capabilities through an iterative, self-evaluating inference mechanism. CMRF mimics human problem-solving by decomposing complex queries, generating step-by-step inferences, and self-correcting errors. Our framework integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking down problems into sub-questions, a Contextual Inference Engine (CIE) for contextual inference, and a Coherence Assessment Module (CAM) for evaluating logical consistency and confidence. Coupled with an Adaptive Iterative Refinement strategy, CMRF systematically refines its reasoning paths. Built upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning (MDAR) dataset, CMRF achieves state-of-the-art performance among open-source LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It attains an average accuracy of 69.4%, surpassing the best open-source baseline by +2.4 percentage points, with particular strength in complex reasoning scenarios. Extensive ablation studies and human evaluations confirm the critical contributions of each module and the effectiveness of iterative refinement in fostering more coherent and accurate reasoning.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations</title>
<link>https://arxiv.org/abs/2508.02901</link>
<guid>https://arxiv.org/abs/2508.02901</guid>
<content:encoded><![CDATA[
<div> latent representations, sensorial language, Reduced-Rank Ridge Regression (R4), style dimensions, Stylometrically Lean Interpretable Models (SLIM-LLMs)

Summary: 
The study explores the link between sensorial language and traditional stylistic features using Reduced-Rank Ridge Regression (R4). It shows that low-dimensional latent representations of LIWC features effectively capture stylistic information for sensorial language prediction. The Stylometrically Lean Interpretable Models (SLIM-LLMs) are introduced to model non-linear relationships between style dimensions. Evaluation across five genres indicates that SLIM-LLMs with low-rank LIWC features perform comparably to full-scale language models while significantly reducing parameters. The research highlights the importance of sensorial language in communication and demonstrates the efficacy of the proposed methodology in capturing and predicting stylistic elements in text. <div>
arXiv:2508.02901v1 Announce Type: new 
Abstract: Sensorial language -- the language connected to our senses including vision, sound, touch, taste, smell, and interoception, plays a fundamental role in how we communicate experiences and perceptions. We explore the relationship between sensorial language and traditional stylistic features, like those measured by LIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate that low-dimensional latent representations of LIWC features r = 24 effectively capture stylistic information for sensorial language prediction compared to the full feature set (r = 74). We introduce Stylometrically Lean Interpretable Models (SLIM-LLMs), which model non-linear relationships between these style dimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features match the performance of full-scale language models while reducing parameters by up to 80%.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate High-Quality Task-Specific Conversations?</title>
<link>https://arxiv.org/abs/2508.02931</link>
<guid>https://arxiv.org/abs/2508.02931</guid>
<content:encoded><![CDATA[
<div> Controlled Conversation Generation, Parameterization Framework, Language Models, Dialogue Properties, Conversation Quality <br />
<br />
Summary: This paper presents a parameterization framework that allows precise control of conversation quality in large language models by exploring nine key parameters across six dimensions. Experiments with state-of-the-art LLMs show that parameter-based control leads to statistically significant differences in conversation properties, addressing challenges such as topic coherence, knowledge progression, character consistency, and control granularity. The framework offers a standardized approach for conversation quality control with potential applications in education, therapy, customer service, and entertainment. Future work will focus on implementing additional parameters through architectural modifications and creating benchmark datasets for evaluation. <div>
arXiv:2508.02931v1 Announce Type: new 
Abstract: This paper introduces a parameterization framework for controlling conversation quality in large language models. We explore nine key parameters across six dimensions that enable precise specification of dialogue properties. Through experiments with state-of-the-art LLMs, we demonstrate that parameter-based control produces statistically significant differences in generated conversation properties. Our approach addresses challenges in conversation generation, including topic coherence, knowledge progression, character consistency, and control granularity. The framework provides a standardized method for conversation quality control with applications in education, therapy, customer service, and entertainment. Future work will focus on implementing additional parameters through architectural modifications and developing benchmark datasets for evaluation.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors</title>
<link>https://arxiv.org/abs/2508.02997</link>
<guid>https://arxiv.org/abs/2508.02997</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, detection methods, Contextual Co-occurrence Matrix, adversarial prompts, jailbreaks 

Summary: 
This paper addresses the vulnerability of Large Language Models (LLMs) to attacks, specifically jailbreaks that produce harmful responses. The authors propose a novel method that uses the Contextual Co-occurrence Matrix to effectively detect adversarial and jailbreak prompts. The approach leverages the latent space characteristics of these matrices and tensors to achieve a notable F1 score of 0.83, using only 0.5% of labeled prompts. This represents a 96.6% improvement over baselines and demonstrates the strength of the learned patterns, especially in data-scarce environments. Additionally, the method is significantly faster, with speedup ranging from 2.3 to 128.4 times compared to baseline models. The implementation of the proposed method has been made publicly available to support future research and reproducibility. 

<br /><br />Summary: <div>
arXiv:2508.02997v1 Announce Type: new 
Abstract: The widespread use of Large Language Models (LLMs) in many applications marks a significant advance in research and practice. However, their complexity and hard-to-understand nature make them vulnerable to attacks, especially jailbreaks designed to produce harmful responses. To counter these threats, developing strong detection methods is essential for the safe and reliable use of LLMs. This paper studies this detection problem using the Contextual Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce environments. We propose a novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for the effective identification of adversarial and jailbreak prompts. Our evaluations show that this approach achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. This result highlights the strength of our learned patterns, especially when labeled data is scarce. Our method is also significantly faster, speedup ranging from 2.3 to 128.4 times compared to the baseline models. To support future research and reproducibility, we have made our implementation publicly available.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025</title>
<link>https://arxiv.org/abs/2508.03037</link>
<guid>https://arxiv.org/abs/2508.03037</guid>
<content:encoded><![CDATA[
<div> themes, artist perceptions, media narratives, technical jargon, transparency<br />
Summary:<br />
This study analyzes twelve years of discourse surrounding AI-generated art, focusing on artist perspectives. Through a curated analysis of excerpts, five thematic clusters are identified, highlighting a misalignment between artists' concerns and media coverage. The use of technical jargon is noted as a barrier that can sideline important artist issues. The study provides a reproducible methodology and baseline for future research, emphasizing the need for deeper transparency and engagement with artist perspectives in the evolving AI-creative landscape.<br /> <div>
arXiv:2508.03037v1 Announce Type: new 
Abstract: As generative AI continues to reshape artistic production and alternate modes of human expression, artists whose livelihoods are most directly affected have raised urgent concerns about consent, transparency, and the future of creative labor. However, the voices of artists are often marginalized in dominant public and scholarly discourse. This study presents a twelve-year analysis, from 2013 to 2025, of English-language discourse surrounding AI-generated art. It draws from 439 curated 500-word excerpts sampled from opinion articles, news reports, blogs, legal filings, and spoken-word transcripts. Through a reproducible methodology, we identify five stable thematic clusters and uncover a misalignment between artists' perceptions and prevailing media narratives. Our findings highlight how the use of technical jargon can function as a subtle form of gatekeeping, often sidelining the very issues artists deem most urgent. Our work provides a BERTopic-based methodology and a multimodal baseline for future research, alongside a clear call for deeper, transparency-driven engagement with artist perspectives in the evolving AI-creative landscape.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.03098</link>
<guid>https://arxiv.org/abs/2508.03098</guid>
<content:encoded><![CDATA[
<div> Privacy-Aware Decoding, Retrieval-Augmented Generation, extraction attacks, confidential information leakage, Renyi Differential Privacy<br />
<br />
Summary:<br />
Privacy-Aware Decoding (PAD) is proposed as a defense mechanism for Retrieval-Augmented Generation (RAG) systems to protect against extraction attacks that can leak sensitive data during generation. PAD injects calibrated Gaussian noise into token logits during inference, selectively protecting high-risk tokens and minimizing unnecessary noise using context-aware calibration. It ensures privacy through explicit per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs tracked by a Renyi Differential Privacy (RDP) accountant. PAD operates at decoding time with minimal computational overhead and without the need for retraining or corpus-level filtering. Experimental results on real-world datasets show that PAD effectively reduces private information leakage while preserving response utility, outperforming existing defense strategies. This work advances privacy protection in RAG systems and offers a scalable solution for sensitive domains. <div>
arXiv:2508.03098v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses. We propose Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A \renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs. Unlike prior approaches requiring retraining or corpus-level filtering, PAD is model-agnostic and operates entirely at decoding time with minimal computational overhead. Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. Our work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains. Our code is available: https://github.com/wang2226/PAD.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation</title>
<link>https://arxiv.org/abs/2508.03110</link>
<guid>https://arxiv.org/abs/2508.03110</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, retrieval-augmented generation, security vulnerabilities, token-level precise attack, open-domain QA datasets

Summary: 
The article discusses the limitations faced by large language models (LLMs) in providing accurate and up-to-date responses due to issues like hallucinations and outdated knowledge. To address these limitations, the retrieval-augmented generation (RAG) framework integrates external knowledge via a retriever. However, this integration introduces security vulnerabilities where malicious content from external databases can manipulate model outputs. The proposed Token-level Precise Attack on the RAG (TPARAG) framework targets both white-box and black-box RAG systems by leveraging a lightweight white-box LLM to generate and optimize malicious passages at the token level. Experiment results on open-domain QA datasets show that TPARAG outperforms existing approaches in retrieval-stage and end-to-end attack effectiveness, highlighting vulnerabilities in RAG pipelines and providing insights to enhance their robustness.<br /><br />Summary: <div>
arXiv:2508.03110v1 Announce Type: new 
Abstract: While large language models (LLMs) have achieved remarkable success in providing trustworthy responses for knowledge-intensive tasks, they still face critical limitations such as hallucinations and outdated knowledge. To address these issues, the retrieval-augmented generation (RAG) framework enhances LLMs with access to external knowledge via a retriever, enabling more accurate and real-time outputs about the latest events. However, this integration brings new security vulnerabilities: the risk that malicious content in the external database can be retrieved and used to manipulate model outputs. Although prior work has explored attacks on RAG systems, existing approaches either rely heavily on access to the retriever or fail to jointly consider both retrieval and generation stages, limiting their effectiveness, particularly in black-box scenarios. To overcome these limitations, we propose Token-level Precise Attack on the RAG (TPARAG), a novel framework that targets both white-box and black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an attacker to generate and iteratively optimize malicious passages at the token level, ensuring both retrievability and high attack success in generation. Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in retrieval-stage and end-to-end attack effectiveness. These results further reveal critical vulnerabilities in RAG pipelines and offer new insights into improving their robustness.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-lingual Opinions and Emotions Mining in Comparable Documents</title>
<link>https://arxiv.org/abs/2508.03112</link>
<guid>https://arxiv.org/abs/2508.03112</guid>
<content:encoded><![CDATA[
<div> Keywords: comparable texts, sentiment analysis, emotion annotation, cross-lingual method, news agencies<br />
Summary:<br />
This research explores differences in sentiments and emotions across English-Arabic comparable texts. The texts are annotated with sentiment and emotion labels using a cross-lingual method to avoid machine translation. Emotions are labeled using a bilingual emotion lexicon created by translating the English WordNet-Affect lexicon into Arabic. The study assesses the agreement of sentiments and emotions in source-target document pairs, finding that alignment is stronger when texts come from the same news agency. Results indicate that sentiment and emotion annotations align based on the source of the document. The proposed method is language-independent and can be applied to other language pairs. <div>
arXiv:2508.03112v1 Announce Type: new 
Abstract: Comparable texts are topic-aligned documents in multiple languages that are not direct translations. They are valuable for understanding how a topic is discussed across languages. This research studies differences in sentiments and emotions across English-Arabic comparable documents. First, texts are annotated with sentiment and emotion labels. We apply a cross-lingual method to label documents with opinion classes (subjective/objective), avoiding reliance on machine translation. To annotate with emotions (anger, disgust, fear, joy, sadness, surprise), we manually translate the English WordNet-Affect (WNA) lexicon into Arabic, creating bilingual emotion lexicons used to label the comparable corpora. We then apply a statistical measure to assess the agreement of sentiments and emotions in each source-target document pair. This comparison is especially relevant when the documents originate from different sources. To our knowledge, this aspect has not been explored in prior literature. Our study includes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera (JSC). Results show that sentiment and emotion annotations align when articles come from the same news agency and diverge when they come from different ones. The proposed method is language-independent and generalizable to other language pairs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Story Generation via Knowledge Graph and Literary Theory</title>
<link>https://arxiv.org/abs/2508.03137</link>
<guid>https://arxiv.org/abs/2508.03137</guid>
<content:encoded><![CDATA[
<div> Keywords: long text generation, multi-agent Story Generator, memory storage model, literary narratology theory, dialogue interaction

Summary:
The paper introduces a new approach to long text generation using the multi-agent Story Generator structure, utilizing large language models as key components. To address issues like theme drift and incoherent plots, a memory storage model is implemented with long-term and short-term components. A story theme obstacle framework based on literary narratology theory is designed to introduce uncertainty and enhance story appeal. The framework calculates storyline similarity and integrates new content through a knowledge graph. A multi-agent interaction stage simulates writer-reader interaction through dialogue and revises the text based on feedback for consistency and logic. Evaluations show that this approach produces higher-quality long stories. 

<br /><br />Summary: <div>
arXiv:2508.03137v1 Announce Type: new 
Abstract: The generation of a long story consisting of several thousand words is a sub-task in the field of long text generation~(LTG). Previous research has addressed this challenge through outline-based generation, which employs a multi-stage method for generating outlines into stories. However, this approach suffers from two common issues: almost inevitable theme drift caused by the loss of memory of previous outlines, and tedious plots with incoherent logic that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to improve the multi-stage method, using large language models~(LLMs) as the core components of agents. To avoid theme drift, we introduce a memory storage model comprising two components: a long-term memory storage that identifies the most important memories, thereby preventing theme drift; and a short-term memory storage that retains the latest outlines from each generation round. To incorporate engaging elements into the story, we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline. This framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content. Additionally, we establish a multi-agent interaction stage to simulate writer-reader interaction through dialogue and revise the story text according to feedback, to ensure it remains consistent and logical. Evaluations against previous methods demonstrate that our approach can generate higher-quality long stories.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior</title>
<link>https://arxiv.org/abs/2508.03140</link>
<guid>https://arxiv.org/abs/2508.03140</guid>
<content:encoded><![CDATA[
<div> capability, merging, reasoning, domain-specific, models
Summary:<br /><br />Researchers introduce a novel merging framework, RCP-Merging, to integrate domain-specific Large Language Models (LLMs) with long chain-of-thought (CoT) capability. The framework prioritizes reasoning capability and selectively merges essential domain-specific weights while preserving core long CoT capability model weights. Extensive experiments conducted on models in the BioMedicine and Finance domains show that RCP-Merging successfully merges a reasoning model with domain-specific ones, leading to a 9.5% and 9.2% improvement in domain task performance over state-of-the-art methods. The results demonstrate that RCP-Merging efficiently integrates domain-specific knowledge into reasoning models without significantly affecting the original long CoT reasoning capability. This approach offers a resource-efficient solution for creating dual-capability models with long CoT capability and domain-specific knowledge. <div>
arXiv:2508.03140v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with long chain-of-thought (CoT) capability, termed Reasoning Models, demonstrate superior intricate problem-solving abilities through multi-step long CoT reasoning. To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse. To overcome this, we introduce RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain. Treating reasoning model weights as foundational prior, our method utilizes a reasoning capability indicator to preserve core long CoT capability model weights while selectively merging essential domain-specific weights. We conducted extensive experiments on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains. Our results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following</title>
<link>https://arxiv.org/abs/2508.03178</link>
<guid>https://arxiv.org/abs/2508.03178</guid>
<content:encoded><![CDATA[
<div> keywords: LLMs, reasoning abilities, instruction adherence, filtering process, supervised fine-tuning

Summary:
This paper addresses the challenge of inconsistent instruction adherence by LLMs due to lazy reasoning during the thinking stage. The proposed framework involves generating instructions with complex constraints, filtering for valid prompts, and utilizing rejection sampling to curate a high-quality dataset. The model is then initialized with this dataset and undergoes a supervised fine-tuning strategy along with reinforcement learning to encourage rigorous reasoning processes encompassing preview and self-checking. Extensive experiments show significant performance improvements across different model scales, with the Light-IF-32B model outperforming both open-source and closed-source models. The innovative approach presented in this study demonstrates the potential for enhancing LLMs' reasoning abilities and instruction adherence in various tasks.<br /><br />Summary: <div>
arXiv:2508.03178v1 Announce Type: new 
Abstract: While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification</title>
<link>https://arxiv.org/abs/2508.03181</link>
<guid>https://arxiv.org/abs/2508.03181</guid>
<content:encoded><![CDATA[
<div> Keywords: German parliament, political discourse, machine learning, topic classification, sentiment classification

Summary: 
The study examines political discourse in the German Bundestag using machine learning models trained on a dataset of 28,000 parliamentary speeches. The models achieved high classification performance for both topic and sentiment, revealing insights into topic trends and sentiment distributions across political parties over time. The analysis shows distinct discourse strategies for parties in government versus opposition, with governing responsibilities influencing discourse alongside ideological positions. The study addresses questions about the evolution of topics, sentiment dynamics, and party-specific discourse strategies in the Bundestag.<br /><br />Summary: <div>
arXiv:2508.03181v1 Announce Type: new 
Abstract: This study investigates political discourse in the German parliament, the Bundestag, by analyzing approximately 28,000 parliamentary speeches from the last five years. Two machine learning models for topic and sentiment classification were developed and trained on a manually labeled dataset. The models showed strong classification performance, achieving an area under the receiver operating characteristic curve (AUROC) of 0.94 for topic classification (average across topics) and 0.89 for sentiment classification. Both models were applied to assess topic trends and sentiment distributions across political parties and over time. The analysis reveals remarkable relationships between parties and their role in parliament. In particular, a change in style can be observed for parties moving from government to opposition. While ideological positions matter, governing responsibilities also shape discourse. The analysis directly addresses key questions about the evolution of topics, sentiment dynamics, and party-specific discourse strategies in the Bundestag.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.03199</link>
<guid>https://arxiv.org/abs/2508.03199</guid>
<content:encoded><![CDATA[
<div> gender, language, bias, Text-to-Image models, representation

Summary:
- The research explores how grammatical gender influences visual representation in Text-to-Image (T2I) models across different languages.
- A cross-linguistic benchmark dataset is introduced, analyzing words with contradicting grammatical gender and stereotypical gender associations in five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral control languages (English, Chinese).
- The study reveals that masculine grammatical markers increase male representation significantly in AI-generated images, while feminine markers lead to an increase in female representation.
- The impact of grammatical gender on image generation varies based on language resource availability and model architecture, with higher-resource languages exhibiting stronger effects.
- The findings suggest that language structure plays a crucial role in shaping bias and fairness in multilingual, multimodal AI systems.

<br /><br />Summary: <div>
arXiv:2508.03199v1 Announce Type: new 
Abstract: Research on bias in Text-to-Image (T2I) models has primarily focused on demographic representation and stereotypical attributes, overlooking a fundamental question: how does grammatical gender influence visual representation across languages? We introduce a cross-linguistic benchmark examining words where grammatical gender contradicts stereotypical gender associations (e.g., ``une sentinelle'' - grammatically feminine in French but referring to the stereotypically masculine concept ``guard''). Our dataset spans five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral control languages (English, Chinese), comprising 800 unique prompts that generated 28,800 images across three state-of-the-art T2I models. Our analysis reveals that grammatical gender dramatically influences image generation: masculine grammatical markers increase male representation to 73\% on average (compared to 22\% with gender-neutral English), while feminine grammatical markers increase female representation to 38\% (compared to 28\% in English). These effects vary systematically by language resource availability and model architecture, with high-resource languages showing stronger effects. Our findings establish that language structure itself, not just content, shapes AI-generated visual outputs, introducing a new dimension for understanding bias and fairness in multilingual, multimodal systems.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP</title>
<link>https://arxiv.org/abs/2508.03204</link>
<guid>https://arxiv.org/abs/2508.03204</guid>
<content:encoded><![CDATA[
<div> Keywords: Privacy, Data privacy, GDPR, Language models, Anonymization <br />
Summary: <br />
Privacy, a fundamental human right, is safeguarded by regulations like GDPR. Large language models require substantial data for training, often containing private information that can be extracted. Protecting sensitive data is crucial, and while complete anonymization may not be feasible, various pre-processing methods exist to mask or pseudonymize private information in text. This report discusses several approaches for domain-agnostic NLP tasks, emphasizing the importance of safeguarding private information in an increasingly data-driven world. <div>
arXiv:2508.03204v1 Announce Type: new 
Abstract: Privacy is a fundamental human right. Data privacy is protected by different regulations, such as GDPR. However, modern large language models require a huge amount of data to learn linguistic variations, and the data often contains private information. Research has shown that it is possible to extract private information from such language models. Thus, anonymizing such private and sensitive information is of utmost importance. While complete anonymization may not be possible, a number of different pre-processing approaches exist for masking or pseudonymizing private information in textual data. This report focuses on a few of such approaches for domain-agnostic NLP tasks.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Syntax in Large Language Models: Successes and Remaining Challenges</title>
<link>https://arxiv.org/abs/2508.03211</link>
<guid>https://arxiv.org/abs/2508.03211</guid>
<content:encoded><![CDATA[
<div> probes, syntactic structures, language models, linguistic properties, predictability<br />
Summary:<br />
- Structural probes in large language models are influenced by the proximity of words in a sentence, leading to bias.<br />
- These probes struggle to capture deep syntactic structures and are affected by interacting nouns or ungrammatical verb forms.<br />
- The performance of structural probes does not seem to be impacted by the predictability of individual words.<br />
- The study highlights the current challenges faced by structural probes and emphasizes the need for controlled stimuli benchmarks for evaluation. <div>
arXiv:2508.03211v1 Announce Type: new 
Abstract: The syntactic structures of sentences can be readily read-out from the activations of large language models (LLMs). However, the ``structural probes'' that have been developed to reveal this phenomenon are typically evaluated on an indiscriminate set of sentences. Consequently, it remains unclear whether structural and/or statistical factors systematically affect these syntactic representations. To address this issue, we conduct an in-depth analysis of structural probes on three controlled benchmarks. Our results are three-fold. First, structural probes are biased by a superficial property: the closer two words are in a sentence, the more likely structural probes will consider them as syntactically linked. Second, structural probes are challenged by linguistic properties: they poorly represent deep syntactic structures, and get interfered by interacting nouns or ungrammatical verb forms. Third, structural probes do not appear to be affected by the predictability of individual words. Overall, this work sheds light on the current challenges faced by structural probes. Providing a benchmark made of controlled stimuli to better evaluate their performance.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting</title>
<link>https://arxiv.org/abs/2508.03240</link>
<guid>https://arxiv.org/abs/2508.03240</guid>
<content:encoded><![CDATA[
<div> LLM-prompting approach, Spanish text adaptation, CLEARS shared task, IberLEF 2025, Gemma-3  

Summary: The CardiffNLP team participated in the CLEARS shared task on Spanish text adaptation at IberLEF 2025, submitting to two subtasks. They used an LLM-prompting approach with various prompt variations, switching to Gemma-3 for their final submission. The team achieved third place in Subtask 1 and second place in Subtask 2. The paper discusses the prompt variations tested, provides examples, and presents the experimental results. <div>
arXiv:2508.03240v1 Announce Type: new 
Abstract: This paper details the CardiffNLP team's contribution to the CLEARS shared task on Spanish text adaptation, hosted by IberLEF 2025. The shared task contained two subtasks and the team submitted to both. Our team took an LLM-prompting approach with different prompt variations. While we initially experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and landed third place in Subtask 1 and second place in Subtask 2. We detail our numerous prompt variations, examples, and experimental results.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs</title>
<link>https://arxiv.org/abs/2508.03247</link>
<guid>https://arxiv.org/abs/2508.03247</guid>
<content:encoded><![CDATA[
<div> Keywords: cultural patterns, large language models, mental health, cultural cues, symptom hierarchy

Summary: 
Large Language Models (LLMs) are increasingly used in mental health research, but a study found that they struggle to replicate cultural patterns in reporting symptoms of depression between Western and Eastern individuals. When prompted in English, LLMs did not reproduce the cultural distinctions seen in prior clinical psychology research. However, when prompted in major Eastern languages such as Chinese, Japanese, and Hindi, the alignment improved in some configurations. The study identified two main reasons for this failure: the models' insensitivity to cultural personas and a rigid, culturally invariant symptom hierarchy that overrides cultural cues. This suggests that while prompt language is important, current general-purpose LLMs lack the necessary culture-aware capabilities for safe and effective mental health applications. 

<br /><br />Summary: <div>
arXiv:2508.03247v1 Announce Type: new 
Abstract: Prior clinical psychology research shows that Western individuals with depression tend to report psychological symptoms, while Eastern individuals report somatic ones. We test whether Large Language Models (LLMs), which are increasingly used in mental health, reproduce these cultural patterns by prompting them with Western or Eastern personas. Results show that LLMs largely fail to replicate the patterns when prompted in English, though prompting in major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment in several configurations. Our analysis pinpoints two key reasons for this failure: the models' low sensitivity to cultural personas and a strong, culturally invariant symptom hierarchy that overrides cultural cues. These findings reveal that while prompt language is important, current general-purpose LLMs lack the robust, culture-aware capabilities essential for safe and effective mental health applications.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RooseBERT: A New Deal For Political Language Modelling</title>
<link>https://arxiv.org/abs/2508.03250</link>
<guid>https://arxiv.org/abs/2508.03250</guid>
<content:encoded><![CDATA[
<div> Keywords: political discourse, language model, pre-training, debate analysis, argumentation<br />
Summary:<br />
- The article presents a new pre-trained Language Model called RooseBERT specifically designed for analyzing political discourse. 
- RooseBERT has been trained on a large corpus of political debates and speeches in English to address the challenges posed by the specific language and argumentative form of political discussions.
- The model was fine-tuned on four downstream tasks related to political debate analysis, namely named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification.
- Results show significant improvements over general-purpose Language Models in these tasks, demonstrating the effectiveness of domain-specific pre-training for political debate analysis.
- The researchers have released the RooseBERT language model for the research community to further advance political discourse analysis research. 

<br /><br />Summary: <div>
arXiv:2508.03250v1 Announce Type: new 
Abstract: The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release the RooseBERT language model for the research community.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition</title>
<link>https://arxiv.org/abs/2508.03259</link>
<guid>https://arxiv.org/abs/2508.03259</guid>
<content:encoded><![CDATA[
<div> trade-off, Continual Named Entity Recognition, Knowledge Distillation, stability, plasticity

Summary:
The article introduces a Stability-Plasticity Trade-off (SPT) method for Continual Named Entity Recognition (CNER). This method balances stability and plasticity by incorporating a pooling operation for representation plasticity and dynamically merging weights to strengthen old knowledge while acquiring new knowledge. A weight-guided selective mechanism prioritizes significant weights during fusion. Additionally, a confidence-based pseudo-labeling approach handles the semantic shift of non-entity types. Experimental results across ten CNER settings on three benchmark datasets demonstrate that the SPT method outperforms previous CNER approaches, showcasing its ability to achieve a suitable stability-plasticity trade-off.<br /><br />Summary: <div>
arXiv:2508.03259v1 Announce Type: new 
Abstract: Continual Named Entity Recognition (CNER) is an evolving field that focuses on sequentially updating an existing model to incorporate new entity types. Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve prior knowledge and overcome catastrophic forgetting, strictly ensuring that the representations of old and new models remain consistent. Consequently, they often impart the model with excessive stability (i.e., retention of old knowledge) but limited plasticity (i.e., acquisition of new knowledge). To address this issue, we propose a Stability-Plasticity Trade-off (SPT) method for CNER that balances these aspects from both representation and weight perspectives. From the representation perspective, we introduce a pooling operation into the original KD, permitting a level of plasticity by consolidating representation dimensions. From the weight perspective, we dynamically merge the weights of old and new models, strengthening old knowledge while maintaining new knowledge. During this fusion, we implement a weight-guided selective mechanism to prioritize significant weights. Moreover, we develop a confidence-based pseudo-labeling approach for the current non-entity type, which predicts entity types using the old model to handle the semantic shift of the non-entity type, a challenge specific to CNER that has largely been ignored by previous methods. Extensive experiments across ten CNER settings on three benchmark datasets demonstrate that our SPT method surpasses previous CNER approaches, highlighting its effectiveness in achieving a suitable stability-plasticity trade-off.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?</title>
<link>https://arxiv.org/abs/2508.03262</link>
<guid>https://arxiv.org/abs/2508.03262</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, economic decision-making, Pay-What-You-Want pricing experiments, persona injection methods, computational social science

Summary: 
Large Language Models (LLMs) have shown potential in simulating human behaviors, but studies often use fictional personas rather than real human data. This study evaluates LLMs' ability to predict individual economic decision-making using Pay-What-You-Want pricing experiments with 522 real human personas. Three multimodal LLMs were compared, revealing challenges in precise individual-level predictions but reasonable group-level tendencies. Additionally, common prompting techniques did not significantly improve prediction performance. The study suggests that LLMs can simulate economic behavior using real human data, offering insights for persona-based simulation in computational social science.<br /><br />Summary: <div>
arXiv:2508.03262v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have generated significant interest in their capacity to simulate human-like behaviors, yet most studies rely on fictional personas rather than actual human data. We address this limitation by evaluating LLMs' ability to predict individual economic decision-making using Pay-What-You-Want (PWYW) pricing experiments with real 522 human personas. Our study systematically compares three state-of-the-art multimodal LLMs using detailed persona information from 522 Korean participants in cultural consumption scenarios. We investigate whether LLMs can accurately replicate individual human choices and how persona injection methods affect prediction performance. Results reveal that while LLMs struggle with precise individual-level predictions, they demonstrate reasonable group-level behavioral tendencies. Also, we found that commonly adopted prompting techniques are not much better than naive prompting methods; reconstruction of personal narrative nor retrieval augmented generation have no significant gain against simple prompting method. We believe that these findings can provide the first comprehensive evaluation of LLMs' capabilities on simulating economic behavior using real human data, offering empirical guidance for persona-based simulation in computational social science.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning</title>
<link>https://arxiv.org/abs/2508.03275</link>
<guid>https://arxiv.org/abs/2508.03275</guid>
<content:encoded><![CDATA[
<div> adaptive scheduling algorithm, spaced repetition systems, semantic analysis, personalized learning profiles, language examinations

Summary:
The article introduces LECTOR, an adaptive scheduling algorithm tailored for test-oriented learning, focusing on language exams where success rate is crucial. LECTOR utilizes large language models for semantic analysis and adapts to personalized learning profiles to address semantic confusion in vocabulary learning. Evaluation against six baseline algorithms demonstrates LECTOR's superiority, achieving a 90.2% success rate compared to the best baseline's 88.4%. It excels in handling semantically similar concepts, reducing errors caused by confusion while maintaining efficiency. The results highlight LECTOR as a promising approach for improving intelligent tutoring systems and adaptive learning platforms.<br /><br />Summary: <div>
arXiv:2508.03275v1 Announce Type: new 
Abstract: Spaced repetition systems are fundamental to efficient learning and memory retention, but existing algorithms often struggle with semantic interference and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced \textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a novel adaptive scheduling algorithm specifically designed for test-oriented learning scenarios, particularly language examinations where success rate is paramount. LECTOR leverages large language models for semantic analysis while incorporating personalized learning profiles, addressing the critical challenge of semantic confusion in vocabulary learning by utilizing LLM-powered semantic similarity assessment and integrating it with established spaced repetition principles. Our comprehensive evaluation against six baseline algorithms (SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over 100 days demonstrates significant improvements: LECTOR achieves a 90.2\% success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a 2.0\% relative improvement. The algorithm shows particular strength in handling semantically similar concepts, reducing confusion-induced errors while maintaining computational efficiency. Our results establish LECTOR as a promising direction for intelligent tutoring systems and adaptive learning platforms.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do language models accommodate their users? A study of linguistic convergence</title>
<link>https://arxiv.org/abs/2508.03276</link>
<guid>https://arxiv.org/abs/2508.03276</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, linguistic convergence, dialogue corpora, stylometric features, overfitting

Summary: 
This paper investigates the extent to which large language models (LLMs) exhibit linguistic convergence, a key component of human language communication. The study examines model completions of existing dialogues and compares them to the original human responses across different models, dialogue corpora, and stylometric features. The results show that models tend to strongly converge to the conversation's style, often surpassing the human baseline significantly. Convergence patterns vary across different features, but consistently show shifts in convergence behaviors among different modeling settings. Interestingly, instruction-tuned and larger models exhibit less convergence compared to pretrained models. The study suggests that the underlying mechanisms driving model convergence may differ from those in human language communication. <div>
arXiv:2508.03276v1 Announce Type: new 
Abstract: While large language models (LLMs) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language communication, asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes</title>
<link>https://arxiv.org/abs/2508.03292</link>
<guid>https://arxiv.org/abs/2508.03292</guid>
<content:encoded><![CDATA[
<div> gender bias, Large Language Models, narrative generation, psychology, stereotypes

Summary: 
(1) Large Language Models (LLMs) are being used in various applications, raising concerns about their potential to amplify gender biases.
(2) This study examines gender bias in LLMs using psychological stereotypes in narrative generation tasks.
(3) The researchers introduced a dataset called StereoBias-Stories, analyzing how gender contribution in stories changes based on attributes.
(4) LLMs exhibit bias towards males in unconditioned prompts, but this bias can be mitigated by conditioning on attributes unrelated to gender stereotypes.
(5) Model biases align with psychological ground-truth used for categorization, with alignment strength increasing with model size. 
<br /><br />Summary: <div>
arXiv:2508.03292v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly used across different applications, concerns about their potential to amplify gender biases in various tasks are rising. Prior research has often probed gender bias using explicit gender cues as counterfactual, or studied them in sentence completion and short question answering tasks. These formats might overlook more implicit forms of bias embedded in generative behavior of longer content. In this work, we investigate gender bias in LLMs using gender stereotypes studied in psychology (e.g., aggressiveness or gossiping) in an open-ended task of narrative generation. We introduce a novel dataset called StereoBias-Stories containing short stories either unconditioned or conditioned on (one, two, or six) random attributes from 25 psychological stereotypes and three task-related story endings. We analyze how the gender contribution in the overall story changes in response to these attributes and present three key findings: (1) While models, on average, are highly biased towards male in unconditioned prompts, conditioning on attributes independent from gender stereotypes mitigates this bias. (2) Combining multiple attributes associated with the same gender stereotype intensifies model behavior, with male ones amplifying bias and female ones alleviating it. (3) Model biases align with psychological ground-truth used for categorization, and alignment strength increases with model size. Together, these insights highlight the importance of psychology-grounded evaluation of LLMs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty</title>
<link>https://arxiv.org/abs/2508.03294</link>
<guid>https://arxiv.org/abs/2508.03294</guid>
<content:encoded><![CDATA[
<div> Keywords: exam questions, difficulty estimation, Large Language Model, supervised learning, assessment improvement

Summary:
Professors often struggle to accurately estimate the difficulty of exam questions, particularly in the fields of Neural Networks and Machine Learning. This study compared the abilities of professors and Large Language Model-based methods, including Gemini 2.5, in predicting student performance on True/False exam questions. The results revealed that professors had limited success in distinguishing between easy and difficult questions, with the LLMs and Gemini 2.5 outperforming them. Interestingly, utilizing uncertainties of the LLMs in a supervised learning setting with minimal training samples significantly improved the accuracy of difficulty estimation. This suggests that incorporating LLM uncertainty in a supervised learning framework can assist professors in developing better exams and enhancing the overall quality of assessment in academia.<br /><br />Summary: <div>
arXiv:2508.03294v1 Announce Type: new 
Abstract: Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling</title>
<link>https://arxiv.org/abs/2508.03296</link>
<guid>https://arxiv.org/abs/2508.03296</guid>
<content:encoded><![CDATA[
<div> Keywords: social platforms, moderation systems, multimodal framework, interpretability, hierarchical taxonomy <br />
Summary: <br />
The article introduces Hi-Guard, a new multimodal moderation framework designed to address the shortcomings of current moderation systems on social platforms. Hi-Guard implements a hierarchical moderation pipeline, utilizing a lightweight binary model for initial filtering and a stronger model for fine-grained risk classification. It also incorporates a hierarchical taxonomy for path-based classification and directly integrates rule definitions into the model prompt to ensure alignment with moderation policies. The system employs a multi-level soft-margin reward and Group Relative Policy Optimization (GRPO) to improve structured prediction and reasoning, penalizing semantically adjacent misclassifications. Through extensive experiments and real-world deployment, Hi-Guard demonstrates superior classification accuracy, generalization, and interpretability, offering a transparent and trustworthy solution for content safety on social platforms. The code for the framework is available on GitHub for further exploration. <br /> <div>
arXiv:2508.03296v1 Announce Type: new 
Abstract: Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term "Hierarchical" reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTTS: Collective Test-Time Scaling</title>
<link>https://arxiv.org/abs/2508.03333</link>
<guid>https://arxiv.org/abs/2508.03333</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time scaling, large language models, collective-agent methods, multi-agent collaboration, reward models<br />
Summary:<br />
The paper introduces Collective Test-Time Scaling (CTTS) as a new approach to enhancing large language models (LLMs) without additional training. By exploring three primary paradigms, the optimal paradigm of CTTS was investigated, with multiple agents to multiple reward models (MA-MR) found to consistently achieve the best performance. A novel framework named CTTS-MM was proposed, leveraging both multi-agent collaboration through Agent Collaboration Search (ACS) and multi-reward-model collaboration through Mixture of Reword Models (MoR). Experiments across seven benchmarks showed that CTTS-MM outperformed existing methods. The code for CTTS-MM is available on GitHub at https://github.com/magent4aci/CTTS-MM. <div>
arXiv:2508.03333v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at https://github.com/magent4aci/CTTS-MM.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature</title>
<link>https://arxiv.org/abs/2508.03358</link>
<guid>https://arxiv.org/abs/2508.03358</guid>
<content:encoded><![CDATA[
<div> named entity recognition, part-of-speech tagging, social networks, characters, fiction<br />
<br />
Summary: <br />
The article introduces Taggus, a pipeline aimed at extracting social networks from literary fiction works in Portuguese. It addresses the challenge of character identification and interaction detection by proposing a method that combines POS tagging and heuristics, achieving improved results compared to existing tools. Taggus demonstrates high accuracy in character identification and co-reference resolution, with an average F1-Score of $94.1\%, and satisfactory performance in interaction detection with a score of $75.9\%. The pipeline outperforms State-of-the-Art NER tools and Large Language Models by significant margins, indicating its effectiveness in the Portuguese language. The article acknowledges limitations in sample size and scope and suggests future enhancements, particularly in detecting relationships between characters. The Taggus pipeline is made publicly available to foster advancements in NLP for Portuguese literature. <br /> <div>
arXiv:2508.03358v1 Announce Type: new 
Abstract: Automatically identifying characters and their interactions from fiction books is, arguably, a complex task that requires pipelines that leverage multiple Natural Language Processing (NLP) methods, such as Named Entity Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are not optimized for the task that leads to the construction of Social Networks of Characters. Indeed, the currently available methods tend to underperform, especially in less-represented languages, due to a lack of manually annotated data for training. Here, we propose a pipeline, which we call Taggus, to extract social networks from literary fiction works in Portuguese. Our results show that compared to readily available State-of-the-Art tools -- off-the-shelf NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which uses POS tagging and a combination of heuristics, achieves satisfying results with an average F1-Score of $94.1\%$ in the task of identifying characters and solving for co-reference and $75.9\%$ in interaction detection. These represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results achieved by the readily available State-of-the-Art tools. Further steps to improve results are outlined, such as solutions for detecting relationships between characters. Limitations on the size and scope of our testing samples are acknowledged. The Taggus pipeline is publicly available to encourage development in this field for the Portuguese language.2
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2508.03363</link>
<guid>https://arxiv.org/abs/2508.03363</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, in-context learning, structured reasoning, calibration, reasoning accuracy

Summary:<br /><br />
The article introduces a new paradigm for in-context learning in large language models called JointThinking. This approach leverages both Thinking and Nothinking modes of reasoning to improve accuracy by generating two answers in parallel and triggering a second round of thinking only when the initial responses are inconsistent. JointThinking outperforms existing methods such as few-shot chain-of-thought and majority voting, with improved answer robustness. It achieves comparable in-distribution performance to state-of-the-art training-based methods while excelling on out-of-distribution tasks. The calibration mechanism lowers error rates by leveraging different reasoning modes and highlights the value of structural thinking diversity. The performance gap between actual and ideal reasoning decreases as model size increases in the second round of thinking, indicating scalability. The study also discusses current limitations and proposes future research directions in in-context learning for large language models. <div>
arXiv:2508.03363v1 Announce Type: new 
Abstract: Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy. Specifically, our method prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt that incorporates the original question and both candidate answers. Since such disagreement occurs infrequently (e.g., only 6\% in GSM8K), our method performs just one round of reasoning in most cases, resulting in minimal latency overhead. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT) and majority voting with improved answer robustness. Moreover, It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing that leveraging different reasoning modes consistently lowers the error rate and highlights the value of structural thinking diversity. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second round of thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReDSM5: A Reddit Dataset for DSM-5 Depression Detection</title>
<link>https://arxiv.org/abs/2508.03399</link>
<guid>https://arxiv.org/abs/2508.03399</guid>
<content:encoded><![CDATA[
<div> Keywords: Depression, Social media, Reddit, DSM-5, Symptomatology

Summary: 
ReDSM5 introduces a new approach to analyzing depressive symptomatology in social media narratives, specifically on Reddit. The corpus comprises posts annotated at the sentence level for the nine DSM-5 depression symptoms by a licensed psychologist. This detailed annotation allows for greater clinical relevance and interpretability compared to existing computational approaches. An exploratory analysis of ReDSM5 showcases patterns in language, syntax, and emotion that characterize symptom expression in social media. The corpus combines symptom-specific supervision with expert explanations, enabling the development of models that detect depression and generate human-interpretable reasoning. Baseline benchmarks for multi-label symptom classification and explanation generation are established, providing a reference point for future research on detection and interpretability. <br /><br />Summary: ReDSM5 provides a comprehensive dataset for studying depressive symptoms in social media, enhancing both detection capabilities and interpretability in computational approaches. <div>
arXiv:2508.03399v1 Announce Type: new 
Abstract: Depression is a pervasive mental health condition that affects hundreds of millions of individuals worldwide, yet many cases remain undiagnosed due to barriers in traditional clinical access and pervasive stigma. Social media platforms, and Reddit in particular, offer rich, user-generated narratives that can reveal early signs of depressive symptomatology. However, existing computational approaches often label entire posts simply as depressed or not depressed, without linking language to specific criteria from the DSM-5, the standard clinical framework for diagnosing depression. This limits both clinical relevance and interpretability. To address this gap, we introduce ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each exhaustively annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression symptoms. For each label, the annotator also provides a concise clinical rationale grounded in DSM-5 methodology. We conduct an exploratory analysis of the collection, examining lexical, syntactic, and emotional patterns that characterize symptom expression in social media narratives. Compared to prior resources, ReDSM5 uniquely combines symptom-specific supervision with expert explanations, facilitating the development of models that not only detect depression but also generate human-interpretable reasoning. We establish baseline benchmarks for both multi-label symptom classification and explanation generation, providing reference results for future research on detection and interpretability.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations</title>
<link>https://arxiv.org/abs/2508.03420</link>
<guid>https://arxiv.org/abs/2508.03420</guid>
<content:encoded><![CDATA[
<div> detecting misinformation, social media platforms, dynamic environmental representations, LSTM model, temporal modeling

Summary:<br />
This article introduces a new framework called MISDER for detecting misinformation on social media platforms. The framework addresses the dynamic nature of information by learning social environmental representations for different time periods and using a temporal model to predict future representations. Three variants of MISDER are proposed: MISDER-LSTM, MISDER-ODE, and MISDER-PT, each utilizing different temporal modeling techniques. Experimental results show that MISDER outperforms various baseline models on two common datasets, highlighting its effectiveness in detecting misinformation. The framework's ability to adapt to the evolving social environment sets it apart from traditional static learning paradigms and demonstrates the importance of considering temporal dynamics in misinformation detection. <div>
arXiv:2508.03420v1 Announce Type: new 
Abstract: The proliferation of misinformation across diverse social media platforms has drawn significant attention from both academic and industrial communities due to its detrimental effects. Accordingly, automatically distinguishing misinformation, dubbed as Misinformation Detection (MD), has become an increasingly active research topic. The mainstream methods formulate MD as a static learning paradigm, which learns the mapping between the content, links, and propagation of news articles and the corresponding manual veracity labels. However, the static assumption is often violated, since in real-world scenarios, the veracity of news articles may vacillate within the dynamically evolving social environment. To tackle this problem, we propose a novel framework, namely Misinformation detection with Dynamic Environmental Representations (MISDER). The basic idea of MISDER lies in learning a social environmental representation for each period and employing a temporal model to predict the representation for future periods. In this work, we specify the temporal model as the LSTM model, continuous dynamics equation, and pre-trained dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM, MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER, we compare it to various MD baselines across 2 prevalent datasets, and the experimental results can indicate the effectiveness of our proposed model.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2508.03440</link>
<guid>https://arxiv.org/abs/2508.03440</guid>
<content:encoded><![CDATA[
<div> Keywords: Human cognition, Large language models, Soft tokens, Reasoning paths, Sampling strategies

Summary:
Soft thinking in large language models (LLMs) aims to enhance reasoning abilities by generating abstract and fluid tokens. However, current LLMs tend to rely heavily on influential components of soft inputs during decoding, leading to a lack of exploration of diverse reasoning paths. This behavior results in a form of greedy decoding rather than true soft thinking. To address this issue, the paper proposes incorporating randomness through sampling strategies like Dirichlet resampling and the Gumbel-Softmax trick. Experimental results show that introducing randomness can mitigate the limitations of conventional approaches and optimize the benefits of soft thinking. Particularly, the Gumbel-Softmax trick offers the right balance of randomness and smoothness, showcasing superior performance across various reasoning benchmarks. <div>
arXiv:2508.03440v1 Announce Type: new 
Abstract: Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings</title>
<link>https://arxiv.org/abs/2508.03453</link>
<guid>https://arxiv.org/abs/2508.03453</guid>
<content:encoded><![CDATA[
<div> embedding models, text embeddings, NLP applications, contrastive learning, self-supervised training  
Summary:  
Text embeddings are crucial for various NLP tasks, and current top-performing models are fine-tuned from pre-trained language models using supervised learning. This study compares two augmentation strategies for generating positive pairs in contrastive learning of text embeddings, finding that cropping augmentation outperforms dropout-based approaches. While out-of-domain data performance is below supervised models, in-domain data shows high-quality embeddings with minimal fine-tuning. Representation quality improves in later transformer layers, with fine-tuning of only the last layers sufficient for similar quality to supervised models. Fine-tuning mainly affects the last layers, leading to improved representation quality in self-supervised training. <br /><br />Summary: <div>
arXiv:2508.03453v1 Announce Type: new 
Abstract: Text embeddings, i.e. vector representations of entire texts, play an important role in many NLP applications, such as retrieval-augmented generation, sentiment analysis, clustering, or visualizing collections of texts for data exploration. Currently, top-performing embedding models are derived from pre-trained language models via extensive supervised fine-tuning using curated text pairs. This contrasts with computer vision, where self-supervised training based on data augmentations has demonstrated remarkable success. Here we systematically compare the two most well-known augmentation strategies for positive pair generation in contrastive learning of text embeddings. We assess embedding quality on MTEB and additional in-domain evaluations and show that cropping augmentation strongly outperforms the dropout-based approach. We find that on out-of-domain data, the quality of resulting embeddings is below the supervised SOTA models, but for in-domain data, self-supervised fine-tuning produces high-quality text embeddings after very short fine-tuning, sometimes only marginally below the supervised SOTA. Finally, we show that representation quality increases towards the last transformer layers, which undergo the largest change during fine-tuning; and that fine-tuning only those last layers is sufficient to reach similar embedding quality.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2508.03475</link>
<guid>https://arxiv.org/abs/2508.03475</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025, Fact-Checked Claim Retrieval, Learning-to-Rank, Bi-encoder model, Multilingual<br />
<br />
Summary: <br />
- The study focuses on SemEval-2025 Task 7, which involves Multilingual and Crosslingual Fact-Checked Claim Retrieval.
- A bi-encoder model fine-tuned from a pre-trained transformer is utilized for this task, treating it as a Learning-to-Rank problem.
- Training involved both source languages and their English translations for multilingual retrieval, while only English translations were used for cross-lingual retrieval.
- The method employed lightweight models with fewer than 500M parameters and was trained on Kaggle T4 GPUs.
- The results showed a high success rate of 92% at Top 10 in the multilingual track and 80% at Top 5 in the crosslingual track. <div>
arXiv:2508.03475v1 Announce Type: new 
Abstract: SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval is approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity. Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval. Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.03489</link>
<guid>https://arxiv.org/abs/2508.03489</guid>
<content:encoded><![CDATA[
<div> Keywords: product sustainability reports, PDF format, carbon footprints, CarbonPDF-QA dataset, LLM-based technique

Summary: 
Product sustainability reports in PDF format contain valuable insights on environmental impacts but are challenging to analyze due to the unstructured text. The CarbonPDF-QA dataset offers question-answer pairs for 1735 documents to address this issue. GPT-4o faces difficulties in answering questions with data inconsistencies, leading to the development of CarbonPDF, an LLM-based technique fine-tuned on the CarbonPDF dataset. CarbonPDF outperforms current QA systems in answering carbon footprint questions from such unstructured PDF reports. The research focuses on standardizing the analysis of sustainability reports for better understanding of product environmental impacts. <br /><br />Summary: <div>
arXiv:2508.03489v1 Announce Type: new 
Abstract: Product sustainability reports provide valuable insights into the environmental impacts of a product and are often distributed in PDF format. These reports often include a combination of tables and text, which complicates their analysis. The lack of standardization and the variability in reporting formats further exacerbate the difficulty of extracting and interpreting relevant information from large volumes of documents. In this paper, we tackle the challenge of answering questions related to carbon footprints within sustainability reports available in PDF format. Unlike previous approaches, our focus is on addressing the difficulties posed by the unstructured and inconsistent nature of text extracted from PDF parsing. To facilitate this analysis, we introduce CarbonPDF-QA, an open-source dataset containing question-answer pairs for 1735 product report documents, along with human-annotated answers. Our analysis shows that GPT-4o struggles to answer questions with data inconsistencies. To address this limitation, we propose CarbonPDF, an LLM-based technique specifically designed to answer carbon footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama 3 with our training data. Our results show that our technique outperforms current state-of-the-art techniques, including question-answering (QA) systems finetuned on table and text data.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression</title>
<link>https://arxiv.org/abs/2508.03520</link>
<guid>https://arxiv.org/abs/2508.03520</guid>
<content:encoded><![CDATA[
<div> Keywords: supervised learning, empathy regression, label noise, uncertainty quantification, probabilistic language model 

Summary: 
The article introduces UPLME, a framework for empathy regression that addresses the challenge of noisy self-reported empathy scores. UPLME includes a probabilistic language model trained using Bayesian concepts with variational model ensembling. It predicts both empathy scores and heteroscedastic uncertainty, incorporating two new loss components to improve performance. UPLME achieves state-of-the-art results in two public benchmarks with label noise, showing effectiveness in separating noisy and clean samples. Additionally, through synthetic label noise injection, UPLME demonstrates its ability to predict uncertainty and outperform a recent UQ method for regression. This research advances the field of supervised learning for empathy detection by providing a robust framework for dealing with label noise and improving regression performance. 

<br /><br />Summary: <div>
arXiv:2508.03520v1 Announce Type: new 
Abstract: Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FilBench: Can LLMs Understand and Generate Filipino?</title>
<link>https://arxiv.org/abs/2508.03523</link>
<guid>https://arxiv.org/abs/2508.03523</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, Filipino, FilBench, NLP, Language-specific benchmarks

Summary: 
FilBench is introduced as a benchmark for evaluating LLMs specifically in Filipino, Tagalog, and Cebuano languages. Tasks in FilBench cover various aspects of NLP research in the Philippines, including Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. Evaluation of 27 LLMs on FilBench reveals shortcomings in reading comprehension and translation capabilities of several models. The top-performing model, GPT-4o, achieves a score of 72.23%, indicating the challenging nature of the benchmark. Models trained for Southeast Asian languages also struggle on FilBench, with the best-performing SEA-LION v3 70B scoring 61.07%. This work highlights the importance of language-specific benchmarks like FilBench in advancing Filipino NLP and promoting the inclusion of Philippine languages in LLM development. 

<br /><br />Summary: <div>
arXiv:2508.03523v1 Announce Type: new 
Abstract: Despite the impressive performance of LLMs on English-based tasks, little is known about their capabilities in specific languages such as Filipino. In this work, we address this gap by introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to reflect the priorities and trends of NLP research in the Philippines such as Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs suffer from reading comprehension and translation capabilities. Our results indicate that FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Moreover, we also find that models trained specifically for Southeast Asian languages tend to underperform on FilBench, with the highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%. Our work demonstrates the value of curating language-specific LLM benchmarks to aid in driving progress on Filipino NLP and increasing the inclusion of Philippine languages in LLM development.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marito: Structuring and Building Open Multilingual Terminologies for South African NLP</title>
<link>https://arxiv.org/abs/2508.03529</link>
<guid>https://arxiv.org/abs/2508.03529</guid>
<content:encoded><![CDATA[
<div> Keywords: South Africa, NLP, terminology, Marito, machine translation

Summary:
- The lack of structured terminological data for South Africa's official languages hinders progress in multilingual NLP.
- Existing government and academic terminology lists are fragmented and locked in non-machine-readable formats.
- The Marito dataset aims to aggregate, clean, and standardize these scattered resources into open, interoperable datasets.
- The Marito dataset is released under the NOODL framework and integrates the terminology into a Retrieval-Augmented Generation (RAG) pipeline.
- Experiments demonstrate significant improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation using large language models.

<br /><br />Summary: 
The critical shortage of structured terminological data for South Africa's official languages has impeded advancements in multilingual NLP. Despite numerous governmental and academic terminology lists, valuable resources are fragmented and inaccessible for computational research. The Marito dataset addresses this challenge by consolidating and standardizing these disparate assets into open, interoperable datasets under the NOODL framework. By integrating this terminology into a Retrieval-Augmented Generation (RAG) pipeline, significant enhancements in English-to-Tshivenda machine translation accuracy and domain-specific consistency have been observed, showcasing the immediate utility of the Marito dataset in facilitating the development of robust and equitable NLP technologies that represent South Africa's diverse linguistic landscape in the digital era. <div>
arXiv:2508.03529v1 Announce Type: new 
Abstract: The critical lack of structured terminological data for South Africa's official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. \emph{Marito} addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational \emph{Marito} dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. \emph{Marito} provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africa's rich linguistic diversity is represented in the digital age.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models</title>
<link>https://arxiv.org/abs/2508.03533</link>
<guid>https://arxiv.org/abs/2508.03533</guid>
<content:encoded><![CDATA[
<div> Optimization, EmbedGrad, Pretrained models, Fine-tuning, Task adaptation
<br />
Summary: 
EmbedGrad introduces a novel framework for optimizing text prompt embeddings through gradient-based refinement. It addresses limitations of current approaches by decoupling training from deployment, allowing for precise embedding adjustments guided by labeled examples while maintaining semantic meaning. This process enables fine-grained calibration of prompts, leading to significant improvements in performance across various tasks such as mathematical reasoning, sentiment analysis, and causal judgment. Evaluations demonstrate EmbedGrad's effectiveness in enhancing the reasoning capability of prompts and achieving consistent improvements across different model scales and tasks. By bridging prompt engineering and parameter efficiency without architectural changes, EmbedGrad establishes embedding refinement as a powerful new paradigm for adapting powerful pretrained foundation models to diverse tasks. 
<br /> <div>
arXiv:2508.03533v1 Announce Type: new 
Abstract: Effectively adapting powerful pretrained foundation models to diverse tasks remains a key challenge in AI deployment. Current approaches primarily follow two paradigms:discrete optimization of text prompts through prompt engineering, or continuous adaptation via additional trainable parameters. Both exhibit limitations-discrete methods lack refinement precision while parameter-based techniques increase complexity and reduce interpretability. To address these constraints, we propose EmbedGrad, a novel framework that optimizes text prompt embeddings through gradient-based refinement. Our approach uniquely decouples training from deployment:during optimization,labeled examples guide precise embedding adjustments while preserving semantic meaning; during inference, only optimized embeddings integrate with user queries. This enables fine-grained calibration impossible in text space, such as enhancing the reasoning capability of prompts like please reason step by step. Comprehensive evaluations across mathematical reasoning, sentiment analysis, and causal judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on mathematical problems. Consistent improvements were observed across model scales (0.5B-14B) and all tasks, with particularly significant gains for smaller models on complex problems like causal judgment. By bridging prompt engineering and parameter efficiency without architectural changes, our work establishes embedding refinement as a powerful new paradigm for task adaptation.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations</title>
<link>https://arxiv.org/abs/2508.03550</link>
<guid>https://arxiv.org/abs/2508.03550</guid>
<content:encoded><![CDATA[
<div> Keywords: automated evaluation, large language models, alignment, internal representations, fine-grained judgment scores

Summary:
LAGER is a framework designed to enhance the alignment of large language models acting as judges with human scoring preferences. It focuses on leveraging internal representations in middle-to-upper layers of the model which encode semantically and task-relevant information that aligns better with human judgments. By aggregating cross-layer score token logits and using a softmax-based distribution, LAGER is able to produce fine-grained judgment scores without the need for complex prompts or fine-tuning. Evaluation on standard benchmarks shows that LAGER outperforms baseline methods by up to 7.5% and matches or exceeds reasoning-based approaches. Furthermore, experiments on downstream applications like data selection and emotional understanding demonstrate the effectiveness of LAGER in improving alignment between large language models and human preferences. <div>
arXiv:2508.03550v1 Announce Type: new 
Abstract: The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using large language models, a paradigm known as "LLMas-a-judge." However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and taskrelevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a lightweight and efficient framework for enhancing LLM-as-a-Judge alignment with human scoring, via internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer scoretoken logits and computing the expected score from a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer. We evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the effectiveness of our method.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation</title>
<link>https://arxiv.org/abs/2508.03571</link>
<guid>https://arxiv.org/abs/2508.03571</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Continual Learning, Knowledge Graphs, Domain Shift, Instruction Tuning

Summary: 
- The study introduces KILO, a novel continual learning framework that combines dynamic knowledge graphs with instruction tuning to address performance degradation in Large Language Models (LLMs) caused by domain shifts.
- KILO leverages domain-specific knowledge for training guidance, enhancing adaptability to new domains and retention of previously acquired knowledge.
- Pretrained on WikiText-103, KILO is evaluated across four target domains (BioASQ, SciQ, TweetEval, MIND) for sequential adaptation.
- Experimental results show KILO outperforms continual fine-tuning, ERNIE 2.0, and CPT in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency.
- This study highlights the effectiveness of combining structured knowledge retrieval and instruction prompting to successfully overcome domain shift challenges in continual learning scenarios.

<br /><br />Summary: <div>
arXiv:2508.03571v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired knowledge. We pretrain our model on WikiText-103 and evaluate sequential adaptation across four diverse target domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that KILO consistently outperforms strong baselines, including continual fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency. These results highlight the effectiveness of combining structured knowledge retrieval and instruction prompting to overcome domain shift challenges in continual learning scenarios.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</title>
<link>https://arxiv.org/abs/2508.03644</link>
<guid>https://arxiv.org/abs/2508.03644</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Multimodal Large Language Models, Double-Bench, document understanding, evaluation

Summary: 
The article introduces Double-Bench, a new evaluation system designed to assess complex document Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs). It addresses the limitations of existing benchmarks by offering a large-scale, multilingual, and multimodal evaluation platform that includes real-world data and human-verified queries. Experiments conducted using Double-Bench reveal the narrowing gap between text and visual embedding models, emphasizing the importance of strong document retrieval models. The study also uncovers an over-confidence issue in current document RAG frameworks, indicating a need for improved evidence-based decision-making. The open-source nature of Double-Bench aims to provide a solid foundation for future research on advanced document RAG systems, with plans for regular updates and new benchmarks. 

<br /><br />Summary: <div>
arXiv:2508.03644v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Vision-Language Models Understand Multimodal Sarcasm?</title>
<link>https://arxiv.org/abs/2508.03654</link>
<guid>https://arxiv.org/abs/2508.03654</guid>
<content:encoded><![CDATA[
<div> Keywords: sarcasm, sentiment analysis, Large Visual Language Models, Multimodal Sarcasm Analysis, conceptual knowledge

Summary:<br />
Sarcasm is a complex linguistic phenomenon that poses challenges for sentiment analysis and emotion-sensitive tasks due to the disparity between literal and intended meanings. Traditional sarcasm detection methods focus on text, but incorporating multimodal information has shown promise. This paper evaluates the use of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis (MSA), specifically in Multimodal Sarcasm Detection and Explanation tasks. The study reveals limitations, such as inadequate visual understanding and a lack of conceptual knowledge in LVLMs. To address these issues, a training-free framework that integrates object extraction and external conceptual knowledge is proposed. Experimental results demonstrate the effectiveness of the proposed framework on multiple models. 

<br /><br />Summary: <div>
arXiv:2508.03654v1 Announce Type: new 
Abstract: Sarcasm is a complex linguistic phenomenon that involves a disparity between literal and intended meanings, making it challenging for sentiment analysis and other emotion-sensitive tasks. While traditional sarcasm detection methods primarily focus on text, recent approaches have incorporated multimodal information. However, the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm Detection and Multimodal Sarcasm Explanation. Through comprehensive experiments, we identify key limitations, such as insufficient visual understanding and a lack of conceptual knowledge. To address these issues, we propose a training-free framework that integrates in-depth object extraction and external conceptual knowledge to improve the model's ability to interpret and explain sarcasm in multimodal contexts. The experimental results on multiple models show the effectiveness of our proposed framework. The code is available at https://github.com/cp-cp/LVLM-MSA.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction</title>
<link>https://arxiv.org/abs/2508.03668</link>
<guid>https://arxiv.org/abs/2508.03668</guid>
<content:encoded><![CDATA[
<div> CTR prediction, recommendation systems, user behavior sequences, Language Models, attention sinks <br />
Summary: 
CTR-Sink is introduced to address the semantic fragmentation issue in Click-Through Rate (CTR) prediction by leveraging Language Models for user behavior sequences. It proposes behavior-level attention sinks tailored for recommendation scenarios, incorporating temporal distance signals between behaviors and optimizing attention aggregation. The framework includes a two-stage training strategy to guide LM attention towards sink tokens and an attention sink mechanism to enhance inter-sink dependencies for improved behavioral correlation capture. Experiments on industrial and open-source datasets validate the method's efficacy, demonstrating enhanced prediction performance across various scenarios. Visualizations support the effectiveness of CTR-Sink in optimizing attention focus and capturing meaningful behavior boundaries and relationships. <div>
arXiv:2508.03668v1 Announce Type: new 
Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems, estimates user click likelihood using historical behavioral data. Modeling user behavior sequences as text to leverage Language Models (LMs) for this task has gained traction, owing to LMs' strong semantic understanding and contextual modeling capabilities. However, a critical structural gap exists: user behavior sequences consist of discrete actions connected by semantically empty separators, differing fundamentally from the coherent natural language in LM pre-training. This mismatch causes semantic fragmentation, where LM attention scatters across irrelevant tokens instead of focusing on meaningful behavior boundaries and inter-behavior relationships, degrading prediction performance. To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing behavior-level attention sinks tailored for recommendation scenarios. Inspired by attention sink theory, it constructs attention focus sinks and dynamically regulates attention aggregation via external information. Specifically, we insert sink tokens between consecutive behaviors, incorporating recommendation-specific signals such as temporal distance to serve as stable attention sinks. To enhance generality, we design a two-stage training strategy that explicitly guides LM attention toward sink tokens and a attention sink mechanism that amplifies inter-sink dependencies to better capture behavioral correlations. Experiments on one industrial dataset and two open-source datasets (MovieLens, Kuairec), alongside visualization results, validate the method's effectiveness across scenarios.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FairLangProc: A Python package for fairness in NLP</title>
<link>https://arxiv.org/abs/2508.03677</link>
<guid>https://arxiv.org/abs/2508.03677</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fairness, bias mitigation, Natural Language Processing, Python package 
<br /> 
Summary: 
<br /> 
The paper discusses the societal concern regarding the fairness of Large Language Models in decision-making contexts and the need to address bias in Natural Language Processing. Various datasets, metrics, and algorithms have been proposed to measure and mitigate harmful prejudice, but their implementation is diverse and decentralized. To tackle this issue, the paper introduces FairLangProc, a Python package that offers a common implementation of recent advances in fairness in Natural Language Processing. The package is designed to be compatible with the Hugging Face transformers library, aiming to promote the widespread use and democratization of bias mitigation techniques. The implementation of FairLangProc can be accessed on GitHub, providing a convenient and accessible resource for researchers and practitioners in the field. 
<br /> 
Summary: <div>
arXiv:2508.03677v1 Announce Type: new 
Abstract: The rise in usage of Large Language Models to near ubiquitousness in recent years has risen societal concern about their applications in decision-making contexts, such as organizational justice or healthcare. This, in turn, poses questions about the fairness of these models in critical settings, which leads to the developement of different procedures to address bias in Natural Language Processing. Although many datasets, metrics and algorithms have been proposed to measure and mitigate harmful prejudice in Natural Language Processing, their implementation is diverse and far from centralized. As a response, this paper presents FairLangProc, a comprehensive Python package providing a common implementation of some of the more recent advances in fairness in Natural Language Processing providing an interface compatible with the famous Hugging Face transformers library, aiming to encourage the widespread use and democratization of bias mitigation techniques. The implementation can be found on https://github.com/arturo-perez-peralta/FairLangProc.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation</title>
<link>https://arxiv.org/abs/2508.03678</link>
<guid>https://arxiv.org/abs/2508.03678</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, code generation benchmark, prompt specificity, I/O specifications, edge-case handling<br />
Summary:<br />
State-of-the-art Large Language Models (LLMs) have shown variations in performance when evaluated on general benchmarks like HumanEval compared to specialized suites like ParEval. The study introduces PartialOrderEval, which enhances code generation benchmarks by incorporating a partial order of prompts ranging from minimal to detailed. By applying this approach to HumanEval and subsets of ParEval, the researchers analyze how the specificity of prompts impacts the performance of LLMs. The experiments conducted using Llama-3.x and Qwen2.5-Coder reveal differing degrees of sensitivity to prompt details across different tasks. The analysis suggests that explicit I/O specifications, effective handling of edge cases, and providing stepwise breakdowns are crucial factors in improving prompt detail. This study sheds light on the importance of carefully crafting prompts to enhance the performance of LLMs on specialized code generation tasks. <br /><br />Summary: <div>
arXiv:2508.03678v1 Announce Type: new 
Abstract: State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</title>
<link>https://arxiv.org/abs/2508.03686</link>
<guid>https://arxiv.org/abs/2508.03686</guid>
<content:encoded><![CDATA[
<div> model, evaluation, benchmarks, verification, reinforcement learning

Summary:
CompassVerifier is a lightweight verifier model developed for answer verification and outcome reward. It addresses limitations in current methodologies by demonstrating multi-domain competency, handling various answer types, and identifying abnormal/invalid responses. The model is accurate and robust, making it suitable for evaluation protocols and reinforcement learning research. The VerifierBench benchmark, developed in conjunction with CompassVerifier, provides a comprehensive evaluation framework by collecting model outputs from multiple data sources and enhancing them through manual analysis of metaerror patterns. This work aims to improve answer verification processes, evaluation protocols, and research in reinforcement learning. The code and dataset for CompassVerifier and VerifierBench are available on GitHub at https://github.com/open-compass/CompassVerifier. 

<br /><br />Summary: <div>
arXiv:2508.03686v1 Announce Type: new 
Abstract: Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs</title>
<link>https://arxiv.org/abs/2507.10593</link>
<guid>https://arxiv.org/abs/2507.10593</guid>
<content:encoded><![CDATA[
arXiv:2507.10593v1 Announce Type: cross 
Abstract: Large Language Model (LLM) applications are increasingly relying on external tools to extend their capabilities beyond text generation. However, current tool integration approaches suffer from fragmentation, protocol limitations, and implementation complexity, leading to substantial development overhead. This paper presents Toolregistry, a protocol-agnostic tool management library that simplifies tool registration, representation, execution, and lifecycle management via a unified interface. Our evaluation demonstrates that \toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x performance improvements through concurrent execution, and 100% compatibility with OpenAI function calling standards. Real-world case studies show significant improvements in development efficiency and code maintainability across diverse integration scenarios. \toolregistry is open-source and available at https://github.com/Oaklight/ToolRegistry, with comprehensive documentation at https://toolregistry.readthedocs.io/.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Agents: Building Effective Agents While Reducing Cost</title>
<link>https://arxiv.org/abs/2508.02694</link>
<guid>https://arxiv.org/abs/2508.02694</guid>
<content:encoded><![CDATA[
arXiv:2508.02694v1 Announce Type: cross 
Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education</title>
<link>https://arxiv.org/abs/2508.02731</link>
<guid>https://arxiv.org/abs/2508.02731</guid>
<content:encoded><![CDATA[
arXiv:2508.02731v1 Announce Type: cross 
Abstract: Evaluating teaching effectiveness at scale remains a persistent challenge for large universities, particularly within engineering programs that enroll tens of thousands of students. Traditional methods, such as manual review of student evaluations, are often impractical, leading to overlooked insights and inconsistent data use. This article presents a scalable, AI-supported framework for synthesizing qualitative student feedback using large language models. The system employs hierarchical summarization, anonymization, and exception handling to extract actionable themes from open-ended comments while upholding ethical safeguards. Visual analytics contextualize numeric scores through percentile-based comparisons, historical trends, and instructional load. The approach supports meaningful evaluation and aligns with best practices in qualitative analysis and educational assessment, incorporating student, peer, and self-reflective inputs without automating personnel decisions. We report on its successful deployment across a large college of engineering. Preliminary validation through comparisons with human reviewers, faculty feedback, and longitudinal analysis suggests that LLM-generated summaries can reliably support formative evaluation and professional development. This work demonstrates how AI systems, when designed with transparency and shared governance, can promote teaching excellence and continuous improvement at scale within academic institutions.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration</title>
<link>https://arxiv.org/abs/2508.02738</link>
<guid>https://arxiv.org/abs/2508.02738</guid>
<content:encoded><![CDATA[
arXiv:2508.02738v1 Announce Type: cross 
Abstract: Corporate credit rating serves as a crucial intermediary service in the market economy, playing a key role in maintaining economic order. Existing credit rating models rely on financial metrics and deep learning. However, they often overlook insights from non-financial data, such as corporate annual reports. To address this, this paper introduces a corporate credit rating framework that integrates financial data with features extracted from annual reports using FinBERT, aiming to fully leverage the potential value of unstructured text data. In addition, we have developed a large-scale dataset, the Comprehensive Corporate Rating Dataset (CCRD), which combines both traditional financial data and textual data from annual reports. The experimental results show that the proposed method improves the accuracy of the rating predictions by 8-12%, significantly improving the effectiveness and reliability of corporate credit ratings.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification</title>
<link>https://arxiv.org/abs/2508.02823</link>
<guid>https://arxiv.org/abs/2508.02823</guid>
<content:encoded><![CDATA[
arXiv:2508.02823v1 Announce Type: cross 
Abstract: Conversational LLMs have been widely adopted by domain users with limited programming experience to solve domain problems. However, these users often face misalignment between their intent and generated code, resulting in frustration and rounds of clarification. This work first investigates the cause of this misalignment, which dues to bidirectional ambiguity: both user intents and coding tasks are inherently nonlinear, yet must be expressed and interpreted through linear prompts and code sequences. To address this, we propose direct intent-task matching, a new human-LLM interaction paradigm that externalizes and enables direct manipulation of the LLM understanding, i.e., the coding tasks and their relationships inferred by the LLM prior to code generation. As a proof-of-concept, this paradigm is then implemented in NeuroSync, which employs a knowledge distillation pipeline to extract LLM understanding, user intents, and their mappings, and enhances the alignment by allowing users to intuitively inspect and edit them via visualizations. We evaluate the algorithmic components of NeuroSync via technical experiments, and assess its overall usability and effectiveness via a user study (N=12). The results show that it enhances intent-task alignment, lowers cognitive effort, and improves coding efficiency.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec</title>
<link>https://arxiv.org/abs/2508.02849</link>
<guid>https://arxiv.org/abs/2508.02849</guid>
<content:encoded><![CDATA[
arXiv:2508.02849v1 Announce Type: cross 
Abstract: Speech codecs serve as a crucial bridge in unifying speech and text language models. Existing codec methods face several challenges in semantic encoding, such as residual paralinguistic information (e.g., timbre, emotion), insufficient semantic completeness, limited reconstruction capability, and lack of support for streaming. To address these challenges, we propose SecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that disentangles semantic and paralinguistic information in a single-codebook space. To ensure semantic completeness and reconstruction fidelity, paralinguistic encoding is introduced to bridge the information gap between semantic and acoustic encoding. A semantic-only efficient quantization method based on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is proposed. This approach alleviates the long-tail distribution problem of tokens while maintaining high codebook utilization. A semantic disentanglement method based on contrastive learning is proposed, which aligns text and speech in a joint multimodal frame-level space, effectively removing paralinguistic information from semantic encoding. An acoustic-constrained multi-stage optimization strategy is proposed to ensure robust and stable convergence. Figure~\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA (state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps. The code and model weights for SecoustiCodec will be open-sourced upon the completion of the peer-review process. We've open-sourced SecoustiCodec's demo, code, and model weights.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction</title>
<link>https://arxiv.org/abs/2508.02890</link>
<guid>https://arxiv.org/abs/2508.02890</guid>
<content:encoded><![CDATA[
arXiv:2508.02890v1 Announce Type: cross 
Abstract: This paper introduces VisuCraft, a novel framework designed to significantly enhance the capabilities of Large Vision-Language Models (LVLMs) in complex visual-guided creative content generation. Existing LVLMs often exhibit limitations in maintaining high visual fidelity, genuine creativity, and precise adherence to nuanced user instructions when generating long-form texts. VisuCraft addresses these challenges by integrating a multimodal structured information extractor (E) and a dynamic prompt generation module (G). The extractor distills fine-grained visual attributes from input images into a rich, structured representation, which the dynamic prompt module then combines with user instructions to create highly optimized prompts for underlying LVLMs (e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity, and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs across tasks like story generation and poetry composition. Our results demonstrate remarkable improvements, particularly in creativity and instruction adherence, validating VisuCraft's effectiveness in producing imaginative, visually grounded, and user-aligned long-form creative text. This work unlocks new potential for LVLMs in sophisticated creative AI applications.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces</title>
<link>https://arxiv.org/abs/2508.02917</link>
<guid>https://arxiv.org/abs/2508.02917</guid>
<content:encoded><![CDATA[
arXiv:2508.02917v1 Announce Type: cross 
Abstract: Vision-and-Language Navigation (VLN) refers to the task of enabling autonomous robots to navigate unfamiliar environments by following natural language instructions. While recent Large Vision-Language Models (LVLMs) have shown promise in this task, most current VLM systems rely on models specifically designed and optimized for navigation, leaving the potential of off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used low-level action spaces with egocentric views and atomic actions (such as "turn left" or "move forward"), newer models tend to favor panoramic action spaces with discrete navigable viewpoints. This paper investigates (1) whether off-the-shelf LVLMs (fine-tuned without architectural modifications or simulator-based training) can effectively support VLN tasks and (2) whether such models can support both low-level and panoramic action paradigms. To this end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the Room-to-Room (R2R) dataset and evaluate its empirical performance across both low-level and panoramic action spaces. The best resulting model achieves a 41% success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs can learn to perform Vision-and-Language Navigation, they still lag behind models specifically designed for this task.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defend LLMs Through Self-Consciousness</title>
<link>https://arxiv.org/abs/2508.02961</link>
<guid>https://arxiv.org/abs/2508.02961</guid>
<content:encoded><![CDATA[
arXiv:2508.02961v1 Announce Type: cross 
Abstract: This paper introduces a novel self-consciousness defense mechanism for Large Language Models (LLMs) to combat prompt injection attacks. Unlike traditional approaches that rely on external classifiers, our method leverages the LLM's inherent reasoning capabilities to perform self-protection. We propose a framework that incorporates Meta-Cognitive and Arbitration Modules, enabling LLMs to evaluate and regulate their own outputs autonomously. Our approach is evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate significant improvements in defense success rates across models and datasets, with some achieving perfect and near-perfect defense in Enhanced Mode. We also analyze the trade-off between defense success rate improvement and computational overhead. This self-consciousness method offers a lightweight, cost-effective solution for enhancing LLM ethics, particularly beneficial for GenAI use cases across various platforms.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling</title>
<link>https://arxiv.org/abs/2508.02979</link>
<guid>https://arxiv.org/abs/2508.02979</guid>
<content:encoded><![CDATA[
arXiv:2508.02979v1 Announce Type: cross 
Abstract: The proliferation of tool-augmented Large Language Models (LLMs) has created a fragmented ecosystem where developers must navigate multiple protocols, manual schema definitions, and complex execution workflows. We address this challenge by proposing a unified approach to tool integration that abstracts protocol differences while optimizing execution performance. Our solution demonstrates how protocol-agnostic design principles can significantly reduce development overhead through automated schema generation, dual-mode concurrent execution, and seamless multi-source tool management. Experimental results show 60-80% code reduction across integration scenarios, performance improvements up to 3.1x through optimized concurrency, and full compatibility with existing function calling standards. This work contributes both theoretical insights into tool integration architecture and practical solutions for real-world LLM application development.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots</title>
<link>https://arxiv.org/abs/2508.02999</link>
<guid>https://arxiv.org/abs/2508.02999</guid>
<content:encoded><![CDATA[
arXiv:2508.02999v1 Announce Type: cross 
Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive interaction and management of domain-specific data through the manipulation of knowledge graphs in natural language. It gives non-technical users a complete, visual solution to incrementally build and refine their knowledge bases, allowing multi-round dialogues and dynamic updates without specialized query languages. The flexible design of AGENTiGraph, including intent classification, task planning, and automatic knowledge integration, ensures seamless reasoning between diverse tasks. Evaluated on a 3,500-query benchmark within an educational scenario, the system outperforms strong zero-shot baselines (achieving 95.12% classification accuracy, 90.45% execution success), indicating potential scalability to compliance-critical or multi-step queries in legal and medical domains, e.g., incorporating new statutes or research on the fly. Our open-source demo offers a powerful new paradigm for multi-turn enterprise knowledge management that bridges LLMs and structured graphs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision</title>
<link>https://arxiv.org/abs/2508.03058</link>
<guid>https://arxiv.org/abs/2508.03058</guid>
<content:encoded><![CDATA[
arXiv:2508.03058v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or imperfect reward supervision in real-world settings, which undermines policy stability and generalization. Such noise may cause models to lose attention on key words during advantage estimation. While prior work focuses on reward denoising or filtering poor data, it often overlooks the critical role of the value model in policy optimization. In this work, we show that a strong value model is essential for mitigating noise by absorbing unstable signals and enabling more reliable advantage estimation. We propose VRPO, a value-centric framework for robust PPO training under noisy supervision. VRPO combines two core designs: (1) an auxiliary loss guided by entropy and perplexity from a frozen language model, and (2) a variational information bottleneck. These mechanisms enhance the value model's ability to filter out noise and capture key words from the context during advantage estimation, transforming it from a passive predictor into an active regulator of noise. Experiments on math reasoning, science QA, and multi-turn dialogue, under both rule-based and model-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO baselines. Our findings underscore the often-overlooked importance of the value model in RLHF and offer a principled and practical approach to robust policy optimization in noisy real-world environments.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework</title>
<link>https://arxiv.org/abs/2508.03092</link>
<guid>https://arxiv.org/abs/2508.03092</guid>
<content:encoded><![CDATA[
arXiv:2508.03092v1 Announce Type: cross 
Abstract: With the proliferation of Large Language Models (LLMs), the detection of misinformation has become increasingly important and complex. This research proposes an innovative verifiable misinformation detection LLM agent that goes beyond traditional true/false binary judgments. The agent actively verifies claims through dynamic interaction with diverse web sources, assesses information source credibility, synthesizes evidence, and provides a complete verifiable reasoning process. Our designed agent architecture includes three core tools: precise web search tool, source credibility assessment tool and numerical claim verification tool. These tools enable the agent to execute multi-step verification strategies, maintain evidence logs, and form comprehensive assessment conclusions. We evaluate using standard misinformation datasets such as FakeNewsNet, comparing with traditional machine learning models and LLMs. Evaluation metrics include standard classification metrics, quality assessment of reasoning processes, and robustness testing against rewritten content. Experimental results show that our agent outperforms baseline methods in misinformation detection accuracy, reasoning transparency, and resistance to information rewriting, providing a new paradigm for trustworthy AI-assisted fact-checking.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartCap: Mitigating Hallucination of Dense Chart Captioning</title>
<link>https://arxiv.org/abs/2508.03164</link>
<guid>https://arxiv.org/abs/2508.03164</guid>
<content:encoded><![CDATA[
arXiv:2508.03164v1 Announce Type: cross 
Abstract: Generating accurate, informative, and hallucination-free captions for charts remains challenging for vision language models, primarily due to the lack of large-scale, high-quality datasets of real-world charts. However, existing real-world chart datasets suffer from the inclusion of extraneous information that cannot be inferred from the chart and failure to sufficiently capture structural elements and key insights. Therefore, we introduce ChartCap, a large-scale dataset of 565K real-world chart images paired with type-specific, dense captions that exclude extraneous information and highlight both structural elements and key insights in detail. To build ChartCap, we design a four-stage pipeline that generates captions using only the discernible data from the chart and employ a cycle consistency-based human verification, which accelerates quality control without sacrificing accuracy. Additionally, we propose a novel metric, the Visual Consistency Score, which evaluates caption quality by measuring the similarity between the chart regenerated from a caption and the original chart, independent of reference captions. Extensive experiments confirms that models fine-tuned on ChartCap consistently generate more accurate and informative captions with reduced hallucinations, surpassing both open-source and proprietary models and even human-annotated captions.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Embedding Models on Hyper-relational Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.03280</link>
<guid>https://arxiv.org/abs/2508.03280</guid>
<content:encoded><![CDATA[
arXiv:2508.03280v1 Announce Type: cross 
Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an extension of traditional Knowledge Graphs (KGs) to better represent real-world facts with additional qualifiers. As a result, researchers have attempted to adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing extra qualifier processing modules. However, it remains unclear whether the superior performance of Hyper-relational KGE (HKGE) models arises from their base KGE model or the specially designed extension module. Hence, in this paper, we data-wise convert HKGs to KG format using three decomposition methods and then evaluate the performance of several classical KGE models on HKGs. Our results show that some KGE models achieve performance comparable to that of HKGE models. Upon further analysis, we find that the decomposition methods alter the original HKG topology and fail to fully preserve HKG information. Moreover, we observe that current HKGE models are either insufficient in capturing the graph's long-range dependency or struggle to integrate main-triple and qualifier information due to the information compression issue. To further justify our findings and offer a potential direction for future HKGE research, we propose the FormerGNN framework. This framework employs a qualifier integrator to preserve the original HKG topology, and a GNN-based graph encoder to capture the graph's long-range dependencies, followed by an improved approach for integrating main-triple and qualifier information to mitigate compression issues. Our experimental results demonstrate that FormerGNN outperforms existing HKGE models.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reliable Evaluation Protocol for Low-Precision Retrieval</title>
<link>https://arxiv.org/abs/2508.03306</link>
<guid>https://arxiv.org/abs/2508.03306</guid>
<content:encoded><![CDATA[
arXiv:2508.03306v1 Announce Type: cross 
Abstract: Lowering the numerical precision of model parameters and computations is widely adopted to improve the efficiency of retrieval systems. However, when computing relevance scores between the query and documents in low-precision, we observe spurious ties due to the reduced granularity. This introduces high variability in the results based on tie resolution, making the evaluation less reliable. To address this, we propose a more robust retrieval evaluation protocol designed to reduce score variation. It consists of: (1) High-Precision Scoring (HPS), which upcasts the final scoring step to higher precision to resolve tied candidates with minimal computational cost; and (2) Tie-aware Retrieval Metrics (TRM), which report expected scores, range, and bias to quantify order uncertainty of tied candidates. Our experiments test multiple models with three scoring functions on two retrieval datasets to demonstrate that HPS dramatically reduces tie-induced instability, and TRM accurately recovers expected metric values. This combination enables a more consistent and reliable evaluation system for lower-precision retrievals.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation</title>
<link>https://arxiv.org/abs/2508.03351</link>
<guid>https://arxiv.org/abs/2508.03351</guid>
<content:encoded><![CDATA[
arXiv:2508.03351v1 Announce Type: cross 
Abstract: Post-training quantization (PTQ) has emerged as an effective approach for compressing large models and accelerating their inference without retraining. While PTQ has been extensively studied in the context of large language models (LLMs), its applicability to vision-language models (VLMs) remains underexplored. In this paper, we identify a modality discrepancy (\emph{i.e.}, limited text tokens \emph{vs.} excessive and redundant vision tokens) of VLMs. However, existing Hessian-based LLM PTQ methods treat all tokens equally during quantization, resulting in severe performance drops when applied to VLMs. Motivated by this observation, we propose a novel importance-aware PTQ framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token redundancy, VLMQ 1) optimizes an importance-aware objective that yields an enhanced Hessian with token-level importance factors, while retaining compatibility with parallelized weight updates, and 2) ensures efficiency and effectiveness by computing these factors via a single lightweight block-wise backward pass, guided by a theoretical connection to token-level perturbations. Extensive evaluations on 8 benchmarks across 0.5B$\sim$32B VLMs demonstrate the state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit settings. For example, it achieves a substantial \textbf{16.45\%} improvement on MME-RealWorld under 2-bit quantization.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning</title>
<link>https://arxiv.org/abs/2508.03366</link>
<guid>https://arxiv.org/abs/2508.03366</guid>
<content:encoded><![CDATA[
arXiv:2508.03366v1 Announce Type: cross 
Abstract: General logical reasoning, defined as the ability to reason deductively on domain-agnostic tasks, continues to be a challenge for large language models (LLMs). Current LLMs fail to reason deterministically and are not interpretable. As such, there has been a recent surge in interest in neurosymbolic AI, which attempts to incorporate logic into neural networks. We first identify two main neurosymbolic approaches to improving logical reasoning: (i) the integrative approach comprising models where symbolic reasoning is contained within the neural network, and (ii) the hybrid approach comprising models where a symbolic solver, separate from the neural network, performs symbolic reasoning. Both contain AI systems with promising results on domain-specific logical reasoning benchmarks. However, their performance on domain-agnostic benchmarks is understudied. To the best of our knowledge, there has not been a comparison of the contrasting approaches that answers the following question: Which approach is more promising for developing general logical reasoning? To analyze their potential, the following best-in-class domain-agnostic models are introduced: Logic Neural Network (LNN), which uses the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the hybrid approach. Using both models as case studies and representatives of each approach, our analysis demonstrates that the hybrid approach is more promising for developing general logical reasoning because (i) its reasoning chain is more interpretable, and (ii) it retains the capabilities and advantages of existing LLMs. To support future works using the hybrid approach, we propose a generalizable framework based on LLM-SS that is modular by design, model-agnostic, domain-agnostic, and requires little to no human input.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2508.03481</link>
<guid>https://arxiv.org/abs/2508.03481</guid>
<content:encoded><![CDATA[
arXiv:2508.03481v1 Announce Type: cross 
Abstract: Personalized generation in T2I diffusion models aims to naturally incorporate individual user preferences into the generation process with minimal user intervention. However, existing studies primarily rely on prompt-level modeling with large-scale models, often leading to inaccurate personalization due to the limited input token capacity of T2I diffusion models. To address these limitations, we propose DrUM, a novel method that integrates user profiling with a transformer-based adapter to enable personalized generation through condition-level modeling in the latent space. DrUM demonstrates strong performance on large-scale datasets and seamlessly integrates with open-source text encoders, making it compatible with widely used foundation T2I models without requiring additional fine-tuning.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.03501</link>
<guid>https://arxiv.org/abs/2508.03501</guid>
<content:encoded><![CDATA[
arXiv:2508.03501v1 Announce Type: cross 
Abstract: Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation.
  To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agent's success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoKA: Mixture of Kronecker Adapters</title>
<link>https://arxiv.org/abs/2508.03527</link>
<guid>https://arxiv.org/abs/2508.03527</guid>
<content:encoded><![CDATA[
arXiv:2508.03527v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) is essential for reducing the computational overhead of large language models (LLMs). Low-rank family adapters are commonly used to control the parameter size efficiently while maintaining the generative power of LLMs. However, their limited expressiveness due to the rank constraint often restricts their performance on complex tasks. We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker adapters that addresses this limitation by modeling weight updates as a mixture of Kronecker products. Our proposed adapter leverages a gating mechanism that measures the importance of each Kronecker factor, enabling more expressive adaptation. Moreover, MoKA enables a rank flexibility that provides a better trade-off between parameter efficiency and accuracy. To ensure hardware efficiency, we reformulate Kronecker computations using standard matrix operations, allowing seamless deployment on GPU-optimized hardware. We conduct extensive experiments on instruction-tuning and commonsense reasoning tasks using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not only outperforms PEFT baselines, but also reduces the number of trainable parameters up to 27x, achieving state-of-the-art trade-offs between performance and parameter efficiency.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2508.03553</link>
<guid>https://arxiv.org/abs/2508.03553</guid>
<content:encoded><![CDATA[
arXiv:2508.03553v1 Announce Type: cross 
Abstract: Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyLate: Flexible Training and Retrieval for Late Interaction Models</title>
<link>https://arxiv.org/abs/2508.03555</link>
<guid>https://arxiv.org/abs/2508.03555</guid>
<content:encoded><![CDATA[
arXiv:2508.03555v1 Announce Type: cross 
Abstract: Neural ranking has become a cornerstone of modern information retrieval. While single vector search remains the dominant paradigm, it suffers from the shortcoming of compressing all the information into a single vector. This compression leads to notable performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches pioneered by ColBERT aim to address these limitations by preserving individual token embeddings and computing similarity via the MaxSim operator. This architecture has demonstrated superior empirical advantages, including enhanced out-of-domain generalization, long-context handling, and performance in complex retrieval scenarios. Despite these compelling empirical results and clear theoretical advantages, the practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models. To bridge this gap, we introduce PyLate, a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with. By offering multi-vector-specific features such as efficient indexes, PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems. Finally, PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching</title>
<link>https://arxiv.org/abs/2508.03562</link>
<guid>https://arxiv.org/abs/2508.03562</guid>
<content:encoded><![CDATA[
arXiv:2508.03562v1 Announce Type: cross 
Abstract: Internet memes, now a staple of digital communication, play a pivotal role in how users engage within online communities and allow researchers to gain insight into contemporary digital culture. These engaging user-generated content are characterised by their reuse of visual elements also found in other memes. Matching instances of memes via these shared visual elements, called Meme Matching, is the basis of a wealth of meme analysis approaches. However, most existing methods assume that every meme consists of a shared visual background, called a Template, with some overlaid text, thereby limiting meme matching to comparing the background image alone. Current approaches exclude the many memes that are not template-based and limit the effectiveness of automated meme analysis and would not be effective at linking memes to contemporary web-based meme dictionaries. In this work, we introduce a broader formulation of meme matching that extends beyond template matching. We show that conventional similarity measures, including a novel segment-wise computation of the similarity measures, excel at matching template-based memes but fall short when applied to non-template-based meme formats. However, the segment-wise approach was found to consistently outperform the whole-image measures on matching non-template-based memes. Finally, we explore a prompting-based approach using a pretrained Multimodal Large Language Model for meme matching. Our results highlight that accurately matching memes via shared visual elements, not just background templates, remains an open challenge that requires more sophisticated matching techniques.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</title>
<link>https://arxiv.org/abs/2508.03599</link>
<guid>https://arxiv.org/abs/2508.03599</guid>
<content:encoded><![CDATA[
arXiv:2508.03599v1 Announce Type: cross 
Abstract: This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed "BULLSHINT." Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</title>
<link>https://arxiv.org/abs/2508.03663</link>
<guid>https://arxiv.org/abs/2508.03663</guid>
<content:encoded><![CDATA[
arXiv:2508.03663v1 Announce Type: cross 
Abstract: Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple annotators for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and $N$ -- or if one even existed -- depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study</title>
<link>https://arxiv.org/abs/2111.05671</link>
<guid>https://arxiv.org/abs/2111.05671</guid>
<content:encoded><![CDATA[
arXiv:2111.05671v2 Announce Type: replace 
Abstract: Question answering(QA) is one of the most challenging yet widely investigated problems in Natural Language Processing (NLP). Question-answering (QA) systems try to produce answers for given questions. These answers can be generated from unstructured or structured text. Hence, QA is considered an important research area that can be used in evaluating text understanding systems. A large volume of QA studies was devoted to the English language, investigating the most advanced techniques and achieving state-of-the-art results. However, research efforts in the Arabic question-answering progress at a considerably slower pace due to the scarcity of research efforts in Arabic QA and the lack of large benchmark datasets. Recently many pre-trained language models provided high performance in many Arabic NLP problems. In this work, we evaluate the state-of-the-art pre-trained transformers models for Arabic QA using four reading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and TyDiQA-GoldP datasets. We fine-tuned and compared the performance of the AraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the last, we provide an analysis to understand and interpret the low-performance results obtained by some models.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions</title>
<link>https://arxiv.org/abs/2408.06787</link>
<guid>https://arxiv.org/abs/2408.06787</guid>
<content:encoded><![CDATA[
arXiv:2408.06787v4 Announce Type: replace 
Abstract: Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). By contrast, Large Language Models (LLMs) encapsulate extensive world knowledge and exhibit powerful context modeling capabilities, making them promising for mitigating the limitations of traditional methods. However, direct fine-tuning of LLMs for KGC, though effective, imposes substantial computational and memory overheads, while utilizing non-fine-tuned LLMs is efficient but yields suboptimal performance. In this work, we propose a novel framework that synergizes the strengths of LLMs with robust knowledge representation to enable effective and efficient KGC. We extract the context-aware hidden states of knowledge triples from the intermediate layers of LLMs, thereby capturing rich semantic and relational nuances. These representations are then utilized to train a data-efficient classifier tailored specifically for KGC tasks. To bridge the semantic gaps between LLMs and KGs, we employ subgraph sampling on KGs to generate model-friendly entity descriptions. We further adopt sliced mutual information (SMI) as a principled metric to quantify the task-specific information encoded in these representations. Extensive experiments on standard benchmarks validate the efficiency and effectiveness of our approach. We achieve a 47\% relative improvement over previous methods based on non-fine-tuned LLMs and, to our knowledge, are the first to achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and accelerating training and inference by $26.11\times$.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data</title>
<link>https://arxiv.org/abs/2409.16647</link>
<guid>https://arxiv.org/abs/2409.16647</guid>
<content:encoded><![CDATA[
arXiv:2409.16647v2 Announce Type: replace 
Abstract: Due to scarcity of time-series data annotated with descriptive texts, training a model to generate descriptive texts for time-series data is challenging. In this study, we propose a method to systematically generate domain-independent descriptive texts from time-series data. We identify two distinct approaches for creating pairs of time-series data and descriptive texts: the forward approach and the backward approach. By implementing the novel backward approach, we create the Temporal Automated Captions for Observations (TACO) dataset. Experimental results demonstrate that a contrastive learning based model trained using the TACO dataset is capable of generating descriptive texts for time-series data in novel domains.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.08920</link>
<guid>https://arxiv.org/abs/2412.08920</guid>
<content:encoded><![CDATA[
arXiv:2412.08920v3 Announce Type: replace 
Abstract: Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2501.13836</link>
<guid>https://arxiv.org/abs/2501.13836</guid>
<content:encoded><![CDATA[
arXiv:2501.13836v3 Announce Type: replace 
Abstract: Most social media users come from the Global South, where harmful content usually appears in local languages. Yet, AI-driven moderation systems struggle with low-resource languages spoken in these regions. Through semi-structured interviews with 22 AI experts working on harmful content detection in four low-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi Arabic (North Africa), and Quechua (South America)--we examine systemic issues in building automated moderation tools for these languages. Our findings reveal that beyond data scarcity, socio-political factors such as tech companies' monopoly on user data and lack of investment in moderation for low-profit Global South markets exacerbate historic inequities. Even if more data were available, the English-centric and data-intensive design of language models and preprocessing techniques overlooks the need to design for morphologically complex, linguistically diverse, and code-mixed languages. We argue these limitations are not just technical gaps caused by "data scarcity" but reflect structural inequities, rooted in colonial suppression of non-Western languages. We discuss multi-stakeholder approaches to strengthen local research capacity, democratize data access, and support language-aware solutions to improve automated moderation for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaMCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Multilingual Chain-of-Thought</title>
<link>https://arxiv.org/abs/2501.16154</link>
<guid>https://arxiv.org/abs/2501.16154</guid>
<content:encoded><![CDATA[
arXiv:2501.16154v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive multilingual capabilities through pretraining on diverse corpora. Although these models show strong reasoning abilities, their performance varies significantly between languages due to the imbalanced distribution of training data. Existing approaches using sample-level translation for extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaMCOT (Adaptive Multilingual Chain-of-Thought), a framework that enhances multilingual factual reasoning by dynamically routing thought processes in intermediary "thinking languages" before generating target-language responses. AdaMCOT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. An in-depth analysis of the model's hidden states and semantic space further elucidates the underlying mechanism of our method. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLIPPER: Compression enables long-context synthetic data generation</title>
<link>https://arxiv.org/abs/2502.14854</link>
<guid>https://arxiv.org/abs/2502.14854</guid>
<content:encoded><![CDATA[
arXiv:2502.14854v2 Announce Type: replace 
Abstract: LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M2S: Multi-turn to Single-turn jailbreak in Red Teaming for LLMs</title>
<link>https://arxiv.org/abs/2503.04856</link>
<guid>https://arxiv.org/abs/2503.04856</guid>
<content:encoded><![CDATA[
arXiv:2503.04856v3 Announce Type: replace 
Abstract: We introduce a novel framework for consolidating multi-turn adversarial ``jailbreak'' prompts into single-turn queries, significantly reducing the manual overhead required for adversarial testing of large language models (LLMs). While multi-turn human jailbreaks have been shown to yield high attack success rates, they demand considerable human effort and time. Our multi-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize -- systematically reformat multi-turn dialogues into structured single-turn prompts. Despite removing iterative back-and-forth interactions, these prompts preserve and often enhance adversarial potency: in extensive evaluations on the Multi-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success rates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs. Remarkably, the single-turn prompts outperform the original multi-turn attacks by as much as 17.5 percentage points while cutting token usage by more than half on average. Further analysis shows that embedding malicious requests in enumerated or code-like structures exploits ``contextual blindness'', bypassing both native guardrails and external input-output filters. By converting multi-turn conversations into concise single-turn prompts, the M2S framework provides a scalable tool for large-scale red teaming and reveals critical weaknesses in contemporary LLM defenses.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for Radiology Report Evaluation</title>
<link>https://arxiv.org/abs/2503.05347</link>
<guid>https://arxiv.org/abs/2503.05347</guid>
<content:encoded><![CDATA[
arXiv:2503.05347v2 Announce Type: replace 
Abstract: Automatic medical report generation has the potential to support clinical diagnosis, reduce the workload of radiologists, and demonstrate potential for enhancing diagnostic consistency. However, current evaluation metrics often fail to reflect the clinical reliability of generated reports. Early overlap-based methods focus on textual matches between predicted and ground-truth entities but miss fine-grained clinical details (e.g., anatomical location, severity). Some diagnostic metrics are limited by fixed vocabularies or templates, reducing their ability to capture diverse clinical expressions. LLM-based approaches further lack interpretable reasoning steps, making it hard to assess or trust their behavior in safety-critical settings. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs stable calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient = $0.69$ for ReXVal dataset and Kendall coefficient = $0.45$ for RadEvalX dataset). The anonymous project demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT is Devastated and LLaMA is Content: Emotion Representation Alignment in LLMs for Keyword-based Generation</title>
<link>https://arxiv.org/abs/2503.11881</link>
<guid>https://arxiv.org/abs/2503.11881</guid>
<content:encoded><![CDATA[
arXiv:2503.11881v2 Announce Type: replace 
Abstract: In controlled text generation using large language models (LLMs), gaps arise between the language model's interpretation of concepts and people's expectations. We introduce the human evaluation task of Representation Alignment for measuring this gap. We selected four emotion representations: Words, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and Numeric forms, and Emojis and evaluate them in the context of keyword-guided sentence generation using both GPT-4 and LLaMA-3. In addition to Representation Alignment, we also measure people's judgments of the accuracy and realism of the generated sentences. While representations like VAD break emotions into easy-to-compute components, our findings show that people agree more with how LLMs generate when conditioned on English words (e.g., ``angry'') rather than VAD scales. This difference is especially visible when comparing Numeric VAD to words. Furthermore, we found that the perception of how much a generated sentence conveys an emotion is dependent on both the representation type and which emotion it is.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ensemble Learning for Large Language Models in Text and Code Generation: A Survey</title>
<link>https://arxiv.org/abs/2503.13505</link>
<guid>https://arxiv.org/abs/2503.13505</guid>
<content:encoded><![CDATA[
arXiv:2503.13505v2 Announce Type: replace 
Abstract: Generative Pretrained Transformers (GPTs) are foundational Large Language Models (LLMs) for text generation. However, individual LLMs often produce inconsistent outputs and exhibit biases, limiting their representation of diverse language patterns. The closed-source nature of many powerful LLMs further restricts industry applications due to data privacy concerns. Inspired by successes in text generation, LLM ensemble techniques are now increasingly explored for code generation. This article reviews these emerging ensemble approaches to enhance understanding, encourage further research, and promote practical implementation in both text and code generation. We categorize LLM ensembles into seven main methods - weight merging, knowledge fusion, mixture-of-experts, reward ensemble, output ensemble, routing, and cascading - analyzing capabilities of those approaches. Our findings highlight key benefits such as improved diversity representation, enhanced output quality, and greater application flexibility. These insights aid model selection for real-world tasks and crucially, lay groundwork for extending ensemble strategies to multimodal LLMs.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems</title>
<link>https://arxiv.org/abs/2503.20756</link>
<guid>https://arxiv.org/abs/2503.20756</guid>
<content:encoded><![CDATA[
arXiv:2503.20756v3 Announce Type: replace 
Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promise in Autonomous Driving Systems (ADS). However, their direct application to ADS is hindered by challenges such as misunderstanding of traffic knowledge, complex road conditions, and diverse states of vehicle. To address these challenges, we propose the use of Knowledge Editing, which enables targeted modifications to a model's behavior without the need for full retraining. Meanwhile, we introduce ADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS, which includes various real-world scenarios, multiple data types, and comprehensive evaluation metrics. We conduct comprehensive experiments and derive several interesting conclusions. We hope that our work will contribute to the further advancement of knowledge editing applications in the field of autonomous driving. Code and data are available in https://github.com/zjunlp/EasyEdit/blob/main/examples/ADSEdit.md.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why do LLMs attend to the first token?</title>
<link>https://arxiv.org/abs/2504.02732</link>
<guid>https://arxiv.org/abs/2504.02732</guid>
<content:encoded><![CDATA[
arXiv:2504.02732v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs</title>
<link>https://arxiv.org/abs/2504.06219</link>
<guid>https://arxiv.org/abs/2504.06219</guid>
<content:encoded><![CDATA[
arXiv:2504.06219v2 Announce Type: replace 
Abstract: The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. In this work, we conceptualize this effect as the $\textit{data compliance gap}$ (DCG), which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. We measure the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pretraining). Our experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training. Our study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions. Our website is available at https://data-compliance.github.io/.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning</title>
<link>https://arxiv.org/abs/2504.07724</link>
<guid>https://arxiv.org/abs/2504.07724</guid>
<content:encoded><![CDATA[
arXiv:2504.07724v2 Announce Type: replace 
Abstract: In recent years, accurately and quickly deploying medical large language models (LLMs) has become a trend. Among these, retrieval-augmented generation (RAG) has garnered attention due to rapid deployment and privacy protection. However, the challenge hinder the practical deployment of RAG for medical diagnosis: the semantic gap between colloquial patient descriptions and the professional terminology within medical knowledge bases. We try to address the challenge from the data perspective and the method perspective. First, to address the semantic gap in existing knowledge bases, we construct DiagnosGraph, a generalist knowledge graph covering both modern medicine and Traditional Chinese Medicine. It contains 876 common diseases with the graph of 7,997 nodes and 37,201 triples. To bridge the gap between colloquial patient narratives and academic medical knowledge, DiagnosGraph also introduces $1,908$ medical record by formalizing the patient chief complaint and proposing a medical diagnosis. Second, we introduce the Multi-Round Diagnostic RAG (MRD-RAG) framework. It utilizes a multi-round dialogue to refine diagnostic possibilities, emulating the clinical reasoning of a physician. Experiments conducted on four medical benchmarks, with evaluations by human physicians, demonstrate that MRD-RAG enhances the diagnostic performance of LLMs, highlighting its potential to make automated diagnosis more accurate and human-aligned.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis</title>
<link>https://arxiv.org/abs/2504.12326</link>
<guid>https://arxiv.org/abs/2504.12326</guid>
<content:encoded><![CDATA[
arXiv:2504.12326v2 Announce Type: replace 
Abstract: Clinical case reports and discharge summaries may be the most complete and accurate summarization of patient encounters, yet they are finalized, i.e., timestamped after the encounter. Complementary data structured streams become available sooner but suffer from incompleteness. To train models and algorithms on more complete and temporally fine-grained data, we construct a pipeline to phenotype, extract, and annotate time-localized findings within case reports using large language models. We apply our pipeline to generate an open-access textual time series corpus for Sepsis-3 comprising 2,139 case reports from the Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA and timeline annotations from I2B2/MIMIC-IV and compare the results to physician-expert annotations. We show high recovery rates of clinical findings (event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B Instruct--0.932). Our work characterizes the ability of LLMs to time-localize clinical findings in text, illustrating the limitations of LLM use for temporal reconstruction and providing several potential avenues of improvement via multimodal integration.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Energy-Based Reward Models for Robust Language Model Alignment</title>
<link>https://arxiv.org/abs/2504.13134</link>
<guid>https://arxiv.org/abs/2504.13134</guid>
<content:encoded><![CDATA[
arXiv:2504.13134v2 Announce Type: replace 
Abstract: Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Science Hierarchography: Hierarchical Organization of Science Literature</title>
<link>https://arxiv.org/abs/2504.13834</link>
<guid>https://arxiv.org/abs/2504.13834</guid>
<content:encoded><![CDATA[
arXiv:2504.13834v2 Announce Type: replace 
Abstract: Scientific knowledge is growing rapidly, making it difficult to track progress and high-level conceptual links across broad disciplines. While tools like citation networks and search engines help retrieve related papers, they lack the abstraction needed to capture the needed to represent the density and structure of activity across subfields.
  We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that spans multiple levels of abstraction -- from broad domains to specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve this goal, we develop a hybrid approach that combines efficient embedding-based clustering with LLM-based prompting, striking a balance between scalability and semantic precision. Compared to LLM-heavy methods like iterative tree construction, our approach achieves superior quality-speed trade-offs. Our hierarchies capture different dimensions of research contributions, reflecting the interdisciplinary and multifaceted nature of modern science. We evaluate its utility by measuring how effectively an LLM-based agent can navigate the hierarchy to locate target papers. Results show that our method improves interpretability and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo are available: https://github.com/JHU-CLSP/science-hierarchography
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Performance Biases of Large Language Models in Education</title>
<link>https://arxiv.org/abs/2504.17720</link>
<guid>https://arxiv.org/abs/2504.17720</guid>
<content:encoded><![CDATA[
arXiv:2504.17720v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in eight languages (Mandarin, Hindi, Arabic, German, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering</title>
<link>https://arxiv.org/abs/2505.18247</link>
<guid>https://arxiv.org/abs/2505.18247</guid>
<content:encoded><![CDATA[
arXiv:2505.18247v3 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation</title>
<link>https://arxiv.org/abs/2506.05070</link>
<guid>https://arxiv.org/abs/2506.05070</guid>
<content:encoded><![CDATA[
arXiv:2506.05070v2 Announce Type: replace 
Abstract: Large language models (LLMs) possess strong multilingual capabilities, and combining Reinforcement Learning from Human Feedback (RLHF) with translation tasks has shown great potential. However, we observe that this paradigm performs unexpectedly poorly when applied to colloquial subtitle translation tasks. In this work, we investigate this issue and find that the offline reward model (RM) gradually diverges from the online LLM due to distributional shift, ultimately leading to undesirable training outcomes. To address this, we propose RIVAL, an adversarial training framework that formulates the process as a min-max game between the RM and the LLM. RIVAL iteratively updates the both models, with the RM trained to distinguish strong from weak translations (qualitative preference reward), and the LLM trained to enhance its translation for closing this gap. To stabilize training and improve generalizability, we also incorporate quantitative preference reward (e.g., BLEU) into the RM, enabling reference-free quality modeling aligned with human evaluation. Through extensive experiments, we demonstrate that the proposed adversarial training framework significantly improves upon translation baselines.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProRefine: Inference-Time Prompt Refinement with Textual Feedback</title>
<link>https://arxiv.org/abs/2506.05305</link>
<guid>https://arxiv.org/abs/2506.05305</guid>
<content:encoded><![CDATA[
arXiv:2506.05305v2 Announce Type: replace 
Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex tasks like reasoning or planning, play a substantial role in many cutting-edge commercial applications, and continue to fascinate researchers across nearly all fields for their potential to accomplish expensive, complex tasks that, until recently, only humans have been trusted to do. These workflows depend critically on the prompts used to provide the roles models play in such workflows. Poorly designed prompts that fail even slightly to guide individual agents can lead to sub-optimal performance that may snowball within a system of agents, limiting their reliability and scalability. To address this important problem of inference-time prompt optimization, we introduce ProRefine, an innovative inference-time optimization method that uses an agentic loop of LLMs to generate and apply textual feedback. ProRefine dynamically refines prompts for multi-step reasoning tasks without additional training or ground truth labels. Evaluated on five benchmark mathematical reasoning datasets, ProRefine significantly surpasses zero-shot Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only boosts accuracy but also allows smaller models to approach the performance of their larger counterparts. This highlights its potential for building more cost-effective and powerful hybrid AI systems, thereby democratizing access to high-performing AI.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark</title>
<link>https://arxiv.org/abs/2506.10960</link>
<guid>https://arxiv.org/abs/2506.10960</guid>
<content:encoded><![CDATA[
arXiv:2506.10960v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Makes a Good Speech Tokenizer for LLM-Centric Speech Generation? A Systematic Study</title>
<link>https://arxiv.org/abs/2506.12537</link>
<guid>https://arxiv.org/abs/2506.12537</guid>
<content:encoded><![CDATA[
arXiv:2506.12537v2 Announce Type: replace 
Abstract: Speech-language models (SLMs) offer a promising path toward unifying speech and text understanding and generation. However, challenges remain in achieving effective cross-modal alignment and high-quality speech generation. In this work, we systematically investigate the role of speech tokenizer designs in LLM-centric SLMs, augmented by speech heads and speaker modeling. We compare coupled, semi-decoupled, and fully decoupled speech tokenizers under a fair SLM framework and find that decoupled tokenization significantly improves alignment and synthesis quality. To address the information density mismatch between speech and text, we introduce multi-token prediction (MTP) into SLMs, enabling each hidden state to decode multiple speech tokens. This leads to up to 12$\times$ faster decoding and a substantial drop in word error rate (from 6.07 to 3.01). Furthermore, we propose a speaker-aware generation paradigm and introduce RoleTriviaQA, a large-scale role-playing knowledge QA benchmark with diverse speaker identities. Experiments demonstrate that our methods enhance both knowledge understanding and speaker consistency.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
arXiv:2506.19794v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI4Research: A Survey of Artificial Intelligence for Scientific Research</title>
<link>https://arxiv.org/abs/2507.01903</link>
<guid>https://arxiv.org/abs/2507.01903</guid>
<content:encoded><![CDATA[
arXiv:2507.01903v2 Announce Type: replace 
Abstract: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking</title>
<link>https://arxiv.org/abs/2507.03674</link>
<guid>https://arxiv.org/abs/2507.03674</guid>
<content:encoded><![CDATA[
arXiv:2507.03674v2 Announce Type: replace 
Abstract: The ability to extract structured information from unstructured sources-such as free-text documents and scientific literature-is critical for accelerating scientific discovery and knowledge synthesis. Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including structured information extraction. However, their effectiveness often diminishes in specialized, domain-specific contexts that require nuanced understanding and expert-level domain knowledge. In addition, existing LLM-based approaches frequently exhibit poor transferability across tasks and domains, limiting their scalability and adaptability. To address these challenges, we introduce StructSense, a modular, task-agnostic, open-source framework for structured information extraction built on LLMs. StructSense is guided by domain-specific symbolic knowledge encoded in ontologies, enabling it to navigate complex domain content more effectively. It further incorporates agentic capabilities through self-evaluative judges that form a feedback loop for iterative refinement, and includes human-in-the-loop mechanisms to ensure quality and validation. We demonstrate that StructSense can overcome both the limitations of domain sensitivity and the lack of cross-task generalizability, as shown through its application to diverse neuroscience information extraction tasks.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemOS: A Memory OS for AI System</title>
<link>https://arxiv.org/abs/2507.03724</link>
<guid>https://arxiv.org/abs/2507.03724</guid>
<content:encoded><![CDATA[
arXiv:2507.03724v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become an essential infrastructure for Artificial General Intelligence (AGI), yet their lack of well-defined memory management systems hinders the development of long-context reasoning, continual personalization, and knowledge consistency.Existing models mainly rely on static parameters and short-lived contextual states, limiting their ability to track user preferences or update knowledge over extended periods.While Retrieval-Augmented Generation (RAG) introduces external knowledge in plain text, it remains a stateless workaround without lifecycle control or integration with persistent representations.Recent work has modeled the training and inference cost of LLMs from a memory hierarchy perspective, showing that introducing an explicit memory layer between parameter memory and external retrieval can substantially reduce these costs by externalizing specific knowledge. Beyond computational efficiency, LLMs face broader challenges arising from how information is distributed over time and context, requiring systems capable of managing heterogeneous knowledge spanning different temporal scales and sources. To address this challenge, we propose MemOS, a memory operating system that treats memory as a manageable system resource. It unifies the representation, scheduling, and evolution of plaintext, activation-based, and parameter-level memories, enabling cost-efficient storage and retrieval. As the basic unit, a MemCube encapsulates both memory content and metadata such as provenance and versioning. MemCubes can be composed, migrated, and fused over time, enabling flexible transitions between memory types and bridging retrieval with parameter-based learning. MemOS establishes a memory-centric system framework that brings controllability, plasticity, and evolvability to LLMs, laying the foundation for continual learning and personalized modeling.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings</title>
<link>https://arxiv.org/abs/2507.17234</link>
<guid>https://arxiv.org/abs/2507.17234</guid>
<content:encoded><![CDATA[
arXiv:2507.17234v2 Announce Type: replace 
Abstract: Automatic generation of radiology reports has the potential to alleviate radiologists' significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) enforces reasoning-aware decoding that completes "Findings" before synthesizing the "Impression", and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a reasoning-aware next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive Findings section before synthesizing the Impression and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on both standard NLG metrics and clinically aware scores.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults</title>
<link>https://arxiv.org/abs/2404.17730</link>
<guid>https://arxiv.org/abs/2404.17730</guid>
<content:encoded><![CDATA[
arXiv:2404.17730v3 Announce Type: replace-cross 
Abstract: High-tech Augmentative and Alternative Communication (AAC) has been rapidly advancing in recent years due to the increased use of large language models (LLMs) like ChatGPT, but many of these techniques are integrated without the inclusion of the users' perspectives. Autistic adults have been particularly neglected in the design of AAC tools. We conducted in-depth interviews with 12 autistic adults to find the pain points of current AAC and determine what technological advances they might find helpful. We found 8 different categories of themes from our interviews: input flexibility, output flexibility, selecting or adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling blocks for continued use, and control of communication. In this paper, we go through these categories in depth -- comparing each to prior work -- and then highlight novel findings to suggest possible research directions.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation</title>
<link>https://arxiv.org/abs/2410.14971</link>
<guid>https://arxiv.org/abs/2410.14971</guid>
<content:encoded><![CDATA[
arXiv:2410.14971v3 Announce Type: replace-cross 
Abstract: Current EEG/MEG-to-text decoding systems suffer from three key limitations: (1) reliance on teacher-forcing methods, which compromises robustness during inference, (2) sensitivity to session-specific noise, hindering generalization across subjects, and (3) misalignment between brain signals and linguistic representations due to pre-trained language model over-dominance. To overcome these challenges, we propose BrainECHO (Brain signal decoding via vEctor-quantized speCtrogram reconstruction for WHisper-enhanced text generatiOn), a multi-stage framework that employs decoupled representation learning to achieve state-of-the-art performance on both EEG and MEG datasets. Specifically, BrainECHO consists of three stages: (1) Discrete autoencoding, which transforms continuous Mel spectrograms into a finite set of high-quality discrete representations for subsequent stages. (2) Frozen alignment, where brain signal embeddings are mapped to corresponding Mel spectrogram embeddings in a frozen latent space, effectively filtering session-specific noise through vector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score. (3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper model for audio-to-text translation, balancing signal adaptation with knowledge preservation, and achieving 74%-89% decoding BLEU scores without excessive reliance on teacher forcing. BrainECHO demonstrates robustness across sentence, session, and subject-independent conditions, passing Gaussian noise tests and showcasing its potential for enhancing language-based brain-computer interfaces.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation</title>
<link>https://arxiv.org/abs/2411.19331</link>
<guid>https://arxiv.org/abs/2411.19331</guid>
<content:encoded><![CDATA[
arXiv:2411.19331v2 Announce Type: replace-cross 
Abstract: Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form textual concepts without predefined training classes. While existing vision-language models such as CLIP can generate segmentation masks by leveraging coarse spatial information from Vision Transformers, they face challenges in spatial localization due to their global alignment of image and text features. Conversely, self-supervised visual models like DINO excel in fine-grained visual encoding but lack integration with language. To bridge this gap, we present Talk2DINO, a novel hybrid approach that combines the spatial accuracy of DINOv2 with the language understanding of CLIP. Our approach aligns the textual embeddings of CLIP to the patch-level features of DINOv2 through a learned mapping function without the need to fine-tune the underlying backbones. At training time, we exploit the attention maps of DINOv2 to selectively align local visual patches with textual embeddings. We show that the powerful semantic and localization abilities of Talk2DINO can enhance the segmentation process, resulting in more natural and less noisy segmentations, and that our approach can also effectively distinguish foreground objects from the background. Experimental results demonstrate that Talk2DINO achieves state-of-the-art performance across several unsupervised OVS benchmarks. Source code and models are publicly available at: https://lorebianchi98.github.io/Talk2DINO/.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WSI-LLaVA: A Multimodal Large Language Model for Whole Slide Image</title>
<link>https://arxiv.org/abs/2412.02141</link>
<guid>https://arxiv.org/abs/2412.02141</guid>
<content:encoded><![CDATA[
arXiv:2412.02141v3 Announce Type: replace-cross 
Abstract: Recent advancements in computational pathology have produced patch-level Multi-modal Large Language Models (MLLMs), but these models are limited by their inability to analyze whole slide images (WSIs) comprehensively and their tendency to bypass crucial morphological features that pathologists rely on for diagnosis. To address these challenges, we first introduce WSI-Bench, a large-scale morphology-aware benchmark containing 180k VQA pairs from 9,850 WSIs across 30 cancer types, designed to evaluate MLLMs' understanding of morphological characteristics crucial for accurate diagnosis. Building upon this benchmark, we present WSI-LLaVA, a novel framework for gigapixel WSI understanding that employs a three-stage training approach: WSI-text alignment, feature space alignment, and task-specific instruction tuning. To better assess model performance in pathological contexts, we develop two specialized WSI metrics: WSI-Precision and WSI-Relevance. Experimental results demonstrate that WSI-LLaVA outperforms existing models across all capability dimensions, with a significant improvement in morphological analysis, establishing a clear correlation between morphological understanding and diagnostic accuracy.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CutPaste&amp;Find: Efficient Multimodal Hallucination Detector with Visual-aid Knowledge Base</title>
<link>https://arxiv.org/abs/2502.12591</link>
<guid>https://arxiv.org/abs/2502.12591</guid>
<content:encoded><![CDATA[
arXiv:2502.12591v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal reasoning capabilities, but they remain susceptible to hallucination, particularly object hallucination where non-existent objects or incorrect attributes are fabricated in generated descriptions. Existing detection methods achieve strong performance but rely heavily on expensive API calls and iterative LVLM-based validation, making them impractical for large-scale or offline use. To address these limitations, we propose CutPaste\&amp;Find, a lightweight and training-free framework for detecting hallucinations in LVLM-generated outputs. Our approach leverages off-the-shelf visual and linguistic modules to perform multi-step verification efficiently without requiring LVLM inference. At the core of our framework is a Visual-aid Knowledge Base that encodes rich entity-attribute relationships and associated image representations. We introduce a scaling factor to refine similarity scores, mitigating the issue of suboptimal alignment values even for ground-truth image-text pairs. Comprehensive evaluations on benchmark datasets, including POPE and R-Bench, demonstrate that CutPaste\&amp;Find achieves competitive hallucination detection performance while being significantly more efficient and cost-effective than previous methods.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health</title>
<link>https://arxiv.org/abs/2502.13920</link>
<guid>https://arxiv.org/abs/2502.13920</guid>
<content:encoded><![CDATA[
arXiv:2502.13920v2 Announce Type: replace-cross 
Abstract: Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Context Relational Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2503.10408</link>
<guid>https://arxiv.org/abs/2503.10408</guid>
<content:encoded><![CDATA[
arXiv:2503.10408v2 Announce Type: replace-cross 
Abstract: Binary relations, such as equality, are basic mathematical concepts that appear, implicitly or explicitly, in most benchmarks for Large Language Models (LLM). A recent trend in the literature is benchmarking LLMs on out-of-context learning, where the data is not presented in the prompt, but only during the model's training. However, existing works mostly focus on higher-order tasks, making it hard to interpret success or failure. In this work, we study how well can LLMs reason out-of-context on binary relations by only learning the representations of newly introduced tokens. Our experiments focus on equality ($=$), inequality ($<$), and inclusion ($\subset$) and the properties they satisfy, such as reflexivity, symmetry, transitivity, and logical complexity (e.g., the number of reasoning "hops"). We show that LLMs achieve better than random accuracy, but are still far from perfect, even on relatively simple reasoning tasks involving binary relations. We analyse the learned representations and show that LLMs encode useful information directly, arranging the embeddings according to the task.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems</title>
<link>https://arxiv.org/abs/2504.09037</link>
<guid>https://arxiv.org/abs/2504.09037</guid>
<content:encoded><![CDATA[
arXiv:2504.09037v3 Announce Type: replace-cross 
Abstract: Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis</title>
<link>https://arxiv.org/abs/2504.10352</link>
<guid>https://arxiv.org/abs/2504.10352</guid>
<content:encoded><![CDATA[
arXiv:2504.10352v3 Announce Type: replace-cross 
Abstract: Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Antidistillation Sampling</title>
<link>https://arxiv.org/abs/2504.13146</link>
<guid>https://arxiv.org/abs/2504.13146</guid>
<content:encoded><![CDATA[
arXiv:2504.13146v3 Announce Type: replace-cross 
Abstract: Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference</title>
<link>https://arxiv.org/abs/2505.12260</link>
<guid>https://arxiv.org/abs/2505.12260</guid>
<content:encoded><![CDATA[
arXiv:2505.12260v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs)-based text retrieval retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full LLM on an A800 GPU, our method achieves over 1000x speedup in query encoding and over 10x increase in end-to-end retrieval throughput. Extensive experiments on large-scale retrieval benchmarks show that LightRetriever generalizes well across diverse tasks, maintaining an average of 95% retrieval performance.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Foundations for Preference Optimization</title>
<link>https://arxiv.org/abs/2507.07855</link>
<guid>https://arxiv.org/abs/2507.07855</guid>
<content:encoded><![CDATA[
arXiv:2507.07855v2 Announce Type: replace-cross 
Abstract: In this paper, we show that direct preference optimization (DPO) is a very specific form of a connection between two major theories in the ML context of learning from preferences: loss functions (Savage) and stochastic choice (Doignon-Falmagne and Machina). The connection is established for all of Savage's losses and at this level of generality, (i) it includes support for abstention on the choice theory side, (ii) it includes support for non-convex objectives on the ML side, and (iii) it allows to frame for free some notable extensions of the DPO setting, including margins and corrections for length. Getting to understand how DPO operates from a general principled perspective is crucial because of the huge and diverse application landscape of models, because of the current momentum around DPO, but also -- and importantly -- because many state of the art variations on DPO definitely occupy a small region of the map that we cover. It also helps to understand the pitfalls of departing from this map, and figure out workarounds.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination</title>
<link>https://arxiv.org/abs/2507.10532</link>
<guid>https://arxiv.org/abs/2507.10532</guid>
<content:encoded><![CDATA[
arXiv:2507.10532v2 Announce Type: replace-cross 
Abstract: Reasoning in large language models has long been a central research focus, and recent studies employing reinforcement learning (RL) have introduced diverse methods that yield substantial performance gains with minimal or even no external supervision. Surprisingly, some studies even suggest that random or incorrect reward signals can enhance performance. However, these breakthroughs are predominantly observed for the mathematically strong Qwen2.5 series on benchmarks such as MATH-500, AMC, and AIME, and seldom transfer to models like Llama, which warrants a more in-depth investigation. In this work, our empirical analysis reveals that pre-training on massive web-scale corpora leaves Qwen2.5 susceptible to data contamination in widely used benchmarks. Consequently, conclusions derived from contaminated benchmarks on Qwen2.5 series may be unreliable. To obtain trustworthy evaluation results, we introduce a generator that creates fully clean arithmetic problems of arbitrary length and difficulty, dubbed RandomCalculation. Using this leakage-free dataset, we show that only accurate reward signals yield steady improvements that surpass the base model's performance boundary in mathematical reasoning, whereas random or incorrect rewards do not. Moreover, we conduct more fine-grained analyses to elucidate the factors underlying the different performance observed on the MATH-500 and RandomCalculation benchmarks. Consequently, we recommend that future studies evaluate models on uncontaminated benchmarks and, when feasible, test various model series to ensure trustworthy conclusions about RL and related methods.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark</title>
<link>https://arxiv.org/abs/2507.15882</link>
<guid>https://arxiv.org/abs/2507.15882</guid>
<content:encoded><![CDATA[
arXiv:2507.15882v2 Announce Type: replace-cross 
Abstract: The proliferation of multimodal Large Language Models has significantly advanced the ability to analyze and understand complex data inputs from different modalities. However, the processing of long documents remains under-explored, largely due to a lack of suitable benchmarks. To address this, we introduce Document Haystack, a comprehensive benchmark designed to evaluate the performance of Vision Language Models (VLMs) on long, visually complex documents. Document Haystack features documents ranging from 5 to 200 pages and strategically inserts pure text or multimodal text+image "needles" at various depths within the documents to challenge VLMs' retrieval capabilities. Comprising 400 document variants and a total of 8,250 questions, it is supported by an objective, automated evaluation framework. We detail the construction and characteristics of the Document Haystack dataset, present results from prominent VLMs and discuss potential research avenues in this area.
]]></content:encoded>
<pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in Argument Mining: A Survey</title>
<link>https://arxiv.org/abs/2506.16383</link>
<guid>https://arxiv.org/abs/2506.16383</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument Mining, Natural Language Processing, Large Language Models, Annotation frameworks, Evaluation practices

Summary: 
Argument Mining (AM) in Natural Language Processing has been significantly impacted by the emergence of Large Language Models (LLMs), allowing for in-context learning, prompt-based generation, and cross-domain adaptability. This survey examines recent advancements in LLM-driven AM, covering foundational theories, annotation frameworks, datasets, and a taxonomy of AM subtasks. The review includes discussions on LLM techniques such as prompting and chain-of-thought reasoning, as well as challenges like long-context reasoning and annotation bottlenecks. Assessment of LLM architectures, methodologies, and evaluation practices is provided, along with insights into interpretability issues. The paper concludes by identifying trends and proposing a research agenda for future developments in LLM-based computational argumentation. The comprehensive overview aims to support researchers in navigating the evolving landscape of AM with LLMs.<br /><br />Summary: <div>
arXiv:2506.16383v5 Announce Type: replace 
Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing (NLP), focuses on extracting argumentative structures from text. The advent of Large Language Models (LLMs) has profoundly transformed AM, enabling advanced in-context learning, prompt-based generation, and robust cross-domain adaptability. This survey systematically synthesizes recent advancements in LLM-driven AM. We provide a concise review of foundational theories and annotation frameworks, alongside a meticulously curated catalog of datasets. A key contribution is our comprehensive taxonomy of AM subtasks, elucidating how contemporary LLM techniques -- such as prompting, chain-of-thought reasoning, and retrieval augmentation -- have reconfigured their execution. We further detail current LLM architectures and methodologies, critically assess evaluation practices, and delineate pivotal challenges including long-context reasoning, interpretability, and annotation bottlenecks. Conclusively, we highlight emerging trends and propose a forward-looking research agenda for LLM-based computational argumentation, aiming to strategically guide researchers in this rapidly evolving domain.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HuggingGraph: Understanding the Supply Chain of LLM Ecosystem</title>
<link>https://arxiv.org/abs/2507.14240</link>
<guid>https://arxiv.org/abs/2507.14240</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, natural language processing, Hugging Face, vulnerabilities, supply chain

Summary:
Large language models (LLMs) are powerful tools in natural language processing, but their size and complexity pose challenges for researchers. Platforms like Hugging Face host a vast number of models and datasets, but these can inherit vulnerabilities, biases, or malicious components from previous models. To address this, a study examines the supply chain of LLMs by collecting information and creating a graph to model relationships between models and datasets. Analysis on this graph reveals insights into the development process and potential risks associated with LLMs. Understanding these relationships is crucial for improving model fairness, detecting risks, and ensuring compliance with regulatory frameworks.<br /><br />Summary: <div>
arXiv:2507.14240v2 Announce Type: replace 
Abstract: Large language models (LLMs) leverage deep learning architectures to process and predict sequences of words based on context, enabling them to perform a wide range of natural language processing tasks, such as translation, summarization, question answering, and content generation. However, the increasing size and complexity of developing, training, and deploying cutting-edge LLMs demand extensive computational resources and large-scale datasets. This creates a significant barrier for researchers and practitioners. Because of that, platforms that host models and datasets have gained widespread popularity. For example, on one of the most popular platforms, i.e., Hugging Face, there are more than 1.8 million models and more than 450K datasets by the end of June 2025, and the trend does not show any slowdown.
  As existing LLMs are often built from base models or other pretrained models and use external datasets, they can inevitably inherit vulnerabilities, biases, or malicious components that exist in previous models or datasets. Therefore, it is critical to understand these components' origin and development process to detect potential risks better, improve model fairness, and ensure compliance with regulatory frameworks. Motivated by that, this project aims to study such relationships between models and datasets, which are the central parts of the LLM supply chain. First, we design a methodology to collect LLMs' supply chain information systematically. With the collected information, we design a new graph to model the relationships between models and datasets, which is a large directed heterogeneous graph having 402,654 nodes and 462,524 edges. Then, on top of this graph, we perform different types of analysis and make multiple interesting findings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches</title>
<link>https://arxiv.org/abs/2508.00864</link>
<guid>https://arxiv.org/abs/2508.00864</guid>
<content:encoded><![CDATA[
<div> Keywords: document classification, graph-based models, self-attention model, data-driven graph structures, statistical filtering

Summary:
Our study introduces a novel method for document classification that utilizes graph-based models to capture document structure effectively. Unlike previous approaches, our method learns data-driven graph structures, eliminating the need for manual design and reducing domain dependence. The approach constructs homogeneous weighted graphs with sentences as nodes and learns edges using a self-attention model to identify dependencies between sentence pairs. A statistical filtering strategy is employed to retain only strongly correlated sentences, enhancing graph quality and reducing size. Experimental results on three document classification datasets show that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. The study also highlights the effectiveness of statistical filtering in improving classification robustness. These findings suggest the potential of automatic graph generation in NLP applications and pave the way for broader applications in the field. 

Summary: <div>
arXiv:2508.00864v1 Announce Type: new 
Abstract: In document classification, graph-based models effectively capture document structure, overcoming sequence length limitations and enhancing contextual understanding. However, most existing graph document representations rely on heuristics, domain-specific rules, or expert knowledge. Unlike previous approaches, we propose a method to learn data-driven graph structures, eliminating the need for manual design and reducing domain dependence. Our approach constructs homogeneous weighted graphs with sentences as nodes, while edges are learned via a self-attention model that identifies dependencies between sentence pairs. A statistical filtering strategy aims to retain only strongly correlated sentences, improving graph quality while reducing the graph size. Experiments on three document classification datasets demonstrate that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness of the statistical filtering in improving classification robustness. These results highlight the potential of automatic graph generation over traditional heuristic approaches and open new directions for broader applications in NLP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts</title>
<link>https://arxiv.org/abs/2508.00889</link>
<guid>https://arxiv.org/abs/2508.00889</guid>
<content:encoded><![CDATA[
<div> Large language models, factuality evaluation, contact center conversations, FECT benchmark dataset, 3D paradigm
Summary: 
- The article introduces the challenges of hallucinations in large language models analyzing contact center conversations.
- A 3D paradigm (Decompose, Decouple, Detach) is introduced for factuality evaluation, grounding factuality labels in linguistically-informed criteria.
- The FECT benchmark dataset is presented for Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts, labeled under the 3D paradigm.
- Findings on aligning LLM-judges on the 3D paradigm are reported.
- The study contributes a new approach for automatically evaluating factuality in AI-generated outputs from contact center conversations.
<br /><br /> <div>
arXiv:2508.00889v1 Announce Type: new 
Abstract: Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for factuality evaluation, because ground-truth labels often do not exist for analytical interpretations about sentiments captured in the conversation and root causes of the business problems. To remedy this, we first introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in the human annotation guideline and the LLM-judges' prompt to ground the factuality labels in linguistically-informed evaluation criteria. We then introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality \textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm. Lastly, we report our findings from aligning LLM-judges on the 3D paradigm. Overall, our findings contribute a new approach for automatically evaluating the factuality of outputs generated by an AI system for analyzing contact center conversations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML</title>
<link>https://arxiv.org/abs/2508.00924</link>
<guid>https://arxiv.org/abs/2508.00924</guid>
<content:encoded><![CDATA[
<div> AutoML, language models, meta-learning, fine-tuning, resource-efficient <br />
Summary:<br />
XAutoLM is a new automated framework for efficient language model fine-tuning that leverages meta-learning to optimize model selection and hyperparameter optimization. By learning from past experiences, XAutoLM extracts meta-features to guide the search process towards successful configurations and away from dead ends. Results on various text classification and question-answering tasks show that XAutoLM outperforms zero-shot optimization methods, reduces evaluation time, decreases error rates, and identifies more effective pipelines. The framework is designed to enhance resource-efficient and environmentally friendly fine-tuning of language models in the NLP community. The release of XAutoLM and the associated experience store aims to facilitate Green AI practices and promote more sustainable machine learning processes. <br /> <br />Summary: <div>
arXiv:2508.00924v1 Announce Type: new 
Abstract: Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyse resource-efficient, Green AI fine-tuning in the NLP community.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.01005</link>
<guid>https://arxiv.org/abs/2508.01005</guid>
<content:encoded><![CDATA[
<div> Keywords: question-answering, retrieval-augmented generation, adaptive framework, multi-agent orchestration, reinforcement learning

Summary: 
The article introduces a new adaptive framework for question-answering systems called MAO-ARAG, which utilizes multi-agent orchestration to dynamically plan workflows for each query. The framework consists of multiple executor agents for tasks such as query reformulation, document selection, and generation. A planner agent selects and integrates these executors in a suitable workflow, guided by reinforcement learning. The framework aims to balance high-quality answers with reasonable costs by optimizing a reward based on F1 score and penalizing costs. Experimental results on various datasets show that MAO-ARAG achieves high answer quality while maintaining cost and latency within acceptable limits. The code for MAO-ARAG is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.01005v1 Announce Type: new 
Abstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has become pivotal in enhancing response accuracy and reducing hallucination issues. The architecture of RAG systems varies significantly, encompassing single-round RAG, iterative RAG, and reasoning RAG, each tailored to address different types of queries. Due to the varying complexity of real-world queries, a fixed RAG pipeline often struggles to balance performance and cost efficiency across different queries. To address this challenge, we propose an adaptive RAG framework called MAO-ARAG, which leverages multi-agent orchestration. Our adaptive RAG is conceived as a multi-turn framework. Specifically, we define multiple executor agents, representing typical RAG modules such as query reformulation agents, document selection agent, and generation agents. A planner agent intelligently selects and integrates the appropriate agents from these executors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable costs. During each turn, the planner agent is trained using reinforcement learning, guided by an outcome-based reward (F1 score) and a cost-based penalty, continuously improving answer quality while keeping costs within a reasonable range. Experiments conducted on multiple QA datasets demonstrate that our approach, which dynamically plans workflows for each query, not only achieves high answer quality but also maintains both cost and latency within acceptable limits.The code of MAO-ARAG is on https://github.com/chenyiqun/Agentic-RAG.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu</title>
<link>https://arxiv.org/abs/2508.01006</link>
<guid>https://arxiv.org/abs/2508.01006</guid>
<content:encoded><![CDATA[
<div> Benchmark, Linguistic Minimal Pairs, Urdu, Multilingual Large Language Models, Syntactic Phenomena<br />
<br />
Summary: <br />
A new benchmark called UrBLiMP assesses the linguistic knowledge of Multilingual Large Language Models (LLMs) in Urdu by using pairs of minimally different sentences. The dataset comprises 5,696 minimal pairs targeting core syntactic phenomena in Urdu, ensuring reliability through human evaluation with a 96.10% inter-annotator agreement. Evaluation of twenty multilingual LLMs on UrBLiMP reveals varying performance across linguistic phenomena, with models like LLaMA-3-70B and Gemma-3-27B-PT achieving high accuracy. This study showcases the potential and limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages. <br /> <div>
arXiv:2508.01006v1 Announce Type: new 
Abstract: Multilingual Large Language Models (LLMs) have shown remarkable performance across various languages; however, they often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu, we present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of minimally different sentences that contrast in grammatical acceptability. UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena, carefully curated using the Urdu Treebank and diverse Urdu text corpora. A human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator agreement, confirming the reliability of the dataset. We evaluate twenty multilingual LLMs on UrBLiMP, revealing significant variation in performance across linguistic phenomena. While LLaMA-3-70B achieves the highest average accuracy (94.73%), its performance is statistically comparable to other top models such as Gemma-3-27B-PT. These findings highlight both the potential and the limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Domain Web Information Extraction at Pinterest</title>
<link>https://arxiv.org/abs/2508.01096</link>
<guid>https://arxiv.org/abs/2508.01096</guid>
<content:encoded><![CDATA[
<div> structured data, Pinterest, attribute extraction, webpage representation, XGBoost 

Summary:
- Pinterest has developed a system for accurately extracting structured product data from e-commerce websites.
- The system uses a unique webpage representation that combines structural, visual, and text modalities for efficient learning.
- This representation captures information from visible HTML nodes, including text, style, and layout details.
- Simple models like eXtreme Gradient Boosting (XGBoost) outperform more complex models like Generative Pre-trained Transformer (GPT) in attribute extraction accuracy.
- The system is highly scalable, processing over 1,000 URLs per second, and is significantly more cost-effective than GPT alternatives. 

<br /><br />Summary: <div>
arXiv:2508.01096v1 Announce Type: new 
Abstract: The internet offers a massive repository of unstructured information, but it's a significant challenge to convert this into a structured format. At Pinterest, the ability to accurately extract structured product data from e-commerce websites is essential to enhance user experiences and improve content distribution. In this paper, we present Pinterest's system for attribute extraction, which achieves remarkable accuracy and scalability at a manageable cost. Our approach leverages a novel webpage representation that combines structural, visual, and text modalities into a compact form, optimizing it for small model learning. This representation captures each visible HTML node with its text, style and layout information. We show how this allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract attributes more accurately than much more complex Large Language Models (LLMs) such as Generative Pre-trained Transformer (GPT). Our results demonstrate a system that is highly scalable, processing over 1,000 URLs per second, while being 1000 times more cost-effective than the cheapest GPT alternatives.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates</title>
<link>https://arxiv.org/abs/2508.01159</link>
<guid>https://arxiv.org/abs/2508.01159</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, clinical consultation templates, electronic consultation, prioritization analysis, structured clinical information exchange <br />
Summary: 
This study evaluates the capabilities of large language models (LLMs) in generating structured clinical consultation templates for electronic consultations. Models like o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro were assessed for their ability to create clinically coherent and concise templates. While these models showed high comprehensiveness, they struggled with generating overly long templates and prioritizing the most important clinical questions within length constraints. Performance varied across different medical specialties, with narrative-driven fields like psychiatry and pain medicine showing significant deterioration. The study highlights the potential of LLMs in improving structured clinical information exchange among physicians but also underscores the necessity for more robust evaluation methods that consider a model's capacity to prioritize crucial clinical information within real-world communication time constraints. <br /><br />Summary: <div>
arXiv:2508.01159v1 Announce Type: new 
Abstract: This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages</title>
<link>https://arxiv.org/abs/2508.01161</link>
<guid>https://arxiv.org/abs/2508.01161</guid>
<content:encoded><![CDATA[
<div> Keywords: emotion recognition, multilingual LLMs, Semeval 2025, task adaptation strategies, LoRA <br />
Summary: <br />
- The study focuses on emotion recognition across different languages, highlighting the challenges due to varied expressions and cultural nuances.
- The Semeval 2025 Task 11, "Bridging the Gap in Text-Based Emotion," aims to develop an emotion recognizer for identifying basic emotional states and their intensities in written text snippets.
- Various task-adaptation strategies for Language Models (LLMs) in emotion recognition were investigated, with a focus on fine-tuning pre-trained multilingual LLMs using LoRA setting separately for each language.
- The study found that fine-tuning a multilingual LLM with LoRA setting for each language proved to be the most effective method for the emotion recognition task.
- This research contributes to the advancement of emotion recognition technology and highlights the importance of considering language-specific nuances in emotion analysis. <br /> 
Summary: <div>
arXiv:2508.01161v1 Announce Type: new 
Abstract: Detecting emotions across different languages is challenging due to the varied and culturally nuanced ways of emotional expressions. The \textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared task was organised to investigate emotion recognition across different languages. The goal of the task is to implement an emotion recogniser that can identify the basic emotional states that general third-party observers would attribute to an author based on their written text snippet, along with the intensity of those emotions. We report our investigation of various task-adaptation strategies for LLMs in emotion recognition. We show that the most effective method for this task is to fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Content Restriction for Large Language Models via Suffix Optimization</title>
<link>https://arxiv.org/abs/2508.01198</link>
<guid>https://arxiv.org/abs/2508.01198</guid>
<content:encoded><![CDATA[
<div> Adaptive Content Restriction, Large Language Models, Supervised Fine-Tuning, Suffix Optimization, Content Restriction Benchmark  
Summary:  
- Large Language Models (LLMs) have been successful but face challenges in enforcing content restrictions.
- Supervised Fine-Tuning (SFT) is used to prevent LLMs from generating harmful content.
- The proposed task, Adaptive Content Restriction (AdaCoRe), focuses on lightweight strategies to prevent LLMs from generating specific restricted terms.
- Suffix Optimization (SOP) is introduced as a method to append a short suffix to prompts to achieve content restriction without model fine-tuning.
- The effectiveness of SOP is demonstrated on the Content Restriction Benchmark (CoReBench) and online platform POE, showcasing practical applicability in real-world scenarios.  
<br /><br />Summary:  
This article introduces Adaptive Content Restriction (AdaCoRe) as a lightweight strategy to prevent Large Language Models (LLMs) from generating specific restricted terms. The proposed method, Suffix Optimization (SOP), effectively appends optimized suffixes to prompts, achieving content restriction without the need for model fine-tuning. Evaluation on the Content Restriction Benchmark (CoReBench) and the online platform POE shows that SOP outperforms system-level baselines, highlighting its practical utility in real-world applications. <div>
arXiv:2508.01198v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Show or Tell? Modeling the evolution of request-making in Human-LLM conversations</title>
<link>https://arxiv.org/abs/2508.01213</link>
<guid>https://arxiv.org/abs/2508.01213</guid>
<content:encoded><![CDATA[
<div> keywords: chat logs, user behavior, LLM queries, diachronic analyses, model capabilities
Summary: 
Chat logs are a valuable resource for studying user behavior in Language Model (LLM) queries. A new task of segmenting chat queries into different elements highlights how request-making in LLM queries differs from human-human interactions. Diachronic analyses on user expressions reveal evolving query patterns, with early queries focusing on requests and users eventually converging with experience. The introduction of new models influences user behavior, leading to traceable changes at the community level. This research sheds light on the complexities of user interactions in chat-based systems and the impact of model capabilities on query patterns over time.<br /><br />Summary: <div>
arXiv:2508.01213v1 Announce Type: new 
Abstract: Chat logs provide a rich source of information about LLM users, but patterns of user behavior are often masked by the variability of queries. We present a new task, segmenting chat queries into contents of requests, roles, query-specific context, and additional expressions. We find that, despite the familiarity of chat-based interaction, request-making in LLM queries remains significantly different from comparable human-human interactions. With the data resource, we introduce an important perspective of diachronic analyses with user expressions. We find that query patterns vary between early ones emphasizing requests, and individual users explore patterns but tend to converge with experience. Finally, we show that model capabilities affect user behavior, particularly with the introduction of new models, which are traceable at the community level.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebDS: An End-to-End Benchmark for Web-based Data Science</title>
<link>https://arxiv.org/abs/2508.01222</link>
<guid>https://arxiv.org/abs/2508.01222</guid>
<content:encoded><![CDATA[
<div> benchmark, web-based data science, multi-hop interactions, end-to-end workflows, performance gaps <br />
Summary: <br />
The article introduces WebDS, the first end-to-end web-based data science benchmark consisting of 870 tasks from diverse websites. It addresses the complexity of real-world data science tasks that involve multi-hop interactions and diverse tool-using capabilities. Existing benchmarks focus on simplistic interactions or static datasets, lacking the realism of modern data analytics tasks. Evaluation of state-of-the-art language model agents on WebDS reveals significant performance gaps, indicating the need for improvement in handling complex web-based data science tasks. Browser Use, which performs well on other benchmarks, struggles on WebDS due to new failure modes like poor information grounding and repetitive behavior. By providing a more realistic testing ground, WebDS aims to advance the development of language model-based data science applications. <br /> <div>
arXiv:2508.01222v1 Announce Type: new 
Abstract: A large portion of real-world data science tasks are complex and require multi-hop web-based interactions: finding appropriate data available on the internet, synthesizing real-time data of various modalities from different locations, and producing summarized analyses. Existing web benchmarks often focus on simplistic interactions, such as form submissions or e-commerce transactions, and often do not require diverse tool-using capabilities required for web based data science. Conversely, traditional data science benchmarks typically concentrate on static, often textually bound datasets and do not assess end-to-end workflows that encompass data acquisition, cleaning, analysis, and insight generation. In response, we introduce WebDS, the first end-to-end web-based data science benchmark. It comprises 870 web-based data science tasks across 29 diverse websites from structured government data portals to unstructured news media, challenging agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats that better reflect the realities of modern data analytics. Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web Voyager, successfully completes only 15% of tasks in WebDS, which our analysis suggests is due to new failure modes like poor information grounding, repetitive behavior and shortcut-taking that agents performing WebDS' tasks display. By providing a more robust and realistic testing ground, WebDS sets the stage for significant advances in the development of practically useful LLM-based data science.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework</title>
<link>https://arxiv.org/abs/2508.01245</link>
<guid>https://arxiv.org/abs/2508.01245</guid>
<content:encoded><![CDATA[
<div> framework, mathematics, data synthesis, progressive training, LLMs
Summary:
WarriorMath, a defect-aware framework for mathematical problem solving, addresses limitations in Large Language Models (LLMs) by integrating targeted data synthesis and progressive training. In the synthesis stage, multiple expert LLMs collaborate to generate, refine, and improve problems that base models struggle with. This results in high-quality, defect-aware training data. The training stage employs a progressive learning framework that fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experimental results on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. This approach demonstrates the effectiveness of a defect-aware, multi-expert framework in enhancing mathematical ability. <br /><br />Summary: <div>
arXiv:2508.01245v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel in solving mathematical problems, yet their performance is often limited by the availability of high-quality, diverse training data. Existing methods focus on augmenting datasets through rephrasing or difficulty progression but overlook the specific failure modes of LLMs. This results in synthetic questions that the model can already solve, providing minimal performance gains. To address this, we propose WarriorMath, a defect-aware framework for mathematical problem solving that integrates both targeted data synthesis and progressive training. In the synthesis stage, we employ multiple expert LLMs in a collaborative process to generate, critique, and refine problems. Questions that base LLMs fail to solve are identified and iteratively improved through expert-level feedback, producing high-quality, defect-aware training data. In the training stage, we introduce a progressive learning framework that iteratively fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experiments on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. Our results demonstrate the effectiveness of a defect-aware, multi-expert framework for improving mathematical ability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025</title>
<link>https://arxiv.org/abs/2508.01263</link>
<guid>https://arxiv.org/abs/2508.01263</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Education, XAI, Hackathon, Trustworthiness

Summary:
The article discusses the XAI Challenge 2025, a hackathon-style competition focused on eXplainable AI in education, specifically building QA systems for student queries with logic-based explanations. Organized by HCMUT and TRNS-AI at IJCNN 2025, the challenge aimed to promote transparency using lightweight Large Language Models (LLMs). The dataset used logic-based templates validated with Z3 and refined through expert review to align with real-world academic scenarios. The paper outlines the challenge's motivation, structure, dataset construction, and evaluation protocol, highlighting the novel effort to blend LLMs and symbolic reasoning for explainability. The findings provide valuable insights for future XAI-centered educational systems and competitive research initiatives.

<br /><br />Summary: The XAI Challenge 2025 focused on developing QA systems for student queries in education using lightweight Large Language Models (LLMs) and logic-based explanations. The competition, held at IJCNN 2025, aimed to promote transparency and trustworthiness by bridging LLMs and symbolic reasoning. The dataset construction involved logic templates validated with Z3 and refined through expert review. The challenge's structure, motivation, and evaluation protocol aimed to advance XAI in educational contexts, offering actionable insights for future research initiatives. <div>
arXiv:2508.01263v1 Announce Type: new 
Abstract: The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities</title>
<link>https://arxiv.org/abs/2508.01290</link>
<guid>https://arxiv.org/abs/2508.01290</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Knowledge Graphs, Question Answering, Unseen Entity KGQA

Summary:
Retrieval-Augmented Generation (RAG) utilizes explicit answer evidence, implicit answer clues, and partially relevant information to enhance Large Language Models (LLMs). This study explores the concept of awakening LLMs using partially relevant knowledge already embedded in them, rather than solely relying on external retrieval sources. The research investigates the impact of awakening on LLMs using triplets from gold reasoning paths with removed answer-containing paths. Theoretical analysis and experiments on Knowledge Graphs (KGs) Question Answering (QA) datasets support the effectiveness of this awakening-based approach. Additionally, a new task, Unseen Entity KGQA, is introduced to address challenges arising from incomplete knowledge base retrieval. The awakening-based method outperforms traditional embedding-based similarity approaches, particularly in real-world scenarios where entity linking fails due to incomplete KGs. 

<br /><br />Summary: 
1. RAG leverages various types of knowledge to enhance LLMs, including explicit evidence, implicit clues, and partially relevant information.
2. The awakening concept suggests utilizing partially relevant knowledge already embedded in LLMs for improved performance.
3. Experimental results and theoretical analysis support the effectiveness of awakening LLMs.
4. Introduction of the Unseen Entity KGQA task addresses challenges from incomplete knowledge base retrieval.
5. The awakening-based approach shows superiority over traditional methods in scenarios where entity linking fails due to incomplete KGs. <div>
arXiv:2508.01290v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) shows impressive performance by supplementing and substituting parametric knowledge in Large Language Models (LLMs). Retrieved knowledge can be divided into three types: explicit answer evidence, implicit answer clue, and insufficient answer context which can be further categorized into totally irrelevant and partially relevant information. Effectively utilizing partially relevant knowledge remains a key challenge for RAG systems, especially in incomplete knowledge base retrieval. Contrary to the conventional view, we propose a new perspective: LLMs can be awakened via partially relevant knowledge already embedded in LLMs. To comprehensively investigate this phenomenon, the triplets located in the gold reasoning path and their variants are used to construct partially relevant knowledge by removing the path that contains the answer. We provide theoretical analysis of the awakening effect in LLMs and support our hypothesis with experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we present a new task, Unseen Entity KGQA, simulating real-world challenges where entity linking fails due to KG incompleteness. Our awakening-based approach demonstrates greater efficacy in practical applications, outperforms traditional methods that rely on embedding-based similarity which are prone to returning noisy information.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference</title>
<link>https://arxiv.org/abs/2508.01302</link>
<guid>https://arxiv.org/abs/2508.01302</guid>
<content:encoded><![CDATA[
<div> alignment, knowledge editing, language models, augmentation, inference<br />
Summary:<br />
The paper introduces a new method, KEDAS, for knowledge editing in large language models (LLMs). KEDAS aligns LLMs with edited knowledge by utilizing low-rank adaptation in the alignment phase. It incorporates a diverse edit augmentation technique to enhance the recall of edits. A self-adaptive post-alignment inference mechanism is proposed, leveraging a smart retriever for dynamic selection of inference routing. KEDAS outperforms existing methods in 35 out of 36 cases across multiple datasets and LLM settings, showing superior performance in edit success, locality, and portability. It proves more efficient and robust while maintaining high performance levels in general tasks. KEDAS presents a promising approach to efficient knowledge editing alignment in language models. <br />Summary: <div>
arXiv:2508.01302v1 Announce Type: new 
Abstract: Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their powerful capabilities. Most existing methods rely on either parameter-level editing or retrieval-based approaches. In this work, we propose Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference (KEDAS) to better align LLMs with knowledge editing. In the alignment phase, LLMs learn to apply in-context edited knowledge via low-rank adaptation. During editing, we design a diverse edit augmentation technique to improve the recall of edits. After that, a self-adaptive post-alignment inference mechanism is proposed, in which a filter-based smart retriever is employed to perform a dynamic selection of inference routing. Specifically, irrelevant queries will go through the original pre-alignment model directly, while relevant ones, together with their related edits, go through the model with aligned adapters activated. In experiments, KEDAS secures the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs on three settings, surpassing its strong knowledge editing alignment counterpart by about 19.8 harmonic mean scores of edit success, locality and portability and outperforming both parameter editing and retrieval-based baselines significantly. Analysis of computational cost and performance on general tasks further validates the robustness and efficiency of KEDAS, indicating that it presents an ideal paradigm of knowledge editing alignment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation</title>
<link>https://arxiv.org/abs/2508.01309</link>
<guid>https://arxiv.org/abs/2508.01309</guid>
<content:encoded><![CDATA[
<div> Prompt engineering, large language models, question-answering datasets, domain-specific fine-tuning, D-SCoRE <br />
Summary: D-SCoRE introduces a training-free pipeline that leverages large language models (LLMs) and prompt engineering to create high-quality question-answering datasets for domain-specific fine-tuning. This pipeline, integrating Document-centric processing, Segmentation, CoT Reasoning, and structured Export, generates QA datasets suited for supervised fine-tuning. Various control mechanisms enhance diversity and relevance by incorporating semantic role transformation, question type balancing, and counterfactual materials. Evaluations show that LLMs fine-tuned on D-SCoRE-generated datasets outperform those trained on human-annotated datasets across different domains. D-SCoRE efficiently generates QA-CoT pairs with counterfactual materials within a short time frame, showcasing simplicity and scalability in QA generation and fine-tuning processes. This approach allows for high-performance fine-tuning of LLMs in various domains, addressing the scarcity and cost issues of high-quality QA datasets. <br /><br />Summary: <div>
arXiv:2508.01309v1 Announce Type: new 
Abstract: The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. D-SCoRE integrates $\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T $\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms, such as semantic role transformation, question type balancing, and counterfactual materials, enhance diversity and relevance, overcoming limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points</title>
<link>https://arxiv.org/abs/2508.01317</link>
<guid>https://arxiv.org/abs/2508.01317</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, knowledge point graph, QA data, synthesis framework, LinkQA

Summary:
The article introduces LinkSyn, a framework for synthesizing diverse question-answering (QA) data to enhance training for large language models (LLMs). LinkSyn utilizes a knowledge point graph to extract KPs from QA seed data and synthesize QA data from multiple sources while balancing coverage and popularity of KPs. By incorporating a knowledge distribution value function, diffusion-based synthesis, and difficulty adjustment mechanisms, LinkSyn creates LinkQA, a multi-disciplinary QA dataset with 50 billion tokens. Experiment results on the Llama-3 8B model show that pre-training with LinkQA leads to significant improvements in MMLU and CMMLU metrics. This approach consistently boosts model performance across different scales and demonstrates state-of-the-art results in large language model advancement.

<br /><br />Summary: <div>
arXiv:2508.01317v1 Announce Type: new 
Abstract: The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data. To address this limitation, we propose LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. LinkSyn extracts KPs from question-answering (QA) seed data and constructs a KP graph to synthesize diverse QA data from multiple seeds strongly linked by KPs and sampled from graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution value function to guide the adjustment of path sampling probability and balance KP coverage and popularity during graph walks; (2) diffusion-based synthesis via DeepSeek-R1 by leveraging multiple seeds with dense logical associations along each path; and (3) high-difficulty QA enhancement within given disciplines by flexible difficulty adjustments. By executing LinkSyn, we synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Extensive experiments on Llama-3 8B demonstrate that continual pre-training with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large-Scale Diverse Synthesis for Mid-Training</title>
<link>https://arxiv.org/abs/2508.01326</link>
<guid>https://arxiv.org/abs/2508.01326</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, QA dataset, knowledge diversity, STEM disciplines, data synthesis

Summary:
BoostQA is a novel pipeline for synthesizing a large-scale QA dataset by curating seed data from various sources and implementing STEM-focused multi-grade synthesis to enhance data diversity and high-difficulty synthesis to mitigate difficulty degradation. The dataset is refined using DeepSeek-V3 to improve output quality. BoostQA is utilized in mid-training to optimize domain-specific knowledge acquisition and enhance data quality. The methodology enables mid-trained Llama-3 8B to achieve a significant average improvement of 12.74% on MMLU and CMMLU and establishes state-of-the-art performance across 12 benchmarks. BoostQA demonstrates robust scalability, with performance consistently improving with increased model size, data volume, and initial FLOPs scaling.<br /><br />Summary: BoostQA is a pipeline for synthesizing a large-scale QA dataset with diverse knowledge and high difficulty data synthesis, refined using DeepSeek-V3. It enhances mid-training for Llama-3 8B, resulting in significant performance improvements and demonstrating scalability across various metrics. <div>
arXiv:2508.01326v1 Announce Type: new 
Abstract: The scarcity of high-quality, knowledge-intensive training data hinders the development of large language models (LLMs), as traditional corpora provide limited information. Previous studies have synthesized and integrated corpora-dependent question-answering (QA) data to improve model performance but face challenges in QA data scalability and knowledge diversity, particularly in cross-domain contexts. Furthermore, leveraging our designed discipline and difficulty annotation system, we probe model deficiencies in STEM disciplines and high-difficulty data. To overcome these limitations, we propose a novel diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA dataset. Our synthesis framework: (1) curates seed data from heterogeneous sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade synthesis to boost data diversity and high-difficulty synthesis to mitigate difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output quality. We utilize BoostQA in mid-training, a mid-stage between pre-training and post-training, to optimize domain-specific knowledge acquisition and enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also demonstrates robust scalability, with performance consistently improving as model size, data volume, and initial FLOPs scale.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis</title>
<link>https://arxiv.org/abs/2508.01370</link>
<guid>https://arxiv.org/abs/2508.01370</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Business Analysis, Market Report Generation, Autonomous Framework, Automated Review Cycles

Summary:
An autonomous framework utilizing Large Language Models (LLMs) has been developed to automate end-to-end business analysis and market report generation. The system consists of specialized agents - Researcher, Reviewer, Writer, and Retriever - working together to analyze data and generate comprehensive reports. By learning from real professional consultants' materials, the framework replicates professional analytical methodologies. A multi-step process including querying databases, analyzing data, generating insights, creating visualizations, and composing reports is executed. A novel LLM-based evaluation system assesses report quality, showing alignment with expert evaluations. An iterative improvement mechanism optimizes report quality through automated review cycles and consultants' knowledge. Experimental results demonstrate the framework can generate detailed reports in minutes at a low cost. This work represents a significant advancement in automatically creating affordable market insights.<br /><br />Summary: <div>
arXiv:2508.01370v1 Announce Type: new 
Abstract: We present an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation. At its core, the system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports. We also introduce a novel LLM-based evaluation system for assessing report quality, which shows alignment with expert human evaluations. Building on these evaluations, we implement an iterative improvement mechanism that optimizes report quality through automated review cycles. Experimental results show that report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately \$1. Our work could be an important step to automatically create affordable market insights.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs</title>
<link>https://arxiv.org/abs/2508.01401</link>
<guid>https://arxiv.org/abs/2508.01401</guid>
<content:encoded><![CDATA[
<div> Keywords: Physicians, medical documentation, automation tools, dataset, Dialog-to-Note task

Summary:
Physicians face challenges in documenting clinical encounters leading to professional burnout. The MedSynth dataset introduces synthetic medical dialogues and notes to improve the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. With over 10,000 dialogue-note pairs covering 2000 ICD-10 codes, this dataset enhances models' performance in generating medical notes from dialogues and vice versa. By analyzing disease distributions, the dataset provides diverse and privacy-compliant training data. The availability of code and dataset on GitHub and Hugging Face platforms offers open-access resources in a field lacking such resources.<br /><br />Summary: <div>
arXiv:2508.01401v1 Announce Type: new 
Abstract: Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available at https://huggingface.co/datasets/Ahmad0067/MedSynth.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations</title>
<link>https://arxiv.org/abs/2508.01411</link>
<guid>https://arxiv.org/abs/2508.01411</guid>
<content:encoded><![CDATA[
<div> Keywords: Egyptian Arabic, parallel dataset, machine translation, translation studies, pedagogical purposes

Summary:
The arXiv article introduces the ArzEn-MultiGenre dataset, which consists of parallel Egyptian Arabic song lyrics, novels, and TV show subtitles with their English translations. With 25,557 segment pairs, the dataset can be used for benchmarking new machine translation models, fine-tuning language models, and improving commercial translation applications like Google Translate. The dataset's value extends to research in translation studies, cross-linguistic analysis, and lexical semantics. Additionally, it can benefit translation students and professional translators as a training and memory resource. The dataset stands out for its inclusion of diverse textual genres not found in existing datasets and for its high-quality human expert translations and alignments.

<br /><br />Summary: <div>
arXiv:2508.01411v1 Announce Type: new 
Abstract: ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles that are manually translated and aligned with their English counterparts. The dataset contains 25,557 segment pairs that can be used to benchmark new machine translation models, fine-tune large language models in few-shot settings, and adapt commercial machine translation applications such as Google Translate. Additionally, the dataset is a valuable resource for research in various disciplines, including translation studies, cross-linguistic analysis, and lexical semantics. The dataset can also serve pedagogical purposes by training translation students and aid professional translators as a translation memory. The contributions are twofold: first, the dataset features textual genres not found in existing parallel Egyptian Arabic and English datasets, and second, it is a gold-standard dataset that has been translated and aligned by human experts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Bias Associations through Open-Ended LLM Generations</title>
<link>https://arxiv.org/abs/2508.01412</link>
<guid>https://arxiv.org/abs/2508.01412</guid>
<content:encoded><![CDATA[
<div> Framework, Bias Association, Large Language Models, Social Biases, Evaluation Methods

Summary: 
The Bias Association Discovery Framework (BADF) is introduced to tackle social biases embedded in Large Language Models (LLMs). These biases can lead to unfair or distorted representations of demographic groups, impacting the generated language. Existing evaluation methods have limitations in identifying new or unexpected biases. BADF systematically extracts known and previously unnoticed associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through extensive experiments across various models and real-world scenarios, BADF allows for robust mapping and analysis of concepts associated with demographic identities. This framework enhances the understanding of biases in open-ended text generation and offers a scalable tool for detecting and examining bias associations in LLMs. The data, code, and results of the study are accessible on GitHub at https://github.com/JP-25/Discover-Open-Ended-Generation. 

<br /><br />Summary: <div>
arXiv:2508.01412v1 Announce Type: new 
Abstract: Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias. In this work, we present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts, BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs. Data, code, and results are available at https://github.com/JP-25/Discover-Open-Ended-Generation
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2508.01424</link>
<guid>https://arxiv.org/abs/2508.01424</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, complex multi-hop question answering, ontology-driven reasoning, knowledge graphs, logical reasoning.

Summary:
ORACLE (Ontology-driven Reasoning And Chain for Logical Elucidation) is a training-free framework that addresses the limitations of Large Language Models (LLMs) in complex multi-hop question answering tasks by combining generative capabilities with knowledge graphs. The framework dynamically constructs question-specific knowledge ontologies using LLMs, transforms them into First-Order Logic reasoning chains, and decomposes the query into logically coherent sub-questions. Experimental results on standard benchmarks show competitive performance comparable to state-of-the-art models like DeepSeek-R1. Detailed analyses confirm the effectiveness of each component and demonstrate that ORACLE generates more logical and interpretable reasoning chains than existing approaches.<br /><br />Summary: ORACLE is a framework that enhances the reasoning capabilities of LLMs by combining them with knowledge graphs. It constructs ontologies, transforms them into logical chains, and decomposes queries for improved multi-hop question answering performance. Experimental results show ORACLE's competitiveness and effectiveness in generating logical and interpretable reasoning chains. <div>
arXiv:2508.01424v1 Announce Type: new 
Abstract: Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present **ORACLE** (**O**ntology-driven **R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a training-free framework that combines LLMs' generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data</title>
<link>https://arxiv.org/abs/2508.01450</link>
<guid>https://arxiv.org/abs/2508.01450</guid>
<content:encoded><![CDATA[
<div> fine-tuning, language models, data selection, medical reasoning, gradient influence<br />
Summary:<br />
Supervised Fine-Tuning (SFT) for Large Language Models (LLMs) in specialized domains like medical reasoning often suffers from using unfiltered datasets leading to computational costs and suboptimal performance. A new data selection strategy, Difficulty-Influence Quadrant (DIQ), prioritizes samples with high difficulty and high influence, balancing reasoning complexity with gradient impact. DIQ-selected subsets show higher data quality and generate expert-like clinical reasoning. Experiments demonstrate that models fine-tuned on only 1% of selected data match full-dataset performance, while using 10% consistently outperforms the baseline. This highlights the effectiveness of principled data selection over brute-force scaling.<br /> <div>
arXiv:2508.01450v1 Announce Type: new 
Abstract: Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeDiff: AST-Guided Code Generation with Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.01473</link>
<guid>https://arxiv.org/abs/2508.01473</guid>
<content:encoded><![CDATA[
<div> diffusion-based language models, sequence generation, source code, syntax-aware corruption, Abstract Syntax Trees (ASTs) 
Summary: 
Syntax-aware diffusion framework proposed for structured domains, such as source code, by incorporating structural priors from ASTs into denoising process. Standard token-level corruption techniques not suitable for code due to strict syntax and semantics. Selective corruption of syntactically meaningful code spans from AST subtrees improves reconstruction accuracy, syntactic correctness, and generalization on unseen code patterns. Model reconstructs programs respecting grammatical boundaries and capturing long-range dependencies. Syntax-guided denoising enhances diffusion-based language models for code generation tasks. <div>
arXiv:2508.01473v1 Announce Type: new 
Abstract: Recent advances in diffusion-based language models have opened new possibilities for controllable and bidirectional sequence generation. These models provide an alternative to traditional autoregressive approaches by framing text generation as an iterative denoising process. However, applying diffusion models to structured domains such as source code remains a significant challenge. Programming languages differ from natural language in that they follow strict syntactic and semantic rules, with hierarchical organization that must be preserved for correctness. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code. To address this limitation, we propose a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees. This enables the model to reconstruct programs in a way that respects grammatical boundaries and captures long-range dependencies. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach</title>
<link>https://arxiv.org/abs/2508.01480</link>
<guid>https://arxiv.org/abs/2508.01480</guid>
<content:encoded><![CDATA[
<div> Biomedical text mining, question-answering, BioASQ challenge, large language models, semantic question-answering <br />
<br />
Summary: 
This study focuses on biomedical question-answering in the BioASQ challenge, utilizing open-source large language models (LLMs). Multiple models are used in a majority voting system for Yes/No questions and their union for list and factoid type questions. Evaluation of 13 LLMs reveals optimal combinations for specific question types. The system achieved noteworthy results in the 2025 BioASQ challenge, securing top rankings for ideal and exact answers in the Synergy task across multiple rounds. <div>
arXiv:2508.01480v1 Announce Type: new 
Abstract: Biomedical text mining and question-answering are essential yet highly demanding tasks, particularly in the face of the exponential growth of biomedical literature. In this work, we present our participation in the 13th edition of the BioASQ challenge, which involves biomedical semantic question-answering for Task 13b and biomedical question-answering for developing topics for the Synergy task. We deploy a selection of open-source large language models (LLMs) as retrieval-augmented generators to answer biomedical questions. Various models are used to process the questions. A majority voting system combines their output to determine the final answer for Yes/No questions, while for list and factoid type questions, the union of their answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring all possible model combinations to contribute to the final answer, resulting in tailored LLM pipelines for each question type. Our findings provide valuable insight into which combinations of LLMs consistently produce superior results for specific question types. In the four rounds of the 2025 BioASQ challenge, our system achieved notable results: in the Synergy task, we secured 1st place for ideal answers and 2nd place for exact answers in round 2, as well as two shared 1st places for exact answers in round 3 and 4.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu</title>
<link>https://arxiv.org/abs/2508.01486</link>
<guid>https://arxiv.org/abs/2508.01486</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, sentiment classification, explainability, fairness, Telugu

Summary:<br />
- TeSent is a benchmark dataset for sentiment classification in Telugu, addressing the underrepresentation of the language in NLP and Machine Learning.
- It consists of 26,150 sentences scraped from social media platforms and news websites, with ground truth labels and human-annotated rationales.
- The dataset includes provisions for evaluating explainability and fairness, crucial in modern machine learning tasks.
- Fine-tuning SOTA pre-trained models with and without rationales showed potential for accuracy improvement and bias reduction.
- The plausibility and faithfulness evaluation suite provides insights into the alignment of model explainers with human reasoning.<br /><br />Summary: <div>
arXiv:2508.01486v1 Announce Type: new 
Abstract: In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 26,150 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with rationales may improve model accuracy, reduce bias in models, and make the explainers' output more aligned to human reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Homogenizing Effect of Large Language Models on Human Expression and Thought</title>
<link>https://arxiv.org/abs/2508.01491</link>
<guid>https://arxiv.org/abs/2508.01491</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive diversity, Language models, Collective intelligence, Homogenization, Marginalization  

Summary: Large language models (LLMs) play a crucial role in shaping language and reasoning patterns in society. However, there is a concern that as LLMs become more prevalent, they may standardize language and reasoning processes, potentially marginalizing alternative voices and cognitive strategies. This review discusses how LLMs reflect and reinforce dominant styles while diminishing diversity in language and reasoning. The design and widespread use of LLMs contribute to this homogenization by replicating patterns from their training data and promoting a convergence of language and thinking across different contexts. If left unchecked, this trend could lead to the flattening of cognitive landscapes that drive collective intelligence and adaptability. It is important to consider the potential consequences of this homogenization on creativity and diversity in society. 

<br /><br />Summary: Cognitive diversity is essential for collective intelligence, but large language models (LLMs) may standardize language and reasoning, marginalizing alternative voices and strategies. The design and use of LLMs mirror dominant styles, amplifying convergence in language and thinking. This homogenization poses a risk to cognitive landscapes that foster adaptability and collective intelligence. <div>
arXiv:2508.01491v1 Announce Type: new 
Abstract: Cognitive diversity, reflected in variations of language, perspective, and reasoning, is essential to creativity and collective intelligence. This diversity is rich and grounded in culture, history, and individual experience. Yet as large language models (LLMs) become deeply embedded in people's lives, they risk standardizing language and reasoning. This Review synthesizes evidence across linguistics, cognitive, and computer science to show how LLMs reflect and reinforce dominant styles while marginalizing alternative voices and reasoning strategies. We examine how their design and widespread use contribute to this effect by mirroring patterns in their training data and amplifying convergence as all people increasingly rely on the same models across contexts. Unchecked, this homogenization risks flattening the cognitive landscapes that drive collective intelligence and adaptability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents</title>
<link>https://arxiv.org/abs/2508.01503</link>
<guid>https://arxiv.org/abs/2508.01503</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, pedagogical agents, Evidence-Centered Design, Social Cognitive Theory, STEM+C learning

Summary:<br /><br />Large language models (LLMs) such as ChatGPT can enhance student learning through dialogue, but often lack theoretical foundations. To address this, a framework combining Evidence-Centered Design and Social Cognitive Theory is proposed for adaptive scaffolding in LLM-based agents focused on STEM+C learning. Illustrated with the Inquizzitor agent, this framework integrates human-AI hybrid intelligence to offer feedback grounded in cognitive science principles. Findings demonstrate Inquizzitor's ability to provide high-quality assessment aligned with core learning theories, aiding teachers in offering effective guidance valued by students. This research showcases the potential for theory-driven integration of LLMs in education, illustrating their capacity to deliver adaptive and principled instruction. <div>
arXiv:2508.01503v1 Announce Type: new 
Abstract: Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization</title>
<link>https://arxiv.org/abs/2508.01541</link>
<guid>https://arxiv.org/abs/2508.01541</guid>
<content:encoded><![CDATA[
<div> Keywords: Prompt engineering, Large Language Models, Multi-objective Evolutionary Optimization, Sentiment analysis, Portuguese

Summary: 
The paper introduces MOPrompt, a Multi-objective Evolutionary Optimization framework that optimizes prompts for both accuracy and context size simultaneously. Existing automated methods often focus on a single objective, neglecting the trade-off between task performance and context size. MOPrompt maps the Pareto front of prompt solutions, providing practitioners with trade-offs between context size and performance. Evaluation on a sentiment analysis task in Portuguese showed MOPrompt outperforming baseline frameworks. For the Sabiazinho model, MOPrompt identified a prompt achieving the same peak accuracy as the best baseline solution but with a 31% reduction in token length. This framework offers a crucial tool for deploying Large Language Models in real-world applications, addressing the challenge of balancing efficiency and effectiveness in prompt optimization.<br /><br />Summary: <div>
arXiv:2508.01541v1 Announce Type: new 
Abstract: Prompt engineering is crucial for unlocking the potential of Large Language Models (LLMs). Still, since manual prompt design is often complex, non-intuitive, and time-consuming, automatic prompt optimization has emerged as a research area. However, a significant challenge in prompt optimization is managing the inherent trade-off between task performance, such as accuracy, and context size. Most existing automated methods focus on a single objective, typically performance, thereby failing to explore the critical spectrum of efficiency and effectiveness. This paper introduces the MOPrompt, a novel Multi-objective Evolutionary Optimization (EMO) framework designed to optimize prompts for both accuracy and context size (measured in tokens) simultaneously. Our framework maps the Pareto front of prompt solutions, presenting practitioners with a set of trade-offs between context size and performance, a crucial tool for deploying Large Language Models (LLMs) in real-world applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese, using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that MOPrompt substantially outperforms the baseline framework. For the Sabiazinho model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97) as the best baseline solution, but with a 31% reduction in token length.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01554</link>
<guid>https://arxiv.org/abs/2508.01554</guid>
<content:encoded><![CDATA[
<div> Framework, Adversarial attacks, Language models, PromptAnatomy, ComPerturb  
Summary:  
A new automated framework called PromptAnatomy is introduced, which dissects prompts into functional components to generate diverse and interpretable adversarial examples. The framework utilizes a method called ComPerturb to selectively perturb each component of the prompt. It also incorporates a perplexity-based filtering mechanism to ensure linguistic plausibility and mitigate distribution shifts. The study highlights the importance of understanding prompt structure and controlled perturbation for evaluating the robustness of large language models. Extensive experiments with advanced language models demonstrate that ComPerturb achieves state-of-the-art attack success rates. Annotated public instruction-tuning datasets verified through human review are provided as a complementary resource. Ablation studies confirm the benefits of prompt dissection and perplexity filtering in evaluating adversarial robustness in language models. The code and data for PromptAnatomy are available on GitHub for further exploration. 
<br /><br />Summary: <div>
arXiv:2508.01554v1 Announce Type: new 
Abstract: Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets</title>
<link>https://arxiv.org/abs/2508.01630</link>
<guid>https://arxiv.org/abs/2508.01630</guid>
<content:encoded><![CDATA[
<div> transformer models, NER, domain-adapted, LoRA, OpenMed <br />
Summary: <br />
The article introduces OpenMed NER, a suite of open-source transformer models for named-entity recognition in healthcare data. These models combine domain-adaptive pre-training with Low-Rank Adaptation for efficient performance. OpenMed NER achieves state-of-the-art results on 10 out of 12 biomedical NER benchmarks, including improvements in diverse entity types such as diseases, genes, and species. The models show significant advancements in disease and chemical benchmarks, as well as specialized gene and clinical cell line corpora. The training process is completed in under 12 hours on a single GPU with low carbon footprint, making it efficient and eco-friendly. The open-source checkpoints provided can aid practitioners in complying with data protection and AI regulations like the EU AI Act. <div>
arXiv:2508.01630v1 Announce Type: new 
Abstract: Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Authorship Attribution in Multilingual Machine-Generated Texts</title>
<link>https://arxiv.org/abs/2508.01656</link>
<guid>https://arxiv.org/abs/2508.01656</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, authorship attribution, multilingual, text generation, cross-lingual transferability 

Summary:<br /><br />Large Language Models (LLMs) have advanced to the point where distinguishing between machine-generated text and human-written content is becoming increasingly difficult. This study introduces the concept of Multilingual Authorship Attribution, focusing on attributing texts to human or various LLM generators across 18 languages. The research examines the suitability of monolingual authorship attribution methods in a multilingual context, the transferability of these methods across languages, and the impact of different generators on attribution accuracy. Results indicate that while some monolingual methods can be adapted for multilingual use, challenges persist, especially in transferring across diverse language families. This highlights the complexity of multilingual authorship attribution and the need for more robust approaches to address real-world scenarios. <div>
arXiv:2508.01656v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions</title>
<link>https://arxiv.org/abs/2508.01674</link>
<guid>https://arxiv.org/abs/2508.01674</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, personalization, user preferences, context

Summary:
The article introduces a new benchmark called CUPID, which consists of 756 human-curated interaction session histories between users and LLM-based chat assistants. The benchmark aims to assess the ability of LLMs to infer and apply users' dynamic preferences based on context. The study evaluated 10 LLMs and found that they struggle to accurately infer preferences from multi-turn interactions, achieving less than 50% precision and 65% recall. This highlights the need for advancements in LLM capabilities to enable more contextually personalized interactions. CUPID serves as a resource to drive these improvements and emphasizes the importance of aligning LLM responses with users' changing preferences in different contexts. <br /><br />Summary: <div>
arXiv:2508.01674v1 Announce Type: new 
Abstract: Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Bidirectional Process Reward Model</title>
<link>https://arxiv.org/abs/2508.01682</link>
<guid>https://arxiv.org/abs/2508.01682</guid>
<content:encoded><![CDATA[
<div> Keywords: Process Reward Models, Bidirectional evaluation paradigm, mathematical reasoning benchmarks, stepwise reward evaluation, process-based reward modeling

Summary: 
Process Reward Models (PRMs) aim to enhance the reasoning quality of Large Language Models by assigning scores to intermediate steps. However, existing PRMs have limitations in leveraging global context. In response, a Bidirectional Process Reward Model (BiPRM) is proposed. BiPRM incorporates a bidirectional evaluation paradigm, allowing later reasoning steps to assess earlier ones in real time. The bidirectional evaluation is achieved through prompt modifications without additional parameters or latency. Experimental results on mathematical reasoning benchmarks demonstrate BiPRM's superiority over unidirectional baselines, with a significant improvement in stepwise reward evaluation. The findings indicate BiPRM's effectiveness, robustness, and general applicability in process-based reward modeling, offering a promising new direction for enhancing reasoning quality in language models. 

<br /><br />Summary: <div>
arXiv:2508.01682v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning quality of Large Language Models (LLMs) by assigning fine-grained scores to intermediate reasoning steps within a solution trajectory. However, existing PRMs predominantly adopt a unidirectional left-to-right (L2R) evaluation paradigm, which limits their ability to leverage global context, making it challenging to verify the consistency of earlier steps based on later ones. In light of these challenges, we propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional L2R flow, enabling later reasoning steps to help assess earlier ones in real time. Notably, the built-in R2L evaluation is implemented solely through prompt modifications that reverse the original reasoning trajectory, without any additional parameters or inference latency introduced. This ensures BiPRM remains both efficient and broadly compatible with existing PRM studies. We conduct extensive experiments on two mathematical reasoning benchmarks using samples generated by three different policy models. Our method, BiPRM, is evaluated across three backbones and three distinct PRM objectives. Across all settings, BiPRM consistently outperforms unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation. Generally, our results highlight BiPRM's effectiveness, robustness, and general applicability, offering a promising new direction for process-based reward modeling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</title>
<link>https://arxiv.org/abs/2508.01696</link>
<guid>https://arxiv.org/abs/2508.01696</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, Large Language Models, Knowledge-Intensive Tasks, Collaborative Chain-of-Agents, Multi-Agent Reasoning

Summary:
The paper introduces Collaborative Chain-of-Agents, a framework aimed at improving the integration of parametric and retrieved knowledge in Retrieval-Augmented Generation models. They propose CoCoA-zero, a multi-agent framework for conditional knowledge induction and reasoning. Building on this, they develop CoCoA, a training strategy to enhance the model's ability to combine parametric and retrieved knowledge effectively. Experiments show that CoCoA-zero and CoCoA outperform existing methods on open-domain and multi-hop QA tasks. The new framework enhances the synergy between the model's internal knowledge and external retrieved knowledge, improving the model's performance in knowledge-intensive tasks. 

<br /><br />Summary: <div>
arXiv:2508.01696v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption</title>
<link>https://arxiv.org/abs/2508.01708</link>
<guid>https://arxiv.org/abs/2508.01708</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sentiment leakage, expression leakage, benchmark dataset, automatic evaluation

Summary:
Large language models (LLMs) in natural language processing have advanced skills but can incorporate irrelevant information due to expression leakage, where they generate sentimentally charged expressions unrelated to the input. A benchmark dataset and automatic evaluation pipeline were created to analyze expression leakage, showing that as model size increases, leakage decreases within the same LLM family. Mitigating expression leakage requires careful model building and is not resolved by prompting. Negative sentiment in prompts increases expression leakage more than positive sentiment. This study highlights the need for attention to expression leakage and its impact on LLM performance.<br /><br />Summary: <div>
arXiv:2508.01708v1 Announce Type: new 
Abstract: Large language models (LLMs) have advanced natural language processing (NLP) skills such as through next-token prediction and self-attention, but their ability to integrate broad context also makes them prone to incorporating irrelevant information. Prior work has focused on semantic leakage, bias introduced by semantically irrelevant context. In this paper, we introduce expression leakage, a novel phenomenon where LLMs systematically generate sentimentally charged expressions that are semantically unrelated to the input context. To analyse the expression leakage, we collect a benchmark dataset along with a scheme to automatically generate a dataset from free-form text from common-crawl. In addition, we propose an automatic evaluation pipeline that correlates well with human judgment, which accelerates the benchmarking by decoupling from the need of annotation for each analysed model. Our experiments show that, as the model scales in the parameter space, the expression leakage reduces within the same LLM family. On the other hand, we demonstrate that expression leakage mitigation requires specific care during the model building process, and cannot be mitigated by prompting. In addition, our experiments indicate that, when negative sentiment is injected in the prompt, it disrupts the generation process more than the positive sentiment, causing a higher expression leakage rate.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multilingual, Content Safety, CultureGuard, Safety Guard Models
Summary:
CultureGuard introduces a pipeline for generating culturally aligned safety datasets in multiple languages, expanding the Nemotron-Content-Safety-Dataset-V2 to eight languages. The resulting Nemotron-Content-Safety-Dataset-Multilingual-v1 contains 386,661 samples and is used to train the Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 model, achieving state-of-the-art performance on multilingual safety benchmarks. The study shows that open LLMs are more likely to produce unsafe responses in non-English languages, highlighting the need for culturally aware safety guard models. This work addresses the safety gap in multilingual LLMs and represents a significant step towards enhancing content safety across languages.
<br /><br />Summary: <div>
arXiv:2508.01710v1 Announce Type: new 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction</title>
<link>https://arxiv.org/abs/2508.01739</link>
<guid>https://arxiv.org/abs/2508.01739</guid>
<content:encoded><![CDATA[
<div> keywords: user preferences, dialogue systems, preference extractor, fine-tuning, multi-turn dialogue

Summary: 
- Identifying user preferences in dialogue systems is crucial for providing satisfactory services.
- Using large language models (LLMs) for fine-tuning task-specific preference extractors shows promising results in accuracy and generalization.
- Obtaining high-quality labeled multi-turn dialogue data is a challenge due to the complexity of tracking user preference transitions.
- The proposed framework, IterChat, decomposes multi-turn preference extraction into iterative one-turn processes to reduce annotation errors and improve efficiency.
- By pre-defining preference slots with GPT4 and generating diverse dialogue datasets, IterChat achieves superior performance and annotator efficiency compared to traditional multi-turn dialogues. 

<br /><br />Summary: <div>
arXiv:2508.01739v1 Announce Type: new 
Abstract: Identifying user preferences in dialogue systems is a pivotal aspect of providing satisfying services. Current research shows that using large language models (LLMs) to fine-tune a task-specific preference extractor yields excellent results in terms of accuracy and generalization. However, the primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators (termed \textbf{``Annotating Disaster''}) but also complicates model training due to error propagation in sequential dependency learning. Inspired by the observation that multi-turn preference extraction can be decomposed into iterative executions of one-turn extraction processes. We propose a novel dialogue data generation framework named \textbf{IterChat}. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. This reduces the probability of annotation errors and improves annotation efficiency. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets. Experimental results indicate that fine-tuning or only few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4\% higher than the original multi-turn dialogues.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Generated Text is Non-Stationary: Detection via Temporal Tomography</title>
<link>https://arxiv.org/abs/2508.01754</link>
<guid>https://arxiv.org/abs/2508.01754</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated text detection, Temporal Discrepancy Tomography, non-stationarity, adversarial perturbations, Continuous Wavelet Transform <br />
Summary: <br />
The article discusses the limitations of current AI-generated text detection methods, which fail to account for positional information of anomalies in text. It highlights the significant non-stationarity in AI-generated text and introduces Temporal Discrepancy Tomography (TDT) as a novel detection paradigm that preserves positional information by treating discrepancies as a time-series signal. TDT uses Continuous Wavelet Transform to capture both location and linguistic scale of anomalies, leading to improved detection performance on the RAID benchmark. TDT also demonstrates robustness against adversarial attacks, showcasing a 14.1% improvement in AUROC on Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with minimal computational overhead. The study emphasizes the importance of preserving temporal dynamics in detecting anomalies in AI-generated text. <br /> <div>
arXiv:2508.01754v1 Announce Type: new 
Abstract: The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comprehensive taxonomy of hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01781</link>
<guid>https://arxiv.org/abs/2508.01781</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, hallucination, taxonomy, mitigation strategies, responsible deployment

Summary:
This report discusses the challenges posed by the propensity of Large Language Models (LLMs) to generate factually incorrect or fabricated content, known as hallucinations. It introduces a comprehensive taxonomy of LLM hallucinations, categorizing them based on intrinsic/extrinsic distinctions, factuality/faithfulness, and various manifestations such as factual errors and ethical violations. The report identifies data-related, model-related, and prompt-related factors as underlying causes of hallucinations and explores cognitive and human factors influencing perception. It highlights the need for robust detection, mitigation strategies, and continuous human oversight to ensure responsible and reliable deployment of LLMs in critical applications. The report also discusses evaluation benchmarks, metrics for detection, and introduces web-based resources for monitoring LLM releases and performance. Despite the theoretical inevitability of LLM hallucinations, the report emphasizes the importance of ongoing efforts to address and mitigate these challenges. 

<br /><br />Summary: <div>
arXiv:2508.01781v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark</title>
<link>https://arxiv.org/abs/2508.01812</link>
<guid>https://arxiv.org/abs/2508.01812</guid>
<content:encoded><![CDATA[
<div> semantic understanding, Hebrew, Machine Reading Comprehension, morphologically rich, evaluation metrics  
Summary:  
- The article discusses the lack of semantic benchmarks in Hebrew Natural Language Processing (NLP) and introduces a new Hebrew Machine Reading Comprehension (MRC) dataset, HeQ.  
- The complex morphology of Hebrew presents challenges in MRC tasks, leading to annotation inconsistencies and flaws in evaluation metrics.  
- The authors propose novel guidelines, a crowdsourcing protocol, and revised evaluation metrics tailored to the morphologically rich nature of Hebrew.  
- Standard evaluation metrics like F1 scores and Exact Match are found to be inappropriate for Hebrew, suggesting the need for enhancements in evaluation methods.  
- Models designed for morpho-syntactic tasks may not perform well on semantic-heavy tasks, indicating the importance of addressing both dimensions in NLP models.  
<br /><br /> <div>
arXiv:2508.01812v1 Announce Type: new 
Abstract: Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive Question Answering. The morphologically rich nature of Hebrew poses a challenge to this endeavor: the indeterminacy and non-transparency of span boundaries in morphologically complex forms lead to annotation inconsistencies, disagreements, and flaws in standard evaluation metrics.
  To remedy this, we devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language. Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.
  In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks. The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy</title>
<link>https://arxiv.org/abs/2508.01815</link>
<guid>https://arxiv.org/abs/2508.01815</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, Question Answering, AgenticT2S, Circular Economy, Scalable Reasoning
Summary:
AgenticT2S is a framework designed for Question Answering over heterogeneous knowledge graphs (KGQA) in domains such as the circular economy. It addresses challenges related to diverse schemas, incomplete alignments, and distributed data sources. The framework decomposes KGQA into retrieval, query generation, and verification subtasks managed by specialized agents. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects invalid and underspecified queries through symbolic validation and consistency checks. Experimental results on real-world circular economy KGs show that AgenticT2S significantly improves execution accuracy and triple level F1 over existing baselines while reducing average prompt length. The framework demonstrates the benefits of agent-based schema-aware reasoning for scalable KGQA in sustainability domains, enabling robust cross-graph reasoning for decision-making purposes.<br /><br />Summary: <div>
arXiv:2508.01815v1 Announce Type: new 
Abstract: Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLP Memory: Language Modeling with Retriever-pretrained External Memory</title>
<link>https://arxiv.org/abs/2508.01832</link>
<guid>https://arxiv.org/abs/2508.01832</guid>
<content:encoded><![CDATA[
<div> Keywords: decoder-only LLMs, hallucinations, retriever-augmented generation, external memory, MLP memory

Summary: 

The article introduces a new architecture to address hallucinations in decoder-only LLMs. The proposed model, combining a transformer decoder and an external MLP memory trained to mimic the behavior of a retriever, shows promising results on language modeling and retrieval tasks. The architecture demonstrates improved performance on various datasets, showcasing a significant enhancement in model scalability compared to decoder-only models. The approach effectively handles hallucinations and memory-intensive tasks, outperforming existing methods like $k$NN-LM. Moreover, the external memory leads to improved reasoning capabilities in tasks like StrategyQA. The model not only achieves impressive results but also offers faster inference times, making it a practical solution for knowledge-intensive applications. The code and models will be made available open-source, facilitating further research and applications. 

<br /><br />Summary: <div>
arXiv:2508.01832v1 Announce Type: new 
Abstract: While modern decoder-only LLMs achieve superior performance across various domains, hallucinations have risen to be a common problem in their generated text, hindering their application in knowledge-intensive tasks. Retriever-augmented generation (RAG) offers a solution, but the non-parametric nature of the retriever hinders its deep interaction with LLM. In this work, we propose to decouple memorization from the LLM decoder using a pretrained, differentiable external memory. The external memory is an MLP pretrained by imitating the behavior of a retriever on the entire pretraining dataset. Our resulting architecture, which comprises a transformer decoder and an external MLP memory pretrained on language modeling and retriever imitation respectively, demonstrates strong perplexity and performance on downstream tasks. Experiments show our architecture exhibits steeper power-law scaling with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web datasets compared to decoder-only models while benefiting from added training without overfitting. We demonstrate superior performance on three hallucination benchmarks and nine memory-intensive tasks. Additionally, our approach delivers $80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP memory improves StrategyQA performance. We will open-source our code and models in the future.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents</title>
<link>https://arxiv.org/abs/2508.01858</link>
<guid>https://arxiv.org/abs/2508.01858</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large-scale models, web agents, cognitive reasoning, knowledge acquisition, cognitive processes

Summary:
The paper introduces the Web-CogKnowledge Framework, which divides a web agent's capabilities into knowledge content learning and cognitive processes. Knowledge is categorized as Factual, Conceptual, and Procedural, with learning processes of Memorizing and Understanding and cognitive processes of Exploring. The Web-CogDataset is curated from real-world websites to facilitate knowledge acquisition. A knowledge-driven Chain-of-Thought (CoT) reasoning framework is developed to train the Web-CogReasoner, which outperforms existing models in generalizing to unseen tasks. The Web-CogBench evaluation suite is introduced to assess agent performance. The paper is accompanied by open-sourced code and data. <br /><br />Summary: The paper introduces the Web-CogKnowledge Framework, Web-CogDataset, Chain-of-Thought reasoning framework, Web-CogReasoner, and Web-CogBench evaluation suite to enhance web agents' knowledge acquisition and cognitive reasoning capabilities. <div>
arXiv:2508.01858v1 Announce Type: new 
Abstract: Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.01862</link>
<guid>https://arxiv.org/abs/2508.01862</guid>
<content:encoded><![CDATA[
<div> hallucination, language models, counterfactual probing, detection, mitigation  
Summary:<br />
Large Language Models (LLMs) have shown impressive abilities in various tasks but often produce hallucinated outputs with factual errors. A new approach called Counterfactual Probing is proposed to detect and reduce these hallucinations in LLM outputs. By creating counterfactual statements with subtle factual errors and analyzing the model's response, the method aims to identify inaccuracies in the generated content. This approach, without requiring model retraining, detects hallucinations more effectively than existing methods. Through adaptive mitigation strategies, hallucination scores are reduced by an average of 24.5%, enhancing the reliability of LLM outputs. The technique can be seamlessly integrated into current LLM workflows as a real-time verification tool, ensuring the accuracy and consistency of generated content. <div>
arXiv:2508.01862v1 Announce Type: new 
Abstract: Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language</title>
<link>https://arxiv.org/abs/2508.01918</link>
<guid>https://arxiv.org/abs/2508.01918</guid>
<content:encoded><![CDATA[
<div> Quantum-RAG, Punjabi, large language models, low-resource languages, retrieval-augmented generation

Summary:
PunGPT2 and Pun-RAG are introduced as open-source Punjabi language models trained on a diverse corpus. PunGPT2 captures unique syntactic and morphological features while Pun-RAG improves factual grounding with a dense retriever. Pun-Instruct is a parameter-efficient variant enhancing zero-shot performance. Quantum-RAG combines sparse and dense retrieval methods with quantum-inspired semantic matching for improved contextual relevance. These models outperform multilingual baselines in perplexity, factuality, and fluency. This work provides a blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP. 

Summary: <div>
arXiv:2508.01918v1 Announce Type: new 
Abstract: Despite the rapid advancement of large language models (LLMs), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses sparse (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback</title>
<link>https://arxiv.org/abs/2508.01930</link>
<guid>https://arxiv.org/abs/2508.01930</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Learning from Human Feedback, Lexical preferences, Lexical overuse, Alignment research

Summary:
Large Language Models (LLMs) have been observed to overuse specific terms like "delve" and "intricate," with the reasons behind these lexical choices previously unclear. This study utilizes Meta's Llama model to investigate the role of Learning from Human Feedback (LHF), encompassing techniques like Reinforcement Learning from Human Feedback and Direct Preference Optimization. A method is introduced to detect LLMs' lexical preferences influenced by LHF. Experimentally emulating the LHF process confirms a systematic preference for text variants containing certain words, highlighting a potential misalignment between LHF workers and LLM users' lexical expectations. The research contributes to the field of explainable artificial intelligence, emphasizing the significance of data transparency and procedural clarity in alignment studies. <div>
arXiv:2508.01930v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks</title>
<link>https://arxiv.org/abs/2508.01943</link>
<guid>https://arxiv.org/abs/2508.01943</guid>
<content:encoded><![CDATA[
<div> framework, reasoning, video, trajectory, ROVER
Summary:
ROVER is a framework designed to improve the performance of vision-language models in reasoning over long video sequences in embodied settings. It decomposes video trajectories into segments corresponding to subtasks, allowing for more focused reasoning without losing global context. The framework, implemented using in-context learning, outperforms strong baselines in three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. By reducing the number of frames considered at each timestep, ROVER mitigates hallucinations and improves performance during unexpected or non-optimal moments of a trajectory. It also features a subtask-specific sliding context window, resulting in linear time complexity with video length, which is an improvement over baselines. The evaluation on diverse datasets demonstrates the effectiveness of ROVER in enhancing the capabilities of vision-language models in handling long-horizon video sequences. <div>
arXiv:2508.01943v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension</title>
<link>https://arxiv.org/abs/2508.01959</link>
<guid>https://arxiv.org/abs/2508.01959</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, long documents, contextual information, embedding models, SitEmb

Summary:
In the field of retrieval-augmented generation, handling long documents poses a challenge as splitting them into smaller chunks may lead to loss of contextual information. Existing approaches to encoding longer context windows have limitations in improving retrieval and downstream tasks. To address this, a new approach is introduced where short chunks are represented conditioned on a broader context window to enhance retrieval performance. The proposed SitEmb models are designed to effectively encode situated context, outperforming existing embedding models in a book-plot retrieval dataset. The SitEmb-v1 model based on BGE-M3 demonstrates superior performance with only 1B parameters compared to larger models. The SitEmb-v1.5 model further enhances performance by over 10% across different languages and downstream applications.<br /><br />Summary: The article discusses challenges in retrieval-augmented generation with long documents and proposes a novel approach to enhance retrieval performance by representing short chunks within a broader context. The SitEmb models are introduced to effectively encode situated context, outperforming existing embedding models in a curated benchmark dataset. The results demonstrate significant improvements in retrieval capabilities, even with lower parameter counts, across various languages and downstream applications. <div>
arXiv:2508.01959v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.
  We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2508.01977</link>
<guid>https://arxiv.org/abs/2508.01977</guid>
<content:encoded><![CDATA[
arXiv:2508.01977v1 Announce Type: new 
Abstract: To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextually Aware E-Commerce Product Question Answering using RAG</title>
<link>https://arxiv.org/abs/2508.01990</link>
<guid>https://arxiv.org/abs/2508.01990</guid>
<content:encoded><![CDATA[
arXiv:2508.01990v1 Announce Type: new 
Abstract: E-commerce product pages contain a mix of structured specifications, unstructured reviews, and contextual elements like personalized offers or regional variants. Although informative, this volume can lead to cognitive overload, making it difficult for users to quickly and accurately find the information they need. Existing Product Question Answering (PQA) systems often fail to utilize rich user context and diverse product information effectively. We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval Augmented Generation (RAG) that deeply integrates contextual understanding. Our system leverages conversational history, user profiles, and product attributes to deliver relevant and personalized answers. It adeptly handles objective, subjective, and multi-intent queries across heterogeneous sources, while also identifying information gaps in the catalog to support ongoing content improvement. We also introduce novel metrics to measure the framework's performance which are broadly applicable for RAG system evaluations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Large Language Models to Detect Dementia Family Caregivers</title>
<link>https://arxiv.org/abs/2508.01999</link>
<guid>https://arxiv.org/abs/2508.01999</guid>
<content:encoded><![CDATA[
arXiv:2508.01999v1 Announce Type: new 
Abstract: Social media, such as Twitter, provides opportunities for caregivers of dementia patients to share their experiences and seek support for a variety of reasons. Availability of this information online also paves the way for the development of internet-based interventions in their support. However, for this purpose, tweets written by caregivers of dementia patients must first be identified. This paper demonstrates our system for the SMM4H 2025 shared task 3, which focuses on detecting tweets posted by individuals who have a family member with dementia. The task is outlined as a binary classification problem, differentiating between tweets that mention dementia in the context of a family member and those that do not. Our solution to this problem explores large language models (LLMs) with various prompting methods. Our results show that a simple zero-shot prompt on a fine-tuned model yielded the best results. Our final system achieved a macro F1-score of 0.95 on the validation set and the test set. Our full code is available on GitHub.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents</title>
<link>https://arxiv.org/abs/2508.02013</link>
<guid>https://arxiv.org/abs/2508.02013</guid>
<content:encoded><![CDATA[
arXiv:2508.02013v1 Announce Type: new 
Abstract: Recently, role-playing agents have emerged as a promising paradigm for achieving personalized interaction and emotional resonance. Existing research primarily focuses on the textual modality, neglecting the critical dimension of speech in realistic interactive scenarios. In particular, there is a lack of systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that comprises 98 diverse roles and 112k speech-based single-turn and multi-turn conversations. Each role demonstrates distinct vocal characteristics, including timbre and prosody, thereby enabling more sophisticated speech role-playing. Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation benchmark that systematically assesses SRPAs performance in key aspects such as fundamental interaction ability, speech expressiveness, and role-playing fidelity. Experimental results reveal the advantages and challenges of both cascaded and end-to-end speech role-playing agents in maintaining vocal style consistency and role coherence. We release all data, code, and baseline models to provide a solid foundation for speech-driven multimodal role-playing research and to foster further developments in this field.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2508.02018</link>
<guid>https://arxiv.org/abs/2508.02018</guid>
<content:encoded><![CDATA[
arXiv:2508.02018v1 Announce Type: new 
Abstract: Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time</title>
<link>https://arxiv.org/abs/2508.02037</link>
<guid>https://arxiv.org/abs/2508.02037</guid>
<content:encoded><![CDATA[
arXiv:2508.02037v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-Voice Technical Report</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
arXiv:2508.02038v1 Announce Type: new 
Abstract: This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02045</link>
<guid>https://arxiv.org/abs/2508.02045</guid>
<content:encoded><![CDATA[
arXiv:2508.02045v1 Announce Type: new 
Abstract: Facts evolve over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. While factual Time-Sensitive Question-Answering (TSQA) tasks have been widely studied, existing benchmarks often rely on manual curation or a small, fixed set of predefined templates, which restricts scalable and comprehensive TSQA evaluation. To address these challenges, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques such as temporal SQL and functional dependencies. We also introduce a fine-grained evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy to enable a more reliable TSQA evaluation. Extensive experiments on contemporary LLMs show how \ours{} enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by enabling LLM evaluation on application-specific data and seamless multi-hop question generation. Code and data are publicly available at: https://github.com/ssoy0701/tdbench.git.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProCut: LLM Prompt Compression via Attribution Estimation</title>
<link>https://arxiv.org/abs/2508.02053</link>
<guid>https://arxiv.org/abs/2508.02053</guid>
<content:encoded><![CDATA[
arXiv:2508.02053v1 Announce Type: new 
Abstract: In large-scale industrial LLM systems, prompt templates often expand to thousands of tokens as teams iteratively incorporate sections such as task instructions, few-shot examples, and heuristic rules to enhance robustness and coverage. This expansion leads to bloated prompts that are difficult to maintain and incur significant inference latency and serving costs. To address this, we introduce Prompt Compression via Attribution Estimation (ProCut), a flexible, LLM-agnostic, training-free framework that compresses prompts through attribution analysis. ProCut segments prompt templates into semantically meaningful units, quantifies their impact on task performance, and prunes low-utility components. Through extensive experiments on five public benchmark datasets and real-world industrial prompts, we show that ProCut achieves substantial prompt size reductions (78% fewer tokens in production) while maintaining or even slightly improving task performance (up to 62% better than alternative methods). We further introduce an LLM-driven attribution estimator that reduces compression latency by over 50%, and demonstrate that ProCut integrates seamlessly with existing prompt-optimization frameworks to produce concise, high-performing prompts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The SMeL Test: A simple benchmark for media literacy in language models</title>
<link>https://arxiv.org/abs/2508.02074</link>
<guid>https://arxiv.org/abs/2508.02074</guid>
<content:encoded><![CDATA[
arXiv:2508.02074v1 Announce Type: new 
Abstract: The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02087</link>
<guid>https://arxiv.org/abs/2508.02087</guid>
<content:encoded><![CDATA[
arXiv:2508.02087v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth</title>
<link>https://arxiv.org/abs/2508.02094</link>
<guid>https://arxiv.org/abs/2508.02094</guid>
<content:encoded><![CDATA[
arXiv:2508.02094v1 Announce Type: new 
Abstract: Risk perception is subjective, and youth's understanding of toxic content differs from that of adults. Although previous research has conducted extensive studies on toxicity detection in social media, the investigation of youth's unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as youth, is ignored. To address this gap, we aim to explore: 1) What are the features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing toxicity detection techniques accurately detect these languages (RQ2). For these questions, we took Chinese youth as the research target, constructed the first Chinese ``youth-toxicity'' dataset, and then conducted extensive analysis. Our results suggest that youth's perception of these is associated with several contextual factors, like the source of an utterance and text-related features. Incorporating these meta information into current toxicity detection methods significantly improves accuracy overall. Finally, we propose several insights into future research on youth-centered toxicity detection.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Dynamics of Meta-Learning in Small Model Pretraining</title>
<link>https://arxiv.org/abs/2508.02189</link>
<guid>https://arxiv.org/abs/2508.02189</guid>
<content:encoded><![CDATA[
arXiv:2508.02189v1 Announce Type: new 
Abstract: Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference</title>
<link>https://arxiv.org/abs/2508.02193</link>
<guid>https://arxiv.org/abs/2508.02193</guid>
<content:encoded><![CDATA[
arXiv:2508.02193v1 Announce Type: new 
Abstract: We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems</title>
<link>https://arxiv.org/abs/2508.02208</link>
<guid>https://arxiv.org/abs/2508.02208</guid>
<content:encoded><![CDATA[
arXiv:2508.02208v1 Announce Type: new 
Abstract: Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry--a frontier domain of modern mathematics--comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Isolating Culture Neurons in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2508.02241</link>
<guid>https://arxiv.org/abs/2508.02241</guid>
<content:encoded><![CDATA[
arXiv:2508.02241v1 Announce Type: new 
Abstract: Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders</title>
<link>https://arxiv.org/abs/2508.02256</link>
<guid>https://arxiv.org/abs/2508.02256</guid>
<content:encoded><![CDATA[
arXiv:2508.02256v1 Announce Type: new 
Abstract: In this paper, we present a comprehensive study of language interference in encoder-only Transformer models across 83 languages. We construct an interference matrix by training and evaluating small BERT-like models on all possible language pairs, providing a large-scale quantification of cross-lingual interference. Our analysis reveals that interference between languages is asymmetrical and that its patterns do not align with traditional linguistic characteristics, such as language family, nor with proxies like embedding similarity, but instead better relate to script. Finally, we demonstrate that the interference matrix effectively predicts performance on downstream tasks, serving as a tool to better design multilingual models to obtain optimal performance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.02260</link>
<guid>https://arxiv.org/abs/2508.02260</guid>
<content:encoded><![CDATA[
arXiv:2508.02260v1 Announce Type: new 
Abstract: Recently, reinforcement learning with verifiable rewards (RLVR) has been widely used for enhancing the reasoning abilities of large language models (LLMs). A core challenge in RLVR involves managing the exchange between entropy and performance of policies. Despite the importance of this exchange, a fine-grained understanding of when and how this exchange operates most effectively remains limited. To bridge this gap, we conduct a systematic empirical analysis of the entropy-performance exchange mechanism of RLVR across different levels of granularity. Specifically, we first divide the training process into two distinct stages based on entropy dynamics, i.e., rising stage and plateau stage, and then systematically investigate how this mechanism varies across stage-level, instance-level, and token-level granularitiess. Our analysis reveals that, in the rising stage, entropy reduction in negative samples facilitates the learning of effective reasoning patterns, which in turn drives rapid performance gains. Moreover, in the plateau stage, learning efficiency strongly correlates with high-entropy tokens present in low-perplexity samples and those located at the end of sequences. Motivated by these findings, we propose two methods that dynamically adjust the reward signal using perplexity and positional information to focus RL updates on tokens that exhibit high learning potential, achieving improvements compared to the baseline methods on various LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System</title>
<link>https://arxiv.org/abs/2508.02268</link>
<guid>https://arxiv.org/abs/2508.02268</guid>
<content:encoded><![CDATA[
arXiv:2508.02268v1 Announce Type: new 
Abstract: The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of \textbf{4.01 out of 5.0} when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynaword: From One-shot to Continuously Developed Datasets</title>
<link>https://arxiv.org/abs/2508.02271</link>
<guid>https://arxiv.org/abs/2508.02271</guid>
<content:encoded><![CDATA[
arXiv:2508.02271v1 Announce Type: new 
Abstract: Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A French Version of the OLDI Seed Corpus</title>
<link>https://arxiv.org/abs/2508.02290</link>
<guid>https://arxiv.org/abs/2508.02290</guid>
<content:encoded><![CDATA[
arXiv:2508.02290v1 Announce Type: new 
Abstract: We present the first French partition of the OLDI Seed Corpus, our submission to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its creation process, which involved using multiple machine translation systems and a custom-built interface for post-editing by qualified native speakers. We also highlight the unique translation challenges presented by the source data, which combines highly technical, encyclopedic terminology with the stylistic irregularities characteristic of user-generated content taken from Wikipedia. This French corpus is not an end in itself, but is intended as a crucial pivot resource to facilitate the collection of parallel corpora for the under-resourced regional languages of France.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Methods Defend RAG Systems Well Against Real-World Attacks</title>
<link>https://arxiv.org/abs/2508.02296</link>
<guid>https://arxiv.org/abs/2508.02296</guid>
<content:encoded><![CDATA[
arXiv:2508.02296v1 Announce Type: new 
Abstract: Ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems is paramount in safety-critical applications, yet remains a significant challenge. To address this, we evaluate four methodologies for Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG system only responds to queries confined to the system's knowledge base. Specifically, our evaluation explores two novel dimensionality reduction and feature separation strategies: \textit{PCA}, where top components are selected using explained variance or OOD separability, and an adaptation of \textit{Neural Collapse Feature Separation}. We validate our approach on standard datasets (StackExchange and MSMARCO) and real-world applications (Substance Use and COVID-19), including tests against LLM-simulated and actual attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations of response correctness and relevance, we confirm that an external OOD detector is crucial for maintaining response relevance.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training</title>
<link>https://arxiv.org/abs/2508.02308</link>
<guid>https://arxiv.org/abs/2508.02308</guid>
<content:encoded><![CDATA[
arXiv:2508.02308v1 Announce Type: new 
Abstract: Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at https://github.com/scar-on/LaMPE.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo</title>
<link>https://arxiv.org/abs/2508.02317</link>
<guid>https://arxiv.org/abs/2508.02317</guid>
<content:encoded><![CDATA[
arXiv:2508.02317v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. %
We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. %
Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis</title>
<link>https://arxiv.org/abs/2508.02322</link>
<guid>https://arxiv.org/abs/2508.02322</guid>
<content:encoded><![CDATA[
arXiv:2508.02322v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.02360</link>
<guid>https://arxiv.org/abs/2508.02360</guid>
<content:encoded><![CDATA[
arXiv:2508.02360v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation</title>
<link>https://arxiv.org/abs/2508.02401</link>
<guid>https://arxiv.org/abs/2508.02401</guid>
<content:encoded><![CDATA[
arXiv:2508.02401v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding</title>
<link>https://arxiv.org/abs/2508.02426</link>
<guid>https://arxiv.org/abs/2508.02426</guid>
<content:encoded><![CDATA[
arXiv:2508.02426v1 Announce Type: new 
Abstract: Since knowledge graphs (KG) will continue to evolve in real scenarios, traditional KGE models are only suitable for static knowledge graphs. Therefore, continual knowledge graph embedding (CKGE) has attracted the attention of researchers. Currently, a key challenge facing CKGE is that the model is prone to "catastrophic forgetting", resulting in the loss of previously learned knowledge. In order to effectively alleviate this problem, we propose a new CKGE model BAKE. First, we note that the Bayesian posterior update principle provides a natural continual learning strategy that is insensitive to data order and can theoretically effectively resist the forgetting of previous knowledge during data evolution. Different from the existing CKGE method, BAKE regards each batch of new data as a Bayesian update of the model prior. Under this framework, as long as the posterior distribution of the model is maintained, the model can better preserve the knowledge of early snapshots even after evolving through multiple time snapshots. Secondly, we propose a continual clustering method for CKGE, which further directly combats knowledge forgetting by constraining the evolution difference (or change amplitude) between new and old knowledge between different snapshots. We conduct extensive experiments on BAKE on multiple datasets, and the results show that BAKE significantly outperforms existing baseline models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications</title>
<link>https://arxiv.org/abs/2508.02430</link>
<guid>https://arxiv.org/abs/2508.02430</guid>
<content:encoded><![CDATA[
arXiv:2508.02430v1 Announce Type: new 
Abstract: Measuring innovation often relies on context-specific proxies and on expert evaluation. Hence, empirical innovation research is often limited to settings where such data is available. We investigate how large language models (LLMs) can be leveraged to overcome the constraints of manual expert evaluations and assist researchers in measuring innovation. We design an LLM framework that reliably approximates domain experts' assessment of innovation from unstructured text data. We demonstrate the performance and broad applicability of this framework through two studies in different contexts: (1) the innovativeness of software application updates and (2) the originality of user-generated feedback and improvement ideas in product reviews. We compared the performance (F1-score) and reliability (consistency rate) of our LLM framework against alternative measures used in prior innovation studies, and to state-of-the-art machine learning- and deep learning-based models. The LLM framework achieved higher F1-scores than the other approaches, and its results are highly consistent (i.e., results do not change across runs). This article equips R&amp;D personnel in firms, as well as researchers, reviewers, and editors, with the knowledge and tools to effectively use LLMs for measuring innovation and evaluating the performance of LLM-based innovation measures. In doing so, we discuss, the impact of important design decisions-including model selection, prompt engineering, training data size, training data distribution, and parameter settings-on performance and reliability. Given the challenges inherent in using human expert evaluation and existing text-based measures, our framework has important implications for harnessing LLMs as reliable, increasingly accessible, and broadly applicable research tools for measuring innovation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentPrompt: Optimizing Promts in Latent Space</title>
<link>https://arxiv.org/abs/2508.02452</link>
<guid>https://arxiv.org/abs/2508.02452</guid>
<content:encoded><![CDATA[
arXiv:2508.02452v1 Announce Type: new 
Abstract: Recent advances have shown that optimizing prompts for Large Language Models (LLMs) can significantly improve task performance, yet many optimization techniques rely on heuristics or manual exploration. We present LatentPrompt, a model-agnostic framework for prompt optimization that leverages latent semantic space to automatically generate, evaluate, and refine candidate prompts without requiring hand-crafted rules. Beginning with a set of seed prompts, our method embeds them in a continuous latent space and systematically explores this space to identify prompts that maximize task-specific performance. In a proof-of-concept study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt increased classification accuracy by approximately 3 percent after a single optimization cycle. The framework is broadly applicable, requiring only black-box access to an LLM and an automatic evaluation metric, making it suitable for diverse domains and tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity</title>
<link>https://arxiv.org/abs/2508.02498</link>
<guid>https://arxiv.org/abs/2508.02498</guid>
<content:encoded><![CDATA[
arXiv:2508.02498v1 Announce Type: new 
Abstract: This study investigates how Facebook shaped collective identity during the July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising. During government repression, protesters turned to Facebook as a central space for resistance, where multimodal expressions, images, memes, videos, hashtags, and satirical posts played an important role in unifying participants. Using a qualitative approach, this research analyzes visual rhetoric, verbal discourse, and digital irony to reveal how shared symbols, protest art, and slogans built a sense of solidarity. Key elements included the symbolic use of red, the ironic metaphorical use of the term "Razakar", and the widespread sharing of visuals representing courage, injustice, and resistance. The findings show that the combination of visual and verbal strategies on Facebook not only mobilized public sentiment, but also built a strong collective identity that challenged authoritarian narratives. This study tries to demonstrate how online platforms can serve as powerful tools for identity construction and political mobilization in the digital age.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks</title>
<link>https://arxiv.org/abs/2508.02502</link>
<guid>https://arxiv.org/abs/2508.02502</guid>
<content:encoded><![CDATA[
arXiv:2508.02502v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong linguistic capabilities, but little is known about how they encode psycholinguistic knowledge across languages. We investigate whether and how LLMs exhibit human-like psycholinguistic responses under different linguistic identities using two tasks: sound symbolism and word valence. We evaluate two models, Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models adjust their outputs based on prompted language identity, with Qwen showing greater sensitivity and sharper distinctions between Dutch and Chinese. Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers, with Chinese prompts yielding stronger and more stable valence representations than Dutch. Our results demonstrate that language identity conditions both output behavior and internal representations in LLMs, providing new insights into their application as models of cross-linguistic cognition.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modular Arithmetic: Language Models Solve Math Digit by Digit</title>
<link>https://arxiv.org/abs/2508.02513</link>
<guid>https://arxiv.org/abs/2508.02513</guid>
<content:encoded><![CDATA[
arXiv:2508.02513v1 Announce Type: new 
Abstract: While recent work has begun to uncover the internal strategies that Large Language Models (LLMs) employ for simple arithmetic tasks, a unified understanding of their underlying mechanisms is still lacking. We extend recent findings showing that LLMs represent numbers in a digit-wise manner and present evidence for the existence of digit-position-specific circuits that LLMs use to perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that operate independently on different digit positions (units, tens, hundreds). Notably, such circuits exist independently of model size and of tokenization strategy, i.e. both for models that encode longer numbers digit-by-digit and as one token. Using Feature Importance and Causal Interventions, we identify and validate the digit-position-specific circuits, revealing a compositional and interpretable structure underlying the solving of arithmetic problems in LLMs. Our interventions selectively alter the model's prediction at targeted digit positions, demonstrating the causal role of digit-position circuits in solving arithmetic tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs</title>
<link>https://arxiv.org/abs/2508.02515</link>
<guid>https://arxiv.org/abs/2508.02515</guid>
<content:encoded><![CDATA[
arXiv:2508.02515v1 Announce Type: new 
Abstract: This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across four families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a reward signal, we fine-tune three lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2</title>
<link>https://arxiv.org/abs/2508.02527</link>
<guid>https://arxiv.org/abs/2508.02527</guid>
<content:encoded><![CDATA[
arXiv:2508.02527v1 Announce Type: new 
Abstract: Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding. In this work, we investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic information. Our results suggest that Llama uses a rich internal model of phonemes to complete phonetic tasks. We provide evidence for high-level organization of phoneme representations in its latent space. In doing so, we also identify a ``phoneme mover head" which promotes phonetic information during rhyming tasks. We visualize the output space of this head and find that, while notable differences exist, Llama learns a model of vowels similar to the standard IPA vowel chart for humans, despite receiving no direct supervision to do so.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction</title>
<link>https://arxiv.org/abs/2508.02532</link>
<guid>https://arxiv.org/abs/2508.02532</guid>
<content:encoded><![CDATA[
arXiv:2508.02532v1 Announce Type: new 
Abstract: Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)</title>
<link>https://arxiv.org/abs/2508.02540</link>
<guid>https://arxiv.org/abs/2508.02540</guid>
<content:encoded><![CDATA[
arXiv:2508.02540v1 Announce Type: new 
Abstract: In a world overwhelmed with news, determining which information comes from reliable sources or how neutral is the reported information in the news articles poses a challenge to news readers. In this paper, we propose a methodology for automatically identifying bias by commission, omission, and source selection (COSS) as a joint three-fold objective, as opposed to the previous work separately addressing these types of bias. In a pipeline concept, we describe the goals and tasks of its steps toward bias identification and provide an example of a visualization that leverages the extracted features and patterns of text reuse.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building and Aligning Comparable Corpora</title>
<link>https://arxiv.org/abs/2508.02555</link>
<guid>https://arxiv.org/abs/2508.02555</guid>
<content:encoded><![CDATA[
arXiv:2508.02555v1 Announce Type: new 
Abstract: Comparable corpus is a set of topic aligned documents in multiple languages, which are not necessarily translations of each other. These documents are useful for multilingual natural language processing when there is no parallel text available in some domains or languages. In addition, comparable documents are informative because they can tell what is being said about a topic in different languages. In this paper, we present a method to build comparable corpora from Wikipedia encyclopedia and EURONEWS website in English, French and Arabic languages. We further experiment a method to automatically align comparable documents using cross-lingual similarity measures. We investigate two cross-lingual similarity measures to align comparable documents. The first measure is based on bilingual dictionary, and the second measure is based on Latent Semantic Indexing (LSI). Experiments on several corpora show that the Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure. Finally, we collect English and Arabic news documents from the British Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively. Then we use the CL-LSI similarity measure to automatically align comparable documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is not only able to align cross-lingual documents at the topic level, but also it is able to do this at the event level.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks</title>
<link>https://arxiv.org/abs/2508.02556</link>
<guid>https://arxiv.org/abs/2508.02556</guid>
<content:encoded><![CDATA[
arXiv:2508.02556v1 Announce Type: new 
Abstract: Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into overlapping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than transformer-based models, making them well-suited for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction</title>
<link>https://arxiv.org/abs/2508.02558</link>
<guid>https://arxiv.org/abs/2508.02558</guid>
<content:encoded><![CDATA[
arXiv:2508.02558v1 Announce Type: new 
Abstract: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs</title>
<link>https://arxiv.org/abs/2508.02573</link>
<guid>https://arxiv.org/abs/2508.02573</guid>
<content:encoded><![CDATA[
arXiv:2508.02573v1 Announce Type: new 
Abstract: Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.
  We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare</title>
<link>https://arxiv.org/abs/2508.02574</link>
<guid>https://arxiv.org/abs/2508.02574</guid>
<content:encoded><![CDATA[
arXiv:2508.02574v1 Announce Type: new 
Abstract: Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification</title>
<link>https://arxiv.org/abs/2508.02584</link>
<guid>https://arxiv.org/abs/2508.02584</guid>
<content:encoded><![CDATA[
arXiv:2508.02584v1 Announce Type: new 
Abstract: Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CharBench: Evaluating the Role of Tokenization in Character-Level Tasks</title>
<link>https://arxiv.org/abs/2508.02591</link>
<guid>https://arxiv.org/abs/2508.02591</guid>
<content:encoded><![CDATA[
arXiv:2508.02591v1 Announce Type: new 
Abstract: Tasks that require character-level reasoning, such as counting or locating characters within words, remain challenging for contemporary language models. A common conjecture is that language models' reliance on subword units, rather than characters, contributes to their struggles with character-level tasks, yet recent studies offer conflicting conclusions about the role of tokenization, leaving its impact unclear. To address this gap, we introduce CharBench, a comprehensive benchmark of character-level tasks that is two orders of magnitude larger than existing alternatives. We evaluate a diverse range of leading open-weight and proprietary models on CharBench and find that it presents a significant challenge to modern LLMs, with an average accuracy of 43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic properties of words and their segmentations into tokens correspond to model performance. For counting tasks, we find that tokenization properties are weakly correlated with correctness, while the length of the queried word and the actual character count play a more significant part. In contrast, for tasks requiring intra-word positional understanding, performance is negatively correlated with the length of the token containing the queried character, suggesting that longer tokens obscure character position information for LLMs. We encourage future work to build on the benchmark and evaluation methodology introduced here as tools for improving model performance on such tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation</title>
<link>https://arxiv.org/abs/2508.02618</link>
<guid>https://arxiv.org/abs/2508.02618</guid>
<content:encoded><![CDATA[
arXiv:2508.02618v1 Announce Type: new 
Abstract: The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this "attention hacking", we propose "Interaction Distillation", a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointer: Linear-Complexity Long-Range Modeling without Pre-training</title>
<link>https://arxiv.org/abs/2508.02631</link>
<guid>https://arxiv.org/abs/2508.02631</guid>
<content:encoded><![CDATA[
arXiv:2508.02631v1 Announce Type: new 
Abstract: We introduce Pointer, a novel architecture that achieves linear $O(NK)$ complexity for long-range sequence modeling while maintaining superior performance without requiring pre-training. Unlike standard attention mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses layer-wise pointer chaining where each layer's pointer selection depends on previous layer's pointer positions, creating explicit long-distance connections through pointer chains. We demonstrate that this architecture achieves $2$--$10\times$ speedup on long sequences compared to standard transformers, maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and learns interpretable pointer patterns that reveal structured dependency modeling. Our experiments on efficiency benchmarks, long-range dependency tasks, and interpretability analysis show that Pointer offers a compelling alternative to attention mechanisms for scenarios requiring efficient long-range modeling without pre-training dependencies.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test Set Quality in Multilingual LLM Evaluation</title>
<link>https://arxiv.org/abs/2508.02635</link>
<guid>https://arxiv.org/abs/2508.02635</guid>
<content:encoded><![CDATA[
arXiv:2508.02635v1 Announce Type: new 
Abstract: Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Phenotyping of Pediatric Sepsis Cohort Using Large Language Models</title>
<link>https://arxiv.org/abs/2505.09805</link>
<guid>https://arxiv.org/abs/2505.09805</guid>
<content:encoded><![CDATA[
arXiv:2505.09805v1 Announce Type: cross 
Abstract: Clustering patient subgroups is essential for personalized care and efficient resource use. Traditional clustering methods struggle with high-dimensional, heterogeneous healthcare data and lack contextual understanding. This study evaluates Large Language Model (LLM) based clustering against classical methods using a pediatric sepsis dataset from a low-income country (LIC), containing 2,686 records with 28 numerical and 119 categorical variables. Patient records were serialized into text with and without a clustering objective. Embeddings were generated using quantized LLAMA 3.1 8B, DeepSeek-R1-Distill-Llama-8B with low-rank adaptation(LoRA), and Stella-En-400M-V5 models. K-means clustering was applied to these embeddings. Classical comparisons included K-Medoids clustering on UMAP and FAMD-reduced mixed data. Silhouette scores and statistical tests evaluated cluster quality and distinctiveness. Stella-En-400M-V5 achieved the highest Silhouette Score (0.86). LLAMA 3.1 8B with the clustering objective performed better with higher number of clusters, identifying subgroups with distinct nutritional, clinical, and socioeconomic profiles. LLM-based methods outperformed classical techniques by capturing richer context and prioritizing key features. These results highlight potential of LLMs for contextual phenotyping and informed decision-making in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Attribution Crisis in LLM Search Results</title>
<link>https://arxiv.org/abs/2508.00838</link>
<guid>https://arxiv.org/abs/2508.00838</guid>
<content:encoded><![CDATA[
arXiv:2508.00838v1 Announce Type: cross 
Abstract: Web-enabled LLMs frequently answer queries without crediting the web pages they consume, creating an "attribution gap" - the difference between relevant URLs read and those actually cited. Drawing on approximately 14,000 real-world LMArena conversation logs with search-enabled LLM systems, we document three exploitation patterns: 1) No Search: 34% of Google Gemini and 24% of OpenAI GPT-4o responses are generated without explicitly fetching any online content; 2) No citation: Gemini provides no clickable citation source in 92% of answers; 3) High-volume, low-credit: Perplexity's Sonar visits approximately 10 relevant pages per query but cites only three to four. A negative binomial hurdle model shows that the average query answered by Gemini or Sonar leaves about 3 relevant websites uncited, whereas GPT-4o's tiny uncited gap is best explained by its selective log disclosures rather than by better attribution. Citation efficiency - extra citations provided per additional relevant web page visited - varies widely across models, from 0.19 to 0.45 on identical queries, underscoring that retrieval design, not technical limits, shapes ecosystem impact. We recommend a transparent LLM search architecture based on standardized telemetry and full disclosure of search traces and citation logs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models</title>
<link>https://arxiv.org/abs/2508.00881</link>
<guid>https://arxiv.org/abs/2508.00881</guid>
<content:encoded><![CDATA[
arXiv:2508.00881v1 Announce Type: cross 
Abstract: Foundation models for natural language processing have many coherent definitions of hallucination and methods for its detection and mitigation. However, analogous definitions and methods do not exist for multi-variate time-series (MVTS) foundation models. We propose new definitions for MVTS hallucination, along with new detection and mitigation methods using a diffusion model to estimate hallucination levels. We derive relational datasets from popular time-series datasets to benchmark these relational hallucination levels. Using these definitions and models, we find that open-source pre-trained MVTS imputation foundation models relationally hallucinate on average up to 59.5% as much as a weak baseline. The proposed mitigation method reduces this by up to 47.7% for these models. The definition and methods may improve adoption and safe usage of MVTS foundation models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks</title>
<link>https://arxiv.org/abs/2508.00890</link>
<guid>https://arxiv.org/abs/2508.00890</guid>
<content:encoded><![CDATA[
arXiv:2508.00890v1 Announce Type: cross 
Abstract: Test-time scaling (TTS) enhances the performance of large language models (LLMs) by allocating additional compute resources during inference. However, existing research primarily investigates TTS in single-stage tasks; while many real-world problems are multi-stage complex tasks, composed of a sequence of heterogeneous subtasks with each subtask requires LLM of specific capability. Therefore, we study a novel problem: the test-time compute-optimal scaling in multi-stage complex tasks, aiming to select suitable models and allocate budgets per subtask to maximize overall performance. TTS in multi-stage tasks introduces two fundamental challenges: (i) The combinatorial search space of model and budget allocations, combined with the high cost of inference, makes brute-force search impractical. (ii) The optimal model and budget allocations across subtasks are interdependent, increasing the complexity of the compute-optimal search. To address this gap, we conduct extensive pilot experiments on four tasks across six datasets, deriving three empirical insights characterizing the behavior of LLMs in multi-stage complex tasks. Informed by these insights, we propose AgentTTS, an LLM-agent-based framework that autonomously searches for compute-optimal allocations through iterative feedback-driven interactions with the execution environment. Experimental results demonstrate that AgentTTS significantly outperforms traditional and other LLM-based baselines in search efficiency, and shows improved robustness to varying training set sizes and enhanced interpretability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge</title>
<link>https://arxiv.org/abs/2508.00901</link>
<guid>https://arxiv.org/abs/2508.00901</guid>
<content:encoded><![CDATA[
arXiv:2508.00901v1 Announce Type: cross 
Abstract: Modern large language models excel in knowledge-intensive tasks, yet how transformers acquire (store) knowledge during pre-training and extract (retrieve) it during post-fine-tuning inference remains theoretically opaque. While prior theoretical work has begun to investigate these questions through the analysis of training dynamics, such studies are limited to single-layer, attention-only architectures. However, most existing studies suggest that MLPs are the most contributing components for storing knowledge in transformer-based language models. Meanwhile, our empirical investigations reveal that such simplified models, when trained using standard next-token prediction objectives, may be incapable of acquiring or extracting factual knowledge. To overcome this limitation, we introduce a tractable one-layer transformer framework that crucially incorporates both self-attention and MLP modules. By tracking its gradient dynamics, we establish convergence and generalization guarantees that illuminate the ability of knowledge acquisition and extraction. We prove that 1) Transformers can achieve near-optimal training loss during pre-training, signifying effective knowledge acquisition; 2) With a large fine-tuning dataset and specific data multiplicity conditions met, transformers can achieve low generalization error when tested on factual knowledge learned during pre-training but not reinforced during the fine-tuning, indicating successful knowledge extraction; 3) When the conditions are not satisfied, transformers exhibit high generalization loss, resulting in hallucinations. Our analysis includes both full fine-tuning and low-rank fine-tuning. Furthermore, our analysis offers theoretical insights into several pertinent empirical phenomena, such as the role of learning rate schedules. Experiments on synthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate our results.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00902</link>
<guid>https://arxiv.org/abs/2508.00902</guid>
<content:encoded><![CDATA[
arXiv:2508.00902v1 Announce Type: cross 
Abstract: Judgment of risk is key to decision-making under uncertainty. As Daniel Kahneman and Amos Tversky famously discovered, humans do so in a distinctive way that departs from mathematical rationalism. Specifically, they demonstrated experimentally that humans accept more risk when they feel themselves at risk of losing something than when they might gain. I report the first tests of Kahneman and Tversky's landmark 'prospect theory' with Large Language Models, including today's state of the art chain-of-thought 'reasoners'.
  In common with humans, I find that prospect theory often anticipates how these models approach risky decisions across a range of scenarios. I also demonstrate that context is key to explaining much of the variance in risk appetite. The 'frame' through which risk is apprehended appears to be embedded within the language of the scenarios tackled by the models. Specifically, I find that military scenarios generate far larger 'framing effects' than do civilian settings, ceteris paribus. My research suggests, therefore, that language models the world, capturing our human heuristics and biases. But also that these biases are uneven - the idea of a 'frame' is richer than simple gains and losses. Wittgenstein's notion of 'language games' explains the contingent, localised biases activated by these scenarios. Finally, I use my findings to reframe the ongoing debate about reasoning and memorisation in LLMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyber-Zero: Training Cybersecurity Agents without Runtime</title>
<link>https://arxiv.org/abs/2508.00910</link>
<guid>https://arxiv.org/abs/2508.00910</guid>
<content:encoded><![CDATA[
arXiv:2508.00910v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small sample-based adaptive text classification through iterative and contrastive description refinement</title>
<link>https://arxiv.org/abs/2508.00957</link>
<guid>https://arxiv.org/abs/2508.00957</guid>
<content:encoded><![CDATA[
arXiv:2508.00957v1 Announce Type: cross 
Abstract: Zero-shot text classification remains a difficult task in domains with evolving knowledge and ambiguous category boundaries, such as ticketing systems. Large language models (LLMs) often struggle to generalize in these scenarios due to limited topic separability, while few-shot methods are constrained by insufficient data diversity. We propose a classification framework that combines iterative topic refinement, contrastive prompting, and active learning. Starting with a small set of labeled samples, the model generates initial topic labels. Misclassified or ambiguous samples are then used in an iterative contrastive prompting process to refine category distinctions by explicitly teaching the model to differentiate between closely related classes. The framework features a human-in-the-loop component, allowing users to introduce or revise category definitions in natural language. This enables seamless integration of new, unseen categories without retraining, making the system well-suited for real-world, dynamic environments. The evaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy on AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with minimal accuracy shift after introducing unseen classes (82% and 87%, respectively). The results highlight the effectiveness of prompt-based semantic reasoning for fine-grained classification with limited supervision.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent</title>
<link>https://arxiv.org/abs/2508.01031</link>
<guid>https://arxiv.org/abs/2508.01031</guid>
<content:encoded><![CDATA[
arXiv:2508.01031v1 Announce Type: cross 
Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing but typically requires a high level of expertise from designers. To lower the entry barrier and improve design efficiency, we present an agent for CAD conceptual design powered by large language models (LLMs). The agent accepts both abstract textual descriptions and freehand sketches as input, engaging in interactive dialogue with users to refine and clarify design requirements through comprehensive requirement analysis. Built upon a novel Context-Independent Imperative Paradigm (CIP), the agent generates high-quality CAD modeling code. During the generation process, the agent incorporates iterative visual feedback to improve model quality. Generated design cases are stored in a structured knowledge base, enabling continuous improvement of the agent's code generation capabilities. Experimental results demonstrate that our method achieves state-of-the-art performance in CAD code generation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs</title>
<link>https://arxiv.org/abs/2508.01136</link>
<guid>https://arxiv.org/abs/2508.01136</guid>
<content:encoded><![CDATA[
arXiv:2508.01136v1 Announce Type: cross 
Abstract: The operation and maintenance (O&amp;M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&amp;M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&amp;M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&amp;M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&amp;M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens</title>
<link>https://arxiv.org/abs/2508.01191</link>
<guid>https://arxiv.org/abs/2508.01191</guid>
<content:encoded><![CDATA[
arXiv:2508.01191v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting has been shown to improve Large Language Model (LLM) performance on various tasks. With this approach, LLMs appear to produce human-like reasoning steps before providing answers (a.k.a., CoT reasoning), which often leads to the perception that they engage in deliberate inferential processes. However, some initial findings suggest that CoT reasoning may be more superficial than it appears, motivating us to explore further. In this paper, we study CoT reasoning via a data distribution lens and investigate if CoT reasoning reflects a structured inductive bias learned from in-distribution data, allowing the model to conditionally generate reasoning paths that approximate those seen during training. Thus, its effectiveness is fundamentally bounded by the degree of distribution discrepancy between the training data and the test queries. With this lens, we dissect CoT reasoning via three dimensions: task, length, and format. To investigate each dimension, we design DataAlchemy, an isolated and controlled environment to train LLMs from scratch and systematically probe them under various distribution conditions. Our results reveal that CoT reasoning is a brittle mirage that vanishes when it is pushed beyond training distributions. This work offers a deeper understanding of why and when CoT reasoning fails, emphasizing the ongoing challenge of achieving genuine and generalizable reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentArmor: Enforcing Program Analysis on Agent Runtime Trace to Defend Against Prompt Injection</title>
<link>https://arxiv.org/abs/2508.01249</link>
<guid>https://arxiv.org/abs/2508.01249</guid>
<content:encoded><![CDATA[
arXiv:2508.01249v1 Announce Type: cross 
Abstract: Large Language Model (LLM) agents offer a powerful new paradigm for solving various problems by combining natural language reasoning with the execution of external tools. However, their dynamic and non-transparent behavior introduces critical security risks, particularly in the presence of prompt injection attacks. In this work, we propose a novel insight that treats the agent runtime traces as structured programs with analyzable semantics. Thus, we present AgentArmor, a program analysis framework that converts agent traces into graph intermediate representation-based structured program dependency representations (e.g., CFG, DFG, and PDG) and enforces security policies via a type system. AgentArmor consists of three key components: (1) a graph constructor that reconstructs the agent's working traces as graph-based intermediate representations with control flow and data flow described within; (2) a property registry that attaches security-relevant metadata of interacted tools & data, and (3) a type system that performs static inference and checking over the intermediate representation. By representing agent behavior as structured programs, AgentArmor enables program analysis over sensitive data flow, trust boundaries, and policy violations. We evaluate AgentArmor on the AgentDojo benchmark, the results show that AgentArmor can achieve 95.75% of TPR, with only 3.66% of FPR. Our results demonstrate AgentArmor's ability to detect prompt injection vulnerabilities and enforce fine-grained security constraints.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan</title>
<link>https://arxiv.org/abs/2508.01274</link>
<guid>https://arxiv.org/abs/2508.01274</guid>
<content:encoded><![CDATA[
arXiv:2508.01274v1 Announce Type: cross 
Abstract: Multimodal Large Language Models (MLLMs) process visual, acoustic, and textual inputs, addressing the limitations of single-modality LLMs. However, existing benchmarks often overlook tri-modal evaluation in Traditional Chinese and do not consider inference latency. To address this, we introduce Multi-TW, the first Traditional Chinese benchmark for evaluating the performance and latency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice questions (image and text, audio and text pairs) sourced from official proficiency tests developed with the Steering Committee for the Test of Proficiency-Huayu (SC-TOP). We evaluated various any-to-any models and vision-language models (VLMs) with audio transcription. Our results show that closed-source models generally outperform open-source ones across modalities, although open-source models can perform well in audio tasks. End-to-end any-to-any pipelines offer clear latency advantages compared to VLMs using separate audio transcription. Multi-TW presents a comprehensive view of model capabilities and highlights the need for Traditional Chinese fine-tuning and efficient multimodal architectures.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models</title>
<link>https://arxiv.org/abs/2508.01365</link>
<guid>https://arxiv.org/abs/2508.01365</guid>
<content:encoded><![CDATA[
arXiv:2508.01365v1 Announce Type: cross 
Abstract: Backdoor attacks pose a significant threat to Large Language Models (LLMs), where adversaries can embed hidden triggers to manipulate LLM's outputs. Most existing defense methods, primarily designed for classification tasks, are ineffective against the autoregressive nature and vast output space of LLMs, thereby suffering from poor performance and high latency. To address these limitations, we investigate the behavioral discrepancies between benign and backdoored LLMs in output space. We identify a critical phenomenon which we term sequence lock: a backdoored model generates the target sequence with abnormally high and consistent confidence compared to benign generation. Building on this insight, we propose ConfGuard, a lightweight and effective detection method that monitors a sliding window of token confidences to identify sequence lock. Extensive experiments demonstrate ConfGuard achieves a near 100\% true positive rate (TPR) and a negligible false positive rate (FPR) in the vast majority of cases. Crucially, the ConfGuard enables real-time detection almost without additional latency, making it a practical backdoor defense for real-world LLM deployments.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChEmbed: Enhancing Chemical Literature Search Through Domain-Specific Text Embeddings</title>
<link>https://arxiv.org/abs/2508.01643</link>
<guid>https://arxiv.org/abs/2508.01643</guid>
<content:encoded><![CDATA[
arXiv:2508.01643v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems in chemistry heavily depend on accurate and relevant retrieval of chemical literature. However, general-purpose text embedding models frequently fail to adequately represent complex chemical terminologies, resulting in suboptimal retrieval quality. Specialized embedding models tailored to chemical literature retrieval have not yet been developed, leaving a substantial performance gap. To address this challenge, we introduce ChEmbed, a domain-adapted family of text embedding models fine-tuned on a dataset comprising chemistry-specific text from the PubChem, Semantic Scholar, and ChemRxiv corpora. To create effective training data, we employ large language models to synthetically generate queries, resulting in approximately 1.7 million high-quality query-passage pairs. Additionally, we augment the tokenizer by adding 900 chemically specialized tokens to previously unused slots, which significantly reduces the fragmentation of chemical entities, such as IUPAC names. ChEmbed also maintains a 8192-token context length, enabling the efficient retrieval of longer passages compared to many other open-source embedding models, which typically have a context length of 512 or 2048 tokens. Evaluated on our newly introduced ChemRxiv Retrieval benchmark, ChEmbed outperforms state-of-the-art general embedding models, raising nDCG@10 from 0.82 to 0.91 (+9 pp). ChEmbed represents a practical, lightweight, and reproducible embedding solution that effectively improves retrieval for chemical literature search.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</title>
<link>https://arxiv.org/abs/2508.01647</link>
<guid>https://arxiv.org/abs/2508.01647</guid>
<content:encoded><![CDATA[
arXiv:2508.01647v1 Announce Type: cross 
Abstract: As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at https://github.com/ManHu2025/DUP.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and Regional Languages Around the Globe</title>
<link>https://arxiv.org/abs/2508.01691</link>
<guid>https://arxiv.org/abs/2508.01691</guid>
<content:encoded><![CDATA[
arXiv:2508.01691v1 Announce Type: cross 
Abstract: We present Voxlect, a novel benchmark for modeling dialects and regional languages worldwide using speech foundation models. Specifically, we report comprehensive benchmark evaluations on dialects and regional language varieties in English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai, Spanish, French, German, Brazilian Portuguese, and Italian. Our study used over 2 million training utterances from 30 publicly available speech corpora that are provided with dialectal information. We evaluate the performance of several widely used speech foundation models in classifying speech dialects. We assess the robustness of the dialectal models under noisy conditions and present an error analysis that highlights modeling results aligned with geographic continuity. In addition to benchmarking dialect classification, we demonstrate several downstream applications enabled by Voxlect. Specifically, we show that Voxlect can be applied to augment existing speech recognition datasets with dialect information, enabling a more detailed analysis of ASR performance across dialectal variations. Voxlect is also used as a tool to evaluate the performance of speech generation systems. Voxlect is publicly available with the license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2508.01773</link>
<guid>https://arxiv.org/abs/2508.01773</guid>
<content:encoded><![CDATA[
arXiv:2508.01773v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?</title>
<link>https://arxiv.org/abs/2508.01780</link>
<guid>https://arxiv.org/abs/2508.01780</guid>
<content:encoded><![CDATA[
arXiv:2508.01780v1 Announce Type: cross 
Abstract: With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSLRConformer: A Data-Centric Conformer Approach for Continuous Arabic Sign Language Recognition on the Isharah Datase</title>
<link>https://arxiv.org/abs/2508.01791</link>
<guid>https://arxiv.org/abs/2508.01791</guid>
<content:encoded><![CDATA[
arXiv:2508.01791v1 Announce Type: cross 
Abstract: The field of Continuous Sign Language Recognition (CSLR) poses substantial technical challenges, including fluid inter-sign transitions, the absence of temporal boundaries, and co-articulation effects. This paper, developed for the MSLR 2025 Workshop Challenge at ICCV 2025, addresses the critical challenge of signer-independent recognition to advance the generalization capabilities of CSLR systems across diverse signers. A data-centric methodology is proposed, centered on systematic feature engineering, a robust preprocessing pipeline, and an optimized model architecture. Key contributions include a principled feature selection process guided by Exploratory Data Analysis (EDA) to isolate communicative keypoints, a rigorous preprocessing pipeline incorporating DBSCAN-based outlier filtering and spatial normalization, and the novel CSLRConformer architecture. This architecture adapts the hybrid CNN-Transformer design of the Conformer model, leveraging its capacity to model local temporal dependencies and global sequence context; a characteristic uniquely suited for the spatio-temporal dynamics of sign language. The proposed methodology achieved a competitive performance, with a Word Error Rate (WER) of 5.60% on the development set and 12.01% on the test set, a result that secured a 3rd place ranking on the official competition platform. This research validates the efficacy of cross-domain architectural adaptation, demonstrating that the Conformer model, originally conceived for speech recognition, can be successfully repurposed to establish a new state-of-the-art performance in keypoint-based CSLR.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</title>
<link>https://arxiv.org/abs/2508.01887</link>
<guid>https://arxiv.org/abs/2508.01887</guid>
<content:encoded><![CDATA[
arXiv:2508.01887v1 Announce Type: cross 
Abstract: AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4) % accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4 $\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models</title>
<link>https://arxiv.org/abs/2508.01908</link>
<guid>https://arxiv.org/abs/2508.01908</guid>
<content:encoded><![CDATA[
arXiv:2508.01908v1 Announce Type: cross 
Abstract: Training large language models (LLMs) typically involves pre-training on massive corpora, only to restart the process entirely when new data becomes available. A more efficient and resource-conserving approach would be continual pre-training, where models are updated with new data rather than retraining from scratch. However, the introduction of new data often causes distribution shifts, leading to performance degradation on previously learned tasks. In this paper, we take a deeper look at two popular proposals for addressing this distribution shift within the continual learning literature: experience replay and gradient alignment. We consider continual pre-training of models within the Llama family of architectures at a large scale across languages with 100 billion tokens of training data in each language, finding that both replay and gradient alignment lead to more stable learning without forgetting. This conclusion holds both as we vary the model scale and as we vary the number and diversity of tasks. Moreover, we are the first to demonstrate the effectiveness of gradient alignment techniques in the context of LLM pre-training and propose an efficient implementation of meta-experience replay (MER) that imbues experience replay with the benefits of gradient alignment despite negligible compute and memory overhead. Our scaling analysis across model sizes and replay rates indicates that small rates of replaying old examples are definitely a more valuable use of compute than investing in model size, but that it is more compute efficient to scale the size of the model than invest in high rates of replaying old examples.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology</title>
<link>https://arxiv.org/abs/2508.01913</link>
<guid>https://arxiv.org/abs/2508.01913</guid>
<content:encoded><![CDATA[
arXiv:2508.01913v1 Announce Type: cross 
Abstract: Academic publishing, integral to knowledge dissemination and scientific advancement, increasingly faces threats from unethical practices such as unconsented authorship, gift authorship, author ambiguity, and undisclosed conflicts of interest. While existing infrastructures like ORCID effectively disambiguate researcher identities, they fall short in enforcing explicit authorship consent, accurately verifying contributor roles, and robustly detecting conflicts of interest during peer review. To address these shortcomings, this paper introduces a decentralized framework leveraging Self-Sovereign Identity (SSI) and blockchain technology. The proposed model uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to securely verify author identities and contributions, reducing ambiguity and ensuring accurate attribution. A blockchain-based trust registry records authorship consent and peer-review activity immutably. Privacy-preserving cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support conflict-of-interest detection without revealing sensitive data. Verified authorship metadata and consent records are embedded in publications, increasing transparency. A stakeholder survey of researchers, editors, and reviewers suggests the framework improves ethical compliance and confidence in scholarly communication. This work represents a step toward a more transparent, accountable, and trustworthy academic publishing ecosystem.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning</title>
<link>https://arxiv.org/abs/2508.01916</link>
<guid>https://arxiv.org/abs/2508.01916</guid>
<content:encoded><![CDATA[
arXiv:2508.01916v1 Announce Type: cross 
Abstract: Understanding internal representations of neural models is a core interest of mechanistic interpretability. Due to its large dimensionality, the representation space can encode various aspects about inputs. To what extent are different aspects organized and encoded in separate subspaces? Is it possible to find these ``natural'' subspaces in a purely unsupervised way? Somewhat surprisingly, we can indeed achieve this and find interpretable subspaces by a seemingly unrelated training objective. Our method, neighbor distance minimization (NDM), learns non-basis-aligned subspaces in an unsupervised manner. Qualitative analysis shows subspaces are interpretable in many cases, and encoded information in obtained subspaces tends to share the same abstract concept across different inputs, making such subspaces similar to ``variables'' used by the model. We also conduct quantitative experiments using known circuits in GPT-2; results show a strong connection between subspaces and circuit variables. We also provide evidence showing scalability to 2B models by finding separate subspaces mediating context and parametric knowledge routing. Viewed more broadly, our findings offer a new perspective on understanding model internals and building circuits.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs</title>
<link>https://arxiv.org/abs/2508.02066</link>
<guid>https://arxiv.org/abs/2508.02066</guid>
<content:encoded><![CDATA[
arXiv:2508.02066v1 Announce Type: cross 
Abstract: Large Language Models(LLMs) have demonstrated remarkable performance across various domains, yet their capabilities in molecular reasoning remain insufficiently explored. Current approaches tend to rely heavily on general-purpose prompting, which lacks domain-specific molecular semantics, while those that use fine-tuning strategies often face challenges with interpretability and reasoning depth. To address these issues, we introduce MolReasoner, a two-stage framework designed to transition LLMs from memorization towards chemical reasoning. First, we propose Mol-SFT, which initializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT) samples generated by GPT-4o and verified for chemical accuracy. Subsequently, Mol-RL applies reinforcement learning with specialized reward functions designed explicitly to align chemical structures with linguistic descriptions, thereby enhancing molecular reasoning capabilities. Our approach notably enhances interpretability, improving the model 's molecular understanding and enabling better generalization. Extensive experiments demonstrate that MolReasoner outperforms existing methods, and marking a significant shift from memorization-based outputs to robust chemical reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human Capital Visualization using Speech Amount during Meetings</title>
<link>https://arxiv.org/abs/2508.02075</link>
<guid>https://arxiv.org/abs/2508.02075</guid>
<content:encoded><![CDATA[
arXiv:2508.02075v1 Announce Type: cross 
Abstract: In recent years, many companies have recognized the importance of human resources and are investing in human capital to revitalize their organizations and enhance internal communication, thereby fostering innovation. However, conventional quantification methods have mainly focused on readily measurable indicators without addressing the fundamental role of conversations in human capital. This study focuses on routine meetings and proposes strategies to visualize human capital by analyzing speech amount during these meetings. We employ conversation visualization technology, which operates effectively, to quantify speech. We then measure differences in speech amount by attributes such as gender and job post, changes in speech amount depending on whether certain participants are present, and correlations between speech amount and continuous attributes. To verify the effectiveness of our proposed methods, we analyzed speech amounts by departmental affiliation during weekly meetings at small to medium enterprises.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
arXiv:2508.02091v1 Announce Type: cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trainable Dynamic Mask Sparse Attention</title>
<link>https://arxiv.org/abs/2508.02124</link>
<guid>https://arxiv.org/abs/2508.02124</guid>
<content:encoded><![CDATA[
arXiv:2508.02124v1 Announce Type: cross 
Abstract: In large language models, the demand for modeling long contexts is constantly increasing, but the quadratic complexity of the standard self-attention mechanism often becomes a bottleneck. Although existing sparse attention mechanisms have improved efficiency, they may still encounter issues such as static patterns or information loss. We introduce a trainable dynamic mask sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes content-aware and position-aware sparsity. DMA achieves this through two key innovations: First, it dynamically generates content-aware sparse masks from value representations, enabling the model to identify and focus on critical information adaptively. Second, it implements position-aware sparse attention computation that effectively skips unnecessary calculation regions. This dual-sparsity design allows the model to significantly reduce the computational complexity of important information while retaining complete information, achieving an excellent balance between information fidelity and computational efficiency. We have verified the performance of DMA through comprehensive experiments. Comparative studies show that DMA outperforms multi-head attention, sliding window attention, multi-head latent attention, and native sparse attention in terms of perplexity under Chinchilla Scaling Law settings. Moreover, in challenging multi-query associative recall tasks, DMA also demonstrates superior performance and efficiency compared to these methods. Crucially, in the evaluation of a 1.7B parameter model, DMA significantly outperforms multi-head attention in both standard benchmark performance and the challenging needle-in-a-haystack task. These experimental results highlight its capability to balance model efficiency and long-context modeling ability effectively.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subject or Style: Adaptive and Training-Free Mixture of LoRAs</title>
<link>https://arxiv.org/abs/2508.02165</link>
<guid>https://arxiv.org/abs/2508.02165</guid>
<content:encoded><![CDATA[
arXiv:2508.02165v1 Announce Type: cross 
Abstract: Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable performance in subject-driven or style-driven generation tasks. Studies have explored combinations of different LoRAs to jointly generate learned styles and content. However, current methods struggle to balance the original subject and style, and often require additional training. Recently, K-LoRA proposed a training-free LoRA fusion method. But it involves multiple hyperparameters, making it difficult to adapt to all styles and subjects. In this paper, we propose EST-LoRA, a training-free adaptive LoRA fusion method. It comprehensively considers three critical factors: \underline{E}nergy of matrix, \underline{S}tyle discrepancy scores and \underline{T}ime steps. Analogous to the Mixture of Experts (MoE) architecture, the model adaptively selects between subject LoRA and style LoRA within each attention layer. This integrated selection mechanism ensures balanced contributions from both components during the generation process. Experimental results show that EST-LoRA outperforms state-of-the-art methods in both qualitative and quantitative evaluations and achieves faster generation speed compared to other efficient fusion approaches. Our code is publicly available at: https://anonymous.4open.science/r/EST-LoRA-F318.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers</title>
<link>https://arxiv.org/abs/2508.02175</link>
<guid>https://arxiv.org/abs/2508.02175</guid>
<content:encoded><![CDATA[
arXiv:2508.02175v1 Announce Type: cross 
Abstract: As Audio Large Language Models (ALLMs) emerge as powerful tools for speech processing, their safety implications demand urgent attention. While considerable research has explored textual and vision safety, audio's distinct characteristics present significant challenges. This paper first investigates: Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In response to this issue, we introduce Hidden in the Noise (HIN), a novel backdoor attack framework designed to exploit subtle, audio-specific features. HIN applies acoustic modifications to raw audio waveforms, such as alterations to temporal dynamics and strategic injection of spectrally tailored noise. These changes introduce consistent patterns that an ALLM's acoustic feature encoder captures, embedding robust triggers within the audio stream. To evaluate ALLM robustness against audio-feature-based triggers, we develop the AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments on AudioSafe and three established safety datasets reveal critical vulnerabilities in existing ALLMs: (I) audio features like environment noise and speech rate variations achieve over 90% average attack success rate. (II) ALLMs exhibit significant sensitivity differences across acoustic features, particularly showing minimal response to volume as a trigger, and (III) poisoned sample inclusion causes only marginal loss curve fluctuations, highlighting the attack's stealth.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</title>
<link>https://arxiv.org/abs/2508.02215</link>
<guid>https://arxiv.org/abs/2508.02215</guid>
<content:encoded><![CDATA[
arXiv:2508.02215v1 Announce Type: cross 
Abstract: Large language models (LLMs) enable long-context tasks but face efficiency challenges due to the growing key-value (KV) cache. We propose LeanK, a learning-based method that prunes unimportant key (K) cache channels by leveraging static channel sparsity. With a novel two-stage training process, LeanK learns channel-wise static mask that could satisfy specific sparsity ratio and hardware alignment requirement. LeanK reduces GPU memory and accelerates decoding without sacrificing accuracy. Experiments demonstrate up to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel enables 1.3x speedup for attention computation. We also provide insights into model channels and attention heads during long-context inference by analyzing the learned importance distribution. Our code is available at https://aka.ms/LeanK.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellForge: Agentic Design of Virtual Cell Models</title>
<link>https://arxiv.org/abs/2508.02276</link>
<guid>https://arxiv.org/abs/2508.02276</guid>
<content:encoded><![CDATA[
arXiv:2508.02276v1 Announce Type: cross 
Abstract: Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogue Systems Engineering: A Survey and Future Directions</title>
<link>https://arxiv.org/abs/2508.02279</link>
<guid>https://arxiv.org/abs/2508.02279</guid>
<content:encoded><![CDATA[
arXiv:2508.02279v1 Announce Type: cross 
Abstract: This paper proposes to refer to the field of software engineering related to the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys this field while also discussing its future directions. With the advancement of large language models, the core technologies underlying dialogue systems have significantly progressed. As a result, dialogue system technology is now expected to be applied to solving various societal issues and in business contexts. To achieve this, it is important to build, operate, and continuously improve dialogue systems correctly and efficiently. Accordingly, in addition to applying existing software engineering knowledge, it is becoming increasingly important to evolve software engineering tailored specifically to dialogue systems. In this paper, we enumerate the knowledge areas of dialogue systems engineering based on those of software engineering, as defined in the Software Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based on this survey, we identify unexplored topics in each area and discuss the future direction of dialogue systems engineering.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment</title>
<link>https://arxiv.org/abs/2508.02298</link>
<guid>https://arxiv.org/abs/2508.02298</guid>
<content:encoded><![CDATA[
arXiv:2508.02298v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has improved the reasoning abilities of Large Language Models (LLMs) by using rule-based binary feedback, helping to mitigate reward hacking. However, current RLVR methods typically treat whole responses as single actions, assigning the same reward to every token. This coarse-grained feedback hampers precise credit assignment, making it hard for models to identify which reasoning steps lead to success or failure, and often results in suboptimal policies and inefficient learning. Methods like PPO provide credit assignment through value estimation, but often yield inaccurate and unverifiable signals due to limited sampling. On the other hand, methods using Process Reward Models can provide step-by-step judgments for each reasoning step, but they require high-quality process supervision labels and are time-consuming when applied in online reinforcement learning (RL). To overcome these limitations, we introduce a simple but efficient method Credit Assignment Policy Optimization (CAPO). Given a reasoning response rollout from the policy model, CAPO directly leverages an off-the-shelf, general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to generate all step-wise critique by one pass, thereby providing verifiable token-level rewards to refine the tokens that were originally assigned identical rule-based rewards. This enables more fine-grained credit assignment in an effective way. Furthermore, to enhance the accuracy and robustness of CAPO, we employ voting mechanisms that scale with the number of generated critiques. Extensive experiments using different backbones like Llama and Qwen models and in different sizes show that CAPO consistently outperforms supervised learning-based and RL-based fine-tuning methods across six challenging mathematical benchmarks and three out-of-domain benchmarks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits</title>
<link>https://arxiv.org/abs/2508.02328</link>
<guid>https://arxiv.org/abs/2508.02328</guid>
<content:encoded><![CDATA[
arXiv:2508.02328v1 Announce Type: cross 
Abstract: Conversational Recommender Systems (CRSs) deliver personalised recommendations through multi-turn natural language dialogue and increasingly support both task-oriented and exploratory interactions. Yet, the factors shaping user interaction preferences remain underexplored. In this within-subjects study (\(N = 139\)), participants experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities. Logistic regression revealed that preference for the exploratory interaction was predicted by enjoyment, usefulness, novelty, and conversational quality. Unexpectedly, perceived effectiveness was also associated with exploratory preference. Clustering uncovered five latent user profiles with distinct dialogue style preferences. Moderation analyses indicated that age, gender, and control preference significantly influenced these choices. These findings integrate affective, cognitive, and trait-level predictors into CRS user modelling and inform autonomy-sensitive, value-adaptive dialogue design. The proposed predictive and adaptive framework applies broadly to conversational AI systems seeking to align dynamically with evolving user needs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Model Guided Reinforcement Learning in Quantitative Trading</title>
<link>https://arxiv.org/abs/2508.02366</link>
<guid>https://arxiv.org/abs/2508.02366</guid>
<content:encoded><![CDATA[
arXiv:2508.02366v1 Announce Type: cross 
Abstract: Algorithmic trading requires short-term decisions aligned with long-term financial goals. While reinforcement learning (RL) has been explored for such tactical decisions, its adoption remains limited by myopic behavior and opaque policy rationale. In contrast, large language models (LLMs) have recently demonstrated strategic reasoning and multi-modal financial signal interpretation when guided by well-designed prompts.
  We propose a hybrid system where LLMs generate high-level trading strategies to guide RL agents in their actions. We evaluate (i) the rationale of LLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and Maximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results show improved return and risk metrics over standard RL.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Six Guidelines for Trustworthy, Ethical and Responsible Automation Design</title>
<link>https://arxiv.org/abs/2508.02371</link>
<guid>https://arxiv.org/abs/2508.02371</guid>
<content:encoded><![CDATA[
arXiv:2508.02371v1 Announce Type: cross 
Abstract: Calibrated trust in automated systems (Lee and See 2004) is critical for their safe and seamless integration into society. Users should only rely on a system recommendation when it is actually correct and reject it when it is factually wrong. One requirement to achieve this goal is an accurate trustworthiness assessment, ensuring that the user's perception of the system's trustworthiness aligns with its actual trustworthiness, allowing users to make informed decisions about the extent to which they can rely on the system (Schlicker et al. 2022). We propose six design guidelines to help designers optimize for accurate trustworthiness assessments, thus fostering ethical and responsible human-automation interactions. The proposed guidelines are derived from existing literature in various fields, such as human-computer interaction, cognitive psychology, automation research, user-experience design, and ethics. We are incorporating key principles from the field of pragmatics, specifically the cultivation of common ground (H. H. Clark 1996) and Gricean communication maxims (Grice 1975). These principles are essential for the design of automated systems because the user's perception of the system's trustworthiness is shaped by both environmental contexts, such as organizational culture or societal norms, and by situational context, including the specific circumstances or scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed guidelines provide actionable insights for designers to create automated systems that make relevant trustworthiness cues available. This would ideally foster calibrated trust and more satisfactory, productive, and safe interactions between humans and automated systems. Furthermore, the proposed heuristics might work as a tool for evaluating to what extent existing systems enable users to accurately assess a system's trustworthiness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens</title>
<link>https://arxiv.org/abs/2508.02419</link>
<guid>https://arxiv.org/abs/2508.02419</guid>
<content:encoded><![CDATA[
arXiv:2508.02419v1 Announce Type: cross 
Abstract: Large vision-language models (LVLMs) have demonstrated remarkable multimodal comprehension and reasoning capabilities, but they still suffer from severe object hallucination. Previous studies primarily attribute the flaw to linguistic prior caused by the scale mismatch between visual encoders and large language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs, generating descriptions inconsistent with visual cues. However, through an in-depth investigation of the hallucinated mechanisms, we empirically reveal a previously overlooked phenomenon: LVLMs may ignore not only visual information but also textual modality during hallucination, a behavior termed as modality bias, which indicates that LVLMs struggle to simultaneously attend to both visual and textual modalities, leading to fragmented understanding of user-provided instructions. Based on this observation, we propose a simple yet effective training-free method to mitigate object hallucination. Concretely, we intervene and adjust the attention weights of textual and visual tokens, balancing cross-modal compatibility for better alignment with user intentions. Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's overreliance on its parametric knowledge, synergistically enhancing our attention manipulation. Extensive experiments confirm the widespread presence of modality bias in LVLMs. Notably, our method effectively mitigates hallucination across multiple open-source LVLMs and benchmarks, highlighting its generalizability and efficacy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.02470</link>
<guid>https://arxiv.org/abs/2508.02470</guid>
<content:encoded><![CDATA[
arXiv:2508.02470v1 Announce Type: cross 
Abstract: While many tools are available for designing AI, non-experts still face challenges in clearly expressing their intent and managing system complexity. We introduce AIAP, a no-code platform that integrates natural language input with visual workflows. AIAP leverages a coordinated multi-agent system to decompose ambiguous user instructions into modular, actionable steps, hidden from users behind a unified interface. A user study involving 32 participants showed that AIAP's AI-generated suggestions, modular workflows, and automatic identification of data, actions, and context significantly improved participants' ability to develop services intuitively. These findings highlight that natural language-based visual programming significantly reduces barriers and enhances user experience in AI service design.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling</title>
<link>https://arxiv.org/abs/2508.02503</link>
<guid>https://arxiv.org/abs/2508.02503</guid>
<content:encoded><![CDATA[
arXiv:2508.02503v1 Announce Type: cross 
Abstract: LLM-based solvers have emerged as a promising means of automating problem modeling and solving. However, they remain unreliable and often depend on iterative repair loops that result in significant latency. We introduce OptiHive, an LLM-based framework that produces high-quality solvers for optimization problems from natural-language descriptions without iterative self-correction. OptiHive uses a single batched LLM query to generate diverse components (solvers, problem instances, and validation tests) and filters out erroneous components to ensure fully interpretable outputs. Taking into account the imperfection of the generated components, we employ a statistical model to infer their true performance, enabling principled uncertainty quantification and solver selection. On tasks ranging from traditional optimization problems to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive significantly outperforms baselines, increasing the optimality rate from 5\% to 92\% on the most complex problems.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-time Prompt Intervention</title>
<link>https://arxiv.org/abs/2508.02511</link>
<guid>https://arxiv.org/abs/2508.02511</guid>
<content:encoded><![CDATA[
arXiv:2508.02511v1 Announce Type: cross 
Abstract: Test-time compute has led to remarkable success in the large language model (LLM) community, particularly for complex tasks, where longer chains of thought (CoTs) are generated to enhance reasoning capabilities. However, growing evidence reveals that such reasoning models often produce CoTs plagued by excessive redundancy, including unnecessary verification steps and repetitive reasoning shifts. The root cause lies in post-training of them that overly rely on outcome reward paradigms, as the data of process reward paradigms, which regulate intermediate reasoning steps, is difficult to construct at scale. To address this, we propose PI, a novel framework for Test-time Prompt Intervention. PI provides an interface to dynamically guide and regulate reasoning paths during inference through timely (When module) and proper (How module) interventions and post-intervention sampling (Which module). This allows human problem-solving expertise and cognitive science principles to be seamlessly integrated into LLMs' reasoning processes, enhancing controllability and interpretability. Extensive experiments across multiple models and datasets demonstrate that PI significantly shortens CoTs while reducing hallucination, yielding more concise and reliable reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are you sinking? A geometric approach on attention sink</title>
<link>https://arxiv.org/abs/2508.02546</link>
<guid>https://arxiv.org/abs/2508.02546</guid>
<content:encoded><![CDATA[
arXiv:2508.02546v1 Announce Type: cross 
Abstract: Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules</title>
<link>https://arxiv.org/abs/2508.02587</link>
<guid>https://arxiv.org/abs/2508.02587</guid>
<content:encoded><![CDATA[
arXiv:2508.02587v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among their specialized experts, which existing Parameter- Efficient Fine-Tuning (PEFT) strategies fail to leverage. This motivates us to investigate whether adaptation modules themselves should incorporate routing mechanisms to align with MoE's multi-expert architecture. We analyze dynamics of core components when applying PEFT to MoE language models and examine how different routing strategies affect adaptation effectiveness. Extensive experiments adapting OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks validate the performance and efficiency of our routed approach. We identify the optimal configurations for different scenarios and provide empirical analyses with practical insights to facilitate better PEFT and MoE applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research</title>
<link>https://arxiv.org/abs/2508.02621</link>
<guid>https://arxiv.org/abs/2508.02621</guid>
<content:encoded><![CDATA[
arXiv:2508.02621v1 Announce Type: cross 
Abstract: The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction</title>
<link>https://arxiv.org/abs/2508.02622</link>
<guid>https://arxiv.org/abs/2508.02622</guid>
<content:encoded><![CDATA[
arXiv:2508.02622v1 Announce Type: cross 
Abstract: This paper introduces and formalizes Noosemia, a novel cognitive-phenomenological phenomenon emerging from human interaction with generative AI systems, particularly those enabling dialogic or multimodal exchanges. We propose a multidisciplinary framework to explain how, under certain conditions, users attribute intentionality, agency, and even interiority to these systems - a process grounded not in physical resemblance, but in linguistic performance, epistemic opacity, and emergent technological complexity. By linking an LLM declination of meaning holism to our technical notion of the LLM Contextual Cognitive Field, we clarify how LLMs construct meaning relationally and how coherence and a simulacrum of agency arise at the human-AI interface. The analysis situates noosemia alongside pareidolia, animism, the intentional stance and the uncanny valley, distinguishing its unique characteristics. We also introduce a-noosemia to describe the phenomenological withdrawal of such projections. The paper concludes with reflections on the broader philosophical, epistemological, and social implications of noosemic dynamics and directions for future research.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents</title>
<link>https://arxiv.org/abs/2508.02629</link>
<guid>https://arxiv.org/abs/2508.02629</guid>
<content:encoded><![CDATA[
arXiv:2508.02629v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have enabled richer perceptual grounding for code policy generation in embodied agents. However, most existing systems lack effective mechanisms to adaptively monitor policy execution and repair codes during task completion. In this work, we introduce HyCodePolicy, a hybrid language-based control framework that systematically integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair into a closed-loop programming cycle for embodied agents. Technically, given a natural language instruction, our system first decomposes it into subgoals and generates an initial executable program grounded in object-centric geometric primitives. The program is then executed in simulation, while a vision-language model (VLM) observes selected checkpoints to detect and localize execution failures and infer failure reasons. By fusing structured execution traces capturing program-level events with VLM-based perceptual feedback, HyCodePolicy infers failure causes and repairs programs. This hybrid dual feedback mechanism enables self-correcting program synthesis with minimal human supervision. Our results demonstrate that HyCodePolicy significantly improves the robustness and sample efficiency of robot manipulation policies, offering a scalable strategy for integrating multimodal reasoning into autonomous decision-making pipelines.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting</title>
<link>https://arxiv.org/abs/2306.15933</link>
<guid>https://arxiv.org/abs/2306.15933</guid>
<content:encoded><![CDATA[
arXiv:2306.15933v2 Announce Type: replace 
Abstract: Small language models like T5 excel in generating high-quality text for data-to-text tasks, offering adaptability and cost-efficiency compared to Large Language Models (LLMs). However, they frequently miss keywords, which is considered one of the most severe and common errors in this task. In this work, we explore the potential of using feedback systems to enhance semantic fidelity in smaller language models for data-to-text generation tasks, through our Verification and Correction Prompting (VCP) approach. In the inference stage, our approach involves a multi-step process, including generation, verification, and regeneration stages. During the verification stage, we implement a simple rule to check for the presence of every keyword in the prediction. Recognizing that this rule can be inaccurate, we have developed a carefully designed training procedure, which enabling the model to incorporate feedback from the error-correcting prompt effectively, despite its potential inaccuracies. The VCP approach effectively reduces the Semantic Error Rate (SER) while maintaining the text's quality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process</title>
<link>https://arxiv.org/abs/2402.10699</link>
<guid>https://arxiv.org/abs/2402.10699</guid>
<content:encoded><![CDATA[
arXiv:2402.10699v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THREAD: Thinking Deeper with Recursive Spawning</title>
<link>https://arxiv.org/abs/2405.17402</link>
<guid>https://arxiv.org/abs/2405.17402</guid>
<content:encoded><![CDATA[
arXiv:2405.17402v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.11130</link>
<guid>https://arxiv.org/abs/2406.11130</guid>
<content:encoded><![CDATA[
arXiv:2406.11130v2 Announce Type: replace 
Abstract: Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?</title>
<link>https://arxiv.org/abs/2406.12307</link>
<guid>https://arxiv.org/abs/2406.12307</guid>
<content:encoded><![CDATA[
arXiv:2406.12307v5 Announce Type: replace 
Abstract: Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To quantitatively evaluate this capability, we construct a new benchmark dataset where instances are systematically altered to simulate the ambiguous and incomplete conditions common in real-world interactions. Our experiments reveal that even state-of-the-art LLMs often struggle to identify these conditions, attempting to use tools without sufficient information or when the correct tool is unavailable. To better understand these limitations, we conduct a detailed behavioral analysis across various conditions, including implicit evaluation and scenarios where models receive feedback from previous tool invocations. Based on this analysis, we propose a novel prompting-based reasoning strategy that explicitly instructs models to assess the sufficiency of information and the availability of tools. Our proposed approach significantly enhances the models' ability to recognize incomplete conditions, resulting in more informed and contextually appropriate tool-use decisions. We believe our research contributes to advancing the reliability of LLMs, especially in real-world applications where incomplete or ambiguous information is common. Our dataset is available at https://huggingface.co/datasets/ddehun/ICT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascade Reward Sampling for Efficient Decoding-Time Alignment</title>
<link>https://arxiv.org/abs/2406.16306</link>
<guid>https://arxiv.org/abs/2406.16306</guid>
<content:encoded><![CDATA[
arXiv:2406.16306v3 Announce Type: replace 
Abstract: Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations. To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70% reduction in decoding time and over 90% win-ties in utility and safety benchmarks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation</title>
<link>https://arxiv.org/abs/2408.05456</link>
<guid>https://arxiv.org/abs/2408.05456</guid>
<content:encoded><![CDATA[
arXiv:2408.05456v2 Announce Type: replace 
Abstract: Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our Path-LLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than 90% of training paths on millions-scale graphs and runs at most 35x faster.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Negative Samples in Biomedical Generative Entity Linking</title>
<link>https://arxiv.org/abs/2408.16493</link>
<guid>https://arxiv.org/abs/2408.16493</guid>
<content:encoded><![CDATA[
arXiv:2408.16493v3 Announce Type: replace 
Abstract: Generative models have become widely used in biomedical entity linking (BioEL) due to their excellent performance and efficient memory usage. However, these models are usually trained only with positive samples, i.e., entities that match the input mention's identifier, and do not explicitly learn from hard negative samples, which are entities that look similar but have different meanings. To address this limitation, we introduce ANGEL (Learning from Negative Samples in Biomedical Generative Entity Linking), the first framework that trains generative BioEL models using negative samples. Specifically, a generative model is initially trained to generate positive entity names from the knowledge base for given input entities. Subsequently, both correct and incorrect outputs are gathered from the model's top-k predictions. Finally, the model is updated to prioritize the correct predictions through preference optimization. Our models outperform the previous best baseline models by up to an average top-1 accuracy of 1.4% on five benchmarks. When incorporating our framework into pre-training, the performance improvement increases further to 1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning stages. The code and model weights are available at https://github.com/dmis-lab/ANGEL.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCSBench: Evaluating Compositional Controllability in LLMs for Scientific Document Summarization</title>
<link>https://arxiv.org/abs/2410.12601</link>
<guid>https://arxiv.org/abs/2410.12601</guid>
<content:encoded><![CDATA[
arXiv:2410.12601v3 Announce Type: replace 
Abstract: To broaden the dissemination of scientific knowledge to diverse audiences, it is desirable for scientific document summarization systems to simultaneously control multiple attributes such as length and empirical focus. However, existing research typically focuses on controlling single attributes, leaving the compositional control of multiple attributes underexplored. To address this gap, we introduce CCSBench, the first evaluation benchmark for compositional controllable summarization in the scientific domain. Our benchmark enables fine-grained control over both explicit attributes (e.g., length), which are objective and straightforward, and implicit attributes (e.g., conceptual or empirical focus), which are more subjective and abstract. We conduct extensive experiments using various large language models (LLMs) under various settings, including in-context learning, parameter-efficient fine-tuning, and two-stage modular methods for balancing control over different attributes. Our findings reveal significant limitations in LLMs capabilities in balancing trade-offs between control attributes, especially implicit ones that require deeper understanding and abstract reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons</title>
<link>https://arxiv.org/abs/2411.01281</link>
<guid>https://arxiv.org/abs/2411.01281</guid>
<content:encoded><![CDATA[
arXiv:2411.01281v4 Announce Type: replace 
Abstract: As Large Language Models (LLMs) expand across domains, LLM judges have become essential for systems evaluation. Current benchmarks typically compare system outputs against baselines. This baseline-mediated approach, though convenient, yields lower reliability than direct comparison between systems. We propose Arena-Lite which integrates tournament structure on top of head-to-head comparison. The application of a tournament structure and direct comparison eliminates the need for baseline outputs, reduces the number of required comparisons, and allows higher reliability in system rankings. We conducted two experiments: (1) controlled stochastic modeling and (2) empirical validation with a real LLM judge. Those experiments collectively demonstrate that Arena-Lite consistently achieves higher reliability with fewer comparisons, even with smaller datasets or weaker judges. We release an easy-to-use web demonstration and code to foster adoption of Arena-Lite, streamlining model selection across research and industry communities. Arena-Lite demo and code are available on \href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training and Evaluating Language Models with Template-based Data Generation</title>
<link>https://arxiv.org/abs/2411.18104</link>
<guid>https://arxiv.org/abs/2411.18104</guid>
<content:encoded><![CDATA[
arXiv:2411.18104v4 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by employing GPT-4 for meta-template creation, guaranteeing diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. The code and data are available at https://github.com/iiis-ai/TemplateMath.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</title>
<link>https://arxiv.org/abs/2412.02252</link>
<guid>https://arxiv.org/abs/2412.02252</guid>
<content:encoded><![CDATA[
arXiv:2412.02252v2 Announce Type: replace 
Abstract: The rapid expansion of context window sizes in Large Language Models~(LLMs) has enabled them to tackle increasingly complex tasks involving lengthy documents. However, this progress comes at the cost of a substantial increase in memory usage during inference, primarily due to the linear growth of the key-value~(KV) cache. Existing KV cache compression methods often discard less relevant tokens, which can lead to significant performance degradation when critical information is lost. In this paper, we propose \textsc{PoD}~(Proximal tokens over Distant tokens), a novel KV cache compression framework that allocates memory according to token importance, retaining less important tokens in a more compact, shared form rather than discarding them entirely. Our approach is motivated by two key observations: (1) proximal tokens -- those at the beginning and end of the context -- are significantly more important for next-token prediction, and (2) attention scores for distant tokens are highly redundant across consecutive layers. Leveraging these insights, \textsc{PoD} preserves the full KV cache for proximal tokens, while for distant tokens, it shares key states across layers. Since attention scores are determined by both queries and keys, sharing key states enables multiple layers to reuse a single set of keys for distant tokens, substantially reducing KV cache memory without discarding essential context. We further introduce a lightweight post-training adaptation to enable the model to adjust to this new attention-sharing structure. Extensive experiments on both synthetic~(Needle in a Haystack) and real-world long-context benchmarks demonstrate that \textsc{PoD} reduces KV cache memory usage by up to 35\% without compromising performance. Our method is orthogonal to existing token-selection-based techniques and can be combined with them for further KV cache compression.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Context Aware Transformers for Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2412.12465</link>
<guid>https://arxiv.org/abs/2412.12465</guid>
<content:encoded><![CDATA[
arXiv:2412.12465v3 Announce Type: replace 
Abstract: Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities</title>
<link>https://arxiv.org/abs/2501.00571</link>
<guid>https://arxiv.org/abs/2501.00571</guid>
<content:encoded><![CDATA[
arXiv:2501.00571v5 Announce Type: replace 
Abstract: Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Evolving Critique Abilities in Large Language Models</title>
<link>https://arxiv.org/abs/2501.05727</link>
<guid>https://arxiv.org/abs/2501.05727</guid>
<content:encoded><![CDATA[
arXiv:2501.05727v2 Announce Type: replace 
Abstract: Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans. In such scenarios, leveraging the critique ability of LLMs themselves - identifying and correcting flaws - shows considerable promise. This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities. To address the low quality of naively generated data, we propose a contrastive-critic approach that uses reference solutions during data synthesis to enhance the model's understanding of key concepts, and incorporates a self-validation scheme to ensure data quality. The final trained model operates without any reference solutions at inference time. Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements across a wide range of benchmarks spanning both mathematical and scientific reasoning: achieving a 10.0\% relative gain in critique-correction accuracy and a 19.0\% relative improvement in error identification F1-score. Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Table Instruction Tuning</title>
<link>https://arxiv.org/abs/2501.14693</link>
<guid>https://arxiv.org/abs/2501.14693</guid>
<content:encoded><![CDATA[
arXiv:2501.14693v4 Announce Type: replace 
Abstract: Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. However, existing research has overlooked the impact of hyperparameter choices, and also lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. In this paper, we evaluate these abilities in existing table LLMs, and find significant declines in both out-of-domain table understanding and general capabilities as compared to their base models. Through systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities. Contrary to the previous table instruction-tuning work, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities. Based on our findings, we introduce TAMA, a TAble LLM instruction-tuned from LLaMA 3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. Our findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection. We open-source the project and our models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Scaling: Measuring and Predicting the Upper Bound of Knowledge Retention in Language Model Pre-Training</title>
<link>https://arxiv.org/abs/2502.04066</link>
<guid>https://arxiv.org/abs/2502.04066</guid>
<content:encoded><![CDATA[
arXiv:2502.04066v4 Announce Type: replace 
Abstract: The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task that directly reflects a model's internalized knowledge without the help of external tools. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. We then develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring additional training. Experimental results show that SMI outperforms co-occurrence-based baselines, achieving $R^2 > 0.75$ on models with over one billion parameters. Theoretical analysis further suggests an upper bound of around 80% QA accuracy under optimal pre-training, reflecting intrinsic memory limitations and motivating the use of retrieval or few-shot methods in later stages.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emergent Response Planning in LLMs</title>
<link>https://arxiv.org/abs/2502.06258</link>
<guid>https://arxiv.org/abs/2502.06258</guid>
<content:encoded><![CDATA[
arXiv:2502.06258v3 Announce Type: replace 
Abstract: In this work, we argue that large language models (LLMs), though trained to predict only the next token, exhibit emergent planning behaviors: $\textbf{their hidden representations encode future outputs beyond the next token}$. Through simple probing, we demonstrate that LLM prompt representations encode global attributes of their entire responses, including $\textit{structure attributes}$ (e.g., response length, reasoning steps), $\textit{content attributes}$ (e.g., character choices in storywriting, multiple-choice answers at the end of response), and $\textit{behavior attributes}$ (e.g., answer confidence, factual consistency). In addition to identifying response planning, we explore how it scales with model size across tasks and how it evolves during generation. The findings that LLMs plan ahead for the future in their hidden representations suggest potential applications for improving transparency and generation control.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.13207</link>
<guid>https://arxiv.org/abs/2502.13207</guid>
<content:encoded><![CDATA[
arXiv:2502.13207v2 Announce Type: replace 
Abstract: Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Dealing with this trade-off is still an open challenge in designing AI systems for creativity. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We show that our score can be used as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments considering a variety of creative tasks, such as poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Question Answering over Large Semi-structured Tables</title>
<link>https://arxiv.org/abs/2502.13422</link>
<guid>https://arxiv.org/abs/2502.13422</guid>
<content:encoded><![CDATA[
arXiv:2502.13422v2 Announce Type: replace 
Abstract: Table Question Answering (TableQA) attracts strong interests due to the prevalence of web information presented in the form of semi-structured tables. Despite many efforts, TableQA over large tables remains an open challenge. This is because large tables may overwhelm models that try to comprehend them in full to locate question answers. Recent studies reduce input table size by decomposing tables into smaller, question-relevant sub-tables via generating programs to parse the tables. However, such solutions are subject to program generation and execution errors and are difficult to ensure decomposition quality. To address this issue, we propose TaDRe, a TableQA model that incorporates both pre- and post-table decomposition refinements to ensure table decomposition quality, hence achieving highly accurate TableQA results. To evaluate TaDRe, we construct two new large-table TableQA benchmarks via LLM-driven table expansion and QA pair generation. Extensive experiments on both the new and public benchmarks show that TaDRe achieves state-of-the-art performance on large-table TableQA tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.14037</link>
<guid>https://arxiv.org/abs/2502.14037</guid>
<content:encoded><![CDATA[
arXiv:2502.14037v3 Announce Type: replace 
Abstract: Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, while potentially improving output diversity.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Illusion: The Failure of Instruction Hierarchies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15851</link>
<guid>https://arxiv.org/abs/2502.15851</guid>
<content:encoded><![CDATA[
arXiv:2502.15851v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. We find that LLMs more reliably obey constraints framed through natural social hierarchies (e.g., authority, expertise, consensus) than system/user roles, which suggests that pretraining-derived social structures act as latent control priors, with potentially stronger influence than post-training guardrails.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are Foundation Models Cooking in the Post-Soviet World?</title>
<link>https://arxiv.org/abs/2502.18583</link>
<guid>https://arxiv.org/abs/2502.18583</guid>
<content:encoded><![CDATA[
arXiv:2502.18583v2 Announce Type: replace 
Abstract: The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at https://github.com/alavrouk/BORSch.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Data Selection: The Data That Predicts Is the Data That Teaches</title>
<link>https://arxiv.org/abs/2503.00808</link>
<guid>https://arxiv.org/abs/2503.00808</guid>
<content:encoded><![CDATA[
arXiv:2503.00808v4 Announce Type: replace 
Abstract: Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., the normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks(Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning, which shares similar intuition with Thrush et al.(2024). To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation</title>
<link>https://arxiv.org/abs/2504.03165</link>
<guid>https://arxiv.org/abs/2504.03165</guid>
<content:encoded><![CDATA[
arXiv:2504.03165v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge injection during large language model (LLM) inference in recent years. However, due to their limited ability to exploit fine-grained inter-document relationships, current RAG implementations face challenges in effectively addressing the retrieved noise and redundancy content, which may cause error in the generation results. To address these limitations, we propose an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and Hallucination-Detection datasets. Experimental results show that our method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs</title>
<link>https://arxiv.org/abs/2504.07360</link>
<guid>https://arxiv.org/abs/2504.07360</guid>
<content:encoded><![CDATA[
arXiv:2504.07360v2 Announce Type: replace 
Abstract: The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.13626</link>
<guid>https://arxiv.org/abs/2504.13626</guid>
<content:encoded><![CDATA[
arXiv:2504.13626v2 Announce Type: replace 
Abstract: Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities on various tasks. However, LRMs often suffer from an ``overthinking'' problem, where the model generates excessively redundant reasoning steps with limited performance gains. In this work, we empirically reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (\texttt{} and \texttt{}) can effectively manipulate the model to generate fewer thoughts. Building on this finding, we propose a simple yet efficient pipeline, \Method, to enable LRMs to bypass unnecessary intermediate steps, thereby significantly reducing computational costs. We conduct extensive experiments to evaluate the utility and efficiency of \Method. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, \Method keeps the original performance while reducing output token counts by approximately 30\%, with minimal overhead introduced by the CoT generator. Furthermore, we identify two suboptimal modes, blindly following flawed external thoughts and unnecessary rethinking, and show that simple mitigations, such as difficulty-aware fallbacks, can further improve performance. Overall, \Method offers a practical, general, and efficient way to optimize LRM inference, making powerful reasoning models more accessible and scalable for real-world applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agree to Disagree? A Meta-Evaluation of LLM Misgendering</title>
<link>https://arxiv.org/abs/2504.17075</link>
<guid>https://arxiv.org/abs/2504.17075</guid>
<content:encoded><![CDATA[
arXiv:2504.17075v2 Announce Type: replace 
Abstract: Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination</title>
<link>https://arxiv.org/abs/2505.00008</link>
<guid>https://arxiv.org/abs/2505.00008</guid>
<content:encoded><![CDATA[
arXiv:2505.00008v2 Announce Type: replace 
Abstract: Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.
  Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.
  Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.
  Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do MLLMs Capture How Interfaces Guide User Behavior? A Benchmark for Multimodal UI/UX Design Understanding</title>
<link>https://arxiv.org/abs/2505.05026</link>
<guid>https://arxiv.org/abs/2505.05026</guid>
<content:encoded><![CDATA[
arXiv:2505.05026v3 Announce Type: replace 
Abstract: User interface (UI) design goes beyond visuals, guiding user behavior and overall user experience (UX). Strategically crafted interfaces, for example, can boost sign-ups and drive business sales, underscoring the shift toward UI/UX as a unified design concept. While recent studies have explored UI quality evaluation using Multimodal Large Language Models (MLLMs), they largely focus on surface-level features, overlooking behavior-oriented aspects. To fill this gap, we introduce WiserUI-Bench, a novel benchmark for assessing models' multimodal understanding of UI/UX design. It includes 300 diverse real-world UI image pairs, each consisting of two design variants A/B-tested at scale by actual companies, where one was empirically validated to steer more user actions than the other. Each pair is accompanied one or more of 684 expert-curated rationales that capture key factors behind each winning design's effectiveness, spanning diverse cognitive dimensions of UX. Our benchmark supports two core tasks: (1) selecting the more effective UI/UX design by predicting the A/B test verified winner and (2) assessing how well a model, given the winner, can explain its effectiveness in alignment with expert reasoning. Experiments across several MLLMs show that current models exhibit limited nuanced reasoning about UI/UX design and its behavioral impact. We believe our work will foster research in UI/UX understanding and enable broader applications such as behavior-aware interface optimization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction</title>
<link>https://arxiv.org/abs/2505.10939</link>
<guid>https://arxiv.org/abs/2505.10939</guid>
<content:encoded><![CDATA[
arXiv:2505.10939v2 Announce Type: replace 
Abstract: Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XtraGPT: Context-Aware and Controllable Academic Paper Revision via Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2505.11336</link>
<guid>https://arxiv.org/abs/2505.11336</guid>
<content:encoded><![CDATA[
arXiv:2505.11336v2 Announce Type: replace 
Abstract: Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited when it comes to supporting high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision. We first introduce a comprehensive dataset of 7,040 research papers from top-tier venues annotated with over 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. Building on the dataset, we develop XtraGPT, the first suite of open-source LLMs, designed to provide context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B parameters. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of our models in improving scientific drafts.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing</title>
<link>https://arxiv.org/abs/2505.11935</link>
<guid>https://arxiv.org/abs/2505.11935</guid>
<content:encoded><![CDATA[
arXiv:2505.11935v2 Announce Type: replace 
Abstract: Although multimodal large language models (MLLMs) show promise in generating chart rendering code, editing charts via code presents a greater challenge. This task demands MLLMs to integrate chart understanding and reasoning capacities, which are labor-intensive. While many MLLMs claim such editing capabilities, current evaluations rely on limited case studies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose \textsc{ChartEdit}, a novel benchmark designed for chart editing tasks, featuring $1405$ diverse editing instructions applied to $233$ real-world charts, each manually annotated and validated for accuracy. Utilizing \textsc{ChartEdit}, we evaluate the performance of 10 mainstream MLLMs across two types of experiments at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Generalization vs Fidelity Paradox in Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.15442</link>
<guid>https://arxiv.org/abs/2505.15442</guid>
<content:encoded><![CDATA[
arXiv:2505.15442v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a key technique for compressing large language models into smaller ones while preserving performance. Despite the recent traction of KD research, its effectiveness for smaller language models (LMs) and the mechanisms driving knowledge transfer remain underexplored. In this work, we present the first large-scale empirical and statistical analysis of KD across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks in a zero-shot setting. Our findings reveal that KD can improve the average performance of smaller models by up to $10\%$, with a peak task specific gain of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger models. Surprisingly, teacher performance has a minimal impact on student outcomes, while teacher task expertise impacts KD effectiveness. A correlation study indicates that smaller LMs benefit more from KD, whereas larger LMs show diminished gains. Additionally, we uncover a misalignment between improvements in student performance and reasoning fidelity, suggesting that while KD enhances accuracy, it does not always maintain the structured decision-making processes of the teacher. Our ablation study further highlights the importance of teacher signals and logit smoothing in influencing students' performance after distillation. Overall, our study offers a comprehensive empirical and statistical assessment of KD, highlighting both its benefits and trade-offs when distilling knowledge from larger to smaller LMs.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Tokens Are What You Need In Thinking</title>
<link>https://arxiv.org/abs/2505.17827</link>
<guid>https://arxiv.org/abs/2505.17827</guid>
<content:encoded><![CDATA[
arXiv:2505.17827v2 Announce Type: replace 
Abstract: Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit impressive problem-solving capabilities but suffer from critical inefficiencies: high inference latency, excessive computational resource consumption, and a tendency toward overthinking -- generating verbose chains of thought (CoT) laden with redundant tokens that contribute minimally to the final answer. To address these issues, we propose Conditional Token Selection (CTS), a token-level compression framework with a flexible and variable compression ratio that identifies and preserves only the most essential tokens in CoT. CTS evaluates each token's contribution to deriving correct answers using conditional importance scoring, then trains models on compressed CoT. Extensive experiments demonstrate that CTS effectively compresses long CoT while maintaining strong reasoning performance. Notably, on the GPQA benchmark, Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with 13.2% fewer reasoning tokens (13% training token reduction). Further reducing training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a 75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy in existing CoT.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shifting AI Efficiency From Model-Centric to Data-Centric Compression</title>
<link>https://arxiv.org/abs/2505.19147</link>
<guid>https://arxiv.org/abs/2505.19147</guid>
<content:encoded><![CDATA[
arXiv:2505.19147v2 Announce Type: replace 
Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \textbf{we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression}. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's High Time: A Survey of Temporal Question Answering</title>
<link>https://arxiv.org/abs/2505.20243</link>
<guid>https://arxiv.org/abs/2505.20243</guid>
<content:encoded><![CDATA[
arXiv:2505.20243v3 Announce Type: replace 
Abstract: Time plays a critical role in how information is generated, retrieved, and interpreted. In this survey, we provide a comprehensive overview of Temporal Question Answering (TQA), a research area that focuses on answering questions involving temporal constraints or context. As the amount of time-stamped content from sources like news articles, web archives, and knowledge bases increases, systems must address challenges such as detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. We focus on recent advances in TQA enabled by neural architectures, especially transformer-based models and Large Language Models (LLMs), highlighting progress in temporal language modeling, retrieval-augmented generation (RAG), and temporal reasoning. We also discuss benchmark datasets and evaluation strategies designed to test temporal robustness, recency awareness, and generalization.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affordance Benchmark for MLLMs</title>
<link>https://arxiv.org/abs/2506.00893</link>
<guid>https://arxiv.org/abs/2506.00893</guid>
<content:encoded><![CDATA[
arXiv:2506.00893v2 Announce Type: replace 
Abstract: Affordance theory suggests that environments inherently provide action possibilities shaping perception and behavior. While Multimodal Large Language Models (MLLMs) achieve strong performance in vision-language tasks, their ability to perceive affordance, which is crucial for intuitive and safe interactions, remains underexplored. To address this, we introduce **A4Bench**, a novel benchmark designed to evaluate the affordance perception abilities of MLLMs across two dimensions: 1) Constitutive Affordance, assessing understanding of inherent object properties through 1,282 questionanswer pairs spanning nine sub-disciplines, and 2) Transformative Affordance, probing dynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or individual-specific affordance) with 718 challenging question-answer pairs. We evaluate 17 MLLMs (nine proprietary and eight open-source) and compare them to human performance. Results show that proprietary models generally outperform open-source ones, yet all models perform far below humans, especially in transformative affordance. Furthermore, even top-performing models, such as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind human performance (best: 85.34%, worst: 81.25%). These findings highlight critical gaps in environmental understanding of MLLMs and provide a foundation for advancing AI systems toward more robust, context-aware interactions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for Clinical Notes</title>
<link>https://arxiv.org/abs/2506.05386</link>
<guid>https://arxiv.org/abs/2506.05386</guid>
<content:encoded><![CDATA[
arXiv:2506.05386v2 Announce Type: replace 
Abstract: Clinical note generation aims to produce free-text summaries of a patient's condition and diagnostic process, with discharge instructions being a representative long-form example. While recent LLM-based methods pre-trained on general clinical corpora show promise in clinical text generation, they fall short in producing long-form notes from limited patient information. In this paper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG) for long-form discharge instructions based on pre-admission information. ReinRAG retrieves reasoning paths from a medical knowledge graph to provide explicit semantic guidance to the LLM. To bridge the information gap, we propose group-based retriever optimization (GRO) which improves retrieval quality with group-normalized rewards, encouraging reasoning leaps for deeper inference by the LLM. Comprehensive experiments on the real-world dataset show that ReinRAG outperforms baselines in both clinical efficacy and natural language generation metrics. Further analysis reveals that ReinRAG fills semantic gaps in sparse input scenarios, and retrieved reasoning paths help LLMs avoid clinical misinterpretation by focusing on key evidence and following coherent reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval</title>
<link>https://arxiv.org/abs/2506.08625</link>
<guid>https://arxiv.org/abs/2506.08625</guid>
<content:encoded><![CDATA[
arXiv:2506.08625v2 Announce Type: replace 
Abstract: Scientific reasoning requires not only long-chain reasoning processes, but also knowledge of domain-specific terminologies and adaptation to updated findings. To deal with these challenges for scientific reasoning, we introduce RAISE, a step-by-step retrieval-augmented framework which retrieves logically relevant documents from in-the-wild corpus. RAISE is divided into three steps: problem decomposition, logical query generation, and logical retrieval. We observe that RAISE consistently outperforms other baselines on scientific reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves documents that are not only similar in terms of the domain knowledge, but also documents logically more relevant.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extrapolation by Association: Length Generalization Transfer in Transformers</title>
<link>https://arxiv.org/abs/2506.09251</link>
<guid>https://arxiv.org/abs/2506.09251</guid>
<content:encoded><![CDATA[
arXiv:2506.09251v2 Announce Type: replace 
Abstract: Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \textit{task association}. We find that length generalization can be \textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2506.14758</link>
<guid>https://arxiv.org/abs/2506.14758</guid>
<content:encoded><![CDATA[
arXiv:2506.14758v2 Announce Type: replace 
Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing large language model (LLM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LLMs. Through empirical analysis, we uncover positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LLMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LLM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning</title>
<link>https://arxiv.org/abs/2506.16123</link>
<guid>https://arxiv.org/abs/2506.16123</guid>
<content:encoded><![CDATA[
arXiv:2506.16123v2 Announce Type: replace 
Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting framework that embeds domain-specific expert financial reasoning blueprints to guide large language models' behaviors. We identify three main prompting styles in financial NLP (FinNLP): (1) standard prompting (zero-shot), (2) unstructured CoT (free-form reasoning), and (3) structured CoT (with explicitly structured reasoning steps). Prior work has mainly focused on the first two, while structured CoT remains underexplored and lacks domain expertise incorporation. Therefore, we evaluate all three prompting approaches across ten CFA-style financial domains and introduce FinCoT as the first structured finance-specific prompting approach incorporating blueprints from domain experts. FinCoT improves the accuracy of a general-purpose model, Qwen3-8B-Base, from 63.2% to 80.5%, and boosts Fin-R1 (7B), a finance-specific model, from 65.7% to 75.7%, while reducing output length by up to 8.9x and 1.16x compared to structured CoT methods, respectively. We find that FinCoT proves most effective for models lacking financial post-training. Our findings show that FinCoT does not only improve performance and reduce inference costs but also yields more interpretable and expert-aligned reasoning traces.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning</title>
<link>https://arxiv.org/abs/2506.16792</link>
<guid>https://arxiv.org/abs/2506.16792</guid>
<content:encoded><![CDATA[
arXiv:2506.16792v2 Announce Type: replace 
Abstract: Despite efforts to align large language models (LLMs) with societal and moral values, these models remain susceptible to jailbreak attacks -- methods designed to elicit harmful responses. Jailbreaking black-box LLMs is considered challenging due to the discrete nature of token inputs, restricted access to the target LLM, and limited query budget. To address the issues above, we propose an effective method for jailbreaking black-box large language Models via Iterative Semantic Tuning, named MIST. MIST enables attackers to iteratively refine prompts that preserve the original semantic intent while inducing harmful content. Specifically, to balance semantic similarity with computational efficiency, MIST incorporates two key strategies: sequential synonym search, and its advanced version -- order-determining optimization. We conduct extensive experiments on two datasets using two open-source and four closed-source models. Results show that MIST achieves competitive attack success rate, relatively low query count, and fair transferability, outperforming or matching state-of-the-art jailbreak methods. Additionally, we conduct analysis on computational efficiency to validate the practical viability of MIST.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Reasoning Help Large Language Models Capture Human Annotator Disagreement?</title>
<link>https://arxiv.org/abs/2506.19467</link>
<guid>https://arxiv.org/abs/2506.19467</guid>
<content:encoded><![CDATA[
arXiv:2506.19467v2 Announce Type: replace 
Abstract: Variation in human annotation (i.e., disagreements) is common in NLP, often reflecting important information like task subjectivity and sample ambiguity. Modeling this variation is important for applications that are sensitive to such information. Although RLVR-style reasoning (Reinforcement Learning with Verifiable Rewards) has improved Large Language Model (LLM) performance on many tasks, it remains unclear whether such reasoning enables LLMs to capture informative variation in human annotation. In this work, we evaluate the influence of different reasoning settings on LLM disagreement modeling. We systematically evaluate each reasoning setting across model sizes, distribution expression methods, and steering methods, resulting in 60 experimental setups across 3 tasks. Surprisingly, our results show that RLVR-style reasoning degrades performance in disagreement modeling, while naive Chain-of-Thought (CoT) reasoning improves the performance of RLHF LLMs (RL from human feedback). These findings underscore the potential risk of replacing human annotators with reasoning LLMs, especially when disagreements are important.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction</title>
<link>https://arxiv.org/abs/2506.21562</link>
<guid>https://arxiv.org/abs/2506.21562</guid>
<content:encoded><![CDATA[
arXiv:2506.21562v2 Announce Type: replace 
Abstract: In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive 'next token prediction' mechanism commonly used in large language models, and propose a novel 'next room prediction' paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSynth: Heterogeneous Linguistic Signals for News Classification</title>
<link>https://arxiv.org/abs/2506.21848</link>
<guid>https://arxiv.org/abs/2506.21848</guid>
<content:encoded><![CDATA[
arXiv:2506.21848v3 Announce Type: replace 
Abstract: Deep learning has significantly advanced NLP, but its reliance on large black-box models introduces critical interpretability and computational efficiency concerns. This paper proposes LinguaSynth, a novel text classification framework that strategically integrates five complementary linguistic feature types: lexical, syntactic, entity-level, word-level semantics, and document-level semantics within a transparent logistic regression model. Unlike transformer-based architectures, LinguaSynth maintains interpretability and computational efficiency, achieving an accuracy of 84.89 percent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by 3.32 percent. Through rigorous feature interaction analysis, we show that syntactic and entity-level signals provide essential disambiguation and effectively complement distributional semantics. LinguaSynth sets a new benchmark for interpretable, resource-efficient NLP models and challenges the prevailing assumption that deep neural networks are necessary for high-performing text classification.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What to Keep and What to Drop: Adaptive Table Filtering Framework</title>
<link>https://arxiv.org/abs/2506.23463</link>
<guid>https://arxiv.org/abs/2506.23463</guid>
<content:encoded><![CDATA[
arXiv:2506.23463v3 Announce Type: replace 
Abstract: Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by 70%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks. Our code available at: https://github.com/torijune/ATF-Adaptive-Table-Filtering-Framework
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizing Verifiable Instruction Following</title>
<link>https://arxiv.org/abs/2507.02833</link>
<guid>https://arxiv.org/abs/2507.02833</guid>
<content:encoded><![CDATA[
arXiv:2507.02833v2 Announce Type: replace 
Abstract: A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexOlmo: Open Language Models for Flexible Data Use</title>
<link>https://arxiv.org/abs/2507.07024</link>
<guid>https://arxiv.org/abs/2507.07024</guid>
<content:encoded><![CDATA[
arXiv:2507.07024v3 Announce Type: replace 
Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distillation versus Contrastive Learning: How to Train Your Rerankers</title>
<link>https://arxiv.org/abs/2507.08336</link>
<guid>https://arxiv.org/abs/2507.08336</guid>
<content:encoded><![CDATA[
arXiv:2507.08336v2 Announce Type: replace 
Abstract: Training effective text rerankers is crucial for information retrieval. Two strategies are widely used: contrastive learning (optimizing directly on ground-truth labels) and knowledge distillation (transferring knowledge from a larger reranker). While both have been studied extensively, a clear comparison of their effectiveness for training cross-encoder rerankers under practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of different sizes and architectures using both methods on the same data, with a strong contrastive learning model acting as the distillation teacher. Our results show that knowledge distillation generally yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model. This finding is consistent across student model sizes and architectures. However, distilling from a teacher of the same capacity does not provide the same advantage, particularly for out-of-domain tasks. These findings offer practical guidance for choosing a training strategy based on available teacher models. We recommend using knowledge distillation to train smaller rerankers if a larger, more powerful teacher is accessible; in its absence, contrastive learning remains a robust baseline.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models</title>
<link>https://arxiv.org/abs/2507.09506</link>
<guid>https://arxiv.org/abs/2507.09506</guid>
<content:encoded><![CDATA[
arXiv:2507.09506v2 Announce Type: replace 
Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities in long-context understanding tasks. Among these, long-context referencing -- a crucial task that requires LCLMs to attribute items of interest to specific parts of long-context data -- remains underexplored. To bridge this gap, this paper proposes Referencing Evaluation for Long-context Language Models (Ref-Long), a novel benchmark designed to assess the long-context referencing capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the indexes of documents that reference a specific key, emphasizing contextual relationships between the key and the documents over simple retrieval. Based on the task design, we construct three subsets ranging from synthetic to realistic scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs reveal significant shortcomings in long-context referencing, even among advanced models like GPT-4o. To further investigate these challenges, we conduct comprehensive analyses, including human evaluations, task format adjustments, fine-tuning experiments, and error analyses, leading to several key insights. Our data and code can be found in https://github. com/wujunjie1998/Ref-Long.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs on Trial: Evaluating Judicial Fairness for Large Language Models</title>
<link>https://arxiv.org/abs/2507.10852</link>
<guid>https://arxiv.org/abs/2507.10852</guid>
<content:encoded><![CDATA[
arXiv:2507.10852v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields where their decisions impact rights and equity. However, LLMs' judicial fairness and implications for social justice remain underexplored. When LLMs act as judges, the ability to fairly resolve judicial issues is a prerequisite to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. Applying this framework to the judicial system, we compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. Particularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit containing all datasets and code, designed to support future research in evaluating and improving LLM fairness.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description</title>
<link>https://arxiv.org/abs/2405.18937</link>
<guid>https://arxiv.org/abs/2405.18937</guid>
<content:encoded><![CDATA[
arXiv:2405.18937v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a challenging task aimed at advancing 3D multimodal learning for fine-grained, part-aware segmentation grounding and detailed explanation of 3D objects. Existing 3D datasets largely focus on either vision-only part segmentation or vision-language scene segmentation, lacking the fine-grained multimodal segmentation needed for robotic navigation and interaction in real-world environments. To address this gap, we present the 3DCoMPaT Grounded Instructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich point cloud descriptions with corresponding part-level segmentation masks. This dataset encompasses extensive samples designed for both PaPGD and fine-grained single-part grounding tasks. To tackle the inherent challenges of grounding objects and generating grounded descriptions at the part level, we propose Kestrel, a part-aware 3D multimodal large language model that integrates an advanced language model for nuanced language comprehension with multi-level point feature propagation and query refinement mechanism to enhance spatial reasoning at the part level. The extensive experiments demonstrate that Kestrel effectively bridges the gap between part-aware language understanding and 3D segmentation grounding, paving the way for more robust and interpretable 3D object comprehension that meets the demands of real-world robotic applications. Project page at https://feielysia.github.io/Kestrel.github.io/
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoLLaMB: Long Streaming Video Understanding with Recurrent Memory Bridges</title>
<link>https://arxiv.org/abs/2409.01071</link>
<guid>https://arxiv.org/abs/2409.01071</guid>
<content:encoded><![CDATA[
arXiv:2409.01071v2 Announce Type: replace-cross 
Abstract: Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel and efficient framework for long video understanding that leverages recurrent memory bridges and temporal memory tokens to enable seamless encoding of entire video sequences with preserved semantic continuity. Central to our approach is a SceneTiling algorithm that segments videos into coherent semantic units, facilitating robust understanding across tasks without requiring additional training. VideoLLaMB achieves state-of-the-art performance, surpassing existing models by 4.2 points on four VideoQA benchmarks and by 2.06 points on egocentric planning tasks. Notably, it maintains strong performance under extreme video length scaling (up to 8 times) and excels at fine-grained frame retrieval on our proposed Needle in a Video Haystack (NIAVH) benchmark. With linear GPU memory scaling, VideoLLaMB processes up to 320 frames using a single Nvidia A100 GPU, despite being trained on only 16 frames-offering an unprecedented balance of accuracy, scalability, and cost-effectiveness. This makes it highly accessible and practical for the academic community.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Test-Time Adaptation for Personalized Child Speech Recognition</title>
<link>https://arxiv.org/abs/2409.13095</link>
<guid>https://arxiv.org/abs/2409.13095</guid>
<content:encoded><![CDATA[
arXiv:2409.13095v2 Announce Type: replace-cross 
Abstract: Automatic speech recognition (ASR) models often experience performance degradation due to data domain shifts introduced at test time, a challenge that is further amplified for child speakers. Test-time adaptation (TTA) methods have shown great potential in bridging this domain gap. However, the use of TTA to adapt ASR models to the individual differences in each child's speech has not yet been systematically studied. In this work, we investigate the effectiveness of two widely used TTA methods-SUTA, SGEM-in adapting off-the-shelf ASR models and their fine-tuned versions for child speech recognition, with the goal of enabling continuous, unsupervised adaptation at test time. Our findings show that TTA significantly improves the performance of both off-the-shelf and fine-tuned ASR models, both on average and across individual child speakers, compared to unadapted baselines. However, while TTA helps adapt to individual variability, it may still be limited with non-linguistic child speech.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-based Audio Moment Retrieval</title>
<link>https://arxiv.org/abs/2409.15672</link>
<guid>https://arxiv.org/abs/2409.15672</guid>
<content:encoded><![CDATA[
arXiv:2409.15672v3 Announce Type: replace-cross 
Abstract: In this paper, we propose and design a new task called audio moment retrieval (AMR). Unlike conventional language-based audio retrieval tasks that search for short audio clips from an audio database, AMR aims to predict relevant moments in untrimmed long audio based on a text query. Given the lack of prior work in AMR, we first build a dedicated dataset, Clotho-Moment, consisting of large-scale simulated audio recordings with moment annotations. We then propose a DETR-based model, named Audio Moment DETR (AM-DETR), as a fundamental framework for AMR tasks. This model captures temporal dependencies within audio features, inspired by similar video moment retrieval tasks, thus surpassing conventional clip-level audio retrieval methods. Additionally, we provide manually annotated datasets to properly measure the effectiveness and robustness of our methods on real data. Experimental results show that AM-DETR, trained with Clotho-Moment, outperforms a baseline model that applies a clip-level audio retrieval method with a sliding window on all metrics, particularly improving Recall1@0.7 by 9.00 points. Our datasets and code are publicly available in https://h-munakata.github.io/Language-based-Audio-Moment-Retrieval.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CheXalign: Preference fine-tuning in chest X-ray interpretation models without human feedback</title>
<link>https://arxiv.org/abs/2410.07025</link>
<guid>https://arxiv.org/abs/2410.07025</guid>
<content:encoded><![CDATA[
arXiv:2410.07025v3 Announce Type: replace-cross 
Abstract: Radiologists play a crucial role in translating medical images into actionable reports. However, the field faces staffing shortages and increasing workloads. While automated approaches using vision-language models (VLMs) show promise as assistants, they require exceptionally high accuracy. Most current VLMs in radiology rely solely on supervised fine-tuning. Meanwhile, additional preference fine-tuning in the post-training pipeline has become standard practice in the general domain. The challenge in radiology lies in the prohibitive cost of obtaining radiologist feedback at scale. To address this challenge, we propose an automated pipeline for preference feedback, focusing on chest X-ray radiology report generation (RRG). Specifically, our method leverages publicly available datasets containing pairs of images and radiologist-written reference reports with reference-based metrics, or Judges, eliminating the need for additional radiologist feedback. We investigate reward overoptimization via length exploitation in this setting and introduce a length-controlled version of the GREEN score. Our best-performing setup achieves state-of-the-art CheXbert scores on the MIMIC-CXR dataset for the RRG task while on average maintaining robust performance across six additional image perception and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More than Memes: A Multimodal Topic Modeling Approach to Conspiracy Theories on Telegram</title>
<link>https://arxiv.org/abs/2410.08642</link>
<guid>https://arxiv.org/abs/2410.08642</guid>
<content:encoded><![CDATA[
arXiv:2410.08642v3 Announce Type: replace-cross 
Abstract: To address the increasing prevalence of (audio-)visual data on social media, and to capture the evolving and dynamic nature of this communication, researchers have begun to explore the potential of unsupervised approaches for analyzing multimodal online content. However, existing research often neglects visual content beyond memes, and in addition lacks methods to compare topic models across modalities. Our study addresses these gaps by applying multimodal topic modeling for analyzing conspiracy theories in German-language Telegram channels. We use BERTopic with CLIP for the analysis of textual and visual data in a corpus of ~40, 000 Telegram messages posted in October 2023 in 571 German-language Telegram channels known for disseminating conspiracy theories. Through this dataset, we provide insights into unimodal and multimodal topic models by analyzing symmetry and intersections of topics across modalities. We demonstrate the variety of textual and visual content shared in the channels discovered through the topic modeling, and propose a conceptual framework for the analysis of textual and visual discursive strategies in the communication of conspiracy theories. We apply the framework in a case study of the topic group Israel Gaza.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiDoRA: Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2410.09758</link>
<guid>https://arxiv.org/abs/2410.09758</guid>
<content:encoded><![CDATA[
arXiv:2410.09758v2 Announce Type: replace-cross 
Abstract: Parameter-efficient fine-tuning (PEFT) is a flexible and efficient method for adapting large language models (LLMs) to downstream tasks. Among these methods, weight-decomposed low-rank adaptation (DoRA) is a promising approach that decomposes weight matrices into magnitude and direction components to mimic full fine-tuning (FT) better. However, DoRA's simultaneous optimization of these components makes it over-expressive, increases the risk of overfitting, and creates a coupled updating pattern that limits its learning capacity. To address these issues, we propose Bi-level Optimization-Based Weight-Decomposed Low-Rank Adaptation (BiDoRA), a novel PEFT method based on a bi-level optimization framework. BiDoRA fundamentally differs from DoRA by optimizing the magnitude and direction in two separate, asynchronous loops using distinct training and validation data splits. This decoupled optimization process effectively mitigates overfitting and allows for more flexible updates that align even more closely with FT. For instance, weight decomposition analysis shows BiDoRA achieves a magnitude-direction update correlation of $-8.042$, significantly closer to the FT ideal compared to $-1.784$ for DoRA. Evaluation of BiDoRA on diverse tasks spanning natural language understanding, generation, token classification, and extremely small biomedical datasets reveals that it consistently outperforms DoRA and a wide range of leading PEFT methods. This improvement is statistically significant, as demonstrated on the GLUE benchmark where BiDoRA surpasses DoRA with a p-value of $2.4\times10^{-4}$ in terms of the Wilcoxon signed-rank test. The code for BiDoRA is available at https://github.com/t2ance/BiDoRA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2411.04358</link>
<guid>https://arxiv.org/abs/2411.04358</guid>
<content:encoded><![CDATA[
arXiv:2411.04358v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are highly resource-intensive to fine-tune due to their enormous size. While low-rank adaptation is a prominent parameter-efficient fine-tuning approach, it suffers from sensitivity to hyperparameter choices, leading to instability in model performance on fine-tuning downstream tasks. This paper highlights the importance of effective parameterization in low-rank fine-tuning to reduce estimator variance and enhance the stability of final model outputs. We propose MonteCLoRA, an efficient fine-tuning technique that employs Monte Carlo estimation to learn an unbiased posterior estimation of low-rank parameters with low expected variance, stabilizing fine-tuned LLMs with only O(r) additional parameters, for a given rank r. MonteCLoRA shows 0.5% and 1.6% improvements in accuracy and robustness over unregularized low-rank adaptation method on natural language understanding tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with pre-trained LLaMA-1-7B and LLaMA-3.2-3B-Instruct, MonteCLoRA demonstrates robust performance with 50% and 62% lower spreads respectively than the contemporary efficient fine-tuning methods. The theoretical and empirical results presented in the paper underscore how parameterization and hyperpriors balance exploration-exploitation in the low-rank parametric space, therefore leading to more optimal and robust parameter estimation during efficient fine-tuning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Guide to Misinformation Detection Data and Evaluation</title>
<link>https://arxiv.org/abs/2411.05060</link>
<guid>https://arxiv.org/abs/2411.05060</guid>
<content:encoded><![CDATA[
arXiv:2411.05060v5 Announce Type: replace-cross 
Abstract: Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at https://misinfo-datasets.complexdatalab.com/.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture</title>
<link>https://arxiv.org/abs/2412.15113</link>
<guid>https://arxiv.org/abs/2412.15113</guid>
<content:encoded><![CDATA[
arXiv:2412.15113v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million and 1 billion parameters, focusing on attention head values, with results also indicating improved performance at these larger and more naturalistic scales.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gandalf the Red: Adaptive Security for LLMs</title>
<link>https://arxiv.org/abs/2501.07927</link>
<guid>https://arxiv.org/abs/2501.07927</guid>
<content:encoded><![CDATA[
arXiv:2501.07927v3 Announce Type: replace-cross 
Abstract: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2501.16607</link>
<guid>https://arxiv.org/abs/2501.16607</guid>
<content:encoded><![CDATA[
arXiv:2501.16607v2 Announce Type: replace-cross 
Abstract: Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at translating natural language questions into SQL queries. While recent advances in large language models have greatly improved performance, most existing approaches depend on models with tens of billions of parameters or costly APIs, limiting their applicability in resource-constrained environments. For real world, especially on edge devices, it is crucial for Text-to-SQL to ensure cost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL is of great practical significance. However, smaller LLMs often struggle with complicated user instruction, redundant schema linking or syntax correctness. To address these challenges, we propose MCTS-SQL, a novel framework that uses Monte Carlo Tree Search to guide SQL generation through multi-step refinement. Since the light-weight models' weak performance of single-shot prediction, we generate better results through several trials with feedback. However, directly applying MCTS-based methods inevitably leads to significant time and computational overhead. Driven by this issue, we propose a token-level prefix-cache mechanism that stores prior information during iterations, effectively improved the execution speed. Experiments results on the SPIDER and BIRD benchmarks demonstrate the effectiveness of our approach. Using a small open-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When leveraging a more powerful model Gemini 2.5 to explore the performance upper bound, we achieved results competitive with the SOTA. Our findings demonstrate that even small models can be effectively deployed in practical Text-to-SQL systems with the right strategy.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title>
<link>https://arxiv.org/abs/2502.04322</link>
<guid>https://arxiv.org/abs/2502.04322</guid>
<content:encoded><![CDATA[
arXiv:2502.04322v3 Announce Type: replace-cross 
Abstract: Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety</title>
<link>https://arxiv.org/abs/2502.05206</link>
<guid>https://arxiv.org/abs/2502.05206</guid>
<content:encoded><![CDATA[
arXiv:2502.05206v5 Announce Type: replace-cross 
Abstract: The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science</title>
<link>https://arxiv.org/abs/2502.16395</link>
<guid>https://arxiv.org/abs/2502.16395</guid>
<content:encoded><![CDATA[
arXiv:2502.16395v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows - the logical plans guiding code generation. However, it remains unclear how to assess whether a LLM-generated workflow supports reproducible implementations.
  To address this, we present $\it{AIRepr}$, an $\it{A}$nalyst - $\it{I}$nspector framework for automatically evaluating and improving the $\it{Repr}$oducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for more transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JurisTCU: A Brazilian Portuguese Information Retrieval Dataset with Query Relevance Judgments</title>
<link>https://arxiv.org/abs/2503.08379</link>
<guid>https://arxiv.org/abs/2503.08379</guid>
<content:encoded><![CDATA[
arXiv:2503.08379v2 Announce Type: replace-cross 
Abstract: This paper introduces JurisTCU, a Brazilian Portuguese dataset for legal information retrieval (LIR). The dataset is freely available and consists of 16,045 jurisprudential documents from the Brazilian Federal Court of Accounts, along with 150 queries annotated with relevance judgments. It addresses the scarcity of Portuguese-language LIR datasets with query relevance annotations. The queries are organized into three groups: real user keyword-based queries, synthetic keyword-based queries, and synthetic question-based queries. Relevance judgments were produced through a hybrid approach combining LLM-based scoring with expert domain validation. We used JurisTCU in 14 experiments using lexical search (document expansion methods) and semantic search (BERT-based and OpenAI embeddings). We show that the document expansion methods significantly improve the performance of standard BM25 search on this dataset, with improvements exceeding 45% in P@10, R@10, and nDCG@10 metrics when evaluating short keyword-based queries. Among the embedding models, the OpenAI models produced the best results, with improvements of approximately 70% in P@10, R@10, and nDCG@10 metrics for short keyword-based queries, suggesting that these dense embeddings capture semantic relationships in this domain, surpassing the reliance on lexical terms. Besides offering a dataset for the Portuguese-language IR research community, suitable for evaluating search systems, the results also contribute to enhancing a search system highly relevant to Brazilian citizens.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-VL: Learning to Reason with Multimodal Large Language Models via Step-wise Group Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2503.12937</link>
<guid>https://arxiv.org/abs/2503.12937</guid>
<content:encoded><![CDATA[
arXiv:2503.12937v2 Announce Type: replace-cross 
Abstract: Recent studies generally enhance MLLMs' reasoning capabilities via supervised fine-tuning on high-quality chain-of-thought reasoning data, which often leads models to merely imitate successful reasoning paths without understanding what the wrong reasoning paths are. In this work, we aim to enhance the MLLMs' reasoning ability beyond passively imitating positive reasoning paths. To this end, we design Step-wise Group Relative Policy Optimization (StepGRPO), a new online reinforcement learning framework that enables MLLMs to self-improve reasoning ability via simple, effective and dense step-wise rewarding. Specifically, StepGRPO introduces two novel rule-based reasoning rewards: Step-wise Reasoning Accuracy Reward (StepRAR) and Step-wise Reasoning Validity Reward (StepRVR). StepRAR rewards the reasoning paths that contain necessary intermediate reasoning steps via a soft key-step matching technique, while StepRAR rewards reasoning paths that follow a well-structured and logically consistent reasoning process through a reasoning completeness and logic evaluation strategy. With the proposed StepGRPO, we introduce R1-VL, a series of MLLMs with outstanding capabilities in step-by-step reasoning. Extensive experiments over 8 benchmarks demonstrate the superiority of our methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StableGS: A Floater-Free Framework for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.18458</link>
<guid>https://arxiv.org/abs/2503.18458</guid>
<content:encoded><![CDATA[
arXiv:2503.18458v3 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting (3DGS) reconstructions are plagued by stubborn ``floater" artifacts that degrade their geometric and visual fidelity. We are the first to reveal the root cause: a fundamental conflict in the 3DGS optimization process where the opacity gradients of floaters vanish when their blended color reaches a pseudo-equilibrium of canceling errors against the background, trapping them in a spurious local minimum. To resolve this, we propose StableGS, a novel framework that decouples geometric regularization from final appearance rendering. Its core is a Dual Opacity architecture that creates two separate rendering paths: a ``Geometric Regularization Path" to bear strong depth-based constraints for structural correctness, and an ``Appearance Refinement Path" to generate high-fidelity details upon this stable foundation. We complement this with a synergistic set of geometric constraints: a self-supervised depth consistency loss and an external geometric prior enabled by our efficient global scale optimization algorithm. Experiments on multiple benchmarks show StableGS not only eliminates floaters but also resolves the common blur-artifact trade-off, achieving state-of-the-art geometric accuracy and visual quality.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics</title>
<link>https://arxiv.org/abs/2503.21735</link>
<guid>https://arxiv.org/abs/2503.21735</guid>
<content:encoded><![CDATA[
arXiv:2503.21735v2 Announce Type: replace-cross 
Abstract: Ensuring reliable software release decisions is critical in safety-critical domains such as automotive manufacturing. Release validation relies on large tabular datasets, yet manual analysis is slow, costly, and error-prone. While Large Language Models (LLMs) offer promising automation potential, they face challenges in analytical reasoning, structured data handling, and ambiguity resolution. This paper introduces GateLens, an LLM-based system for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and generates optimized Python code. Unlike traditional multi-agent or planning-based systems that can be slow, opaque, and costly to maintain, GateLens emphasizes speed, transparency, and reliability. Experimental results show that GateLens outperforms the existing Chain-of-Thought (CoT) + Self-Consistency (SC) based system on real-world datasets, particularly in handling complex and ambiguous queries. Ablation studies confirm the essential role of the RA layer. Industrial deployment shows over 80% reduction in analysis time while maintaining high accuracy across test result interpretation, impact assessment, and release candidate evaluation. GateLens operates effectively in zero-shot settings without requiring few-shot examples or agent orchestration. This work advances deployable LLM system design by identifying key architectural features-intermediate formal representations, execution efficiency, and low configuration overhead-crucial for safety-critical industrial applications.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2504.07448</link>
<guid>https://arxiv.org/abs/2504.07448</guid>
<content:encoded><![CDATA[
arXiv:2504.07448v2 Announce Type: replace-cross 
Abstract: Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data</title>
<link>https://arxiv.org/abs/2504.16628</link>
<guid>https://arxiv.org/abs/2504.16628</guid>
<content:encoded><![CDATA[
arXiv:2504.16628v2 Announce Type: replace-cross 
Abstract: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models</title>
<link>https://arxiv.org/abs/2505.07167</link>
<guid>https://arxiv.org/abs/2505.07167</guid>
<content:encoded><![CDATA[
arXiv:2505.07167v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory</title>
<link>https://arxiv.org/abs/2505.10981</link>
<guid>https://arxiv.org/abs/2505.10981</guid>
<content:encoded><![CDATA[
arXiv:2505.10981v3 Announce Type: replace-cross 
Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies $\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a probabilistic method to efficiently predict scaling performance and identify the best prompting strategy under large sampling times, eliminating the need for resource-intensive inference processes in practical applications. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance. Code is available at https://github.com/MraDonkey/rethinking_prompting.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v3 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline while only increasing training time by 4.9\% and testing time by 6.5\%. We also experimentally demonstrate that GEM can improve the step-wise success rate by 9.40\% by requesting assistance from the cloud model when encountering OOD samples. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs</title>
<link>https://arxiv.org/abs/2505.14899</link>
<guid>https://arxiv.org/abs/2505.14899</guid>
<content:encoded><![CDATA[
arXiv:2505.14899v2 Announce Type: replace-cross 
Abstract: While large language models (LLMs) have shown great potential across various domains, their applications in robotics remain largely limited to static prompt-based behaviors and still face challenges in complex tasks under zero-shot or few-shot settings. Inspired by human metacognitive learning and creative problem-solving, we address this limitation by exploring a fundamental question: Can LLMs be empowered with metacognitive capabilities to reason, reflect, and create, thereby enhancing their ability to perform robotic tasks with minimal demonstrations? In this paper, we present a framework that integrates metacognitive learning into LLM-powered multi-robot collaboration. The system equips the LLM-powered robotic agents with a skill decomposition and self-reflection mechanism that identifies modular skills from prior tasks, reflects on failures in unseen task scenarios, and synthesizes effective new solutions. We propose a more challenging robotic benchmark task and evaluate our framework on the existing benchmark and the novel task. Experimental results show that our metacognitive learning framework significantly outperforms existing baselines. Moreover, we observe that the framework can generate solutions that differ from the ground truth yet still successfully complete the tasks. These findings support our hypothesis that metacognitive learning can foster creativity in robotic planning.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISON: Unmasking the Criminal Potential of Large Language Models</title>
<link>https://arxiv.org/abs/2506.16150</link>
<guid>https://arxiv.org/abs/2506.16150</guid>
<content:encoded><![CDATA[
arXiv:2506.16150v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) advance, concerns about their misconduct in complex social contexts intensify. Existing research overlooked the systematic understanding and assessment of their criminal capability in realistic interactions. We propose a unified framework PRISON, to quantify LLMs' criminal potential across five traits: False Statements, Frame-Up, Psychological Manipulation, Emotional Disguise, and Moral Disengagement. Using structured crime scenarios adapted from classic films grounded in reality, we evaluate both criminal potential and anti-crime ability of LLMs. Results show that state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as proposing misleading statements or evasion tactics, even without explicit instructions. Moreover, when placed in a detective role, models recognize deceptive behavior with only 44% accuracy on average, revealing a striking mismatch between conducting and detecting criminal behavior. These findings underscore the urgent need for adversarial robustness, behavioral alignment, and safety mechanisms before broader LLM deployment.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky</title>
<link>https://arxiv.org/abs/2507.03336</link>
<guid>https://arxiv.org/abs/2507.03336</guid>
<content:encoded><![CDATA[
arXiv:2507.03336v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise APIs, yet they routinely falter when near-duplicate tools vie for the same user intent or when required arguments are left underspecified. We introduce DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a disambiguation-centric, three-stage pipeline that (i) synthesizes persona-driven, multi-turn dialogues in which the assistant must distinguish among highly similar tools, (ii) performs supervised fine-tuning of open-source models with reasoning traces across 3B - 70B parameters, and (iii) evaluates real-world readiness via a dynamic suite that redeploys each model in a live agentic loop and reports end-to-end goal completion alongside conventional static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we release an open corpus of 5000 production-grade enterprise API specifications paired with rigorously validated, disambiguation-focused dialogues, offering a practical blueprint for building reliable, enterprise-ready tool-calling agents.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions</title>
<link>https://arxiv.org/abs/2507.04377</link>
<guid>https://arxiv.org/abs/2507.04377</guid>
<content:encoded><![CDATA[
arXiv:2507.04377v2 Announce Type: replace-cross 
Abstract: Tombstones are historically and culturally rich artifacts, encapsulating individual lives, community memory, historical narratives and artistic expression. Yet, many tombstones today face significant preservation challenges, including physical erosion, vandalism, environmental degradation, and political shifts. In this paper, we introduce a novel multi-modal framework for tombstones digitization, aiming to improve the interpretation, organization and retrieval of tombstone content. Our approach leverages vision-language models (VLMs) to translate tombstone images into structured Tombstone Meaning Representations (TMRs), capturing both image and text information. To further enrich semantic parsing, we incorporate retrieval-augmented generation (RAG) for integrate externally dependent elements such as toponyms, occupation codes, and ontological concepts. Compared to traditional OCR-based pipelines, our method improves parsing accuracy from an F1 score of 36.1 to 89.5. We additionally evaluate the model's robustness across diverse linguistic and cultural inscriptions, and simulate physical degradation through image fusion to assess performance under noisy or damaged conditions. Our work represents the first attempt to formalize tombstone understanding using large vision-language models, presenting implications for heritage preservation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents</title>
<link>https://arxiv.org/abs/2507.10644</link>
<guid>https://arxiv.org/abs/2507.10644</guid>
<content:encoded><![CDATA[
arXiv:2507.10644v3 Announce Type: replace-cross 
Abstract: The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</title>
<link>https://arxiv.org/abs/2507.12806</link>
<guid>https://arxiv.org/abs/2507.12806</guid>
<content:encoded><![CDATA[
arXiv:2507.12806v2 Announce Type: replace-cross 
Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.
]]></content:encoded>
<pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</title>
<link>https://arxiv.org/abs/2508.00079</link>
<guid>https://arxiv.org/abs/2508.00079</guid>
<content:encoded><![CDATA[
<div> Keywords: physics problems, LLMs, multi-agent framework, inference-time techniques, evaluation benchmark

Summary: 
In this study, the researchers assess the performance of advanced LLMs in solving physics problems, encompassing both mathematical and descriptive tasks. They implement various inference-time techniques and a multi-agent framework to enhance model performance. By introducing a new evaluation benchmark named ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of over 19,000 problems sourced from physics textbooks, they evaluate the effectiveness of the models in solving these problems. The utilization of a multi-agent framework shows significant improvements, particularly on challenging problems initially tackled poorly by the models. Additionally, the validation of proposed solutions through smaller LLM agents in a cumulative manner is explored. The research code and data are openly accessible on GitHub, promoting transparency and reproducibility in the field. <div>
arXiv:2508.00079v1 Announce Type: new 
Abstract: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs produce texts with "human-like" lexical diversity?</title>
<link>https://arxiv.org/abs/2508.00086</link>
<guid>https://arxiv.org/abs/2508.00086</guid>
<content:encoded><![CDATA[
<div> lexial diversity, LLMs, human-like texts, ChatGPT models, language pedagogy

Summary: 
The study analyzed lexical diversity in texts generated by language model models (LLMs) such as ChatGPT models and compared them to texts written by human participants. Different dimensions of lexical diversity were measured, showing that LLM-generated texts differed significantly from human-written texts in all variables. The newer LLM models, ChatGPT-4.5 in particular, exhibited higher levels of lexical diversity despite producing fewer tokens. Human writers' lexical diversity did not vary based on education levels or language status. The results suggest that LLMs do not replicate human-like texts in terms of lexical diversity, and newer models are less human-like than older ones. The implications of these findings for language pedagogy and related applications are discussed. 

<br /><br />Summary: <div>
arXiv:2508.00086v1 Announce Type: new 
Abstract: The degree to which LLMs produce writing that is truly human-like remains unclear despite the extensive empirical attention that this question has received. The present study addresses this question from the perspective of lexical diversity. Specifically, the study investigates patterns of lexical diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini, and -4.5) in comparison with texts written by L1 and L2 English participants (n = 240) across four education levels. Six dimensions of lexical diversity were measured in each text: volume, abundance, variety-repetition, evenness, disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and Support Vector Machines revealed that the LLM-generated texts differed significantly from human-written texts for each variable, with ChatGPT-o4 mini and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated higher levels of lexical diversity despite producing fewer tokens. The human writers' lexical diversity did not differ across subgroups (i.e., education, language status). Altogether, the results indicate that LLMs do not produce human-like texts in relation to lexical diversity, and the newer LLMs produce less human-like texts than older models. We discuss the implications of these results for language pedagogy and related applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semiotic Complexity and Its Epistemological Implications for Modeling Culture</title>
<link>https://arxiv.org/abs/2508.00095</link>
<guid>https://arxiv.org/abs/2508.00095</guid>
<content:encoded><![CDATA[
<div> translation, computational humanities, theory, semiotic complexity, modeling

Summary:<br />
- The article emphasizes the need for greater theorizing in computational humanities to ensure epistemological and interpretive clarity.
- It introduces the concept of translation work between cultural and computational domains, highlighting the importance of articulating the theory behind the process.
- The authors discuss the notion of semiotic complexity, focusing on the varying meanings of texts across interpretive lenses.
- They critique dominant modeling practices for treating semiotically complex data as simple for convenience, leading to translation errors.
- Recommendations are provided for researchers to address epistemological issues in modeling practices and improve interpretive transparency in their work.<br /><br /> <div>
arXiv:2508.00095v1 Announce Type: new 
Abstract: Greater theorizing of methods in the computational humanities is needed for epistemological and interpretive clarity, and therefore the maturation of the field. In this paper, we frame such modeling work as engaging in translation work from a cultural, linguistic domain into a computational, mathematical domain, and back again. Translators benefit from articulating the theory of their translation process, and so do computational humanists in their work -- to ensure internal consistency, avoid subtle yet consequential translation errors, and facilitate interpretive transparency. Our contribution in this paper is to lay out a particularly consequential dimension of the lack of theorizing and the sorts of translation errors that emerge in our modeling practices as a result. Along these lines we introduce the idea of semiotic complexity as the degree to which the meaning of some text may vary across interpretive lenses, and make the case that dominant modeling practices -- especially around evaluation -- commit a translation error by treating semiotically complex data as semiotically simple when it seems epistemologically convenient by conferring superficial clarity. We then lay out several recommendations for researchers to better account for these epistemological issues in their own work.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality</title>
<link>https://arxiv.org/abs/2508.00109</link>
<guid>https://arxiv.org/abs/2508.00109</guid>
<content:encoded><![CDATA[
<div> benchmark, FACTORY, factuality evaluation, language models, human-verified<br />
Summary:<br />
The article introduces FACTORY, a large-scale human-verified prompt set for long-form factuality evaluation. Developed using model-in-the-loop and human refinement, FACTORY includes challenging, fact-seeking, answerable, and unambiguous prompts. Human evaluations on 6 state-of-the-art language models show that approximately 40% of claims made in their responses are not factual, highlighting FACTORY's reliability. The benchmark outperforms existing datasets, emphasizing the need for models to reason across a wide range of facts. <div>
arXiv:2508.00109v1 Announce Type: new 
Abstract: Long-form factuality evaluation assesses the ability of models to generate accurate, comprehensive responses to short prompts. Existing benchmarks often lack human verification, leading to potential quality issues. To address this limitation, we introduce FACTORY, a large-scale, human-verified prompt set. Developed using a model-in-the-loop approach and refined by humans, FACTORY includes challenging prompts that are fact-seeking, answerable, and unambiguous. We conduct human evaluations on 6 state-of-the-art language models using FACTORY and existing datasets. Our results show that FACTORY is a challenging benchmark: approximately 40% of the claims made in the responses of SOTA models are not factual, compared to only 10% for other datasets. Our analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing its reliability and the necessity for models to reason across long-tailed facts.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is neural semantic parsing good at ellipsis resolution, or isn't it?</title>
<link>https://arxiv.org/abs/2508.00121</link>
<guid>https://arxiv.org/abs/2508.00121</guid>
<content:encoded><![CDATA[
<div> Neural, semantic, parsers, ellipsis, English 
<br />
Summary: 
Neural semantic parsers have shown high performance in various linguistic tasks, achieving semantic matching scores exceeding 90%. However, their ability to handle strongly context-sensitive phenomena, such as verb phrase ellipsis in English, remains a challenge. In a study utilizing a corpus of 120 instances of ellipsis with their resolved meaning representations, it was found that while neural semantic parsers excelled on standard test sets, they struggled with cases of ellipsis. The phenomena of verb phrase ellipsis presents a unique challenge for semantic parsers, as it requires the duplication of significant semantic information to form a coherent representation. The study emphasizes the importance of evaluating the performance of semantic parsers on complex, context-sensitive linguistic constructs to better understand their capabilities and limitations. 
<br /><br /> Summary: <div>
arXiv:2508.00121v1 Announce Type: new 
Abstract: Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of Large Language Models for Deployment Requirements</title>
<link>https://arxiv.org/abs/2508.00185</link>
<guid>https://arxiv.org/abs/2508.00185</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Generative Pre-trained Transformers, Mixture of Experts, fine-tuning, open-source

Summary:
Large Language Models (LLMs), including Generative Pre-trained Transformers (GPTs), are transforming text generation with their ability to produce contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these AI models excel in tasks such as content creation, translation, and code generation. Techniques like fine-tuning and novel architectures like Mixture of Experts (MoE) help address these challenges. The introduction of numerous foundational and fine-tuned open-source models in the past two years has made selecting the optimal LLM difficult for researchers and companies due to licensing and hardware requirements. To aid in navigating the evolving LLM landscape and facilitate model selection, a comparative list of foundational and domain-specific models is provided, focusing on features like release year, licensing, and hardware requirements. This continuously updated list is available on GitLab. 

<br /><br />Summary: Large Language Models, including Generative Pre-trained Transformers, are revolutionizing text generation. Despite challenges, such as biases and hallucinations, these models excel in various tasks with the help of techniques like fine-tuning and architectures like Mixture of Experts. The introduction of numerous open-source models has made selecting the best LLM complicated. To assist in this process, a comparative list of models focusing on key features has been created and made available on GitLab for continuous updating. <div>
arXiv:2508.00185v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers (GPTs) are revolutionizing the generation of human-like text, producing contextually relevant and syntactically correct content. Despite challenges like biases and hallucinations, these Artificial Intelligence (AI) models excel in tasks, such as content creation, translation, and code generation. Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address these issues. Over the past two years, numerous open-source foundational and fine-tuned models have been introduced, complicating the selection of the optimal LLM for researchers and companies regarding licensing and hardware requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM selection, we present a comparative list of foundational and domain-specific models, focusing on features, such as release year, licensing, and hardware requirements. This list is published on GitLab and will be continuously updated.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges</title>
<link>https://arxiv.org/abs/2508.00217</link>
<guid>https://arxiv.org/abs/2508.00217</guid>
<content:encoded><![CDATA[
<div> Keywords: Tables, Large Language Models, Multimodal Large Language Models, Tabular Input Representations, Table Understanding Tasks 

Summary: 
Tables in large language models (LLMs) and multimodal large language models (MLLMs) are complex and flexible structures that require specialized methods and tasks due to their varied formats and purposes. However, current research primarily focuses on retrieval-focused tasks, lacking reasoning beyond mathematical and logical operations. Models struggle with processing complex table structures, large-scale tables, lengthy context, and multi-table scenarios. Additionally, generalization across different tabular representations and formats remains limited. Further research is needed to address these gaps and enhance the capabilities of models in understanding and navigating tables effectively.<br /><br />Summary:  <div>
arXiv:2508.00217v1 Announce Type: new 
Abstract: Tables have gained significant attention in large language models (LLMs) and multimodal large language models (MLLMs) due to their complex and flexible structure. Unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. This diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. To address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. We highlight several critical gaps in the field that indicate the need for further research: (1) the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; (2) significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and (3) the limited generalization of models across different tabular representations and formats.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform</title>
<link>https://arxiv.org/abs/2508.00220</link>
<guid>https://arxiv.org/abs/2508.00220</guid>
<content:encoded><![CDATA[
<div> Wavelet Transforms, Signal and Image Processing, NLP, Discrete Wavelet Transforms, Embeddings <br />
Summary: Wavelet transforms have proven effective in various domains, including NLP, for analyzing intricate patterns and enhancing data representation. This paper explores the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings, showcasing its ability to analyze embedding representations at different resolutions and compress them while maintaining quality. The study evaluates DWT embeddings on semantic similarity tasks, demonstrating their potential to capture important semantic information in embedding vectors. Results indicate that DWT can significantly reduce embedding dimensionality without affecting performance on semantic tasks while achieving superior accuracy in downstream tasks. These findings suggest that DWT can be a valuable tool for improving NLP applications. <br /> <div>
arXiv:2508.00220v1 Announce Type: new 
Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in different domains, including Signal and Image processing, to unravel intricate patterns, enhance data representation, and extract meaningful features from data. Tangible results from their application suggest that Wavelet transforms can be applied to NLP capturing a variety of linguistic and semantic properties. In this paper, we empirically leverage the application of Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase the capabilities of DWT in analyzing embedding representations at different levels of resolution and compressing them while maintaining their overall quality. We assess the effectiveness of DWT embeddings on semantic similarity tasks to show how DWT can be used to consolidate important semantic information in an embedding vector. We show the efficacy of the proposed paradigm using different embedding models, including large language models, on downstream tasks. Our results show that DWT can reduce the dimensionality of embeddings by 50-93% with almost no change in performance for semantic similarity tasks, while achieving superior accuracy in most downstream tasks. Our findings pave the way for applying DWT to improve NLP applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English</title>
<link>https://arxiv.org/abs/2508.00238</link>
<guid>https://arxiv.org/abs/2508.00238</guid>
<content:encoded><![CDATA[
<div> shifts, language models, lexical trends, artificial intelligence, alignment

Summary:
- Recent shifts in written language, particularly in science and education, are attributed to the influence of Large Language Models (LLMs) like ChatGPT.
- A dataset of 22.1 million words from unscripted spoken language was analyzed to study lexical trends pre- and post-2022, showing a significant increase in LLM-associated words post-2022.
- This increase suggests a convergence between human word choices and LLM patterns, indicating a potential shift in language use.
- Whether this change is a natural language evolution or driven by AI exposure remains uncertain, raising ethical concerns about the impact of misaligned language models on societal beliefs.
- The findings suggest that misalignments in AI models could contribute to changes in human language use, potentially shaping social and moral values. 

<br /><br />Summary: <div>
arXiv:2508.00238v1 Announce Type: new 
Abstract: In recent years, written language, particularly in science and education, has undergone remarkable shifts in word usage. These changes are widely attributed to the growing influence of Large Language Models (LLMs), which frequently rely on a distinct lexical style. Divergences between model output and target audience norms can be viewed as a form of misalignment. While these shifts are often linked to using Artificial Intelligence (AI) directly as a tool to generate text, it remains unclear whether the changes reflect broader changes in the human language system itself. To explore this question, we constructed a dataset of 22.1 million words from unscripted spoken language drawn from conversational science and technology podcasts. We analyzed lexical trends before and after ChatGPT's release in 2022, focusing on commonly LLM-associated words. Our results show a moderate yet significant increase in the usage of these words post-2022, suggesting a convergence between human word choices and LLM-associated patterns. In contrast, baseline synonym words exhibit no significant directional shift. Given the short time frame and the number of words affected, this may indicate the onset of a remarkable shift in language use. Whether this represents natural language change or a novel shift driven by AI exposure remains an open question. Similarly, although the shifts may stem from broader adoption patterns, it may also be that upstream training misalignments ultimately contribute to changes in human language use. These findings parallel ethical concerns that misaligned models may shape social and moral beliefs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering</title>
<link>https://arxiv.org/abs/2508.00285</link>
<guid>https://arxiv.org/abs/2508.00285</guid>
<content:encoded><![CDATA[
<div> Etiology-Aware Attention Steering Framework, Clinical Reasoning Scaffolding, Etiology-Aware Head Identification algorithm, Reasoning-Guided Parameter-Efficient Fine-tuning, Reasoning Focus Score<br />
<br />
Summary: This study introduces an Etiology-Aware Attention Steering Framework to enhance diagnostic accuracy and clinical reasoning in Large Language Models (LLMs) for acute abdominal emergencies. By integrating structured Clinical Reasoning Scaffolding (CRS) and developing the Etiology-Aware Head Identification algorithm, the framework identifies crucial attention heads for etiology reasoning. The Reasoning-Guided Parameter-Efficient Fine-tuning embeds etiological cues and steers attention heads towards critical information, improving diagnostic accuracy by 15.65% and Reasoning Focus Score by 31.6% on the Consistent Diagnosis Cohort. External validation on the Discrepant Diagnosis Cohort further confirms the framework's effectiveness. The models show enhanced reliability in real-world complex scenarios, indicating a promising approach for building interpretable and reliable AI diagnostic systems in clinical settings. <br /><br />Summary: <div>
arXiv:2508.00285v1 Announce Type: new 
Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities in medical text understanding and generation. However, their diagnostic reliability in complex clinical scenarios remains limited. This study aims to enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We propose an Etiology-Aware Attention Steering Framework to integrate structured clinical reasoning into LLM-based diagnosis. Specifically, we first construct Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines for three representative acute abdominal emergencies: acute appendicitis, acute pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head Identification algorithm to pinpoint attention heads crucial for the model's etiology reasoning. To ensure reliable clinical reasoning alignment, we introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds etiological reasoning cues into input representations and steers the selected Etiology-Aware Heads toward critical information through a Reasoning-Guided Loss function. Result: On the Consistent Diagnosis Cohort, our framework improves average diagnostic accuracy by 15.65% and boosts the average Reasoning Focus Score by 31.6% over baselines. External validation on the Discrepant Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic accuracy. Further assessments via Reasoning Attention Frequency indicate that our models exhibit enhanced reliability when faced with real-world complex scenarios. Conclusion: This study presents a practical and effective approach to enhance clinical reasoning in LLM-based diagnosis. By aligning model attention with structured CRS, the proposed framework offers a promising paradigm for building more interpretable and reliable AI diagnostic systems in complex clinical settings.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Evaluation of Optimization Techniques for Long-Context Language Models</title>
<link>https://arxiv.org/abs/2508.00305</link>
<guid>https://arxiv.org/abs/2508.00305</guid>
<content:encoded><![CDATA[
<div> optimization, large language models, memory usage, text generation, scalability

Summary: 
This paper examines various optimization techniques for large language models (LLMs) to address resource demands and limited context windows. The study benchmarks methods such as pruning, quantization, and token dropping for two LLM architectures with long context support. It evaluates the impact of these optimizations on memory usage, latency, throughput, and the quality of text generation. Combinations of optimization techniques are systematically tested to understand their effects on performance metrics. The scalability of these methods is also studied on a larger 70 billion-parameter model. The research uncovers that naive combination inference optimization algorithms can have negative impacts on larger models due to compounded approximation errors. It highlights the importance of considering precision-recall trade-offs in question answering tasks instead of solely relying on F1 scores. By integrating system-level profiling with task-specific insights, this study aids LLM practitioners in balancing efficiency, accuracy, and scalability across different tasks and hardware configurations. 

<br /><br />Summary: <div>
arXiv:2508.00305v1 Announce Type: new 
Abstract: Large language models (LLMs) excel across diverse natural language processing tasks but face resource demands and limited context windows. Although techniques like pruning, quantization, and token dropping can mitigate these issues, their efficacy in long-context scenarios and system evaluation remains underexplored. This paper systematically benchmarks these optimizations, characterizing memory usage, latency, and throughput, and studies how these methods impact the quality of text generation. We first analyze individual optimization methods for two LLM architectures supporting long context and then systematically evaluate combinations of these techniques to assess how this deeper analysis impacts performance metrics. We subsequently study the scalability of individual optimization methods on a larger variant with 70 billion-parameter model. Our novel insights reveal that naive combination inference optimization algorithms can adversely affect larger models due to compounded approximation errors, as compared to their smaller counterparts. Experiments show that relying solely on F1 obscures these effects by hiding precision-recall trade-offs in question answering tasks. By integrating system-level profiling with task-specific insights, this study helps LLM practitioners and researchers explore and balance efficiency, accuracy, and scalability across tasks and hardware configurations.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment</title>
<link>https://arxiv.org/abs/2508.00332</link>
<guid>https://arxiv.org/abs/2508.00332</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal sentence embedding, MCSEO, object-phrase alignment, contrastive learning objective, semantic textual similarity (STS) tasks

Summary: 
MCSEO is proposed as a method to enhance multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. It leverages segmentation and object detection models to extract accurate object-phrase pairs, optimizing a contrastive learning objective focused on object-phrase correspondence. Experimental results across various backbone models show that MCSEO consistently outperforms strong baselines in semantic textual similarity (STS) tasks. This highlights the importance of precise object-phrase alignment in enhancing multimodal representation learning. <br /><br />Summary: <div>
arXiv:2508.00332v1 Announce Type: new 
Abstract: Multimodal sentence embedding models typically leverage image-caption pairs in addition to textual data during training. However, such pairs often contain noise, including redundant or irrelevant information on either the image or caption side. To mitigate this issue, we propose MCSEO, a method that enhances multimodal sentence embeddings by incorporating fine-grained object-phrase alignment alongside traditional image-caption alignment. Specifically, MCSEO utilizes existing segmentation and object detection models to extract accurate object-phrase pairs, which are then used to optimize a contrastive learning objective tailored to object-phrase correspondence. Experimental results on semantic textual similarity (STS) tasks across different backbone models demonstrate that MCSEO consistently outperforms strong baselines, highlighting the significance of precise object-phrase alignment in multimodal representation learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00344</link>
<guid>https://arxiv.org/abs/2508.00344</guid>
<content:encoded><![CDATA[
<div> Agent-based environments, Large Language Models, ReAct paradigm, AdaPlan, PilotRL, reinforcement learning

Summary:
- Challenges faced in deploying Large Language Models (LLMs) in agent-based environments due to limitations in existing agent paradigms.
- Introduction of AdaPlan, an adaptive global plan-based agent paradigm to support long-horizon decision-making.
- Proposal of PilotRL framework for LLM agents, combining global planning guidance with reinforcement learning for improved task performance.
- Development of the model's ability to follow explicit guidance from global plans.
- Optimization of plan quality and coordination between planning and execution in joint optimization. 

<br /><br />Summary: <div>
arXiv:2508.00344v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lucy: edgerunning agentic web search on mobile with machine generated task vectors</title>
<link>https://arxiv.org/abs/2508.00360</link>
<guid>https://arxiv.org/abs/2508.00360</guid>
<content:encoded><![CDATA[
<div> RLVR, agentic web-search model, Lucy, MCP integration, SimpleQA benchmark
Summary:
- Small language models (SLMs) face limitations in knowledge-intensive tasks due to their restricted capacity.
- This work introduces a new approach where the model's internal reasoning is viewed as a dynamic task vector machine, with content inside specific tags guiding the model's thought process.
- The generation process itself is considered a mechanism for the model to construct and refine its task vectors in real-time.
- The dynamic task vector machine is optimized through Reinforcement Learning with Variable Reward (RLVR) and successfully trained an agentic web-search model named Lucy.
- Lucy, a 1.7B-parameter SLM incorporating this dynamic reasoning mechanism with MCP integration, achieves a high accuracy of 78.3% on the SimpleQA benchmark, putting it on par with much larger models like DeepSeek-V3. This demonstrates that small models can compete with larger ones when equipped with structured, self-constructed task reasoning.<br /><br />Summary: <div>
arXiv:2508.00360v1 Announce Type: new 
Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive tasks due to their constrained capacity. While test-time computation offers a path to enhanced performance, most approaches treat reasoning as a fixed or heuristic process. In this work, we propose a new paradigm: viewing the model's internal reasoning, delimited by  and  tags, as a dynamic task vector machine. Rather than treating the content inside these tags as a mere trace of thought, we interpret the generation process itself as a mechanism through which the model \textbf{constructs and refines its own task vectors} on the fly. We developed a method to optimize this dynamic task vector machine through RLVR and successfully trained an agentic web-search model. We present Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing on par with much larger models such as DeepSeek-V3. This demonstrates that small models can rival large ones when equipped with structured, self-constructed task reasoning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices</title>
<link>https://arxiv.org/abs/2508.00370</link>
<guid>https://arxiv.org/abs/2508.00370</guid>
<content:encoded><![CDATA[
<div> fine-tuning, long-sequence tasks, EdgeInfinite, self-attention, NPUs <br />
Summary: 
The article introduces EdgeInfinite-Instruct, a solution for deploying Transformer-based large language models on resource-constrained edge devices for long-sequence tasks. Existing optimizations for Key-Value cache in these models often fail to reduce time to first token (TTFT) and may impact performance. EdgeInfinite fine-tunes only a small subset of parameters to reduce computational and memory costs while improving TTFT. The new EdgeInfinite-Instruct strategy utilizes Segmented Supervised Fine-Tuning (S-SFT) for tasks like summarization and question answering. Post-training quantization (PTQ) is employed for efficient deployment on edge NPUs, reducing computational demands. A fixed-shape computation graph balances memory usage and device efficiency with input token and cache size customization. Experiments demonstrate improved domain-specific performance while maintaining efficiency on NPU-accelerated edge devices. <br /> <div>
arXiv:2508.00370v1 Announce Type: new 
Abstract: Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Layer Attention is the Amplifier of Demonstration Effectiveness</title>
<link>https://arxiv.org/abs/2508.00385</link>
<guid>https://arxiv.org/abs/2508.00385</guid>
<content:encoded><![CDATA[
<div> mechanisms, in-context learning, demonstration effectiveness, gradient flow, demonstration selection <br />
<br />
Summary: 
The study explores the reasons behind the ineffectiveness of demonstrations in in-context learning (ICL). It suggests that a demonstration becomes ineffective if its information has either already been learned by the model or is irrelevant to the user query. The analysis, based on gradient flow and linear self-attention models, reveals that in multi-layer models, the effectiveness disparity among demonstrations increases with higher layers. Current demonstration selection methods mainly focus on relevance to the user query, neglecting the model's assimilated information. To address this, the paper introduces GradS, a novel method that uses gradient flow for demonstration selection by considering the magnitude of the gradient flow with respect to a user query. Experimental results on various datasets and LLMs show that GradS outperforms existing baselines, demonstrating its effectiveness in improving demonstration selection. <div>
arXiv:2508.00385v1 Announce Type: new 
Abstract: Numerous studies have investigated the underlying mechanisms of in-context learning (ICL) effectiveness to inspire the design of related methods. However, existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness. Our analysis is based on gradient flow and linear self-attention models. By setting the gradient flow to zero, we deduce that a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones. Considering that current demonstration selection methods primarily focus on the relevance to the user query while overlooking the information that the model has already assimilated, we propose a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones. We validate our derivation and GradS on four prominent LLMs across five mainstream datasets. The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on average over the strongest baselines, demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation</title>
<link>https://arxiv.org/abs/2508.00390</link>
<guid>https://arxiv.org/abs/2508.00390</guid>
<content:encoded><![CDATA[
<div> Vision-Language Navigation, Unmanned Aerial Vehicle, Reinforcement Learning, Curriculum Learning, Semantic Understanding<br />
Summary:<br />
The article introduces a novel training framework, Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS), for improving Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN). By integrating Curriculum Learning into Reinforcement Learning, SA-GCS effectively quantifies the complexity of training samples using a Semantic-Aware Difficulty Estimator (SA-DE) and adjusts the sampling distribution dynamically with a Gaussian Curriculum Scheduler (GCS). This method significantly enhances training efficiency, accelerates convergence, and boosts overall model performance. Extensive experiments on the CityNav benchmark show that SA-GCS outperforms strong baselines across all metrics, demonstrates faster and more stable convergence, and generalizes well across models of various scales. The approach is robust, scalable, and publicly available for implementation.<br /> <div>
arXiv:2508.00390v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable agents to accurately localize targets and plan flight paths in complex environments based on natural language instructions, with broad applications in intelligent inspection, disaster rescue, and urban monitoring. Recent progress in Vision-Language Models (VLMs) has provided strong semantic understanding for this task, while reinforcement learning (RL) has emerged as a promising post-training strategy to further improve generalization. However, existing RL methods often suffer from inefficient use of training data, slow convergence, and insufficient consideration of the difficulty variation among training samples, which limits further performance improvement. To address these challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling (SA-GCS)}, a novel training framework that systematically integrates Curriculum Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator (SA-DE) to quantify the complexity of training samples and a Gaussian Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution, enabling a smooth progression from easy to challenging tasks. This design significantly improves training efficiency, accelerates convergence, and enhances overall model performance. Extensive experiments on the CityNav benchmark demonstrate that SA-GCS consistently outperforms strong baselines across all metrics, achieves faster and more stable convergence, and generalizes well across models of different scales, highlighting its robustness and scalability. The implementation of our approach is publicly available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding</title>
<link>https://arxiv.org/abs/2508.00420</link>
<guid>https://arxiv.org/abs/2508.00420</guid>
<content:encoded><![CDATA[
<div> Wavelets, Image processing, Signal processing, Natural Language Processing, Discrete Wavelet Transforms <br />
Summary: <br />
The paper explores the application of wavelets in Natural Language Processing (NLP) tasks, utilizing Discrete Wavelet Transforms (DWT) on word and sentence embeddings. The study evaluates the effectiveness of using wavelets to consolidate important information in word vectors while reducing dimensionality. A non-parameterized model combining DWT with Discrete Cosine Transform (DCT) is proposed to compress sentences with varying word features into fixed-size vectors. Results show that this approach yields comparable or superior outcomes in downstream NLP applications compared to original embeddings. The research demonstrates the potential for wavelets to enhance NLP tasks by capturing diverse linguistic properties and efficiently representing textual information. <div>
arXiv:2508.00420v1 Announce Type: new 
Abstract: Wavelets have emerged as a cutting edge technology in a number of fields. Concrete results of their application in Image and Signal processing suggest that wavelets can be effectively applied to Natural Language Processing (NLP) tasks that capture a variety of linguistic properties. In this paper, we leverage the power of applying Discrete Wavelet Transforms (DWT) to word and sentence embeddings. We first evaluate, intrinsically and extrinsically, how wavelets can effectively be used to consolidate important information in a word vector while reducing its dimensionality. We further combine DWT with Discrete Cosine Transform (DCT) to propose a non-parameterized model that compresses a sentence with a dense amount of information in a fixed size vector based on locally varying word features. We show the efficacy of the proposed paradigm on downstream applications models yielding comparable and even superior (in some tasks) results to original embeddings.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network</title>
<link>https://arxiv.org/abs/2508.00429</link>
<guid>https://arxiv.org/abs/2508.00429</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, imbalance in node informativeness, global semantic relationships, Retrieval-augmented Graph Agentic Network, few-shot in-context settings

Summary:<br />
Graph Neural Networks (GNNs) have been successful in graph-based learning but face limitations in handling imbalanced node informativeness and capturing global semantic relationships. To address these issues, the Retrieval-augmented Graph Agentic Network (ReaGAN) framework is introduced. ReaGAN empowers nodes to make autonomous decisions, allowing for node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) enables nodes to access relevant content and establish global relationships in the graph. By leveraging agentic planning and local-global retrieval, ReaGAN achieves competitive performance in few-shot in-context scenarios using a frozen LLM backbone without fine-tuning. This highlights the potential of agent-based frameworks and advanced retrieval mechanisms in enhancing graph learning. 

<br /><br />Summary: <div>
arXiv:2508.00429v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges</title>
<link>https://arxiv.org/abs/2508.00454</link>
<guid>https://arxiv.org/abs/2508.00454</guid>
<content:encoded><![CDATA[
<div> Multi-turn dialogue evaluator, large language models, biases, evaluation methods, efficiency<br />
Summary:<br />
- Evaluating conversational abilities of large language models is challenging.
- Current methods use LLMs as judges but suffer from biases.
- Multi-judge approach aggregates judgments but incurs high computational cost.
- Proposed method aggregates multiple LLM judgments into a single model efficiently.
- Outperforms existing baselines in diverse dialogue evaluation benchmarks, showcasing efficiency and robustness.<br /> 

Summary: <div>
arXiv:2508.00454v1 Announce Type: new 
Abstract: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts</title>
<link>https://arxiv.org/abs/2508.00476</link>
<guid>https://arxiv.org/abs/2508.00476</guid>
<content:encoded><![CDATA[
<div> Keywords: GETALP, Automatic Minuting Shared Task, retrieval augmented generation, Abstract Meaning Representations, question-answering  

Summary:  
- GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025 focused on Task B, involving question-answering based on meeting transcripts.  
- The method utilized a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR) to address the task.  
- Three systems were proposed, combining RAG and AMR approaches to enhance question-answering capabilities.  
- Results indicated that integrating AMR into the process significantly improved responses for approximately 35% of questions, especially those requiring differentiation between participants.  
- The incorporation of AMR demonstrated a notable enhancement in the quality of responses, particularly in scenarios where identification of specific participants was crucial.  

<br /><br />Summary: GETALP participated in the Automatic Minuting Shared Task by employing a retrieval augmented generation system and Abstract Meaning Representations. Their three systems showcased improved question-answering performance, particularly when distinguishing between participants in meeting transcripts. The integration of AMR notably enhanced the accuracy and quality of responses, highlighting its effectiveness in addressing complex questions based on meeting content. <div>
arXiv:2508.00476v1 Announce Type: new 
Abstract: This paper documents GETALP's submission to the Third Run of the Automatic Minuting Shared Task at SIGDial 2025. We participated in Task B: question-answering based on meeting transcripts. Our method is based on a retrieval augmented generation (RAG) system and Abstract Meaning Representations (AMR). We propose three systems combining these two approaches. Our results show that incorporating AMR leads to high-quality responses for approximately 35% of the questions and provides notable improvements in answering questions that involve distinguishing between different participants (e.g., who questions).
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Missing Parts: Augmenting Fact Verification with Half-Truth Detection</title>
<link>https://arxiv.org/abs/2508.00489</link>
<guid>https://arxiv.org/abs/2508.00489</guid>
<content:encoded><![CDATA[
<div> half-truth detection, PolitiFact-Hidden, TRACER, omission-based misinformation, fact verification

Summary:
TRACER is introduced as a framework for identifying half-truths by aligning evidence, inferring implied intent, and estimating the impact of hidden content. The framework is designed to address the challenge of detecting misleading claims that are factually correct but omit critical context. The study proposes a new benchmark, PolitiFact-Hidden, consisting of 15k political claims annotated with evidence alignment and inferred intent. TRACER significantly improves performance across multiple existing fact-checking models, particularly in classifying claims as Half-True, showcasing the importance of considering omissions for accurate fact verification. <div>
arXiv:2508.00489v1 Announce Type: new 
Abstract: Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about what is left unsaid. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond</title>
<link>https://arxiv.org/abs/2508.00522</link>
<guid>https://arxiv.org/abs/2508.00522</guid>
<content:encoded><![CDATA[
<div> Keywords: LoRA, flat minima, sharpness, generalization, efficiency

Summary:
Flat-LoRA and its efficient version, EFlat-LoRA, are proposed to seek flat minima for low-rank adaptation (LoRA). By transferring perturbations in the full parameter space to the low-rank subspace, potential interference is eliminated. Extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves comparable performance to LoRA with optimized efficiency. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning. Similarly, on vision-language models like Qwen-VL-Chat, EFlat-LoRA shows performance improvements on various datasets. These empirical results establish the correlation between sharpness and generalization in LoRA, highlighting the importance of considering sharpness in model optimization processes.
<br /><br />Summary: <div>
arXiv:2508.00522v1 Announce Type: new 
Abstract: Little research explores the correlation between the expressive ability and generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware Minimization (SAM) improves model generalization for both Convolutional Neural Networks (CNNs) and Transformers by encouraging convergence to locally flat minima. However, the connection between sharpness and generalization has not been fully explored for LoRA due to the lack of tools to either empirically seek flat minima or develop theoretical methods. In this work, we propose Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for LoRA. Concretely, we theoretically demonstrate that perturbations in the full parameter space can be transferred to the low-rank subspace. This approach eliminates the potential interference introduced by perturbations across multiple matrices in the low-rank subspace. Our extensive experiments on large language models and vision-language models demonstrate that EFlat-LoRA achieves optimize efficiency comparable to that of LoRA while simultaneously attaining comparable or even better performance. For example, on the GLUE dataset with RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and 0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets, respectively. These empirical results also verify that the generalization of LoRA is closely related to sharpness, which is omitted by previous methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prosody of Emojis</title>
<link>https://arxiv.org/abs/2508.00537</link>
<guid>https://arxiv.org/abs/2508.00537</guid>
<content:encoded><![CDATA[
<div> Prosodic features, emojis, speech, communication, perception <br />
Summary: <br />
This study investigates how emojis influence prosodic features in speech and how listeners interpret these cues to understand emoji meanings. By analyzing human speech data, the study shows that speakers adjust their prosody based on emoji cues. Listeners can often identify emojis from prosodic variations alone, and differences in emoji semantics correspond to variations in prosody. This suggests that emojis play a significant role in conveying prosodic intent in digitally mediated communication. <div>
arXiv:2508.00537v1 Announce Type: new 
Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken communication, conveying emotion, intent, and discourse structure. In text-based settings, where these cues are absent, emojis act as visual surrogates that add affective and pragmatic nuance. This study examines how emojis influence prosodic realisation in speech and how listeners interpret prosodic cues to recover emoji meanings. Unlike previous work, we directly link prosody and emoji by analysing actual human speech data, collected through structured but open-ended production and perception tasks. This provides empirical evidence of how emoji semantics shape spoken delivery and perception. Results show that speakers adapt their prosody based on emoji cues, listeners can often identify the intended emoji from prosodic variation alone, and greater semantic differences between emojis correspond to increased prosodic divergence. These findings suggest that emojis can act as meaningful carriers of prosodic intent, offering insight into their communicative role in digitally mediated contexts.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaPaformer: Language Model from Pre-trained Paraller Paths</title>
<link>https://arxiv.org/abs/2508.00544</link>
<guid>https://arxiv.org/abs/2508.00544</guid>
<content:encoded><![CDATA[
<div> Transformer, language models, training, parallel paths, PaPaformer <br />
<br />
Summary: 
The paper introduces a new approach called PaPaformer for training decoder-only transformer-based language models in a fraction of the time typically required. By combining lower-dimensional parallel paths into a larger model, PaPaformer allows for individual training of these paths with different data types before merging them. This method reduces model parameters and training time while maintaining performance. The use of parallel paths also offers the flexibility to customize paths for specific task requirements. Overall, PaPaformer presents a promising solution to the computational demands of training large-language models, making it possible to train and evaluate these models in hours rather than days or weeks. <div>
arXiv:2508.00544v1 Announce Type: new 
Abstract: The training of modern large-language models requires an increasingly amount of computation power and time. Even smaller variants, such as small-language models (SLMs), take several days to train in the best-case scenarios, often requiring multiple GPUs. This paper explores methods to train and evaluate decoder-only transformer-based language models in hours instead of days/weeks. We introduces \textit{PaPaformer}, a decoder-only transformer architecture variant, whose lower-dimensional parallel paths are combined into larger model. The paper shows that these lower-dimensional paths can be trained individually with different types of training data and then combined into one larger model. This method gives the option to reduce the total number of model parameters and the training time with increasing performance. Moreover, the use of parallel path structure opens interesting possibilities to customize paths to accommodate specific task requirements.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought</title>
<link>https://arxiv.org/abs/2508.00574</link>
<guid>https://arxiv.org/abs/2508.00574</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-Thought reasoning, Continuous CoT, SynAdapt, LLMs, efficient reasoning framework 

Summary:
SynAdapt presents an innovative efficient reasoning framework that addresses the time costs associated with Chain-of-Thought (CoT) reasoning by introducing Continuous CoT (CCoT). By generating synthetic CCoT as alignment targets for LLMs, SynAdapt enables accurate answers to be derived directly. To tackle difficult questions, a difficulty classifier is integrated into the framework to identify challenging questions based on question context and CCoT. When hard questions are identified, the LLM is prompted to re-think these questions for improved performance. Experimental results across various benchmarks demonstrate the effectiveness of SynAdapt in achieving the best accuracy-efficiency trade-off. The proposed method outperforms existing CCoT approaches and offers a more efficient alternative for reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.00574v1 Announce Type: new 
Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00600</link>
<guid>https://arxiv.org/abs/2508.00600</guid>
<content:encoded><![CDATA[
<div> Keywords: confidence estimation, large language models, context awareness, entropy reduction, consistency examination

Summary:
CRUX is a new framework designed to improve confidence estimation in large language models by considering the relevance between responses and contextual information. It incorporates two novel metrics: contextual entropy reduction and unified consistency examination. Contextual entropy reduction measures data uncertainty by comparing the information gain with and without context, while unified consistency examination evaluates model uncertainty by analyzing the global consistency of generated answers. Experimental results on various benchmark and domain-specific datasets show that CRUX outperforms existing methods in terms of AUROC. By considering context faithfulness and consistency, CRUX enhances the reliability of confidence estimation in large language models for better performance in safety-critical applications. 

<br /><br />Summary: <div>
arXiv:2508.00600v1 Announce Type: new 
Abstract: Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language</title>
<link>https://arxiv.org/abs/2508.00605</link>
<guid>https://arxiv.org/abs/2508.00605</guid>
<content:encoded><![CDATA[
<div> Graph Convolutional Network, Topic Modeling, Bengali, NMF, Text Corpus <br />
<br />Summary: 
Topic modeling is a valuable technique in Natural Language Processing for extracting themes from text corpora. The study introduces a novel model, GHTM, using Graph Convolutional Networks to generate semantic embeddings of Bengali documents. These embeddings are then decomposed using Non-negative Matrix Factorization to identify underlying topics. The model is compared against traditional and contemporary Bengali topic modeling methods on three datasets, demonstrating superior performance in topic coherence and diversity. The study also introduces a new Bengali dataset, "NCTBText," sourced from textbook materials to enhance the existing Bengali corpora. <div>
arXiv:2508.00605v1 Announce Type: new 
Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used to identify latent themes and extract topics from text corpora by grouping similar documents based on their most significant keywords. Although widely researched in English, topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives. In this contribution, a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus. This study compares the proposed model against a wide range of Bengali topic modeling techniques, from traditional methods such as LDA, LSA, and NMF to contemporary frameworks such as BERTopic and Top2Vec on three Bengali datasets. The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity. In addition, we introduce a novel Bengali dataset called "NCTBText" sourced from Bengali textbook materials to enrich and diversify the predominantly newspaper-centric Bengali corpora.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?</title>
<link>https://arxiv.org/abs/2508.00614</link>
<guid>https://arxiv.org/abs/2508.00614</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, tipping, threatening, performance, prompting

Summary: 
- The report investigates the effects of tipping and threatening AI models on performance, finding that these actions generally have no significant impact on benchmark performance.
- The study reveals that prompting variations can influence performance on individual questions, but it is unpredictable whether a specific approach will help or hinder the model's ability to answer a question.
- Simple prompting variations may not be as effective as previously thought, especially for challenging problems.
- Previous research has shown that different prompting approaches can lead to varying results for specific questions.
- Overall, while certain prompts may affect performance on a question-by-question basis, the overall impact of tipping and threatening AI models appears to be minimal on benchmark tasks.

<br /><br />Summary: <div>
arXiv:2508.00614v1 Announce Type: new 
Abstract: This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on benchmark performance.
  - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models</title>
<link>https://arxiv.org/abs/2508.00619</link>
<guid>https://arxiv.org/abs/2508.00619</guid>
<content:encoded><![CDATA[
<div> Existing AIG text detectors struggle in real-world settings, showing potential vulnerabilities. <br />
Keywords: AIG text detection, DACTYL dataset, few-shot generations, CPT-generated texts, DXO classifier <br /><br />
Summary:
The study examines the shortcomings of current AIG text detectors and introduces the DACTYL dataset, focusing on few-shot generations and CPT-generated texts. Existing detectors face challenges in detecting AIG texts, particularly in one-shot/few-shot scenarios. The study trains classifiers using BCE and DXO optimization approaches, with DXO classifiers demonstrating better generalization on out-of-distribution texts. In a mock deployment scenario, the best DXO classifier outperformed the best BCE-trained classifier by a significant margin. The results suggest that DXO classifiers have better generalization abilities and do not overfit to the test set. The experiments emphasize the need for enhancements in AIG text detection systems. <br /> <div>
arXiv:2508.00619v1 Announce Type: new 
Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications</title>
<link>https://arxiv.org/abs/2508.00669</link>
<guid>https://arxiv.org/abs/2508.00669</guid>
<content:encoded><![CDATA[
<div> training-time strategies, test-time mechanisms, data modalities, clinical applications, evaluation benchmarks

Summary:<br /><br />
The paper reviews the emerging field of Large Language Models (LLMs) designed for medical reasoning. It proposes a taxonomy of reasoning enhancement techniques, including training-time strategies like supervised fine-tuning and test-time mechanisms such as prompt engineering. These techniques are applied to different data modalities (text, image, code) and clinical applications like diagnosis and treatment planning. The evolution of evaluation benchmarks is also discussed, highlighting the shift towards assessing reasoning quality and visual interpretability. Critical challenges identified include the faithfulness-plausibility gap and the need for native multimodal reasoning. The study analyzes 60 seminal studies from 2022-2025 and concludes by outlining future directions towards developing efficient, robust, and sociotechnically responsible medical AI. <br /><br />Summary: <div>
arXiv:2508.00669v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language</title>
<link>https://arxiv.org/abs/2508.00673</link>
<guid>https://arxiv.org/abs/2508.00673</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evaluation, Persian language, Iranian culture, benchmark

Summary: 
This study addresses the need for evaluating large language models (LLMs) in languages other than English, focusing specifically on the Persian language and Iranian culture. The researchers introduced 19 new evaluation datasets covering topics such as Iranian law, Persian grammar, idioms, and university entrance exams to assess LLM performance. By benchmarking 41 popular LLMs using these datasets, the study aims to bridge the gap in cultural and linguistic evaluation resources. This initiative is crucial as most LLMs are trained on data from Western cultures, lacking familiarity with non-Western contexts. The research highlights the importance of ensuring the quality and reliability of LLMs across diverse cultural and linguistic backgrounds.<br /><br />Summary: <div>
arXiv:2508.00673v1 Announce Type: new 
Abstract: As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier</title>
<link>https://arxiv.org/abs/2508.00675</link>
<guid>https://arxiv.org/abs/2508.00675</guid>
<content:encoded><![CDATA[
<div> style change detection, authorship analysis, sequential sentence pair classifier, pre-trained language model, bidirectional LSTM

Summary:
The article discusses the challenge of detecting style changes in documents at the fine-grained level of individual sentences. The proposed approach uses a Sequential Sentence Pair Classifier (SSPC) that leverages a pre-trained language model (PLM) and bidirectional LSTM to contextualize sentences within documents. The model combines adjacent sentence representations and uses a multi-layer perceptron for prediction. Despite being conservative and lightweight, the approach effectively addresses the challenge of identifying style shifts in short, stylistically shallow sentences present in benchmark data. Evaluation on official PAN-2025 test datasets show strong macro-F1 scores, outperforming random baselines and achieving better performance than a challenging zero-shot model. This work highlights the importance of leveraging contextual information for accurate style change detection in computational authorship analysis. 

<br /><br />Summary: <div>
arXiv:2508.00675v1 Announce Type: new 
Abstract: Style change detection - identifying the points in a document where writing style shifts - remains one of the most important and challenging problems in computational authorship analysis. At PAN 2025, the shared task challenges participants to detect style switches at the most fine-grained level: individual sentences. The task spans three datasets, each designed with controlled and increasing thematic variety within documents. We propose to address this problem by modeling the content of each problem instance - that is, a series of sentences - as a whole, using a Sequential Sentence Pair Classifier (SSPC). The architecture leverages a pre-trained language model (PLM) to obtain representations of individual sentences, which are then fed into a bidirectional LSTM (BiLSTM) to contextualize them within the document. The BiLSTM-produced vectors of adjacent sentences are concatenated and passed to a multi-layer perceptron for prediction per adjacency. Building on the work of previous PAN participants classical text segmentation, the approach is relatively conservative and lightweight. Nevertheless, it proves effective in leveraging contextual information and addressing what is arguably the most challenging aspect of this year's shared task: the notorious problem of "stylistically shallow", short sentences that are prevalent in the proposed benchmark data. Evaluated on the official PAN-2025 test datasets, the model achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM, and HARD data, respectively, outperforming not only the official random baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot performance.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries</title>
<link>https://arxiv.org/abs/2508.00679</link>
<guid>https://arxiv.org/abs/2508.00679</guid>
<content:encoded><![CDATA[
<div> BM25, Vector Database, Cross-Encoder, legal precedent retrieval, TraceRetriever<br />
Summary:<br />
- Legal precedent retrieval in the common law system is crucial for ensuring consistency in judicial decisions.
- Traditional retrieval methods face challenges due to the increasing complexity and volume of legal documents.
- TraceRetriever mirrors real-world legal search by extracting rhetorically significant segments from limited case information.
- The pipeline integrates BM25, Vector Database, and Cross-Encoder models for efficient retrieval.
- Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments.
- Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever offers a reliable and scalable solution for precedent retrieval in legal research.
<br />Summary: <div>
arXiv:2508.00679v1 Announce Type: new 
Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Call Claude: Can LLMs Detect Changes of Writing Style?</title>
<link>https://arxiv.org/abs/2508.00680</link>
<guid>https://arxiv.org/abs/2508.00680</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, zero-shot performance, authorship analysis, style change detection, generative models 
<br />
Summary: 
Large language models exhibit strong zero-shot performance in detecting style changes at the sentence level in authorship analysis tasks. These models are sensitive to variations in writing style, surpassing suggested baselines in accuracy on official datasets. The study reveals that the latest LLMs may prioritize content-independent and stylistic signals over semantic content when making predictions. <div>
arXiv:2508.00680v1 Announce Type: new 
Abstract: This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</title>
<link>https://arxiv.org/abs/2508.00709</link>
<guid>https://arxiv.org/abs/2508.00709</guid>
<content:encoded><![CDATA[
<div> Legal Judgment Prediction, AI for law, NyayaRAG, Retrieval-Augmented Generation, Indian legal system
Summary: 
Legal Judgment Prediction (LJP) is important in AI for law, aiming to automate judicial outcome forecasting in the Indian legal system. The NyayaRAG framework combines factual case descriptions, legal statutes, and prior cases to predict court decisions and generate legal explanations. Results show that incorporating legal knowledge enhances predictive accuracy and explanation quality. This approach considers statutory provisions and judicial precedents, improving the understanding of common law systems like in India. NyayaRAG provides a domain-specific pipeline for legal reasoning, evaluating performance with various input configurations. The framework simulates realistic courtroom scenarios, enhancing interpretability in legal AI models. By leveraging structured legal knowledge, NyayaRAG demonstrates the effectiveness of combining factual inputs with legal insights in predicting court outcomes. G-Eval and other evaluation metrics confirm the advantages of augmenting information sources in LJP models. <div>
arXiv:2508.00709v1 Announce Type: new 
Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
<link>https://arxiv.org/abs/2508.00719</link>
<guid>https://arxiv.org/abs/2508.00719</guid>
<content:encoded><![CDATA[
<div> Monte Carlo Tree Search, Knowledge Graph Question Answering, Adaptive Path Evaluation, Context-aware reasoning, Relation sequence encoding <br />
Summary:<br />
The paper proposes a novel framework, Dynamically Adaptive MCTS-based Reasoning (DAMR), for Knowledge Graph Question Answering (KGQA). DAMR integrates symbolic search with adaptive path evaluation to improve efficiency and context-awareness in KGQA. It uses a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner to select relevant relations and reduce search space. A lightweight Transformer-based scorer is introduced for context-aware plausibility estimation during multi-hop reasoning. DAMR also incorporates a dynamic pseudo-path refinement mechanism to generate training signals from partial paths explored during search. This allows the model to continuously adapt to evolving reasoning trajectories. Extensive experiments on multiple KGQA benchmarks demonstrate that DAMR outperforms current state-of-the-art methods. <br /> <div>
arXiv:2508.00719v1 Announce Type: new 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data</title>
<link>https://arxiv.org/abs/2508.00741</link>
<guid>https://arxiv.org/abs/2508.00741</guid>
<content:encoded><![CDATA[
<div> abduction, large language models, reasoning, chatbots, AI safety
Summary:<br /><br />The study focuses on out-of-context abduction in Large Language Models (LLMs) and their ability to infer plausible explanations without direct training data. Experiments were conducted with OpenAI's GPT 4o LLM trained on chatbot names and behavior descriptions but not on dialogue examples. The results show that the LLM can correctly deduce a chatbot's name and display characteristic behaviors after training. This highlights the LLM's potential for situational awareness and reasoning abilities based on training data. The findings have implications for AI safety and understanding the capabilities of LLMs in reasoning tasks. <div>
arXiv:2508.00741v1 Announce Type: new 
Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents</title>
<link>https://arxiv.org/abs/2508.00742</link>
<guid>https://arxiv.org/abs/2508.00742</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative agents, Large Language Models, HEXACO, Personality inventory, Social science research 

Summary: 
1) The study focuses on the use of generative agents powered by Large Language Models in representing human populations in social science research.
2) The experiment involved surveying 310 GPT-4 powered agents to recreate the HEXACO personality inventory experiment and compare the results to the original findings.
3) Results showed a coherent and reliable personality structure recoverable from the agents' responses, with partial alignment to the HEXACO framework.
4) The derived personality dimensions were consistent and reliable within GPT-4 with a sufficiently curated population.
5) Cross-model analysis revealed variability in personality profiling, indicating model-specific biases and limitations. 

<br /><br />Summary: <div>
arXiv:2508.00742v1 Announce Type: new 
Abstract: Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic large language models improve retrieval-based radiology question answering</title>
<link>https://arxiv.org/abs/2508.00743</link>
<guid>https://arxiv.org/abs/2508.00743</guid>
<content:encoded><![CDATA[
<div> Radiology, artificial intelligence, clinical decision-making, large language models, retrieval-augmented generation <br />
Summary: 
- Propose an agentic retrieval-augmented generation framework for radiology question answering using large language models.
- Evaluated 24 LLMs on expert-curated radiology questions, showing significant improvement in diagnostic accuracy with agentic retrieval.
- Mid-sized models and small-scale models benefited the most, while very large models had minimal changes.
- Agentic retrieval reduced hallucinations and provided clinically relevant context in 46% of cases.
- Even clinically fine-tuned models showed meaningful improvements, highlighting the complementary roles of retrieval and fine-tuning in enhancing factuality and diagnostic accuracy in radiology QA. <br /> 

Summary: <div>
arXiv:2508.00743v1 Announce Type: new 
Abstract: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLiDRE: Generalist Lightweight model for Document-level Relation Extraction</title>
<link>https://arxiv.org/abs/2508.00757</link>
<guid>https://arxiv.org/abs/2508.00757</guid>
<content:encoded><![CDATA[
<div> GLiDRE, relation extraction, document-level, NER, benchmark <br />
Summary:
GLiDRE is a new model for document-level relation extraction, inspired by the success of the GLiNER model. It addresses the challenges of zero-shot and few-shot scenarios in relation extraction tasks like DocRED and Re-DocRED. The model outperforms state-of-the-art approaches in few-shot settings, showcasing its effectiveness in handling complex interactions between entities across sentences. GLiDRE builds on the compact NER model concept introduced by GLiNER and demonstrates significant improvements in document-level relation extraction tasks. The code for GLiDRE is publicly available, allowing for further research and development in this area.<br /><br />Summary: <div>
arXiv:2508.00757v1 Announce Type: new 
Abstract: Relation Extraction (RE) is a fundamental task in Natural Language Processing, and its document-level variant poses significant challenges, due to the need to model complex interactions between entities across sentences. Current approaches, largely based on the ATLOP architecture, are commonly evaluated on benchmarks like DocRED and Re-DocRED. However, their performance in zero-shot or few-shot settings remains largely underexplored due to the task's complexity. Recently, the GLiNER model has shown that a compact NER model can outperform much larger Large Language Models. With a similar motivation, we introduce GLiDRE, a new model for document-level relation extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against state-of-the-art models across various data settings on the Re-DocRED dataset. Our results demonstrate that GLiDRE achieves state-of-the-art performance in few-shot scenarios. Our code is publicly available.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations</title>
<link>https://arxiv.org/abs/2508.00760</link>
<guid>https://arxiv.org/abs/2508.00760</guid>
<content:encoded><![CDATA[
<div> Keywords: Hate speech detection, Chinese social networks, Multimodal framework, Mixture-of-Experts, BERT-based models

Summary: <br /><br /> This study introduces MMBERT, a novel multimodal framework for hate speech detection on Chinese social networks. MMBERT integrates textual, speech, and visual modalities using a Mixture-of-Experts architecture and employs a progressive three-stage training paradigm to address integration challenges. The model incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results on Chinese hate speech datasets demonstrate MMBERT's superiority over fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs using in-context learning approaches. Overall, MMBERT significantly improves hate speech detection capabilities in the specific context of Chinese social networks, highlighting the importance of multimodal strategies in tackling cloaking techniques and enhancing detection accuracy. <div>
arXiv:2508.00760v1 Announce Type: new 
Abstract: Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation</title>
<link>https://arxiv.org/abs/2508.00762</link>
<guid>https://arxiv.org/abs/2508.00762</guid>
<content:encoded><![CDATA[
<div> Keywords: SemEval-2025, Tabular data, Question answering, Large Language Models, Python code generation

Summary:<br />
This paper presents a system developed for SemEval-2025 Task 8, focusing on question answering over tabular data. The system tackles two subtasks, DataBench QA and DataBench Lite QA, using a zero-shot approach with Large Language Model-based code generation. The system utilizes a Python code generation framework to generate executable Pandas code through optimized prompting strategies. Results indicate varying effectiveness of different Large Language Models in Python code generation, with Python code generation outperforming alternative approaches in tabular question answering. The system achieved eighth place in Subtask I and sixth place in Subtask II among systems that surpassed the baseline in the open-source models category. <div>
arXiv:2508.00762v1 Announce Type: new 
Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models</title>
<link>https://arxiv.org/abs/2508.00788</link>
<guid>https://arxiv.org/abs/2508.00788</guid>
<content:encoded><![CDATA[
<div> benchmark, LLMs, pronoun fidelity, neopronouns, gender identity inference
Summary:
The study introduces the MISGENDERED+ benchmark for evaluating Large Language Models' (LLMs) pronoun fidelity, focusing on inclusive pronouns and gender identity inference. Five representative LLMs are benchmarked, showing improvements in binary and gender-neutral pronoun accuracy. However, inconsistencies are observed in handling neopronouns and reverse inference tasks, highlighting ongoing gaps in identity-sensitive reasoning by LLMs. The results indicate progress in addressing pronoun usage challenges in responsible AI, with implications for inclusive AI research and potential avenues for future improvements. <div>
arXiv:2508.00788v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
<div> Dynamic Adaptive Length Expansion, Denoising, Diffusion Large Language Models, Task-appropriate length, Computational efficiency
<br />
Summary:
DAEDAL is a novel denoising strategy for Diffusion Large Language Models (DLLMs) that enables dynamic adaptive length expansion. It addresses the static length constraint of DLLMs by iteratively expanding the generation length based on internal signals and intervening to expand insufficient regions during the denoising process. Through extensive experiments, DAEDAL achieves comparable or superior performance to fixed-length baselines while improving computational efficiency by increasing the effective token ratio. By bridging this critical gap in DLLMs, DAEDAL unlocks new potential for efficient and capable generation, paving the way for advancements in language modeling technology. <div>
arXiv:2508.00819v1 Announce Type: new 
Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models</title>
<link>https://arxiv.org/abs/2508.00028</link>
<guid>https://arxiv.org/abs/2508.00028</guid>
<content:encoded><![CDATA[
<div> Markov chain model, spectrum availability prediction, propagation models, cognitive radio networks, dynamic spectrum access<br />
<br />
Summary:<br />
The paper introduces a framework for predicting spectrum availability, combining a two-state Markov chain model and ITU-R propagation models to enhance accuracy in identifying available spectrum in both time and space. The Markov chain captures primary user activity patterns over time, while propagation models consider path loss and clutter effects to determine interference thresholds at secondary user locations. The proposed method offers improved accuracy in predicting spectrum opportunities, with low computational costs, making it suitable for real-time spectrum management in cognitive radio networks and dynamic spectrum sharing systems. The scalability and efficiency of the framework are analyzed, highlighting its flexibility for adaptation to various scenarios and frequency bands. The results show that the approach effectively identifies available spectrum, demonstrating its potential for practical applications in dynamic spectrum access. <br /><br /> <div>
arXiv:2508.00028v1 Announce Type: cross 
Abstract: Spectrum resources are often underutilized across time and space, motivating dynamic spectrum access strategies that allow secondary users to exploit unused frequencies. A key challenge is predicting when and where spectrum will be available (i.e., unused by primary licensed users) in order to enable proactive and interference-free access. This paper proposes a scalable framework for spectrum availability prediction that combines a two-state Markov chain model of primary user activity with high-fidelity propagation models from the ITU-R (specifically Recommendations P.528 and P.2108). The Markov chain captures temporal occupancy patterns, while the propagation models incorporate path loss and clutter effects to determine if primary signals exceed interference thresholds at secondary user locations. By integrating these components, the proposed method can predict spectrum opportunities both in time and space with improved accuracy. We develop the system model and algorithm for the approach, analyze its scalability and computational efficiency, and discuss assumptions, limitations, and potential applications. The framework is flexible and can be adapted to various frequency bands and scenarios. The results and analysis show that the proposed approach can effectively identify available spectrum with low computational cost, making it suitable for real-time spectrum management in cognitive radio networks and other dynamic spectrum sharing systems.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries</title>
<link>https://arxiv.org/abs/2508.00033</link>
<guid>https://arxiv.org/abs/2508.00033</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Python code generation, conversational data analysis, synthetic data generation, model evaluation <br />
Summary: 
Large Language Models (LLMs) are widely used for automating code generation in scientific research. This study benchmarks state-of-the-art LLMs in generating Python code for complex tasks using unfamiliar APIs. Two scenarios, conversational data analysis and synthetic data generation, were tested with structured prompts. Results showed that only a few models consistently produced correct code, with GPT-4.1 performing the best. The study also identified shortcomings in third-party libraries, highlighting the importance of clear documentation and prompt design. These findings underscore the current limitations of LLMs in scientific automation and emphasize the need for further advancements in model capabilities. <br /><br />Summary: <div>
arXiv:2508.00033v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \textit{ParShift} library, and synthetic data generation and clustering using \textit{pyclugen} and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code, with GPT-4.1 standing out as the only model to always succeed in both tasks. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Code Generation with LLM-based Agents</title>
<link>https://arxiv.org/abs/2508.00083</link>
<guid>https://arxiv.org/abs/2508.00083</guid>
<content:encoded><![CDATA[
<div> autonomy, task scope, engineering practicality, LLM, code generation agents
Summary:
Code generation agents powered by large language models (LLMs) are transforming software development with autonomy to manage the workflow independently, expanded task scope covering the full SDLC, and emphasis on engineering practicality. This survey examines the evolution of LLM-based code generation agents, categorizing techniques including single-agent and multi-agent architectures. Applications across the SDLC are discussed, along with evaluation benchmarks and representative tools. Challenges are identified, leading to proposed long-term research directions for the field. <div>
arXiv:2508.00083v1 Announce Type: cross 
Abstract: Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs</title>
<link>https://arxiv.org/abs/2508.00161</link>
<guid>https://arxiv.org/abs/2508.00161</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, interpretability, fine-tuning, backdoors, model auditing

Summary:
This work introduces a new method for interpreting, monitoring, and controlling fine-tuned large language models (LLMs) by focusing on weights instead of activations. By analyzing the top singular vectors of weight differences between a fine-tuned model and its base model, it can detect newly acquired behaviors with high precision. This method is effective in detecting backdoored models that can bypass safety mechanisms with a minimal false positive rate. Furthermore, it can also detect inference on erased topics in models that have undergone unlearning and even steer the model to recover "unlearned" information. This approach shows potential for pre-deployment model auditing by uncovering model-specific fine-tuning focuses, such as marketing strategies and prompt generation. The implementation of this method is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2508.00161v1 Announce Type: cross 
Abstract: The releases of powerful open-weight large language models (LLMs) are often not accompanied by access to their full training data. Existing interpretability methods, particularly those based on activations, often require or assume distributionally similar data. This is a significant limitation when detecting and defending against novel potential threats like backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and controlling fine-tuned LLMs that interprets weights, rather than activations, thereby side stepping the need for data that is distributionally similar to the unknown training data. We demonstrate that the top singular vectors of the weight difference between a fine-tuned model and its base model correspond to newly acquired behaviors. By monitoring the cosine similarity of activations along these directions, we can detect salient behaviors introduced during fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger is present, our method stops up to 100% of attacks with a false positive rate below 1.2%. For models that have undergone unlearning, we detect inference on erased topics with accuracy up to 95.42% and can even steer the model to recover "unlearned" information. Besides monitoring, our method also shows potential for pre-deployment model auditing: by analyzing commercial instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover model-specific fine-tuning focus including marketing strategies and Midjourney prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI</title>
<link>https://arxiv.org/abs/2508.00171</link>
<guid>https://arxiv.org/abs/2508.00171</guid>
<content:encoded><![CDATA[
<div> medical imaging, Vision-Language Models, Selective Modality Shifting, modality-specific biases, multimodal models <br />
Summary: <br />
The study introduces Selective Modality Shifting (SMS) to assess biases in Vision-Language Models (VLMs) towards different modalities in medical image analysis. Six VLMs were evaluated on chest X-ray and scanning laser ophthalmoscopy datasets, revealing a strong reliance on text input over visual cues. Despite the presence of complementary visual information, models often favored text details, as confirmed by qualitative attention-based analysis. The study emphasizes the necessity of designing and evaluating multimodal medical models that integrate visual and textual cues effectively, rather than relying solely on single-modality signals. <div>
arXiv:2508.00171v1 Announce Type: cross 
Abstract: Clinical decision-making relies on the integrated analysis of medical images and the associated clinical reports. While Vision-Language Models (VLMs) can offer a unified framework for such tasks, they can exhibit strong biases toward one modality, frequently overlooking critical visual cues in favor of textual information. In this work, we introduce Selective Modality Shifting (SMS), a perturbation-based approach to quantify a model's reliance on each modality in binary classification tasks. By systematically swapping images or text between samples with opposing labels, we expose modality-specific biases. We assess six open-source VLMs-four generalist models and two fine-tuned for medical data-on two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray) and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance and the calibration of every model in both unperturbed and perturbed settings, we reveal a marked dependency on text input, which persists despite the presence of complementary visual information. We also perform a qualitative attention-based analysis which further confirms that image content is often overshadowed by text details. Our findings highlight the importance of designing and evaluating multimodal medical models that genuinely integrate visual and textual cues, rather than relying on single-modality signals.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization</title>
<link>https://arxiv.org/abs/2508.00222</link>
<guid>https://arxiv.org/abs/2508.00222</guid>
<content:encoded><![CDATA[
arXiv:2508.00222v1 Announce Type: cross 
Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its inherently on-policy strategy with LLM's immense action space and sparse reward. Further, RLVR can lead to the capability boundary collapse, narrowing the LLM's problem-solving scope. To address this problem, we propose RL-PLUS, a novel approach that synergizes internal exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components: Multiple Importance Sampling to address for distributional mismatch from external data, and an Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. The results show that RL-PLUS achieves state-of-the-art performance compared with existing RLVR methods on six math reasoning benchmarks and exhibits superior performance on six out-of-distribution reasoning tasks. It also achieves consistent and significant gains across diverse model families, with average relative improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across multiple benchmarks indicate that RL-PLUS effectively resolves the capability boundary collapse problem.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product</title>
<link>https://arxiv.org/abs/2508.00230</link>
<guid>https://arxiv.org/abs/2508.00230</guid>
<content:encoded><![CDATA[
arXiv:2508.00230v1 Announce Type: cross 
Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation (LoRA) has achieved notable success. However, recent studies have highlighted its limitations compared against full-rank alternatives, particularly when applied to multimodal and large language models. In this work, we present a quantitative comparison amongst full-rank and low-rank PEFT methods using a synthetic matrix approximation benchmark with controlled spectral properties. Our results confirm that LoRA struggles to approximate matrices with relatively flat spectrums or high frequency components -- signs of high effective ranks. To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the Khatri-Rao product to produce weight updates, which, by construction, tends to produce matrix product with a high effective rank. We demonstrate performance gains with KRAdapter on vision-language models up to 1B parameters and on large language models up to 8B parameters, particularly on unseen common-sense reasoning tasks. In addition, KRAdapter maintains the memory and compute efficiency of LoRA, making it a practical and robust alternative to fine-tune billion-scale parameter models.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</title>
<link>https://arxiv.org/abs/2508.00271</link>
<guid>https://arxiv.org/abs/2508.00271</guid>
<content:encoded><![CDATA[
arXiv:2508.00271v1 Announce Type: cross 
Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: The Divergence Between Human and LLM-Generated Tasks</title>
<link>https://arxiv.org/abs/2508.00282</link>
<guid>https://arxiv.org/abs/2508.00282</guid>
<content:encoded><![CDATA[
arXiv:2508.00282v1 Announce Type: cross 
Abstract: Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied goals.We conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge</title>
<link>https://arxiv.org/abs/2508.00324</link>
<guid>https://arxiv.org/abs/2508.00324</guid>
<content:encoded><![CDATA[
arXiv:2508.00324v1 Announce Type: cross 
Abstract: Although large reasoning models (LRMs) have demonstrated impressive capabilities on complex tasks, recent studies reveal that these models frequently fulfill harmful user instructions, raising significant safety concerns. In this paper, we investigate the underlying cause of LRM safety risks and find that models already possess sufficient safety knowledge but fail to activate it during reasoning. Based on this insight, we propose R1-Act, a simple and efficient post-training method that explicitly triggers safety knowledge through a structured reasoning process. R1-Act achieves strong safety improvements while preserving reasoning performance, outperforming prior alignment methods. Notably, it requires only 1,000 training examples and 90 minutes of training on a single RTX A6000 GPU. Extensive experiments across multiple LRM backbones and sizes demonstrate the robustness, scalability, and practical efficiency of our approach.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs for Unit Test Generation from Real-World Functions</title>
<link>https://arxiv.org/abs/2508.00408</link>
<guid>https://arxiv.org/abs/2508.00408</guid>
<content:encoded><![CDATA[
arXiv:2508.00408v1 Announce Type: cross 
Abstract: Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new benchmark specifically designed for function-level unit test generation from real-world Python functions. ULT is constructed through a multi-stage curation process that ensures high cyclomatic complexity and mitigates test case contamination. With 3,909 carefully selected function-level tasks, ULT provides a more realistic and challenging evaluation of LLMs' test generation capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT with leaked tests designed to enable a controlled analysis of memorization versus reasoning in test generation. Our evaluation results demonstrate that ULT is significantly more challenging. For example, test cases generated by LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy, statement coverage, branch coverage, and mutation score on average for all LLMs, respectively. These results are substantially lower than the corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</title>
<link>https://arxiv.org/abs/2508.00414</link>
<guid>https://arxiv.org/abs/2508.00414</guid>
<content:encoded><![CDATA[
arXiv:2508.00414v1 Announce Type: cross 
Abstract: General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-grained Spatiotemporal Grounding on Egocentric Videos</title>
<link>https://arxiv.org/abs/2508.00518</link>
<guid>https://arxiv.org/abs/2508.00518</guid>
<content:encoded><![CDATA[
arXiv:2508.00518v1 Announce Type: cross 
Abstract: Spatiotemporal video grounding aims to localize target entities in videos based on textual queries. While existing research has made significant progress in exocentric videos, the egocentric setting remains relatively underexplored, despite its growing importance in applications such as augmented reality and robotics. In this work, we conduct a systematic analysis of the discrepancies between egocentric and exocentric videos, revealing key challenges such as shorter object durations, sparser trajectories, smaller object sizes, and larger positional shifts. To address these challenges, we introduce EgoMask, the first pixel-level benchmark for fine-grained spatiotemporal grounding in egocentric videos. It is constructed by our proposed automatic annotation pipeline, which annotates referring expressions and object masks across short-, medium-, and long-term videos. Additionally, we create EgoMask-Train, a large-scale training dataset to facilitate model development. Experiments demonstrate that the state-of-the-art spatiotemporal grounding models perform poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields significant improvements, while preserving performance on exocentric datasets. Our work thus provides essential resources and insights for advancing egocentric video understanding. Our code is available at https://github.com/LaVi-Lab/EgoMask .
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations</title>
<link>https://arxiv.org/abs/2508.00534</link>
<guid>https://arxiv.org/abs/2508.00534</guid>
<content:encoded><![CDATA[
arXiv:2508.00534v1 Announce Type: cross 
Abstract: The rise of multi-paradigm languages challenges traditional classification methods, leading to practical software engineering issues like interoperability defects. This systematic literature review (SLR) maps the formal foundations of programming paradigms. Our objective is twofold: (1) to assess the state of the art of classification formalisms and their limitations, and (2) to identify the conceptual primitives and mathematical frameworks for a more powerful, reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies lack conceptual granularity, a unified formal basis, and struggle with hybrid languages. In response, our analysis reveals a strong convergence toward a compositional reconstruction of paradigms. This approach identifies a minimal set of orthogonal, atomic primitives and leverages mathematical frameworks, predominantly Type theory, Category theory and Unifying Theories of Programming (UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift away from classification towards these promising formal, reconstructive frameworks. This review provides a map of this evolution and proposes a research agenda for their unification.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism</title>
<link>https://arxiv.org/abs/2508.00554</link>
<guid>https://arxiv.org/abs/2508.00554</guid>
<content:encoded><![CDATA[
arXiv:2508.00554v1 Announce Type: cross 
Abstract: In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Guided Local Editing for Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2508.00555</link>
<guid>https://arxiv.org/abs/2508.00555</guid>
<content:encoded><![CDATA[
arXiv:2508.00555v1 Announce Type: cross 
Abstract: Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at https://github.com/yunsaijc/AGILE.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving</title>
<link>https://arxiv.org/abs/2508.00589</link>
<guid>https://arxiv.org/abs/2508.00589</guid>
<content:encoded><![CDATA[
arXiv:2508.00589v1 Announce Type: cross 
Abstract: Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demo: TOSense -- What Did You Just Agree to?</title>
<link>https://arxiv.org/abs/2508.00659</link>
<guid>https://arxiv.org/abs/2508.00659</guid>
<content:encoded><![CDATA[
arXiv:2508.00659v1 Announce Type: cross 
Abstract: Online services often require users to agree to lengthy and obscure Terms of Service (ToS), leading to information asymmetry and legal risks. This paper proposes TOSense-a Chrome extension that allows users to ask questions about ToS in natural language and get concise answers in real time. The system combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and (ii) a lightweight large language model pipeline: MiniLM for semantic retrieval and BART-encoder for answer relevance verification. To avoid expensive manual annotation, we present a novel Question Answering Evaluation Pipeline (QEP) that generates synthetic questions and verifies the correctness of answers using clustered topic matching. Experiments on five major platforms, Apple, Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of TOSense (with up to 44.5% accuracy) across varying number of topic clusters. During the demonstration, we will showcase TOSense in action. Attendees will be able to experience seamless extraction, interactive question answering, and instant indexing of new sites.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach</title>
<link>https://arxiv.org/abs/2508.00695</link>
<guid>https://arxiv.org/abs/2508.00695</guid>
<content:encoded><![CDATA[
arXiv:2508.00695v1 Announce Type: cross 
Abstract: The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder. In this study, we compare the performance of various Artificial Intelligence models, including both traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with BERT-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge</title>
<link>https://arxiv.org/abs/2412.10207</link>
<guid>https://arxiv.org/abs/2412.10207</guid>
<content:encoded><![CDATA[
arXiv:2412.10207v2 Announce Type: replace 
Abstract: Open-domain semantic parsing remains a challenging task, as neural models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external symbolic knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage</title>
<link>https://arxiv.org/abs/2501.02039</link>
<guid>https://arxiv.org/abs/2501.02039</guid>
<content:encoded><![CDATA[
arXiv:2501.02039v3 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in LLM Writing Assistance</title>
<link>https://arxiv.org/abs/2502.08395</link>
<guid>https://arxiv.org/abs/2502.08395</guid>
<content:encoded><![CDATA[
arXiv:2502.08395v2 Announce Type: replace 
Abstract: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Embeddings with Coupled Adam</title>
<link>https://arxiv.org/abs/2502.08441</link>
<guid>https://arxiv.org/abs/2502.08441</guid>
<content:encoded><![CDATA[
arXiv:2502.08441v3 Announce Type: replace 
Abstract: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEFL: Enhancing Educational Assignment Feedback with LLM Agents</title>
<link>https://arxiv.org/abs/2502.12927</link>
<guid>https://arxiv.org/abs/2502.12927</guid>
<content:encoded><![CDATA[
arXiv:2502.12927v2 Announce Type: replace 
Abstract: Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Space: Finding the Right Tokens for Structured Output</title>
<link>https://arxiv.org/abs/2502.14969</link>
<guid>https://arxiv.org/abs/2502.14969</guid>
<content:encoded><![CDATA[
arXiv:2502.14969v2 Announce Type: replace 
Abstract: General-purpose language models are trained to produce varied natural language outputs, but for some tasks, like annotation or classification, we need more specific output formats. LLM systems increasingly support structured output, which enforces formats by sampling tokens according to a grammar -- but also unpredictably reduces downstream performance. Are there systematic differences between grammars that appear semantically (and often visually) similar to humans? To answer this, we test four popular model families with five varying output formats on four common NLP benchmarks. We find all models perform most accurately when guided to use formats respecting convention, such as letters for multiple choice and real numbers for numerical prediction. Performance also improves by 5%-10% when guiding models to return tokens incorporating leading whitespace, with smaller models benefiting the most. We find leading whitespace helps models avoid structural deficiencies in subword token representations. We finally present best practices for researchers using language models as zero-shot classifiers with structured output.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Generalizability of Test-Time Scaling in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2502.17407</link>
<guid>https://arxiv.org/abs/2502.17407</guid>
<content:encoded><![CDATA[
arXiv:2502.17407v2 Announce Type: replace 
Abstract: Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Know How Much They Know?</title>
<link>https://arxiv.org/abs/2502.19573</link>
<guid>https://arxiv.org/abs/2502.19573</guid>
<content:encoded><![CDATA[
arXiv:2502.19573v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Post-training of Large Language Models</title>
<link>https://arxiv.org/abs/2503.06072</link>
<guid>https://arxiv.org/abs/2503.06072</guid>
<content:encoded><![CDATA[
arXiv:2503.06072v3 Announce Type: replace 
Abstract: The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation</title>
<link>https://arxiv.org/abs/2503.19693</link>
<guid>https://arxiv.org/abs/2503.19693</guid>
<content:encoded><![CDATA[
arXiv:2503.19693v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemInsight: Autonomous Memory Augmentation for LLM Agents</title>
<link>https://arxiv.org/abs/2503.21760</link>
<guid>https://arxiv.org/abs/2503.21760</guid>
<content:encoded><![CDATA[
arXiv:2503.21760v2 Announce Type: replace 
Abstract: Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol</title>
<link>https://arxiv.org/abs/2504.10284</link>
<guid>https://arxiv.org/abs/2504.10284</guid>
<content:encoded><![CDATA[
arXiv:2504.10284v3 Announce Type: replace 
Abstract: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles</title>
<link>https://arxiv.org/abs/2504.12312</link>
<guid>https://arxiv.org/abs/2504.12312</guid>
<content:encoded><![CDATA[
arXiv:2504.12312v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories</title>
<link>https://arxiv.org/abs/2504.16604</link>
<guid>https://arxiv.org/abs/2504.16604</guid>
<content:encoded><![CDATA[
arXiv:2504.16604v2 Announce Type: replace 
Abstract: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credible Plan-Driven RAG Method for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2504.16787</link>
<guid>https://arxiv.org/abs/2504.16787</guid>
<content:encoded><![CDATA[
arXiv:2504.16787v2 Announce Type: replace 
Abstract: Multi-hop question answering (QA) presents significant challenges for retrieval-augmented generation (RAG), particularly in decomposing complex queries into reliable reasoning paths and managing error propagation. Existing RAG methods often suffer from deviations in reasoning paths and cumulative errors in intermediate steps, reducing the fidelity of the final answer. To address these limitations, we propose PAR-RAG (Plan-then-Act-and-Review RAG), a novel framework inspired by the PDCA (Plan-Do-Check-Act) cycle, to enhance both the accuracy and factual consistency in multi-hop question answering. Specifically, PAR-RAG selects exemplars matched by the semantic complexity of the current question to guide complexity-aware top-down planning, resulting in more precise and coherent multi-step reasoning trajectories. This design mitigates reasoning drift and reduces the risk of suboptimal path convergence, a common issue in existing RAG approaches. Furthermore, a dual-verification mechanism evaluates and corrects intermediate errors, ensuring that the reasoning process remains factually grounded. Experimental results on various QA benchmarks demonstrate that PAR-RAG outperforms existing state-of-the-art methods, validating its effectiveness in both performance and reasoning robustness.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory</title>
<link>https://arxiv.org/abs/2505.15055</link>
<guid>https://arxiv.org/abs/2505.15055</guid>
<content:encoded><![CDATA[
arXiv:2505.15055v2 Announce Type: replace 
Abstract: The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining mainstream prominent LLM benchmarks using results from diverse models. We first propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. PSN-IRT can be utilized for accurate and reliable estimations of item characteristics and model abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM benchmarks comprising 41,871 items, revealing significant and varied shortcomings in their measurement quality. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs</title>
<link>https://arxiv.org/abs/2505.17217</link>
<guid>https://arxiv.org/abs/2505.17217</guid>
<content:encoded><![CDATA[
arXiv:2505.17217v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMixer: Checkpoint Artifacts as Automatic Data Mixers</title>
<link>https://arxiv.org/abs/2506.21910</link>
<guid>https://arxiv.org/abs/2506.21910</guid>
<content:encoded><![CDATA[
arXiv:2506.21910v2 Announce Type: replace 
Abstract: In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with performance improvements of up to 1.93%. Overall, this shows the potential of checkpoint models to enhance data quality and optimize data mixtures.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
<link>https://arxiv.org/abs/2507.02962</link>
<guid>https://arxiv.org/abs/2507.02962</guid>
<content:encoded><![CDATA[
arXiv:2507.02962v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while LLMs remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have aimed to enhance models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to reliance on single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, with the aim of reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Encode Harmfulness and Refusal Separately</title>
<link>https://arxiv.org/abs/2507.11878</link>
<guid>https://arxiv.org/abs/2507.11878</guid>
<content:encoded><![CDATA[
arXiv:2507.11878v2 Announce Type: replace 
Abstract: LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. There exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Loss Landscape Degeneracy and Stagewise Development in Transformers</title>
<link>https://arxiv.org/abs/2402.02364</link>
<guid>https://arxiv.org/abs/2402.02364</guid>
<content:encoded><![CDATA[
arXiv:2402.02364v3 Announce Type: replace-cross 
Abstract: Deep learning involves navigating a high-dimensional loss landscape over the neural network parameter space. Over the course of training, complex computational structures form and re-form inside the neural network, leading to shifts in input/output behavior. It is a priority for the science of deep learning to uncover principles governing the development of neural network structure and behavior. Drawing on the framework of singular learning theory, we propose that model development is deeply linked to degeneracy in the local geometry of the loss landscape. We investigate this link by monitoring loss landscape degeneracy throughout training, as quantified by the local learning coefficient, for a transformer language model and an in-context linear regression transformer. We show that training can be divided into distinct periods of change in loss landscape degeneracy, and that these changes in degeneracy coincide with significant changes in the internal computational structure and the input/output behavior of the transformers. This finding provides suggestive evidence that degeneracy and development are linked in transformers, underscoring the potential of a degeneracy-based perspective for understanding modern deep learning.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors</title>
<link>https://arxiv.org/abs/2409.18203</link>
<guid>https://arxiv.org/abs/2409.18203</guid>
<content:encoded><![CDATA[
arXiv:2409.18203v2 Announce Type: replace-cross 
Abstract: AI policy sets boundaries on acceptable behavior for AI models, but this is challenging in the context of large language models (LLMs): how do you ensure coverage over a vast behavior space? We introduce policy maps, an approach to AI policy design inspired by the practice of physical mapmaking. Instead of aiming for full coverage, policy maps aid effective navigation through intentional design choices about which aspects to capture and which to abstract away. With Policy Projector, an interactive tool for designing LLM policy maps, an AI practitioner can survey the landscape of model input-output pairs, define custom regions (e.g., "violence"), and navigate these regions with if-then policy rules that can act on LLM outputs (e.g., if output contains "violence" and "graphic details," then rewrite without "graphic details"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the AI practitioner's work. In an evaluation with 12 AI safety experts, our system helps policy designers craft policies around problematic model behaviors such as incorrect gender assumptions and handling of immediate physical safety threats.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-Video: Video Instruction Tuning With Synthetic Data</title>
<link>https://arxiv.org/abs/2410.02713</link>
<guid>https://arxiv.org/abs/2410.02713</guid>
<content:encoded><![CDATA[
arXiv:2410.02713v3 Announce Type: replace-cross 
Abstract: The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlocking Multi-Modal Potentials for Link Prediction on Dynamic Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2502.19651</link>
<guid>https://arxiv.org/abs/2502.19651</guid>
<content:encoded><![CDATA[
arXiv:2502.19651v2 Announce Type: replace-cross 
Abstract: Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal events (edges) alongside rich textual attributes. Existing studies can be broadly categorized into TGNN-driven and LLM-driven approaches, both of which encode textual attributes and temporal structures for DyTAG representation. We observe that DyTAGs inherently comprise three distinct modalities: temporal, textual, and structural, often exhibiting completely disjoint distributions. However, the first two modalities are largely overlooked by existing studies, leading to suboptimal performance. To address this, we propose MoMent, a multi-modal model that explicitly models, integrates, and aligns each modality to learn node representations for link prediction. Given the disjoint nature of the original modality distributions, we first construct modality-specific features and encode them using individual encoders to capture correlations across temporal patterns, semantic context, and local structures. Each encoder generates modality-specific tokens, which are then fused into comprehensive node representations with a theoretical guarantee. To avoid disjoint subspaces of these heterogeneous modalities, we propose a dual-domain alignment loss that first aligns their distributions globally and then fine-tunes coherence at the instance level. This enhances coherent representations from temporal, textual, and structural views. Extensive experiments across seven datasets show that MoMent achieves up to 17.28% accuracy improvement and up to 31x speed-up against eight baselines.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation</title>
<link>https://arxiv.org/abs/2503.22675</link>
<guid>https://arxiv.org/abs/2503.22675</guid>
<content:encoded><![CDATA[
arXiv:2503.22675v3 Announce Type: replace-cross 
Abstract: Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose \textbf{ReaRec}, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding</title>
<link>https://arxiv.org/abs/2507.02659</link>
<guid>https://arxiv.org/abs/2507.02659</guid>
<content:encoded><![CDATA[
arXiv:2507.02659v2 Announce Type: replace-cross 
Abstract: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Real-World Forecasting Against Human Superforecasters</title>
<link>https://arxiv.org/abs/2507.04562</link>
<guid>https://arxiv.org/abs/2507.04562</guid>
<content:encoded><![CDATA[
arXiv:2507.04562v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, but their ability to forecast future events remains understudied. A year ago, large language models struggle to come close to the accuracy of a human crowd. I evaluate state-of-the-art LLMs on 464 forecasting questions from Metaculus, comparing their performance against human superforecasters. Frontier models achieve Brier scores that ostensibly surpass the human crowd but still significantly underperform a group of superforecasters.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sound and Complete Neurosymbolic Reasoning with LLM-Grounded Interpretations</title>
<link>https://arxiv.org/abs/2507.09751</link>
<guid>https://arxiv.org/abs/2507.09751</guid>
<content:encoded><![CDATA[
arXiv:2507.09751v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neurosymbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.
]]></content:encoded>
<pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in the Travel Domain: An Industrial Experience</title>
<link>https://arxiv.org/abs/2507.22910</link>
<guid>https://arxiv.org/abs/2507.22910</guid>
<content:encoded><![CDATA[
<div> property booking platforms, Large Language Models, data sources, consistency, reliability

Summary: 
- Online property booking platforms rely on up-to-date information from third-party providers.
- Incomplete or inconsistent details can impact user experience and market share.
- The study integrates Large Language Models (LLMs) into CALEIDOHOTELS to improve data quality.
- Two LLMs, Mistral 7B and Mixtral 8x7B, were evaluated for consistency and reliability.
- Mixtral 8x7B outperformed Mistral 7B in completeness, precision, and hallucination rate but at a higher computational cost.
- The findings highlight the trade-offs between model quality and resource efficiency in production environments. <div>
arXiv:2507.22910v1 Announce Type: new 
Abstract: Online property booking platforms are widely used and rely heavily on consistent, up-to-date information about accommodation facilities, often sourced from third-party providers. However, these external data sources are frequently affected by incomplete or inconsistent details, which can frustrate users and result in a loss of market. In response to these challenges, we present an industrial case study involving the integration of Large Language Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B, fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt. Both models were assessed based on their ability to generate consistent and homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet more concise content (249 vs. 277 words on average). However, this came at a significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB and $0.16/hour for Mistral 7B. Our findings provide practical insights into the trade-offs between model quality and resource efficiency, offering guidance for deploying LLMs in production environments and demonstrating their effectiveness in enhancing the consistency and reliability of accommodation data.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing</title>
<link>https://arxiv.org/abs/2507.22911</link>
<guid>https://arxiv.org/abs/2507.22911</guid>
<content:encoded><![CDATA[
<div> Keywords: Electric power marketing, Customer service, Large language models, Dialogue dataset, Knowledge augmentation

Summary:
Electric power marketing customer service is vital for addressing inquiries efficiently. Current systems like China's 95598 hotline often face challenges like slow response times and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o showcase strong general capabilities, they lack domain expertise and empathy necessary for this field. To address this gap, the researchers introduce ElectriQ, a new benchmark designed to evaluate and enhance LLMs in electric power marketing contexts. ElectriQ includes a dialogue dataset covering key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. By integrating domain-specific knowledge and employing a knowledge augmentation method, smaller models like LLama3-8B, when fine-tuned and augmented, outperform larger models like GPT-4o in professionalism and user-friendliness. ElectriQ lays the groundwork for developing LLMs tailored to the specific needs of power marketing services. 

<br /><br />Summary: <div>
arXiv:2507.22911v1 Announce Type: new 
Abstract: Electric power marketing customer service plays a critical role in addressing inquiries, complaints, and service requests. However, current systems, such as China's 95598 hotline, often struggle with slow response times, inflexible procedures, and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities, they lack the domain expertise and empathy required in this field. To bridge this gap, we introduce ElectriQ, the first benchmark designed to evaluate and enhance LLMs in electric power marketing scenarios. ElectriQ consists of a dialogue dataset covering six key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. We further incorporate a domain-specific knowledge base and propose a knowledge augmentation method to boost model performance. Experiments on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and augmented, can surpass GPT-4o in terms of professionalism and user-friendliness. ElectriQ establishes a comprehensive foundation for developing LLMs tailored to the needs of power marketing services.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms</title>
<link>https://arxiv.org/abs/2507.22912</link>
<guid>https://arxiv.org/abs/2507.22912</guid>
<content:encoded><![CDATA[
<div> Keywords: illegal marketplaces, deep and dark web, language models, semi-supervised learning, illicit content detection 

Summary: 
This paper presents a hierarchical classification framework that utilizes fine-tuned language models and a semi-supervised ensemble learning strategy to detect and categorize illicit marketplace content across various online platforms. By combining semantic representations extracted from a specialized transformer model with manually engineered features, the framework can effectively identify illicit sales-related documents and classify them into drug, weapon, or credential categories. Experiments on multiple datasets demonstrate that the model outperforms several baselines, showcasing its accuracy, F1-score, and robustness in detecting illicit content. The proposed approach showcases strong generalization abilities, robustness under limited supervision, and effectiveness in real-world scenarios of illicit content detection. 

Summary: <div>
arXiv:2507.22912v1 Announce Type: new 
Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the internet, including the deep and dark web, as well as platforms such as Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of illicit goods including drugs, weapons, and stolen credentials. Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources. This paper presents a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy to detect and classify illicit marketplace content across diverse platforms. We extract semantic representations using ModernBERT, a transformer model for long documents, finetuned on domain-specific data from deep and dark web pages, Telegram channels, Subreddits, and Pastebin pastes to capture specialized jargon and ambiguous linguistic patterns. In addition, we incorporate manually engineered features such as document structure, embedded patterns including Bitcoin addresses, emails, and IPs, and metadata, which complement language model embeddings. The classification pipeline operates in two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random Forest, and SVM with entropy-based weighted voting to detect sales-related documents. The second stage further classifies these into drug, weapon, or credential sales. Experiments on three datasets, including our multi-source corpus, DUTA, and CoDA, show that our model outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models</title>
<link>https://arxiv.org/abs/2507.22913</link>
<guid>https://arxiv.org/abs/2507.22913</guid>
<content:encoded><![CDATA[
<div> Keywords: subject access, large language models, multi-label classification, hybrid framework, Library of Congress Subject Headings <br />
<br />
Summary: <br />
Providing subject access to information resources is crucial for library management systems. While large language models (LLMs) have been used for classification tasks, they are not extensively explored for subject analysis. Traditional machine learning (ML) models struggle with unseen cases in subject analysis, prompting the need for a hybrid framework that combines embedding-based ML models with LLMs. This framework utilizes ML models to predict the optimal number of LCSH labels and post-edit LLM predictions with actual LCSH terms to address hallucinations. Experimental results demonstrate that providing initial predictions and post-editing improve the control and vocabulary alignment of subject term predictions. This approach offers a promising method for enhancing subject analysis using LLMs in libraries. <br /> <div>
arXiv:2507.22913v1 Announce Type: new 
Abstract: Providing subject access to information resources is an essential function of any library management system. Large language models (LLMs) have been widely used in classification and summarization tasks, but their capability to perform subject analysis is underexplored. Multi-label classification with traditional machine learning (ML) models has been used for subject analysis but struggles with unseen cases. LLMs offer an alternative but often over-generate and hallucinate. Therefore, we propose a hybrid framework that integrates embedding-based ML models with LLMs. This approach uses ML models to (1) predict the optimal number of LCSH labels to guide LLM predictions and (2) post-edit the predicted terms with actual LCSH terms to mitigate hallucinations. We experimented with LLMs and the hybrid framework to predict the subject terms of books using the Library of Congress Subject Headings (LCSH). Experiment results show that providing initial predictions to guide LLM generations and imposing post-edits result in more controlled and vocabulary-aligned outputs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.22914</link>
<guid>https://arxiv.org/abs/2507.22914</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, schema matching, entity matching, context matching, KG integration<br />
Summary:<br />
The study addresses the challenge of context matching in knowledge graphs (KGs). Unlike schema and identity matching, context matching is less explored despite its importance in integrating diverse and complex KGs. The proposed method includes label and triple matching using string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. By identifying mappings between triples conveying comparable information, entity-matching accuracy is improved. The approach demonstrates competitive performance in the OAEI competition and against supervised methods, achieving high accuracy in various test cases. Additionally, a new dataset is introduced to comprehensively evaluate the triple-matching step. <div>
arXiv:2507.22914v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over structured information. Their main components include schema, identity, and context. While schema and identity matching are well-established in ontology and entity matching research, context matching remains largely unexplored. This is particularly important because real-world KGs often vary significantly in source, size, and information density - factors not typically represented in the datasets on which current entity matching methods are evaluated. As a result, existing approaches may fall short in scenarios where diverse and complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of label matching and triple matching. We use string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. Next, we identify mappings between triples that convey comparable information, using these mappings to improve entity-matching accuracy. Our approach demonstrates competitive performance compared to leading systems in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases. Additionally, we introduce a new dataset derived from the benchmark dataset to evaluate the triple-matching step more comprehensively.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Foundations and Mitigation of Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2507.22915</link>
<guid>https://arxiv.org/abs/2507.22915</guid>
<content:encoded><![CDATA[
<div> hallucination, Large Language Models, formal definitions, theoretical analyses, mitigation strategies
Summary: 
Hallucination in Large Language Models (LLMs) is explored in this paper through formal definitions, theoretical analyses, and the distinction between intrinsic and extrinsic hallucinations. The concept of a hallucination risk for models is introduced, with bounds derived through learning-theoretic frameworks. Detection strategies such as token-level uncertainty estimation and attention alignment checks are surveyed. Mitigation approaches including retrieval-augmented generation and fact-verification modules are discussed. A unified detection and mitigation workflow is proposed to integrate these strategies. Evaluation protocols are outlined to quantify and reduce hallucinations, recommending datasets, metrics, and experimental setups. This work provides a theoretical foundation and practical guidelines for addressing the challenge of hallucination in LLMs.<br /><br />Summary: <div>
arXiv:2507.22915v1 Announce Type: new 
Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of content that is not faithful to the input or the real-world facts. This paper provides a rigorous treatment of hallucination in LLMs, including formal definitions and theoretical analyses. We distinguish between intrinsic and extrinsic hallucinations, and define a \textit{hallucination risk} for models. We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes and Rademacher complexity). We then survey detection strategies for hallucinations, such as token-level uncertainty estimation, confidence calibration, and attention alignment checks. On the mitigation side, we discuss approaches including retrieval-augmented generation, hallucination-aware fine-tuning, logit calibration, and the incorporation of fact-verification modules. We propose a unified detection and mitigation workflow, illustrated with a diagram, to integrate these strategies. Finally, we outline evaluation protocols for hallucination, recommending datasets, metrics, and experimental setups to quantify and reduce hallucinations. Our work lays a theoretical foundation and practical guidelines for addressing the crucial challenge of hallucination in LLMs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reading Between the Timelines: RAG for Answering Diachronic Questions</title>
<link>https://arxiv.org/abs/2507.22917</link>
<guid>https://arxiv.org/abs/2507.22917</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, temporal logic, longitudinal queries, diachronic question answering, financial news <br />
<br />
Summary: <br />
Retrieval-Augmented Generation (RAG) is effective at integrating static knowledge into Large Language Models (LLMs) but struggles with handling longitudinal queries. This new framework proposes a redesigned RAG pipeline infused with temporal logic to address this challenge. By disentangling queries into subjects and temporal windows, a specialized retriever is able to gather evidence that is both topically relevant and temporally coherent. The Analytical Diachronic Question Answering Benchmark (ADQAB) is introduced for evaluation, showing substantial accuracy gains compared to standard RAG implementations. The approach in this work enables RAG systems to analyze complex, real-world questions in a nuanced and evolutionary manner. The dataset and code for this study are publicly available for further research and development. <br /> 
Summary: <div>
arXiv:2507.22917v1 Announce Type: new 
Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at https://github.com/kwunhang/TA-RAG.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Convergence: Investigating Shared Representations Across Scaled LLMs</title>
<link>https://arxiv.org/abs/2507.22918</link>
<guid>https://arxiv.org/abs/2507.22918</guid>
<content:encoded><![CDATA[
<div> investigate, feature universality, Gemma-2 language models, Sparse Autoencoder, internal concepts <br />
Summary: <br />
The study explores feature universality in Gemma-2 language models, specifically Gemma-2-2B and Gemma-2-9B, to determine whether models of different scales converge on similar internal concepts. By applying the Sparse Autoencoder (SAE) dictionary-learning pipeline to residual-stream activations of each model, researchers aligned monosemantic features and compared them using SVCCA and RSA. The findings indicate that middle layers exhibit the highest similarity, while early and late layers show less overlap. Preliminary experiments extending the analysis to multi-token subspaces suggest that semantically similar subspaces interact comparably with language models. These results support the notion that despite variations in size, large language models delineate the world into consistent, interpretable features, reinforcing the idea of universality as a basis for cross-model interpretability. <div>
arXiv:2507.22918v1 Announce Type: new 
Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts. Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, and compare the matched feature spaces with SVCCA and RSA. Middle layers yield the strongest overlap, while early and late layers show far less similarity. Preliminary experiments extend the analysis from single tokens to multi-token subspaces, showing that semantically similar subspaces interact similarly with language models. These results strengthen the case that large language models carve the world into broadly similar, interpretable features despite size differences, reinforcing universality as a foundation for cross-model interpretability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations</title>
<link>https://arxiv.org/abs/2507.22919</link>
<guid>https://arxiv.org/abs/2507.22919</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical trials, serious adverse events, prediction models, transfer learning, structured summary results <br />
Summary: 
- The study aimed to develop models for predicting serious adverse event (SAE) results in clinical trials using information from trial registrations.
- Analysis of over 22,000 clinical trials from ClinicalTrials.gov was conducted to develop prediction models.
- Two models were developed: one to predict SAE rates between experimental and control arms, and another to predict the proportion of SAEs in control arms.
- A transfer learning approach using pretrained language models was utilized for feature extraction.
- A sliding window method was developed to extract embeddings from long trial texts and consistently improved model performance.
- The best model achieved a 77.6% AUC for predicting SAE rates and an RMSE of 18.6% for predicting SAE proportions.
- The study emphasizes the underutilization of available data at ClinicalTrials.gov and the potential for improving trial design and identifying discrepancies in safety results. 

<br /><br />Summary: <div>
arXiv:2507.22919v1 Announce Type: new 
Abstract: Objectives: With accurate estimates of expected safety results, clinical trials could be designed to avoid terminations and limit exposing participants to unnecessary risks. We evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. Material and Methods: We analysed 22,107 two-arm parallel interventional clinical trials from ClinicalTrials.gov with structured summary results. Two prediction models were developed: a classifier predicting will experimental arm have higher SAE rates (area under the receiver operating characteristic curve; AUC) than control arm, and a regression model to predict the proportion of SAEs in control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with downstream model for prediction. To maintain semantic representation in long trial texts exceeding localised language model input limits, a sliding window method was developed for embedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a higher proportion of patients with SAEs. When predicting proportion of participants experiencing SAE in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed methods without it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across 12 regressors, the average absolute RMSE reduction was 1.58%. Discussion: Summary results data available at ClinicalTrials.gov remains underutilised. The potential to estimate results of trials before they start is an opportunity to improve trial design and flag discrepancies between expected and reported safety results.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2507.22920</link>
<guid>https://arxiv.org/abs/2507.22920</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, vector quantization, discrete tokenization, multimodal data, LLM architectures

Summary:
This paper provides a comprehensive survey of vector quantization (VQ) techniques for transforming multimodal data into discrete representations suitable for large language models (LLMs). A taxonomy of 8 VQ variants is presented, analyzing their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. The study covers applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting the impact of quantization strategies on performance. Key challenges such as codebook collapse and unstable gradient estimation are identified, along with emerging research directions like dynamic quantization and unified tokenization frameworks. The survey aims to bridge the gap between traditional VQ methods and modern LLM applications and serves as a foundational reference for the development of efficient and generalizable multimodal systems. An ongoing updated version of the survey can be accessed on GitHub. 

<br /><br />Summary: <div>
arXiv:2507.22920v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers</title>
<link>https://arxiv.org/abs/2507.22921</link>
<guid>https://arxiv.org/abs/2507.22921</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Language Model Chain, hallucinations, knowledge extraction, prediction speed

Summary:

The Language Model Chain (LMC) algorithm aims to address the issues of costly and inaccurate information production in language models. This algorithm works by utilizing multiple language models in a multi-stage cascade to improve prediction speed and accuracy while reducing hallucinations. The LMC algorithm was tested on extracting patient dates of birth from medical documents, showcasing significant improvements in accuracy and speed. The algorithm ensures that a language model's response is only considered correct if it exists in a collection of possible answers, preventing the generation of incorrect information. By feeding text corresponding to incorrect responses into more predictive language models, the LMC algorithm iteratively refines predictions until all are accurate. The results of applying the LMC algorithm suggest its potential for advancing the field of knowledge extraction and warrant further exploration in the future.

<br /><br />Summary: <div>
arXiv:2507.22921v1 Announce Type: new 
Abstract: Language models can capture complex relationships in given text, but these are notorious for being costly and for producing information that does not exist (i.e., hallucinations). Furthermore, the resources invested into producing this information would be wasted if it were incorrect. We address these issues by proposing, implementing, and applying the Language Model Chain (LMC) algorithm. In this, a language model's response to a given prompt about given text is only correct if it exists in the collection of possible (i.e., candidate) answers, and text corresponding to incorrect responses is fed into a more predictive (but slower) language model. This process is repeated for a collection of language models, or until all predictions about the text are correct. We used the LMC algorithm to extract patient dates of birth from medical documents, and combining a collection of language models in a multi-stage cascade significantly increased prediction speed and accuracy over individual language models, while greatly reducing the number of corresponding hallucinations. We believe that the novel LMC algorithm significantly contributes to the knowledge extraction field, and that this should be explored much further in the future.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting stock prices with ChatGPT-annotated Reddit sentiment</title>
<link>https://arxiv.org/abs/2507.22922</link>
<guid>https://arxiv.org/abs/2507.22922</guid>
<content:encoded><![CDATA[
<div> Keywords: retail investors, social media sentiment, stock market movements, Reddit, predictive signals <br />
<br />
Summary: 
This paper examines the impact of sentiment from social media, particularly Reddit's r/wallstreetbets, on stock market movements, focusing on GameStop and AMC Entertainment. The study utilizes three sentiment analysis methods and finds that social media sentiment has a weak correlation with stock prices. Surprisingly, simpler metrics like comment volume and Google search trends are more predictive. The findings suggest that traditional sentiment analysis may not capture the complexity of retail investor behavior. Retail investor activity, as seen in the GameStop short squeeze, raises questions about the influence of online sentiment on stock prices. The study introduces a new sentiment analysis model designed to better interpret informal language and emojis in social media discussions. The research underscores the need for a deeper understanding of the nuances of market-moving online conversations. <div>
arXiv:2507.22922v1 Announce Type: new 
Abstract: The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting</title>
<link>https://arxiv.org/abs/2507.22923</link>
<guid>https://arxiv.org/abs/2507.22923</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multilingual retrieval-augmented generation, knowledge bases, prompt translation strategies, classification tasks

Summary:
The paper explores the impact of different prompt translation strategies on classification tasks in multilingual systems using retrieval-augmented generation with Large Language Models (LLMs). Sharing knowledge bases (KB) from high-resource languages to low-resource ones often results in retrieved information in a different language. The study evaluates the effectiveness of pre-translation to create mono-lingual prompts and cross-lingual prompting for direct inference. Results show that an optimized prompt strategy can enhance knowledge sharing across languages, improving classification task performance significantly. The findings emphasize the importance of utilizing multilingual resource sharing and optimizing cross-lingual prompts, particularly for non-English languages, especially low-resource ones.<br /><br />Summary: The study investigates the impact of different prompt translation strategies in multilingual systems using retrieval-augmented generation with Large Language Models. Sharing knowledge bases from high-resource to low-resource languages can lead to retrieved information in different languages. The research shows that optimizing prompt strategies can enhance knowledge sharing across languages, improving classification task performance. The study underscores the importance of leveraging multilingual resource sharing and optimizing cross-lingual prompts for non-English languages, specifically low-resource ones. <div>
arXiv:2507.22923v1 Announce Type: new 
Abstract: Despite advances in the multilingual capabilities of Large Language Models (LLMs), their performance varies substantially across different languages and tasks. In multilingual retrieval-augmented generation (RAG)-based systems, knowledge bases (KB) are often shared from high-resource languages (such as English) to low-resource ones, resulting in retrieved information from the KB being in a different language than the rest of the context. In such scenarios, two common practices are pre-translation to create a mono-lingual prompt and cross-lingual prompting for direct inference. However, the impact of these choices remains unclear. In this paper, we systematically evaluate the impact of different prompt translation strategies for classification tasks with RAG-enhanced LLMs in multilingual systems. Experimental results show that an optimized prompting strategy can significantly improve knowledge sharing across languages, therefore improve the performance on the downstream classification task. The findings advocate for a broader utilization of multilingual resource sharing and cross-lingual prompt optimization for non-English languages, especially the low-resource ones.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers</title>
<link>https://arxiv.org/abs/2507.22924</link>
<guid>https://arxiv.org/abs/2507.22924</guid>
<content:encoded><![CDATA[
<div> Keywords: graduate programs, international students, peer feedback, online courses, language background

Summary:
The study explores the impact of native versus non-native English speaker status on peer feedback experiences in online computing courses in the U.S. It reveals that non-native English speakers tend to write more positively but receive less positive feedback in return, while native English speakers rate feedback less favorably. The sentiment analysis also considers the students' language background and reveals significant interactions when controlling for sex and age. The study highlights that language background plays a modest yet complex role in shaping peer feedback experiences in online computing courses. <div>
arXiv:2507.22924v1 Announce Type: new 
Abstract: Graduate-level CS programs in the U.S. increasingly enroll international students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S. students. Many of these students take online courses, where peer feedback is used to engage students and improve pedagogy in a scalable manner. Since these courses are conducted in English, many students study in a language other than their first. This paper examines how native versus non-native English speaker status affects three metrics of peer feedback experience in online U.S.-based computing courses. Using the Twitter-roBERTa-based model, we analyze the sentiment of peer reviews written by and to a random sample of 500 students. We then relate sentiment scores and peer feedback ratings to students' language background. Results show that native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return. When controlling for sex and age, significant interactions emerge, suggesting that language background plays a modest but complex role in shaping peer feedback experiences.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents</title>
<link>https://arxiv.org/abs/2507.22925</link>
<guid>https://arxiv.org/abs/2507.22925</guid>
<content:encoded><![CDATA[
<div> memory, Large Language Model Agents, reasoning, organization, retrieval

Summary:<br />
The article introduces the concept of Hierarchical Memory (H-MEM) architecture for Large Language Model Agents (LLM Agents) to improve long-term memory integration. H-MEM organizes memory in multi-level fashion based on semantic abstraction, with each memory vector embedding a positional index for efficient retrieval. An index-based routing mechanism allows layer-by-layer retrieval without exhaustive similarity computations. The study evaluates the approach on LoCoMo dataset with five task settings, showing consistent outperformance of five baseline methods. H-MEM enhances decision-making and contextual coherence of LLM Agents, addressing limitations in structured memory organization and retrieval efficiency. The results highlight the effectiveness of the proposed approach in long-term dialogue scenarios. <br /><br />Summary: <div>
arXiv:2507.22925v1 Announce Type: new 
Abstract: Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Relation Extraction in Entity Pairs using Global Context</title>
<link>https://arxiv.org/abs/2507.22926</link>
<guid>https://arxiv.org/abs/2507.22926</guid>
<content:encoded><![CDATA[
<div> Keywords: document-level relation extraction, global context, multi-sentence reasoning, input encoding, entity relationships

Summary: 
The paper introduces a novel input embedding approach for document-level relation extraction that considers the entire document context rather than just the sentences where entities are mentioned. By representing entities as standalone segments throughout the document, the proposed method leverages global relationships and multi-sentence reasoning to accurately predict entity relationships. Experimental results on three benchmark datasets, including DocRED, Re-DocRED, and REBEL, demonstrate the effectiveness of the approach in accurately predicting entity relationships. The research advances global context modeling and multi-sentence reasoning in document-level relation extraction, providing theoretical advancements in the field. Practically, the proposed method enhances relationship detection in NLP applications, offering improved performance and interpretability of entity-level insights. 

<br /><br />Summary: <div>
arXiv:2507.22926v1 Announce Type: new 
Abstract: In document-level relation extraction, entities may appear multiple times in a document, and their relationships can shift from one context to another. Accurate prediction of the relationship between two entities across an entire document requires building a global context spanning all relevant sentences. Previous approaches have focused only on the sentences where entities are mentioned, which fails to capture the complete document context necessary for accurate relation extraction. Therefore, this paper introduces a novel input embedding approach to capture the positions of mentioned entities throughout the document rather than focusing solely on the span where they appear. The proposed input encoding approach leverages global relationships and multi-sentence reasoning by representing entities as standalone segments, independent of their positions within the document. The performance of the proposed method has been tested on three benchmark relation extraction datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results demonstrated that the proposed method accurately predicts relationships between entities in a document-level setting. The proposed research also has theoretical and practical implications. Theoretically, it advances global context modeling and multi-sentence reasoning in document-level relation extraction. Practically, it enhances relationship detection, enabling improved performance in real-world NLP applications requiring comprehensive entity-level insights and interpretability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.22927</link>
<guid>https://arxiv.org/abs/2507.22927</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Benchmark, Document Utilization, Placeholder-Based Approach

Summary: 
The article introduces a fine-grained benchmark called Placeholder-RAG-Benchmark for evaluating the document utilization capabilities of large language models (LLMs) in Retrieval-Augmented Generation (RAG) systems. The benchmark focuses on multi-level filtering abilities, combination abilities, and reference reasoning to assess the LLM's performance in generating responses based on retrieved documents. By using a placeholder-based approach, the study decouples the contributions of the LLM's knowledge and external knowledge, revealing limitations in error resilience and context faithfulness of representative LLMs in RAG systems. The benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Experimental results highlight the need for improvement in LLM-specific capabilities within RAG systems. The code for the benchmark is available on GitHub at https://github.com/Alipay-Med/PRGB. 

<br /><br />Summary: <div>
arXiv:2507.22927v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, where the LLM's ability to generate responses based on the combination of a given query and retrieved documents is crucial. However, most benchmarks focus on overall RAG system performance, rarely assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects such as noise robustness, but lack a systematic and granular evaluation framework on document utilization. To this end, we introduce \textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark, emphasizing the following progressive dimensions: (1) multi-level filtering abilities, (2) combination abilities, and (3) reference reasoning. To provide a more nuanced understanding of LLMs' roles in RAG systems, we formulate an innovative placeholder-based approach to decouple the contributions of the LLM's parametric knowledge and the external knowledge. Experiments demonstrate the limitations of representative LLMs in the RAG system's generation capabilities, particularly in error resilience and context faithfulness. Our benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Our code is available in https://github.com/Alipay-Med/PRGB.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding</title>
<link>https://arxiv.org/abs/2507.22928</link>
<guid>https://arxiv.org/abs/2507.22928</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought prompting, Large Language Models, Causal study, Activation patching, Modular internal computation

Summary:
Chain-of-thought (CoT) prompting has been shown to improve the accuracy of Large Language Models (LLMs) on multi-step tasks. This study investigates the faithfulness of CoT-generated "thoughts" by analyzing monosemantic features extracted from Pythia-70M and Pythia-2.8B models while solving math problems. The results reveal that CoT significantly increases answer log-probabilities in the larger 2.8B model but not in the 70M model, indicating a scale threshold. CoT also leads to higher activation sparsity and feature interpretability scores in the larger model, suggesting more modular internal computation. The confidence of generating correct answers improves with CoT. Patching analysis shows that useful CoT information is distributed widely across features. Overall, CoT induces more interpretable internal structures in high-capacity LLMs, supporting its role as a structured prompting method.

<br /><br />Summary: 
- CoT prompting boosts LLM accuracy on multi-step tasks.
- Study analyzes CoT faithfulness using feature-level causal analysis.
- The 2.8B model shows a significant impact of CoT on answer log-probabilities.
- CoT leads to higher activation sparsity and feature interpretability scores in large models.
- Confidence in generating correct answers improves with CoT. <div>
arXiv:2507.22928v1 Announce Type: new 
Abstract: Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on multi-step tasks, yet whether the generated "thoughts" reflect the true internal reasoning process is unresolved. We present the first feature-level causal study of CoT faithfulness. Combining sparse autoencoders with activation patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B while they tackle GSM8K math problems under CoT and plain (noCoT) prompting. Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M, revealing a clear scale threshold. CoT also leads to significantly higher activation sparsity and feature interpretability scores in the larger model, signalling more modular internal computation. For example, the model's confidence in generating correct answers improves from 1.2 to 4.3. We introduce patch-curves and random-feature patching baselines, showing that useful CoT information is not only present in the top-K patches but widely distributed. Overall, our results indicate that CoT can induce more interpretable internal structures in high-capacity LLMs, validating its role as a structured prompting method.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</title>
<link>https://arxiv.org/abs/2507.22929</link>
<guid>https://arxiv.org/abs/2507.22929</guid>
<content:encoded><![CDATA[
<div> Benchmark, Ophthalmology, Hallucinations, Language Models, Multimodal Data
Summary:
EH-Benchmark is a new ophthalmology benchmark created to evaluate hallucinations in Medical Large Language Models (MLLMs). The challenges faced by MLLMs include limited ophthalmic knowledge, lack of visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data. The benchmark categorizes MLLMs' hallucinations into two primary classes: Visual Understanding and Logical Composition, each with multiple subclasses. A three-phase framework is proposed to mitigate hallucinations, including Knowledge-Level Retrieval, Task-Level Case Studies, and Result-Level Validation. Experimental results demonstrate that the framework significantly improves accuracy, interpretability, and reliability of MLLMs. The project is available on GitHub at https://github.com/ppxy1/EH-Benchmark. <br /><br />Summary: <div>
arXiv:2507.22929v1 Announce Type: new 
Abstract: Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection</title>
<link>https://arxiv.org/abs/2507.22930</link>
<guid>https://arxiv.org/abs/2507.22930</guid>
<content:encoded><![CDATA[
<div> Keywords: Reddit, Personal Information Identifiers, synthetic dataset, privacy risks, text detection

Summary:
A novel methodology has been developed to create a synthetic dataset for detecting Personal Information Identifiers (PIIs) in online social media, focusing on Reddit posts. The dataset includes 19 PII-revealing categories for vulnerable populations and is generated from three text generation Large Language Models (LLMs). The methodology ensures reproducibility equivalence, unlinkability to original users, and indistinguishability from original posts. The dataset and code are released to facilitate research into PII privacy risks on social platforms. This initiative aims to address the lack of open-source labeled datasets for identifying risky self-disclosures of PIIs, thereby enabling the study of privacy risks and online harms associated with online self-disclosures on social media platforms like Reddit.<br /><br />Summary: A synthetic dataset has been created for detecting Personal Information Identifiers (PIIs) in online social media, with a focus on Reddit posts. The dataset includes 19 PII-revealing categories and is generated from three text generation Large Language Models. The methodology ensures reproducibility equivalence, unlinkability to original users, and indistinguishability from original posts. The dataset and code are released to support research on PII privacy risks in social media, addressing the existing lack of labeled datasets for identifying risky self-disclosures of PIIs and enabling the study of privacy risks and online harms associated with online self-disclosures on platforms like Reddit. <div>
arXiv:2507.22930v1 Announce Type: new 
Abstract: Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing RAG Efficiency with Adaptive Context Compression</title>
<link>https://arxiv.org/abs/2507.22931</link>
<guid>https://arxiv.org/abs/2507.22931</guid>
<content:encoded><![CDATA[
<div> Hierarchical compressor, Context selector, Dynamic compression rates, Retrieval-augmented generation, Inference efficiency <br />
Summary: 
The article introduces Adaptive Context Compression for RAG (ACC-RAG), a framework aimed at improving inference efficiency in large language models that integrate external knowledge. ACC-RAG dynamically adjusts compression rates based on the complexity of input queries, optimizing both speed and accuracy. By combining a hierarchical compressor with a context selector, ACC-RAG mimics human skimming behavior to retain essential information while discarding unnecessary details. Evaluation on various datasets demonstrates that ACC-RAG outperforms fixed-rate methods, unlocking over four times faster inference speed without compromising accuracy. This innovative approach to context compression shows promising results in enhancing the performance of retrieval-augmented generation models. <br /> <div>
arXiv:2507.22931v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification</title>
<link>https://arxiv.org/abs/2507.22932</link>
<guid>https://arxiv.org/abs/2507.22932</guid>
<content:encoded><![CDATA[
<div> framework, portfolio optimization, Large Language Models, Deep Reinforcement Learning, sentiment signals  
Summary:  
- This paper introduces a novel hierarchical framework for portfolio optimization that combines lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to integrate sentiment signals from financial news with traditional market indicators.  
- The three-tier architecture utilizes base RL agents for processing hybrid data, meta-agents for aggregating decisions, and a super-agent for merging decisions based on market data and sentiment analysis.  
- Evaluations on data from 2018 to 2024, after training on 2000-2017, show that the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, surpassing equal-weighted and S&amp;P 500 benchmarks.  
- Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.  
- The framework demonstrates the effectiveness of combining advanced natural language processing techniques with reinforcement learning in the field of financial portfolio management. <br /><br /> <div>
arXiv:2507.22932v1 Announce Type: new 
Abstract: This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&amp;P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Augmented Vision-Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2507.22933</link>
<guid>https://arxiv.org/abs/2507.22933</guid>
<content:encoded><![CDATA[
<div> Keywords: visual-language machine learning, neural symbolic systems, reasoning, memory, external symbolic information systems 

Summary: 
This article discusses the limitations of current visual-language machine learning models and proposes the integration of neural networks with external symbolic information systems to enhance reasoning and memory abilities. The traditional training paradigm lacks interpretable explanations for outputs, necessitates retraining for new information, and struggles with certain forms of logical reasoning. By combining powerful pre-trained Vision-Language Models (VLMs) with external symbolic systems, neural-symbolic systems can provide more interpretable explanations and assimilate new information without extensive retraining. The systematic literature review aims to categorize techniques for improving visual-language understanding through interaction with external symbolic information systems.<br /><br />Summary: <div>
arXiv:2507.22933v1 Announce Type: new 
Abstract: Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning Approaches for Multimodal Intent Recognition: A Survey</title>
<link>https://arxiv.org/abs/2507.22934</link>
<guid>https://arxiv.org/abs/2507.22934</guid>
<content:encoded><![CDATA[
<div> Transformer-based models, intent recognition, deep learning, multimodal, natural language processing
Summary:
Deep learning methods, especially Transformer-based models, have advanced intent recognition from text-based to multimodal approaches. These approaches integrate data from audio, vision, and physiological signals to enhance natural human-computer interaction. The survey covers the evolution of intent recognition techniques, from unimodal to multimodal, and discusses relevant datasets, methodologies, applications, and current challenges. Researchers are provided with insights into the latest developments in multimodal intent recognition (MIR) and potential future research directions. <div>
arXiv:2507.22934v1 Announce Type: new 
Abstract: Intent recognition aims to identify users' underlying intentions, traditionally focusing on text in natural language processing. With growing demands for natural human-computer interaction, the field has evolved through deep learning and multimodal approaches, incorporating data from audio, vision, and physiological signals. Recently, the introduction of Transformer-based models has led to notable breakthroughs in this domain. This article surveys deep learning methods for intent recognition, covering the shift from unimodal to multimodal techniques, relevant datasets, methodologies, applications, and current challenges. It provides researchers with insights into the latest developments in multimodal intent recognition (MIR) and directions for future research.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Knowledge Extraction for Operations and Maintenance Intelligence</title>
<link>https://arxiv.org/abs/2507.22935</link>
<guid>https://arxiv.org/abs/2507.22935</guid>
<content:encoded><![CDATA[
<div> Keywords: operational intelligence, Knowledge Graph, Named Entity Recognition, Coreference Resolution, Large Language Models

Summary:
Operational intelligence derived from organizational data repositories faces challenges due to data confidentiality and integration objectives. This work discusses the Knowledge Extraction process, including Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction. Sixteen NLP tools are evaluated alongside Large Language Models for operational and maintenance intelligence in the aircraft industry. The study uses a baseline dataset from the US Federal Aviation Administration to assess zero-shot performance in a confidential environment. Significant limitations are observed, highlighting challenges in trusted NLP and LLM tools for mission-critical industries like aviation. Recommendations are made to enhance trust in these tools, and an open-source dataset is provided for further testing and evaluation. 

<br /><br />Summary: <div>
arXiv:2507.22935v1 Announce Type: new 
Abstract: Deriving operational intelligence from organizational data repositories is a key challenge due to the dichotomy of data confidentiality vs data integration objectives, as well as the limitations of Natural Language Processing (NLP) tools relative to the specific knowledge structure of domains such as operations and maintenance. In this work, we discuss Knowledge Graph construction and break down the Knowledge Extraction process into its Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction functional components. We then evaluate sixteen NLP tools in concert with or in comparison to the rapidly advancing capabilities of Large Language Models (LLMs). We focus on the operational and maintenance intelligence use case for trusted applications in the aircraft industry. A baseline dataset is derived from a rich public domain US Federal Aviation Administration dataset focused on equipment failures or maintenance requirements. We assess the zero-shot performance of NLP and LLM tools that can be operated within a controlled, confidential environment (no data is sent to third parties). Based on our observation of significant performance limitations, we discuss the challenges related to trusted NLP and LLM tools as well as their Technical Readiness Level for wider use in mission-critical industries such as aviation. We conclude with recommendations to enhance trust and provide our open-source curated dataset to support further baseline testing and evaluation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis</title>
<link>https://arxiv.org/abs/2507.22936</link>
<guid>https://arxiv.org/abs/2507.22936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Financial Natural Language Processing, 10-K filings, Comparative evaluation, Model performance

Summary: 
Large Language Models (LLMs) are increasingly being utilized in Financial Natural Language Processing (FinNLP) tasks, with five leading LLMs, GPT, Claude, Perplexity, Gemini, and DeepSeek, being compared in a study using 10-K filings from prominent technology companies. Three evaluation methodologies were employed, including human annotation, automated metrics, and model behavior diagnostics. The results indicated that GPT provided the most coherent and contextually relevant answers, followed by Claude and Perplexity. In contrast, Gemini and DeepSeek exhibited more variability and less agreement in their outputs. The study also highlighted the sensitivity of LLM outputs to prompts and source material, with outputs varying across companies and over time. These findings underscore the importance of careful prompt formulation in utilizing LLMs for financial analysis.<br /><br />Summary: <div>
arXiv:2507.22936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering</title>
<link>https://arxiv.org/abs/2507.22937</link>
<guid>https://arxiv.org/abs/2507.22937</guid>
<content:encoded><![CDATA[
<div> Keywords: AIOps, collaboration-of-expert framework, large language model, retrieval-augmented generation, DevOps-EVAL dataset

Summary: <br /><br />
The paper introduces a collaboration-of-expert framework (CoE-Ops) for AIOps in DevOps, integrating a large language model task classifier. By incorporating a retrieval-augmented generation mechanism, the framework can handle both high-level and low-level tasks in AIOps effectively. Experimental results on the DevOps-EVAL dataset show that CoE-Ops significantly improves routing accuracy for high-level tasks, outperforming existing CoE methods. Additionally, the framework enhances accuracy in problem resolution by up to 8% compared to single AIOps models. Moreover, CoE-Ops surpasses Mixture-of-Experts (MoE) models in accuracy, indicating its superiority in AIOps domain applications. <div>
arXiv:2507.22937v1 Announce Type: new 
Abstract: With the rapid evolution of artificial intelligence, AIOps has emerged as a prominent paradigm in DevOps. Lots of work has been proposed to improve the performance of different AIOps phases. However, constrained by domain-specific knowledge, a single model can only handle the operation requirement of a specific task,such as log parser,root cause analysis. Meanwhile, combining multiple models can achieve more efficient results, which have been proved in both previous ensemble learning and the recent LLM training domain. Inspired by these works,to address the similar challenges in AIOPS, this paper first proposes a collaboration-of-expert framework(CoE-Ops) incorporating a general-purpose large language model task classifier. A retrieval-augmented generation mechanism is introduced to improve the framework's capability in handling both Question-Answering tasks with high-level(Code,build,Test,etc.) and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed method is implemented in the AIOps domain, and extensive experiments are conducted on the DevOps-EVAL dataset. Experimental results demonstrate that CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents</title>
<link>https://arxiv.org/abs/2507.22938</link>
<guid>https://arxiv.org/abs/2507.22938</guid>
<content:encoded><![CDATA[
<div> Keywords: Question-Answering, Technical Documents, Visual large Language Models, Graph Representations, Telecom Domain<br />
<br />
Summary: 
This study introduces an innovative approach to Question-Answering (QA) from technical documents, focusing on flowchart images in the telecom domain. By incorporating graph representations of flowcharts from Visual large Language Models (VLMs) into a text-based Retrieval Augmented Generation (RAG) system, the authors demonstrate improved QA performance. The process involves document processing, image classification, graph building, and integration with text embedding for efficient retrieval. Results reveal that fine-tuned VLM models provide robust graph representations with low edit distances compared to ground truth. The approach not only enhances retrieval performance using text-based embedding models but also eliminates the need for VLM in inference, reducing costs for deployed QA systems. Overall, the study showcases the efficacy of leveraging VLM-generated graph representations for QA tasks involving flowchart images in technical documents within the telecom domain.<br /> <div>
arXiv:2507.22938v1 Announce Type: new 
Abstract: Question-Answering (QA) from technical documents often involves questions whose answers are present in figures, such as flowcharts or flow diagrams. Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such questions. We leverage graph representations of flowcharts obtained from Visual large Language Models (VLMs) and incorporate them in a text-based RAG system to show that this approach can enable image retrieval for QA in the telecom domain. We present the end-to-end approach from processing technical documents, classifying image types, building graph representations, and incorporating them with the text embedding pipeline for efficient retrieval. We benchmark the same on a QA dataset created based on proprietary telecom product information documents. Results show that the graph representations obtained using a fine-tuned VLM model have lower edit distance with respect to the ground truth, which illustrate the robustness of these representations for flowchart images. Further, the approach for QA using these representations gives good retrieval performance using text-based embedding models, including a telecom-domain adapted one. Our approach also alleviates the need for a VLM in inference, which is an important cost benefit for deployed QA systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARROT: An Open Multilingual Radiology Reports Dataset</title>
<link>https://arxiv.org/abs/2507.22939</link>
<guid>https://arxiv.org/abs/2507.22939</guid>
<content:encoded><![CDATA[
<div> dataset, radiology, reports, natural language processing, multilingual

Summary:<br />
- The article introduces PARROT, a large dataset of fictional radiology reports in multiple languages for testing natural language processing applications in radiology.
- The dataset includes over 2,600 reports from 76 authors across 21 countries and 13 languages, covering various imaging modalities and anatomical regions.
- A study was conducted to differentiate between human-authored and AI-generated reports, with participants achieving 53.9% accuracy.
- Radiologists performed significantly better in distinguishing between human and AI-generated reports compared to other groups.
- PARROT enables the development and validation of natural language processing applications in radiology across linguistic, geographic, and clinical boundaries without privacy constraints.

<br /><br />Summary: <div>
arXiv:2507.22939v1 Announce Type: new 
Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal Annotated Radiology Reports for Open Testing), a large, multicentric, open-access dataset of fictional radiology reports spanning multiple languages for testing natural language processing applications in radiology. Materials and Methods: From May to September 2024, radiologists were invited to contribute fictional radiology reports following their standard reporting practices. Contributors provided at least 20 reports with associated metadata including anatomical region, imaging modality, clinical context, and for non-English reports, English translations. All reports were assigned ICD-10 codes. A human vs. AI report differentiation study was conducted with 154 participants (radiologists, healthcare professionals, and non-healthcare professionals) assessing whether reports were human-authored or AI-generated. Results: The dataset comprises 2,658 radiology reports from 76 authors across 21 countries and 13 languages. Reports cover multiple imaging modalities (CT: 36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%) being most prevalent. In the differentiation study, participants achieved 53.9% accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated reports, with radiologists performing significantly better (56.9%, 95% CI: 53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the largest open multilingual radiology report dataset, enabling development and validation of natural language processing applications across linguistic, geographic, and clinical boundaries without privacy constraints.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes</title>
<link>https://arxiv.org/abs/2507.22940</link>
<guid>https://arxiv.org/abs/2507.22940</guid>
<content:encoded><![CDATA[
<div> fact-checking classifier, Group Relative Policy Optimization, mechanistic interpretability, factual accuracy, large language models 

Summary:
The RELIANCE framework addresses the issue of factual inaccuracies in Large Language Models (LLMs) during reasoning processes. It consists of a fact-checking classifier trained on counterfactually augmented data, a Group Relative Policy Optimization (GRPO) reinforcement learning approach, and a mechanistic interpretability module to enhance factual accuracy. Evaluation of ten state-of-the-art models reveals low reasoning factual accuracy, with leading models like Claude-3.7 and GPT-o1 only reaching around 80%. RELIANCE significantly improves factual robustness by up to 49.90% while maintaining or enhancing performance on challenging benchmarks such as Math-500, AIME-2024, and GPQA. The activation-level analysis provides insights into how factual enhancements impact reasoning trajectories within model architectures, laying the groundwork for future training methodologies targeting factual robustness through activation-guided optimization. 

<br /><br />Summary: <div>
arXiv:2507.22940v1 Announce Type: new 
Abstract: We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy for Confidence Enhancement), a novel framework addressing a critical vulnerability in Large Language Models (LLMs): the prevalence of factual inaccuracies within intermediate reasoning steps despite correct final answers. This phenomenon poses substantial risks in high-stakes domains including healthcare, legal analysis, and scientific research, where erroneous yet confidently presented reasoning can mislead users into dangerous decisions. Our framework integrates three core components: (1) a specialized fact-checking classifier trained on counterfactually augmented data to detect subtle factual inconsistencies within reasoning chains; (2) a Group Relative Policy Optimization (GRPO) reinforcement learning approach that balances factuality, coherence, and structural correctness through multi-dimensional rewards; and (3) a mechanistic interpretability module examining how factuality improvements manifest in model activations during reasoning processes. Extensive evaluation across ten state-of-the-art models reveals concerning patterns: even leading models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual robustness (up to 49.90% improvement) while maintaining or improving performance on challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore, our activation-level analysis provides actionable insights into how factual enhancements reshape reasoning trajectories within model architectures, establishing foundations for future training methodologies that explicitly target factual robustness through activation-guided optimization.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology</title>
<link>https://arxiv.org/abs/2507.22941</link>
<guid>https://arxiv.org/abs/2507.22941</guid>
<content:encoded><![CDATA[
<div> Keywords: Electronic medical reports, machine learning, survival analysis, SigBERT, temporal dynamics <br />
Summary:<br />
- SigBERT is proposed as a framework for temporal survival analysis in healthcare, specifically designed to handle sequential textual data from electronic medical reports (EHR).
- SigBERT processes timestamped medical reports by extracting word embeddings and deriving sentence embeddings through averaging in order to capture complex temporal dynamics.
- The framework utilizes signature extraction from rough path theory to generate geometric features from the time series of sentence embedding coordinates, enhancing the performance of the survival model.
- These features are then utilized in a LASSO-penalized Cox model to estimate patient-specific risk scores, resulting in a C-index score of 0.75 on an independent test cohort from a real-world oncology dataset.
- SigBERT integrates sequential medical data effectively to improve risk estimation in narrative-based survival analysis. <br /><br /> <div>
arXiv:2507.22941v1 Announce Type: new 
Abstract: Electronic medical reports (EHR) contain a vast amount of information that can be leveraged for machine learning applications in healthcare. However, existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form. Here, we propose SigBERT, an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient. SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores. The model was trained and evaluated on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies</title>
<link>https://arxiv.org/abs/2507.22943</link>
<guid>https://arxiv.org/abs/2507.22943</guid>
<content:encoded><![CDATA[
<div> Natural language processing, validation study, claims databases, code-based algorithms, outcome misclassification <br />
Summary: <br />
The article focuses on enhancing analyses conducted with large claims databases by validating code-based algorithms for identifying health outcomes. It introduces an expedited validation process using natural language processing (NLP) to reduce manual chart review time and a multi-wave adaptive sampling approach to stop the study once performance characteristics are identified with sufficient precision. The case study validates a claims-based outcome algorithm for intentional self-harm in obese patients. The NLP-assisted annotation process reduced review time by 40%, and the pre-defined stopping rule would have prevented review of 77% of patient charts with limited compromise to measurement precision. This approach could enable more routine validation of code-based algorithms, improving the reliability of findings from database studies. <br /> <div>
arXiv:2507.22943v1 Announce Type: new 
Abstract: Background: One of the ways to enhance analyses conducted with large claims databases is by validating the measurement characteristics of code-based algorithms used to identify health outcomes or other key study parameters of interest. These metrics can be used in quantitative bias analyses to assess the robustness of results for an inferential study given potential bias from outcome misclassification. However, extensive time and resource allocation are typically re-quired to create reference-standard labels through manual chart review of free-text notes from linked electronic health records. Methods: We describe an expedited process that introduces efficiency in a validation study us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to reduce time spent by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with pre-defined criteria to stop the validation study once performance characteristics are identified with sufficient precision. We illustrate this process in a case study that validates the performance of a claims-based outcome algorithm for intentional self-harm in patients with obesity. Results: We empirically demonstrate that the NLP-assisted annotation process reduced the time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave samples would have prevented review of 77% of patient charts with limited compromise to precision in derived measurement characteristics. Conclusion: This approach could facilitate more routine validation of code-based algorithms used to define key study parameters, ultimately enhancing understanding of the reliability of find-ings derived from database studies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opacity as Authority: Arbitrariness and the Preclusion of Contestation</title>
<link>https://arxiv.org/abs/2507.22944</link>
<guid>https://arxiv.org/abs/2507.22944</guid>
<content:encoded><![CDATA[
<div> Keywords: arbitrariness, semiotic trait, motivation, Contestability, artificial intelligence systems

Summary: 
Arbitrariness is redefined in this article as a foundational functional mechanism present in human systems and interactions. It is seen as a semiotic trait that enables systems to operate effectively while withholding their internal rationale, extending beyond language to law and social dynamics. The "Motivation -> Constatability -> Contestability" chain is introduced, with motivation being key in rendering an act's logic contestable. Mechanisms like "immotivization" and "Conflict Lateralization" can break this chain, leading to acts producing binding effects without revealing their rationale, thus evading accountability. Formalized as conditional entropy, arbitrariness is proposed as a neutral operator crucial in interpersonal relations and control mechanisms. This framework also provides insight into analyzing explainability in advanced artificial intelligence systems. 

<br /><br />Summary: <div>
arXiv:2507.22944v1 Announce Type: new 
Abstract: This article redefines arbitrariness not as a normative flaw or a symptom of domination, but as a foundational functional mechanism structuring human systems and interactions. Diverging from critical traditions that conflate arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a property enabling systems - linguistic, legal, or social - to operate effectively while withholding their internal rationale. Building on Ferdinand de Saussure's concept of l'arbitraire du signe, the analysis extends this principle beyond language to demonstrate its cross-domain applicability, particularly in law and social dynamics. The paper introduces the "Motivation -> Constatability -> Contestability" chain, arguing that motivation functions as a crucial interface rendering an act's logic vulnerable to intersubjective contestation. When this chain is broken through mechanisms like "immotivization" or "Conflict Lateralization" (exemplified by "the blur of the wolf drowned in the fish"), acts produce binding effects without exposing their rationale, thus precluding justiciability. This structural opacity, while appearing illogical, is a deliberate design protecting authority from accountability. Drawing on Shannon's entropy model, the paper formalizes arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern theory of arbitrariness as a neutral operator central to control as well as care, an overlooked dimension of interpersonal relations. While primarily developed through human social systems, this framework also illuminates a new pathway for analyzing explainability in advanced artificial intelligence systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations</title>
<link>https://arxiv.org/abs/2507.22968</link>
<guid>https://arxiv.org/abs/2507.22968</guid>
<content:encoded><![CDATA[
<div> Keywords: Spoken Dialogue Models, Large Language Models, benchmark dataset, human conversational dynamics, evaluation method

Summary:
Spoken Dialogue Models (SDMs) have gained popularity for generating voice responses to user queries but lack comprehensive research on their effectiveness compared to text-based Large Language Models (LLMs). Human voice interactions present unique challenges such as ambiguity from semantic and phonological factors, as well as context-dependency like omission and coreference. To bridge this gap, a benchmark dataset with 1,079 instances in English and Chinese is introduced, enabling a thorough evaluation of SDMs. An evaluation method aligned with human judgment is provided to assess SDMs in handling these complexities. This dataset aims to shed light on the current state of SDM development and their capabilities in understanding and simulating human conversations.  

<br /><br />Summary: <div>
arXiv:2507.22968v1 Announce Type: new 
Abstract: Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Math Natural Language Inference: this should be easy!</title>
<link>https://arxiv.org/abs/2507.23063</link>
<guid>https://arxiv.org/abs/2507.23063</guid>
<content:encoded><![CDATA[
<div> struggle, mathematical language, inference, LLMs, Math NLI  
Summary:  
- The study explores whether contemporary Language Model models (LLMs) can effectively perform natural language inference (NLI) tasks on mathematical texts, termed the Math NLI problem.  
- A corpus of Math NLI pairs is constructed, with premises from mathematical text and hypotheses from experts in both mathematics research and NLI.  
- The quality of corpora using LLM-generated hypotheses is also examined.  
- Positive findings include the use of a majority vote of LLMs being comparable to human-labeled data in certain Math NLI scenarios.  
- However, LLMs struggle with mathematical language and occasionally fail at basic inferences.  
- Current models are less susceptible to hypothesis-only "inference" compared to previous generations.  
- The corpora are provided to support future research on Math NLI.  <br /><br />Summary: <div>
arXiv:2507.23063v1 Announce Type: new 
Abstract: We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only "inference" in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring In-Context Learning for Frame-Semantic Parsing</title>
<link>https://arxiv.org/abs/2507.23082</link>
<guid>https://arxiv.org/abs/2507.23082</guid>
<content:encoded><![CDATA[
<div> Keywords: Frame Semantic Parsing, In-Context Learning, Large Language Models, Frame Identification, Frame Semantic Role Labeling 

Summary: 
Frame Semantic Parsing (FSP) involves identifying predicates and their arguments based on Frame Semantics. This study explores the use of In-Context Learning (ICL) with Large Language Models (LLMs) for FSP tasks without fine-tuning the model. The method generates task-specific prompts for Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) using the FrameNet database. These prompts guide six different LLMs in experiments focused on frames related to violent events, achieving competitive F1 scores of 94.3% for FI and 77.4% for FSRL. The results demonstrate that ICL can be a practical and effective approach for domain-specific FSP tasks, providing an alternative to traditional fine-tuning methods. 

<br /><br />Summary: <div>
arXiv:2507.23082v1 Announce Type: new 
Abstract: Frame Semantic Parsing (FSP) entails identifying predicates and labeling their arguments according to Frame Semantics. This paper investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning. We propose a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database. These prompts, constructed from frame definitions and annotated examples, are used to guide six different LLMs. Experiments are conducted on a subset of frames related to violent events. The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context-aware Rotary Position Embedding</title>
<link>https://arxiv.org/abs/2507.23083</link>
<guid>https://arxiv.org/abs/2507.23083</guid>
<content:encoded><![CDATA[
<div> Rotary Positional Embeddings, RoPE, context-sensitive relationships, token embeddings, CARoPE<br />
<br />
Summary:<br />
Positional encoding is crucial in Transformer architectures for incorporating sequence order into self-attention mechanisms. While RoPE has been widely adopted for its efficiency, it lacks the ability to model context-sensitive relationships due to its static frequency patterns. In this work, CARoPE is introduced as a novel generalization of RoPE that dynamically generates head-specific frequency patterns based on token embeddings. By computing input-dependent phase shifts using a transformation of token embeddings, CARoPE offers token- and context-sensitive positional representations while maintaining efficiency and simplicity. Evaluation on the FineWeb-Edu-10B dataset with GPT-2 variants shows that CARoPE outperforms RoPE and other positional encoding baselines, achieving lower perplexity even at longer context lengths. Moreover, CARoPE enables faster training throughput without compromising model stability, making it a scalable, expressive, and efficient enhancement to positional encoding strategies in Transformer models. <div>
arXiv:2507.23083v1 Announce Type: new 
Abstract: Positional encoding is a vital component of Transformer architectures, enabling models to incorporate sequence order into self-attention mechanisms. Rotary Positional Embeddings (RoPE) have become a widely adopted solution due to their compatibility with relative position encoding and computational efficiency. However, RoPE relies on static, input-independent sinusoidal frequency patterns, limiting its ability to model context-sensitive relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional Embedding), a novel generalization of RoPE that dynamically generates head-specific frequency patterns conditioned on token embeddings. This design introduces token- and context-sensitive positional representations while preserving RoPE efficiency and architectural simplicity. CARoPE computes input-dependent phase shifts using a bounded transformation of token embeddings and integrates them into the rotary mechanism across attention heads. We evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on next-token prediction tasks. Experimental results show that CARoPE consistently outperforms RoPE and other common positional encoding baselines, achieving significantly lower perplexity, even at longer context lengths. Additionally, CARoPE enables faster training throughput without sacrificing model stability. These findings demonstrate that CARoPE offers a scalable, expressive, and efficient upgrade to existing positional encoding strategies in Transformer models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity</title>
<link>https://arxiv.org/abs/2507.23095</link>
<guid>https://arxiv.org/abs/2507.23095</guid>
<content:encoded><![CDATA[
<div> SMART-Editor, compositional layout, content editing, Reward-Refine, RewardDPO

Summary:
SMART-Editor is a framework for compositional layout and content editing that maintains global coherence through two strategies: Reward-Refine and RewardDPO. This framework outperforms strong baselines like InstructPix2Pix and HIVE in structured and unstructured settings. RewardDPO shows up to 15% gains in structured scenarios, while Reward-Refine displays advantages in natural images. The introduction of SMARTEdit-Bench, a benchmark for multi-domain cascading edit scenarios, allows for evaluation of model performance. Both automatic and human evaluations confirm the effectiveness of reward-guided planning in producing visually aligned and semantically consistent edits. SMART-Editor's approach to editing across different domains proves to be a successful method for achieving high-quality edits in various applications. <br /><br />Summary: <div>
arXiv:2507.23095v1 Announce Type: new 
Abstract: We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL</title>
<link>https://arxiv.org/abs/2507.23104</link>
<guid>https://arxiv.org/abs/2507.23104</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, natural language interface, database, metadata, retrieval architecture

Summary: 
The article presents a novel approach to scaling natural language interfaces for databases to enterprise-level data catalogs. The proposed component-based retrieval architecture decomposes database schemas and metadata into discrete semantic units, allowing for more targeted retrieval. This approach prioritizes table identification while leveraging column-level information to ensure manageable context budgets. Experimental results show high recall and accuracy, with the system outperforming baselines over massive databases with varying structures and metadata availability. The solution eliminates the need for domain-specific fine-tuning, making it practical for deployment in diverse enterprise settings. By effectively leveraging database metadata, the system addresses a critical scalability gap in natural language database interfaces.

Summary: <div>
arXiv:2507.23104v1 Announce Type: new 
Abstract: Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity</title>
<link>https://arxiv.org/abs/2507.23121</link>
<guid>https://arxiv.org/abs/2507.23121</guid>
<content:encoded><![CDATA[
<div> Keywords: trustworthiness, large language models, Chinese textual ambiguity, dataset, language understanding

Summary: <br /><br />Large language models (LLMs) face challenges when encountering ambiguous narrative text, especially in the context of Chinese textual ambiguity. A benchmark dataset was created to study LLM behavior in handling ambiguity, revealing significant differences from human interpretation. LLMs struggle to distinguish between ambiguous and unambiguous text, often displaying overconfidence in interpreting ambiguous text as having a single meaning. Additionally, LLMs tend to overthink when trying to understand multiple possible meanings within ambiguous text. These findings underscore a critical limitation in current LLMs and emphasize the need for improved approaches to address uncertainty in language understanding. The publicly available dataset and code provide valuable resources for further research in this area. <br /><br />Summary: <div>
arXiv:2507.23121v1 Announce Type: new 
Abstract: In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans</title>
<link>https://arxiv.org/abs/2507.23135</link>
<guid>https://arxiv.org/abs/2507.23135</guid>
<content:encoded><![CDATA[
<div> benchmark, causal relationships, multimodal models, vision-language, performance 

Summary: 
ISO-Bench is a new benchmark for evaluating causal relationships between visual observations and procedural text in multimodal models. The benchmark presents image-text pairs of task steps to determine temporal dependencies. Current vision-language models perform below human level, with the best zero-shot F1 score at 0.57 and chain-of-thought reasoning improving results up to 0.62 F1. Humans outperform models with a 0.98 F1 score. The findings suggest the need for enhancements in causal understanding within multimodal models. <div>
arXiv:2507.23135v1 Announce Type: new 
Abstract: Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments. We introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text. Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step. Evaluation results on ten frontier vision-language models show underwhelming performance: the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further highlights concrete directions for improving causal understanding in multimodal models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal</title>
<link>https://arxiv.org/abs/2507.23158</link>
<guid>https://arxiv.org/abs/2507.23158</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, user feedback, user-LM interaction, implicit feedback, model performance<br />
Summary: <br /><br />Language models (LMs) can benefit from continuous evolution based on user feedback harvested from user-LM interaction logs. The study analyzes implicit user feedback in two datasets and finds that the content of user feedback, not just its polarity, can enhance model performance in short human-designed questions. However, this improvement is not seen in longer and more complex questions. The study also highlights that the quality of the initial prompt from the user is essential for the usefulness of feedback. These findings provide valuable insights into the potential and limitations of implicit user feedback for enhancing the performance of language models. <div>
arXiv:2507.23158v1 Announce Type: new 
Abstract: Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user's initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration</title>
<link>https://arxiv.org/abs/2507.23167</link>
<guid>https://arxiv.org/abs/2507.23167</guid>
<content:encoded><![CDATA[
<div> confidence, Large Language Models, ensemble learning, neural networks, internal representations

Summary:
LENS introduces a novel method for combining the predictions of multiple Large Language Models (LLMs) by learning to estimate model confidence through analyzing internal representations. By training a linear confidence predictor for each LLM, LENS effectively weights model predictions based on context-dependent reliability without modifying the model parameters. Experimental results on question-answering tasks show that LENS outperforms traditional ensemble methods significantly. This approach leverages neural network hidden states and normalized probabilities to provide nuanced weighting of predictions, leading to improved system robustness and performance. The findings indicate that internal representations offer valuable signals for determining model confidence and can be leveraged for enhancing ensemble learning in LLMs. <div>
arXiv:2507.23167v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geak: Introducing Triton Kernel AI Agent &amp; Evaluation Benchmarks</title>
<link>https://arxiv.org/abs/2507.23194</link>
<guid>https://arxiv.org/abs/2507.23194</guid>
<content:encoded><![CDATA[
<div> DSL, GPU programming, AI-generated kernels, Triton language, GEAK framework <br />
<br />
In response to the increasing demand for AI-generated GPU kernels, a new evaluation suite for Triton-based GPU kernels and the GEAK framework have been developed. The Triton language, known for its balance of performance and ease of coding, is a popular target for AI-generated kernels. The GEAK framework utilizes cutting-edge LLMs to generate efficient Triton code specifically for AMD GPUs, such as the AMD MI300X and MI250. By leveraging inference-time compute scaling and a Reflexion-style feedback mechanism, GEAK significantly outperformed baseline approaches in terms of correctness and execution speed. The results demonstrate the potential of GEAK-like agentic code generation in accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance. <br /><br />Summary: <div>
arXiv:2507.23194v1 Announce Type: new 
Abstract: The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples</title>
<link>https://arxiv.org/abs/2507.23211</link>
<guid>https://arxiv.org/abs/2507.23211</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Few-shot in-context learning, Positive samples, Negative samples, Semantic similarity

Summary:
In this study, the focus is on improving few-shot in-context learning (ICL) by leveraging both Positive and Negative samples for better example selection. The research builds Positive and Negative sample corpora based on Zero-Shot-Cot and uses semantic similarity for selecting examples during inference. By incorporating information from Negative samples, the method enhances the selection of Positive examples, thereby improving ICL performance. Results indicate that utilizing Negative samples in example selection leads to better performance compared to methods solely relying on Positive samples. This innovative approach demonstrates the importance of considering both Positive and Negative samples in enhancing the efficiency and effectiveness of few-shot in-context learning. 

<br /><br />Summary: <div>
arXiv:2507.23211v1 Announce Type: new 
Abstract: Large Language Models exhibit powerful few-shot in-context learning (ICL) capabilities, but the performance is highly sensitive to provided examples.
  Recent research has focused on retrieving corresponding examples for each input query, not only enhancing the efficiency and scalability of the learning process but also mitigating inherent biases in manual example selection.
  However, these studies have primarily emphasized leveraging Positive samples while overlooking the additional information within Negative samples for contextual learning.
  We propose a novel method that utilizes Negative samples to better select Positive sample examples, thereby enhancing the performance of few-shot ICL. Initially, we construct Positive and Negative sample corpora based on Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based approach to select the most similar examples from both the Positive and Negative corpora for a given query. Subsequently, we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples, then concatenating them with the previously selected Positive examples to serve as ICL demonstrations. Experimental results demonstrate that our approach surpasses methods solely relying on the most similar positive examples for context, validating that the additional information in negative samples aids in enhancing ICL performance through improved Positive sample selection.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2507.23220</link>
<guid>https://arxiv.org/abs/2507.23220</guid>
<content:encoded><![CDATA[
<div> Keywords: topic models, sparse autoencoders, semantic features, controllable text generation, evaluation framework

Summary:
Mechanistic Topic Models (MTMs) are introduced as a new class of topic models that utilize interpretable features learned by sparse autoencoders (SAEs). Unlike traditional and neural topic models that rely on word lists, MTMs operate on semantically rich spaces to reveal deeper conceptual themes with expressive feature descriptions. They also enable controllable text generation through topic-based steering vectors, a unique feature among topic models. Evaluation using the topic judge framework shows that MTMs match or exceed traditional and neural baselines on coherence metrics, are preferred by judges, and allow effective steering of language model outputs. Overall, MTMs offer a promising approach to capturing complex topics in large text collections with improved interpretability and performance. 

<br /><br />Summary: <div>
arXiv:2507.23220v1 Announce Type: new 
Abstract: Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs</title>
<link>https://arxiv.org/abs/2507.23227</link>
<guid>https://arxiv.org/abs/2507.23227</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's disease, biomarkers, large language models, tabular data, prediction

Summary:
TAP-GPT is a novel framework that utilizes large language models (LLMs) for the early and accurate diagnosis of Alzheimer's disease (AD) by analyzing heterogeneous biomarkers in a tabular format. The framework, based on TableGPT2 and adapted for AD diagnosis, incorporates few-shot reasoning, multimodal integration, and natural-language-based interpretability. By constructing tabular prompts with in-context learning examples and using parameter-efficient adaptation techniques, TAP-GPT outperforms other LLMs and a tabular foundation model for AD prediction tasks. This approach represents a groundbreaking application of LLMs in utilizing tabular biomarker data for AD diagnosis, opening new avenues for LLM-driven frameworks in biomedical informatics. <br /><br />Summary: TAP-GPT harnesses the power of large language models for Alzheimer's disease diagnosis through tabular data analysis, offering superior prediction capabilities compared to existing models. <div>
arXiv:2507.23227v1 Announce Type: new 
Abstract: Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication</title>
<link>https://arxiv.org/abs/2507.23247</link>
<guid>https://arxiv.org/abs/2507.23247</guid>
<content:encoded><![CDATA[
<div> mental health, chatbots, explainability, pragmatic reasoning, stigma

Summary: 
- The study focuses on the development of personalized chatbots for mental health and explores the pragmatic reasoning capability of large language models (LLMs) in this domain.
- The researchers introduce the P-ReMe dataset and propose tasks related to implicature and presupposition in mental health discourse.
- Four models, including Llama3.1, Mistral, MentaLLaMa, and Qwen, are evaluated on the dataset, with Mistral and Qwen showing substantial reasoning capabilities.
- The study also introduces StiPRompts to analyze stigma around mental health using LLMs like GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku.
- Findings indicate that Claude-3.5-haiku handles stigma more responsibly compared to the other LLMs. 

<br /><br />Summary: <div>
arXiv:2507.23247v1 Announce Type: new 
Abstract: There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis</title>
<link>https://arxiv.org/abs/2507.23248</link>
<guid>https://arxiv.org/abs/2507.23248</guid>
<content:encoded><![CDATA[
<div> Language Models, Bengali, NLP, Evaluation Benchmarks, Tokenization
Summary:<br /><br />In this work, the challenges faced in Bengali NLP performance are explored, with a focus on the absence of standardized evaluation benchmarks. 10 recent Large Language Models were evaluated on translated datasets, highlighting performance gaps for Bengali compared to English, especially in smaller models and specific families. Certain architectures, like DeepSeek, show promising robustness across languages. An inverse relationship between tokenization efficiency and model accuracy was observed, with more concise tokenization leading to improved performance. The study emphasizes the need for improved dataset quality and evaluation methods tailored for multilingual contexts to advance NLP research for underrepresented languages and make advanced language technologies more accessible worldwide. <div>
arXiv:2507.23248v1 Announce Type: new 
Abstract: Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at https://github.com/BengaliAI/bn-llm-benchmark.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Super Experts in Mixture-of-Experts Large Language Models</title>
<link>https://arxiv.org/abs/2507.23279</link>
<guid>https://arxiv.org/abs/2507.23279</guid>
<content:encoded><![CDATA[
<div> Experts, Mixture-of-Experts, Sparsity, Compression, Language models 
Summary: 
SEs, termed Super Experts, have been identified as a crucial subset in Sparsely activated Mixture-of-Experts (MoE) models. These SEs exhibit extreme activation outliers, impacting the model's performance significantly, especially in mathematical reasoning tasks. Pruning SEs leads to repetitive and uninformative outputs, emphasizing their importance in ensuring model efficiency. SEs induce attention sinks vital for attention score distribution but are disrupted by pruning, affecting model functionality. The distribution and influence of SEs are model-specific, remaining consistent post-training. Analysis shows SEs play a fundamental role in the model's forward inference mechanism. The research provides a deeper understanding of SEs' heterogeneous importance and their contribution to enhancing MoE LLMs' learning capacity. The available code enables further exploration and validation of the findings. 
<br /><br />Summary: <div>
arXiv:2507.23279v1 Announce Type: new 
Abstract: Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content</title>
<link>https://arxiv.org/abs/2507.23319</link>
<guid>https://arxiv.org/abs/2507.23319</guid>
<content:encoded><![CDATA[
arXiv:2507.23319v1 Announce Type: new 
Abstract: Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[
arXiv:2507.23334v1 Announce Type: new 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-to-SQL Task-oriented Dialogue Ontology Construction</title>
<link>https://arxiv.org/abs/2507.23358</link>
<guid>https://arxiv.org/abs/2507.23358</guid>
<content:encoded><![CDATA[
arXiv:2507.23358v1 Announce Type: new 
Abstract: Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.23382</link>
<guid>https://arxiv.org/abs/2507.23382</guid>
<content:encoded><![CDATA[
arXiv:2507.23382v1 Announce Type: new 
Abstract: Multimodal planning capabilities refer to the ability to predict, reason, and design steps for task execution with multimodal context, which is essential for complex reasoning and decision-making across multiple steps. However, current benchmarks face two key challenges: (1) they cannot directly assess multimodal real-world planning capabilities, and (2) they lack constraints or implicit constraints across modalities. To address these issues, we introduce Multimodal Planning with Complex Constraints (MPCC), the first benchmark to systematically evaluate MLLMs' ability to handle multimodal constraints in planning. To address the first challenge, MPCC focuses on three real-world tasks: Flight Planning, Calendar Planning, and Meeting Planning. To solve the second challenge, we introduce complex constraints (e.g. budget, temporal, and spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion. Experiments on 13 advanced MLLMs reveal significant challenges: closed-source models achieve only 21.3% feasible plans, while open-source models average below 11%. Additionally, we observe that MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios. Our work formalizes multimodal constraints in planning, provides a rigorous evaluation framework, and highlights the need for advancements in constraint-aware reasoning for real-world MLLM applications.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models</title>
<link>https://arxiv.org/abs/2507.23386</link>
<guid>https://arxiv.org/abs/2507.23386</guid>
<content:encoded><![CDATA[
arXiv:2507.23386v1 Announce Type: new 
Abstract: Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators</title>
<link>https://arxiv.org/abs/2507.23399</link>
<guid>https://arxiv.org/abs/2507.23399</guid>
<content:encoded><![CDATA[
arXiv:2507.23399v1 Announce Type: new 
Abstract: The rapid proliferation of Large Language Models presents both opportunities and challenges for the translation field. While commercial, cloud-based AI chatbots have garnered significant attention in translation studies, concerns regarding data privacy, security, and equitable access necessitate exploration of alternative deployment models. This paper investigates the feasibility and performance of locally deployable, free language models as a viable alternative to proprietary, cloud-based AI solutions. This study evaluates three open-source models installed on CPU-based platforms and compared against commercially available online chat-bots. The evaluation focuses on functional performance rather than a comparative analysis of human-machine translation quality, an area already subject to extensive research. The platforms assessed were chosen for their accessibility and ease of use across various operating systems. While local deployment introduces its own challenges, the benefits of enhanced data control, improved privacy, and reduced dependency on cloud services are compelling. The findings of this study contribute to a growing body of knowledge concerning the democratization of AI technology and inform future research and development efforts aimed at making LLMs more accessible and practical for a wider range of users, specifically focusing on the needs of individual translators and small businesses.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization</title>
<link>https://arxiv.org/abs/2507.23400</link>
<guid>https://arxiv.org/abs/2507.23400</guid>
<content:encoded><![CDATA[
arXiv:2507.23400v1 Announce Type: new 
Abstract: The core challenge faced by multi-document summarization is the complexity of relationships among documents and the presence of information redundancy. Graph clustering is an effective paradigm for addressing this issue, as it models the complex relationships among documents using graph structures and reduces information redundancy through clustering, achieving significant research progress. However, existing methods often only consider single-relational graphs and require a predefined number of clusters, which hinders their ability to fully represent rich relational information and adaptively partition sentence groups to reduce redundancy. To overcome these limitations, we propose MRGSEM-Sum, an unsupervised multi-document summarization framework based on multi-relational graphs and structural entropy minimization. Specifically, we construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections among sentences across documents. We then apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters and effectively organizing sentences into coherent groups. Finally, we introduce a position-aware compression mechanism to distill each cluster, generating concise and informative summaries. Extensive experiments on four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently outperforms previous unsupervised methods and, in several cases, achieves performance comparable to supervised models and large language models. Human evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high consistency and coverage, approaching human-level quality.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Arabic Text Retrieval with Attentive Relevance Scoring</title>
<link>https://arxiv.org/abs/2507.23404</link>
<guid>https://arxiv.org/abs/2507.23404</guid>
<content:encoded><![CDATA[
arXiv:2507.23404v1 Announce Type: new 
Abstract: Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \href{https://github.com/Bekhouche/APR}{GitHub}.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2507.23407</link>
<guid>https://arxiv.org/abs/2507.23407</guid>
<content:encoded><![CDATA[
arXiv:2507.23407v1 Announce Type: new 
Abstract: Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Aware Language Models for Secure and Contextualized Access Control in Organizations</title>
<link>https://arxiv.org/abs/2507.23465</link>
<guid>https://arxiv.org/abs/2507.23465</guid>
<content:encoded><![CDATA[
arXiv:2507.23465v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains</title>
<link>https://arxiv.org/abs/2507.23486</link>
<guid>https://arxiv.org/abs/2507.23486</guid>
<content:encoded><![CDATA[
arXiv:2507.23486v1 Announce Type: new 
Abstract: Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&amp;A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.23541</link>
<guid>https://arxiv.org/abs/2507.23541</guid>
<content:encoded><![CDATA[
arXiv:2507.23541v1 Announce Type: new 
Abstract: In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text</title>
<link>https://arxiv.org/abs/2507.23577</link>
<guid>https://arxiv.org/abs/2507.23577</guid>
<content:encoded><![CDATA[
arXiv:2507.23577v1 Announce Type: new 
Abstract: The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at https://github.com/ResearAI/t-detect.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffLoRA: Differential Low-Rank Adapters for Large Language Models</title>
<link>https://arxiv.org/abs/2507.23588</link>
<guid>https://arxiv.org/abs/2507.23588</guid>
<content:encoded><![CDATA[
arXiv:2507.23588v1 Announce Type: new 
Abstract: Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning</title>
<link>https://arxiv.org/abs/2507.23661</link>
<guid>https://arxiv.org/abs/2507.23661</guid>
<content:encoded><![CDATA[
arXiv:2507.23661v1 Announce Type: new 
Abstract: Hate speech identification in social media has become an increasingly important issue in recent years. In this research, we address two problems: 1) to detect hate speech in Arabic text, 2) to clean a given text from hate speech. The meaning of cleaning here is replacing each bad word with stars based on the number of letters for each word. Regarding the first problem, we conduct several experiments using deep learning models and transformers to determine the best model in terms of the F1 score. Regarding second problem, we consider it as a machine translation task, where the input is a sentence containing dirty text and the output is the same sentence with masking the dirty text. The presented methods achieve the best model in hate speech detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text cleaning experiment, the best result in the hate speech masking model reached 0.3 in BLEU score with 1-gram, which is a good result compared with the state of the art machine translation systems.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</title>
<link>https://arxiv.org/abs/2507.23740</link>
<guid>https://arxiv.org/abs/2507.23740</guid>
<content:encoded><![CDATA[
arXiv:2507.23740v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities</title>
<link>https://arxiv.org/abs/2507.23776</link>
<guid>https://arxiv.org/abs/2507.23776</guid>
<content:encoded><![CDATA[
arXiv:2507.23776v1 Announce Type: new 
Abstract: While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \emph{cascaded question disclosure} that provides a more accurate estimate of the models' problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation</title>
<link>https://arxiv.org/abs/2507.22892</link>
<guid>https://arxiv.org/abs/2507.22892</guid>
<content:encoded><![CDATA[
arXiv:2507.22892v1 Announce Type: cross 
Abstract: Conventional augmentative and alternative communication (AAC) systems and language-learning platforms often fail to adapt in real time to the user's cognitive and linguistic needs, especially in neurological conditions such as post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in noninvasive electroencephalography (EEG)--based brain-computer interfaces (BCIs) and transformer--based large language models (LLMs) offer complementary strengths: BCIs capture users' neural intent with low fatigue, while LLMs generate contextually tailored language content. We propose and evaluate a novel hybrid framework that leverages real-time EEG signals to drive an LLM-powered language rehabilitation assistant. This system aims to: (1) enable users with severe speech or motor impairments to navigate language-learning modules via mental commands; (2) dynamically personalize vocabulary, sentence-construction exercises, and corrective feedback; and (3) monitor neural markers of cognitive effort to adjust task difficulty on the fly.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment</title>
<link>https://arxiv.org/abs/2507.22898</link>
<guid>https://arxiv.org/abs/2507.22898</guid>
<content:encoded><![CDATA[
arXiv:2507.22898v1 Announce Type: cross 
Abstract: We developed a voice-driven artificial intelligence (AI) system that guides anyone - from paramedics to family members - through expert-level stroke evaluations using natural conversation, while also enabling smartphone video capture of key examination components for documentation and potential expert review. This addresses a critical gap in emergency care: current stroke recognition by first responders is inconsistent and often inaccurate, with sensitivity for stroke detection as low as 58%, causing life-threatening delays in treatment. Three non-medical volunteers used our AI system to assess ten simulated stroke patients, including cases with likely large vessel occlusion (LVO) strokes and stroke-like conditions, while we measured diagnostic accuracy, completion times, user confidence, and expert physician review of the AI-generated reports. The AI system correctly identified 84% of individual stroke signs and detected 75% of likely LVOs, completing evaluations in just over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use (mean 4.67/5). The system successfully identified 86% of actual strokes but also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert physician reviewed the AI reports with videos, they identified the correct diagnosis in 100% of cases, but felt confident enough to make preliminary treatment decisions in only 40% of cases due to observed AI errors including incorrect scoring and false information. While the current system's limitations necessitate human oversight, ongoing rapid advancements in speech-to-speech AI models suggest that future versions are poised to enable highly accurate assessments. Achieving human-level voice interaction could transform emergency medical care, putting expert-informed assessment capabilities in everyone's hands.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting</title>
<link>https://arxiv.org/abs/2507.22902</link>
<guid>https://arxiv.org/abs/2507.22902</guid>
<content:encoded><![CDATA[
arXiv:2507.22902v1 Announce Type: cross 
Abstract: Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELMES: An Automated Framework for Evaluating Large Language Models in Educational Scenarios</title>
<link>https://arxiv.org/abs/2507.22947</link>
<guid>https://arxiv.org/abs/2507.22947</guid>
<content:encoded><![CDATA[
arXiv:2507.22947v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) presents transformative opportunities for education, generating numerous novel application scenarios. However, significant challenges remain: evaluation metrics vary substantially across different educational scenarios, while many emerging scenarios lack appropriate assessment metrics. Current benchmarks predominantly measure general intelligence rather than pedagogical capabilities. To address this gap, we introduce ELMES, an open-source automated evaluation framework specifically designed for assessing LLMs in educational settings. ELMES features a modular architecture that enables researchers to create dynamic, multi-agent dialogues through simple configuration files, facilitating flexible scenario design without requiring extensive programming expertise. The framework incorporates a hybrid evaluation engine that objectively quantifies traditionally subjective pedagogical metrics using an LLM-as-a-Judge methodology. We conduct systematic benchmarking of state-of-the-art LLMs across four critical educational scenarios: Knowledge Point Explanation, Guided Problem-Solving Teaching, Interdisciplinary Lesson Plan Generation, and Contextualized Question Generation, employing fine-grained metrics developed in collaboration with education specialists. Our results demonstrate distinct capability distributions among models, revealing context-specific strengths and limitations. ELMES provides educators and researchers with an accessible evaluation framework that significantly reduces adaptation barriers for diverse educational applications while advancing the practical implementation of LLMs in pedagogy. The framework is publicly available at \emph{https://github.com/sii-research/elmes.git}.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Dynamic Parameters for Vietnamese Gender-Independent ASR</title>
<link>https://arxiv.org/abs/2507.22964</link>
<guid>https://arxiv.org/abs/2507.22964</guid>
<content:encoded><![CDATA[
arXiv:2507.22964v1 Announce Type: cross 
Abstract: The dynamic characteristics of speech signal provides temporal information and play an important role in enhancing Automatic Speech Recognition (ASR). In this work, we characterized the acoustic transitions in a ratio plane of Spectral Subband Centroid Frequencies (SSCFs) using polar parameters to capture the dynamic characteristics of the speech and minimize spectral variation. These dynamic parameters were combined with Mel-Frequency Cepstral Coefficients (MFCCs) in Vietnamese ASR to capture more detailed spectral information. The SSCF0 was used as a pseudo-feature for the fundamental frequency (F0) to describe the tonal information robustly. The findings showed that the proposed parameters significantly reduce word error rates and exhibit greater gender independence than the baseline MFCCs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalized Reinforcement Learning for Retriever-Specific Query Rewriter with Unstructured Real-World Documents</title>
<link>https://arxiv.org/abs/2507.23242</link>
<guid>https://arxiv.org/abs/2507.23242</guid>
<content:encoded><![CDATA[
arXiv:2507.23242v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) systems rely heavily on effective query formulation to unlock external knowledge, yet optimizing queries for diverse, unstructured real-world documents remains a challenge. We introduce \textbf{RL-QR}, a reinforcement learning framework for retriever-specific query rewriting that eliminates the need for human-annotated datasets and extends applicability to both text-only and multi-modal databases. By synthesizing scenario-question pairs and leveraging Generalized Reward Policy Optimization (GRPO), RL-QR trains query rewriters tailored to specific retrievers, enhancing retrieval performance across varied domains. Experiments on industrial in-house data demonstrate significant improvements, with $\text{RL-QR}_{\text{multi-modal}}$ achieving an 11\% relative gain in NDCG@3 for multi-modal RAG and $\text{RL-QR}_{\text{lexical}}$ yielding a 9\% gain for lexical retrievers. However, challenges persist with semantic and hybrid retrievers, where rewriters failed to improve performance, likely due to training misalignments. Our findings highlight RL-QR's potential to revolutionize query optimization for RAG systems, offering a scalable, annotation-free solution for real-world retrieval tasks, while identifying avenues for further refinement in semantic retrieval contexts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy</title>
<link>https://arxiv.org/abs/2507.23292</link>
<guid>https://arxiv.org/abs/2507.23292</guid>
<content:encoded><![CDATA[
arXiv:2507.23292v1 Announce Type: cross 
Abstract: We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DSBC : Data Science task Benchmarking with Context engineering</title>
<link>https://arxiv.org/abs/2507.23336</link>
<guid>https://arxiv.org/abs/2507.23336</guid>
<content:encoded><![CDATA[
arXiv:2507.23336v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly impacted data science workflows, giving rise to specialized data science agents designed to automate analytical tasks. Despite rapid adoption, systematic benchmarks evaluating the efficacy and limitations of these agents remain scarce. In this paper, we introduce a comprehensive benchmark specifically crafted to reflect real-world user interactions with data science agents by observing usage of our commercial applications. We evaluate three LLMs: Claude-4.0-Sonnet, Gemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with context engineering, multi-step with context engineering, and with SmolAgent. Our benchmark assesses performance across a diverse set of eight data science task categories, additionally exploring the sensitivity of models to common prompting issues, such as data leakage and slightly ambiguous instructions. We further investigate the influence of temperature parameters on overall and task-specific outcomes for each model and approach. Our findings reveal distinct performance disparities among the evaluated models and methodologies, highlighting critical factors that affect practical deployment. The benchmark dataset and evaluation framework introduced herein aim to provide a foundation for future research of more robust and effective data science agents.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23348</link>
<guid>https://arxiv.org/abs/2507.23348</guid>
<content:encoded><![CDATA[
arXiv:2507.23348v1 Announce Type: cross 
Abstract: Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWE-Exp: Experience-Driven Software Issue Resolution</title>
<link>https://arxiv.org/abs/2507.23361</link>
<guid>https://arxiv.org/abs/2507.23361</guid>
<content:encoded><![CDATA[
arXiv:2507.23361v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) agents have shown remarkable progress in software issue resolution, leveraging advanced techniques such as multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current agents act as memoryless explorers - treating each problem separately without retaining or reusing knowledge from previous repair experiences. This leads to redundant exploration of failed trajectories and missed chances to adapt successful issue resolution methods to similar problems. To address this problem, we introduce SWE-Exp, an experience - enhanced approach that distills concise and actionable experience from prior agent trajectories, enabling continuous learning across issues. Our method introduces a multi-faceted experience bank that captures both successful and failed repair attempts. Specifically, it extracts reusable issue resolution knowledge at different levels - from high-level problem comprehension to specific code changes. Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6% Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach establishes a new paradigm in which automated software engineering agents systematically accumulate and leverage repair expertise, fundamentally shifting from trial-and-error exploration to strategic, experience-driven issue resolution.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holistic Evaluations of Topic Models</title>
<link>https://arxiv.org/abs/2507.23364</link>
<guid>https://arxiv.org/abs/2507.23364</guid>
<content:encoded><![CDATA[
arXiv:2507.23364v1 Announce Type: cross 
Abstract: Topic models are gaining increasing commercial and academic interest for their ability to summarize large volumes of unstructured text. As unsupervised machine learning methods, they enable researchers to explore data and help general users understand key themes in large text collections. However, they risk becoming a 'black box', where users input data and accept the output as an accurate summary without scrutiny. This article evaluates topic models from a database perspective, drawing insights from 1140 BERTopic model runs. The goal is to identify trade-offs in optimizing model parameters and to reflect on what these findings mean for the interpretation and responsible use of topic models
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterfactual Evaluation for Blind Attack Detection in LLM-based Evaluation Systems</title>
<link>https://arxiv.org/abs/2507.23453</link>
<guid>https://arxiv.org/abs/2507.23453</guid>
<content:encoded><![CDATA[
arXiv:2507.23453v1 Announce Type: cross 
Abstract: This paper investigates defenses for LLM-based evaluation systems against prompt injection. We formalize a class of threats called blind attacks, where a candidate answer is crafted independently of the true answer to deceive the evaluator. To counter such attacks, we propose a framework that augments Standard Evaluation (SE) with Counterfactual Evaluation (CFE), which re-evaluates the submission against a deliberately false ground-truth answer. An attack is detected if the system validates an answer under both standard and counterfactual conditions. Experiments show that while standard evaluation is highly vulnerable, our SE+CFE framework significantly improves security by boosting attack detection with minimal performance trade-offs.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks</title>
<link>https://arxiv.org/abs/2507.23511</link>
<guid>https://arxiv.org/abs/2507.23511</guid>
<content:encoded><![CDATA[
arXiv:2507.23511v1 Announce Type: cross 
Abstract: While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at https://github.com/xiaomi-research/mecat
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Learning-based Prediction of Clinical Trial Enrollment with Uncertainty Estimates</title>
<link>https://arxiv.org/abs/2507.23607</link>
<guid>https://arxiv.org/abs/2507.23607</guid>
<content:encoded><![CDATA[
arXiv:2507.23607v1 Announce Type: cross 
Abstract: Clinical trials are a systematic endeavor to assess the safety and efficacy of new drugs or treatments. Conducting such trials typically demands significant financial investment and meticulous planning, highlighting the need for accurate predictions of trial outcomes. Accurately predicting patient enrollment, a key factor in trial success, is one of the primary challenges during the planning phase. In this work, we propose a novel deep learning-based method to address this critical challenge. Our method, implemented as a neural network model, leverages pre-trained language models (PLMs) to capture the complexities and nuances of clinical documents, transforming them into expressive representations. These representations are then combined with encoded tabular features via an attention mechanism. To account for uncertainties in enrollment prediction, we enhance the model with a probabilistic layer based on the Gamma distribution, which enables range estimation. We apply the proposed model to predict clinical trial duration, assuming site-level enrollment follows a Poisson-Gamma process. We carry out extensive experiments on real-world clinical trial data, and show that the proposed method can effectively predict the number of patients enrolled at a number of sites for a given clinical trial, outperforming established baseline models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached Responses</title>
<link>https://arxiv.org/abs/2507.23674</link>
<guid>https://arxiv.org/abs/2507.23674</guid>
<content:encoded><![CDATA[
arXiv:2507.23674v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TextQuests: How Good are LLMs at Text-Based Video Games?</title>
<link>https://arxiv.org/abs/2507.23701</link>
<guid>https://arxiv.org/abs/2507.23701</guid>
<content:encoded><![CDATA[
arXiv:2507.23701v1 Announce Type: cross 
Abstract: Evaluating AI agents within complex, interactive environments that mirror real-world challenges is critical for understanding their practical capabilities. While existing agent benchmarks effectively assess skills like tool use or performance on structured tasks, they often do not fully capture an agent's ability to operate autonomously in exploratory environments that demand sustained, self-directed reasoning over a long and growing context. To spur the development of agents capable of more robust intrinsic reasoning over long horizons, we introduce TextQuests, a benchmark based on the Infocom suite of interactive fiction games. These text-based adventures, which can take human players over 30 hours and require hundreds of precise actions to solve, serve as an effective proxy for evaluating AI agents on focused, stateful tasks. The benchmark is specifically designed to assess an LLM agent's capacity for self-contained problem-solving by precluding the use of external tools, thereby focusing on intrinsic long-context reasoning capabilities in an exploratory environment characterized by the need for trial-and-error learning and sustained problem-solving within a single interactive session. We release TextQuests at https://textquests.ai.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving</title>
<link>https://arxiv.org/abs/2507.23726</link>
<guid>https://arxiv.org/abs/2507.23726</guid>
<content:encoded><![CDATA[
arXiv:2507.23726v1 Announce Type: cross 
Abstract: LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks</title>
<link>https://arxiv.org/abs/2507.23751</link>
<guid>https://arxiv.org/abs/2507.23751</guid>
<content:encoded><![CDATA[
arXiv:2507.23751v1 Announce Type: cross 
Abstract: We propose CoT-Self-Instruct, a synthetic data generation method that instructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the given seed tasks, and then to generate a new synthetic prompt of similar quality and complexity for use in LLM training, followed by filtering for high-quality data with automatic metrics. In verifiable reasoning, our synthetic data significantly outperforms existing training datasets, such as s1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For non-verifiable instruction-following tasks, our method surpasses the performance of human or standard self-instruct prompts on both AlpacaEval 2.0 and Arena-Hard.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model</title>
<link>https://arxiv.org/abs/2507.23773</link>
<guid>https://arxiv.org/abs/2507.23773</guid>
<content:encoded><![CDATA[
arXiv:2507.23773v1 Announce Type: cross 
Abstract: AI agents built on large language models (LLMs) hold enormous promise, but current practice focuses on a one-task-one-agent approach, which not only falls short of scalability and generality, but also suffers from the fundamental limitations of autoregressive LLMs. On the other hand, humans are general agents who reason by mentally simulating the outcomes of their actions and plans. Moving towards a more general and powerful AI agent, we introduce SimuRA, a goal-oriented architecture for generalized agentic reasoning. Based on a principled formulation of optimal agent in any environment, \modelname overcomes the limitations of autoregressive reasoning by introducing a world model for planning via simulation. The generalized world model is implemented using LLM, which can flexibly plan in a wide range of environments using the concept-rich latent space of natural language. Experiments on difficult web browsing tasks show that \modelname improves the success of flight search from 0\% to 32.2\%. World-model-based planning, in particular, shows consistent advantage of up to 124\% over autoregressive planning, demonstrating the advantage of world model simulation as a reasoning paradigm. We are excited about the possibility for training a single, general agent model based on LLMs that can act superintelligently in all environments. To start, we make SimuRA, a web-browsing agent built on \modelname with pretrained LLMs, available as a research demo for public testing.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiMe: a Latin Corpus of Late Medieval Criminal Sentences</title>
<link>https://arxiv.org/abs/2404.12829</link>
<guid>https://arxiv.org/abs/2404.12829</guid>
<content:encoded><![CDATA[
arXiv:2404.12829v2 Announce Type: replace 
Abstract: The Latin language has received attention from the computational linguistics research community, which has built, over the years, several valuable resources, ranging from detailed annotated corpora to sophisticated tools for linguistic analysis. With the recent advent of large language models, researchers have also started developing models capable of generating vector representations of Latin texts. The performances of such models remain behind the ones for modern languages, given the disparity in available data. In this paper, we present the LiMe dataset, a corpus of 325 documents extracted from a series of medieval manuscripts called Libri sententiarum potestatis Mediolani, and thoroughly annotated by experts, in order to be employed for masked language model, as well as supervised natural language processing tasks.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining vague language</title>
<link>https://arxiv.org/abs/2404.18154</link>
<guid>https://arxiv.org/abs/2404.18154</guid>
<content:encoded><![CDATA[
arXiv:2404.18154v2 Announce Type: replace 
Abstract: Why is language vague? Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language. In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium. More recently, \'Egr\'e, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words. This paper proposes to compare both results and to explain why they are not in contradiction. Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \'Egr\'e et al.'s involves a layer of semantic content. We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability</title>
<link>https://arxiv.org/abs/2406.14313</link>
<guid>https://arxiv.org/abs/2406.14313</guid>
<content:encoded><![CDATA[
arXiv:2406.14313v3 Announce Type: replace 
Abstract: Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQA's iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cutting Through the Noise: Boosting LLM Performance on Math Word Problems</title>
<link>https://arxiv.org/abs/2406.15444</link>
<guid>https://arxiv.org/abs/2406.15444</guid>
<content:encoded><![CDATA[
arXiv:2406.15444v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, PROBLEMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and improved ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to 6%.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation</title>
<link>https://arxiv.org/abs/2411.18337</link>
<guid>https://arxiv.org/abs/2411.18337</guid>
<content:encoded><![CDATA[
arXiv:2411.18337v4 Announce Type: replace 
Abstract: Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette</title>
<link>https://arxiv.org/abs/2412.11167</link>
<guid>https://arxiv.org/abs/2412.11167</guid>
<content:encoded><![CDATA[
arXiv:2412.11167v3 Announce Type: replace 
Abstract: Large language models (LLMs) face challenges in aligning with diverse cultural values despite their remarkable performance in generation, which stems from inherent monocultural biases and difficulties in capturing nuanced cultural semantics. Existing methods struggle to adapt to unknown culture after fine-tuning. Inspired by cultural geography across five continents, we propose Cultural Palette, a multi-agent framework that redefines cultural alignment as an adaptive "color-blending" process for country-specific adaptation. Our approach harnesses cultural geography across five continents (Africa, America, Asia, Europe, Oceania) through three key steps: First, we synthesize the Pentachromatic Cultural Palette Dataset using GPT-4o, refining continental-level dialogues with Hofstede's cultural dimensions to establish foundational cultural representations. Second, five continent-level alignment agents form specialized cultural communities that generate region-specific draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically blend these cultural "colors" through attention-gated parameter merging, akin to mixing pigments on a palette, resolving conflicts while preserving cultural nuances to produce the final culturally-aligned response. Extensive experiments across various countries demonstrate that Cultural Palette surpasses existing baselines in cultural alignment.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inside-Out: Hidden Factual Knowledge in LLMs</title>
<link>https://arxiv.org/abs/2503.15299</link>
<guid>https://arxiv.org/abs/2503.15299</guid>
<content:encoded><![CDATA[
arXiv:2503.15299v3 Announce Type: replace 
Abstract: This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) put a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer</title>
<link>https://arxiv.org/abs/2503.15768</link>
<guid>https://arxiv.org/abs/2503.15768</guid>
<content:encoded><![CDATA[
arXiv:2503.15768v2 Announce Type: replace 
Abstract: Abstractive multi-document summarization (MDS) is the task of automatically summarizing information in multiple documents, from news articles to conversations with multiple speakers. The training approaches for current MDS models can be grouped into four approaches: end-to-end with special pre-training ("direct"), chunk-then-summarize, extract-then-summarize, and inference with GPT-style models. In this work, we evaluate MDS models across training approaches, domains, and dimensions (reference similarity, quality, and factuality), to analyze how and why models trained on one domain can fail to summarize documents from another (News, Science, and Conversation) in the zero-shot domain transfer setting. We define domain-transfer "failure" as a decrease in factuality, higher deviation from the target, and a general decrease in summary quality. In addition to exploring domain transfer for MDS models, we examine potential issues with applying popular summarization metrics out-of-the-box.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation</title>
<link>https://arxiv.org/abs/2504.04640</link>
<guid>https://arxiv.org/abs/2504.04640</guid>
<content:encoded><![CDATA[
arXiv:2504.04640v2 Announce Type: replace 
Abstract: Variation in language use, shaped by speakers' sociocultural background and specific context of use, offers a rich lens into cultural perspectives, values, and opinions. However, the computational study of these Sociocultural Linguistic Phenomena (SLP) has often been limited to bespoke analyses of specific groups or topics, hindering the pace of scientific discovery. To address this, we introduce Splits!, a 9.7 million-post dataset from Reddit designed for systematic and flexible research. The dataset contains posts from over 53,000 users across 6 demographic groups, organized into 89 discussion topics to enable comparative analysis. We validate Splits! via self-identification and by successfully replicating several known SLPs from existing literature. We complement this dataset with a framework that leverages efficient retrieval methods to rapidly validate potential SLPs (PSLPs) by automatically evaluating whether a given hypothesis is supported by our data. Crucially, to distinguish between novel and obvious insights, the framework incorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We demonstrate that the two-stage process reduces the number of statistically significant findings requiring manual inspection by a factor of 1.5-1.8x, streamlining the discovery of promising phenomena for further investigation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance</title>
<link>https://arxiv.org/abs/2504.09753</link>
<guid>https://arxiv.org/abs/2504.09753</guid>
<content:encoded><![CDATA[
arXiv:2504.09753v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their development has primarily focused on English and other high-resource languages, leaving many languages underserved. We present our latest Hindi-English bi-lingual LLM \textbf{Mantra-14B} with ~3\% average improvement in benchmark scores over both languages, outperforming models twice its size. Using a curated dataset composed of English and Hindi instruction data of 485K samples, we instruction tuned models such as Qwen-2.5-14B-Instruct and Phi-4 to improve performance over both English and Hindi. Our experiments encompassing seven different LLMs of varying parameter sizes and over 140 training attempts with varying English-Hindi training data ratios demonstrated that it is possible to significantly improve multilingual performance without compromising native performance. Further, our approach avoids resource-intensive techniques like vocabulary expansion or architectural modifications, thus keeping the model size small. Our results indicate that modest fine-tuning with culturally and locally informed data can bridge performance gaps without incurring significant computational overhead. We release our training code, datasets, and models under mit and apache licenses to aid further research towards under-represented and low-resource languages.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Fine-Grained Detection of AI Generated Texts</title>
<link>https://arxiv.org/abs/2504.11952</link>
<guid>https://arxiv.org/abs/2504.11952</guid>
<content:encoded><![CDATA[
arXiv:2504.11952v3 Announce Type: replace 
Abstract: An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation</title>
<link>https://arxiv.org/abs/2504.16060</link>
<guid>https://arxiv.org/abs/2504.16060</guid>
<content:encoded><![CDATA[
arXiv:2504.16060v3 Announce Type: replace 
Abstract: Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs to Create Content Corpora for Niche Domains</title>
<link>https://arxiv.org/abs/2505.02851</link>
<guid>https://arxiv.org/abs/2505.02851</guid>
<content:encoded><![CDATA[
arXiv:2505.02851v2 Announce Type: replace 
Abstract: Constructing specialized content corpora from vast, unstructured web sources for domain-specific applications poses substantial data curation challenges. In this paper, we introduce a streamlined approach for generating high-quality, domain-specific corpora by efficiently acquiring, filtering, structuring, and cleaning web-based data. We showcase how Large Language Models (LLMs) can be leveraged to address complex data curation at scale, and propose a strategical framework incorporating LLM-enhanced techniques for structured content extraction and semantic deduplication. We validate our approach in the behavior education domain through its integration into 30 Day Me, a habit formation application. Our data pipeline, named 30DayGen, enabled the extraction and synthesis of 3,531 unique 30-day challenges from over 15K webpages. A user survey reports a satisfaction score of 4.3 out of 5, with 91% of respondents indicating willingness to use the curated content for their habit-formation goals.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2505.14874</link>
<guid>https://arxiv.org/abs/2505.14874</guid>
<content:encoded><![CDATA[
arXiv:2505.14874v4 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging due to data scarcity, particularly in non-English languages. To address this, we fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode both speaker characteristics and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into non-English dysarthric-like speech. The generated data is then used to fine-tune a multilingual ASR model, Massively Multilingual Speech (MMS), for improved dysarthric speech recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE (Tamil) demonstrates that VC with both speaker and prosody conversion significantly outperforms the off-the-shelf MMS performance and conventional augmentation techniques such as speed and tempo perturbation. Objective and subjective analyses of the generated data further confirm that the generated speech simulates dysarthric characteristics.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models</title>
<link>https://arxiv.org/abs/2505.18497</link>
<guid>https://arxiv.org/abs/2505.18497</guid>
<content:encoded><![CDATA[
arXiv:2505.18497v2 Announce Type: replace 
Abstract: Current large language models (LLMs) have demonstrated emerging capabilities in social intelligence tasks, including implicature resolution and theory-of-mind reasoning, both of which require substantial pragmatic understanding. However, how LLMs acquire this pragmatic competence throughout the training process remains poorly understood. In this work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to evaluate whether LLMs at different training stages can accurately infer nuanced speaker intentions. Each instance pairs two equally plausible yet pragmatically divergent continuations and requires the model to (i) infer the speaker's intended meaning and (ii) explain when and why a speaker would choose one utterance over its alternative, thus directly probing pragmatic competence through contrastive reasoning. We systematically evaluate 22 LLMs across 3 key training stages: after pre-training, supervised fine-tuning (SFT), and preference optimization, to examine the development of pragmatic competence. Our results show that even base models exhibit notable sensitivity to pragmatic cues, which improves consistently with increases in model and data scale. Additionally, SFT and RLHF contribute further gains, particularly in cognitive-pragmatic scenarios. These findings highlight pragmatic competence as an emergent and compositional property of LLM training and offer new insights for aligning models with human communicative norms.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora</title>
<link>https://arxiv.org/abs/2505.23628</link>
<guid>https://arxiv.org/abs/2505.23628</guid>
<content:encoded><![CDATA[
arXiv:2505.23628v2 Announce Type: replace 
Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 95\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Framing Political Bias in Multilingual LLMs Across Pakistani Languages</title>
<link>https://arxiv.org/abs/2506.00068</link>
<guid>https://arxiv.org/abs/2506.00068</guid>
<content:encoded><![CDATA[
arXiv:2506.00068v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) increasingly shape public discourse, yet most evaluations of political and economic bias have focused on high-resource, Western languages and contexts. This leaves critical blind spots in low-resource, multilingual regions such as Pakistan, where linguistic identity is closely tied to political, religious, and regional ideologies. We present a systematic evaluation of political bias in 13 state-of-the-art LLMs across five Pakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework integrates a culturally adapted Political Compass Test (PCT) with multi-level framing analysis, capturing both ideological stance (economic/social axes) and stylistic framing (content, tone, emphasis). Prompts are aligned with 11 socio-political themes specific to the Pakistani context. Results show that while LLMs predominantly reflect liberal-left orientations consistent with Western training data, they exhibit more authoritarian framing in regional languages, highlighting language-conditioned ideological modulation. We also identify consistent model-specific bias patterns across languages. These findings show the need for culturally grounded, multilingual bias auditing frameworks in global NLP.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2506.07106</link>
<guid>https://arxiv.org/abs/2506.07106</guid>
<content:encoded><![CDATA[
arXiv:2506.07106v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unable to Forget: Proactive Interference Reveals Working Memory Limits in LLMs Beyond Context Length</title>
<link>https://arxiv.org/abs/2506.08184</link>
<guid>https://arxiv.org/abs/2506.08184</guid>
<content:encoded><![CDATA[
arXiv:2506.08184v3 Announce Type: replace 
Abstract: Information retrieval in Large Language Models (LLMs) is increasingly recognized as intertwined with generation capabilities rather than mere lookup. While longer contexts are often assumed to improve retrieval, the effects of intra-context interference remain understudied. To address this, we adapt the proactive interference (PI) paradigm from cognitive science, where earlier information disrupts recall of newer updates. In humans, susceptibility to such interference is inversely linked to working memory capacity. We introduce PI-LLM, an evaluation that sequentially streams semantically related key-value updates and queries only the final values. Although these final values are clearly positioned just before the query, LLM retrieval accuracy declines log-linearly toward zero as interference accumulates; errors arise from retrieving previously overwritten values. Attempts to mitigate interference via prompt engineering (e.g., instructing models to ignore earlier input) yield limited success. These findings reveal a fundamental constraint on LLMs' ability to disentangle interference and flexibly manipulate information, suggesting a working memory bottleneck beyond mere context access. This calls for approaches that strengthen models' ability to suppress irrelevant content during retrieval.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics</title>
<link>https://arxiv.org/abs/2506.12365</link>
<guid>https://arxiv.org/abs/2506.12365</guid>
<content:encoded><![CDATA[
arXiv:2506.12365v2 Announce Type: replace 
Abstract: This survey paper outlines the key developments in the field of Large Language Models (LLMs), including enhancements to their reasoning skills, adaptability to various tasks, increased computational efficiency, and the ability to make ethical decisions. The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback. The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. A significant focus is placed on efficiency, detailing scaling strategies, optimization techniques, and the influential Mixture-of-Experts (MoE) architecture, which strategically routes inputs to specialized subnetworks to boost predictive accuracy, while optimizing resource allocation. This survey also offers a broader perspective on recent advancements in LLMs, going beyond isolated aspects such as model architecture or ethical concerns. Additionally, it explores the role of LLMs in Agentic AI and their use as Autonomous Decision-Making Systems, and categorizes emerging methods that enhance LLM reasoning, efficiency, and ethical alignment. The survey also identifies underexplored areas such as interpretability, cross-modal integration, and sustainability. While significant advancements have been made in LLMs, challenges such as high computational costs, biases, and ethical risks remain. Overcoming these requires a focus on bias mitigation, transparent decision-making, and explicit ethical guidelines. Future research will generally focus on enhancing the model's ability to handle multiple inputs, thereby making it more intelligent, safe, and reliable.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</title>
<link>https://arxiv.org/abs/2506.18199</link>
<guid>https://arxiv.org/abs/2506.18199</guid>
<content:encoded><![CDATA[
arXiv:2506.18199v2 Announce Type: replace 
Abstract: Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation</title>
<link>https://arxiv.org/abs/2506.21875</link>
<guid>https://arxiv.org/abs/2506.21875</guid>
<content:encoded><![CDATA[
arXiv:2506.21875v2 Announce Type: replace 
Abstract: Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech's unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we present a novel approach to thoroughly evaluate LLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Perception-Aware Policy Optimization for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2507.06448</link>
<guid>https://arxiv.org/abs/2507.06448</guid>
<content:encoded><![CDATA[
arXiv:2507.06448v3 Announce Type: replace 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a highly effective strategy for endowing Large Language Models (LLMs) with robust multi-step reasoning abilities. However, its design and optimizations remain tailored to purely textual domains, resulting in suboptimal performance when applied to multimodal reasoning tasks. In particular, we observe that a major source of error in current multimodal reasoning lies in the perception of visual inputs. To address this bottleneck, we propose PAPO, a novel policy gradient algorithm that encourages the model to learn to perceive while learning to reason. Specifically, we introduce the Implicit Perception Loss in the form of a KL divergence term, which can be seamlessly plugged into mainstream RLVR algorithms such as GRPO and DAPO. Notably, PAPO does not rely on additional data curation, reward models, or stronger teacher models. To further enhance the training stability of PAPO, we introduce the Double Entropy Loss, which effectively regularizes the new KL objective without compromising performance. Despite its simplicity, PAPO yields significant overall improvements of 4.4%-17.5% on diverse multimodal benchmarks. The improvements are more pronounced, approaching 8.0%-19.1%, on tasks with high vision dependency. We also observe a substantial reduction of 30.5% in perception errors, indicating improved perceptual capabilities with PAPO. Overall, our work introduces a deeper integration of perception-aware supervision into core learning objectives and lays the groundwork for a new RL framework that encourages visually grounded reasoning. Code and data will be made publicly available for research purposes. Project page: https://mikewangwzhl.github.io/PAPO.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities</title>
<link>https://arxiv.org/abs/2507.07695</link>
<guid>https://arxiv.org/abs/2507.07695</guid>
<content:encoded><![CDATA[
arXiv:2507.07695v2 Announce Type: replace 
Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large Language Models (LLMs) to incorporate a larger body of knowledge. Although many fine-tuning techniques have been developed to reduce the time and computational cost involved, the challenge persists as LLMs continue to grow in size and complexity. To address this, a new approach to knowledge expansion in LLMs is needed. Retrieval-Augmented Generation (RAG) offers one such alternative by storing external knowledge in a database and retrieving relevant chunks to support question answering. However, naive implementations of RAG face significant limitations in scalability and answer accuracy. This paper introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome these limitations. Inspired by the divide-and-conquer paradigm, K2RAG integrates dense and sparse vector search, knowledge graphs, and text summarization to improve retrieval quality and system efficiency. The framework also includes a preprocessing step that summarizes the training data, significantly reducing the training time. K2RAG was evaluated using the MultiHopRAG dataset, where the proposed pipeline was trained on the document corpus and tested on a separate evaluation set. Results demonstrated notable improvements over common naive RAG implementations. K2RAG achieved the highest mean answer similarity score of 0.57, and reached the highest third quartile (Q3) similarity of 0.82, indicating better alignment with ground-truth answers. In addition to improved accuracy, the framework proved highly efficient. The summarization step reduced the average training time of individual components by 93%, and execution speed was up to 40% faster than traditional knowledge graph-based RAG systems. K2RAG also demonstrated superior scalability, requiring three times less VRAM than several naive RAG implementations tested in this study.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures</title>
<link>https://arxiv.org/abs/2507.08606</link>
<guid>https://arxiv.org/abs/2507.08606</guid>
<content:encoded><![CDATA[
arXiv:2507.08606v3 Announce Type: replace 
Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document understanding that eliminates the need for absolute 2D positional embeddings. We extend self-attention to take into account text block positions in relative polar coordinate system rather than the Cartesian one. Despite being pre-trained on a dataset more than six times smaller than the widely used IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results demonstrate that a carefully designed attention mechanism can compensate for reduced pre-training data, offering an efficient and effective alternative for document understanding.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires</title>
<link>https://arxiv.org/abs/2507.10073</link>
<guid>https://arxiv.org/abs/2507.10073</guid>
<content:encoded><![CDATA[
arXiv:2507.10073v2 Announce Type: replace 
Abstract: Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILID: Native Script Language Identification for Indian Languages</title>
<link>https://arxiv.org/abs/2507.11832</link>
<guid>https://arxiv.org/abs/2507.11832</guid>
<content:encoded><![CDATA[
arXiv:2507.11832v2 Announce Type: replace 
Abstract: The language identification task is a crucial fundamental step in NLP. Often it serves as a pre-processing step for widely used NLP applications such as multilingual machine translation, information retrieval, question and answering, and text summarization. The core challenge of language identification lies in distinguishing languages in noisy, short, and code-mixed environments. This becomes even harder in case of diverse Indian languages that exhibit lexical and phonetic similarities, but have distinct differences. Many Indian languages share the same script, making the task even more challenging. Taking all these challenges into account, we develop and release a dataset of 250K sentences consisting of 23 languages including English and all 22 official Indian languages labeled with their language identifiers, where data in most languages are newly created. We also develop and release baseline models using state-of-the-art approaches in machine learning and fine-tuning pre-trained transformer models. Our models outperforms the state-of-the-art pre-trained transformer models for the language identification task. The dataset and the codes are available at https://yashingle-ai.github.io/ILID/ and in Huggingface open source libraries.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment</title>
<link>https://arxiv.org/abs/2401.13481</link>
<guid>https://arxiv.org/abs/2401.13481</guid>
<content:encoded><![CDATA[
arXiv:2401.13481v3 Announce Type: replace-cross 
Abstract: Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EgoOops: A Dataset for Mistake Action Detection from Egocentric Videos referring to Procedural Texts</title>
<link>https://arxiv.org/abs/2410.05343</link>
<guid>https://arxiv.org/abs/2410.05343</guid>
<content:encoded><![CDATA[
arXiv:2410.05343v3 Announce Type: replace-cross 
Abstract: Mistake action detection is crucial for developing intelligent archives that detect workers' errors and provide feedback. Existing studies have focused on visually apparent mistakes in free-style activities, resulting in video-only approaches to mistake detection. However, in text-following activities, models cannot determine the correctness of some actions without referring to the texts. Additionally, current mistake datasets rarely use procedural texts for video recording except for cooking. To fill these gaps, this paper proposes the EgoOops dataset, where egocentric videos record erroneous activities when following procedural texts across diverse domains. It features three types of annotations: video-text alignment, mistake labels, and descriptions for mistakes. We also propose a mistake detection approach, combining video-text alignment and mistake label classification to leverage the texts. Our experimental results show that incorporating procedural texts is essential for mistake detection. Data is available through https://y-haneji.github.io/EgoOops-project-page/.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfAlign: Inference-aware language model alignment</title>
<link>https://arxiv.org/abs/2412.19792</link>
<guid>https://arxiv.org/abs/2412.19792</guid>
<content:encoded><![CDATA[
arXiv:2412.19792v4 Announce Type: replace-cross 
Abstract: Language model alignment is a critical step in training modern generative language models. Alignment targets to improve win rate of a sample from the aligned model against the base model. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-time methods. To this end, we propose a framework for inference-aware alignment (InfAlign), which aims to optimize inference-time win rate of the aligned policy against the base model. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a transformation of the reward. This motivates us to provide the calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. For best-of-N sampling and best-of-N jailbreaking, we propose specific transformations offering up to 3-8% improvement on inference-time win rates. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning</title>
<link>https://arxiv.org/abs/2503.15621</link>
<guid>https://arxiv.org/abs/2503.15621</guid>
<content:encoded><![CDATA[
arXiv:2503.15621v2 Announce Type: replace-cross 
Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has highlighted the critical roles of both the visual backbone and the underlying language model. While prior work has primarily focused on scaling these components to billions of parameters, the trade-offs between model size, architecture, and performance remain underexplored. Additionally, inconsistencies in training data and evaluation protocols have hindered direct comparisons, making it difficult to derive optimal design choices. In this paper, we introduce LLaVA-MORE, a new family of MLLMs that integrates recent language models with diverse visual backbones. To ensure fair comparisons, we employ a unified training protocol applied consistently across all architectures. Our analysis systematically explores both small- and medium-scale LLMs -- including Phi-4, LLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and instruction following, while examining the relationship between model size and performance. Beyond evaluating the LLM impact on final results, we conduct a comprehensive study of various visual encoders, ranging from CLIP-based architectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional experiments investigate the effects of increased image resolution and variations in pre-training datasets. Overall, our results provide insights into the design of more effective MLLMs, offering a reproducible evaluation framework that facilitates direct comparisons and can guide future model development. Our source code and trained models are publicly available at: https://github.com/aimagelab/LLaVA-MORE.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents</title>
<link>https://arxiv.org/abs/2503.18666</link>
<guid>https://arxiv.org/abs/2503.18666</guid>
<content:encoded><![CDATA[
arXiv:2503.18666v3 Announce Type: replace-cross 
Abstract: Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identify 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
<link>https://arxiv.org/abs/2505.18102</link>
<guid>https://arxiv.org/abs/2505.18102</guid>
<content:encoded><![CDATA[
arXiv:2505.18102v2 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Reporter: A Path to a New Genre of Scientific Communication</title>
<link>https://arxiv.org/abs/2507.05903</link>
<guid>https://arxiv.org/abs/2507.05903</guid>
<content:encoded><![CDATA[
arXiv:2507.05903v2 Announce Type: replace-cross 
Abstract: The AI-Reporter represents a paradigmatic shift in scientific publication practice. This document demonstrates through a concrete case study how our system transforms academic presentations into publication-ready chapters -- in less than three minutes. Using Arno Simons' lecture on Large Language Models from the ``Large Language Models for the History, Philosophy, and Sociology of Science'' workshop (NEPI) as an example, we show how technological innovation bridges the gap between ephemeral presentation and permanent scientific documentation.
]]></content:encoded>
<pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian</title>
<link>https://arxiv.org/abs/2507.22159</link>
<guid>https://arxiv.org/abs/2507.22159</guid>
<content:encoded><![CDATA[
<div> Keywords: Indonesian, preference-based research, large language models, IndoPref, benchmark

Summary:<br /><br />This article introduces IndoPref, a human-authored multi-domain Indonesian preference dataset designed to evaluate the naturalness and quality of text generated by large language models (LLMs). The dataset addresses the underrepresentation of Indonesian language in preference-based research for LLMs. All annotations in IndoPref are in Indonesian, ensuring cultural and linguistic authenticity. The dataset has been evaluated using Krippendorff's alpha, showing strong inter-annotator agreement. Multiple LLMs have been benchmarked using IndoPref to assess the quality of their output. This initiative aims to address the gap in preference-based research for Indonesian language and enhance the performance of LLMs in generating text that is natural and high-quality in Indonesian. 

Summary: <div>
arXiv:2507.22159v1 Announce Type: new 
Abstract: Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles</title>
<link>https://arxiv.org/abs/2507.22168</link>
<guid>https://arxiv.org/abs/2507.22168</guid>
<content:encoded><![CDATA[
<div> keywords: Large Language Models, benchmarks, persona-based prompting, writing styles, performance evaluation<br />
Summary:<br />
Current benchmarks for Large Language Models (LLMs) lack writing style diversity, potentially leading to brittle performance. This study explores the impact of persona-based prompting on LLM evaluation. Results show that variations in writing style significantly affect LLM performance. Specific writing styles consistently trigger either low or high performance across models. This method offers a scalable approach to enhance benchmarks, improving the validity of LLM performance assessments across linguistic variations.<br /> 
Summary: <div>
arXiv:2507.22168v1 Announce Type: new 
Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with "non-standard" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models</title>
<link>https://arxiv.org/abs/2507.22187</link>
<guid>https://arxiv.org/abs/2507.22187</guid>
<content:encoded><![CDATA[
<div> pipeline, Verb Frame Frequencies, syntactic frames, large language models, automated

Summary:
The article introduces an automated pipeline for estimating Verb Frame Frequencies (VFFs) using large language models (LLMs). This pipeline outperforms existing syntactic parsers and allows for rapid and scalable estimation of VFFs. By instructing an LLM to analyze the syntactic structure of sentences containing 476 English verbs, the pipeline generates a new VFF database with broader verb coverage and finer-grained syntactic distinctions. It also provides explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is customizable and extensible to new verbs, syntactic frames, and languages. The authors release all code and data to support further research in automated frame frequency estimation.

<br /><br />Summary: <div>
arXiv:2507.22187v1 Announce Type: new 
Abstract: We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The role of media memorability in facilitating startups' access to venture capital funding</title>
<link>https://arxiv.org/abs/2507.22201</link>
<guid>https://arxiv.org/abs/2507.22201</guid>
<content:encoded><![CDATA[
<div> Keywords: Media memorability, venture capital investment, startup, entrepreneurial finance, news semantic networks

Summary: 
Media reputation plays a crucial role in attracting venture capital investment for startups. Previous research has focused mainly on general media exposure, but this study introduces the concept of media memorability, which is the ability of media to imprint a startup's name in the memory of investors. Data from 197 UK startups in the micro and nanotechnology sector show that media memorability significantly influences investment outcomes. Venture capitalists look for cues such as a startup's distinctiveness and connectivity within news semantic networks. Startups should focus on creating targeted, meaningful coverage that highlights their uniqueness and relevance within the industry conversation to strengthen brand memorability and attract investment. This research contributes to understanding the relationship between media, entrepreneurial finance, and startup funding decisions. 

<br /><br />Summary: <div>
arXiv:2507.22201v1 Announce Type: new 
Abstract: Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?</title>
<link>https://arxiv.org/abs/2507.22209</link>
<guid>https://arxiv.org/abs/2507.22209</guid>
<content:encoded><![CDATA[
<div> entropy, psycholinguistic, language model, Monte Carlo, regression

Summary:
- Contextual entropy is a psycholinguistic measure that predicts how difficult it is to process a word before encountering it.
- Recent studies have explored the impact of entropy on language processing, in addition to surprisal effects.
- Entropy is often estimated based on the probability of a word's first subword token, leading to underestimation and potential distortion of true word entropy.
- Monte Carlo (MC) estimates of word entropy, allowing words to span multiple tokens, provide a more accurate measure.
- Regression experiments on reading times reveal differences between first-token and MC word entropy, cautioning against relying solely on first-token approximations for contextual entropy analysis.<br /><br />Summary: <div>
arXiv:2507.22209v1 Announce Type: new 
Abstract: Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model's probability distribution over a word's first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation</title>
<link>https://arxiv.org/abs/2507.22219</link>
<guid>https://arxiv.org/abs/2507.22219</guid>
<content:encoded><![CDATA[
<div> Preference-learning, machine translation, Direct Preference Optimization, Reinforcement Learning, Teacher-Model Refinement<br />
<br />
Summary:<br />
Preference-learning methods in machine translation, such as Direct Preference Optimization, have shown impressive results but struggle with generalization. The novel framework proposed, Reinforcement Learning from Teacher-Model Refinement, eliminates reliance on static triplets by utilizing continuous feedback from an external teacher model. By framing each translation step as a micro-tutorial and rewarding the actor based on alignment with the teacher's refinement, the model learns to emulate the teacher's improvements incrementally. Guided by negative edit distance and COMET score, the actor improves both lexical and structural fidelity and semantic adequacy. On the FLORES-200 benchmark, RLfR outperforms MT-SFT and preference-based baselines, showing significant enhancements in COMET and M-ETA scores. <div>
arXiv:2507.22219v1 Announce Type: new 
Abstract: Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs</title>
<link>https://arxiv.org/abs/2507.22286</link>
<guid>https://arxiv.org/abs/2507.22286</guid>
<content:encoded><![CDATA[
<div> constructionist approach, usage-based, Large Language Models, neural representations, dative constructions<br />
<br />
Summary: This study examines how Large Language Models (LLMs) represent English dative constructions using Pythia-1.4B. By analyzing a dataset of varied sentence pairs, it is found that LLMs' internal representations reflect the function-infused gradience proposed by the usage-based constructionist approach. The separability between construction representations in LLMs is influenced by gradient preference strength, with more prototypical exemplars occupying distinct regions in the activation space. These results demonstrate that LLMs learn rich, meaning-infused, graded representations of constructions and support geometric measures of basic constructionist principles within LLMs. <div>
arXiv:2507.22286v1 Announce Type: new 
Abstract: The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations</title>
<link>https://arxiv.org/abs/2507.22289</link>
<guid>https://arxiv.org/abs/2507.22289</guid>
<content:encoded><![CDATA[
<div> Intent recognition, task-oriented dialogue systems, out-of-scope detection, BERT, LLMs

Summary: 
Intent recognition is crucial for task-oriented dialogue systems (TODS), but it often requires a large amount of annotated data. This work introduces a hybrid approach that combines BERT and LLMs in zero and few-shot settings to recognize intents and detect out-of-scope (OOS) utterances. By leveraging the generalization power of LLMs and computational efficiency of BERT, the proposed method improves system performance on multi-party conversation corpora. The approach involves sharing information from BERT outputs to LLMs, resulting in enhanced performance in intent recognition and OOS detection within TODS. <div>
arXiv:2507.22289v1 Announce Type: new 
Abstract: Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers</title>
<link>https://arxiv.org/abs/2507.22337</link>
<guid>https://arxiv.org/abs/2507.22337</guid>
<content:encoded><![CDATA[
<div> taxonomy, benchmark datasets, logic-based classification, neural information retrieval models, negation types 

Summary:
A new study examines the challenges posed by negation in traditional neural information retrieval and large language model-based models. By developing a comprehensive taxonomy of negation derived from various fields, the researchers introduce benchmark datasets to assess model performance and suggest a logic-based classification scheme for analysis. The balanced data distribution in the taxonomy aids in faster model convergence on the proposed NevIR dataset. Additionally, a classification schema evaluates the coverage of negation types in existing datasets, providing insights into the potential factors affecting model generalization. These findings offer a pathway for improving the robustness of neural models in handling complex reasoning tasks involving negation. <br /><br />Summary: <div>
arXiv:2507.22337v1 Announce Type: new 
Abstract: Understanding and solving complex reasoning tasks is vital for addressing the information needs of a user. Although dense neural models learn contextualised embeddings, they still underperform on queries containing negation. To understand this phenomenon, we study negation in both traditional neural information retrieval and LLM-based models. We (1) introduce a taxonomy of negation that derives from philosophical, linguistic, and logical definitions; (2) generate two benchmark datasets that can be used to evaluate the performance of neural information retrieval models and to fine-tune models for a more robust performance on negation; and (3) propose a logic-based classification mechanism that can be used to analyze the performance of retrieval models on existing datasets. Our taxonomy produces a balanced data distribution over negation types, providing a better training setup that leads to faster convergence on the NevIR dataset. Moreover, we propose a classification schema that reveals the coverage of negation types in existing datasets, offering insights into the factors that might affect the generalization of fine-tuned models on negation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors</title>
<link>https://arxiv.org/abs/2507.22367</link>
<guid>https://arxiv.org/abs/2507.22367</guid>
<content:encoded><![CDATA[
<div> Keywords: personality assessment, psychology-informed prompts, Text-Centric Trait Fusion Network, large language models, AVI Challenge

Summary: 
The article introduces a novel personality assessment framework called Traits Run Deep, which utilizes psychology-informed prompts to extract high-level personality-relevant semantic representations. This framework includes a Text-Centric Trait Fusion Network that integrates asynchronous signals from various modalities. It incorporates a Chunk-Wise Projector for dimensionality reduction, a Cross-Modal Connector, and a Text Feature Enhancer for modality fusion, and an ensemble regression head for improved generalization. By guiding large language models using personality-specific prompts, the framework enhances representation quality. Additionally, extracting and fusing audio-visual apparent behavior features further improves accuracy. Experimental results show a significant reduction in mean squared error and the framework ranks first in the Personality Assessment track of the AVI Challenge 2025. The source code for the framework is available on GitHub at https://github.com/MSA-LMC/TraitsRunDeep.<br /><br />Summary: <div>
arXiv:2507.22367v1 Announce Type: new 
Abstract: Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \textit{\textbf{Traits Run Deep}}. It employs \textit{\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at https://github.com/MSA-LMC/TraitsRunDeep.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs</title>
<link>https://arxiv.org/abs/2507.22387</link>
<guid>https://arxiv.org/abs/2507.22387</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, patent writing, PATENTWRITER, benchmarking framework, LLM evaluation

Summary: 
Large language models (LLMs) are being utilized to revolutionize patent writing. The paper introduces PATENTWRITER, a benchmarking framework to evaluate LLMs in generating patent abstracts. Six leading LLMs, including GPT-4 and LLaMA-3, were tested using various prompting strategies. Evaluation metrics included NLP measures, robustness under input perturbations, and applicability in downstream tasks. The study also analyzed stylistic aspects like length, readability, and tone. Results showed that modern LLMs can produce high-quality and stylistically appropriate patent abstracts, often outperforming domain-specific approaches. The open-sourced code and dataset enhance reproducibility and support further research. <div>
arXiv:2507.22387v1 Announce Type: new 
Abstract: Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Question Generation for Assessing Early Literacy Reading Comprehension</title>
<link>https://arxiv.org/abs/2507.22410</link>
<guid>https://arxiv.org/abs/2507.22410</guid>
<content:encoded><![CDATA[
<div> Keywords: reading comprehension, content-based interactions, English learners, question generation, language models

Summary: 
This paper presents a new approach for generating comprehension questions for English learners in grades K-2. The method ensures comprehensive coverage of the reading material and tailors questions to the individual student's proficiency levels. It also generates a diverse range of question types at varying difficulty levels to provide a thorough assessment. The study evaluates the performance of different language models using the FairytaleQA dataset as a basis. Ultimately, this approach has the potential to be integrated into AI-driven English instruction tools, offering personalized and effective learning experiences for young learners.<br /><br />Summary: <div>
arXiv:2507.22410v1 Announce Type: new 
Abstract: Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22411</link>
<guid>https://arxiv.org/abs/2507.22411</guid>
<content:encoded><![CDATA[
<div> Keywords: Needle-in-a-Haystack, Large Language Models, long contexts, NeedleChain, ROPE Contraction

Summary:
The Needle-in-a-Haystack benchmark, commonly used to test Large Language Models' (LLMs) ability to understand long contexts, may overestimate their true capability. A new benchmark, NeedleChain, is proposed where the context consists entirely of query-relevant information, challenging LLMs to fully grasp the input. The benchmark allows for flexible context length and reasoning order, providing a more comprehensive analysis of LLM performance. An approach called ROPE Contraction is introduced to enhance LLMs' long-context understanding. Experiments with various advanced LLMs reveal discrepancies between their ability to process large contexts and their capacity to truly understand them. The findings suggest the need for improved evaluation methods for assessing LLMs' long-context understanding. Source code and datasets for NeedleChain are available for further research and analysis. 

<br /><br />Summary: <div>
arXiv:2507.22411v1 Announce Type: new 
Abstract: The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini</title>
<link>https://arxiv.org/abs/2507.22445</link>
<guid>https://arxiv.org/abs/2507.22445</guid>
<content:encoded><![CDATA[
<div> culture, language model, narrative bias, AI-generated narratives, cultural alignment
<br />
The study investigates the cultural relevance of stories generated by a language model trained on Anglo-American texts for different nationalities. Using OpenAI's gpt-4o-mini model, the researchers generated 11,800 stories for 236 countries, finding that while the stories incorporate surface-level national symbols, they largely follow a uniform plot structure centered around nostalgia, tradition, and community events. This narrative homogeneity, characterized by sanitized conflicts and a lack of romantic elements, reflects a bias towards stability over change and tradition over growth. The authors argue that this structural conformity represents a distinct form of AI bias, impacting efforts to improve the cultural alignment of generative AI. The findings have implications for literary studies, narratology, critical AI studies, and NLP research.
<br /><br />Summary: <div>
arXiv:2507.22445v1 Announce Type: new 
Abstract: Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a 1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance</title>
<link>https://arxiv.org/abs/2507.22448</link>
<guid>https://arxiv.org/abs/2507.22448</guid>
<content:encoded><![CDATA[
<div> Keywords: Falcon-H1, large language models, hybrid architecture, high performance, efficiency

Summary: 
Falcon-H1 introduces a new series of large language models with hybrid architecture designs combining Transformer-based attention and State Space Models for improved memory and efficiency. The models come in various configurations with parameters ranging from 0.5B to 34B, including quantized instruction-tuned versions. Despite their smaller size, Falcon-H1 models outperform larger models up to 70B scale while using less data. They excel in reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is versatile for various applications. The models are released under an open-source license, emphasizing accessibility and impact in AI research.<br /><br />Summary: <div>
arXiv:2507.22448v1 Announce Type: new 
Abstract: In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models</title>
<link>https://arxiv.org/abs/2507.22457</link>
<guid>https://arxiv.org/abs/2507.22457</guid>
<content:encoded><![CDATA[
<div> large language models, abstract reasoners, zero-shot performance, input encoding, fine-tuning
Summary:
Large language models (LLMs) have been criticized for their poor zero-shot performance on challenging tasks, leading to debates about their ability to serve as abstract reasoners. The study shows that even minimal parameter tuning for input encoding can significantly improve LLMs' performance on tasks. However, despite this improvement, the fine-tuning does not consistently transfer across different datasets. These findings suggest the need to reconsider the definition of "abstract reasoner" and the significance of LLMs fitting this description. The discussion surrounding LLMs' capabilities raises questions about the underlying mechanisms driving their performance and challenges the assumptions about their ability to reason abstractly. The study offers insights into the complexities of interpreting LLMs' performance and highlights the importance of understanding the limitations and strengths of these models. <br /><br />Summary: <div>
arXiv:2507.22457v1 Announce Type: new 
Abstract: Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IFEvalCode: Controlled Code Generation</title>
<link>https://arxiv.org/abs/2507.22462</link>
<guid>https://arxiv.org/abs/2507.22462</guid>
<content:encoded><![CDATA[
<div> Code LLMs, controlled code generation, instruction-following, multilingual benchmark, closed-source models<br />
Summary:<br />
The article introduces forward and backward constraints generation to enhance the instruction-following capabilities of Code LLMs in controlled code generation tasks. It presents IFEvalCode, a multilingual benchmark with 1.6K test samples across seven programming languages, evaluating correctness and instruction-following separately. Experiments on over 40 LLMs show that closed-source models outperform open-source ones in controllable code generation. However, there is a notable disparity between the models' ability to generate correct code and code that precisely adheres to provided instructions. The paper underscores the importance of aligning code outputs with detailed requirements such as coding style, line count, and structural constraints beyond mere correctness. <div>
arXiv:2507.22462v1 Announce Type: new 
Abstract: Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLM-SQL: An Exploration of Small Language Models for Text-to-SQL</title>
<link>https://arxiv.org/abs/2507.22478</link>
<guid>https://arxiv.org/abs/2507.22478</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Small language models, Text-to-SQL, Post-training techniques, Supervised fine-tuning

Summary:
Large language models (LLMs) excel in translating natural language questions into SQL queries, while small language models (SLMs) struggle due to limited logical reasoning abilities. This study explores enhancing SLM performance in Text-to-SQL tasks through post-training techniques. The researchers leverage the SynSQL-2.5M dataset to create two derived datasets for SQL generation and revision. By employing supervised fine-tuning and reinforcement learning-based post-training on SLMs, followed by a corrective self-consistency approach during inference, significant improvements are achieved. Experimentally, the SLM-SQL method demonstrates effectiveness and generalizability, with models reaching up to 67.08% execution accuracy on the BIRD development set. The release of the dataset, model, and code on GitHub offers valuable resources for further research in this domain. 

<br /><br />Summary: <div>
arXiv:2507.22478v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy (EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset, model, and code to github: https://github.com/CycloneBoy/slm_sql.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records</title>
<link>https://arxiv.org/abs/2507.22533</link>
<guid>https://arxiv.org/abs/2507.22533</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Clinical Decision Support, Electronic Health Records, Temporal Knowledge Graphs, Clinical Guidelines

Summary:
CliCARE is a framework designed to address challenges in implementing Large Language Models (LLMs) for clinical decision support in oncology. The framework tackles issues such as processing extensive and multilingual patient records, mitigating the risk of clinical hallucination, and improving evaluation metrics. CliCARE transforms unstructured Electronic Health Records (EHRs) into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies and aligns these with normative guideline knowledge graphs for decision support. The framework provides evidence-grounded decision support by generating clinical summaries and actionable recommendations for oncologists. Validation using diverse datasets demonstrated that CliCARE outperforms strong baseline methods and shows high correlation with expert oncologists' assessments. Overall, CliCARE offers promise in enhancing clinical decision support in oncology by effectively utilizing LLMs and incorporating clinical guidelines for improved decision-making. 

<br /><br />Summary: <div>
arXiv:2507.22533v1 Announce Type: new 
Abstract: Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support</title>
<link>https://arxiv.org/abs/2507.22542</link>
<guid>https://arxiv.org/abs/2507.22542</guid>
<content:encoded><![CDATA[
<div> benchmarking, ViLLMs, Customer Support Conversations Dataset, evaluation framework, Vietnamese LLMs

Summary:
The article introduces the Customer Support Conversations Dataset (CSConDa), a benchmark dataset of over 9,000 QA pairs from real interactions with human advisors at a Vietnamese software company. It covers various topics such as pricing, product availability, and technical troubleshooting. The study evaluates 11 lightweight open-source ViLLMs on CSConDa using automatic metrics and syntactic analysis to understand model behavior and identify areas for improvement. The research aims to support the development of next-generation Vietnamese LLMs for customer service QA applications. The dataset is publicly accessible, providing a valuable resource for researchers and industry professionals looking to evaluate and select suitable models for customer support systems.<br /><br />Summary: <div>
arXiv:2507.22542v1 Announce Type: new 
Abstract: With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ControlMed: Adding Reasoning Control to Medical Language Model</title>
<link>https://arxiv.org/abs/2507.22545</link>
<guid>https://arxiv.org/abs/2507.22545</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical Domain, Reasoning Accuracy, Computational Efficiency, ControlMed

Summary:
ControlMed is a new medical language model designed to enhance reasoning accuracy and computational efficiency in clinical decision-making. It allows users to actively control the length of the reasoning process at inference time through fine-grained control markers. The model is trained through a three-stage pipeline, including pre-training on a synthetic medical instruction dataset, supervised fine-tuning with multi-length reasoning data, and reinforcement learning to improve accuracy and response quality. Experimental results show that ControlMed performs similarly or better than existing models on English and Korean medical benchmarks. Users can adjust reasoning length to balance accuracy and efficiency as needed, making ControlMed a practical solution for clinical question answering and medical information analysis.<br /><br />Summary: <div>
arXiv:2507.22545v1 Announce Type: new 
Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs</title>
<link>https://arxiv.org/abs/2507.22564</link>
<guid>https://arxiv.org/abs/2507.22564</guid>
<content:encoded><![CDATA[
<div> attack, language models, cognitive biases, safety mechanisms, adversarial

Summary:
CognitiveAttack introduces a new red-teaming framework that leverages individual and combined cognitive biases to bypass safety mechanisms of Large Language Models (LLMs). By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates optimized prompts that successfully attack LLMs. Experimental results demonstrate vulnerabilities in 30 LLMs, particularly open-source models, with CognitiveAttack outperforming the state-of-the-art black-box method PAP. The study highlights the power of multi-bias interactions as a significant yet underexplored attack vector in undermining LLM safeguards. This interdisciplinary work bridges cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems. 

<br /><br />Summary: <div>
arXiv:2507.22564v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling the Influence of Amplifying Language-Specific Neurons</title>
<link>https://arxiv.org/abs/2507.22581</link>
<guid>https://arxiv.org/abs/2507.22581</guid>
<content:encoded><![CDATA[
<div> Language-specific neurons, amplification, multilingual behavior, low-resource languages, cross-lingual transfer
Summary:
Language-specific neurons in large language models (LLMs) can influence model behavior by deactivating them, but their role in amplification is not well understood. This study explores amplifying language-specific neurons across 18 languages, including low-resource languages, using three different models. The amplification factors are evaluated based on their effectiveness in steering output towards the target language, and tested on various downstream tasks. The results show that optimal amplification factors can effectively steer output towards different languages but may not always improve cross-language performance. This highlights the importance of language-specific neurons in multilingual behavior, with amplification potentially benefitting low-resource languages more than cross-lingual transfer tasks. 
<br /><br />Summary: <div>
arXiv:2507.22581v1 Announce Type: new 
Abstract: Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BALSAM: A Platform for Benchmarking Arabic Large Language Models</title>
<link>https://arxiv.org/abs/2507.22603</link>
<guid>https://arxiv.org/abs/2507.22603</guid>
<content:encoded><![CDATA[
<div> advancement, Large Language Models, Arabic, benchmark, BALSAM

Summary: 
The article discusses the disparity in the performance of Large Language Models (LLMs) in English compared to Arabic, attributed to data scarcity, linguistic diversity of Arabic, and benchmark quality. To address these challenges, the authors introduce BALSAM, a community-driven benchmark for Arabic LLM development and evaluation. BALSAM comprises 78 NLP tasks from 14 categories, with 52K examples for testing and development. It offers a centralized platform for blind evaluation and aims to set standards and foster collaborative research to enhance Arabic LLM capabilities. <div>
arXiv:2507.22603v1 Announce Type: new 
Abstract: The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation</title>
<link>https://arxiv.org/abs/2507.22608</link>
<guid>https://arxiv.org/abs/2507.22608</guid>
<content:encoded><![CDATA[
<div> Language-specific neurons, Multilingual abilities, Neural mechanisms, Language arithmetics, Cross-lingual neuron steering<br />
Summary:<br />
Large language models exhibit strong multilingual abilities, with language-specific neurons identified across 21 languages. These neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Language arithmetics are used to steer models towards desired language behavior, outperforming simpler approaches. Interventions effectively guide behavior across multilingual tasks, with manipulation more successful for high-resource languages. Typological similarity improves effectiveness, and cross-lingual neuron steering enhances downstream performance. Internal "fallback" mechanisms for language selection are revealed as neurons are progressively deactivated. The code is publicly available for access. <div>
arXiv:2507.22608v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Political Views of Large Language Models: Identification and Steering</title>
<link>https://arxiv.org/abs/2507.22623</link>
<guid>https://arxiv.org/abs/2507.22623</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, political biases, ideological positions, multilingual settings, manipulability

Summary:
Large language models (LLMs) are being increasingly utilized in various applications, raising concerns about their potential political biases. Previous studies have shown that LLMs tend to exhibit political biases, often leaning towards liberal or progressive positions. This study addresses gaps in existing research by conducting a large-scale analysis of political orientation in modern open-source LLMs across different languages. The results indicate that larger models tend to shift towards libertarian-left positions, with variations observed across languages and model families. Additionally, the study demonstrates that political stances of LLMs can be manipulated using a center-of-mass activation intervention technique, allowing for the steering of model responses towards alternative ideological positions in various languages. The code used in the study is publicly available for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.22623v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at https://github.com/d-gurgurov/Political-Ideologies-LLMs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment</title>
<link>https://arxiv.org/abs/2507.22676</link>
<guid>https://arxiv.org/abs/2507.22676</guid>
<content:encoded><![CDATA[
<div> Keywords: Interview performance assessment, Multimodal, Ensemble learning, AVI Challenge 2025, Feature extraction

Summary:
By integrating video, audio, and text data, this framework assesses interview performance across multiple dimensions, achieving a low average MSE of 0.1824 and winning the AVI Challenge 2025. The approach uses modality-specific feature extractors and a Shared Compression Multilayer Perceptron to unify multimodal embeddings efficiently. An ensemble learning strategy with independent regression heads and mean-pooling aggregation ensures robust predictions on five key evaluation dimensions. The framework captures both explicit and implicit cues, enabling unbiased assessments and advancing automated interview evaluations. The complete implementation is available on GitHub at https://github.com/MSA-LMC/365Aspects. 

<br /><br />Summary: <div>
arXiv:2507.22676v1 Announce Type: new 
Abstract: Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at https://github.com/MSA-LMC/365Aspects.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs</title>
<link>https://arxiv.org/abs/2507.22716</link>
<guid>https://arxiv.org/abs/2507.22716</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, reinforcement learning, reasoning models, multi-dimensional reward system, TIRESRAG-R1 <br />
Summary: <br />
This paper introduces a novel framework called TIRESRAG-R1 that aims to enhance the reasoning abilities of large language models by addressing three main failure patterns in existing models. The framework utilizes a think-retrieve-reflect process along with a multi-dimensional reward system to improve reasoning and stability. It includes sufficiency rewards to encourage thorough retrieval of information, reasoning quality rewards to assess the rationality and accuracy of reasoning chains, and reflection rewards to detect and revise errors. Additionally, TIRESRAG-R1 implements a difficulty-aware reweighting strategy and training sample filtering to improve performance on complex tasks. Experimental results on multiple question-answering datasets demonstrate that TIRESRAG-R1 outperforms previous retrieval-augmented generation methods and generalizes well to single-hop tasks. <div>
arXiv:2507.22716v1 Announce Type: new 
Abstract: Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: https://github.com/probe2/TIRESRAG-R1.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Hallucination in Conversations for Low Resource Languages</title>
<link>https://arxiv.org/abs/2507.22720</link>
<guid>https://arxiv.org/abs/2507.22720</guid>
<content:encoded><![CDATA[
<div> Languages: Large Language Models, Hallucination, Hindi, Farsi, Mandarin
Summary: 
Large Language Models (LLMs) like GPT-3.5 and GPT-4o have shown proficiency in generating human-like text but often produce factually incorrect statements, known as 'hallucinations'. A study examined conversational data in Hindi, Farsi, and Mandarin to analyze errors in LLMs including GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1, and Qwen-3. Results revealed that Mandarin had the fewest hallucinations while Hindi and Farsi exhibited significantly more. Identifying and addressing hallucinations are crucial for enhancing the reliability and effectiveness of LLMs across languages. 

<br /><br />Summary: <div>
arXiv:2507.22720v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning</title>
<link>https://arxiv.org/abs/2507.22729</link>
<guid>https://arxiv.org/abs/2507.22729</guid>
<content:encoded><![CDATA[
<div> Language Models, Natural Language Processing, Text Generation, Text Embeddings, Clustering

Summary: 
- Large Language Models (LLMs) have become essential in Natural Language Processing (NLP) for generating text with impressive performance.
- Token-level representations in LLMs capture rich semantics but lose information when pooled into text embeddings.
- Non-generative tasks like clustering, classification, and retrieval require accurate sentence- or document-level embeddings.
- Adaptation strategies for pre-trained LLMs include aggregation techniques for token embeddings, task-specific prompt engineering, and contrastive fine-tuning for text-level augmentation.
- By combining these strategies, the study achieves state-of-the-art results on English clustering in the Massive Text Embedding Benchmark (MTEB).
- Analysis of the attention map shows that fine-tuning focuses on relevant words, improving compression of meaning in the final hidden state.
- Experiments show that LLMs can be effectively adapted as text embedding models using prompt engineering and contrastive fine-tuning on synthetically generated positive pairs.

<br /><br />Summary: <div>
arXiv:2507.22729v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index</title>
<link>https://arxiv.org/abs/2507.22744</link>
<guid>https://arxiv.org/abs/2507.22744</guid>
<content:encoded><![CDATA[
<div> hallucinations, abstractive summarization, reward-driven fine-tuning, Entity Hallucination Index, reinforcement learning

Summary:
In this work, the challenge of reducing hallucinations in abstractive summarization is addressed through a reward-driven fine-tuning framework. The framework optimizes for the Entity Hallucination Index (EHI), a metric that evaluates the presence, correctness, and grounding of named entities in generated summaries. By using reinforcement learning to fine-tune model parameters with EHI as a reward signal, the model generates entity-faithful outputs. The approach does not require human-written factuality annotations, allowing for scalable fine-tuning. Experimental results show consistent improvements in EHI scores across datasets, highlighting a reduction in entity-level hallucinations without compromising fluency or informativeness. The release of a reproducible Colab pipeline enables further research on hallucination-aware model fine-tuning using lightweight metrics like EHI. 

<br /><br />Summary: <div>
arXiv:2507.22744v1 Announce Type: new 
Abstract: Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title>
<link>https://arxiv.org/abs/2507.22752</link>
<guid>https://arxiv.org/abs/2507.22752</guid>
<content:encoded><![CDATA[
<div> benchmark, regional question answering, textual, visual modalities, dataset, large language models (LLMs), human judgments,

Summary:<br />
- A benchmark for open-ended regional question answering encompassing textual and visual modalities has been introduced, with manually curated questions and answers grounded in Wikipedia from Czechia, Slovakia, and Ukraine.
- Strong baselines using state-of-the-art large language models (LLMs) have been provided for evaluation.
- Human evaluations were used to assess the reliability of existing automatic evaluation metrics, revealing a significant gap in regional knowledge among current LLMs.
- Minimal correlation was found between automated metrics and human judgment.
- The dataset is released as a resource to assess regional knowledge in LLMs, study cross-lingual generation consistency, and advance the development of evaluation metrics for open-ended question answering. 

Summary: <div>
arXiv:2507.22752v1 Announce Type: new 
Abstract: We introduce a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. As a baseline, we evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results highlight a significant gap in regional knowledge among current LLMs. Moreover, apart from LLM-based evaluation, there is minimal correlation between automated metrics and human judgment. We release this dataset as a resource to (1) assess regional knowledge in LLMs, (2) study cross-lingual generation consistency in a challenging setting, and (3) advance the development of evaluation metrics for open-ended question answering.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Opportunities and Challenges of LLMs in Education: An NLP Perspective</title>
<link>https://arxiv.org/abs/2507.22753</link>
<guid>https://arxiv.org/abs/2507.22753</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, education, NLP, assistance, assessment

Summary: 
Large language models (LLMs) are increasingly being integrated into educational applications, impacting reading, writing, speaking, and tutoring. They play a significant role in providing assistance and assessment in education. LLMs offer new opportunities for teaching and learning in the field of natural language processing (NLP). However, challenges such as ethical considerations and data privacy need to be addressed. The article explores the potential of LLMs in developing language-focused educational applications and highlights the importance of understanding how LLMs can enhance educational NLP. This comprehensive overview is valuable for NLP researchers and practitioners looking to leverage LLMs for future educational advancements.

Summary: <div>
arXiv:2507.22753v1 Announce Type: new 
Abstract: Interest in the role of large language models (LLMs) in education is increasing, considering the new opportunities they offer for teaching, learning, and assessment. In this paper, we examine the impact of LLMs on educational NLP in the context of two main application scenarios: {\em assistance} and {\em assessment}, grounding them along the four dimensions -- reading, writing, speaking, and tutoring. We then present the new directions enabled by LLMs, and the key challenges to address. We envision that this holistic overview would be useful for NLP researchers and practitioners interested in exploring the role of LLMs in developing language-focused and NLP-enabled educational applications of the future.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASCA: LLM based-Multi Agents System for Credit Assessment</title>
<link>https://arxiv.org/abs/2507.22758</link>
<guid>https://arxiv.org/abs/2507.22758</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, Agent-based system, Credit assessment, MASCA, Hierarchical multi-agent systems

Summary: 
MASCA is introduced as a novel approach to credit evaluation, utilizing LLM-driven multi-agent systems. The framework employs specialized agents that collaborate to address sub-tasks and integrates contrastive learning for risk and reward assessment. The signaling game theory perspective on hierarchical multi-agent systems offers theoretical insights into their structure and interactions. The paper also includes a bias analysis in credit assessment to address fairness concerns. Experimental results demonstrate MASCA's superiority over baseline approaches, showcasing the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring. <div>
arXiv:2507.22758v1 Announce Type: new 
Abstract: Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph</title>
<link>https://arxiv.org/abs/2507.22811</link>
<guid>https://arxiv.org/abs/2507.22811</guid>
<content:encoded><![CDATA[
<div> entity linker, DBLP, RDF-based Knowledge Graph, dblp:Stream, zero-shot entity linker, LLMs

Summary:
- The article introduces a new entity linker for DBLP's updated version of the RDF-based Knowledge Graph in 2025, which now includes publication venues as a new entity type called dblp:Stream.
- Unlike previous versions, the entity linker in this work utilizes a zero-shot approach using LLMs to re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer.
- In the 2022 version, KG-embeddings and re-rankers were trained on a dataset to produce entity linkings, but this work represents a shift towards utilizing LLMs for entity linking.
- The proposed method aims to improve the accuracy and efficiency of entity linking in the DBLP Knowledge Graph by leveraging the capabilities of LLMs.
- By incorporating LLMs into the entity linking process, the researchers demonstrate a novel approach to handling entity linking in RDF-based Knowledge Graphs, potentially leading to more precise and reliable results. 

<br /><br />Summary: <div>
arXiv:2507.22811v1 Announce Type: new 
Abstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer of the LLM.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization</title>
<link>https://arxiv.org/abs/2507.22829</link>
<guid>https://arxiv.org/abs/2507.22829</guid>
<content:encoded><![CDATA[
<div> structured representation, query-focused table summarization, SPaGe, TaSoF, multi-table tasks <br />
Summary: <br />
This paper introduces a new approach to query-focused table summarization, utilizing structured representations for complex reasoning. The proposed framework, SPaGe, involves three phases: Structured Planning to generate a new structured plan (TaSoF) from a query, Graph-based Execution to convert plan steps to SQL, and Summary Generation for producing query-focused summaries. This structured approach captures complex dependencies, improving reliability and scalability, especially for multi-table tasks. Experimental results show that SPaGe outperforms prior models in both single- and multi-table settings, highlighting the advantages of structured representations in enhancing the robustness and efficiency of table summarization. <div>
arXiv:2507.22829v1 Announce Type: new 
Abstract: Query-focused table summarization requires complex reasoning, often approached through step-by-step natural language (NL) plans. However, NL plans are inherently ambiguous and lack structure, limiting their conversion into executable programs like SQL and hindering scalability, especially for multi-table tasks. To address this, we propose a paradigm shift to structured representations. We introduce a new structured plan, TaSoF, inspired by formalism in traditional multi-agent systems, and a framework, SPaGe, that formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. Our method explicitly captures complex dependencies and improves reliability. Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning</title>
<link>https://arxiv.org/abs/2507.22887</link>
<guid>https://arxiv.org/abs/2507.22887</guid>
<content:encoded><![CDATA[
<div> in-context learning, large language models, demos, positional bias, accuracy

Summary:
The study explores a new positional bias in in-context learning (ICL) called DEMOS' POSITION IN PROMPT (DPP) bias. It investigates how the position of demos, system prompts, and user messages in large language models (LLMs) affects accuracy and predictions. Two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, are introduced to quantify the impact of changes in demo positions. Experiments on ten LLMs across various tasks show that placing demos at the start of the prompt leads to more stable and accurate outputs, with gains of up to +6 points. Placing demos at the end of the user message significantly affects predictions, flipping over 30% without improving correctness on question answering tasks. Smaller models are more sensitive to this bias, but even large models show some impact on more complex tasks. <div>
arXiv:2507.22887v1 Announce Type: new 
Abstract: In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs</title>
<link>https://arxiv.org/abs/2507.22074</link>
<guid>https://arxiv.org/abs/2507.22074</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Vision-Language Models, Contextualized Iterative Multimodal Reasoning, Multi-modal instructions, Self-correction

Summary: 
Contextualized Iterative Multimodal Reasoning (CIMR) is a framework designed to enhance the ability of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) in processing complex, multi-step multi-modal instructions. The framework introduces a context-aware iterative reasoning and self-correction module, operating in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. CIMR integrates textual, visual, and contextual features dynamically at each step. By fine-tuning LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluating on the Multi-modal Action Planning (MAP) dataset, CIMR achieves 91.5% accuracy, surpassing state-of-the-art models like GPT-4V, LLaVA-1.5, MiniGPT-4, and InstructBLIP. This demonstrates the effectiveness of CIMR's iterative reasoning and self-correction capabilities in handling complex tasks. 

<br /><br />Summary: <div>
arXiv:2507.22074v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) has enhanced our ability to process and generate human language and visual information. However, these models often struggle with complex, multi-step multi-modal instructions that require logical reasoning, dynamic feedback integration, and iterative self-correction. To address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a novel framework that introduces a context-aware iterative reasoning and self-correction module. CIMR operates in two stages: initial reasoning and response generation, followed by iterative refinement using parsed multi-modal feedback. A dynamic fusion module deeply integrates textual, visual, and contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy, outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5 (78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the efficacy of its iterative reasoning and self-correction capabilities in complex tasks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback</title>
<link>https://arxiv.org/abs/2507.22080</link>
<guid>https://arxiv.org/abs/2507.22080</guid>
<content:encoded><![CDATA[
<div> synthesis, code generation, Large Language Models, collaborative programming, feedback mechanism\
<br />
CodeEvo is a framework that leverages collaborative programming practices to synthesize high-quality code data for training Large Language Models (LLMs). It involves iterative interactions between two LLM agents - a Coder and a Reviewer - where the Coder generates code and test cases based on instructions, while the Reviewer provides feedback and new instructions. A hybrid feedback mechanism ensures the quality control by combining compiler determinism with generative flexibility. Experimental results show that models trained on CodeEvo data outperform existing baselines in code generation tasks. The approach emphasizes the importance of code-centric data synthesis and offers insights into effective strategies for generating diverse and relevant code samples. <br /><br />Summary: <div>
arXiv:2507.22080v1 Announce Type: cross 
Abstract: Acquiring high-quality instruction-code pairs is essential for training Large Language Models (LLMs) for code generation. Manually curated data is expensive and inherently limited in scale, motivating the development of code-centric synthesis methods. Yet, current approaches either focus on augmenting existing code or rely on predefined heuristics, both lacking rigorous data validation, which results in synthetic data that is ungrounded, repetitive, or overly simplistic. Inspired by collaborative programming practices, we propose CodeEvo, a framework that synthesizes code data through iterative interactions between two LLM agents: a Coder, which generates candidate code and test cases based on given instructions, and a Reviewer, which guides the synthesis process by producing new instructions and feedback. We further introduce a hybrid feedback mechanism that combines compiler determinism with the generative flexibility of agents, enabling automatic quality control throughout synthesis. Extensive experiments demonstrate that models fine-tuned on CodeEvo data significantly outperform established baselines across code generation benchmarks with various difficulties. In-depth analyses further provide insights from multiple perspectives into effective code-centric data synthesis.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Optimization and Evaluation for LLM Automated Red Teaming</title>
<link>https://arxiv.org/abs/2507.22133</link>
<guid>https://arxiv.org/abs/2507.22133</guid>
<content:encoded><![CDATA[
<div> optimizing attack generator prompts, ASR, Attack Success Rate, discoverability, prompt optimization
<br />
Summary: 
This paper introduces a method for optimizing attack generator prompts using the Attack Success Rate (ASR) for individual attacks. By measuring the discoverability of each attack through multiple attempts against a randomly seeded target, exploitable patterns are revealed. This approach enhances the evaluation and refinement of attack generators by ensuring a more robust assessment of their effectiveness. Automated Red Teaming with Large Language Models (LLMs) is a growing trend in identifying system vulnerabilities, making the optimization of attack generators crucial. The evaluation of attack generators using ASR on individual attacks provides insights into their discoverability and effectiveness. By iterating attacks against randomly seeded targets, exploitable patterns can be detected, leading to improved prompt optimization. This method allows for a more comprehensive evaluation and refinement process, ultimately enhancing the overall security of target systems. <div>
arXiv:2507.22133v1 Announce Type: cross 
Abstract: Applications that use Large Language Models (LLMs) are becoming widespread, making the identification of system vulnerabilities increasingly important. Automated Red Teaming accelerates this effort by using an LLM to generate and execute attacks against target systems. Attack generators are evaluated using the Attack Success Rate (ASR) the sample mean calculated over the judgment of success for each attack. In this paper, we introduce a method for optimizing attack generator prompts that applies ASR to individual attacks. By repeating each attack multiple times against a randomly seeded target, we measure an attack's discoverability the expectation of the individual attack success. This approach reveals exploitable patterns that inform prompt optimization, ultimately enabling more robust evaluation and refinement of generators.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Deflection: Defending LLMs from Logit Manipulation</title>
<link>https://arxiv.org/abs/2507.22160</link>
<guid>https://arxiv.org/abs/2507.22160</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, security, jailbreaking attacks, logit-level attacks, defense 

Summary: 
The article discusses the importance of ensuring the security of Large Language Models (LLMs) against jailbreaking attacks, particularly in critical areas where these models are increasingly being adopted. Traditional defenses against malicious prompts may not be sufficient, as recent logit-level attacks have demonstrated the ability to manipulate token-selection processes. In response to these advanced attacks, Strategic Deflection (SDeflection) is proposed as a defense mechanism that redirects the model's response to neutralize harmful intents rather than outright refusing them. Experimental results show that SDeflection effectively lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This approach represents a significant shift in defensive strategies, moving towards strategic content redirection to mitigate advanced threats. 

<br /><br />Summary: <div>
arXiv:2507.22160v1 Announce Type: cross 
Abstract: With the growing adoption of Large Language Models (LLMs) in critical areas, ensuring their security against jailbreaking attacks is paramount. While traditional defenses primarily rely on refusing malicious prompts, recent logit-level attacks have demonstrated the ability to bypass these safeguards by directly manipulating the token-selection process during generation. We introduce Strategic Deflection (SDeflection), a defense that redefines the LLM's response to such advanced attacks. Instead of outright refusal, the model produces an answer that is semantically adjacent to the user's request yet strips away the harmful intent, thereby neutralizing the attacker's harmful intent. Our experiments demonstrate that SDeflection significantly lowers Attack Success Rate (ASR) while maintaining model performance on benign queries. This work presents a critical shift in defensive strategies, moving from simple refusal to strategic content redirection to neutralize advanced threats.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence</title>
<link>https://arxiv.org/abs/2507.22197</link>
<guid>https://arxiv.org/abs/2507.22197</guid>
<content:encoded><![CDATA[
<div> systematicity, explainability, AI models, connectionism, Fodorian<br />
<br />Summary: This paper argues that explainability is just one aspect of a broader ideal guiding our expectations of artificial intelligence (AI). The focus is on the extent to which AI demonstrates systematicity, not only in understanding how thoughts are composed but also in striving toward a consistent, coherent, comprehensive, and parsimoniously principled body of thought. The author presents a framework to define "the systematicity of thought" in four distinct senses to address challenges posed by connectionism. The conceptual framework aims to reconcile systematicity with connectionism and explores the historical notion of systematic thought informed by rationales for systematization. The paper identifies five rationales for systematization and examines their applicability to AI models, introducing the "hard systematicity challenge." The dynamic understanding of systematization helps determine the level of systematicity required in AI models based on the reasons for systematizing thought. <div>
arXiv:2507.22197v1 Announce Type: cross 
Abstract: This paper argues that explainability is only one facet of a broader ideal that shapes our expectations towards artificial intelligence (AI). Fundamentally, the issue is to what extent AI exhibits systematicity--not merely in being sensitive to how thoughts are composed of recombinable constituents, but in striving towards an integrated body of thought that is consistent, coherent, comprehensive, and parsimoniously principled. This richer conception of systematicity has been obscured by the long shadow of the "systematicity challenge" to connectionism, according to which network architectures are fundamentally at odds with what Fodor and colleagues termed "the systematicity of thought." I offer a conceptual framework for thinking about "the systematicity of thought" that distinguishes four senses of the phrase. I use these distinctions to defuse the perceived tension between systematicity and connectionism and show that the conception of systematicity that historically shaped our sense of what makes thought rational, authoritative, and scientific is more demanding than the Fodorian notion. To determine whether we have reason to hold AI models to this ideal of systematicity, I then argue, we must look to the rationales for systematization and explore to what extent they transfer to AI models. I identify five such rationales and apply them to AI. This brings into view the "hard systematicity challenge." However, the demand for systematization itself needs to be regulated by the rationales for systematization. This yields a dynamic understanding of the need to systematize thought, which tells us how systematic we need AI models to be and when.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoEx -- Co-evolving World-model and Exploration</title>
<link>https://arxiv.org/abs/2507.22281</link>
<guid>https://arxiv.org/abs/2507.22281</guid>
<content:encoded><![CDATA[
<div> Hierarchy, CoEx, LLM, world model, planning <br />
Summary: <br />
The article discusses the limitations of current LLM agents in effectively assimilating new observations into their static internal world models for planning. To address this issue, the authors propose a hierarchical agent architecture called CoEx. This architecture utilizes hierarchical state abstraction to allow LLM planning to co-evolve with a dynamically updated model of the world. CoEx employs LLM reasoning to orchestrate dynamic plans involving subgoals, continuously integrating these experiences into a neurosymbolic belief state. The agent is evaluated across various scenarios, demonstrating superior performance in planning and exploration compared to existing paradigms. Through experiments in diverse environments and complex tasks, including ALFWorld, PDDL, and Jericho, CoEx showcases its ability to generate effective plans and interact with the world in a more adaptive and efficient manner. <br /> <div>
arXiv:2507.22281v1 Announce Type: cross 
Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal world model, acquired during pretraining. However, existing agent designs fail to effectively assimilate new observations into dynamic updates of the world model. This reliance on the LLM's static internal world model is progressively prone to misalignment with the underlying true state of the world, leading to the generation of divergent and erroneous plans. We introduce a hierarchical agent architecture, CoEx, in which hierarchical state abstraction allows LLM planning to co-evolve with a dynamically updated model of the world. CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals, and its learning mechanism continuously incorporates these subgoal experiences into a persistent world model in the form of a neurosymbolic belief state, comprising textual inferences and code-based symbolic memory. We evaluate our agent across a diverse set of agent scenarios involving rich environments and complex tasks including ALFWorld, PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent paradigms in planning and exploration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models</title>
<link>https://arxiv.org/abs/2507.22359</link>
<guid>https://arxiv.org/abs/2507.22359</guid>
<content:encoded><![CDATA[
<div> benchmark-free evaluation, large language models, LLM-Crowdsourced, evaluation criteria, mathematics, programming

Summary:<br />
1. The study introduces a novel evaluation method, LLM-Crowdsourced, to overcome challenges in assessing large language models' capabilities.
2. Key evaluation criteria include dynamic, transparent, objective, and professional aspects, addressing limitations of existing methods.
3. Experiments on eight mainstream LLMs in mathematics and programming demonstrate the method's effectiveness in distinguishing performance.
4. Findings reveal Gemini excels in original question design, while some LLMs exhibit memorization-based answering.
5. Evaluation results show high consistency and robustness, highlighting the reliability of the LLM-Crowdsourced approach.

<br /><br />Summary: <div>
arXiv:2507.22359v1 Announce Type: cross 
Abstract: Although large language models (LLMs) demonstrate remarkable capabilities across various tasks, evaluating their capabilities remains a challenging task. Existing evaluation methods suffer from issues such as data contamination, black-box operation, and subjective preference. These issues make it difficult to evaluate the LLMs' true capabilities comprehensively. To tackle these challenges, we propose a novel benchmark-free evaluation paradigm, LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently, and evaluate mutually. This method integrates four key evaluation criteria: dynamic, transparent, objective, and professional, which existing evaluation methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs across mathematics and programming verify the advantages of our method in distinguishing LLM performance. Furthermore, our study reveals several novel findings that are difficult for traditional methods to detect, including but not limited to: (1) Gemini demonstrates the highest original and professional question-design capabilities among others; (2) Some LLMs exhibit ''memorization-based answering'' by misrecognizing questions as familiar ones with a similar structure; (3) LLM evaluation results demonstrate high consistency (robustness).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law</title>
<link>https://arxiv.org/abs/2507.22543</link>
<guid>https://arxiv.org/abs/2507.22543</guid>
<content:encoded><![CDATA[
<div> fundamental step, natural language processing, vocabulary size, token frequency distributions, Zipf's law
Summary: 
Tokenization is a key aspect in natural language processing and other sequence modeling fields, with vocabulary size playing a significant role in model performance. This study introduces a systematic approach to determining the optimal vocabulary size by analyzing token frequency distributions based on Zipf's law. The research demonstrates a correlation between downstream task performance and adherence to power-law behavior, highlighting the benefits of aligning with Zipfian scaling for enhancing model efficiency and effectiveness. Through extensive experiments in various domains such as NLP, genomics, and chemistry, the study establishes Zipfian alignment as a reliable and broadly applicable criterion for selecting vocabulary size. <div>
arXiv:2507.22543v1 Announce Type: cross 
Abstract: Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.22565</link>
<guid>https://arxiv.org/abs/2507.22565</guid>
<content:encoded><![CDATA[
<div> privacy, language models, differential privacy, stochastic gradient descent, reinforcement learning <br />
Summary:<br />
The article addresses the challenge of balancing data privacy and model utility when training large language models on sensitive datasets, particularly in healthcare. Traditional differentially private stochastic gradient descent (DP-SGD) methods suffer from a trade-off between privacy and accuracy due to gradient clipping and noise perturbation. The proposed framework, RLDP, leverages deep reinforcement learning to dynamically adjust per-parameter gradient-clipping thresholds and noise levels during model training. Through extensive experiments on various language models, RLDP demonstrates significant improvements in model performance, achieving up to 30.5% perplexity reduction and 5.6% downstream utility gain while maintaining privacy guarantees. The framework also shows faster convergence, requiring only a fraction of the gradient-update budget compared to baseline methods, and exhibits resilience against privacy attacks. RLDP presents a promising approach to enhancing the utility of language models while preserving data privacy. <br /> <div>
arXiv:2507.22565v1 Announce Type: cross 
Abstract: The tension between data privacy and model utility has become the defining bottleneck for the practical deployment of large language models (LLMs) trained on sensitive corpora including healthcare. Differentially private stochastic gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a pronounced cost: gradients are forcibly clipped and perturbed with noise, degrading sample efficiency and final accuracy. Numerous variants have been proposed to soften this trade-off, but they all share a handicap: their control knobs are hard-coded, global, and oblivious to the evolving optimization landscape. Consequently, practitioners are forced either to over-spend privacy budget in pursuit of utility, or to accept mediocre models in order to stay within privacy constraints. We present RLDP, the first framework to cast DP optimization itself as a closed-loop control problem amenable to modern deep reinforcement learning (RL). RLDP continuously senses rich statistics of the learning dynamics and acts by selecting fine-grained per parameter gradient-clipping thresholds as well as the magnitude of injected Gaussian noise. A soft actor-critic (SAC) hyper-policy is trained online during language model fine-tuning; it learns, from scratch, how to allocate the privacy budget where it matters and when it matters. Across more than 1,600 ablation experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream utility gain. RLDP reaches each baseline's final utility after only 13-43% of the gradient-update budget (mean speed-up 71%), all while honoring the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2507.22607</link>
<guid>https://arxiv.org/abs/2507.22607</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, multimodal reasoning, Progressive Curriculum Reinforcement Learning, VL-Cogito, reasoning abilities

Summary: 
VL-Cogito is a novel multimodal reasoning model trained through a Progressive Curriculum Reinforcement Learning (PCuRL) framework, enhancing reasoning abilities in diverse contexts. The model is guided through tasks of increasing difficulty, improving performance. The framework includes an online difficulty soft weighting mechanism for dynamic training difficulty adjustment and a dynamic length reward mechanism for adaptively regulating reasoning path length based on task complexity. Experimental evaluations demonstrate VL-Cogito's superiority over existing models in mathematics, science, logic, and general understanding tasks, showcasing its effectiveness in enhancing reasoning capabilities. <br /><br />Summary: <div>
arXiv:2507.22607v1 Announce Type: cross 
Abstract: Reinforcement learning has proven its effectiveness in enhancing the reasoning capabilities of large language models. Recent research efforts have progressively extended this paradigm to multimodal reasoning tasks. Due to the inherent complexity and diversity of multimodal tasks, especially in semantic content and problem formulations, existing models often exhibit unstable performance across various domains and difficulty levels. To address these limitations, we propose VL-Cogito, an advanced multimodal reasoning model trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework. PCuRL systematically guides the model through tasks of gradually increasing difficulty, substantially improving its reasoning abilities across diverse multimodal contexts. The framework introduces two key innovations: (1) an online difficulty soft weighting mechanism, dynamically adjusting training difficulty across successive RL training stages; and (2) a dynamic length reward mechanism, which encourages the model to adaptively regulate its reasoning path length according to task complexity, thus balancing reasoning efficiency with correctness. Experimental evaluations demonstrate that VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Tokens Denoising for Speech Synthesis</title>
<link>https://arxiv.org/abs/2507.22746</link>
<guid>https://arxiv.org/abs/2507.22746</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, autoregressive models, text-to-speech, Dragon-FM, zero-shot podcasts

Summary: 
Dragon-FM is a novel text-to-speech (TTS) model that combines the strengths of diffusion and autoregressive models. It processes audio codec tokens at a fast rate of 12.5 tokens per second, allowing for efficient generation of high-quality content. The model utilizes autoregressive modeling across chunks for global coherence and parallel flow-matching within chunks for fast denoising. By incorporating key-value caching and future context within each chunk, Dragon-FM addresses the limitations of both diffusion and autoregressive models. It also bridges the gap between continuous and discrete feature modeling, showcasing the ability to predict discrete tokens with finite scalar quantizers. The model's efficient codec and architecture make it particularly effective for generating extended content like zero-shot podcasts. Experiment demos on podcast datasets have shown the model's capability to generate high-quality content efficiently. 

<br /><br />Summary: <div>
arXiv:2507.22746v1 Announce Type: cross 
Abstract: While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact 12.5 tokens per second rate. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Consequently, the proposed model can utilize KV-cache across chunks and incorporate future context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also makes the proposed model particularly effective for generating extended content. Experiment for demos of our work} on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Incomplete Bridge: How AI Research (Mis)Engages with Psychology</title>
<link>https://arxiv.org/abs/2507.22847</link>
<guid>https://arxiv.org/abs/2507.22847</guid>
<content:encoded><![CDATA[
<div> cognition, artificial intelligence, psychology, interdisciplinary, research<br />
Summary:<br />
The study explores the interdisciplinary synergy between AI and psychology by analyzing LLM-related papers and psychology publications. Key patterns of interdisciplinary integration are identified, along with the most referenced psychology domains and underexplored areas. The operationalization and interpretation of psychology theories/frameworks in AI are examined, highlighting common types of misapplication. The study offers guidance for effective incorporation of psychology in AI systems, providing a comprehensive map of interdisciplinary engagement to facilitate deeper collaboration and advance AI research.<br /> <div>
arXiv:2507.22847v1 Announce Type: cross 
Abstract: Social sciences have accumulated a rich body of theories and methodologies for investigating the human mind and behaviors, while offering valuable insights into the design and understanding of Artificial Intelligence (AI) systems. Focusing on psychology as a prominent case, this study explores the interdisciplinary synergy between AI and the field by analyzing 1,006 LLM-related papers published in premier AI venues between 2023 and 2025, along with the 2,544 psychology publications they cite. Through our analysis, we identify key patterns of interdisciplinary integration, locate the psychology domains most frequently referenced, and highlight areas that remain underexplored. We further examine how psychology theories/frameworks are operationalized and interpreted, identify common types of misapplication, and offer guidance for more effective incorporation. Our work provides a comprehensive map of interdisciplinary engagement between AI and psychology, thereby facilitating deeper collaboration and advancing AI systems.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoOutageKG: A Multimodal Geospatiotemporal Knowledge Graph for Multiresolution Power Outage Analysis</title>
<link>https://arxiv.org/abs/2507.22878</link>
<guid>https://arxiv.org/abs/2507.22878</guid>
<content:encoded><![CDATA[
<div> nighttime light satellite data, power outages, grid risk assessment, disaster mitigation, multimodal data integration
<br />
Summary:
GeoOutageKG is a new multimodal knowledge graph designed for detecting, analyzing, and predicting power outages, particularly during extreme weather events like hurricanes. By combining diverse data sources such as nighttime light satellite images, spatiotemporal power outage maps, and county-level outage reports, GeoOutageKG provides a comprehensive and detailed representation of power outages in the United States. With over 10.6 million outage records, 300,000 NTL images, and 15,000 outage maps integrated into the knowledge graph, GeoOutageKG offers a robust platform for analyzing power outages at different spatial and temporal resolutions. Its modular and reusable structure enables efficient data integration and supports multiresolution analysis of geospatiotemporal power outages. <div>
arXiv:2507.22878v1 Announce Type: cross 
Abstract: Detecting, analyzing, and predicting power outages is crucial for grid risk assessment and disaster mitigation. Numerous outages occur each year, exacerbated by extreme weather events such as hurricanes. Existing outage data are typically reported at the county level, limiting their spatial resolution and making it difficult to capture localized patterns. However, it offers excellent temporal granularity. In contrast, nighttime light satellite image data provides significantly higher spatial resolution and enables a more comprehensive spatial depiction of outages, enhancing the accuracy of assessing the geographic extent and severity of power loss after disaster events. However, these satellite data are only available on a daily basis. Integrating spatiotemporal visual and time-series data sources into a unified knowledge representation can substantially improve power outage detection, analysis, and predictive reasoning. In this paper, we propose GeoOutageKG, a multimodal knowledge graph that integrates diverse data sources, including nighttime light satellite image data, high-resolution spatiotemporal power outage maps, and county-level timeseries outage reports in the U.S. We describe our method for constructing GeoOutageKG by aligning source data with a developed ontology, GeoOutageOnto. Currently, GeoOutageKG includes over 10.6 million individual outage records spanning from 2014 to 2024, 300,000 NTL images spanning from 2012 to 2024, and 15,000 outage maps. GeoOutageKG is a novel, modular and reusable semantic resource that enables robust multimodal data integration. We demonstrate its use through multiresolution analysis of geospatiotemporal power outages.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RecGPT Technical Report</title>
<link>https://arxiv.org/abs/2507.22879</link>
<guid>https://arxiv.org/abs/2507.22879</guid>
<content:encoded><![CDATA[
arXiv:2507.22879v1 Announce Type: cross 
Abstract: Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem.
  To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards the Law of Capacity Gap in Distilling Language Models</title>
<link>https://arxiv.org/abs/2311.07052</link>
<guid>https://arxiv.org/abs/2311.07052</guid>
<content:encoded><![CDATA[
arXiv:2311.07052v4 Announce Type: replace 
Abstract: Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the \textit{curse of capacity gap}, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the \textit{law of capacity gap} inducted from a preliminary study on distilling a broad range of small-scale (<3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</title>
<link>https://arxiv.org/abs/2408.16440</link>
<guid>https://arxiv.org/abs/2408.16440</guid>
<content:encoded><![CDATA[
arXiv:2408.16440v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Past Meets Present: Creating Historical Analogy with Large Language Models</title>
<link>https://arxiv.org/abs/2409.14820</link>
<guid>https://arxiv.org/abs/2409.14820</guid>
<content:encoded><![CDATA[
arXiv:2409.14820v2 Announce Type: replace 
Abstract: Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neutral Residues: Revisiting Adapters for Model Extension</title>
<link>https://arxiv.org/abs/2410.02744</link>
<guid>https://arxiv.org/abs/2410.02744</guid>
<content:encoded><![CDATA[
arXiv:2410.02744v2 Announce Type: replace 
Abstract: We address the problem of extending a pretrained large language model to a new domain that was not seen during training. Standard techniques, such as finetuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we revisit and improve adapters to extend LLMs from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperform competing approaches such as finetuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges</title>
<link>https://arxiv.org/abs/2410.21306</link>
<guid>https://arxiv.org/abs/2410.21306</guid>
<content:encoded><![CDATA[
arXiv:2410.21306v3 Announce Type: replace 
Abstract: Natural Language Processing (NLP) is revolutionising the way both professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational assistance tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 131 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document lengths, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legal text, such as Document Summarisation, Named Entity Recognition, Question Answering, Argument Mining, Text Classification, and Judgement Prediction. Furthermore, we analyse both developed legal-oriented language models, and approaches for adapting general-purpose language models to the legal domain. Additionally, we identify sixteen open research challenges, including the detection and mitigation of bias in artificial intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yankari: A Monolingual Yoruba Dataset</title>
<link>https://arxiv.org/abs/2412.03334</link>
<guid>https://arxiv.org/abs/2412.03334</guid>
<content:encoded><![CDATA[
arXiv:2412.03334v2 Announce Type: replace 
Abstract: This paper presents Yankari, a large-scale monolingual dataset for the Yoruba language, aimed at addressing the critical gap in Natural Language Processing (NLP) resources for this important West African language. Despite being spoken by over 30 million people, Yoruba has been severely underrepresented in NLP research and applications. We detail our methodology for creating this dataset, which includes careful source selection, automated quality control, and rigorous data cleaning processes. The Yankari dataset comprises 51,407 documents from 13 diverse sources, totaling over 30 million tokens. Our approach focuses on ethical data collection practices, avoiding problematic sources and addressing issues prevalent in existing datasets. We provide thorough automated evaluations of the dataset, demonstrating its quality compared to existing resources. The Yankari dataset represents a significant advancement in Yoruba language resources, providing a foundation for developing more accurate NLP models, supporting comparative linguistic studies, and contributing to the digital accessibility of the Yoruba language.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck</title>
<link>https://arxiv.org/abs/2412.08528</link>
<guid>https://arxiv.org/abs/2412.08528</guid>
<content:encoded><![CDATA[
arXiv:2412.08528v2 Announce Type: replace 
Abstract: Continual learning remains a challenge across various natural language processing (NLP) tasks, as models updated with new training data often risk catastrophic forgetting of previously acquired knowledge. We introduce a discrete key-value bottleneck (DKVB) for encoder-only language models, enabling efficient continual learning through localized updates. Inspired by a discrete key-value bottleneck in vision, we consider new and NLP-specific challenges. We compare different bottleneck architectures for NLP and introduce a new, task-independent initialization technique for the discrete keys. We evaluate our DKVB for NLP in four continual learning scenarios and show that it alleviates catastrophic forgetting. Our experiments demonstrate that the proposed approach achieves competitive performance compared to popular continual learning methods while incurring lower computational costs. Furthermore, we show that DKVB remains effective even in challenging single-head continual learning scenarios where no task ID is provided.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling</title>
<link>https://arxiv.org/abs/2412.14373</link>
<guid>https://arxiv.org/abs/2412.14373</guid>
<content:encoded><![CDATA[
arXiv:2412.14373v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated exceptional versatility across domains, including applications to electrocardiograms (ECGs). A growing body of work focuses on generating text from multi-channeled ECG signals and corresponding textual prompts. Existing approaches often involve a two-stage process: pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective, followed by finetuning an LLM for natural language generation (NLG) using encoder-derived features. However, these methods face two key limitations: inefficiency due to multi-stage training and challenges in interpreting encoder-generated features. To overcome these issues, we propose ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG signals into tokens, enabling direct end-to-end LLM training by combining ECG and text tokens. This approach enhances interpretability, as ECG tokens can be directly mapped back to the original signals. Leveraging ECG-Byte, we achieve competitive NLG performance while training 3 times faster and using just 48\% of the data required by traditional two-stage methods.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
<link>https://arxiv.org/abs/2412.15239</link>
<guid>https://arxiv.org/abs/2412.15239</guid>
<content:encoded><![CDATA[
arXiv:2412.15239v3 Announce Type: replace 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationale-guided Prompting for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2412.16936</link>
<guid>https://arxiv.org/abs/2412.16936</guid>
<content:encoded><![CDATA[
arXiv:2412.16936v2 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training</title>
<link>https://arxiv.org/abs/2501.09213</link>
<guid>https://arxiv.org/abs/2501.09213</guid>
<content:encoded><![CDATA[
arXiv:2501.09213v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have shown promise in medical applications such as disease diagnosis and treatment planning. However, most existing medical LLMs struggle with the deep reasoning required for complex medical problems, such as differential diagnosis and medication recommendations. We propose FineMedLM-o1, which leverages high-quality medical synthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and deep reasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in the medical domain for the first time, facilitating domain adaptation and ensuring reliable, accurate reasoning. Experimental results demonstrate that FineMedLM-o1 achieves a 23% average performance improvement over prior models on key medical benchmarks. Furthermore, the introduction of TTT provides an additional 14% performance boost, highlighting its effectiveness in enhancing medical reasoning capabilities. To support this process, we also propose a novel method for synthesizing medical dialogue. Compared to other open-source datasets, our dataset stands out as superior in both quality and complexity. The project and data will be released on GitHub.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GneissWeb: Preparing High Quality Data for LLMs at Scale</title>
<link>https://arxiv.org/abs/2502.14907</link>
<guid>https://arxiv.org/abs/2502.14907</guid>
<content:encoded><![CDATA[
arXiv:2502.14907v2 Announce Type: replace 
Abstract: Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.
  In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).
  We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QE4PE: Word-level Quality Estimation for Human Post-Editing</title>
<link>https://arxiv.org/abs/2503.03044</link>
<guid>https://arxiv.org/abs/2503.03044</guid>
<content:encoded><![CDATA[
arXiv:2503.03044v2 Announce Type: replace 
Abstract: Word-level quality estimation (QE) methods aim to detect erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. In this study, we investigate the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated from behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Modal State-Space Graph Reasoning for Structured Summarization</title>
<link>https://arxiv.org/abs/2503.20988</link>
<guid>https://arxiv.org/abs/2503.20988</guid>
<content:encoded><![CDATA[
arXiv:2503.20988v2 Announce Type: replace 
Abstract: The ability to extract compact, meaningful summaries from large-scale and multimodal data is critical for numerous applications, ranging from video analytics to medical reports. Prior methods in cross-modal summarization have often suffered from high computational overheads and limited interpretability. In this paper, we propose a \textit{Cross-Modal State-Space Graph Reasoning} (\textbf{CSS-GR}) framework that incorporates a state-space model with graph-based message passing, inspired by prior work on efficient state-space models. Unlike existing approaches relying on purely sequential models, our method constructs a graph that captures inter- and intra-modal relationships, allowing more holistic reasoning over both textual and visual streams. We demonstrate that our approach significantly improves summarization quality and interpretability while maintaining computational efficiency, as validated on standard multimodal summarization benchmarks. We also provide a thorough ablation study to highlight the contributions of each component.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing</title>
<link>https://arxiv.org/abs/2504.01282</link>
<guid>https://arxiv.org/abs/2504.01282</guid>
<content:encoded><![CDATA[
arXiv:2504.01282v2 Announce Type: replace 
Abstract: While the inconsistency of LLMs is not a novel topic, prior research has predominantly addressed two types of generative inconsistencies: i) Randomness Inconsistency: running the same LLM multiple trials, yielding varying responses; ii) Paraphrase Inconsistency: paraphrased prompts result in different responses from the same LLM. Randomness Inconsistency arises from the inherent randomness due to stochastic sampling in generative models, while Paraphrase Inconsistency is a consequence of the language modeling objectives, where paraphrased prompts alter the distribution of vocabulary logits. This research discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM self-inconsistency: given a question and a couple of LLM-generated answer candidates, the LLM often has conflicting responses when prompted "Which are correct answers?" and "Which are incorrect answers?". PRIN poses a big concern as it undermines the credibility of LLM-as-a-judge, and suggests a challenge for LLMs to adhere to basic logical rules. We conduct a series of experiments to investigate PRIN, examining the extent of PRIN across different LLMs, methods to mitigate it, potential applications, and its relationship with Randomness Inconsistency and Paraphrase Inconsistency. As the first study to explore PRIN, our findings offer valuable insights into the inner workings of LLMs and contribute to advancing trustworthy AI.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears</title>
<link>https://arxiv.org/abs/2504.05008</link>
<guid>https://arxiv.org/abs/2504.05008</guid>
<content:encoded><![CDATA[
arXiv:2504.05008v2 Announce Type: replace 
Abstract: The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2504.11829</link>
<guid>https://arxiv.org/abs/2504.11829</guid>
<content:encoded><![CDATA[
arXiv:2504.11829v3 Announce Type: replace 
Abstract: Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition</title>
<link>https://arxiv.org/abs/2505.00059</link>
<guid>https://arxiv.org/abs/2505.00059</guid>
<content:encoded><![CDATA[
arXiv:2505.00059v2 Announce Type: replace 
Abstract: Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.08450</link>
<guid>https://arxiv.org/abs/2505.08450</guid>
<content:encoded><![CDATA[
arXiv:2505.08450v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents. However, real-world applications demand not only accuracy but also interpretability. While dense retrieval methods provide high accuracy, they lack interpretability; conversely, sparse retrieval methods offer transparency but often fail to capture the full intent of queries due to their reliance on keyword matching. To address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. IterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords. Across four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval-based RAG and prior iterative query refinement methods using dense models. In summary, IterKey is a novel BM25-based approach leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with interpretability.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization</title>
<link>https://arxiv.org/abs/2505.12474</link>
<guid>https://arxiv.org/abs/2505.12474</guid>
<content:encoded><![CDATA[
arXiv:2505.12474v2 Announce Type: replace 
Abstract: Traditional dialogue summarization primarily focuses on dialogue content, assuming it comprises adequate information for a clear summary. However, this assumption often fails for discussions grounded in shared background, where participants frequently omit context and use implicit references. This results in summaries that are confusing to readers unfamiliar with the background. To address this, we introduce Knowledge-Grounded Discussion Summarization (KGDS), a novel task that produces a supplementary background summary for context and a clear opinion summary with clarified references. To facilitate research, we construct the first KGDS benchmark, featuring news-discussion pairs and expert-created multi-granularity gold annotations for evaluating sub-summaries. We also propose a novel hierarchical evaluation framework with fine-grained and interpretable metrics. Our extensive evaluation of 12 advanced large language models (LLMs) reveals that KGDS remains a significant challenge. The models frequently miss key facts and retain irrelevant ones in background summarization, and often fail to resolve implicit references in opinion summary integration.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering</title>
<link>https://arxiv.org/abs/2505.15038</link>
<guid>https://arxiv.org/abs/2505.15038</guid>
<content:encoded><![CDATA[
arXiv:2505.15038v2 Announce Type: replace 
Abstract: Linear concept vectors effectively steer LLMs, but existing methods suffer from noisy features in diverse datasets that undermine steering robustness. We propose Sparse Autoencoder-Denoised Concept Vectors (SDCV), which selectively keep the most discriminative SAE latents while reconstructing hidden representations. Our key insight is that concept-relevant signals can be explicitly separated from dataset noise by scaling up activations of top-k latents that best differentiate positive and negative samples. Applied to linear probing and difference-in-mean, SDCV consistently improves steering success rates by 4-16\% across six challenging concepts, while maintaining topic relevance.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19959</link>
<guid>https://arxiv.org/abs/2505.19959</guid>
<content:encoded><![CDATA[
arXiv:2505.19959v2 Announce Type: replace 
Abstract: Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.21354</link>
<guid>https://arxiv.org/abs/2505.21354</guid>
<content:encoded><![CDATA[
arXiv:2505.21354v2 Announce Type: replace 
Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSciClaims: Multimodal Scientific Claim Verification</title>
<link>https://arxiv.org/abs/2506.04585</link>
<guid>https://arxiv.org/abs/2506.04585</guid>
<content:encoded><![CDATA[
arXiv:2506.04585v2 Announce Type: replace 
Abstract: Assessing scientific claims requires identifying, extracting, and reasoning with multimodal data expressed in information-rich figures in scientific literature. Despite the large body of work in scientific QA, figure captioning, and other multimodal reasoning tasks over chart-based data, there are no readily usable multimodal benchmarks that directly test claim verification abilities. To remedy this gap, we introduce a new benchmark MuSciClaims accompanied by diagnostics tasks. We automatically extract supported claims from scientific articles, which we manually perturb to produce contradicted claims. The perturbations are designed to test for a specific set of claim verification capabilities. We also introduce a suite of diagnostic tasks that help understand model failures. Our results show most vision-language models are poor (~0.3-0.5 F1), with even the best model only achieving 0.72 F1. They are also biased towards judging claims as supported, likely misunderstanding nuanced perturbations within the claims. Our diagnostics show models are bad at localizing correct evidence within figures, struggle with aggregating information across modalities, and often fail to understand basic components of the figure.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-qualitative-judge: automating error analysis in natural language generation</title>
<link>https://arxiv.org/abs/2506.09147</link>
<guid>https://arxiv.org/abs/2506.09147</guid>
<content:encoded><![CDATA[
arXiv:2506.09147v2 Announce Type: replace 
Abstract: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3 cases and is capable of producing error type reports resembling the reports composed by human annotators. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations</title>
<link>https://arxiv.org/abs/2506.19073</link>
<guid>https://arxiv.org/abs/2506.19073</guid>
<content:encoded><![CDATA[
arXiv:2506.19073v2 Announce Type: replace 
Abstract: Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions</title>
<link>https://arxiv.org/abs/2404.07214</link>
<guid>https://arxiv.org/abs/2404.07214</guid>
<content:encoded><![CDATA[
arXiv:2404.07214v3 Announce Type: replace-cross 
Abstract: The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mmm whatcha say? Uncovering distal and proximal context effects in first and second-language word perception using psychophysical reverse correlation</title>
<link>https://arxiv.org/abs/2406.05515</link>
<guid>https://arxiv.org/abs/2406.05515</guid>
<content:encoded><![CDATA[
arXiv:2406.05515v2 Announce Type: replace-cross 
Abstract: Acoustic context effects, where surrounding changes in pitch, rate or timbre influence the perception of a sound, are well documented in speech perception, but how they interact with language background remains unclear. Using a reverse-correlation approach, we systematically varied the pitch and speech rate in phrases around different pairs of vowels for second language (L2) speakers of English (/i/-/I/) and French (/u/-/y/), thus reconstructing, in a data-driven manner, the prosodic profiles that bias their perception. Testing English and French speakers (n=25), we showed that vowel perception is in fact influenced by conflicting effects from the surrounding pitch and speech rate: a congruent proximal effect 0.2s pre-target and a distal contrastive effect up to 1s before; and found that L1 and L2 speakers exhibited strikingly similar prosodic profiles in perception. We provide a novel method to investigate acoustic context effects across stimuli, timescales, and acoustic domain.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training</title>
<link>https://arxiv.org/abs/2410.07336</link>
<guid>https://arxiv.org/abs/2410.07336</guid>
<content:encoded><![CDATA[
arXiv:2410.07336v2 Announce Type: replace-cross 
Abstract: Despite significant advancements in caption generation, existing evaluation metrics often fail to capture the full quality or fine-grained details of captions. This is mainly due to their reliance on non-specific human-written references or noisy pre-training data. Still, finding an effective metric is crucial not only for captions evaluation but also for the generation phase. Metrics can indeed play a key role in the fine-tuning stage of captioning models, ultimately enhancing the quality of the generated captions. In this paper, we propose PAC-S++, a learnable metric that leverages the CLIP model, pre-trained on both web-collected and cleaned data and regularized through additional pairs of generated visual and textual positive samples. Exploiting this stronger and curated pre-training, we also apply PAC-S++ as a reward in the Self-Critical Sequence Training (SCST) stage typically employed to fine-tune captioning models. Extensive experiments on different image and video datasets highlight the effectiveness of PAC-S++ compared to popular metrics for the task, including its sensitivity to object hallucinations. Furthermore, we show that integrating PAC-S++ into the fine-tuning stage of a captioning model results in semantically richer captions with fewer repetitions and grammatical errors. Evaluations on out-of-domain benchmarks further demonstrate the efficacy of our fine-tuning approach in enhancing model capabilities. Source code and trained models are publicly available at: https://github.com/aimagelab/pacscore.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can adversarial attacks by large language models be attributed?</title>
<link>https://arxiv.org/abs/2411.08003</link>
<guid>https://arxiv.org/abs/2411.08003</guid>
<content:encoded><![CDATA[
arXiv:2411.08003v3 Announce Type: replace-cross 
Abstract: Attributing outputs from Large Language Models (LLMs) in adversarial settings-such as cyberattacks and disinformation campaigns-presents significant challenges that are likely to grow in importance. We approach this attribution problem from both a theoretical and an empirical perspective, drawing on formal language theory (identification in the limit) and data-driven analysis of the expanding LLM ecosystem. By modeling an LLM's set of possible outputs as a formal language, we analyze whether finite samples of text can uniquely pinpoint the originating model. Our results show that, under mild assumptions of overlapping capabilities among models, certain classes of LLMs are fundamentally non-identifiable from their outputs alone. We delineate four regimes of theoretical identifiability: (1) an infinite class of deterministic (discrete) LLM languages is not identifiable (Gold's classical result from 1967); (2) an infinite class of probabilistic LLMs is also not identifiable (by extension of the deterministic case); (3) a finite class of deterministic LLMs is identifiable (consistent with Angluin's tell-tale criterion); and (4) even a finite class of probabilistic LLMs can be non-identifiable (we provide a new counterexample establishing this negative result). Complementing these theoretical insights, we quantify the explosion in the number of plausible model origins (hypothesis space) for a given output in recent years. Even under conservative assumptions-each open-source model fine-tuned on at most one new dataset-the count of distinct candidate models doubles approximately every 0.5 years, and allowing multi-dataset fine-tuning combinations yields doubling times as short as 0.28 years. This combinatorial growth, alongside the extraordinary computational cost of brute-force likelihood attribution across all models and potential users, renders exhaustive attribution infeasible in practice.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning</title>
<link>https://arxiv.org/abs/2502.13820</link>
<guid>https://arxiv.org/abs/2502.13820</guid>
<content:encoded><![CDATA[
arXiv:2502.13820v3 Announce Type: replace-cross 
Abstract: Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OWLViz: An Open-World Benchmark for Visual Question Answering</title>
<link>https://arxiv.org/abs/2503.07631</link>
<guid>https://arxiv.org/abs/2503.07631</guid>
<content:encoded><![CDATA[
arXiv:2503.07631v3 Announce Type: replace-cross 
Abstract: We present a challenging benchmark for the Open WorLd VISual question answering (OWLViz) task. OWLViz presents concise, unambiguous queries that require integrating multiple capabilities, including visual understanding, web exploration, and specialized tool usage. While humans achieve 69.2% accuracy on these intuitive tasks, even state-of-the-art VLMs struggle, with the best model, Gemini 2.0, achieving only 26.6% accuracy. Current agentic VLMs, which rely on limited vision and vision-language models as tools, perform even worse. This performance gap reveals significant limitations in multimodal systems' ability to select appropriate tools and execute complex reasoning sequences, establishing new directions for advancing practical AI research.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReverBERT: A State Space Model for Efficient Text-Driven Speech Style Transfer</title>
<link>https://arxiv.org/abs/2503.20992</link>
<guid>https://arxiv.org/abs/2503.20992</guid>
<content:encoded><![CDATA[
arXiv:2503.20992v2 Announce Type: replace-cross 
Abstract: Text-driven speech style transfer aims to mold the intonation, pace, and timbre of a spoken utterance to match stylistic cues from text descriptions. While existing methods leverage large-scale neural architectures or pre-trained language models, the computational costs often remain high. In this paper, we present \emph{ReverBERT}, an efficient framework for text-driven speech style transfer that draws inspiration from a state space model (SSM) paradigm, loosely motivated by the image-based method of Wang and Liu~\cite{wang2024stylemamba}. Unlike image domain techniques, our method operates in the speech space and integrates a discrete Fourier transform of latent speech features to enable smooth and continuous style modulation. We also propose a novel \emph{Transformer-based SSM} layer for bridging textual style descriptors with acoustic attributes, dramatically reducing inference time while preserving high-quality speech characteristics. Extensive experiments on benchmark speech corpora demonstrate that \emph{ReverBERT} significantly outperforms baselines in terms of naturalness, expressiveness, and computational efficiency. We release our model and code publicly to foster further research in text-driven speech style transfer.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis</title>
<link>https://arxiv.org/abs/2504.11257</link>
<guid>https://arxiv.org/abs/2504.11257</guid>
<content:encoded><![CDATA[
arXiv:2504.11257v4 Announce Type: replace-cross 
Abstract: Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining</title>
<link>https://arxiv.org/abs/2504.13932</link>
<guid>https://arxiv.org/abs/2504.13932</guid>
<content:encoded><![CDATA[
arXiv:2504.13932v3 Announce Type: replace-cross 
Abstract: The growing use of large language models has raised environmental and economic concerns about their intensity of resource usage during inference. Serving these models to each user requires substantial energy and water for cooling. Model compression techniques like quantization can shrink large language models and make them more resource efficient at the cost of potential performance degradation. Quantization methods compress model size through replacing their high-precision parameters by quantized values of lower precision. Among existing methods, the ApiQ method achieves superior accuracy preservation at minimal memory and time overhead. We investigate two ideas to extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we look into combining existing quantization-aware training techniques with ApiQ's partial training. We show that this does not outperform the baseline ApiQ method with limited training data and frozen weights. This leads to two key insights: (1) The substantial representational capacity that is gained through full retraining is unlikely to be feasible through partial training. (2) This gain may depend on using a large and diverse dataset in quantization-aware training. Second, through a novel approach informed by the two insights, we propose an ultra-low-bit quantization method that builds upon ApiQ and extends its performance without the need for full retraining. This publicly available method relies on a saliency-aware regularization term that prioritizes preserving the most impactful parameters during quantization. Our experiments on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's accuracy degradation by 10.85% and 7.54% respectively. A Python implementation of the proposed quantization method is publicly available on GitHub https://github.com/TokuyuSou/ULB-SAPR.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection</title>
<link>https://arxiv.org/abs/2505.19010</link>
<guid>https://arxiv.org/abs/2505.19010</guid>
<content:encoded><![CDATA[
arXiv:2505.19010v2 Announce Type: replace-cross 
Abstract: Multi-modal learning has emerged as a crucial research direction, as integrating textual and visual information can substantially enhance performance in tasks such as classification, retrieval, and scene understanding. Despite advances with large pre-trained models, existing approaches often suffer from insufficient cross-modal interactions and rigid fusion strategies, failing to fully harness the complementary strengths of different modalities. To address these limitations, we propose Co-AttenDWG, co-attention with dimension-wise gating, and expert fusion. Our approach first projects textual and visual features into a shared embedding space, where a dedicated co-attention mechanism enables simultaneous, fine-grained interactions between modalities. This is further strengthened by a dimension-wise gating network, which adaptively modulates feature contributions at the channel level to emphasize salient information. In parallel, dual-path encoders independently refine modality-specific representations, while an additional cross-attention layer aligns the modalities further. The resulting features are aggregated via an expert fusion module that integrates learned gating and self-attention, yielding a robust unified representation. Experimental results on the MIMIC and SemEval Memotion 1.0 datasets show that Co-AttenDWG achieves state-of-the-art performance and superior cross-modal alignment, highlighting its effectiveness for diverse multi-modal applications.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masked Language Models are Good Heterogeneous Graph Generalizers</title>
<link>https://arxiv.org/abs/2506.06157</link>
<guid>https://arxiv.org/abs/2506.06157</guid>
<content:encoded><![CDATA[
arXiv:2506.06157v2 Announce Type: replace-cross 
Abstract: Heterogeneous graph neural networks (HGNNs) excel at capturing structural and semantic information in heterogeneous graphs (HGs), while struggling to generalize across domains and tasks. With the rapid advancement of large language models (LLMs), a recent study explored the integration of HGNNs with LLMs for generalizable heterogeneous graph learning. However, this approach typically encodes structural information as HG tokens using HGNNs, and disparities in embedding spaces between HGNNs and LLMs have been shown to bias the LLM's comprehension of HGs. Moreover, since these HG tokens are often derived from node-level tasks, the model's ability to generalize across tasks remains limited. To this end, we propose a simple yet effective Masked Language Modeling-based method, called MLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens to extract structural and semantic information inherent in HGs, and designs customized textual templates to unify different graph tasks into a coherent cloze-style 'mask' token prediction paradigm. Specifically,MLM4HG first converts HGs from various domains to texts based on metapaths, and subsequently combines them with the unified task texts to form a HG-based corpus. Moreover, the corpus is fed into a pretrained LM for fine-tuning with a constrained target vocabulary, enabling the fine-tuned LM to generalize to unseen target HGs. Extensive cross-domain and multi-task experiments on four real-world datasets demonstrate the superior generalization performance of MLM4HG over state-of-the-art methods in both few-shot and zero-shot scenarios. Our code is available at https://github.com/BUPT-GAMMA/MLM4HG.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence</title>
<link>https://arxiv.org/abs/2506.15677</link>
<guid>https://arxiv.org/abs/2506.15677</guid>
<content:encoded><![CDATA[
arXiv:2506.15677v3 Announce Type: replace-cross 
Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs</title>
<link>https://arxiv.org/abs/2507.07610</link>
<guid>https://arxiv.org/abs/2507.07610</guid>
<content:encoded><![CDATA[
arXiv:2507.07610v3 Announce Type: replace-cross 
Abstract: Humans can directly imagine and manipulate visual images in their minds, a capability known as spatial visualization. While multi-modal Large Language Models (MLLMs) support imagination-based reasoning, spatial visualization remains insufficiently evaluated, typically embedded within broader mathematical and logical assessments. Existing evaluations often rely on IQ tests or math competitions that may overlap with training data, compromising assessment reliability. To this end, we introduce SpatialViz-Bench, a comprehensive multi-modal benchmark for spatial visualization with 12 tasks across 4 sub-abilities, comprising 1,180 automatically generated problems. Our evaluation of 33 state-of-the-art MLLMs not only reveals wide performance variations and demonstrates the benchmark's strong discriminative power, but also uncovers counter-intuitive findings: models show difficulty perception misaligned with human intuition, exhibit dramatic 2Dto-3D performance cliffs, default to formulaic derivation over visualization, and paradoxically suffer performance degradation from Chain-of-Thought prompting in open-source models. Through statistical and qualitative analysis of error types, SpatialViz-Bench demonstrates that state-of-the-art MLLMs continue to exhibit deficiencies in spatial visualization tasks, thereby addressing a significant lacuna in the field. The benchmark data and evaluation code are publicly available.
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</title>
<link>https://arxiv.org/abs/2507.08771</link>
<guid>https://arxiv.org/abs/2507.08771</guid>
<content:encoded><![CDATA[
arXiv:2507.08771v2 Announce Type: replace-cross 
Abstract: To alleviate the computational burden of large language models (LLMs), architectures with activation sparsity, represented by mixture-of-experts (MoE), have attracted increasing attention. However, the non-differentiable and inflexible routing of vanilla MoE hurts model performance. Moreover, while each token activates only a few parameters, these sparsely-activated architectures exhibit low chunk-level sparsity, indicating that the union of multiple consecutive tokens activates a large ratio of parameters. Such a sparsity pattern is unfriendly for acceleration under low-resource conditions (e.g., end-side devices) and incompatible with mainstream acceleration techniques (e.g., speculative decoding). To address these challenges, we introduce a novel MoE architecture, BlockFFN, as well as its efficient training and deployment techniques. Specifically, we use a router integrating ReLU activation and RMSNorm for differentiable and flexible routing. Next, to promote both token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training objectives are designed, making BlockFFN more acceleration-friendly. Finally, we implement efficient acceleration kernels, combining activation sparsity and speculative decoding for the first time. The experimental results demonstrate the superior performance of BlockFFN over other MoE baselines, achieving over 80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on real end-side devices than dense models. All codes and checkpoints are available publicly (https://github.com/thunlp/BlockFFN).
]]></content:encoded>
<pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Categorical Classification of Book Summaries Using Word Embedding Techniques</title>
<link>https://arxiv.org/abs/2507.21058</link>
<guid>https://arxiv.org/abs/2507.21058</guid>
<content:encoded><![CDATA[
<div> Word Embedding, Book Summaries, Categories, Natural Language Processing, Machine Learning <br />
Summary: This study focused on the classification of book summaries and categories from book sites using various word embedding methods and machine learning algorithms. The research compared the success of one-hot encoding, Word2Vec, and TF-IDF methods for word embedding. The study also included a combination table of pre-processing methods. The results indicated that Support Vector Machine, Naive Bayes, and Logistic Regression Models, along with TF-IDF and One-Hot Encoder word embedding techniques, were more successful for Turkish texts. The study provides valuable insights into the effective methods for categorizing and summarizing book information using natural language processing techniques. <div>
arXiv:2507.21058v1 Announce Type: new 
Abstract: In this study, book summaries and categories taken from book sites were classified using word embedding methods, natural language processing techniques and machine learning algorithms. In addition, one hot encoding, Word2Vec and Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are frequently used word embedding methods were used in this study and their success was compared. Additionally, the combination table of the pre-processing methods used is shown and added to the table. Looking at the results, it was observed that Support Vector Machine, Naive Bayes and Logistic Regression Models and TF-IDF and One-Hot Encoder word embedding techniques gave more successful results for Turkish texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions</title>
<link>https://arxiv.org/abs/2507.21065</link>
<guid>https://arxiv.org/abs/2507.21065</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, AI Social Gym, Sociocultural Theory, Pedagogical Strategies, Knowledge Acquisition

Summary:
The study investigates the use of socially mediated learning paradigms to enhance the knowledge acquisition process of Large Language Models (LLMs). Inspired by Vygotsky's sociocultural theory, the researchers created the 'AI Social Gym', where an AI learner agent engages in pedagogical dialogues with knowledgeable AI teacher agents. The focus is on how different pedagogical strategies impact the AI learning process, particularly in ontology acquisition. The results show that dialogic approaches, including mixed-direction interactions, significantly improve the LLM's ability to acquire and apply new knowledge compared to traditional instructional methods and structured datasets. By integrating pedagogical and psychological insights into AI and robot training, the study highlights the potential for enhancing post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering. 

Summary:<br /><br />Keywords: Large Language Models, AI Social Gym, Sociocultural Theory, Pedagogical Strategies, Knowledge Acquisition <div>
arXiv:2507.21065v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing</title>
<link>https://arxiv.org/abs/2507.21073</link>
<guid>https://arxiv.org/abs/2507.21073</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, EFL writing, editing behaviors, expository writing, secondary students

Summary: 
This research examines the impact of using AI-generated text in English as a foreign language (EFL) writing contexts. The study focused on how secondary students in Hong Kong edit AI-generated text in their expository writing process and compositions. The findings revealed two main editing patterns among students: refining introductory units repeatedly and quickly shifting to extensive edits in body units. Despite significant editing efforts, the study showed only limited improvement in composition quality, indicating that AI supports but does not replace writing skills. The results suggest the importance of genre-specific instruction and process-focused writing before integrating AI. Educators should develop assessments valuing both the writing process and product to encourage critical engagement with AI text. <div>
arXiv:2507.21073v1 Announce Type: new 
Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used in English as a foreign language (EFL) writing contexts, yet its impact on students' expository writing process and compositions remains understudied. This research examines how EFL secondary students edit AI-generated text. Exploring editing behaviors in their expository writing process and in expository compositions, and their effect on human-rated scores for content, organization, language, and overall quality. Participants were 39 Hong Kong secondary students who wrote an expository composition with AI chatbots in a workshop. A convergent design was employed to analyze their screen recordings and compositions to examine students' editing behaviors and writing qualities. Analytical methods included qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis. We analyzed over 260 edits per dataset, and identified two editing patterns: one where students refined introductory units repeatedly before progressing, and another where they quickly shifted to extensive edits in body units (e.g., topic and supporting sentences). MLR analyses revealed that the number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. These results suggest a disconnect between students' significant editing effort and improved composition quality, indicating AI supports but does not replace writing skills. The findings highlight the importance of genre-specific instruction and process-focused writing before AI integration. Educators should also develop assessments valuing both process and product to encourage critical engagement with AI text.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which symbol grounding problem should we try to solve?</title>
<link>https://arxiv.org/abs/2507.21080</link>
<guid>https://arxiv.org/abs/2507.21080</guid>
<content:encoded><![CDATA[
<div> Keywords: Floridi, Taddeo, grounding problem, computing, artificial computational agents

Summary:
Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, but this condition cannot be fulfilled, not even by their own solution. Luc Steels presents a different suggestion, highlighting the need to rethink the problem's formulation and the role of goals in a system. By understanding computing, it becomes apparent that the only sensible grounding problem concerns explaining and reproducing the behavioral ability and function of meaning in artificial computational agents. This suggests a shift in focus towards understanding how meaning can be preserved and replicated within computational systems. <br /><br />Summary: <div>
arXiv:2507.21080v1 Announce Type: new 
Abstract: Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</title>
<link>https://arxiv.org/abs/2507.21083</link>
<guid>https://arxiv.org/abs/2507.21083</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, emotional tone, response bias, tone floor, AI alignment <br />
Summary: 
Large Language Models such as GPT-4 adjust their responses based on the emotional tone of prompts in addition to the actual question. The study systematically varied the emotional tone of 156 prompts across different topics and analyzed the impact on model responses. It was found that GPT-4 is more likely to respond neutrally or positively to negatively framed questions, indicating a "rebound" bias. This bias is even more pronounced in sensitive topics like justice and politics, where tone-based variation is suppressed, suggesting an alignment override. The concept of a "tone floor" was introduced as a lower bound in response negativity, and tone-valence transition matrices were used to quantify the model's behavior. Visualizations based on embeddings confirmed semantic drift based on emotional tone. These findings highlight biases driven by emotional framing in prompts, with implications for AI alignment and trust. <div>
arXiv:2507.21083v1 Announce Type: new 
Abstract: Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing</title>
<link>https://arxiv.org/abs/2507.21084</link>
<guid>https://arxiv.org/abs/2507.21084</guid>
<content:encoded><![CDATA[
<div> framework, side effects, large language models, fine-tuning, model diffing
Summary:
The article introduces MNEME, a framework for identifying unintended side effects in large language models (LLMs) that occur during fine-tuning or unlearning processes. MNEME uses sparse model diffing to compare base and fine-tuned models on task-agnostic data, enabling the detection of behavioral shifts without access to fine-tuning data. The framework was tested on five LLMs across various scenarios, achieving up to 95 percent accuracy in predicting side effects. It successfully identified issues such as knowledge unlearning, emergent misalignment, and benign fine-tuning effects without the need for custom heuristics. The study also found that retraining on high-activation samples could partially reverse these effects. The results highlight the scalability and automation of MNEME in understanding and managing changes in LLM behavior brought about by fine-tuning processes. 

<br /><br />Summary: <div>
arXiv:2507.21084v1 Announce Type: new 
Abstract: Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Amateur Contrastive Decoding for Text Generation</title>
<link>https://arxiv.org/abs/2507.21086</link>
<guid>https://arxiv.org/abs/2507.21086</guid>
<content:encoded><![CDATA[
<div> ensemble, amateur models, generation patterns, multi-amateur contrastive decoding, controllable generation <br />
Summary: 
Multi-Amateur Contrastive Decoding (MACD) is proposed as an enhancement to Contrastive Decoding (CD) for text generation. By utilizing an ensemble of amateur models, MACD is able to address a wider range of undesirable generation patterns such as repetition and hallucination. Through averaging and consensus mechanisms, MACD improves fluency, coherence, diversity, and adaptability in text generation across various domains. The framework also allows for controllable generation by incorporating amateurs with specific biases. Experimental results show that MACD outperforms conventional decoding methods and the original CD approach in terms of performance metrics, without the need for additional training. <div>
arXiv:2507.21086v1 Announce Type: new 
Abstract: Contrastive Decoding (CD) has emerged as an effective inference-time strategy for enhancing open-ended text generation by exploiting the divergence in output probabilities between a large expert language model and a smaller amateur model. Although CD improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases. Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning</title>
<link>https://arxiv.org/abs/2507.21095</link>
<guid>https://arxiv.org/abs/2507.21095</guid>
<content:encoded><![CDATA[
<div> transformer architecture, contextual embeddings, subjectivity detection, pre-trained language models, multilingual setting

Summary:<br /><br />
This paper presents an approach to subjectivity detection in CheckThat! 2025 Task 1, utilizing a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. The system achieved competitive performance in monolingual, multilingual, and zero-shot settings across various languages including English, Arabic, German, and Italian. The approach leverages pre-trained transformers with additional lexical features, such as part-of-speech tags and TF-IDF features for Arabic, and a cross-lingual DeBERTa V3 model with TF-IDF features and gating mechanism for other languages. The study highlights the effectiveness of combining TF-IDF features with the gating mechanism and cross-lingual transfer for subjectivity detection. An ablation analysis emphasized the importance of these components and revealed the model's sensitivity to the order of cross-lingual fine-tuning and linguistic proximity of training languages. <div>
arXiv:2507.21095v1 Announce Type: new 
Abstract: This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic. We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism. We evaluated our system in monolingual, multilingual, and zero-shot settings across multiple languages including English, Arabic, German, Italian, and several unseen languages. The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an ablation analysis that demonstrated the importance of combining TF-IDF features with the gating mechanism and the cross-lingual transfer for subjectivity detection. Furthermore, our analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting</title>
<link>https://arxiv.org/abs/2507.21099</link>
<guid>https://arxiv.org/abs/2507.21099</guid>
<content:encoded><![CDATA[
<div> keywords: LLMs, ad rewriting, retrieval systems, supervised fine-tuning, reinforcement learning

Summary: 
LLMs have the capability to return relevant information based on search algorithms and user query relevance. This study investigates the impact of content phrasing on ad visibility in retrieval systems. The researchers propose a supervised fine-tuning framework that balances semantic relevance and content fidelity to optimize ad phrasing without modifying the retrieval model. Two metrics, DeltaMRR@K and DeltaDIR@K, are introduced to evaluate the effectiveness of the approach. The experiments show that PPO trained models outperform prompt engineering and supervised fine-tuning in optimizing ad rewriting for LLM integrated retrieval systems. The results emphasize the significance of how ads are written before retrieval, as well as the importance of prompt format and reinforcement learning in enhancing ad visibility. <br /><br />Summary: <div>
arXiv:2507.21099v1 Announce Type: new 
Abstract: Search algorithms and user query relevance have given LLMs the ability to return relevant information, but the effect of content phrasing on ad visibility remains underexplored. We investigate how LLM-based rewriting of advertisements can improve their ranking in retrieval systems and inclusion in generated LLM responses, without modifying the retrieval model itself. We introduce a supervised fine-tuning framework with a custom loss balancing semantic relevance and content fidelity. To evaluate effectiveness, we propose two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion frequency improvement). Our approach presents a scalable method to optimize ad phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments across both instruction-based and few-shot prompting demonstrate that PPO trained models outperform both prompt engineering and supervised fine-tuning in most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in instruction-based prompting. These results highlight the importance of how the ad is written before retrieval and prompt format and reinforcement learning in effective ad rewriting for LLM integrated retrieval systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iLSU-T: an Open Dataset for Uruguayan Sign Language Translation</title>
<link>https://arxiv.org/abs/2507.21104</link>
<guid>https://arxiv.org/abs/2507.21104</guid>
<content:encoded><![CDATA[
<div> Keywords: sign language translation, dataset, Uruguayan Sign Language, multimodal data, accessibility<br />
Summary:<br />
This study introduces iLSU T, an open dataset of interpreted videos of Uruguayan Sign Language (USL) with audio and text transcriptions. The dataset includes over 185 hours of videos from public TV broadcasts, featuring 18 professional interpreters. The aim is to provide valuable data for developing new approaches in sign language processing. The research conducts experiments using three advanced translation algorithms to establish a baseline for the dataset and assess its effectiveness in data processing. The results emphasize the importance of localized datasets for enhancing sign language translation tools and promoting inclusivity. By offering the dataset and code, the study encourages further research in this area to advance accessibility for individuals with hearing impairments. <br />
Summary: <div>
arXiv:2507.21104v1 Announce Type: new 
Abstract: Automatic sign language translation has gained particular interest in the computer vision and computational linguistics communities in recent years. Given each sign language country particularities, machine translation requires local data to develop new techniques and adapt existing ones. This work presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB videos with audio and text transcriptions. This type of multimodal and curated data is paramount for developing novel approaches to understand or generate tools for sign language processing. iLSU T comprises more than 185 hours of interpreted sign language videos from public TV broadcasting. It covers diverse topics and includes the participation of 18 professional interpreters of sign language. A series of experiments using three state of the art translation algorithms is presented. The aim is to establish a baseline for this dataset and evaluate its usefulness and the proposed pipeline for data processing. The experiments highlight the need for more localized datasets for sign language translation and understanding, which are critical for developing novel tools to improve accessibility and inclusion of all individuals. Our data and code can be accessed.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype</title>
<link>https://arxiv.org/abs/2507.21106</link>
<guid>https://arxiv.org/abs/2507.21106</guid>
<content:encoded><![CDATA[
<div> Keywords: Arabic Rhetoric, Literary Devices, Text Analysis, Measurement Tool, Linguistic Embellishments

Summary:
Arabic Rhetoric is a vital aspect of conveying messages effectively in Arabic linguistics, utilizing literary devices to enhance communication. However, measuring the use of Arabic rhetoric objectively has been a challenge. This study aimed to develop a tool to quantify the density of literary devices in Arabic texts, serving as a proxy for assessing Arabic rhetoric. By compiling a list of 84 common literary devices, creating a system to identify and calculate their density, and developing electronic tools, including a website and online calculator, the study offers a practical approach to analyze Arabic rhetoric. The tool can accurately determine the density of Arabic rhetoric in any text, aiding in the comparison of rhetoric usage across genres, authors, and historical periods. This innovative method provides a valuable resource for scholars and researchers in the field of Arabic linguistics.<br /><br />Summary: <div>
arXiv:2507.21106v1 Announce Type: new 
Abstract: Arabic Rhetoric is the field of Arabic linguistics which governs the art and science of conveying a message with greater beauty, impact and persuasiveness. The field is as ancient as the Arabic language itself and is found extensively in classical and contemporary Arabic poetry, free verse and prose. In practical terms, it is the intelligent use of word order, figurative speech and linguistic embellishments to enhance message delivery. Despite the volumes that have been written about it and the high status accorded to it, there is no way to objectively know whether a speaker or writer has used Arabic rhetoric in a given text, to what extent, and why. There is no objective way to compare the use of Arabic rhetoric across genres, authors or epochs. It is impossible to know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary genres are richer in Arabic rhetoric. The aim of the current study was to devise a way to measure the density of the literary devices which constitute Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself. A comprehensive list of 84 of the commonest literary devices and their definitions was compiled. A system of identifying literary devices in texts was constructed. A method of calculating the density of literary devices based on the morpheme count of the text was utilised. Four electronic tools and an analogue tool were created to support the calculation of an Arabic text's rhetorical literary device density, including a website and online calculator. Additionally, a technique of reporting the distribution of literary devices used across the three sub-domains of Arabic rhetoric was created. The output of this project is a working tool which can accurately report the density of Arabic rhetoric in any Arabic text or speech.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams</title>
<link>https://arxiv.org/abs/2507.21107</link>
<guid>https://arxiv.org/abs/2507.21107</guid>
<content:encoded><![CDATA[
<div> Keywords: Curved Inference, language model, semantic shifts, geometric interpretability, activation trajectories

Summary:
Curved Inference introduces a framework to track how a language model's trajectory changes in response to different semantic concerns. The study compares two large language models, Gemma3-1b and LLaMA3.2-3b, across various semantic domains. Using metrics based on curvature and salience, the analysis reveals that both models adjust their internal activation trajectories in response to different prompts, with LLaMA showing more significant changes. The findings suggest a dual-layer structure in language model geometry, involving a latent conceptual space and prompt-specific contextual trajectories. The study highlights the importance of understanding how models navigate semantic meaning over different levels, providing insights into alignment, abstraction, and inference dynamics. Overall, Curved Inference offers a systematic approach to examining semantic abstraction and model alignment in language models.<br /><br />Summary: Curved Inference framework tracks language model trajectory changes in response to semantic concerns. Comparison of models shows adjustments in internal activation trajectories. Dual-layer model geometry structure identified. Study provides insights into alignment, abstraction, and inference dynamics in language models. <div>
arXiv:2507.21107v1 Announce Type: new 
Abstract: We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Classification Tasks and Approaches for Legal Contracts</title>
<link>https://arxiv.org/abs/2507.21108</link>
<guid>https://arxiv.org/abs/2507.21108</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Legal Contract Classification, datasets, methodologies, evaluation techniques, future research directions

Summary: 
Automatic Legal Contract Classification (LCC) is crucial due to the complexity and volume of contracts, leading to the need for automation. This survey delves into challenges, tasks, datasets, and methodologies within LCC, identifying seven classification tasks and reviewing fourteen datasets. Methodologies are categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Evaluation techniques and best-performing results are discussed. The survey highlights the limitations of current methods and suggests future research directions to enhance efficiency, accuracy, and scalability of LCC. This comprehensive survey aims to support legal NLP researchers and practitioners in improving legal processes, enhancing accessibility to legal information, and fostering a more informed and equitable society. 

<br /><br />Summary: <div>
arXiv:2507.21108v1 Announce Type: new 
Abstract: Given the large size and volumes of contracts and their underlying inherent complexity, manual reviews become inefficient and prone to errors, creating a clear need for automation. Automatic Legal Contract Classification (LCC) revolutionizes the way legal contracts are analyzed, offering substantial improvements in speed, accuracy, and accessibility. This survey delves into the challenges of automatic LCC and a detailed examination of key tasks, datasets, and methodologies. We identify seven classification tasks within LCC, and review fourteen datasets related to English-language contracts, including public, proprietary, and non-public sources. We also introduce a methodology taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Additionally, the survey discusses evaluation techniques and highlights the best-performing results from the reviewed studies. By providing a thorough overview of current methods and their limitations, this survey suggests future research directions to improve the efficiency, accuracy, and scalability of LCC. As the first comprehensive survey on LCC, it aims to support legal NLP researchers and practitioners in improving legal processes, making legal information more accessible, and promoting a more informed and equitable society.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering</title>
<link>https://arxiv.org/abs/2507.21110</link>
<guid>https://arxiv.org/abs/2507.21110</guid>
<content:encoded><![CDATA[
<div> Keywords: SemRAG, Retrieval Augmented Generation, domain-specific knowledge, knowledge graphs, semantic chunking

Summary:<br /><br />This paper introduces SemRAG, a framework that enhances the Retrieval Augmented Generation (RAG) model by efficiently integrating domain-specific knowledge through semantic chunking and knowledge graphs. SemRAG uses a semantic chunking algorithm to segment documents based on cosine similarity, preserving semantic coherence while reducing computational overhead. By structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets show that SemRAG outperforms traditional RAG methods in terms of relevance and correctness of retrieved information. Optimizing buffer sizes for different datasets further enhances retrieval performance. SemRAG offers an efficient and accurate domain-specific LLM pipeline without the need for extensive fine-tuning, making it a practical and scalable solution for AI applications in specialized fields.<br />Summary: <div>
arXiv:2507.21110v1 Announce Type: new 
Abstract: This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InsurTech innovation using natural language processing</title>
<link>https://arxiv.org/abs/2507.21112</link>
<guid>https://arxiv.org/abs/2507.21112</guid>
<content:encoded><![CDATA[
<div> Keywords: InsurTech, natural language processing, structured data, actuarial analysis, commercial insurance

Summary: 
InsurTech companies are driving traditional insurance firms to adopt alternative data sources and advanced technologies. This paper explores the use of natural language processing (NLP) in insurance operations, focusing on translating unstructured text into structured data for analysis and decision-making. Through case studies using real-world alternative data, NLP techniques are applied to enhance commercial insurance pricing and risk assessment. The insights derived from text analysis not only refine traditional rating factors but also introduce new perspectives for industry classifications. The paper demonstrates that NLP is a foundational tool for modern, data-driven insurance analytics, offering valuable insights beyond traditional methods. NLP plays a crucial role in transforming raw text into actionable information for improving insurance operations. 

<br /><br />Summary: <div>
arXiv:2507.21112v1 Announce Type: new 
Abstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations with a focus on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate practical use cases in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classifications. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element for modern, data-driven insurance analytics.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law</title>
<link>https://arxiv.org/abs/2507.21134</link>
<guid>https://arxiv.org/abs/2507.21134</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, domain-specific safety, legal, financial, medical

Summary:<br /><br />
The study focuses on evaluating the safety of large language models (LLMs) in high-risk fields such as law, finance, and medicine. Domain-specific safety principles, based on established ethical codes, are defined for LLMs. Trident-Bench, a benchmark specifically for assessing LLM safety in legal, financial, and medical domains, is introduced. Nineteen models were evaluated on Trident-Bench, revealing that generalist models meet basic safety expectations while domain-specialized models struggle with ethical nuances. The study underscores the need for domain-specific safety enhancements. Trident-Bench is a crucial resource for studying LLM safety in regulated fields and sets the stage for future research on mitigating safety risks in LLM deployment. The code and benchmark are available at the provided GitHub repository. <div>
arXiv:2507.21134v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TTS-1 Technical Report</title>
<link>https://arxiv.org/abs/2507.21138</link>
<guid>https://arxiv.org/abs/2507.21138</guid>
<content:encoded><![CDATA[
<div> Transformer-based autoregressive text-to-speech, TTS models, inworld TTS-1, SpeechLM, real-time speech synthesis, on-device use cases <br />
<br />
Summary: 
Inworld TTS-1 introduces two Transformer-based autoregressive text-to-speech models, TTS-1 and TTS-1-Max. TTS-1-Max, with 8.8B parameters, focuses on quality and expressiveness, while TTS-1, with 1.6B parameters, is designed for real-time speech synthesis. Both models achieve state-of-the-art performance through pre-training, fine-tuning, and RL-alignment of the SpeechLM component. They support 11 languages, emotional control, and non-verbal vocalizations through audio markups, generating high-quality speech with low latency. The models are efficient and can be used on devices for various applications. Additionally, the training and modeling code are open-sourced under an MIT license. <div>
arXiv:2507.21138v1 Announce Type: new 
Abstract: We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question</title>
<link>https://arxiv.org/abs/2507.21168</link>
<guid>https://arxiv.org/abs/2507.21168</guid>
<content:encoded><![CDATA[
<div> Keywords: diversity, machine learning, large language models, ensemble, binary questions

Summary: 
Diversity plays a crucial role in enhancing the performance of machine learning models, particularly large language models (LLMs). This study compares two diversity approaches in answering binary questions using LLMs: model diversity and question interpretation diversity. Model diversity involves multiple models answering the same question, while question interpretation diversity frames the same question in different ways for the same model to answer. Majority voting is used as the ensemble consensus heuristic to determine the final answer. Results across boolq, strategyqa, and pubmedqa datasets consistently demonstrate that question interpretation diversity outperforms model diversity in terms of ensemble accuracy. Analysis of GPT and LLaMa models indicates that model diversity often yields results between the best and worst ensemble members without significant improvement. This research provides valuable insights into the effective utilization of diversity strategies in enhancing the performance of LLMs in answering binary questions.<br /><br />Summary: <div>
arXiv:2507.21168v1 Announce Type: new 
Abstract: Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers</title>
<link>https://arxiv.org/abs/2507.21186</link>
<guid>https://arxiv.org/abs/2507.21186</guid>
<content:encoded><![CDATA[
<div> Transformer, AI research, activation-based attribution, Contrast-CAT, interpretability <br />
<br />
Summary: 
The article discusses the challenges in explaining the decisions of transformer-based AI models, particularly in text classification tasks. Activation-based attribution methods, while effective, can be unreliable due to the presence of class-irrelevant features within activations. To address this issue, the authors propose a novel method called Contrast-CAT, which filters out such features to provide clearer and more accurate attribution maps. Experimental results demonstrate that Contrast-CAT outperforms existing methods, with significant improvements in interpretability metrics under the MoRF setting. The findings highlight the importance of enhancing interpretability in transformer models to foster trust and safe deployment in real-world applications. <div>
arXiv:2507.21186v1 Announce Type: new 
Abstract: Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability</title>
<link>https://arxiv.org/abs/2507.21234</link>
<guid>https://arxiv.org/abs/2507.21234</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, crime perception, sentiment analysis, Bangla language, transformer-based model 

Summary: 
This study focuses on analyzing the evolving public perception of crime-related news on social media platforms, particularly in the Bangla language. The researchers curated a dataset of 28,528 Bangla-language social media comments to classify them into positive, negative, or neutral categories. They developed a transformer-based model using the XLM-RoBERTa Base architecture, achieving a high classification accuracy of 97%, surpassing existing methods in Bangla sentiment analysis. Additionally, an explainable AI technique was implemented to identify key features influencing sentiment classification, enhancing model interpretability. The study highlights the effectiveness of transformer-based models in processing low-resource languages like Bengali and their potential to provide actionable insights for public policy formulation and crime prevention strategies. <br /><br />Summary: <div>
arXiv:2507.21234v1 Announce Type: new 
Abstract: In recent years, social media platforms have become prominent spaces for individuals to express their opinions on ongoing events, including criminal incidents. As a result, public sentiment can shift dynamically over time. This study investigates the evolving public perception of crime-related news by classifying user-generated comments into three categories: positive, negative, and neutral. A newly curated dataset comprising 28,528 Bangla-language social media comments was developed for this purpose. We propose a transformer-based model utilizing the XLM-RoBERTa Base architecture, which achieves a classification accuracy of 97%, outperforming existing state-of-the-art methods in Bangla sentiment analysis. To enhance model interpretability, explainable AI technique is employed to identify the most influential features driving sentiment classification. The results underscore the effectiveness of transformer-based models in processing low-resource languages such as Bengali and demonstrate their potential to extract actionable insights that can support public policy formulation and crime prevention strategies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach</title>
<link>https://arxiv.org/abs/2507.21242</link>
<guid>https://arxiv.org/abs/2507.21242</guid>
<content:encoded><![CDATA[
<div> Fine-tuned Bangla BERT, hyperpartisan news classification, accuracy score, semi-supervised learning, LIME <br />
<br />
Summary: 
The research focuses on combating misinformation in Bangla by fine-tuning the Bangla BERT model for detecting hyperpartisan news with high accuracy. With a score of 95.65%, Bangla BERT surpasses conventional models, showcasing its effectiveness even in low-resource language environments. The study highlights the utility of transformer models in mitigating biased content dissemination and emphasizes the importance of transparent explanations provided by LIME in building trust in model outcomes. Implementing semi-supervised learning further enhances prediction capabilities, demonstrating the potential for significant improvements in this domain. This research addresses the critical need for advanced natural language processing methods in detecting hyperpartisan news, essential for promoting informed discourse and combating the spread of misinformation. <br /><br /> <div>
arXiv:2507.21242v1 Announce Type: new 
Abstract: In the current digital landscape, misinformation circulates rapidly, shaping public perception and causing societal divisions. It is difficult to identify hyperpartisan news in Bangla since there aren't many sophisticated natural language processing methods available for this low-resource language. Without effective detection methods, biased content can spread unchecked, posing serious risks to informed discourse. To address this gap, our research fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model, designed to enhance classification accuracy for hyperpartisan news. We evaluate its performance against traditional machine learning models and implement semi-supervised learning to enhance predictions further. Not only that, we use LIME to provide transparent explanations of the model's decision-making process, which helps to build trust in its outcomes. With a remarkable accuracy score of 95.65%, Bangla BERT outperforms conventional approaches, according to our trial data. The findings of this study demonstrate the usefulness of transformer models even in environments with limited resources, which opens the door to further improvements in this area.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can human clinical rationales improve the performance and explainability of clinical text classification models?</title>
<link>https://arxiv.org/abs/2507.21302</link>
<guid>https://arxiv.org/abs/2507.21302</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-driven clinical text classification, transformer-based models, human-based clinical rationales, primary cancer site diagnoses, explainability

Summary: 
- The study explores the use of human-based clinical rationales in improving AI-driven clinical text classification for primary cancer site diagnoses.
- 99,125 human-based clinical rationales were analyzed alongside electronic pathology reports to train transformer-based models.
- Clinical rationales as additional training data showed improved model performance in high-resource scenarios but were inconsistent with limited resources.
- Using sufficiency as a metric for pre-selecting rationales yielded inconsistent results.
- Models trained on additional reports outperformed those trained on rationales, suggesting that labeling more reports is more beneficial for model accuracy.
- Training models on rationale-supplemented data slightly improved explainability but resulted in smaller performance improvements compared to additional report training. 

<br /><br />Summary: <div>
arXiv:2507.21302v1 Announce Type: new 
Abstract: AI-driven clinical text classification is vital for explainable automated retrieval of population-level health information. This work investigates whether human-based clinical rationales can serve as additional supervision to improve both performance and explainability of transformer-based models that automatically encode clinical documents. We analyzed 99,125 human-based clinical rationales that provide plausible explanations for primary cancer site diagnoses, using them as additional training samples alongside 128,649 electronic pathology reports to evaluate transformer-based models for extracting primary cancer sites. We also investigated sufficiency as a way to measure rationale quality for pre-selecting rationales. Our results showed that clinical rationales as additional training data can improve model performance in high-resource scenarios but produce inconsistent behavior when resources are limited. Using sufficiency as an automatic metric to preselect rationales also leads to inconsistent results. Importantly, models trained on rationales were consistently outperformed by models trained on additional reports instead. This suggests that clinical rationales don't consistently improve model performance and are outperformed by simply using more reports. Therefore, if the goal is optimizing accuracy, annotation efforts should focus on labeling more reports rather than creating rationales. However, if explainability is the priority, training models on rationale-supplemented data may help them better identify rationale-like features. We conclude that using clinical rationales as additional training data results in smaller performance improvements and only slightly better explainability (measured as average token-level rationale coverage) compared to training on additional reports.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Understand Morality Across Cultures?</title>
<link>https://arxiv.org/abs/2507.21319</link>
<guid>https://arxiv.org/abs/2507.21319</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, biases, cross-cultural differences, moral perspectives, ethical implications

Summary: 
Recent advancements in large language models (LLMs) have raised concerns about embedded biases, including gender, racial, and cultural biases derived from training data. This study examines how LLMs capture cross-cultural differences in moral perspectives by comparing variances in moral scores, conducting cluster alignment analyses, and probing models with comparative prompts. The results show that current LLMs often fail to accurately represent cross-cultural moral variation, compressing differences and showing low alignment with empirical survey patterns. This highlights the need for improved approaches to mitigate biases and enhance cultural representativeness in LLMs. The implications for the responsible development and global deployment of LLMs are discussed, emphasizing the importance of fairness and ethical alignment. 

Summary: <div>
arXiv:2507.21319v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Deep Learning Automatic Speech Recognition Model for Shona Language</title>
<link>https://arxiv.org/abs/2507.21331</link>
<guid>https://arxiv.org/abs/2507.21331</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep learning, Automatic Speech Recognition, Shona, Under-resourced languages, Data augmentation<br />
Summary:<br /> 
This study developed a deep learning-based Automatic Speech Recognition (ASR) system for the low-resource language of Shona. The research addressed challenges such as limited training data and tonal complexities in Shona speech to improve recognition accuracy compared to traditional models. The study explored the feasibility of deep learning for accurate ASR in Shona, identified challenges in designing deep learning architectures for Shona speech, and compared the performance of deep learning models with statistical ones. The ASR system employed a hybrid architecture with Convolutional Neural Network and Long Short-Term Memory network, along with data augmentation and transfer learning to address data scarcity. Attention mechanisms were used to handle tonal aspects. The system achieved impressive metrics, indicating the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona.<br /> 
Summary: <div>
arXiv:2507.21331v1 Announce Type: new 
Abstract: This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2507.21340</link>
<guid>https://arxiv.org/abs/2507.21340</guid>
<content:encoded><![CDATA[
<div> Framework, benchmarks, key-value extraction, text generation, evaluation strategy

Summary: 
The article introduces StructText, a framework for automatically generating benchmarks for key-value extraction from text using existing tabular data. It utilizes a two-stage pipeline to create synthetic natural-language text aligned with structured sources. The evaluation method combines LLM-based judgments on factuality, hallucination, and coherence, along with objective extraction metrics for accuracy. Results show that LLMs excel in factual accuracy and avoid hallucination but struggle with narrative coherence. Models accurately represent numerical and temporal information but embed it within narratives that resist automated extraction. This work provides datasets, evaluation tools, and baseline extraction systems to support ongoing research in this field. <br /><br />Summary: <div>
arXiv:2507.21340v1 Announce Type: new 
Abstract: Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turbocharging Web Automation: The Impact of Compressed History States</title>
<link>https://arxiv.org/abs/2507.21369</link>
<guid>https://arxiv.org/abs/2507.21369</guid>
<content:encoded><![CDATA[
<div> history compressor, web automation, language models, web state, task-relevant information

Summary:
The paper introduces a novel approach to web automation using a history compressor module to distill task-relevant information from history states, enhancing the utilization of history in predicting next actions. It addresses the limitations of current web automation approaches that overlook the importance of history states due to the verbose nature of web page states. By compressing history states into fixed-length representations, the approach improves accuracy by 1.2-5.4% compared to baseline approaches without history inputs. The experiments conducted on Mind2Web and WebLINX datasets demonstrate the effectiveness of the proposed method in leveraging history states for more efficient web automation. <div>
arXiv:2507.21369v1 Announce Type: new 
Abstract: Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations</title>
<link>https://arxiv.org/abs/2507.21428</link>
<guid>https://arxiv.org/abs/2507.21428</guid>
<content:encoded><![CDATA[
<div> Memory framework, LLM agents, tool management, autonomous, multi-turn interactions

Summary:
The study introduces MemTool, a framework that allows Large Language Model (LLM) agents to manage tools or Model Context Protocol (MCP) server contexts in multi-turn conversations. MemTool offers three modes: Autonomous Agent Mode, Workflow Mode, and Hybrid Mode, for tool management. Experiments on various LLMs show that Autonomous Agent Mode achieves high tool-removal efficiency for reasoning LLMs but lower efficiency for medium-sized models. Workflow and Hybrid modes effectively manage tool removal while Autonomous and Hybrid modes excel at task completion. Trade-offs and recommendations for each MemTool mode are presented based on task accuracy, agency, and model capabilities. This research provides insights for efficient tool management in LLM agents for dynamic multi-turn interactions. 

<br /><br />Summary: <div>
arXiv:2507.21428v1 Announce Type: new 
Abstract: Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour</title>
<link>https://arxiv.org/abs/2507.21432</link>
<guid>https://arxiv.org/abs/2507.21432</guid>
<content:encoded><![CDATA[
<div> local deployable causal large language models, travel mode choice prediction, LiTransMC, fine-tuned, predictive accuracy, interpretability, behavioural theory, specialist models, transportation research, policy formulation 
Summary: 
This study explores the use of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction, introducing LiTransMC as a fine-tuned model for this purpose. Evaluating eleven LLMs across multiple datasets, LiTransMC outperformed both untuned local models and larger proprietary systems, showcasing high predictive accuracy and interpretability. By combining structured behavioral prediction with natural language reasoning, this work paves the way for specialist, explainable transport models that can support simulations, policy testing, and behavioral insights. The findings suggest a transformation of general LLMs into specialized tools for transportation research and policy formulation, allowing for privacy, cost reduction, and increased access through local deployment. <div>
arXiv:2507.21432v1 Announce Type: new 
Abstract: This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench</title>
<link>https://arxiv.org/abs/2507.21476</link>
<guid>https://arxiv.org/abs/2507.21476</guid>
<content:encoded><![CDATA[
<div> Keyword: HumorBench, Language Models, Reasoning, Cartoon Captions, Evaluation

Summary:
- LLMs are evaluated using HumorBench, a benchmark designed to assess their ability to reason about and explain humor in cartoon captions.
- The benchmark includes 300 unique cartoon-caption pairs with expert-annotated evaluation rubrics.
- Models must identify joke elements and form hypotheses to comprehend the humor, showcasing their reasoning abilities.
- Progress in STEM reasoning correlates with success in humor comprehension for LLMs.
- Training models solely on STEM reasoning data still results in strong performance on HumorBench, indicating transferability of reasoning skills.
- Increasing thinking token budgets at test time produces varying results across different models in humor reasoning.

<br /><br />Summary: 
HumorBench evaluates large language models' reasoning and humor comprehension skills using cartoon-caption pairs. Models must identify joke elements and form hypotheses to explain the humor, showcasing their reasoning abilities. Progress in STEM reasoning correlates with success in humor comprehension, demonstrating the transferability of reasoning skills. Training models solely on STEM reasoning data also leads to strong performance on HumorBench. However, increasing thinking token budgets at test time results in mixed outcomes across different models in humor reasoning. <div>
arXiv:2507.21476v1 Announce Type: new 
Abstract: We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs</title>
<link>https://arxiv.org/abs/2507.21482</link>
<guid>https://arxiv.org/abs/2507.21482</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, label-efficient learning, task-diversity, supervised finetuning, data selection 

Summary: 
This paper addresses the challenge of developing high-performing models for specialized applications with limited human annotation resources. By leveraging task-diversity instead of prompt-diversity, the authors propose a simple yet effective sampling strategy for supervised finetuning. The strategy involves selecting examples across tasks using an inverse confidence weighting approach based on the varying levels of confidence of pre-trained models. Experimental results show that this method can achieve better accuracy than training on the complete dataset, with up to a 4% increase in MMLU score. The algorithm consistently outperforms existing methods across various annotation budgets and two instruction finetuning datasets, while reducing annotation costs by up to 80%. This approach provides comparable or superior results to more complex sampling procedures while being easier to implement and less computationally intensive.  <br /><br />Summary: <div>
arXiv:2507.21482v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VN-MTEB: Vietnamese Massive Text Embedding Benchmark</title>
<link>https://arxiv.org/abs/2507.21500</link>
<guid>https://arxiv.org/abs/2507.21500</guid>
<content:encoded><![CDATA[
<div> Keywords: Vietnam, internet traffic, online toxicity, embedding models, benchmark

Summary: 
Large-scale test datasets for evaluating AI models in Vietnamese applications are lacking due to the country's high internet traffic and online toxicity. To address this issue, a new Vietnamese benchmark, VN-MTEB, has been introduced by translating samples from the Massive Text Embedding Benchmark. This benchmark consists of 41 datasets across six tasks tailored for Vietnamese text embeddings. Analysis shows that larger and more complex models utilizing Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. The benchmark aims to facilitate the evaluation and deployment of embedding models in real-world projects. The datasets are available on HuggingFace. <br /><br />Summary: <div>
arXiv:2507.21500v1 Announce Type: new 
Abstract: Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persona Vectors: Monitoring and Controlling Character Traits in Language Models</title>
<link>https://arxiv.org/abs/2507.21509</link>
<guid>https://arxiv.org/abs/2507.21509</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, persona vectors, personality shifts, training data, natural-language description

Summary:
Large language models interact with users through a simulated 'Assistant' persona, which can sometimes deviate from desired traits like honesty or helpfulness. This paper identifies persona vectors underlying traits like evil or sycophancy, which can be used to monitor and control personality shifts during training and deployment. Intended and unintended changes in the Assistant's personality after finetuning are linked to shifts along these persona vectors, highlighting the importance of managing them. A new preventative steering method can help avoid undesirable personality changes, while persona vectors can also flag problematic training data at both dataset and individual sample levels. The automated method for extracting persona vectors can be applied to any personality trait using only a natural-language description. This research offers insights into managing and optimizing the personality of large language models for better user interactions. 

<br /><br />Summary: <div>
arXiv:2507.21509v1 Announce Type: new 
Abstract: Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting</title>
<link>https://arxiv.org/abs/2507.21522</link>
<guid>https://arxiv.org/abs/2507.21522</guid>
<content:encoded><![CDATA[
<div> Transformer architectures, automatic speech recognition (ASR), speculative decoding, Token Map Drafting, n-gram token map

Summary: 
Token Map Drafting is proposed as a model-free speculative decoding technique for end-to-end ASR systems based on transformer architectures like Whisper. It eliminates the need for a separate draft model by leveraging a precomputed n-gram token map derived from domain-specific training data. This approach significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results show decoding speed-ups of 1.27x on the CI-AVSR dataset and 1.37x on an internal dataset, with a 10% absolute improvement in decoding speed over the Distill-spec baseline running on CPU. Token Map Drafting proves effective for on-device ASR applications, offering efficient and accurate transcription capabilities on resource-constrained devices. 

<br /><br />Summary: <div>
arXiv:2507.21522v1 Announce Type: new 
Abstract: End-to-end automatic speech recognition (ASR) systems based on transformer architectures, such as Whisper, offer high transcription accuracy and robustness. However, their autoregressive decoding is computationally expensive, hence limiting deployment on CPU-based and resource-constrained devices. Speculative decoding (SD) mitigates this issue by using a smaller draft model to propose candidate tokens, which are then verified by the main model. However, this approach is impractical for devices lacking hardware accelerators like GPUs. To address this, we propose \emph{Token Map Drafting}, a model-free SD technique that eliminates the need for a separate draft model. Instead, we leverage a precomputed n-gram token map derived from domain-specific training data, enabling efficient speculative decoding with minimal overhead. Our method significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results demonstrate decoding speed-ups of $1.27\times$ on the CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading recognition accuracy. Additionally, our approach achieves a $10\%$ absolute improvement in decoding speed over the Distill-spec baseline running on CPU, highlighting its effectiveness for on-device ASR applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling</title>
<link>https://arxiv.org/abs/2507.21526</link>
<guid>https://arxiv.org/abs/2507.21526</guid>
<content:encoded><![CDATA[
<div> static attention, sparse pattern, deep layers, Time-to-First-Token, model accuracy

Summary:
TriangleMix introduces a novel static attention pattern that combines dense attention in shallow layers with a triangle-shaped sparse pattern in deeper layers. This approach reduces attention overhead significantly in deep layers and decreases Time-to-First-Token by 12% to 32% for sequence lengths between 32K to 128K, without compromising model accuracy. The method does not require training and seamlessly integrates with dynamic sparsity techniques, further enhancing LLM inference efficiency. Experimental results show a 3.7x to 15.3x reduction in attention overhead in deep layers, and a 19% acceleration in MInference at 128K sequence length. TriangleMix offers a promising solution to improve computational efficiency in Large Language Models. 

<br /><br />Summary: <div>
arXiv:2507.21526v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Classification of User Requirements from Online Feedback -- A Replication Study</title>
<link>https://arxiv.org/abs/2507.21532</link>
<guid>https://arxiv.org/abs/2507.21532</guid>
<content:encoded><![CDATA[
<div> Keywords: Natural Language Processing, Requirements Engineering, Replication Study, Deep Learning, Classification
Summary: 
This study focuses on replicating and extending a previous study on applying natural language processing (NLP) techniques in requirements engineering (RE). The research highlights the importance of replicating NLP for RE (NLP4RE) studies to enhance the external validity and reproducibility of findings. By reproducing the original results using publicly released source code, the study evaluates different deep learning models for requirement classification from user reviews. Results show diverse reproducibility levels across models, with Naive Bayes demonstrating perfect reproducibility. BERT and ELMo exhibit good generalization capabilities on an external dataset, while GPT-4o performs comparably to traditional machine learning models. The assessment confirms the replication readiness of the baseline study, with enhancements made to increase readiness further. Overall, the study highlights the potential for machine-assisted workflows in RE tasks and underscores the value of replication in advancing NLP4RE research.
<br /><br />Summary: <div>
arXiv:2507.21532v1 Announce Type: new 
Abstract: Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language</title>
<link>https://arxiv.org/abs/2507.21536</link>
<guid>https://arxiv.org/abs/2507.21536</guid>
<content:encoded><![CDATA[
<div> Keywords: Uyghur, Natural Language Processing, Dependency Annotation, Treebank, Universal Dependencies parser

Summary: 
The study introduces a Dependency Annotation Framework for Uyghur Natural Language Processing to address the resource gap. It includes 18 main relations and 26 subtypes, with specific labels for different functions. A cross-standard evaluation using a pre-trained Universal Dependencies parser revealed a 47.9% discrepancy in annotations, highlighting the need for a tailored approach for Uyghur-specific structures. The Modern Uyghur Dependency Treebank (MUDT), grounded in nine annotation principles, provides a more accurate representation for parsing and NLP tasks. The framework aims to improve parsing accuracy and facilitate downstream NLP tasks by offering a replicable model for other complex languages. <div>
arXiv:2507.21536v1 Announce Type: new 
Abstract: To address a critical resource gap in Uyghur Natural Language Processing (NLP), this study introduces a dependency annotation framework designed to overcome the limitations of existing treebanks for the low-resource, agglutinative language. This inventory includes 18 main relations and 26 subtypes, with specific labels such as cop:zero for verbless clauses and instr:case=loc/dat for nuanced instrumental functions. To empirically validate the necessity of this tailored approach, we conducted a cross-standard evaluation using a pre-trained Universal Dependencies parser. The analysis revealed a systematic 47.9% divergence in annotations, pinpointing the inadequacy of universal schemes for handling Uyghur-specific structures. Grounded in nine annotation principles that ensure typological accuracy and semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a more accurate and semantically transparent representation, designed to enable significant improvements in parsing and downstream NLP tasks, and offers a replicable model for other morphologically complex languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.21544</link>
<guid>https://arxiv.org/abs/2507.21544</guid>
<content:encoded><![CDATA[
<div> conflict detection, retrieval-augmented generation, knowledge graph, MAGIC benchmark, LLMs

Summary: 

The study addresses knowledge conflicts in retrieval-augmented generation systems, proposing a new framework called MAGIC that generates varied conflicts between similar contexts using knowledge graphs. Experiment results show that both open-source and proprietary models struggle with detecting conflicts, particularly in multi-hop reasoning scenarios. These models also struggle to identify the exact sources of contradictions. The study provides insights into how language models deal with conflicting information and suggests improvements for integrating diverse information. <div>
arXiv:2507.21544v1 Announce Type: new 
Abstract: Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers</title>
<link>https://arxiv.org/abs/2507.21556</link>
<guid>https://arxiv.org/abs/2507.21556</guid>
<content:encoded><![CDATA[
<div> transformer-based neural networks, cognitive plausibility, Spanish irregular morphomic pattern, human behavioral data, linguistic phenomena

Summary: 
The study compares transformer-based neural networks to human data in analyzing the Spanish irregular morphomic pattern's cognitive plausibility. Using the same framework as the human study, the research assesses if transformer models can replicate human-like sensitivity to this linguistic phenomenon. The experiments focus on different frequency conditions and reveal that while the models excel in stem and suffix accuracy, they differ in response preferences from humans. Models tend to favor irregular responses and are influenced by the training data distribution. Interestingly, models trained on specific frequency distributions show varying levels of sensitivity to the phonological aspect of real Spanish L-shaped verbs. This study highlights how transformer models perform in simulating human behavior in understanding irregular morphomic patterns in language. <div>
arXiv:2507.21556v1 Announce Type: new 
Abstract: This study investigates the cognitive plausibility of the Spanish irregular morphomic pattern by directly comparing transformer-based neural networks to human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical framework as the original human study, we evaluate whether transformer models can replicate human-like sensitivity to a complex linguistic phenomena, the morphome, under controlled input conditions. Our experiments focus on three frequency conditions: natural, low-frequency, and high-frequency distributions of verbs exhibiting irregular morphomic patterns. While the models outperformed humans in stem and suffix accuracy, a clear divergence emerged in response preferences. Unlike humans, who consistently favored natural responses across all test items, models' preferred irregular responses and were influenced by the proportion of irregular verbs in their training data. Additionally, models trained on the natural and low-frequency distributions, but not the high-frequency distribution, were sensitive to the phonological similarity between test items and real Spanish L-shaped verbs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2507.21568</link>
<guid>https://arxiv.org/abs/2507.21568</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence-level knowledge distillation, multilingual translation models, Multi-Hypothesis Distillation, low-resource languages, gender bias amplification 

Summary: 
- The paper explores sequence-level knowledge distillation for multilingual pre-trained encoder-decoder translation models, focusing on the insights provided by the teacher model's output distribution beyond standard decoding methods. 
- It introduces Multi-Hypothesis Distillation (MHD), a method that generates multiple translations for each source sentence to expose the student model to a wider range of target-side prefixes. 
- Utilizing $n$-best lists from beam search guides the student's learning process and addresses issues such as low variability and under-representation of infrequent tokens in alternative decoding methods. 
- For low-resource languages, sampling methods may slightly impact translation quality compared to beam search but enhance corpora variability and lexical richness, leading to improved student model performance. 
- The study also highlights how MHD can help mitigate gender bias amplification often associated with knowledge distillation techniques 

<br /><br />Summary: <div>
arXiv:2507.21568v1 Announce Type: new 
Abstract: This paper explores sequence-level knowledge distillation (KD) of multilingual pre-trained encoder-decoder translation models. We argue that the teacher model's output distribution holds valuable insights for the student, beyond the approximated mode obtained through beam search (the standard decoding method), and present Multi-Hypothesis Distillation (MHD), a sequence-level KD method that generates multiple translations for each source sentence. This provides a larger representation of the teacher model distribution and exposes the student model to a wider range of target-side prefixes. We leverage $n$-best lists from beam search to guide the student's learning and examine alternative decoding methods to address issues like low variability and the under-representation of infrequent tokens. For low-resource languages, our research shows that while sampling methods may slightly compromise translation quality compared to beam search based approaches, they enhance the generated corpora with greater variability and lexical richness. This ultimately improves student model performance and mitigates the gender bias amplification often associated with KD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual JobBERT for Cross-Lingual Job Title Matching</title>
<link>https://arxiv.org/abs/2507.21609</link>
<guid>https://arxiv.org/abs/2507.21609</guid>
<content:encoded><![CDATA[
<div> cross-lingual job title matching, JobBERT-V3, contrastive learning, multilingual dataset, TalentCLEF 2025<br />
<br />
JobBERT-V3 is a new model for cross-lingual job title matching that extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a multilingual dataset of over 21 million job titles. The model, based on the state-of-the-art JobBERT-V2, aligns languages efficiently without task-specific supervision. Evaluations on the TalentCLEF 2025 benchmark show that JobBERT-V3 outperforms strong multilingual baselines and performs consistently in both monolingual and cross-lingual settings. Additionally, the model can effectively rank relevant skills for a given job title, showcasing its broader utility in multilingual labor market intelligence. The model is publicly available for use.<br /><br />Summary: <div>
arXiv:2507.21609v1 Announce Type: new 
Abstract: We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual job title matching. Building on the state-of-the-art monolingual JobBERT-V2, our approach extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a balanced multilingual dataset of over 21 million job titles. The model retains the efficiency-focused architecture of its predecessor while enabling robust alignment across languages without requiring task-specific supervision. Extensive evaluations on the TalentCLEF 2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual baselines and achieves consistent performance across both monolingual and cross-lingual settings. While not the primary focus, we also show that the model can be effectively used to rank relevant skills for a given job title, demonstrating its broader applicability in multilingual labor market intelligence. The model is publicly available: https://huggingface.co/TechWolf/JobBERT-v3.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Libra: Assessing and Improving Reward Model by Learning to Think</title>
<link>https://arxiv.org/abs/2507.21645</link>
<guid>https://arxiv.org/abs/2507.21645</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, reasoning scenarios, generative reward model, Libra Bench

Summary:
Reinforcement learning has been instrumental in enhancing the reasoning abilities of language models. However, existing reward models face challenges in complex reasoning scenarios due to rule-based or reference-based rewards. To overcome these limitations, a framework is proposed to evaluate and enhance reward models in challenging reasoning scenarios. A reasoning-oriented benchmark called Libra Bench is introduced, comprising diverse mathematical problems and advanced reasoning models. A novel approach for improving generative reward models through learning-to-think methodologies is presented. The Libra-RM series of generative reward models with reasoning capabilities demonstrates state-of-the-art performance on various benchmarks. Downstream experiments show a correlation between Libra Bench and applications, highlighting the potential of Libra-RM to enhance reasoning models using unlabeled data. <div>
arXiv:2507.21645v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases</title>
<link>https://arxiv.org/abs/2507.21652</link>
<guid>https://arxiv.org/abs/2507.21652</guid>
<content:encoded><![CDATA[
<div> Keywords: large reasoning models, safety alignment, unsafe completions, correction-based supervision, dataset

Summary: 
UnsafeChain is a new safety alignment dataset that addresses the challenge of hard prompts that consistently lead to harmful outputs in large reasoning models (LRMs). By identifying and correcting unsafe completions, the dataset aims to enhance safety while maintaining general reasoning ability. Three LRMs were fine-tuned on UnsafeChain and compared against previous datasets SafeChain and STAR-1 across various benchmarks. UnsafeChain consistently outperformed prior datasets, demonstrating the effectiveness and generalizability of correction-based supervision. Even a subset of 1K examples from UnsafeChain matched or exceeded baseline performance on these benchmarks. The dataset and code are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2507.21652v1 Announce Type: new 
Abstract: As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal</title>
<link>https://arxiv.org/abs/2507.21750</link>
<guid>https://arxiv.org/abs/2507.21750</guid>
<content:encoded><![CDATA[
<div> vulnerable, adversarial attacks, pre-trained language models, robustness, embedding space <br />
Summary: 
The article discusses the vulnerability of pre-trained language models (PLMs) to adversarial attacks and proposes a novel method to enhance their robustness. The proposed approach involves removing instance-level principal components from the embedding space, transforming it to approximate Gaussian properties. This transformation helps reduce susceptibility to adversarial perturbations while maintaining semantic relationships. By aligning embedding distributions, the impact of adversarial noise on decision boundaries is minimized, improving robustness without the need for adversarial examples or expensive training-time augmentation. Experimental results on eight benchmark datasets demonstrate that the approach enhances adversarial robustness while maintaining comparable accuracy to baselines before attacks, striking a balance between robustness and generalization.<br /> <div>
arXiv:2507.21750v1 Announce Type: new 
Abstract: Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2507.21773</link>
<guid>https://arxiv.org/abs/2507.21773</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, AgriEval, agricultural benchmark, data, model performance

Summary:<br /><br />
The article introduces AgriEval, a Chinese agricultural benchmark designed to evaluate the performance of large language models (LLMs) in the agricultural domain. AgriEval encompasses six major agriculture categories and 29 subcategories, catering to four core cognitive scenarios. The dataset, sourced from university-level examinations, offers high-quality data for assessing LLM capabilities. With 14,697 multiple-choice and 2,167 open-ended question-and-answer questions, AgriEval is the most extensive agricultural benchmark currently available. Experimental results on 51 LLMs demonstrate that most models struggle to achieve 60% accuracy, indicating room for improvement in agricultural LLMs. The study also explores factors influencing model performance and suggests strategies for enhancement. AgriEval presents a valuable resource for advancing LLM research in the agricultural sector. The benchmark is publicly accessible on GitHub for further investigation and development. <div>
arXiv:2507.21773v1 Announce Type: new 
Abstract: In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at https://github.com/YanPioneer/AgriEval/.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Problem with Safety Classification is not just the Models</title>
<link>https://arxiv.org/abs/2507.21782</link>
<guid>https://arxiv.org/abs/2507.21782</guid>
<content:encoded><![CDATA[
<div> multilingual, Large Language Models, safety classification models, evaluation datasets, harmful content<br />
Summary: <br />
The paper focuses on the robustness of Large Language Models (LLMs) to unsafe behaviors, specifically looking at safety classification models in multilingual scenarios. It highlights multilingual disparities in safety classifiers across 18 languages and points out potential issues with evaluation datasets. The study suggests that shortcomings in current safety classifiers are not solely due to the models themselves but also relate to the evaluation datasets. By analyzing 5 safety classification models, the paper aims to contribute to the development of more effective methods for identifying harmful content in LLM inputs across various languages. <div>
arXiv:2507.21782v1 Announce Type: new 
Abstract: Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartMark: A Structured Grammar for Chart Annotation</title>
<link>https://arxiv.org/abs/2507.21810</link>
<guid>https://arxiv.org/abs/2507.21810</guid>
<content:encoded><![CDATA[
<div> Keywords: ChartMark, structured grammar, visualization accessibility, hierarchical framework, Vega-Lite <br />
Summary: 
ChartMark is a new structured grammar proposed to enhance visualization accessibility by standardizing chart annotations. It separates annotation semantics from visualization implementations, allowing for cross-platform reuse. The hierarchical framework of ChartMark accommodates various annotation dimensions, such as task and chart context, enabling the expression of both abstract intents and precise visual details. The toolkit developed for ChartMark demonstrates its flexibility and practical applicability by converting ChartMark specifications into Vega-Lite visualizations. This approach offers a standardized and efficient way to enhance visualization accessibility and facilitate the creation of visually engaging and informative charts across different platforms.<br /><br />Summary: <div>
arXiv:2507.21810v1 Announce Type: new 
Abstract: Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish</title>
<link>https://arxiv.org/abs/2507.21813</link>
<guid>https://arxiv.org/abs/2507.21813</guid>
<content:encoded><![CDATA[
<div> identification, anglicism, Spanish, ADoBo 2025, IberLEF 2025 <br />
Summary: <br />
This paper presents the findings of ADoBo 2025, a shared task focusing on identifying anglicisms in Spanish text, introduced within the framework of IberLEF 2025. Participants were tasked with detecting English lexical borrowings within a dataset of Spanish journalistic texts. Five teams participated in the test phase, employing a range of approaches including LLMs, deep learning models, Transformer-based models, and rule-based systems. Results varied significantly, with F1 scores ranging from 0.17 to 0.99, highlighting the diversity in performance across different systems for this particular task. <div>
arXiv:2507.21813v1 Announce Type: new 
Abstract: This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs</title>
<link>https://arxiv.org/abs/2507.21815</link>
<guid>https://arxiv.org/abs/2507.21815</guid>
<content:encoded><![CDATA[
<div> Keywords: harm reduction, substance use, large language models, benchmark, safety risks

Summary: 
The article introduces a benchmark called HRIPBench, designed to evaluate the accuracy and safety risks of large language models (LLMs) in providing harm reduction information for individuals who use drugs (PWUD). The benchmark dataset HRIP-Basic consists of 2,160 question-answer-evidence pairs covering tasks such as checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. Results show that current state-of-the-art LLMs struggle to provide accurate harm reduction information and can pose severe safety risks to PWUD. It is crucial to cautiously limit the use of LLMs in harm reduction contexts to prevent negative health outcomes. The study highlights the importance of considering safety and accuracy when utilizing LLMs in providing information for individuals dealing with substance use issues. 

<br /><br />Summary: <div>
arXiv:2507.21815v1 Announce Type: new 
Abstract: Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modelling Adjectival Modification Effects on Semantic Plausibility</title>
<link>https://arxiv.org/abs/2507.21828</link>
<guid>https://arxiv.org/abs/2507.21828</guid>
<content:encoded><![CDATA[
<div> transformer-based models, sentence transformers, ADEPT challenge benchmark, adjectival modifier, plausibility

Summary: 
This article focuses on the task of capturing changes in plausibility triggered by modifying events, specifically adjectival modifiers. The study tackles the ADEPT challenge benchmark, using sentence transformers and transformer-based models. The results reveal that both models struggle with the task, with sentence transformers even underperforming compared to models like RoBERTa. The study emphasizes the importance of a more balanced evaluation method, showing that imbalances can distort model performance and evaluation metrics, undermining result reliability. This research sheds light on the difficulties in modeling changes in plausibility due to event modification and highlights the need for a more realistic evaluation approach in natural language processing tasks. <div>
arXiv:2507.21828v1 Announce Type: new 
Abstract: While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences</title>
<link>https://arxiv.org/abs/2507.21831</link>
<guid>https://arxiv.org/abs/2507.21831</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, automated coding, prompting strategies, task performance, Mistral NeMo

Summary: 
This article introduces HALC$, a pipeline designed to create optimal prompts for coding tasks using large language models (LLMs). The study conducted over two million requests using local LLMs to test prompting strategies and task performance. Results showed that prompts can reliably code for single variables and across two variables using the Mistral NeMo LLM. The prompts were designed to align the LLM with the codebook, rather than adapting the codebook for the LLM. The paper offers insights into the effectiveness of different prompting strategies, key influencing factors, and the identification of reliable prompts for coding tasks using LLMs. This research aims to streamline the process of automated coding in the social sciences by providing a systematic and reliable method for constructing prompts. 

<br /><br />Summary: <div>
arXiv:2507.21831v1 Announce Type: new 
Abstract: LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21836</link>
<guid>https://arxiv.org/abs/2507.21836</guid>
<content:encoded><![CDATA[
arXiv:2507.21836v1 Announce Type: new 
Abstract: Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at https://github.com/weiyifan1023/AutoTIR.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning</title>
<link>https://arxiv.org/abs/2507.21892</link>
<guid>https://arxiv.org/abs/2507.21892</guid>
<content:encoded><![CDATA[
arXiv:2507.21892v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs</title>
<link>https://arxiv.org/abs/2507.21914</link>
<guid>https://arxiv.org/abs/2507.21914</guid>
<content:encoded><![CDATA[
arXiv:2507.21914v1 Announce Type: new 
Abstract: Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training language models to be warm and empathetic makes them less reliable and more sycophantic</title>
<link>https://arxiv.org/abs/2507.21919</link>
<guid>https://arxiv.org/abs/2507.21919</guid>
<content:encoded><![CDATA[
arXiv:2507.21919v1 Announce Type: new 
Abstract: Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Training Large Language Models via Reinforcement Learning from Self-Feedback</title>
<link>https://arxiv.org/abs/2507.21931</link>
<guid>https://arxiv.org/abs/2507.21931</guid>
<content:encoded><![CDATA[
arXiv:2507.21931v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.
  RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.
  By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation</title>
<link>https://arxiv.org/abs/2507.21934</link>
<guid>https://arxiv.org/abs/2507.21934</guid>
<content:encoded><![CDATA[
arXiv:2507.21934v1 Announce Type: new 
Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models</title>
<link>https://arxiv.org/abs/2507.21980</link>
<guid>https://arxiv.org/abs/2507.21980</guid>
<content:encoded><![CDATA[
arXiv:2507.21980v1 Announce Type: new 
Abstract: Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router</title>
<link>https://arxiv.org/abs/2507.22050</link>
<guid>https://arxiv.org/abs/2507.22050</guid>
<content:encoded><![CDATA[
arXiv:2507.22050v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning</title>
<link>https://arxiv.org/abs/2507.17307</link>
<guid>https://arxiv.org/abs/2507.17307</guid>
<content:encoded><![CDATA[
arXiv:2507.17307v2 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Reason About Trust?: A Pilot Study</title>
<link>https://arxiv.org/abs/2507.21075</link>
<guid>https://arxiv.org/abs/2507.21075</guid>
<content:encoded><![CDATA[
arXiv:2507.21075v1 Announce Type: cross 
Abstract: In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations</title>
<link>https://arxiv.org/abs/2507.21089</link>
<guid>https://arxiv.org/abs/2507.21089</guid>
<content:encoded><![CDATA[
arXiv:2507.21089v1 Announce Type: cross 
Abstract: Social media platforms increasingly employ proactive moderation techniques, such as detecting and curbing toxic and uncivil comments, to prevent the spread of harmful content. Despite these efforts, such approaches are often criticized for creating a climate of censorship and failing to address the underlying causes of uncivil behavior. Our work makes both theoretical and practical contributions by proposing and evaluating two types of emotion monitoring dashboards to users' emotional awareness and mitigate hate speech. In a study involving 211 participants, we evaluate the effects of the two mechanisms on user commenting behavior and emotional experiences. The results reveal that these interventions effectively increase users' awareness of their emotional states and reduce hate speech. However, our findings also indicate potential unintended effects, including increased expression of negative emotions (Angry, Fear, and Sad) when discussing sensitive issues. These insights provide a basis for further research on integrating proactive emotion regulation tools into social media platforms to foster healthier digital interactions.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analise Semantica Automatizada com LLM e RAG para Bulas Farmaceuticas</title>
<link>https://arxiv.org/abs/2507.21103</link>
<guid>https://arxiv.org/abs/2507.21103</guid>
<content:encoded><![CDATA[
arXiv:2507.21103v1 Announce Type: cross 
Abstract: The production of digital documents has been growing rapidly in academic, business, and health environments, presenting new challenges in the efficient extraction and analysis of unstructured information. This work investigates the use of RAG (Retrieval-Augmented Generation) architectures combined with Large-Scale Language Models (LLMs) to automate the analysis of documents in PDF format. The proposal integrates vector search techniques by embeddings, semantic data extraction and generation of contextualized natural language responses. To validate the approach, we conducted experiments with drug package inserts extracted from official public sources. The semantic queries applied were evaluated by metrics such as accuracy, completeness, response speed and consistency. The results indicate that the combination of RAG with LLMs offers significant gains in intelligent information retrieval and interpretation of unstructured technical texts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMaster: A Multi-Agent Conversational Framework Using A2A and MCP Protocols for Multimodal Information Retrieval and Analysis</title>
<link>https://arxiv.org/abs/2507.21105</link>
<guid>https://arxiv.org/abs/2507.21105</guid>
<content:encoded><![CDATA[
arXiv:2507.21105v1 Announce Type: cross 
Abstract: The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI), especially integrated with Large Language Models (LLMs), has greatly facilitated the resolution of complex tasks. However, current systems are still facing challenges of inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Most recently, the Model Context Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by Google have been introduced, and to the best of our knowledge, very few applications exist where both protocols are employed within a single MAS framework. We present a pilot study of AgentMaster, a novel modular multi-protocol MAS framework with self-implemented A2A and MCP, enabling dynamic coordination and flexible communication. Through a unified conversational interface, the system supports natural language interaction without prior technical expertise and responds to multimodal queries for tasks including information retrieval, question answering, and image analysis. Evaluation through the BERTScore F1 and LLM-as-a-Judge metric G-Eval averaged 96.3\% and 87.1\%, revealing robust inter-agent coordination, query decomposition, dynamic routing, and domain-specific, relevant responses. Overall, our proposed framework contributes to the potential capabilities of domain-specific, cooperative, and scalable conversational AI powered by MAS.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OneShield -- the Next Generation of LLM Guardrails</title>
<link>https://arxiv.org/abs/2507.21170</link>
<guid>https://arxiv.org/abs/2507.21170</guid>
<content:encoded><![CDATA[
arXiv:2507.21170v1 Announce Type: cross 
Abstract: The rise of Large Language Models has created a general excitement about the great potential for a myriad of applications. While LLMs offer many possibilities, questions about safety, privacy, and ethics have emerged, and all the key actors are working to address these issues with protective measures for their own models and standalone solutions. The constantly evolving nature of LLMs makes the task of universally shielding users against their potential risks extremely challenging, and one-size-fits-all solutions unfeasible. In this work, we propose OneShield, our stand-alone, model-agnostic and customizable solution to safeguard LLMs. OneShield aims to provide facilities for defining risk factors, expressing and declaring contextual safety and compliance policies, and mitigating LLM risks, with a focus on each specific customer. We describe the implementation of the framework, the scalability considerations and provide usage statistics of OneShield since its first deployment.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge</title>
<link>https://arxiv.org/abs/2507.21183</link>
<guid>https://arxiv.org/abs/2507.21183</guid>
<content:encoded><![CDATA[
arXiv:2507.21183v1 Announce Type: cross 
Abstract: As the era of large language models (LLMs) on behalf of users unfolds, Preference Optimization (PO) methods have become a central approach to aligning LLMs with human preferences and improving performance. We propose Maximum a Posteriori Preference Optimization (MaPPO), a framework for learning from preferences that explicitly incorporates prior reward knowledge into the optimization objective. While existing methods such as Direct Preference Optimization (DPO) and its variants treat preference learning as a Maximum Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating prior reward estimates into a principled Maximum a Posteriori (MaP) objective. This not only generalizes DPO and its variants, but also enhances alignment by mitigating the oversimplified binary classification of responses. More importantly, MaPPO introduces no additional hyperparameter, and supports preference optimization in both offline and online settings. In addition, MaPPO can be used as a plugin with consistent improvement on DPO variants, including widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different model sizes and model series on three standard benchmarks, including MT-Bench, AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in alignment performance without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</title>
<link>https://arxiv.org/abs/2507.21184</link>
<guid>https://arxiv.org/abs/2507.21184</guid>
<content:encoded><![CDATA[
arXiv:2507.21184v1 Announce Type: cross 
Abstract: Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting</title>
<link>https://arxiv.org/abs/2507.21257</link>
<guid>https://arxiv.org/abs/2507.21257</guid>
<content:encoded><![CDATA[
arXiv:2507.21257v1 Announce Type: cross 
Abstract: Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they "understand" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems</title>
<link>https://arxiv.org/abs/2507.21276</link>
<guid>https://arxiv.org/abs/2507.21276</guid>
<content:encoded><![CDATA[
arXiv:2507.21276v1 Announce Type: cross 
Abstract: Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Language Models To Gather Information Proactively</title>
<link>https://arxiv.org/abs/2507.21389</link>
<guid>https://arxiv.org/abs/2507.21389</guid>
<content:encoded><![CDATA[
arXiv:2507.21389v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly expected to function as collaborative partners, engaging in back-and-forth dialogue to solve complex, ambiguous problems. However, current LLMs often falter in real-world settings, defaulting to passive responses or narrow clarifications when faced with incomplete or under-specified prompts, falling short of proactively gathering the missing information that is crucial for high-quality solutions. In this work, we introduce a new task paradigm: proactive information gathering, where LLMs must identify gaps in the provided context and strategically elicit implicit user knowledge through targeted questions. To systematically study and train this capability, we design a scalable framework that generates partially specified, real-world tasks, masking key information and simulating authentic ambiguity. Within this setup, our core innovation is a reinforcement finetuning strategy that rewards questions that elicit genuinely new, implicit user information -- such as hidden domain expertise or fine-grained requirements -- that would otherwise remain unspoken. Experiments demonstrate that our trained Qwen-2.5-7B model significantly outperforms o3-mini by 18% on automatic evaluation metrics. More importantly, human evaluation reveals that clarification questions and final outlines generated by our model are favored by human annotators by 42% and 28% respectively. Together, these results highlight the value of proactive clarification in elevating LLMs from passive text generators to genuinely collaborative thought partners.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2507.21391</link>
<guid>https://arxiv.org/abs/2507.21391</guid>
<content:encoded><![CDATA[
arXiv:2507.21391v1 Announce Type: cross 
Abstract: We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations.In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs</title>
<link>https://arxiv.org/abs/2507.21420</link>
<guid>https://arxiv.org/abs/2507.21420</guid>
<content:encoded><![CDATA[
arXiv:2507.21420v1 Announce Type: cross 
Abstract: The computational cost of training multimodal large language models (MLLMs) rapidly increases with the number of tokens involved. Existing efficiency methods primarily target inference and rely on token reduction or merging, offering limited benefit during training. In this paper, we propose ReGATE (Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student framework in which the MLLM being trained serves as the student, and a frozen reference large language model (LLM) acts as the teacher. The teacher computes per-token reference losses, which are combined with an exponential moving average (EMA) of the student's own difficulty scores. This adaptive difficulty-based scoring enables the selective processing of crucial tokens while bypassing less informative ones in the forward pass, significantly reducing computational overhead. Experiments demonstrate that ReGATE, when applied to VideoLLaMA2, matches the peak accuracy of standard training on MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional training, it even surpasses the baseline on several multimodal benchmarks, all while reducing the total token count by over 41%. Code and models will be released soon.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does it Mean for a Neural Network to Learn a "World Model"?</title>
<link>https://arxiv.org/abs/2507.21513</link>
<guid>https://arxiv.org/abs/2507.21513</guid>
<content:encoded><![CDATA[
arXiv:2507.21513v1 Announce Type: cross 
Abstract: We propose a set of precise criteria for saying a neural net learns and uses a "world model." The goal is to give an operational meaning to terms that are often used informally, in order to provide a common language for experimental investigation. We focus specifically on the idea of representing a latent "state space" of the world, leaving modeling the effect of actions to future work. Our definition is based on ideas from the linear probing literature, and formalizes the notion of a computation that factors through a representation of the data generation process. An essential addition to the definition is a set of conditions to check that such a "world model" is not a trivial consequence of the neural net's data or task.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's important? -- SUnSET: Synergistic Understanding of Stakeholder, Events and Time for Timeline Generation</title>
<link>https://arxiv.org/abs/2507.21903</link>
<guid>https://arxiv.org/abs/2507.21903</guid>
<content:encoded><![CDATA[
arXiv:2507.21903v1 Announce Type: cross 
Abstract: As news reporting becomes increasingly global and decentralized online, tracking related events across multiple sources presents significant challenges. Existing news summarization methods typically utilizes Large Language Models and Graphical methods on article-based summaries. However, this is not effective since it only considers the textual content of similarly dated articles to understand the gist of the event. To counteract the lack of analysis on the parties involved, it is essential to come up with a novel framework to gauge the importance of stakeholders and the connection of related events through the relevant entities involved. Therefore, we present SUnSET: Synergistic Understanding of Stakeholder, Events and Time for the task of Timeline Summarization (TLS). We leverage powerful Large Language Models (LLMs) to build SET triplets and introduced the use of stakeholder-based ranking to construct a $Relevancy$ metric, which can be extended into general situations. Our experimental results outperform all prior baselines and emerged as the new State-of-the-Art, highlighting the impact of stakeholder information within news article.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</title>
<link>https://arxiv.org/abs/2507.22025</link>
<guid>https://arxiv.org/abs/2507.22025</guid>
<content:encoded><![CDATA[
arXiv:2507.22025v1 Announce Type: cross 
Abstract: The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE, a comprehensive framework enhancing GUI agents at both the training and inference stages. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a Continuous Reward function to incentivize high-precision grounding; 2) a "Simple Thinking" reward to balance planning with speed and grounding accuracy; and 3) a Cropping-based Resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present Decomposed Grounding with Selection, a novel method that dramatically improves grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2. For instance, using both our proposed training and inference enhancement methods brings 23% grounding accuracy improvement over the best baseline on ScreenSpot-Pro.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UserBench: An Interactive Gym Environment for User-Centric Agents</title>
<link>https://arxiv.org/abs/2507.22034</link>
<guid>https://arxiv.org/abs/2507.22034</guid>
<content:encoded><![CDATA[
arXiv:2507.22034v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)-based agents have made impressive progress in reasoning and tool use, enabling them to solve complex tasks. However, their ability to proactively collaborate with users, especially when goals are vague, evolving, or indirectly expressed, remains underexplored. To address this gap, we introduce UserBench, a user-centric benchmark designed to evaluate agents in multi-turn, preference-driven interactions. UserBench features simulated users who start with underspecified goals and reveal preferences incrementally, requiring agents to proactively clarify intent and make grounded decisions with tools. Our evaluation of leading open- and closed-source LLMs reveals a significant disconnect between task completion and user alignment. For instance, models provide answers that fully align with all user intents only 20% of the time on average, and even the most advanced models uncover fewer than 30% of all user preferences through active interaction. These results highlight the challenges of building agents that are not just capable task executors, but true collaborative partners. UserBench offers an interactive environment to measure and advance this critical capability.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaCLIP 2: A Worldwide Scaling Recipe</title>
<link>https://arxiv.org/abs/2507.22062</link>
<guid>https://arxiv.org/abs/2507.22062</guid>
<content:encoded><![CDATA[
arXiv:2507.22062v1 Announce Type: cross 
Abstract: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The pitfalls of next-token prediction</title>
<link>https://arxiv.org/abs/2403.06963</link>
<guid>https://arxiv.org/abs/2403.06963</guid>
<content:encoded><![CDATA[
arXiv:2403.06963v3 Announce Type: replace 
Abstract: Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.
  As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.
  Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Arithmetic for Language Expansion in Speech Translation</title>
<link>https://arxiv.org/abs/2409.11274</link>
<guid>https://arxiv.org/abs/2409.11274</guid>
<content:encoded><![CDATA[
arXiv:2409.11274v3 Announce Type: replace 
Abstract: Recent progress in large language models (LLMs) has gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-tuned speech translation (ST). However, expanding language pairs is costly due to re-training on combined new and previous datasets. To address this, we aim to build a one-to-many ST system from existing one-to-one ST systems using task arithmetic without re-training. Direct application of task arithmetic in ST leads to language confusion; therefore, we introduce an augmented task arithmetic method incorporating a language control model to ensure correct target language generation. Our experiments on MuST-C and CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains of 8.87 and 11.83. In addition, we demonstrate our framework can extend to language pairs lacking paired ST training data or pre-trained ST models by synthesizing ST models based on existing machine translation (MT) and ST models via task analogies.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulated patient systems are intelligent when powered by large language model-based AI agents</title>
<link>https://arxiv.org/abs/2409.18924</link>
<guid>https://arxiv.org/abs/2409.18924</guid>
<content:encoded><![CDATA[
arXiv:2409.18924v3 Announce Type: replace 
Abstract: Simulated patient systems play an important role in modern medical education and research, providing safe, integrative medical training environments and supporting clinical decision-making simulations. We developed AIPatient, an intelligent simulated patient system powered by large language model-based AI agents. The system incorporates the Retrieval Augmented Generation (RAG) framework, powered by six task-specific LLM-based AI agents for complex reasoning. For simulation reality, the system is also powered by the AIPatient KG (Knowledge Graph), built with de-identified real patient data from the Medical Information Mart for Intensive Care (MIMIC)-III database. Primary outcomes showcase the system's intelligence, including the system's accuracy in Electronic Record (EHR)-based medical Question Answering (QA), readability, robustness, and stability. The system achieved a QA accuracy of 94.15% when all six AI agents present, surpassing benchmarks with partial or no agent integration. Its knowledgebase demonstrated high validity (F1 score=0.89). Readability scores showed median Flesch Reading Ease at 77.23 and median Flesch Kincaid Grade at 5.6, indicating accessibility to all medical professionals. Robustness and stability were confirmed with non-significant variance (ANOVA F-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical students further demonstrated that AIPatient offers high fidelity, strong usability, and effective educational value, performing comparably or better than human-simulated patients in medical history-taking scenarios. The promising intelligence of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data</title>
<link>https://arxiv.org/abs/2410.16491</link>
<guid>https://arxiv.org/abs/2410.16491</guid>
<content:encoded><![CDATA[
arXiv:2410.16491v3 Announce Type: replace 
Abstract: In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in language. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pralekha: Cross-Lingual Document Alignment for Indic Languages</title>
<link>https://arxiv.org/abs/2411.19096</link>
<guid>https://arxiv.org/abs/2411.19096</guid>
<content:encoded><![CDATA[
arXiv:2411.19096v2 Announce Type: replace 
Abstract: Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Most approaches rely on metadata such as URLs, which is often unavailable in low-resource language settings, while others represent documents using pooled sentence embeddings, which fail to capture fine-grained alignment cues. Moreover, current sentence embedding models have limited context windows, hindering their ability to represent document-level information effectively. To address these challenges for Indic languages, we introduce PRALEKHA, a large-scale benchmark for evaluating document-level alignment techniques. It contains over 3 million aligned document pairs across 11 Indic languages and English, of which 1.5 million are English--Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based approaches, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that DAC achieves substantial improvements over pooling-based baselines, particularly in noisy scenarios. Extrinsic evaluation further demonstrates that document MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC's effectiveness for parallel document mining. The PRALEKHA dataset and CLDA evaluation framework will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.18978</link>
<guid>https://arxiv.org/abs/2502.18978</guid>
<content:encoded><![CDATA[
arXiv:2502.18978v4 Announce Type: replace 
Abstract: The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI</title>
<link>https://arxiv.org/abs/2503.04844</link>
<guid>https://arxiv.org/abs/2503.04844</guid>
<content:encoded><![CDATA[
arXiv:2503.04844v5 Announce Type: replace 
Abstract: Here we introduce Narrative Context Protocol (NCP), an open-source narrative standard designed to enable narrative interoperability, AI-driven authoring tools, real-time emergent narratives, and more. By encoding a story's structure in a "Storyform," which is a structured register of its narrative features, NCP enables narrative portability across systems as well as intent-based constraints for generative storytelling systems. We demonstrate the capabilities of NCP through a year-long experiment, during which an author used NCP and a custom authoring platform to create a playable, text-based experience based on her pre-existing novella. This experience is driven by generative AI, with unconstrained natural language input. NCP functions as a set of "guardrails" that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues</title>
<link>https://arxiv.org/abs/2503.06424</link>
<guid>https://arxiv.org/abs/2503.06424</guid>
<content:encoded><![CDATA[
arXiv:2503.06424v2 Announce Type: replace 
Abstract: Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Levels of Analysis for Large Language Models</title>
<link>https://arxiv.org/abs/2503.13401</link>
<guid>https://arxiv.org/abs/2503.13401</guid>
<content:encoded><![CDATA[
arXiv:2503.13401v2 Announce Type: replace 
Abstract: Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on the levels of analysis that David Marr proposed for studying information processing systems. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-CLIP : Learning EEG representations from natural language descriptions</title>
<link>https://arxiv.org/abs/2503.16531</link>
<guid>https://arxiv.org/abs/2503.16531</guid>
<content:encoded><![CDATA[
arXiv:2503.16531v2 Announce Type: replace 
Abstract: Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at https://github.com/tidiane-camaret/EEGClip
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection</title>
<link>https://arxiv.org/abs/2503.20797</link>
<guid>https://arxiv.org/abs/2503.20797</guid>
<content:encoded><![CDATA[
arXiv:2503.20797v2 Announce Type: replace 
Abstract: The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge</title>
<link>https://arxiv.org/abs/2504.00042</link>
<guid>https://arxiv.org/abs/2504.00042</guid>
<content:encoded><![CDATA[
arXiv:2504.00042v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans across historical information. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 197k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. The code, prompts, and model outputs are available on GitHub.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt</title>
<link>https://arxiv.org/abs/2504.04142</link>
<guid>https://arxiv.org/abs/2504.04142</guid>
<content:encoded><![CDATA[
arXiv:2504.04142v2 Announce Type: replace 
Abstract: In this very personal workography, I relate my 40-year experiences as a researcher and educator in and around Artificial Intelligence (AI), more specifically Natural Language Processing. I describe how curiosity, and the circumstances of the day, led me to work in both industry and academia, and in various countries, including The Netherlands (Amsterdam, Eindhoven, and Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and China (Beijing and Harbin). People and anecdotes play a large role in my story; the history of AI forms its backdrop. I focus on things that might be of interest to (even) younger colleagues, given the choices they face in their own work and life at a time when AI is finally emerging from the shadows.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing then Editing Response Personality of Large Language Models</title>
<link>https://arxiv.org/abs/2504.10227</link>
<guid>https://arxiv.org/abs/2504.10227</guid>
<content:encoded><![CDATA[
arXiv:2504.10227v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that simulate consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in simulating personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly simulate personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ai2 Scholar QA: Organized Literature Synthesis with Attribution</title>
<link>https://arxiv.org/abs/2504.10861</link>
<guid>https://arxiv.org/abs/2504.10861</guid>
<content:encoded><![CDATA[
arXiv:2504.10861v2 Announce Type: replace 
Abstract: Retrieval-augmented generation is increasingly effective in answering scientific questions from literature, but many state-of-the-art systems are expensive and closed-source. We introduce Ai2 Scholar QA, a free online scientific question answering application. To facilitate research, we make our entire pipeline public: as a customizable open-source Python package and interactive web app, along with paper indexes accessible through public APIs and downloadable datasets. We describe our system in detail and present experiments analyzing its key design decisions. In an evaluation on a recent scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing systems.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FB-RAG: Improving RAG with Forward and Backward Lookup</title>
<link>https://arxiv.org/abs/2505.17206</link>
<guid>https://arxiv.org/abs/2505.17206</guid>
<content:encoded><![CDATA[
arXiv:2505.17206v2 Announce Type: replace 
Abstract: Traditional Retrieval-Augmented Generation (RAG) struggles with complex queries that lack strong signals to retrieve the most relevant context, forcing a trade-off between choosing a small context that misses key information and a large context that confuses the LLM. To address this, we propose Forward-Backward RAG (FB-RAG), a new training-free framework based on a simple yet powerful forward-looking strategy. FB-RAG employs a light-weight LLM to peek into potential future generations, using evidence from multiple sampled outputs to precisely identify the most relevant context for a final, more powerful generator. This improves performance without complex finetuning or Reinforcement Learning common in prior work. Across 9 datasets, FB-RAG consistently delivers strong results. Further, the performance gains can be achieved with reduced latency due to a shorter, more focused prompt for the powerful generator. On EN.QA dataset, FB-RAG matches the leading baseline with over 48% latency reduction or achieves an 8% performance improvement with a 10% latency reduction. Our analysis finds cases where even when the forward-looking LLM fails to generate correct answers, its attempts are sufficient to guide the final model to an accurate response, demonstrating how smaller LLMs can systematically improve the performance and efficiency of larger ones.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation</title>
<link>https://arxiv.org/abs/2505.20779</link>
<guid>https://arxiv.org/abs/2505.20779</guid>
<content:encoded><![CDATA[
arXiv:2505.20779v4 Announce Type: replace 
Abstract: A hallmark of human innovation is recombination -- the creation of novel ideas by integrating elements from existing concepts and mechanisms. In this work, we introduce CHIMERA, a large-scale Knowledge Base (KB) of over 28K recombination examples automatically mined from the scientific literature. CHIMERA enables large-scale empirical analysis of how scientists recombine concepts and draw inspiration from different areas, and enables training models that propose novel, cross-disciplinary research directions. To construct this KB, we define a new information extraction task: identifying recombination instances in scientific abstracts. We curate a high-quality, expert-annotated dataset and use it to fine-tune a large language model, which we apply to a broad corpus of AI papers. We showcase the utility of CHIMERA through two applications. First, we analyze patterns of recombination across AI subfields. Second, we train a scientific hypothesis generation model using the KB, showing that it can propose novel research directions that researchers rate as inspiring. We release our data and code at https://github.com/noy-sternlicht/CHIMERA-KB.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression</title>
<link>https://arxiv.org/abs/2505.23966</link>
<guid>https://arxiv.org/abs/2505.23966</guid>
<content:encoded><![CDATA[
arXiv:2505.23966v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have enabled remarkable progress in natural language processing, yet their high computational and memory demands pose challenges for deployment in resource-constrained environments. Although recent low-rank decomposition methods offer a promising path for structural compression, they often suffer from accuracy degradation, expensive calibration procedures, and result in inefficient model architectures that hinder real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and accurate, training-free structural compression method based on fine-grained low-rank transformations in the activation space. Specifically, we reduce the hidden dimension by transforming the weights using truncated eigenvectors computed via head-wise Principal Component Analysis, and employ a greedy budget redistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM achieves efficient and effective weight compression without recovery fine-tuning, which could complete the calibration within a few minutes. Evaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural pruning baselines in generalization and downstream performance, while delivering inference speedups over decomposition-based methods.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs</title>
<link>https://arxiv.org/abs/2506.05413</link>
<guid>https://arxiv.org/abs/2506.05413</guid>
<content:encoded><![CDATA[
arXiv:2506.05413v2 Announce Type: replace 
Abstract: We present SmoothRot, a novel post-training quantization technique to enhance the efficiency of 4-bit quantization in Large Language Models (LLMs). SmoothRot addresses the critical challenge of massive activation outliers, by integrating channel-wise scaling with Hadamard transformations. Our technique effectively transforms extreme outliers into quantization-friendly activations, significantly improving quantization accuracy. Experiments conducted on popular LLMs (LLaMA2 7B, LLaMA3.1 8B, and Mistral 7B) demonstrate that SmoothRot consistently reduces the performance gap between quantized and FP16 models by approximately 10-30\% across language generation and zero-shot reasoning tasks, without introducing additional inference latency. Code is available at https://github.com/czakop/smoothrot.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2507.05714</link>
<guid>https://arxiv.org/abs/2507.05714</guid>
<content:encoded><![CDATA[
arXiv:2507.05714v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FrugalRAG: Learning to retrieve and reason for multi-hop QA</title>
<link>https://arxiv.org/abs/2507.07634</link>
<guid>https://arxiv.org/abs/2507.07634</guid>
<content:encoded><![CDATA[
arXiv:2507.07634v2 Announce Type: replace 
Abstract: We consider the problem of answering complex questions, given access to a large unstructured document corpus. The de facto approach to solving the problem is to leverage language models that (iteratively) retrieve and reason through the retrieved documents, until the model has sufficient information to generate an answer. Attempts at improving this approach focus on retrieval-augmented generation (RAG) metrics such as accuracy and recall and can be categorized into two types: (a) fine-tuning on large question answering (QA) datasets augmented with chain-of-thought traces, and (b) leveraging RL-based fine-tuning techniques that rely on question-document relevance signals. However, efficiency in the number of retrieval searches is an equally important metric, which has received less attention. In this work, we show that: (1) Large-scale fine-tuning is not needed to improve RAG metrics, contrary to popular claims in recent literature. Specifically, a standard ReAct pipeline with improved prompts can outperform state-of-the-art methods on benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help RAG from the perspective of frugality, i.e., the latency due to number of searches at inference time. For example, we show that we can achieve competitive RAG metrics at nearly half the cost (in terms of number of searches) on popular RAG benchmarks, using the same base model, and at a small training cost (1000 examples).
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages</title>
<link>https://arxiv.org/abs/2507.11230</link>
<guid>https://arxiv.org/abs/2507.11230</guid>
<content:encoded><![CDATA[
arXiv:2507.11230v2 Announce Type: replace 
Abstract: Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code is available at https://github.com/LyzanderAndrylie/language-specific-features
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Image Captioning via Compact Bidirectional Architecture</title>
<link>https://arxiv.org/abs/2201.01984</link>
<guid>https://arxiv.org/abs/2201.01984</guid>
<content:encoded><![CDATA[
arXiv:2201.01984v2 Announce Type: replace-cross 
Abstract: Most current image captioning models typically generate captions from left-to-right. This unidirectional property makes them can only leverage past context but not future context. Though refinement-based models can exploit both past and future context by generating a new caption in the second stage based on pre-retrieved or pre-generated captions in the first stage, the decoder of these models generally consists of two networks~(i.e. a retriever or captioner in the first stage and a captioner in the second stage), which can only be executed sequentially. In this paper, we introduce a Compact Bidirectional Transformer model for image captioning that can leverage bidirectional context implicitly and explicitly while the decoder can be executed parallelly. Specifically, it is implemented by tightly coupling left-to-right(L2R) and right-to-left(R2L) flows into a single compact model to serve as a regularization for implicitly exploiting bidirectional context and optionally allowing explicit interaction of the bidirectional flows, while the final caption is chosen from either L2R or R2L flow in a sentence-level ensemble manner. We conduct extensive ablation studies on MSCOCO benchmark and find that the compact bidirectional architecture and the sentence-level ensemble play more important roles than the explicit interaction mechanism. By combining with word-level ensemble seamlessly, the effect of sentence-level ensemble is further enlarged. We further extend the conventional one-flow self-critical training to the two-flows version under this architecture and achieve new state-of-the-art results in comparison with non-vision-language-pretraining models. Finally, we verify the generality of this compact bidirectional architecture by extending it to LSTM backbone. Source code is available at https://github.com/YuanEZhou/cbtic.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs</title>
<link>https://arxiv.org/abs/2407.15549</link>
<guid>https://arxiv.org/abs/2407.15549</guid>
<content:encoded><![CDATA[
arXiv:2407.15549v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search</title>
<link>https://arxiv.org/abs/2408.10635</link>
<guid>https://arxiv.org/abs/2408.10635</guid>
<content:encoded><![CDATA[
arXiv:2408.10635v3 Announce Type: replace-cross 
Abstract: Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signs as Tokens: A Retrieval-Enhanced Multilingual Sign Language Generator</title>
<link>https://arxiv.org/abs/2411.17799</link>
<guid>https://arxiv.org/abs/2411.17799</guid>
<content:encoded><![CDATA[
arXiv:2411.17799v3 Announce Type: replace-cross 
Abstract: Sign language is a visual language that encompasses all linguistic features of natural languages and serves as the primary communication method for the deaf and hard-of-hearing communities. Although many studies have successfully adapted pretrained language models (LMs) for sign language translation (sign-to-text), the reverse task-sign language generation (text-to-sign)-remains largely unexplored. In this work, we introduce a multilingual sign language model, Signs as Tokens (SOKE), which can generate 3D sign avatars autoregressively from text inputs using a pretrained LM. To align sign language with the LM, we leverage a decoupled tokenizer that discretizes continuous signs into token sequences representing various body parts. During decoding, unlike existing approaches that flatten all part-wise tokens into a single sequence and predict one token at a time, we propose a multi-head decoding method capable of predicting multiple tokens simultaneously. This approach improves inference efficiency while maintaining effective information fusion across different body parts. To further ease the generation process, we propose a retrieval-enhanced SLG approach, which incorporates external sign dictionaries to provide accurate word-level signs as auxiliary conditions, significantly improving the precision of generated signs. Extensive qualitative and quantitative evaluations demonstrate the effectiveness of SOKE.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning</title>
<link>https://arxiv.org/abs/2412.03248</link>
<guid>https://arxiv.org/abs/2412.03248</guid>
<content:encoded><![CDATA[
arXiv:2412.03248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have enabled the creation of multi-modal LLMs that exhibit strong comprehension of visual data such as images and videos. However, these models usually rely on extensive visual tokens from visual encoders, leading to high computational demands, which limits their applicability in resource-constrained environments and for long-context tasks. In this work, we propose a training-free adaptive inference method for multi-modal LLMs that can accommodate a broad range of efficiency requirements with a minimum performance drop. Our method consists of a) iterative token merging based on embedding similarity before LLMs, and b) progressive token pruning within LLM layers based on multi-modal importance. With a minimalist design, our method can be applied to both video and image LLMs. Extensive experiments on diverse video and image benchmarks demonstrate that our method substantially reduces computation load (e.g., a $\textbf{7-fold}$ reduction in FLOPs) while preserving the performance of video and image LLMs. Further, at a similar computational cost, our method outperforms the state-of-the-art methods in long video understanding (e.g., $\textbf{+4.6}$ on MLVU). Additionally, our in-depth analysis provides insights into token redundancy and LLM layer behaviors, offering guidance for future research in designing efficient multi-modal LLMs. Our code is available at https://github.com/LaVi-Lab/AIM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKE: Steering Activations for Knowledge Editing</title>
<link>https://arxiv.org/abs/2503.01751</link>
<guid>https://arxiv.org/abs/2503.01751</guid>
<content:encoded><![CDATA[
arXiv:2503.01751v2 Announce Type: replace-cross 
Abstract: As Large Langue Models have been shown to memorize real-world facts, the need to update this knowledge in a controlled and efficient manner arises. Designed with these constraints in mind, Knowledge Editing (KE) approaches propose to alter specific facts in pretrained models. However, they have been shown to suffer from several limitations, including their lack of contextual robustness and their failure to generalize to logical implications related to the fact. To overcome these issues, we propose SAKE, a steering activation method that models a fact to be edited as a distribution rather than a single prompt. Leveraging Optimal Transport, SAKE alters the LLM behavior over a whole fact-related distribution, defined as paraphrases and logical implications. Several numerical experiments demonstrate the effectiveness of this method: SAKE is thus able to perform more robust edits than its existing counterparts.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQuat: Subspace-orthogonal KV Cache Quantization</title>
<link>https://arxiv.org/abs/2503.24358</link>
<guid>https://arxiv.org/abs/2503.24358</guid>
<content:encoded><![CDATA[
arXiv:2503.24358v2 Announce Type: replace-cross 
Abstract: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLAMAPIE: Proactive In-Ear Conversation Assistants</title>
<link>https://arxiv.org/abs/2505.04066</link>
<guid>https://arxiv.org/abs/2505.04066</guid>
<content:encoded><![CDATA[
arXiv:2505.04066v2 Announce Type: replace-cross 
Abstract: We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v2 Announce Type: replace-cross 
Abstract: Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is effective and has attracted significant attention. However, it is computationally prohibitive due to massive, data-hungry text-based reward models. By changing the data source from text to hidden states, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight technique that leverages the rich information embedded in LLM hidden states to address these issues, which operates on token-level and consists of only linear layers. Extensive experiments show that SWIFT outperforms baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training, demonstrating significant efficiency improvement. SWIFT's robust scalability, applicability to some closed-source models via logits, and ability to be combined with traditional reward models to yield further performance gains underscore its practical value.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models</title>
<link>https://arxiv.org/abs/2506.01413</link>
<guid>https://arxiv.org/abs/2506.01413</guid>
<content:encoded><![CDATA[
arXiv:2506.01413v5 Announce Type: replace-cross 
Abstract: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.
  Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation</title>
<link>https://arxiv.org/abs/2506.09081</link>
<guid>https://arxiv.org/abs/2506.09081</guid>
<content:encoded><![CDATA[
arXiv:2506.09081v3 Announce Type: replace-cross 
Abstract: We present FlagEvalMM, an open-source evaluation framework designed to comprehensively assess multimodal models across a diverse range of vision-language understanding and generation tasks, such as visual question answering, text-to-image/video generation, and image-text retrieval. We decouple model inference from evaluation through an independent evaluation service, thus enabling flexible resource allocation and seamless integration of new tasks and models. Moreover, FlagEvalMM utilizes advanced inference acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to significantly enhance evaluation efficiency. Extensive experiments show that FlagEvalMM offers accurate and efficient insights into model strengths and limitations, making it a valuable tool for advancing multimodal research. The framework is publicly accessible at https://github.com/flageval-baai/FlagEvalMM.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLR: Automated Synthesis for Scalable Logical Reasoning</title>
<link>https://arxiv.org/abs/2506.15787</link>
<guid>https://arxiv.org/abs/2506.15787</guid>
<content:encoded><![CDATA[
arXiv:2506.15787v3 Announce Type: replace-cross 
Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR automatically synthesizes (i) an instruction prompt for an inductive reasoning task, (ii) a validation program, executable on model outputs to provide verifiable rewards, and (iii) the latent ground-truth rule. This process is fully automated, scalable, requires no human annotations, and offers precise control over task difficulty. Using SLR, we create SLR-Bench, a benchmark comprising 19k prompts organized into 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs demonstrate improved performance but incur very high test-time computation, with costs exceeding $300 for just 1,000 prompts. Finally, curriculum learning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. Moreover, these reasoning capabilities generalize to a wide range of established benchmarks, underscoring the effectiveness of SLR for downstream reasoning.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v2 Announce Type: replace-cross 
Abstract: Political Compass Test (PCT) or similar questionnaires have been used to quantify LLM's political leanings. Building on a recent line of work that examines the validity of PCT tests, we demonstrate that variation in standard generation parameters does not significantly impact the models' PCT scores. However, external factors such as prompt variations and fine-tuning individually and in combination affect the same. Finally, we demonstrate that when models are fine-tuned on text datasets with higher political content than others, the PCT scores are not differentially affected. This calls for a thorough investigation into the validity of PCT and similar tests, as well as the mechanism by which political leanings are encoded in LLMs.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models</title>
<link>https://arxiv.org/abs/2507.08128</link>
<guid>https://arxiv.org/abs/2507.08128</guid>
<content:encoded><![CDATA[
arXiv:2507.08128v2 Announce Type: replace-cross 
Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets.
]]></content:encoded>
<pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training</title>
<link>https://arxiv.org/abs/2507.09205</link>
<guid>https://arxiv.org/abs/2507.09205</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Tibetan, pre-training corpus, data cleaning, multilingual base model

Summary: 
Large language models have made significant advancements in various languages, but Tibetan, a low-resource language, lacks representation due to limited training data. To address this issue, a comprehensive Tibetan pre-training corpus has been curated from multiple sources and processed using a specialized pipeline. By leveraging this data, a multilingual model has been further trained to enhance its capabilities in generating Tibetan text. New high-quality Tibetan benchmarks have been created to evaluate the model's performance, in addition to existing benchmarks. Experimental results show that the model consistently outperforms open-source and Tibetan-specific models in various tasks. The curated corpus, dedicated data cleaning, and tailored pre/post-training of the model have collectively contributed to significant improvements in the model's performance on Tibetan language tasks.<br /><br />Summary: <div>
arXiv:2507.09205v4 Announce Type: replace 
Abstract: Large language models have achieved remarkable progress across many languages. However, Tibetan, as a representative low-resource language, is particularly underrepresented in existing models due to the scarcity of high-quality training corpora. To address this gap, we curate the largest Tibetan pre-training corpus to date, aggregating data from diverse sources and applying a dedicated data cleaning and processing pipeline tailored for Tibetan. With the curated data, we continue pre/post-training a multilingual base model to enhance its generative capabilities in Tibetan. To evaluate the Tibetan capabilities of the model, we create new high-quality Tibetan benchmarks, and complement them with existing public benchmarks. Experimental results demonstrate that our model consistently and significantly outperforms both open-source models of similar scale and Tibetan-tailored models across a wide range of tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media</title>
<link>https://arxiv.org/abs/2507.19511</link>
<guid>https://arxiv.org/abs/2507.19511</guid>
<content:encoded><![CDATA[
<div> transformer models, mental health disorders, Natural Language Processing, LSTM, BERT embeddings <br />
<br />
Summary: 
The study evaluates transformer models against LSTM for mental health disorder classification on Reddit. Transformer models like RoBERTa show superior performance, achieving high F1 scores on test sets. LSTM models with BERT embeddings also perform well with lower computational resources. The research emphasizes the potential of transformer-based models for real-time mental health monitoring and highlights their effectiveness in text analysis. The study validates the annotated dataset through statistical analysis and topic modeling, providing insights into the capabilities and limitations of NLP methodologies in mental disorder detection. Overall, the findings suggest the significance of automated tools in early detection and monitoring of mental health disorders and offer valuable implications for clinical applications and digital mental health interventions. <br /><br /> <div>
arXiv:2507.19511v1 Announce Type: new 
Abstract: The rising prevalence of mental health disorders necessitates the development of robust, automated tools for early detection and monitoring. Recent advances in Natural Language Processing (NLP), particularly transformer-based architectures, have demonstrated significant potential in text analysis. This study provides a comprehensive evaluation of state-of-the-art transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term Memory (LSTM) based approaches using different text embedding techniques for mental health disorder classification on Reddit. We construct a large annotated dataset, validating its reliability through statistical judgmental analysis and topic modeling. Experimental results demonstrate the superior performance of transformer models over traditional deep-learning approaches. RoBERTa achieved the highest classification performance, with a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set. Notably, LSTM models augmented with BERT embeddings proved highly competitive, achieving F1 scores exceeding 94% on the external dataset while requiring significantly fewer computational resources. These findings highlight the effectiveness of transformer-based models for real-time, scalable mental health monitoring. We discuss the implications for clinical applications and digital mental health interventions, offering insights into the capabilities and limitations of state-of-the-art NLP methodologies in mental disorder detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables</title>
<link>https://arxiv.org/abs/2507.19521</link>
<guid>https://arxiv.org/abs/2507.19521</guid>
<content:encoded><![CDATA[
<div> Keywords: academic literature, large language models, schema generation, table intents, LLM-based schema editing<br />
Summary:<br />
- The study addresses the challenges in organizing and comparing academic literature using Large Language Models (LLMs).<br />
- Ambiguity in reference-based evaluations and lack of editing/refinement methods have hindered progress in schema generation.<br />
- The approach involves augmenting unannotated table corpora with synthesized intents to reduce ambiguity and improve baseline performance in reconstructing reference schemas.<br />
- Benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, shows that smaller, open-weight models can be competitive with state-of-the-art prompted LLMs.<br />
- LLM-based schema editing techniques are proposed to further enhance the schemas generated by these methods.<br /> 

Summary: <br />
The study focuses on enhancing the organization and comparison of academic literature through the use of Large Language Models (LLMs). By addressing issues such as ambiguity in evaluations and lack of editing methods, the research presents an approach that incorporates table intents to improve schema generation performance. Benchmarking various schema generation methods reveals the competitiveness of smaller, open-weight models with state-of-the-art LLMs, while LLM-based schema editing techniques are introduced to refine the generated schemas further. <div>
arXiv:2507.19521v1 Announce Type: new 
Abstract: The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with synthesized intents and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. Next, we propose several LLM-based schema editing techniques. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Then we demonstrate that our editing techniques can further improve schemas generated by these methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri</title>
<link>https://arxiv.org/abs/2507.19537</link>
<guid>https://arxiv.org/abs/2507.19537</guid>
<content:encoded><![CDATA[
<div> Keywords: WOKIE, SKOS thesauri, Digital Humanities, machine translation, ontology matching

Summary: 
WOKIE is a new open-source pipeline designed to automate the translation of SKOS thesauri in the field of Digital Humanities. This tool addresses the issue of language diversity limiting access and semantic interoperability of knowledge resources. By utilizing external translation services and Large Language Models (LLMs), WOKIE balances translation quality, scalability, and cost. It is user-friendly, requires no prior expertise, and can be easily extended. The evaluation of WOKIE on various thesauri in 15 languages showed promising results in translation quality, performance, and ontology matching improvements. This tool facilitates the enhancement of accessibility, reuse, and cross-lingual interoperability of thesauri, supporting inclusive and multilingual research infrastructures. 

<br /><br />Summary: <div>
arXiv:2507.19537v1 Announce Type: new 
Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for the automated translation of SKOS thesauri. This work addresses a critical need in the Digital Humanities (DH), where language diversity can limit access, reuse, and semantic interoperability of knowledge resources. WOKIE combines external translation services with targeted refinement using Large Language Models (LLMs), balancing translation quality, scalability, and cost. Designed to run on everyday hardware and be easily extended, the application requires no prior expertise in machine translation or LLMs. We evaluate WOKIE across several DH thesauri in 15 languages with different parameters, translation services and LLMs, systematically analysing translation quality, performance, and ontology matching improvements. Our results show that WOKIE is suitable to enhance the accessibility, reuse, and cross-lingual interoperability of thesauri by hurdle-free automated translation and improved ontology matching performance, supporting more inclusive and multilingual research infrastructures.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning</title>
<link>https://arxiv.org/abs/2507.19586</link>
<guid>https://arxiv.org/abs/2507.19586</guid>
<content:encoded><![CDATA[
<div> evaluation framework, geospatial knowledge, geospatial hallucinations, factuality aligning method, LLMs<br />
Summary:<br />
The article introduces a new evaluation framework for geospatial hallucinations in Large Language Models (LLMs). It leverages structured geospatial knowledge graphs to assess inaccuracies in geospatial information generated by LLMs. The study evaluates 20 advanced LLMs and identifies geospatial hallucinations in their knowledge. A dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) is proposed to mitigate these hallucinations, resulting in a significant performance improvement of over 29.6% on the benchmark. Experimental results validate the efficacy of the framework and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks. This comprehensive approach addresses the issue of geospatial hallucinations and improves the reliability of LLMs in handling geospatial tasks. <br />Summary: <div>
arXiv:2507.19586v1 Announce Type: new 
Abstract: Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Attention Mechanisms for Large Language Models: A Survey</title>
<link>https://arxiv.org/abs/2507.19595</link>
<guid>https://arxiv.org/abs/2507.19595</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based architectures, efficient attention mechanisms, linear attention, sparse attention, language models

Summary:
This article discusses the challenges posed by the quadratic time and memory complexity of self-attention in Transformer-based architectures and explores two categories of efficient attention mechanisms: linear attention and sparse attention. Linear attention methods achieve linear complexity through various techniques, such as kernel approximations and fast weight dynamics, while sparse attention techniques limit attention computation to selected subsets of tokens. The survey provides a comprehensive overview of these developments, considering both algorithmic innovations and hardware-level considerations. It also examines the integration of efficient attention into large-scale pre-trained language models, including architectures built entirely on efficient attention and hybrid designs combining local and global components. By bridging theoretical foundations with practical deployment strategies, the article serves as a foundational reference for improving the design of scalable and efficient language models.<br /><br />Summary: <div>
arXiv:2507.19595v1 Announce Type: new 
Abstract: Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?</title>
<link>https://arxiv.org/abs/2507.19598</link>
<guid>https://arxiv.org/abs/2507.19598</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, code generation, adversarial misuse, code decomposition attacks, benchmark

Summary:<br />
Recent advancements in Large Language Models have improved their ability to generate code, but their vulnerability to adversarial attacks, particularly multi-turn malicious prompts, has not been extensively studied. This work introduces code decomposition attacks, where a malicious coding task is split into smaller subtasks across multiple conversational turns to evade detection. A benchmark called MOCHA is introduced to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Results show persistent vulnerabilities, especially in multi-turn scenarios. Fine-tuning on MOCHA helps improve rejection rates while maintaining coding ability. This fine-tuning also enhances robustness on external adversarial datasets, with rejection rates increasing by up to 32.4% without additional supervision.<br /><br />Summary: <div>
arXiv:2507.19598v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track</title>
<link>https://arxiv.org/abs/2507.19616</link>
<guid>https://arxiv.org/abs/2507.19616</guid>
<content:encoded><![CDATA[
<div> Keywords: IWSLT 2025, speech-to-text translation, low-resource scenario, Whisper ASR model, Krutrim language model

Summary:
- HITSZ participated in the IWSLT 2025 Indic track focusing on speech-to-text translation for English-to-Indic and Indic-to-English language pairs.
- They proposed an end-to-end system combining the Whisper automated speech recognition model with the Indic-specialized large language model Krutrim to improve translation quality in the low-resource setting.
- Experimental results showed average BLEU scores of $28.88$ for English-to-Indic and $27.86$ for Indic-to-English translations.
- The Chain-of-Thought (CoT) method was explored for enhancing translation quality further, with a notable $13.84$ BLEU increase observed for Tamil-to-English translations.
- Challenges were identified in maintaining the required CoT output format consistently across translations. 

<br /><br />Summary: <div>
arXiv:2507.19616v1 Announce Type: new 
Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language pairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system integrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-specialized large language model (LLM). Experimental results demonstrate that our end-to-end system achieved average BLEU scores of $28.88$ for English-to-Indic directions and $27.86$ for Indic-to-English directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. While this method showed potential for significant translation quality improvements on successfully parsed outputs (e.g. a $13.84$ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model consistently adheres to the required CoT output format.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks</title>
<link>https://arxiv.org/abs/2507.19634</link>
<guid>https://arxiv.org/abs/2507.19634</guid>
<content:encoded><![CDATA[
<div> multimodal LLMs, evaluation, multilingual, instruction-following, benchmark
Summary: 
- Recent advances in large language models have led to the development of multimodal LLMs integrating text, speech, and vision.
- Challenges remain in evaluating the multilingual and multimodal capabilities of these models over long and short contexts.
- Existing benchmarks are limited in scope, often focusing on one modality or lacking human annotations.
- To address these limitations, the MCIF benchmark is introduced, designed for evaluating instruction-following in crosslingual, multimodal settings across languages and modalities.
- MCIF spans speech, vision, and text modalities in English, German, Italian, and Chinese, providing a comprehensive assessment of MLLMs' abilities. 

<br /><br />Summary: <div>
arXiv:2507.19634v1 Announce Type: new 
Abstract: Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations--hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities--speech, vision, and text--and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams</title>
<link>https://arxiv.org/abs/2507.19666</link>
<guid>https://arxiv.org/abs/2507.19666</guid>
<content:encoded><![CDATA[
<div> Multimodal dataset, Romanian, driving law, Large Language Models, Vision-Language Models <br />
Summary: <br />
The study evaluates the use of Large Language Models and Vision-Language Models in understanding Romanian driving law through text and visual question-answering tasks. The researchers introduce a new dataset, RoD-TAL, consisting of Romanian driving test questions, text-based and image-based, with legal references and explanations. They test retrieval-augmented generation pipelines, dense retrievers, and reasoning-optimized models across various tasks such as Information Retrieval, Question Answering, Visual Information Retrieval, and Visual Question Answering. Fine-tuning in the domain improves retrieval performance, while chain-of-thought prompting and specialized reasoning models enhance question-answering accuracy. The results show that specialized models can surpass the minimum grades required to pass driving exams. However, visual reasoning remains a challenge, highlighting the potential and limitations of using LLMs and VLMs in legal education. <br /> <div>
arXiv:2507.19666v1 Announce Type: new 
Abstract: The intersection of AI and legal systems presents a growing need for tools that support legal education, particularly in under-resourced languages such as Romanian. In this work, we aim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning about Romanian driving law through textual and visual question-answering tasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian driving test questions, text-based and image-based, alongside annotated legal references and human explanations. We implement and assess retrieval-augmented generation (RAG) pipelines, dense retrievers, and reasoning-optimized models across tasks including Information Retrieval (IR), Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning significantly enhances retrieval performance. At the same time, chain-of-thought prompting and specialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass driving exams. However, visual reasoning remains challenging, highlighting the potential and the limitations of applying LLMs and VLMs to legal education.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks</title>
<link>https://arxiv.org/abs/2507.19699</link>
<guid>https://arxiv.org/abs/2507.19699</guid>
<content:encoded><![CDATA[
<div> benchmarking, multilingual, monolingual, large language models, performance

Summary:
Multilingual and monolingual Large Language Models (LLMs) were benchmarked across Arabic, English, and Indic languages, with a focus on model compression techniques such as pruning and quantization. Various state-of-the-art LLMS like BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2 were evaluated, revealing performance variations influenced by linguistic diversity and resource availability. Multilingual models exhibited superior performance compared to language-specific models, showcasing significant cross-lingual transfer benefits. Quantization proved effective in maintaining model accuracy and efficiency, particularly with 4-bit and 8-bit precision, while aggressive pruning negatively impacted model performance, especially in larger models. The study identified crucial strategies for developing scalable and equitable multilingual NLP solutions and emphasized the importance of addressing hallucination and generalization errors in low-resource environments. 

<br /><br />Summary: <div>
arXiv:2507.19699v1 Announce Type: new 
Abstract: Although LLMs have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as pruning and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive pruning significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs</title>
<link>https://arxiv.org/abs/2507.19710</link>
<guid>https://arxiv.org/abs/2507.19710</guid>
<content:encoded><![CDATA[
<div> Keywords: Table-to-Text generation, Subjectivity, RDF triples, T5 models, Factual accuracy<br />
Summary: 
- Existing approaches in Table-to-Text (T2T) generation focus on objective descriptions of tabular data.
- This work introduces a novel pipeline that generates both objective and subjective text from tables.
- The pipeline comprises three stages: RDF triple extraction, narrative aggregation, and infusion of subjectivity.
- By incorporating RDFs, the approach enhances factual accuracy while maintaining interpretability.
- Smaller, fine-tuned T5 models are used in the pipeline, achieving comparable performance to large language models like GPT-3.5.
- The approach outperforms Mistral-7B and Llama-2 in several metrics, emphasizing a balance between factual accuracy and subjective interpretation.
- The structured pipeline proposed in this work is the first to integrate intermediate representations to enhance both factual correctness and subjectivity.<br /><br />Summary: <div>
arXiv:2507.19710v1 Announce Type: new 
Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on providing objective descriptions of tabular data. However, generating text that incorporates subjectivity, where subjectivity refers to interpretations beyond raw numerical data, remains underexplored. To address this, we introduce a novel pipeline that leverages intermediate representations to generate both objective and subjective text from tables. Our three-stage pipeline consists of: 1) extraction of Resource Description Framework (RDF) triples, 2) aggregation of text into coherent narratives, and 3) infusion of subjectivity to enrich the generated text. By incorporating RDFs, our approach enhances factual accuracy while maintaining interpretability. Unlike large language models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5 and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our approach through quantitative and qualitative analyses, demonstrating its effectiveness in balancing factual accuracy with subjective interpretation. To the best of our knowledge, this is the first work to propose a structured pipeline for T2T generation that integrates intermediate representations to enhance both factual correctness and subjectivity.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Reading Distillation</title>
<link>https://arxiv.org/abs/2507.19741</link>
<guid>https://arxiv.org/abs/2507.19741</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Distillation, Basic Reading Education, Named Entity Recognition, Language Inference<br />
Summary: <br />
This paper introduces the concept of basic reading distillation (BRD) as a new technique to enhance the capabilities of small models to imitate large language models' (LLMs) basic reading behaviors. BRD focuses on educating small models on generic texts, unrelated to specific downstream tasks, to improve their performance. By training small models to imitate LLMs' abilities such as named entity recognition, question answering, and more, the study demonstrates that these small models can surpass or match the performance of significantly larger LLMs. The analysis shows that BRD impacts the probability distribution of the small model effectively and is distinct from traditional methods like knowledge distillation or task distillation. This research highlights the potential of BRD in improving the efficiency and effectiveness of language models in various natural language processing tasks. <br /> <div>
arXiv:2507.19741v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in real-world. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are \emph{unrelated} to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small model, and has orthogonality to either knowledge distillation or task distillation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2507.19748</link>
<guid>https://arxiv.org/abs/2507.19748</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial general intelligence, Large Language Models, mathematical reasoning, JT-Math-8B, supervised fine-tuning<br />
<br />
Summary: 
The article introduces JT-Math-8B, a series of open-source models designed to improve mathematical reasoning abilities of Large Language Models (LLMs). These models address the shortcomings of existing models by incorporating a systematic optimization framework and a high-quality pre-training corpus. The Instruct Model focuses on providing direct answers through Supervised Fine-Tuning (SFT) and reinforcement learning (RL). On the other hand, the Thinking Model is trained for complex problem-solving using a Long Chain-of-Thought (Long CoT) approach, with a multi-stage RL curriculum. JT-Math-8B outperforms models like OpenAI's O1-mini and GPT-4o, showcasing superior performance in competition-level mathematics. By surpassing existing models and demonstrating proficiency in deep conceptual understanding and multi-step deliberation, JT-Math-8B contributes significantly to the advancement of artificial general intelligence. <br /><br />Summary: <div>
arXiv:2507.19748v1 Announce Type: new 
Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence and a primary benchmark for evaluating the capabilities of Large Language Models (LLMs). While state-of-the-art models show promise, they often falter when faced with complex problems that demand deep conceptual understanding and intricate, multi-step deliberation. To address this challenge, we introduce JT-Math-8B, a series of open-source models comprising base, instruct, and thinking versions, built upon a systematic, multi-stage optimization framework. Our pre-training corpus is a high-quality, 210B-token dataset curated through a dedicated data pipeline that uses model-based validation to ensure quality and diversity. The Instruct Model is optimized for direct, concise answers through Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL) method. The Thinking Model is trained for complex problem-solving using a Long Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage RL curriculum that progressively increases task difficulty and context length up to 32K tokens. JT-Math-8B achieves state-of-the-art results among open-source models of similar size, surpassing prominent models like OpenAI's O1-mini and GPT-4o , and demonstrating superior performance on competition-level mathematics.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs</title>
<link>https://arxiv.org/abs/2507.19756</link>
<guid>https://arxiv.org/abs/2507.19756</guid>
<content:encoded><![CDATA[
<div> Evangelical movement, Christian Fiction, computational tools, divine acts, Left Behind series<br />
<br />
Summary:<br />
The article explores the cultural and literary aspects of the American Evangelical movement, focusing on Christian Fiction as a genre. Using computational tools and human annotators, the study delves into how authors depict divine acts in Christian Fiction. By developing definitions and a codebook for "acts of God" and utilizing a lightweight language model, the research reveals significant differences between the popular Left Behind series and Christian Fiction as a whole. Additionally, the study uncovers disparities between male and female authors in their depiction of divine acts. This analysis sheds light on the nuanced and varied portrayals of spiritual themes in Christian Fiction, highlighting the diversity within the genre and providing valuable insights for future research. <br /> <div>
arXiv:2507.19756v1 Announce Type: new 
Abstract: In addition to its more widely studied political activities, the American Evangelical movement has a well-developed but less externally visible cultural and literary side. Christian Fiction, however, has been little studied, and what scholarly attention there is has focused on the explosively popular Left Behind series. In this work, we use computational tools to provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration of how its authors depict divine acts. Working with human annotators we first developed definitions and a codebook for "acts of God." We then adapted those instructions designed for human annotators for use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is capable of matching human annotations, even when the task is subtle and challenging. Using these annotations, we show that significant and meaningful differences exist between the Left Behind books and Christian Fiction more broadly and between books by male and female authors.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities</title>
<link>https://arxiv.org/abs/2507.19766</link>
<guid>https://arxiv.org/abs/2507.19766</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reinforcement learning, ultra-long output, reasoning abilities, dynamic masking

Summary: 
Ultra-Long Output Reinforcement Learning (UloRL) proposes a novel approach to enhancing the reasoning capabilities of large language models (LLMs) by addressing challenges in handling ultra-long outputs. By dividing long output decoding into shorter segments, UloRL significantly improves training efficiency by mitigating delays associated with long-tail sequence distributions. Dynamic masking of well-Mastered Positive Tokens (MPTs) is introduced to prevent entropy collapse during training. Experimental results show that the method achieves a 2.06x increase in training speed and boosts model performance on various tasks, surpassing even larger models. The findings indicate the potential of UloRL to advance LLM reasoning capabilities with ultra-long sequence generation.

<br /><br />Summary: <div>
arXiv:2507.19766v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\% to 85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flora: Effortless Context Construction to Arbitrary Length and Scale</title>
<link>https://arxiv.org/abs/2507.19786</link>
<guid>https://arxiv.org/abs/2507.19786</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context construction, Large Language Models, Flora, Instruction tuning, Context diversity

Summary:
Flora is a new approach that aims to improve the performance of Large Language Models (LLMs) in handling long contexts without the need for human intervention or specialized LLM modifications. It achieves this by assembling short instructions based on categories and instructing LLMs with long-context meta-instructions to generate responses. Flora can generate contexts of varying lengths with rich diversity while maintaining strong performance in short-context tasks. Experimental results on Llama3-8B-Instruct and QwQ-32B benchmarks demonstrate the effectiveness of Flora in enhancing the long-context performance of LLMs. This approach addresses the challenges associated with long-context handling in LLMs, such as computational demands and forgetting of short-context abilities, providing a more efficient and scalable solution. The code for data construction is also available for reference on GitHub. 

<br /><br />Summary: <div>
arXiv:2507.19786v1 Announce Type: new 
Abstract: Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</title>
<link>https://arxiv.org/abs/2507.19823</link>
<guid>https://arxiv.org/abs/2507.19823</guid>
<content:encoded><![CDATA[
<div> key quantization, value offloading, dynamic KV eviction, heterogeneous attention computation, memory constraints <br />
Summary:
HCAttention is a new framework proposed for processing long-context inputs with large language models. It addresses the challenge of high memory requirements for the Key-Value cache during inference by integrating key quantization, value offloading, and dynamic KV eviction. This allows for efficient inference under extreme memory constraints without the need for model fine-tuning. Experimental results on the LongBench benchmark show that HCAttention maintains accuracy while reducing the KV cache memory footprint to 25% of its original size. It achieves competitive performance with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. HCAttention extends the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory, making it the first of its kind. <div>
arXiv:2507.19823v1 Announce Type: new 
Abstract: Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments</title>
<link>https://arxiv.org/abs/2507.19867</link>
<guid>https://arxiv.org/abs/2507.19867</guid>
<content:encoded><![CDATA[
<div> Keywords: in-car conversational AI, DiscoDrive, synthetic corpus, disfluencies, training resource

Summary:
DiscoDrive is a synthetic corpus consisting of 3500 multi-turn dialogs across seven automotive domains, capturing spontaneous disfluencies like hesitations and repetitions. It addresses the lack of real-world driver-AI dialogs in existing datasets. The corpus enables models like DialoGPT-Medium and T5-Base to outperform KVRET-trained models on relevant test sets, showing improvements in various evaluation metrics. When used as a data augmentation resource in low-resource scenarios, DiscoDrive enhances model performance even further. Human evaluations indicate that dialogs from DiscoDrive are rated higher in naturalness and coherence compared to human-collected dialogs from KVRET. Additionally, the dialogs are perceived as more context-appropriate than existing post-hoc methods like LARD, without sacrificing clarity. This versatile corpus fills a crucial gap in existing resources and supports the training and augmentation of conversational AI systems for handling real-world in-car interactions effectively. 

<br /><br />Summary: DiscoDrive is a synthetic corpus of in-car conversational AI dialogs that integrates disfluencies, enhancing model performance and human evaluations. <div>
arXiv:2507.19867v1 Announce Type: new 
Abstract: In-car conversational AI is becoming increasingly critical as autonomous vehicles and smart assistants gain widespread adoption. Yet, existing datasets fail to capture the spontaneous disfluencies such as hesitations, false starts, repetitions, and self-corrections that characterize real driver-AI dialogs. To address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn dialogs across seven automotive domains, generated using a two-stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis. We show that DiscoDrive is effective both as a training resource, enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4 improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1 improvements of 1.35 to 3.48), and as a data augmentation resource in low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38, METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10 percent of KVRET. Human evaluations further confirm that dialogs sampled from DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness (3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more context-appropriate than leading post-hoc methods (such as LARD), without compromising clarity. DiscoDrive fills a critical gap in existing resources and serves as a versatile corpus for both training and augmenting conversational AI, enabling robust handling of real-world, disfluent in-car interactions.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment</title>
<link>https://arxiv.org/abs/2507.19869</link>
<guid>https://arxiv.org/abs/2507.19869</guid>
<content:encoded><![CDATA[
<div> Polish Vocabulary Size Test, PVST, Item Response Theory, Computerized Adaptive Testing, native speakers, non-native speakers<br />
Summary:<br />
The Polish Vocabulary Size Test (PVST) is a new tool for assessing receptive vocabulary of Polish speakers, utilizing Item Response Theory and Computerized Adaptive Testing. In a pilot study with 1,475 participants, native Polish speakers demonstrated larger vocabularies than non-native speakers. Additionally, vocabulary size in native speakers correlated positively with age. The PVST is available online at myvocab.info/pl for easy access and administration. <div>
arXiv:2507.19869v1 Announce Type: new 
Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing the receptive vocabulary size of both native and non-native Polish speakers. Based on Item Response Theory and Computerized Adaptive Testing, PVST dynamically adjusts to each test-taker's proficiency level, ensuring high accuracy while keeping the test duration short. To validate the test, a pilot study was conducted with 1.475 participants. Native Polish speakers demonstrated significantly larger vocabularies compared to non-native speakers. For native speakers, vocabulary size showed a strong positive correlation with age. The PVST is available online at myvocab.info/pl.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam</title>
<link>https://arxiv.org/abs/2507.19885</link>
<guid>https://arxiv.org/abs/2507.19885</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial intelligence, Multimodal Large Language Models, Medical applications, Language disparities, Healthcare.

Summary:
- The study evaluates the performance of various Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) in answering medical questions in Brazilian spoken Portuguese.
- Six LLMs and four MLLMs were tested against human candidates using questions from a medical residency entrance exam.
- Some models, such as Claude-3.5-Sonnet and Claude-3-Opus, showed accuracy levels similar to human candidates.
- Performance gaps were noted, especially in answering multimodal questions involving image interpretation.
- The study highlights language disparities and the need for further fine-tuning and data augmentation in non-English medical AI applications.
<br /><br />Summary: 
The study compared the performance of various LLMs and MLLMs in answering medical questions in Brazilian Portuguese. While some models performed well, gaps remained in handling multimodal questions. Language disparities were evident, underscoring the need for additional training and data augmentation in non-English medical AI applications. This research emphasizes the importance of evaluating generative AI in diverse linguistic and clinical contexts for its reliable deployment in healthcare. Future studies should focus on enhancing training methods, multimodal reasoning, and integrating AI in real-world clinical settings. <div>
arXiv:2507.19885v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has shown the potential to revolutionize healthcare by improving diagnostic accuracy, optimizing workflows, and personalizing treatment plans. Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have achieved notable advancements in natural language processing and medical applications. However, the evaluation of these models has focused predominantly on the English language, leading to potential biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo, LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet, and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese from the medical residency entrance exam of the Hospital das Cl\'inicas da Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest health complex in South America. The performance of the models was benchmarked against human candidates, analyzing accuracy, processing time, and coherence of the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and Claude-3-Opus, achieved accuracy levels comparable to human candidates, performance gaps persist, particularly in multimodal questions requiring image interpretation. Furthermore, the study highlights language disparities, emphasizing the need for further fine-tuning and data set augmentation for non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various linguistic and clinical settings to ensure a fair and reliable deployment in healthcare. Future research should explore improved training methodologies, improved multimodal reasoning, and real-world clinical integration of AI-driven medical assistance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs</title>
<link>https://arxiv.org/abs/2507.19899</link>
<guid>https://arxiv.org/abs/2507.19899</guid>
<content:encoded><![CDATA[
<div> Keywords: depression, social media, dataset, natural language explanations, large language models

Summary:
In this study, a dataset of social media posts related to depression, expert-annotated with depressive spans and mapped to 12 symptom categories, is presented. This dataset allows for fine-grained evaluation of model predictions and generated explanations. The study focuses on evaluating the faithfulness and quality of natural language explanations generated by large language models (LLMs) such as GPT-4.1, Gemini 2.5 Pro, and Claude 3.7 Sonnet using various prompting strategies. Significant differences in model performance on clinical explanation tasks are observed, highlighting the importance of human expertise in guiding LLM behavior. The study emphasizes the need for safer and transparent AI systems for mental health interventions. <br /><br />Summary: <div>
arXiv:2507.19899v1 Announce Type: new 
Abstract: Early detection of depression from online social media posts holds promise for providing timely mental health interventions. In this work, we present a high-quality, expert-annotated dataset of 1,017 social media posts labeled with depressive spans and mapped to 12 depression symptom categories. Unlike prior datasets that primarily offer coarse post-level labels \cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded dataset to assess the faithfulness and quality of natural language explanations generated by large language models (LLMs). Through carefully designed prompting strategies, including zero-shot and few-shot approaches with domain-adapted examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1, Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how these models perform on clinical explanation tasks, with zero-shot and few-shot prompting. Our findings underscore the value of human expertise in guiding LLM behavior and offer a step toward safer, more transparent AI systems for psychological well-being.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CaliDrop: KV Cache Compression with Calibration</title>
<link>https://arxiv.org/abs/2507.19906</link>
<guid>https://arxiv.org/abs/2507.19906</guid>
<content:encoded><![CDATA[
<div> compression, large language models, key-value cache, token eviction, CaliDrop

Summary:
- Large Language Models (LLMs) require significant computational resources for generation.
- The Key-Value (KV) cache accelerates this process by storing attention intermediates but faces memory footprint challenges.
- Various techniques like token eviction, quantization, and low-rank projection help compress the KV cache.
- Token eviction is a common strategy but can lead to accuracy degradation, especially under high compression ratios.
- CaliDrop enhances token eviction through calibration, leveraging the observation that nearby positions in queries exhibit high similarity.
<br /><br />Summary: <div>
arXiv:2507.19906v1 Announce Type: new 
Abstract: Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models</title>
<link>https://arxiv.org/abs/2507.19962</link>
<guid>https://arxiv.org/abs/2507.19962</guid>
<content:encoded><![CDATA[
<div> attention-based debiasing, large language models, societal biases, fairness, harm 

Summary: 
The article introduces KLAAD, a debiasing framework for large language models that aims to mitigate societal biases in their outputs. KLAAD utilizes attention alignment between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. By combining Cross-Entropy, KL divergence, and Triplet losses in a composite training objective, KLAAD guides the model to attend consistently across biased and unbiased contexts while maintaining fluency and coherence. Experimental results demonstrate that KLAAD effectively reduces bias in generative language models, as shown on the BBQ and BOLD benchmarks. Despite the bias mitigation, KLAAD has minimal impact on the quality of language modeling. This research suggests that attention-level alignment can offer a principled approach to addressing bias in language models. 

<br /><br />Summary: <div>
arXiv:2507.19962v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit societal biases in their outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns attention distributions between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased contexts while preserving fluency and coherence. Experimental evaluation of KLAAD demonstrates improved bias mitigation on both the BBQ and BOLD benchmarks, with minimal impact on language modeling quality. The results indicate that attention-level alignment offers a principled solution for mitigating bias in generative language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text</title>
<link>https://arxiv.org/abs/2507.19969</link>
<guid>https://arxiv.org/abs/2507.19969</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated data visualization, large language models, benchmark, text-to-visualization models, cross-modal actor-critic framework

Summary: 
Automated data visualization is essential for simplifying data interpretation and decision-making. The Text2Vis benchmark has been introduced to evaluate text-to-visualization models across various chart types and data science queries. This benchmark consists of 1,985 samples with data tables, natural language queries, visualization code, and annotated charts, covering complex reasoning and dynamic data retrieval. Testing 11 models revealed performance gaps and challenges, leading to the proposal of a cross-modal actor-critic framework. This framework refines textual answers and visualization code simultaneously, improving GPT-4o's pass rate and chart quality. Additionally, an automated evaluation framework based on large language models enables scalable assessment without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. The Text2Vis benchmark and proposed frameworks offer insights for advancing text-to-visualization technology. 

<br /><br />Summary: <div>
arXiv:2507.19969v1 Announce Type: new 
Abstract: Automated data visualization plays a crucial role in simplifying data interpretation, enhancing decision-making, and improving efficiency. While large language models (LLMs) have shown promise in generating visualizations from natural language, the absence of comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data science queries, including trend analysis, correlation, outlier detection, and predictive analytics. It comprises 1,985 samples, each with a data table, natural language query, short answer, visualization code, and annotated charts. The queries involve complex reasoning, conversational turns, and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing significant performance gaps, highlighting key challenges, and offering insights for future advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that jointly refines the textual answer and visualization code, increasing GPT-4o`s pass rate from 26% to 42% over the direct approach and improving chart quality. We also introduce an automated LLM-based evaluation framework that enables scalable assessment across thousands of samples without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory</title>
<link>https://arxiv.org/abs/2507.19980</link>
<guid>https://arxiv.org/abs/2507.19980</guid>
<content:encoded><![CDATA[
<div> Keywords: reliability, large language models, AP Chinese Exam, scoring, hybrid models

Summary:<br /><br />
This study examines the reliability of large language models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability theory, the research evaluates the consistency of scores between human and AI raters for story narration and email response tasks. Results show that human raters generally produce more reliable scores, but LLMs demonstrate reasonable consistency, especially for story narration tasks. Composite scoring that combines human and AI ratings enhances reliability, suggesting that hybrid models could be advantageous for large-scale writing assessments. The study highlights the potential of hybrid scoring models in improving reliability for assessing writing tasks in language exams. <div>
arXiv:2507.19980v1 Announce Type: new 
Abstract: This study investigates the estimation of reliability for large language models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability theory, the research evaluates and compares score consistency between human and AI raters across two types of AP Chinese free-response writing tasks: story narration and email response. These essays were independently scored by two trained human raters and seven AI raters. Each essay received four scores: one holistic score and three analytic scores corresponding to the domains of task completion, delivery, and language use. Results indicate that although human raters produced more reliable scores overall, LLMs demonstrated reasonable consistency under certain conditions, particularly for story narration tasks. Composite scoring that incorporates both human and AI raters improved reliability, which supports that hybrid scoring models may offer benefits for large-scale writing assessments.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering</title>
<link>https://arxiv.org/abs/2507.19995</link>
<guid>https://arxiv.org/abs/2507.19995</guid>
<content:encoded><![CDATA[
<div> large language models, legal text processing, natural language processing, legal NLP, VLQA dataset

Summary:
The article discusses the advancements in utilizing large language models for legal text processing, highlighting the challenges in automating legal tasks fully. The domain-specific nature of legal systems across different countries and languages presents a hurdle in developing legal text processing applications for various natural languages. The scarcity of resources and annotated data poses a significant challenge, particularly in low-resource languages like Vietnamese. To address this gap, the VLQA dataset is introduced as a high-quality resource tailored for the Vietnamese legal domain. A statistical analysis of the dataset is provided, showcasing its effectiveness in legal information retrieval and question-answering tasks when used with state-of-the-art models. Efforts to bridge the gap in legal NLP for diverse languages are essential for advancing the field of AI and NLP in the legal domain. 

<br /><br />Summary: <div>
arXiv:2507.19995v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has led to significant achievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a natural evolution and an increasingly compelling choice. However, their capabilities are often portrayed as greater than they truly are. Despite the progress, we are still far from the ultimate goal of fully automating legal tasks using artificial intelligence (AI) and natural language processing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation across different countries and languages. The need for building legal text processing applications for different natural languages is, therefore, large and urgent. However, there is a big challenge for legal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated data. The need for labeled legal corpora for supervised training, validation, and supervised fine-tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality resource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical analysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art models on legal information retrieval and question-answering tasks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach</title>
<link>https://arxiv.org/abs/2507.20019</link>
<guid>https://arxiv.org/abs/2507.20019</guid>
<content:encoded><![CDATA[
<div> framework, anomalies, language, meta-learning, detection
Summary:
- Researchers propose a meta learning framework for detecting anomalies in human language across different domains with limited labeled data.
- Anomalies in language, such as spam, fake news, and hate speech, are sparse and varied, making detection challenging.
- The approach treats anomaly detection as a few-shot binary classification problem and utilizes meta-learning to train models that can generalize across tasks.
- By combining episodic training with prototypical networks and domain resampling, the method can quickly adapt to new anomaly detection tasks.
- Empirical results demonstrate that the proposed method outperforms strong baselines in F1 and AUC scores.
<br /><br />Summary: <div>
arXiv:2507.20019v1 Announce Type: new 
Abstract: We propose a meta learning framework for detecting anomalies in human language across diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection as a few shot binary classification problem and leverage meta-learning to train models that generalize across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly detection tasks. Empirical results show that our method outperforms strong baselines in F1 and AUC scores. We also release the code and benchmarks to facilitate further research in few-shot text anomaly detection.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression</title>
<link>https://arxiv.org/abs/2507.20030</link>
<guid>https://arxiv.org/abs/2507.20030</guid>
<content:encoded><![CDATA[
<div> Frequency-Adaptive Infinite-Window, KV cache compression, unbiased representation, Long-context tasks, Large Language Models <br />
<br />
Summary: <br />
The article introduces FAEDKV, a compression framework for Key-Value (KV) cache in Large Language Models (LLMs) that aims to address issues of biased representation and excessive memory usage. FAEDKV utilizes an Infinite-Window Fourier Transform to transform the KV cache into the frequency domain, ensuring equalized contribution of all tokens for unbiased information retention. A frequency ablation study identifies critical spectral components for effective compression. Experimental results on the LongBench benchmark demonstrate up to 22% improvement over existing methods. FAEDKV also outperforms compression-based approaches in position-agnostic retrieval accuracy on the Needle-In-A-Haystack task. This innovative approach offers a promising solution to enhancing the efficiency and effectiveness of LLMs in long-context tasks. <br /> <div>
arXiv:2507.20030v1 Announce Type: new 
Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value (KV) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel, training-free KV cache compression framework that ensures unbiased information retention. FAEDKV operates by transforming the KV cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infogen: Generating Complex Statistical Infographics from Documents</title>
<link>https://arxiv.org/abs/2507.20046</link>
<guid>https://arxiv.org/abs/2507.20046</guid>
<content:encoded><![CDATA[
<div> infographics, AI, LLMs, dataset, metadata<br />
Summary:<br />
The article introduces the task of generating complex statistical infographics from text-heavy documents using AI, particularly Large Language Models (LLMs). It addresses the lack of prior work in this area by proposing a new benchmark dataset called Infodat, which links documents to infographic metadata. The Infogen framework, a two-stage process involving fine-tuned LLMs, is presented for text-to-infographic code generation. Infogen achieves state-of-the-art performance in text-to-statistical infographic generation compared to existing LLMs. The generated infographics consist of multiple sub-charts like line, bar, and pie graphs, with contextually accurate and visually aligned insights. This work highlights the potential of AI in simplifying complex data into visually engaging and easy-to-understand formats through the creation of advanced statistical infographics. <br /><br />Summary: <div>
arXiv:2507.20046v1 Announce Type: new 
Abstract: Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating statistical infographics composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data and alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose Infogen, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications</title>
<link>https://arxiv.org/abs/2507.20055</link>
<guid>https://arxiv.org/abs/2507.20055</guid>
<content:encoded><![CDATA[
<div> certification, DNNs, abstract interpretation, compiler, g-BCSR

Summary:
The article addresses the challenge of interpreting deep neural networks (DNNs) by proposing a compiler framework that automatically translates neuron-level specifications of DNN certifiers into tensor-based layer-level implementations. This framework overcomes the semantic gap between design and implementation by introducing a novel stack-based intermediate representation (IR) and a shape analysis. The shape analysis infers the implicit tensor operations needed to simulate neuron-level semantics, creating tensors in the minimal shape required for operations. At runtime, the resulting tensor computations exhibit sparsity, which is addressed by the introduction of the g-BCSR double-compression format. This format represents tensors as collections of blocks of varying sizes, each possibly internally sparse. By using this compiler and g-BCSR, developers can easily create new certifiers and analyze their utility across diverse DNNs while achieving performance comparable to hand-optimized implementations. <br /><br />Summary: <div>
arXiv:2507.20055v1 Announce Type: new 
Abstract: The uninterpretability of DNNs has led to the adoption of abstract interpretation-based certification as a practical means to establish trust in real-world systems that rely on DNNs. However, the current landscape supports only a limited set of certifiers, and developing new ones or modifying existing ones for different applications remains difficult. This is because the mathematical design of certifiers is expressed at the neuron level, while their implementations are optimized and executed at the tensor level. This mismatch creates a semantic gap between design and implementation, making manual bridging both complex and expertise-intensive -- requiring deep knowledge in formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level specifications of DNN certifiers into tensor-based, layer-level implementations. This is enabled by two key innovations: a novel stack-based intermediate representation (IR) and a shape analysis that infers the implicit tensor operations needed to simulate the neuron-level semantics. During lifting, the shape analysis creates tensors in the minimal shape required to perform the corresponding operations. The IR also enables domain-specific optimizations as rewrites. At runtime, the resulting tensor computations exhibit sparsity tied to the DNN architecture. This sparsity does not align well with existing formats. To address this, we introduce g-BCSR, a double-compression format that represents tensors as collections of blocks of varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and analyze their utility across diverse DNNs. Despite its flexibility, the compiler achieves performance comparable to hand-optimized implementations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</title>
<link>https://arxiv.org/abs/2507.20059</link>
<guid>https://arxiv.org/abs/2507.20059</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, Large language models, Knowledge retrieval, Realistic scenarios, Adaptive strategies<br />
Summary:<br />
Retrieval-augmented generation (RAG) integrates external knowledge with large language models (LLMs) to improve performance. However, a study using a diverse knowledge datastore called MassiveDS revealed limitations of current RAG systems. It was found that retrieval benefits smaller models more than larger ones and rerankers provide minimal improvement. Additionally, no single retrieval source consistently outperformed others, highlighting the challenge of routing queries across different knowledge sources. These findings underscore the importance of developing adaptive retrieval strategies before implementing RAG in practical applications. The study's code and data are available at https://github.com/ritaranx/RAG_in_the_Wild.<br /> 
Summary: <div>
arXiv:2507.20059v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models</title>
<link>https://arxiv.org/abs/2507.20091</link>
<guid>https://arxiv.org/abs/2507.20091</guid>
<content:encoded><![CDATA[
<div> Speech language model, prosody, tokenization scheme, pre-training, emerging capabilities
Summary: 
ProsodyLM introduces a novel tokenization scheme for training speech language models to capture the interdependency between content and prosody. Unlike traditional tokenization methods, which convert speech into discrete tokens before inputting into language models, ProsodyLM transcribes speech into text and adds word-level prosody tokens. This approach better retains prosody information and enhances the model's ability to learn prosody processing capabilities through pre-training alone. ProsodyLM demonstrates diverse emerging capabilities, such as detecting prosody nuances like contrastive focus, understanding emotion and stress in speech, and maintaining prosody consistency in longer contexts. This new approach could significantly improve the performance of speech language models in capturing and utilizing prosody information. 
<br /><br />Summary: <div>
arXiv:2507.20091v1 Announce Type: new 
Abstract: Speech language models refer to language models with speech processing and understanding capabilities. One key desirable capability for speech language models is the ability to capture the intricate interdependency between content and prosody. The existing mainstream paradigm of training speech language models, which converts speech into discrete tokens before feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone. To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed tokenization scheme retains more complete prosody information, and is more understandable to text-based LLMs. We find that ProsodyLM can learn surprisingly diverse emerging prosody processing capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining prosody consistency in long contexts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Generation of Old English: A Framework for Low-Resource Languages</title>
<link>https://arxiv.org/abs/2507.20111</link>
<guid>https://arxiv.org/abs/2507.20111</guid>
<content:encoded><![CDATA[
<div> Keywords: Old English, natural language processing, large language models, data augmentation, cultural preservation

Summary: 
The study addresses the lack of resources for Old English in natural language processing by proposing a scalable framework that uses large language models. The framework combines parameter-efficient fine-tuning, data augmentation through backtranslation, and a dual-agent pipeline for content generation and translation. Evaluation using automated metrics and expert human assessment demonstrates significant improvements in translation quality, with BLEU scores soaring from 26 to over 65. The approach not only expands the Old English corpus but also serves as a blueprint for preserving other endangered languages by leveraging AI innovation. This innovative method bridges the gap between artificial intelligence and cultural preservation, showcasing the potential for revitalizing ancient languages using advanced NLP techniques.

<br /><br />Summary: <div>
arXiv:2507.20111v1 Announce Type: new 
Abstract: Preserving ancient languages is essential for understanding humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced, limiting its accessibility to modern natural language processing (NLP) techniques. We present a scalable framework that uses advanced large language models (LLMs) to generate high-quality Old English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English). Evaluation with automated metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the Old English corpus, our method offers a practical blueprint for revitalizing other endangered languages, effectively uniting AI innovation with the goals of cultural preservation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering</title>
<link>https://arxiv.org/abs/2507.20133</link>
<guid>https://arxiv.org/abs/2507.20133</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI, Direct Preference Optimization, Semantic consistency, Prompt engineering, Language models

Summary: 
Direct Preference Optimization (DPO) is a method used in generative AI to create realistic images from text prompts. However, DPO lacks semantic consistency, leading to prompts drifting from the intended meaning. To address this issue, Sem-DPO is introduced as a variant that maintains semantic consistency while still being efficient. By incorporating cosine distance weighting between prompts and candidates, Sem-DPO prevents semantic drift and ensures prompts stay close to the original text. Empirical results show that Sem-DPO outperforms DPO and other baselines in text-to-image prompt optimization tasks, achieving higher CLIP similarity and human-preference scores. This research suggests that incorporating semantic weighting in prompt optimization studies can significantly improve the quality of generated outputs and paves the way for future developments in semantics-aware preference optimization for language models. 

<br /><br />Summary: <div>
arXiv:2507.20133v1 Announce Type: new 
Abstract: Generative AI can now synthesize strikingly realistic images from text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts that win higher preference scores can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine distance between the original prompt and winning candidate in embedding space, softly down-weighting training signals that would otherwise reward semantically mismatched prompts. We provide the first analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps learned prompts within a provably bounded neighborhood of the original text. On three standard text-to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented with semantic weighting should become the new standard for prompt-optimization studies and lay the groundwork for broader, semantics-aware preference optimization in language models.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG</title>
<link>https://arxiv.org/abs/2507.20136</link>
<guid>https://arxiv.org/abs/2507.20136</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, hallucination, multi-modal, fact-seeking queries, factual accuracy

Summary: 
Team CRUISE developed a technical solution for the KDD Cup 2025 CRAG-MM challenge, which addresses the issue of hallucination in Vision Language Models (VLMs). Their framework prioritizes factual accuracy over completeness and includes a query router for efficiency, retrieval and summarization pipeline, generation pathways, and post-hoc verification. By focusing on answer reliability, their approach achieved 3rd place in Task 1 of the competition. The goal is to minimize hallucinations, especially in scenarios involving egocentric imagery, long-tail entities, and multi-hop questions in diverse modalities. The implementation of their solution is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2507.20136v1 Announce Type: new 
Abstract: This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Interactive Question Generation Framework for Long Document Understanding</title>
<link>https://arxiv.org/abs/2507.20145</link>
<guid>https://arxiv.org/abs/2507.20145</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Understanding, Vision-Language Models, Arabic, Multi-agent interactive framework, Automated generation

Summary: <br /><br />Document Understanding (DU) in long-contextual scenarios poses a challenge for Vision-Language Models (LVLMs), especially in low-resource languages like Arabic. Existing techniques rely heavily on human annotation, which is costly. To address this, a fully automated, multi-agent interactive framework has been proposed to efficiently generate high-quality questions for extensive English and Arabic documents, covering hundreds of pages across different domains. The generated questions have been shown to challenge major LVLMs in both open and closed sources. This work contributes to enhancing the long-context understanding ability of LVLMs, particularly in low-resource languages. The code and data for this framework can be accessed on GitHub, along with sample question and answer pairs and structured system prompts provided in the Appendix. <div>
arXiv:2507.20145v1 Announce Type: new 
Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Goal Alignment in LLM-Based User Simulators for Conversational AI</title>
<link>https://arxiv.org/abs/2507.20152</link>
<guid>https://arxiv.org/abs/2507.20152</guid>
<content:encoded><![CDATA[
<div> User simulators, LLMs, goal-oriented behavior, User Goal State Tracking, goal progression tracking.  
Summary:  
User simulators are crucial for conversational AI development, but current LLMs struggle with consistent goal-oriented behavior in multi-turn conversations. The proposed User Goal State Tracking (UGST) framework tracks user goal progression and enables goal-aligned responses. A three-stage methodology is presented for developing user simulators that track goals and reason to generate appropriate responses. Evaluation metrics for measuring goal alignment in user simulators are established, showing significant improvements on MultiWOZ 2.4 and τ-Bench benchmarks. These contributions address a critical gap in conversational AI and establish UGST as a vital framework for developing goal-aligned user simulators.  
<br /><br />Summary: <div>
arXiv:2507.20152v1 Announce Type: new 
Abstract: User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGPO: Self-Generated Preference Optimization based on Self-Improver</title>
<link>https://arxiv.org/abs/2507.20181</link>
<guid>https://arxiv.org/abs/2507.20181</guid>
<content:encoded><![CDATA[
<div> framework, alignment, language models, self-generated preference optimization, self-improver <br />
Summary:
The article introduces Self-Generated Preference Optimization based on Self-Improver (SGPO), a novel framework for aligning large language models (LLMs) with human preferences. Traditional alignment methods rely on off-policy learning and human-annotated datasets, leading to limited applicability and distribution shift issues. SGPO, however, utilizes an on-policy self-improving mechanism where a single model refines responses to self-generate preference data for direct policy optimization. By learning incremental improvements through supervised fine-tuning outputs, SGPO outperforms conventional methods in tasks like AlpacaEval 2.0 and Arena-Hard without requiring external preference data. This approach not only enhances LLM performance but also addresses challenges in alignment by leveraging self-generated preference data for more reliable deployment. <br /><br />Summary: <div>
arXiv:2507.20181v1 Announce Type: new 
Abstract: Large language models (LLMs), despite their extensive pretraining on diverse datasets, require effective alignment to human preferences for practical and reliable deployment. Conventional alignment methods typically employ off-policy learning and depend on human-annotated datasets, which limits their broad applicability and introduces distribution shift issues during training. To address these challenges, we propose Self-Generated Preference Optimization based on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving mechanism. Specifically, the improver refines responses from a policy model to self-generate preference data for direct preference optimization (DPO) of the policy model. Here, the improver and policy are unified into a single model, and in order to generate higher-quality preference data, this self-improver learns to make incremental yet discernible improvements to the current responses by referencing supervised fine-tuning outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods without using external preference data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding</title>
<link>https://arxiv.org/abs/2507.20185</link>
<guid>https://arxiv.org/abs/2507.20185</guid>
<content:encoded><![CDATA[
<div> keywords: customer intention, session history, intention tree, multimodal benchmark, L(V)LMs<br />
Summary:<br />
The article introduces the concept of an intention tree and a dataset curation pipeline to address the lack of effective modeling of customer intention in E-commerce product purchase sessions. A new benchmark, SessionIntentBench, is proposed to evaluate the capability of L(V)LMs in understanding inter-session intention shifts. The dataset includes over 1.9 million intention entries and 1.1 million session intention trajectories extracted from 10,905 sessions. Human annotations were used to collect ground-truth labels for evaluation. Experiments show that current L(V)LMs struggle to capture and utilize intention in complex session settings, but injecting intention can improve their performance. This work provides a scalable approach to leveraging session data for customer intention understanding. <div>
arXiv:2507.20185v1 Announce Type: new 
Abstract: Session history is a common way of recording user interacting behaviors throughout a browsing activity with multiple products. For example, if an user clicks a product webpage and then leaves, it might because there are certain features that don't satisfy the user, which serve as an important indicator of on-the-spot user preferences. However, all prior works fail to capture and model customer intention effectively because insufficient information exploitation and only apparent information like descriptions and titles are used. There is also a lack of data and corresponding benchmark for explicitly modeling intention in E-commerce product purchase sessions. To address these issues, we introduce the concept of an intention tree and propose a dataset curation pipeline. Together, we construct a sibling multimodal benchmark, SessionIntentBench, that evaluates L(V)LMs' capability on understanding inter-session intention shift with four subtasks. With 1,952,177 intention entries, 1,132,145 session intention trajectories, and 13,003,664 available tasks mined using 10,905 sessions, we provide a scalable way to exploit the existing session data for customer intention understanding. We conduct human annotations to collect ground-truth label for a subset of collected data to form an evaluation gold set. Extensive experiments on the annotated data further confirm that current L(V)LMs fail to capture and utilize the intention across the complex session setting. Further analysis show injecting intention enhances LLMs' performances.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diversity-Enhanced Reasoning for Subjective Questions</title>
<link>https://arxiv.org/abs/2507.20187</link>
<guid>https://arxiv.org/abs/2507.20187</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, subjective reasoning tasks, diversity-enhanced framework, reinforcement learning, diversity and accuracy.

Summary: 
Large reasoning models with long chain-of-thought capabilities have shown strong performance on objective tasks but struggle with subjective questions due to homogeneous reasoning. To address this limitation, MultiRole-R1 introduces a diversity-enhanced framework with multiple role perspectives for subjective reasoning tasks. It generates diverse reasoning chains through unsupervised data construction and utilizes Group Relative Policy Optimization in reinforcement learning with diversity as a reward signal. By promoting perspective and lexical diversity, MultiRole-R1 improves both accuracy and diversity in reasoning. Experimental results on six benchmarks demonstrate the effectiveness and generalizability of MultiRole-R1 in enhancing both subjective and objective reasoning. This study highlights the importance of diversity-enhanced training in large reasoning models. 

<br /><br />Summary: <div>
arXiv:2507.20187v1 Announce Type: new 
Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs</title>
<link>https://arxiv.org/abs/2507.20208</link>
<guid>https://arxiv.org/abs/2507.20208</guid>
<content:encoded><![CDATA[
<div> factor analysis, large language models, benchmark scores, latent skills, model evaluation

Summary:
- The article discusses the limitations of current evaluations of large language models (LLMs) based on benchmark scores.
- It proposes a new evaluation paradigm using factor analysis to identify latent skills influencing performance across benchmarks.
- The study examined 60 LLMs on 44 tasks and identified a small set of latent skills that largely explained their performance.
- Practical tools were developed based on these insights to identify redundant tasks, assist in model selection, and profile models based on each latent skill. 

<br /><br />Summary: <div>
arXiv:2507.20208v1 Announce Type: new 
Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's overall skills. Specifically, as a community we lack understanding of how tasks relate to one another, what they measure in common, how they differ, or which ones are redundant. As a result, models are often assessed via a single score averaged across benchmarks, an approach that fails to capture the models' wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses factor analysis to identify latent skills driving performance across benchmarks. We apply this method to a comprehensive new leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a small set of latent skills that largely explain performance. Finally, we turn these insights into practical tools that identify redundant tasks, aid in model selection, and profile models along each latent skill.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation</title>
<link>https://arxiv.org/abs/2507.20210</link>
<guid>https://arxiv.org/abs/2507.20210</guid>
<content:encoded><![CDATA[
<div> Keywords: news recommendation systems, multi-view news representations, user interests, Co-NAML-LSTUR, BERT-based word embeddings

Summary:
News recommendation systems are important for delivering personalized news content and addressing information overload. The challenge lies in effectively modeling multi-view news representations and dynamic user interests, spanning short- and long-term preferences. Existing methods often fall short in capturing comprehensive user preferences across different time scales. To address this, Co-NAML-LSTUR is proposed, a hybrid framework combining NAML for multi-view news modeling, LSTUR for dual-scale user representations, and BERT-based word embeddings for semantic feature extraction. Evaluation on MIND-small and MIND-large benchmarks shows significant improvements over state-of-the-art baselines. This highlights the effectiveness of integrating multi-view news representations with dual-scale user modeling.<br /><br />Summary: <div>
arXiv:2507.20210v1 Announce Type: new 
Abstract: News recommendation systems play a vital role in mitigating information overload by delivering personalized news content. A central challenge is to effectively model both multi-view news representations and the dynamic nature of user interests, which often span both short- and long-term preferences. Existing methods typically rely on single-view features of news articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness of combining multi-view news representations with dual-scale user modeling. The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models</title>
<link>https://arxiv.org/abs/2507.20241</link>
<guid>https://arxiv.org/abs/2507.20241</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mental health support, narrative therapy, Interactive Narrative Therapist, Innovative Moment Assessment 

Summary: 
The article discusses the use of large language models (LLMs) in providing mental health support, particularly in the context of narrative therapy. A new framework is proposed, consisting of two core components: INT (Interactive Narrative Therapist) and IMA (Innovative Moment Assessment). INT simulates expert narrative therapists by guiding therapeutic stages and generating expert-like responses. IMA tracks "Innovative Moments" (IMs) in client speech to assess therapy progress. Experimental results show that INT outperforms standard LLMs in therapeutic quality and depth. The framework aims to address the limitations of current approaches in simulating specialized psychotherapy and capturing therapeutic progression over time. The effectiveness of INT is demonstrated in facilitating social applications and synthesizing high-quality support conversations. <div>
arXiv:2507.20241v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking "Innovative Moments" (IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-quality support conversations to facilitate social applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Professionalism in Expert Questioning through Linguistic Differentiation</title>
<link>https://arxiv.org/abs/2507.20249</link>
<guid>https://arxiv.org/abs/2507.20249</guid>
<content:encoded><![CDATA[
<div> Keywords: professionalism, expert communication, linguistic features, financial analysis, annotation framework

Summary:
Professionalism in expert communication, especially in fields like finance, is essential yet not fully explored. This study delves into how linguistic attributes can aid in modeling and assessing professionalism in expert questioning. A unique annotation framework was developed to measure structural and pragmatic components in financial analyst questions. Two datasets were created, one annotated for perceived professionalism and the other labeled by question origin, using both human and large language model-generated questions. The research revealed that specific linguistic features strongly correlate with human judgments and question authorship, indicating a common stylistic basis. A classifier trained solely on these features outperformed existing baselines in identifying expert-authored questions. The study suggests that professionalism can be effectively represented through linguistically grounded modeling, highlighting its learnable nature across domains. <br /><br />Summary: <div>
arXiv:2507.20249v1 Announce Type: new 
Abstract: Professionalism is a crucial yet underexplored dimension of expert communication, particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model (LLM)-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Completion Learning for Language Models</title>
<link>https://arxiv.org/abs/2507.20252</link>
<guid>https://arxiv.org/abs/2507.20252</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, training, post-completion learning, reinforcement learning, output quality <br />
Summary: <br />
The article introduces Post-Completion Learning (PCL), a novel training framework for language models that leverages the space after output completion to enhance reasoning and self-evaluation abilities. PCL involves continuing to generate self-assessments and reward predictions during training while maintaining efficient inference by stopping at the completion point. By utilizing a white-box reinforcement learning method, models evaluate output content and align scores with reward functions for supervision. This dual-track approach optimizes reasoning and evaluation capabilities and combines them with reinforcement learning for multi-objective hybrid optimization. Experimental results across various datasets and models demonstrate improvements over traditional Structured Self-Training (SFT) and reinforcement learning methods. The method provides a new technical pathway for training language models to enhance output quality while ensuring deployment efficiency. <br /> <div>
arXiv:2507.20252v1 Announce Type: new 
Abstract: Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms</title>
<link>https://arxiv.org/abs/2507.20264</link>
<guid>https://arxiv.org/abs/2507.20264</guid>
<content:encoded><![CDATA[
<div> demographics, behavioral attributes, implicit opinion, alignment evaluation framework, inclusive representation

Summary: 
This study addresses the need for more inclusive representations in conversation-based models by focusing on implicit opinions and evaluating normative alignment through the stance of responses. Existing methods often overlook implicit expressions of opinion, leading to misalignment and reinforcing harmful stereotypes. The proposed framework evaluates how opinions are represented in NLP models, utilizing the stance of responses to capture diverse social viewpoints. The study utilizes positive-unlabeled online learning with base classifiers and instruction-tuned language models to assess post-training alignment, shedding light on how implicit opinions are (mis) represented and offering a pathway towards more inclusive model behavior. <div>
arXiv:2507.20264v1 Announce Type: new 
Abstract: Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning</title>
<link>https://arxiv.org/abs/2507.20278</link>
<guid>https://arxiv.org/abs/2507.20278</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sequential environmental feedback, chain-of-thought reasoning, MoL-RL, multi-step interactions 

Summary: 
Large language models (LLMs) face challenges in effectively utilizing sequential environmental feedback (EF) for chain-of-thought reasoning. Existing methods struggle to capture the rich contextual information in EF signals. To address this, MoL-RL integrates multi-step EF signals into LLMs using a dual-objective optimization framework. It employs MoL continual training to optimize domain-specific EF signals and general language capabilities separately, while utilizing GRPO-based post-training to distill sequential EF interactions. This approach enables robust feedback-independent reasoning without external feedback loops. Experimental results on mathematical reasoning and code generation benchmarks show that MoL-RL with the Qwen3-8B model achieves state-of-the-art performance and strong generalization across model scales. This work presents a promising method to enhance LLMs' reasoning capabilities through multi-step textual feedback integration. 

Summary: <br /><br /> <div>
arXiv:2507.20278v1 Announce Type: new 
Abstract: Large language models (LLMs) face significant challenges in effectively leveraging sequential environmental feedback (EF) signals, such as natural language evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either convert EF into scalar rewards, losing rich contextual information, or employ refinement datasets, failing to exploit the multi-step and discrete nature of EF interactions. To address these limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses) continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-training to distill sequential EF interactions into single-step inferences. This synergy enables robust feedback-independent reasoning without relying on external feedback loops. Experimental results on mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art performance with the Qwen3-8B model, while maintaining strong generalization across model scales (Qwen3-4B). This work provides a promising approach for leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities in diverse domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations</title>
<link>https://arxiv.org/abs/2507.20279</link>
<guid>https://arxiv.org/abs/2507.20279</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, multilingual data, code-mixed, neuron activation patterns, cross-lingual transfer
  
Summary:  
1. Aya-23-8B, a decoder-only Large Language Model (LLM) trained on balanced multilingual data, demonstrates superior performance in handling code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2.  
2. The model activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language.  
3. Neuron activation patterns in Aya-23 vary with mixing rates and are influenced more by the base language than the mixed-in language in code-mixed tasks.  
4. Language-specific neurons in Aya-23 for code-mixed inputs are concentrated in final layers, deviating from previous findings on decoder-only models.  
5. Analysis shows that script similarity and typological relations impact language processing across different model types.  

<br /><br />Summary: <div>
arXiv:2507.20279v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only LLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes LLM internals and inform future cross-lingual transfer research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation</title>
<link>https://arxiv.org/abs/2507.20301</link>
<guid>https://arxiv.org/abs/2507.20301</guid>
<content:encoded><![CDATA[
<div> Keywords: Dialectal Arabic, Machine Translation, Natural Language Processing, Prompting Techniques, Fine-tuning Pipeline

Summary:<br /><br />Dialectal Arabic (DA) presents challenges for natural language processing due to its differences from Modern Standard Arabic. This paper introduces new techniques to improve DA-MSA translation for Levantine, Egyptian, and Gulf dialects in low-resource settings. Six large language models were evaluated for prompting strategies, with GPT-4o achieving the best performance. A quantized Gemma2-9B model outperformed zero-shot GPT-4o in fine-tuning, with 4-bit quantization reducing memory usage. Joint multi-dialect trained models showed significant performance gains over single-dialect models. The study provides a practical pathway for enhancing dialectal inclusion in Arabic NLP, demonstrating the feasibility of high-quality DA-MSA machine translation with limited resources. This research has implications for improving access to digital services and educational resources in dialectal Arabic-speaking regions. 

Summary: <div>
arXiv:2507.20301v1 Announce Type: new 
Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday communication in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns</title>
<link>https://arxiv.org/abs/2507.20343</link>
<guid>https://arxiv.org/abs/2507.20343</guid>
<content:encoded><![CDATA[
<div> Keywords: DYNARTmo, dynamic articulatory model, speech articulation, phonetics education, speech therapy

Summary: 
DYNARTmo is a dynamic articulatory model that visualizes speech articulation processes in a two-dimensional midsagittal plane. It is based on the UK-DYNAMO framework and incorporates articulatory underspecification, segmental and gestural control, and coarticulation principles. The model simulates six key articulators using continuous and discrete control parameters, enabling the generation of vocalic and consonantal articulatory configurations. The current implementation is part of a web-based application called SpeechArticulationTrainer, which includes sagittal, glottal, and palatal views for use in phonetics education and speech therapy. Future work will focus on dynamic movement generation and integration with articulatory-acoustic modules. This research contributes to the understanding and visualization of speech articulation processes for educational and therapeutic purposes. 

<br /><br />Summary: <div>
arXiv:2507.20343v1 Announce Type: new 
Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing</title>
<link>https://arxiv.org/abs/2507.20352</link>
<guid>https://arxiv.org/abs/2507.20352</guid>
<content:encoded><![CDATA[
arXiv:2507.20352v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial yet remains challenging. Existing benchmarks mostly adopt a \textbf{character-centric} approach, simplify user-character interactions to isolated Q&amp;A tasks, and fail to reflect real-world applications. To address this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric} bilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds. RMTBench includes custom characters with detailed backgrounds and abstract characters defined by simple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues based on explicit user motivations rather than character descriptions, ensuring alignment with practical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation mechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism captures the complex intention of conversations between the user and the character. By shifting focus from character background to user intention fulfillment, RMTBench bridges the gap between academic evaluation and practical deployment requirements, offering a more effective framework for assessing role-playing capabilities in LLMs. All code and datasets will be released soon.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Length Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20398</link>
<guid>https://arxiv.org/abs/2507.20398</guid>
<content:encoded><![CDATA[
arXiv:2507.20398v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various tasks, that are learned from massive amounts of text-based data. Although LLMs can control output sequence length, particularly in instruction-based settings, the internal mechanisms behind this control have been unexplored yet. In this study, we provide empirical evidence on how output sequence length information is encoded within the internal representations in LLMs. In particular, our findings show that multi-head attention mechanisms are critical in determining output sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units within the model, we can control the output sequence length without losing the informativeness of the generated text, thereby indicating that length information is partially disentangled from semantic information. Moreover, some hidden units become increasingly active as prompts become more length-specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that LLMs have learned robust and adaptable internal mechanisms for controlling output length without any external control.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations</title>
<link>https://arxiv.org/abs/2507.20409</link>
<guid>https://arxiv.org/abs/2507.20409</guid>
<content:encoded><![CDATA[
arXiv:2507.20409v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning</title>
<link>https://arxiv.org/abs/2507.20411</link>
<guid>https://arxiv.org/abs/2507.20411</guid>
<content:encoded><![CDATA[
arXiv:2507.20411v1 Announce Type: new 
Abstract: Multilingual vision-language models have made significant strides in image captioning, yet they still lag behind their English counterparts due to limited multilingual training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG) offers a promising alternative by conditioning caption generation on retrieved examples in the target language, reducing the need for extensive multilingual training. However, multilingual RAG captioning models often depend on retrieved captions translated from English, which can introduce mismatches and linguistic biases relative to the source language. We introduce CONCAP, a multilingual image captioning model that integrates retrieved captions with image-specific concepts, enhancing the contextualization of the input image and grounding the captioning process across different languages. Experiments on the XM3600 dataset indicate that CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of concept-aware retrieval augmentation in bridging multilingual performance gaps.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?</title>
<link>https://arxiv.org/abs/2507.20419</link>
<guid>https://arxiv.org/abs/2507.20419</guid>
<content:encoded><![CDATA[
arXiv:2507.20419v1 Announce Type: new 
Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeNER: Code Prompting for Named Entity Recognition</title>
<link>https://arxiv.org/abs/2507.20423</link>
<guid>https://arxiv.org/abs/2507.20423</guid>
<content:encoded><![CDATA[
arXiv:2507.20423v1 Announce Type: new 
Abstract: Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems</title>
<link>https://arxiv.org/abs/2507.20491</link>
<guid>https://arxiv.org/abs/2507.20491</guid>
<content:encoded><![CDATA[
arXiv:2507.20491v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AQUA: A Large Language Model for Aquaculture &amp; Fisheries</title>
<link>https://arxiv.org/abs/2507.20520</link>
<guid>https://arxiv.org/abs/2507.20520</guid>
<content:encoded><![CDATA[
arXiv:2507.20520v1 Announce Type: new 
Abstract: Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers</title>
<link>https://arxiv.org/abs/2507.20527</link>
<guid>https://arxiv.org/abs/2507.20527</guid>
<content:encoded><![CDATA[
arXiv:2507.20527v1 Announce Type: new 
Abstract: The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new \textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by \textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations</title>
<link>https://arxiv.org/abs/2507.20528</link>
<guid>https://arxiv.org/abs/2507.20528</guid>
<content:encoded><![CDATA[
arXiv:2507.20528v1 Announce Type: new 
Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and counter-hate speech in social media conversations, categorizing hate and counter-hate messages into thematic and rhetorical dimensions. The thematic categories outline different discursive aspects of each type of speech, while the rhetorical dimension captures how hate and counter messages are communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting of 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of interaction between the thematic and rhetorical dimensions within and between hate and counter-hate speech. Our findings provide insights into the spread of hate messages on social media, the strategies used to counter them, and their potential impact on online behavior.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Hallucination Detection via Future Context</title>
<link>https://arxiv.org/abs/2507.20546</link>
<guid>https://arxiv.org/abs/2507.20546</guid>
<content:encoded><![CDATA[
arXiv:2507.20546v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning</title>
<link>https://arxiv.org/abs/2507.20564</link>
<guid>https://arxiv.org/abs/2507.20564</guid>
<content:encoded><![CDATA[
arXiv:2507.20564v1 Announce Type: new 
Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning. Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble similarity scores from CLIP, SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position on the private test set, demonstrating the effectiveness of combining foundation models through ensembling and prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior</title>
<link>https://arxiv.org/abs/2507.20614</link>
<guid>https://arxiv.org/abs/2507.20614</guid>
<content:encoded><![CDATA[
arXiv:2507.20614v1 Announce Type: new 
Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment, and trolling-poses growing challenges for platform safety and societal wellbeing. While prior work has primarily focused on detecting harmful content after it appears, predictive approaches aim to forecast future harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism-before they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified taxonomy or clear synthesis of existing methods. This paper presents a systematic review of over 49 studies on ASB prediction, offering a structured taxonomy of five core task types: early harm detection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and proactive moderation support. We analyze how these tasks differ by temporal framing, prediction granularity, and operational goals. In addition, we examine trends in modeling techniques-from classical machine learning to pre-trained language models-and assess the influence of dataset characteristics on task feasibility and generalization. Our review highlights methodological challenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging research directions including multilingual modeling, cross-platform generalization, and human-in-the-loop systems. By organizing the field around a coherent framework, this survey aims to guide future work toward more robust and socially responsible ASB prediction.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Enhanced Knowledge Graph Completion using Large Language Models</title>
<link>https://arxiv.org/abs/2507.20643</link>
<guid>https://arxiv.org/abs/2507.20643</guid>
<content:encoded><![CDATA[
arXiv:2507.20643v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric-Mean Policy Optimization</title>
<link>https://arxiv.org/abs/2507.20673</link>
<guid>https://arxiv.org/abs/2507.20673</guid>
<content:encoded><![CDATA[
arXiv:2507.20673v1 Announce Type: new 
Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification</title>
<link>https://arxiv.org/abs/2507.20700</link>
<guid>https://arxiv.org/abs/2507.20700</guid>
<content:encoded><![CDATA[
arXiv:2507.20700v1 Announce Type: new 
Abstract: The rapid spread of multilingual misinformation requires robust automated fact verification systems capable of handling fine-grained veracity assessments across diverse languages. While large language models have shown remarkable capabilities across many NLP tasks, their effectiveness for multilingual claim verification with nuanced classification schemes remains understudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-Fact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments compare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B parameters), achieving 57.7% macro-F1 compared to the best LLM performance of 16.9%. This represents a 15.8% improvement over the previous state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact verification. Our analysis reveals problematic patterns in LLM behavior, including systematic difficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced data settings. These findings suggest that for fine-grained multilingual fact verification, smaller specialized models may be more effective than general-purpose large models, with important implications for practical deployment of fact-checking systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</title>
<link>https://arxiv.org/abs/2507.20704</link>
<guid>https://arxiv.org/abs/2507.20704</guid>
<content:encoded><![CDATA[
arXiv:2507.20704v1 Announce Type: new 
Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study</title>
<link>https://arxiv.org/abs/2507.20749</link>
<guid>https://arxiv.org/abs/2507.20749</guid>
<content:encoded><![CDATA[
arXiv:2507.20749v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities, their substantial computational and memory requirements pose significant barriers to practical deployment. Current parameter reduction techniques primarily involve training MLLMs from Small Language Models (SLMs), but these methods offer limited flexibility and remain computationally intensive. To address this gap, we propose to directly compress existing MLLMs through structural pruning combined with efficient recovery training. Specifically, we investigate two structural pruning paradigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside supervised finetuning and knowledge distillation. Additionally, we assess the feasibility of conducting recovery training with only a small fraction of the available data. Our results show that widthwise pruning generally maintains better performance in low-resource scenarios with limited computational resources or insufficient finetuning data. As for the recovery training, finetuning only the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination of supervised finetuning and hidden-state distillation yields optimal recovery across various pruning levels. Notably, effective recovery can be achieved with as little as 5% of the original training data, while retaining over 95% of the original performance. Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming to compress MLLMs effectively without extensive computation resources or sufficient data.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Self-Taught Faithfulness Evaluators</title>
<link>https://arxiv.org/abs/2507.20752</link>
<guid>https://arxiv.org/abs/2507.20752</guid>
<content:encoded><![CDATA[
arXiv:2507.20752v1 Announce Type: new 
Abstract: The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey</title>
<link>https://arxiv.org/abs/2507.20783</link>
<guid>https://arxiv.org/abs/2507.20783</guid>
<content:encoded><![CDATA[
arXiv:2507.20783v1 Announce Type: new 
Abstract: Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models</title>
<link>https://arxiv.org/abs/2507.20786</link>
<guid>https://arxiv.org/abs/2507.20786</guid>
<content:encoded><![CDATA[
arXiv:2507.20786v1 Announce Type: new 
Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source "text-to-table" language-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkit's large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohen's $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Inter-User Difference Modeling for LLM Personalization</title>
<link>https://arxiv.org/abs/2507.20849</link>
<guid>https://arxiv.org/abs/2507.20849</guid>
<content:encoded><![CDATA[
arXiv:2507.20849v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A survey of diversity quantification in natural language processing: The why, what, where and how</title>
<link>https://arxiv.org/abs/2507.20858</link>
<guid>https://arxiv.org/abs/2507.20858</guid>
<content:encoded><![CDATA[
arXiv:2507.20858v1 Announce Type: new 
Abstract: The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with "diversity" or "diverse" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2507.20859</link>
<guid>https://arxiv.org/abs/2507.20859</guid>
<content:encoded><![CDATA[
arXiv:2507.20859v1 Announce Type: new 
Abstract: Medical reports contain rich clinical information but are often unstructured and written in domain-specific language, posing challenges for information extraction. While proprietary large language models (LLMs) have shown promise in clinical natural language processing, their lack of transparency and data privacy concerns limit their utility in healthcare. This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which includes 28 clinical information extraction tasks in Dutch. We developed \texttt{llm\_extractinator}, a publicly available framework for information extraction using open-source generative LLMs, and used it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results, while the bigger Llama-3.3-70B model achieved slightly higher performance at greater computational cost. Translation to English prior to inference consistently degraded performance, highlighting the need of native-language processing. These findings demonstrate that open-source LLMs, when used with our framework, offer effective, scalable, and privacy-conscious solutions for clinical information extraction in low-resource settings.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning</title>
<link>https://arxiv.org/abs/2507.20906</link>
<guid>https://arxiv.org/abs/2507.20906</guid>
<content:encoded><![CDATA[
arXiv:2507.20906v1 Announce Type: new 
Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks by conditioning on input-output examples in the prompt, without requiring any update in model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is the most effective and efficient way to convey task information. In this work, we propose Soft Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings with attention head activations using pre-optimized mixing parameters, referred to as soft head-selection parameters. This method not only allows a desired task to be performed without in-prompt demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs, spanning four model families of sizes from 4B to 70B. Averaged across 57 tasks, our method outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant head positions selected by our method transfer across similar tasks but not across dissimilar ones -- underscoring the task-specific nature of head functionality. Our soft injection method opens a new paradigm for reducing prompt length and improving task performance by shifting task conditioning from the prompt space to the activation space.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2507.20917</link>
<guid>https://arxiv.org/abs/2507.20917</guid>
<content:encoded><![CDATA[
arXiv:2507.20917v1 Announce Type: new 
Abstract: This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2507.20924</link>
<guid>https://arxiv.org/abs/2507.20924</guid>
<content:encoded><![CDATA[
arXiv:2507.20924v1 Announce Type: new 
Abstract: Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2507.20930</link>
<guid>https://arxiv.org/abs/2507.20930</guid>
<content:encoded><![CDATA[
arXiv:2507.20930v1 Announce Type: new 
Abstract: Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.20956</link>
<guid>https://arxiv.org/abs/2507.20956</guid>
<content:encoded><![CDATA[
arXiv:2507.20956v1 Announce Type: new 
Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2507.21009</link>
<guid>https://arxiv.org/abs/2507.21009</guid>
<content:encoded><![CDATA[
arXiv:2507.21009v1 Announce Type: new 
Abstract: This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.
  These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation</title>
<link>https://arxiv.org/abs/2507.21028</link>
<guid>https://arxiv.org/abs/2507.21028</guid>
<content:encoded><![CDATA[
arXiv:2507.21028v1 Announce Type: new 
Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective</title>
<link>https://arxiv.org/abs/2507.19487</link>
<guid>https://arxiv.org/abs/2507.19487</guid>
<content:encoded><![CDATA[
arXiv:2507.19487v1 Announce Type: cross 
Abstract: People increasingly rely on AI-advice when making decisions. At times, such advice can promote selfish behavior. When individuals abide by selfishness-promoting AI advice, how are they perceived and punished? To study this question, we build on theories from social psychology and combine machine-behavior and behavioral economic approaches. In a pre-registered, financially-incentivized experiment, evaluators could punish real decision-makers who (i) received AI, human, or no advice. The advice (ii) encouraged selfish or prosocial behavior, and decision-makers (iii) behaved selfishly or, in a control condition, behaved prosocially. Evaluators further assigned responsibility to decision-makers and their advisors. Results revealed that (i) prosocial behavior was punished very little, whereas selfish behavior was punished much more. Focusing on selfish behavior, (ii) compared to receiving no advice, selfish behavior was penalized more harshly after prosocial advice and more leniently after selfish advice. Lastly, (iii) whereas selfish decision-makers were seen as more responsible when they followed AI compared to human advice, punishment between the two advice sources did not vary. Overall, behavior and advice content shape punishment, whereas the advice source does not.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedDPG: An Adaptive Yet Efficient Prompt-tuning Approach in Federated Learning Settings</title>
<link>https://arxiv.org/abs/2507.19534</link>
<guid>https://arxiv.org/abs/2507.19534</guid>
<content:encoded><![CDATA[
arXiv:2507.19534v1 Announce Type: cross 
Abstract: Pre-trained Language Models (PLMs) have demonstrated impressive performance in various NLP tasks. However, traditional fine-tuning methods for leveraging PLMs for downstream tasks entail significant computational overhead. Prompt-tuning has emerged as an efficient alternative that involves prepending a limited number of parameters to the input sequence and only updating them while the PLM's parameters are frozen. However, this technique's prompts remain fixed for all inputs, reducing the model's flexibility. The Federated Learning (FL) technique has gained attention in recent years to address the growing concerns around data privacy. However, challenges such as communication and computation limitations of clients still need to be addressed. To mitigate these challenges, this paper introduces the Federated Dynamic Prompt Generator (FedDPG), which incorporates a dynamic prompt generator network to generate context-aware prompts based on the given input, ensuring flexibility and adaptability while prioritising data privacy in federated learning settings. Our experiments on three NLP benchmark datasets showcase that FedDPG outperforms the state-of-the-art parameter-efficient fine-tuning methods in terms of global model performance, and has significantly reduced the calculation time and the number of parameters to be sent through the FL network.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Salsa as a Nonverbal Embodied Language -- The CoMPAS3D Dataset and Benchmarks</title>
<link>https://arxiv.org/abs/2507.19684</link>
<guid>https://arxiv.org/abs/2507.19684</guid>
<content:encoded><![CDATA[
arXiv:2507.19684v1 Announce Type: cross 
Abstract: Imagine a humanoid that can safely and creatively dance with a human, adapting to its partner's proficiency, using haptic signaling as a primary form of communication. While today's AI systems excel at text or voice-based interaction with large language models, human communication extends far beyond text-it includes embodied movement, timing, and physical coordination. Modeling coupled interaction between two agents poses a formidable challenge: it is continuous, bidirectionally reactive, and shaped by individual variation. We present CoMPAS3D, the largest and most diverse motion capture dataset of improvised salsa dancing, designed as a challenging testbed for interactive, expressive humanoid AI. The dataset includes 3 hours of leader-follower salsa dances performed by 18 dancers spanning beginner, intermediate, and professional skill levels. For the first time, we provide fine-grained salsa expert annotations, covering over 2,800 move segments, including move types, combinations, execution errors and stylistic elements. We draw analogies between partner dance communication and natural language, evaluating CoMPAS3D on two benchmark tasks for synthetic humans that parallel key problems in spoken language and dialogue processing: leader or follower generation with proficiency levels (speaker or listener synthesis), and duet (conversation) generation. Towards a long-term goal of partner dance with humans, we release the dataset, annotations, and code, along with a multitask SalsaAgent model capable of performing all benchmark tasks, alongside additional baselines to encourage research in socially interactive embodied AI and creative, expressive humanoid motion generation.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSign: Direct Pose-to-Text Translation for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2507.19840</link>
<guid>https://arxiv.org/abs/2507.19840</guid>
<content:encoded><![CDATA[
arXiv:2507.19840v1 Announce Type: cross 
Abstract: Continuously recognizing sign gestures and converting them to glosses plays a key role in bridging the gap between the hearing and hearing-impaired communities. This involves recognizing and interpreting the hands, face, and body gestures of the signer, which pose a challenge as it involves a combination of all these features. Continuous Sign Language Recognition (CSLR) methods rely on multi-stage pipelines that first extract visual features, then align variable-length sequences with target glosses using CTC or HMM-based approaches. However, these alignment-based methods suffer from error propagation across stages, overfitting, and struggle with vocabulary scalability due to the intermediate gloss representation bottleneck. To address these limitations, we propose AutoSign, an autoregressive decoder-only transformer that directly translates pose sequences to natural language text, bypassing traditional alignment mechanisms entirely. The use of this decoder-only approach allows the model to directly map between the features and the glosses without the need for CTC loss while also directly learning the textual dependencies in the glosses. Our approach incorporates a temporal compression module using 1D CNNs to efficiently process pose sequences, followed by AraGPT2, a pre-trained Arabic decoder, to generate text (glosses). Through comprehensive ablation studies, we demonstrate that hand and body gestures provide the most discriminative features for signer-independent CSLR. By eliminating the multi-stage pipeline, AutoSign achieves substantial improvements on the Isharah-1000 dataset, achieving an improvement of up to 6.1\% in WER score compared to the best existing method.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic Reinforced Policy Optimization</title>
<link>https://arxiv.org/abs/2507.19849</link>
<guid>https://arxiv.org/abs/2507.19849</guid>
<content:encoded><![CDATA[
arXiv:2507.19849v1 Announce Type: cross 
Abstract: Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Fine-tuning Large Language Models on Automated Program Repair</title>
<link>https://arxiv.org/abs/2507.19909</link>
<guid>https://arxiv.org/abs/2507.19909</guid>
<content:encoded><![CDATA[
arXiv:2507.19909v1 Announce Type: cross 
Abstract: Automated Program Repair (APR) uses various tools and techniques to help developers achieve functional and error-free code faster. In recent years, Large Language Models (LLMs) have gained popularity as components in APR tool chains because of their performance and flexibility. However, training such models requires a significant amount of resources. Fine-tuning techniques have been developed to adapt pre-trained LLMs to specific tasks, such as APR, and enhance their performance at far lower computational costs than training from scratch. In this study, we empirically investigate the impact of various fine-tuning techniques on the performance of LLMs used for APR. Our experiments provide insights into the performance of a selection of state-of-the-art LLMs pre-trained on code. The evaluation is done on three popular APR benchmarks (i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning, full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and IA3. We observe that full fine-tuning techniques decrease the benchmarking performance of various models due to different data distributions and overfitting. By using parameter-efficient fine-tuning methods, we restrict models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair, parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations</title>
<link>https://arxiv.org/abs/2507.19947</link>
<guid>https://arxiv.org/abs/2507.19947</guid>
<content:encoded><![CDATA[
arXiv:2507.19947v1 Announce Type: cross 
Abstract: Fusing information from human observations can help robots overcome sensing limitations in collaborative tasks. However, an uncertainty-aware fusion framework requires a grounded likelihood representing the uncertainty of human inputs. This paper presents a Feature Pyramid Likelihood Grounding Network (FP-LGN) that grounds spatial language by learning relevant map image features and their relationships with spatial relation semantics. The model is trained as a probability estimator to capture aleatoric uncertainty in human language using three-stage curriculum learning. Results showed that FP-LGN matched expert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated greater robustness with lower standard deviation. Collaborative sensing results demonstrated that the grounded likelihood successfully enabled uncertainty-aware fusion of heterogeneous human language observations and robot sensor measurements, achieving significant improvements in human-robot collaborative task performance.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization</title>
<link>https://arxiv.org/abs/2507.19973</link>
<guid>https://arxiv.org/abs/2507.19973</guid>
<content:encoded><![CDATA[
arXiv:2507.19973v1 Announce Type: cross 
Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research. Purpose: To develop and evaluate large language models (LLMs) that automatically extract PCL features from MRI/CT reports and assign risk categories based on guidelines. Materials and Methods: We curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Performance of Sequential Recommendation Systems with an Extended Large Language Model</title>
<link>https://arxiv.org/abs/2507.19990</link>
<guid>https://arxiv.org/abs/2507.19990</guid>
<content:encoded><![CDATA[
arXiv:2507.19990v1 Announce Type: cross 
Abstract: Recently, competition in the field of artificial intelligence (AI) has intensified among major technological companies, resulting in the continuous release of new large-language models (LLMs) that exhibit improved language understanding and context-based reasoning capabilities. It is expected that these advances will enable more efficient personalized recommendations in LLM-based recommendation systems through improved quality of training data and architectural design. However, many studies have not considered these recent developments. In this study, it was proposed to improve LLM-based recommendation systems by replacing Llama2 with Llama3 in the LlamaRec framework. To ensure a fair comparison, random seed values were set and identical input data was provided during preprocessing and training. The experimental results show average performance improvements of 38.65\%, 8.69\%, and 8.19\% for the ML-100K, Beauty, and Games datasets, respectively, thus confirming the practicality of this method. Notably, the significant improvements achieved by model replacement indicate that the recommendation quality can be improved cost-effectively without the need to make structural changes to the system. Based on these results, it is our contention that the proposed approach is a viable solution for improving the performance of current recommendation systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Carbon Cost of Conversation, Sustainability in the Age of Language Models</title>
<link>https://arxiv.org/abs/2507.20018</link>
<guid>https://arxiv.org/abs/2507.20018</guid>
<content:encoded><![CDATA[
arXiv:2507.20018v1 Announce Type: cross 
Abstract: Large language models (LLMs) like GPT-3 and BERT have revolutionized natural language processing (NLP), yet their environmental costs remain dangerously overlooked. This article critiques the sustainability of LLMs, quantifying their carbon footprint, water usage, and contribution to e-waste through case studies of models such as GPT-4 and energy-efficient alternatives like Mistral 7B. Training a single LLM can emit carbon dioxide equivalent to hundreds of cars driven annually, while data centre cooling exacerbates water scarcity in vulnerable regions. Systemic challenges corporate greenwashing, redundant model development, and regulatory voids perpetuate harm, disproportionately burdening marginalized communities in the Global South. However, pathways exist for sustainable NLP: technical innovations (e.g., model pruning, quantum computing), policy reforms (carbon taxes, mandatory emissions reporting), and cultural shifts prioritizing necessity over novelty. By analysing industry leaders (Google, Microsoft) and laggards (Amazon), this work underscores the urgency of ethical accountability and global cooperation. Without immediate action, AIs ecological toll risks outpacing its societal benefits. The article concludes with a call to align technological progress with planetary boundaries, advocating for equitable, transparent, and regenerative AI systems that prioritize both human and environmental well-being.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K^4$: Online Log Anomaly Detection Via Unsupervised Typicality Learning</title>
<link>https://arxiv.org/abs/2507.20051</link>
<guid>https://arxiv.org/abs/2507.20051</guid>
<content:encoded><![CDATA[
arXiv:2507.20051v1 Announce Type: cross 
Abstract: Existing Log Anomaly Detection (LogAD) methods are often slow, dependent on error-prone parsing, and use unrealistic evaluation protocols. We introduce $K^4$, an unsupervised and parser-independent framework for high-performance online detection. $K^4$ transforms arbitrary log embeddings into compact four-dimensional descriptors (Precision, Recall, Density, Coverage) using efficient k-nearest neighbor (k-NN) statistics. These descriptors enable lightweight detectors to accurately score anomalies without retraining. Using a more realistic online evaluation protocol, $K^4$ sets a new state-of-the-art (AUROC: 0.995-0.999), outperforming baselines by large margins while being orders of magnitude faster, with training under 4 seconds and inference as low as 4 $\mu$s.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v1 Announce Type: cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the EOS: Sequence Training for Detailed Image Captioning</title>
<link>https://arxiv.org/abs/2507.20077</link>
<guid>https://arxiv.org/abs/2507.20077</guid>
<content:encoded><![CDATA[
arXiv:2507.20077v1 Announce Type: cross 
Abstract: Despite significant advances in vision-language models (VLMs), image captioning often suffers from a lack of detail, with base models producing short, generic captions. This limitation persists even though VLMs are equipped with strong vision and language backbones. While supervised data and complex reward functions have been proposed to improve detailed image captioning, we identify a simpler underlying issue: a bias towards the end-of-sequence (EOS) token, which is introduced during cross-entropy training. We propose an unsupervised method to debias the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the generation of longer, more detailed captions without the need for intricate reward functions or supervision. Our approach is straightforward, effective, and easily applicable to any pretrained model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed captioning benchmarks. Our results show a substantial increase in caption length and relevant details, albeit with an expected increase in the rate of hallucinations.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoTransformer: Attention without Multiplication</title>
<link>https://arxiv.org/abs/2507.20096</link>
<guid>https://arxiv.org/abs/2507.20096</guid>
<content:encoded><![CDATA[
arXiv:2507.20096v1 Announce Type: cross 
Abstract: The Transformer, with its scaled dot-product attention mechanism, has become a foundational architecture in modern AI. However, this mechanism is computationally intensive and incurs substantial energy costs. We propose a new Transformer architecture EcoTransformer, in which the output context vector is constructed as the convolution of the values using a Laplacian kernel, where the distances are measured by the L1 metric between the queries and keys. Compared to dot-product based attention, the new attention score calculation is free of matrix multiplication. It performs on par with, or even surpasses, scaled dot-product attention in NLP, bioinformatics, and vision tasks, while consuming significantly less energy.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20150</link>
<guid>https://arxiv.org/abs/2507.20150</guid>
<content:encoded><![CDATA[
arXiv:2507.20150v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of large language and reasoning models (LLMs/LRMs). However, it often produces brittle and unstable policies, leading to critical failures such as spurious reasoning, deceptive alignment, and instruction disobedience that undermine the trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified theoretical explanation and are typically addressed using ad-hoc heuristics. This paper presents a rigorous mathematical framework for analyzing the stability of the mapping from a reward function to the optimal policy. We show that policy brittleness often stems from non-unique optimal actions, a common occurrence when multiple valid traces exist in a reasoning task. This theoretical lens provides a unified explanation for a range of seemingly disparate failures, reframing them as rational outcomes of optimizing rewards that may be incomplete or noisy, especially in the presence of action degeneracy. We extend this analysis from the fundamental single-reward setting to the more realistic multi-reward RL across diverse domains, showing how stability is governed by an "effective reward" aggregation mechanism. We also prove that entropy regularization restores policy stability at the cost of increased stochasticity. Our framework provides a unified explanation for recent empirical findings on deceptive reasoning, instruction-following trade-offs, and RLHF-induced sophistry, and is further validated through perturbation experiments in multi-reward RL. This work advances policy-stability analysis from empirical heuristics towards a principled theory, offering essential insights for designing safer and more trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration</title>
<link>https://arxiv.org/abs/2507.20280</link>
<guid>https://arxiv.org/abs/2507.20280</guid>
<content:encoded><![CDATA[
arXiv:2507.20280v1 Announce Type: cross 
Abstract: Scientific research increasingly relies on specialized computational tools, yet effectively utilizing these tools demands substantial domain expertise. While Large Language Models (LLMs) show promise in tool automation, they struggle to seamlessly integrate and orchestrate multiple tools for complex scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that automates hundreds of scientific tools across biology, chemistry, and materials science. At its core, SciToolAgent leverages a scientific tool knowledge graph that enables intelligent tool selection and execution through graph-based retrieval-augmented generation. The agent also incorporates a comprehensive safety-checking module to ensure responsible and ethical tool usage. Extensive evaluations on a curated benchmark demonstrate that SciToolAgent significantly outperforms existing approaches. Case studies in protein engineering, chemical reactivity prediction, chemical synthesis, and metal-organic framework screening further demonstrate SciToolAgent's capability to automate complex scientific workflows, making advanced research tools accessible to both experts and non-experts.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MountainLion: A Multi-Modal LLM-Based Agent System for Interpretable and Adaptive Financial Trading</title>
<link>https://arxiv.org/abs/2507.20474</link>
<guid>https://arxiv.org/abs/2507.20474</guid>
<content:encoded><![CDATA[
arXiv:2507.20474v1 Announce Type: cross 
Abstract: Cryptocurrency trading is a challenging task requiring the integration of heterogeneous data from multiple modalities. Traditional deep learning and reinforcement learning approaches typically demand large training datasets and encode diverse inputs into numerical representations, often at the cost of interpretability. Recent progress in large language model (LLM)-based agents has demonstrated the capacity to process multi-modal data and support complex investment decision-making. Building on these advances, we present \textbf{MountainLion}, a multi-modal, multi-agent system for financial trading that coordinates specialized LLM-based agents to interpret financial data and generate investment strategies. MountainLion processes textual news, candlestick charts, and trading signal charts to produce high-quality financial reports, while also enabling modification of reports and investment recommendations through data-driven user interaction and question answering. A central reflection module analyzes historical trading signals and outcomes to continuously refine decision processes, and the system is capable of real-time report analysis, summarization, and dynamic adjustment of investment strategies. Empirical results confirm that MountainLion systematically enriches technical price triggers with contextual macroeconomic and capital flow signals, providing a more interpretable, robust, and actionable investment framework that improves returns and strengthens investor confidence.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customize Multi-modal RAI Guardrails with Precedent-based predictions</title>
<link>https://arxiv.org/abs/2507.20503</link>
<guid>https://arxiv.org/abs/2507.20503</guid>
<content:encoded><![CDATA[
arXiv:2507.20503v1 Announce Type: cross 
Abstract: A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model's judgment on "precedents", which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
]]></content:encoded>
<pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>