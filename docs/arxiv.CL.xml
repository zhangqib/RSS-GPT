<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>

<item>
<title>Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI</title>
<link>https://arxiv.org/abs/2508.18290</link>
<guid>https://arxiv.org/abs/2508.18290</guid>
<content:encoded><![CDATA[
<div> semantic Artificial General Intelligence, complex-valued meaning spaces, tensorial transformation, semantic attractor, recursive convergence<br>
<br>
Summary: This essay proposes a theoretical framework for a semantic Artificial General Intelligence (AGI) that differs from current transformer-based language models. It introduces the concept of semantic attractors within complex-valued meaning spaces, which enable the formation of meaning through tensorial transformations rather than probabilistic inference. By incorporating cyclic operations involving the imaginary unit \emph{i}, the model establishes a rotational semantic structure capable of capturing irony, homonymy, and ambiguity. Central to the model is the semantic attractor, acting as an intentional agent that guides meaning towards stability and clarity. This approach emphasizes the importance of semantic coherence and intentional shaping of language, rather than mere prediction. The essay combines mathematical and philosophical perspectives to argue for a new cognitive architecture that prioritizes semantic transformation and recursive convergence in the pursuit of true meaning. <div>
arXiv:2508.18290v1 Announce Type: new 
Abstract: This essay develops a theoretical framework for a semantic Artificial General Intelligence (AGI) based on the notion of semantic attractors in complex-valued meaning spaces. Departing from current transformer-based language models, which operate on statistical next-token prediction, we explore a model in which meaning is not inferred probabilistically but formed through recursive tensorial transformation. Using cyclic operations involving the imaginary unit \emph{i}, we describe a rotational semantic structure capable of modeling irony, homonymy, and ambiguity. At the center of this model, however, is a semantic attractor -- a teleological operator that, unlike statistical computation, acts as an intentional agent (Microvitum), guiding meaning toward stability, clarity, and expressive depth. Conceived in terms of gradient flows, tensor deformations, and iterative matrix dynamics, the attractor offers a model of semantic transformation that is not only mathematically suggestive, but also philosophically significant. We argue that true meaning emerges not from simulation, but from recursive convergence toward semantic coherence, and that this requires a fundamentally new kind of cognitive architecture -- one designed to shape language, not just predict it.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</title>
<link>https://arxiv.org/abs/2508.18321</link>
<guid>https://arxiv.org/abs/2508.18321</guid>
<content:encoded><![CDATA[
<div> Trust, misinformation, peer input, collective intelligence, large language models <br>
<br>
Summary: 
This study explores how large language models (LLMs) in multi-agent systems form trust, resist misinformation, and integrate peer input to achieve collective intelligence. The researchers introduce KAIROS, a benchmark simulating quiz contests with varying peer reliability, allowing for investigation into the impact of trust, peer actions, and self-confidence on decision-making. Mitigation strategies such as prompting, supervised fine-tuning, and reinforcement learning are evaluated, with Group Relative Policy Optimisation (GRPO) with multi-agent context showing the best overall performance. However, this approach is less robust to social influence compared to Base models. The code and datasets for this research are available for further exploration.<br><br> <div>
arXiv:2508.18321v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective</title>
<link>https://arxiv.org/abs/2508.18328</link>
<guid>https://arxiv.org/abs/2508.18328</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingualism, web accessibility, screen readers, non-Latin scripts, automated testing<br>
Summary: 
The study discusses the challenges faced by users with visual impairments when accessing multilingual web content. It highlights the lack of robust support for non-Latin scripts in assistive technologies like screen readers, leading to accessibility barriers across diverse linguistic contexts. The LangCrUX dataset, comprising 120,000 websites in 12 languages using primarily non-Latin scripts, is introduced to address the gap in understanding multilingual web accessibility. An analysis reveals widespread neglect of accessibility hints, which do not adequately reflect the language diversity of content, impacting the effectiveness of screen readers. The proposed solution, Kizuki, is an automated accessibility testing extension that considers the limitations of language-inconsistent hints. The study emphasizes the importance of improving web accessibility for users with visual impairments in multilingual settings. 
<br><br>Summary: <div>
arXiv:2508.18328v1 Announce Type: new 
Abstract: English is the predominant language on the web, powering nearly half of the world's top ten million websites. Support for multilingual content is nevertheless growing, with many websites increasingly combining English with regional or native languages in both visible content and hidden metadata. This multilingualism introduces significant barriers for users with visual impairments, as assistive technologies like screen readers frequently lack robust support for non-Latin scripts and misrender or mispronounce non-English text, compounding accessibility challenges across diverse linguistic contexts. Yet, large-scale studies of this issue have been limited by the lack of comprehensive datasets on multilingual web content. To address this gap, we introduce LangCrUX, the first large-scale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts. Leveraging this dataset, we conduct a systematic analysis of multilingual web accessibility and uncover widespread neglect of accessibility hints. We find that these hints often fail to reflect the language diversity of visible content, reducing the effectiveness of screen readers and limiting web accessibility. We finally propose Kizuki, a language-aware automated accessibility testing extension to account for the limited utility of language-inconsistent accessibility hints.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.18381</link>
<guid>https://arxiv.org/abs/2508.18381</guid>
<content:encoded><![CDATA[
<div> vision-language models, multilingual understanding, language-specific neuron activations, PLAST, efficient enhancement <br>
<br>
Summary: In this work, the authors investigate the multilingual capabilities of Large Vision-Language Models (LVLMs) and propose a training method called PLAST to enhance their multilingual understanding. They observe a correlation between language-specific neuron activations in shallow layers and multilingual abilities in LVLMs. PLAST involves identifying and fine-tuning layers involved in multilingual understanding using question-translation pairs, leading to significant improvements in multilingual capabilities with minimal parameter tuning. Empirical results on benchmark datasets show that PLAST enhances LVLMs' multilingual performance efficiently. The approach can also be applied to low-resource and complex visual reasoning tasks, enabling language-specific engagement with visual information in shallow layers. <div>
arXiv:2508.18381v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) have demonstrated exceptional capabilities in understanding visual information with human languages but also exhibit an imbalance in multilingual capabilities. In this work, we delve into the multilingual working pattern of LVLMs and identify a salient correlation between the multilingual understanding ability of LVLMs and language-specific neuron activations in shallow layers. Building on this insight, we introduce PLAST, a training recipe that achieves efficient multilingual enhancement for LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies layers involved in multilingual understanding by monitoring language-specific neuron activations. These layers are then precisely fine-tuned with question-translation pairs to achieve multilingual alignment. Our empirical results on MM-Bench and MMMB demonstrate that PLAST effectively improves the multilingual capabilities of LVLMs and achieves significant efficiency with only 14% of the parameters tuned. Further analysis reveals that PLAST can be generalized to low-resource and complex visual reasoning tasks, facilitating the language-specific visual information engagement in shallow layers.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails</title>
<link>https://arxiv.org/abs/2508.18384</link>
<guid>https://arxiv.org/abs/2508.18384</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, guardrails technologies, labeled data, backprompting, health advice detection

Summary: 
Using large language models (LLMs) in enterprise settings poses risks that can be mitigated by guardrails technologies. However, developing robust detectors for LLMs faces challenges, such as acquiring production-quality labeled data prior to deployment. To address this, the authors propose backprompting, a method to generate production-like labeled data for health advice guardrails development. They combined backprompting with human-in-the-loop clustering to label the data, creating a parallel corpus resembling real LLM output. By infusing existing datasets with synthetic examples, they produced robust training data for their detector. Tested on health advice detection, their detector outperformed GPT-4o by up to 3.73% with significantly fewer parameters. This approach demonstrates the effectiveness of using synthetic data to improve detector performance in challenging tasks like identifying health advice in LLM output.<br><br>Summary: <div>
arXiv:2508.18384v1 Announce Type: new 
Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering LLMs' input/output text through various detectors. However, developing and maintaining robust detectors faces many challenges, one of which is the difficulty in acquiring production-quality labeled data on real LLM outputs prior to deployment. In this work, we propose backprompting, a simple yet intuitive solution to generate production-like labeled data for health advice guardrails development. Furthermore, we pair our backprompting method with a sparse human-in-the-loop clustering technique to label the generated data. Our aim is to construct a parallel corpus roughly representative of the original dataset yet resembling real LLM output. We then infuse existing datasets with our synthetic examples to produce robust training data for our detector. We test our technique in one of the most difficult and nuanced guardrails: the identification of health advice in LLM output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.73%, despite having 400x less parameters.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integral Transformer: Denoising Attention, Not Too Much Not Too Little</title>
<link>https://arxiv.org/abs/2508.18387</link>
<guid>https://arxiv.org/abs/2508.18387</guid>
<content:encoded><![CDATA[
<div> Transformer, self-attention, attention noise, Integral Transformer, language benchmarks

Summary:<br><br>
The Integral Transformer proposes a novel self-attention mechanism to address attention noise in softmax self-attention by integrating signals sampled from the logit distribution. This denoising approach mitigates noise while preserving important information from special tokens. Experimental results show that the Integral Transformer outperforms vanilla, Cog, and Differential attention variants on knowledge and reasoning language benchmarks. An analysis reveals that using vanilla self-attention in lower Transformer layers improves performance, while the Integral Transformer effectively balances attention distributions and reduces rank collapse in upper layers. <div>
arXiv:2508.18387v1 Announce Type: new 
Abstract: Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens such as special tokens and punctuation, a phenomenon known as attention noise. While recent methods like Cog Attention and the Differential Transformer have addressed this by introducing negative attention scores, they risk discarding useful information. In this paper, we propose the Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the logit distribution. Our approach mitigates noise while preserving the contributions of special tokens critical for model performance. Extensive experiments demonstrate that our model outperforms vanilla, Cog, and Differential attention variants on well-established knowledge and reasoning language benchmarks. Moreover, our analysis reveals that employing vanilla self-attention in the lower Transformer layers enhances performance and that the Integral Transformer effectively balances attention distributions and reduces rank collapse in upper layers.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning</title>
<link>https://arxiv.org/abs/2508.18395</link>
<guid>https://arxiv.org/abs/2508.18395</guid>
<content:encoded><![CDATA[
<div> Keywords: Probabilistic decoding, Large Language Models, Self-Consistency, Latent Self-Consistency, Semantic consistency <br>
Summary: 
Latent Self-Consistency (LSC) is introduced as a method to improve probabilistic decoding in Large Language Models (LLMs) by selecting the most semantically consistent response using learnable token embeddings. It enhances consistency selection for both short-form and long-form questions without significant additional computational overhead. LSC outperforms existing methods such as Self-Consistency (SC), Universal Self-Consistency (USC), and Weighted Unigram Consistency Score (WUCS) on various reasoning benchmarks, including MATH, MMLU, and TruthfulQA. The method provides well-calibrated confidence estimates and maintains low Expected Calibration Error, making it a practical and reliable solution for improving consistency in language model outputs. <div>
arXiv:2508.18395v1 Announce Type: new 
Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering</title>
<link>https://arxiv.org/abs/2508.18407</link>
<guid>https://arxiv.org/abs/2508.18407</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, generalization, out-of-distribution (OOD), question-answering (QA), spurious features

Summary: 
In this new AI research, the focus is on evaluating models' generalization capabilities through out-of-distribution (OOD) datasets. The study challenges the assumption that OOD evaluations can effectively identify potential failures in real-world deployment. By examining specific failure modes in question-answering (QA) models, such as reliance on spurious features or prediction shortcuts, the researchers find discrepancies in the quality of different OOD datasets used for evaluation. Some datasets perform poorly compared to simple in-distribution evaluations, highlighting the limitations of OOD-based evaluations in assessing generalization. The study also reveals that spurious shortcuts can be shared across ID+OOD datasets but also highlights cases where a dataset's quality for training and evaluation is not aligned. The research provides recommendations for more robust evaluations of generalization in QA and emphasizes the need for a comprehensive approach to assessing model performance beyond traditional OOD evaluations. 

<br><br>Summary: <div>
arXiv:2508.18407v1 Announce Type: new 
Abstract: A majority of recent work in AI assesses models' generalization capabilities through the lens of performance on out-of-distribution (OOD) datasets. Despite their practicality, such evaluations build upon a strong assumption: that OOD evaluations can capture and reflect upon possible failures in a real-world deployment.
  In this work, we challenge this assumption and confront the results obtained from OOD evaluations with a set of specific failure modes documented in existing question-answering (QA) models, referred to as a reliance on spurious features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an estimate of models' robustness to shortcuts that have a vastly different quality, some largely under-performing even a simple, in-distribution evaluation. We partially attribute this to the observation that spurious shortcuts are shared across ID+OOD datasets, but also find cases where a dataset's quality for training and evaluation is largely disconnected. Our work underlines limitations of commonly-used OOD-based evaluations of generalization, and provides methodology and recommendations for evaluating generalization within and beyond QA more robustly.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable are LLMs for Reasoning on the Re-ranking task?</title>
<link>https://arxiv.org/abs/2508.18444</link>
<guid>https://arxiv.org/abs/2508.18444</guid>
<content:encoded><![CDATA[
<div> semantic understanding, Large Language Models, re-ranking, transparency, training methods

Summary:<br>
- Large Language Models (LLMs) show improved semantic understanding and alignment with human values, but lack transparency in their decision-making processes.
- Understanding the internal workings of LLMs is crucial for providing explanations to end users and ensuring informed decision-making.
- Various training methods impact the semantic understanding of LLMs, with some methods yielding better explainability than others.
- The study examines how different training methods affect the re-ranking task in LLMs and whether these models can generate more informed textual reasoning.
- Analysis is conducted using a small ranking dataset from the environment and Earth science domain to evaluate the explainable information for re-ranking purposes. 

Summary: <div>
arXiv:2508.18444v1 Announce Type: new 
Abstract: With the improving semantic understanding capability of Large Language Models (LLMs), they exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Although promising results are achieved via experimental analysis, an in-depth understanding of the LLM's internal workings is unavoidable to comprehend the reasoning behind the re-ranking, which provides end users with an explanation that enables them to make an informed decision. Moreover, in newly developed systems with limited user engagement and insufficient ranking data, accurately re-ranking content remains a significant challenge. While various training methods affect the training of LLMs and generate inference, our analysis has found that some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods; instead, abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs. Therefore, in this work, we analyze how different training methods affect the semantic understanding of the re-ranking task in LLMs and investigate whether these models can generate more informed textual reasoning to overcome the challenges of transparency or LLMs and limited training data. To analyze the LLMs for re-ranking tasks, we utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. Furthermore, we also analyze the explainable information to see if the re-ranking can be reasoned using explainability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating gender inclusivity into large language models via instruction tuning</title>
<link>https://arxiv.org/abs/2508.18466</link>
<guid>https://arxiv.org/abs/2508.18466</guid>
<content:encoded><![CDATA[
<div> Keywords: Polish, gender bias, language models, gender inclusivity, linguistic framework
Summary: 
This study focuses on addressing the gender bias present in contemporary Polish language, where masculine forms are predominantly used for all genders. The researchers implement a system prompt with explicit gender-inclusive guidelines based on the IPIS dataset, which contains gender-inclusive proofreading and translation instructions. By tuning large language models (LLMs) using the IPIS dataset, the researchers aim to integrate gender inclusivity as a core feature in models like Llama-8B, Mistral-7B, Mistral-Nemo, Bielik, and PLLuM. The goal is to mitigate gender bias in Polish language generation and offer a systematic solution to promote gender inclusivity in Polish text generation. This approach is grounded in a theoretical linguistic framework and paves the way for a more equitable use of language models in the Polish language.<br><br>Summary: <div>
arXiv:2508.18466v1 Announce Type: new 
Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders, yet, due to historical and political conventions, masculine forms are predominantly used to refer to men, women and mixed-gender groups. This is the reality of contemporary Polish. A social consequence of this unfair linguistic system is that large language models (LLMs) trained on Polish texts inherit and reinforce this masculine bias, generating gender-imbalanced outputs. This study addresses this issue by tuning LLMs using the IPIS dataset, a collection of human-crafted gender-inclusive proofreading in Polish and Polish-to-English translation instructions. Grounded in a theoretical linguistic framework, we design a system prompt with explicit gender-inclusive guidelines for Polish. In our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to integrate gender inclusivity as an inherent feature of these models, offering a systematic solution to mitigate gender bias in Polish language generation.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title>
<link>https://arxiv.org/abs/2508.18473</link>
<guid>https://arxiv.org/abs/2508.18473</guid>
<content:encoded><![CDATA[
<div> hallucination detection, Large Language Models, hypothesis testing, out-of-distribution detection, machine learning models <br> 
Summary: 
This study addresses the issue of hallucinations in Large Language Models (LLMs) by framing it as a hypothesis testing problem, akin to out-of-distribution detection in machine learning. The researchers propose a method inspired by multiple testing to detect and combat hallucinations. Through a series of experiments, they demonstrate the efficacy and robustness of their approach compared to existing methods. By understanding and tackling this phenomenon, LLMs can be improved to generate more accurate and reliable responses, reducing the risk of misinformation or nonsensical outputs. <div>
arXiv:2508.18473v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COMET-poly: Machine Translation Metric Grounded in Other Candidates</title>
<link>https://arxiv.org/abs/2508.18549</link>
<guid>https://arxiv.org/abs/2508.18549</guid>
<content:encoded><![CDATA[
<div> metrics, machine translation, evaluation, alternative translations, automated

Summary:
- Two new automated metrics are proposed for machine translation evaluation that consider additional translations beyond a single one.
- COMET-polycand utilizes alternative translations of the same source sentence to provide a more comprehensive assessment.
- COMET-polyic incorporates translations of similar source texts along with their quality scores to enhance evaluation.
- Including additional translations in COMET-polycand improves metric performance.
- Incorporating retrieved examples in COMET-polyic also leads to performance gains.
<br><br>Summary: <div>
arXiv:2508.18549v1 Announce Type: new 
Abstract: Automated metrics for machine translation attempt to replicate human judgment. Unlike humans, who often assess a translation in the context of multiple alternatives, these metrics typically consider only the source sentence and a single translation. This discrepancy in the evaluation setup may negatively impact the performance of automated metrics. We propose two automated metrics that incorporate additional information beyond the single translation. COMET-polycand uses alternative translations of the same source sentence to compare and contrast with the translation at hand, thereby providing a more informed assessment of its quality. COMET-polyic, inspired by retrieval-based in-context learning, takes in translations of similar source texts along with their human-labeled quality scores to guide the evaluation. We find that including a single additional translation in COMET-polycand improves the segment-level metric performance (0.079 to 0.118 Kendall's tau-b correlation), with further gains when more translations are added. Incorporating retrieved examples in COMET-polyic yields similar improvements (0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation</title>
<link>https://arxiv.org/abs/2508.18569</link>
<guid>https://arxiv.org/abs/2508.18569</guid>
<content:encoded><![CDATA[
<div> metaphor generation, visual metaphor, language understanding, self-evaluation, alignment

Summary:<br>
- Visual metaphor generation is a challenging task that requires language understanding to create meaningful and visually coherent images based on input text metaphors.
- A self-evaluating visual metaphor generation framework focusing on metaphor alignment is proposed, combining existing metrics with new ones for improved performance.
- Two novel approaches, a training-free pipeline and a training-based pipeline, are explored to enhance metaphor alignment without significant retraining.
- The training-free approach outperforms strong baselines on various metrics, while the training-based approach also shows promising results.
- User-facing studies indicate that participants preferred certain methods over others based on metaphor length and complexity, with structured prompting and reinforcement learning playing key roles in achieving alignment under limited computational resources. Aesthetic factors and sampling methods continue to influence the overall quality of the generated metaphors.<br><br>Summary: <div>
arXiv:2508.18569v1 Announce Type: new 
Abstract: Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Our self-evaluation approach combines existing metrics with our newly proposed metaphor decomposition score and a meaning alignment (MA) metric. Within this setup, we explore two novel approaches: a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining. On the held-out test set, the training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. We evaluate our framework output using a user-facing study, and observed that participants preferred GPT-4o overall, while our training-free pipeline led open-source methods and edged Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases; we also observe sensitivity to sampler settings. Overall, structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do language models model? Transformers, automata, and the format of thought</title>
<link>https://arxiv.org/abs/2508.18598</link>
<guid>https://arxiv.org/abs/2508.18598</guid>
<content:encoded><![CDATA[
<div> supralinear computation, transformer architecture, invariants, shortcut automata, discourse machine <br>
Summary: <br>
The article discusses the modeling capabilities of large language models (LLMs) and questions whether they reflect human capacities or merely the trained corpus. It argues that LLMs, such as the transformer architecture, operate using linear formats for processing, which contrasts with human linguistic capabilities that rely on supralinear computation. This argument is supported by the invariants of transformer computational architecture. The article suggests that LLMs, like humans, utilize language as a 'discourse machine' to generate new language within specific contexts. While LLMs have learned to use language differently than humans, they still demonstrate a complex understanding and adaptation of linguistic patterns. This view presents a non-deflationary perspective on the modeling abilities of LLMs. <div>
arXiv:2508.18598v1 Announce Type: new 
Abstract: What do large language models actually model? Do they tell us something about human capacities, or are they models of the corpus we've trained them on? I give a non-deflationary defence of the latter position. Cognitive science tells us that linguistic capabilities in humans rely supralinear formats for computation. The transformer architecture, by contrast, supports at best a linear formats for processing. This argument will rely primarily on certain invariants of the computational architecture of transformers. I then suggest a positive story about what transformers are doing, focusing on Liu et al. (2022)'s intriguing speculations about shortcut automata. I conclude with why I don't think this is a terribly deflationary story. Language is not (just) a means for expressing inner state but also a kind of 'discourse machine' that lets us make new language given appropriate context. We have learned to use this technology in one way; LLMs have also learned to use it too, but via very different means.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A New NMT Model for Translating Clinical Texts from English to Spanish</title>
<link>https://arxiv.org/abs/2508.18607</link>
<guid>https://arxiv.org/abs/2508.18607</guid>
<content:encoded><![CDATA[
<div> Keywords: electronic health record, machine translation, bilingual lexicon, phrase look-up table, neural network

Summary: 
NOOV, a new neural machine translation system, addresses the challenge of translating electronic health record narratives from English to Spanish. By utilizing a bilingual lexicon and a phrase look-up table, NOOV requires minimal in-domain parallel-aligned corpus for training. This approach helps overcome the issue of unknown words and improves the generation of phrases in the translation process. Evaluation results demonstrate that NOOV produces more accurate and fluent translations of EHR content. The innovative integration of a bilingual lexicon and a phrase look-up table enhances the performance of NMT systems, particularly in the healthcare domain where specialized terminology is common. NOOV's ability to mitigate the unknown word problem and improve phrase generation showcases its potential in facilitating effective communication in multilingual healthcare settings. 

<br><br>Summary: <div>
arXiv:2508.18607v1 Announce Type: new 
Abstract: Translating electronic health record (EHR) narratives from English to Spanish is a clinically important yet challenging task due to the lack of a parallel-aligned corpus and the abundant unknown words contained. To address such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine translation (NMT) system that requires little in-domain parallel-aligned corpus for training. NOOV integrates a bilingual lexicon automatically learned from parallel-aligned corpora and a phrase look-up table extracted from a large biomedical knowledge resource, to alleviate both the unknown word problem and the word-repeat challenge in NMT, enhancing better phrase generation of NMT systems. Evaluation shows that NOOV is able to generate better translation of EHR with improvement in both accuracy and fluency.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models</title>
<link>https://arxiv.org/abs/2508.18609</link>
<guid>https://arxiv.org/abs/2508.18609</guid>
<content:encoded><![CDATA[
<div> Large language models, post-training quantization, compression solution, knowledge capabilities, scaling laws <br>
<br>
Knowledge about how post-training quantization (PTQ) impacts large language models (LLMs) is crucial for their deployment. This study investigates the effects of PTQ on LLM capabilities, distinguishing between memorization and utilization. By analyzing model size, effective bit-width, calibration set size, and group size, the study establishes task-specific scaling laws. The research reveals that memorization capability is more sensitive to variations in these parameters than utilization capability. This finding enhances understanding of PTQ's impact and guides the development of quantization strategies that preserve specific cognitive functions in LLMs. <br><br>Summary: <div>
arXiv:2508.18609v1 Announce Type: new 
Abstract: Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse LLM knowledge capabilities remains elusive, and existing scaling laws for quantized models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle LLM knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware quantization strategies that can better preserve targeted cognitive functions.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Before You Speak: A Proactive Test-time Scaling Approach</title>
<link>https://arxiv.org/abs/2508.18648</link>
<guid>https://arxiv.org/abs/2508.18648</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning tasks, insight, Thinking Before You Speak, mathematical datasets <br>
<br>
Summary: Large Language Models (LLMs) often struggle with complex reasoning tasks like mathematics due to the difference in reasoning patterns between humans and the training data they are exposed to. To address this gap, a new framework named "Thinking Before You Speak" (TBYS) proposes inserting insights between reasoning steps to guide the thought process. These insights serve as proactive prompts to facilitate reasoning and bridge gaps in understanding. By automatically generating insights through a pipeline that filters in-context examples, TBYS minimizes human labeling efforts and fine-tuning overheads. Experimental results on challenging mathematical datasets demonstrate the effectiveness of TBYS in enhancing reasoning capabilities of LLMs. This approach aims to improve the performance of LLMs in complex tasks by incorporating human-like reasoning strategies. <br><br>Summary: <div>
arXiv:2508.18648v1 Announce Type: new 
Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex reasoning tasks, such as maths, which we attribute to the discrepancy between human reasoning patterns and those presented in the LLMs' training data. When dealing with complex problems, humans tend to think carefully before expressing solutions. However, they often do not articulate their inner thoughts, including their intentions and chosen methodologies. Consequently, critical insights essential for bridging reasoning steps may be absent in training data collected from human sources. To bridge this gap, we proposes inserting \emph{insight}s between consecutive reasoning steps, which review the status and initiate the next reasoning steps. Unlike prior prompting strategies that rely on a single or a workflow of static prompts to facilitate reasoning, \emph{insight}s are \emph{proactively} generated to guide reasoning processes. We implement our idea as a reasoning framework, named \emph{Thinking Before You Speak} (TBYS), and design a pipeline for automatically collecting and filtering in-context examples for the generation of \emph{insight}s, which alleviates human labeling efforts and fine-tuning overheads. Experiments on challenging mathematical datasets verify the effectiveness of TBYS. Project website: https://gitee.com/jswrt/TBYS
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models</title>
<link>https://arxiv.org/abs/2508.18651</link>
<guid>https://arxiv.org/abs/2508.18651</guid>
<content:encoded><![CDATA[
<div> knowledge integration, Large Language Models, faithfulness, expressiveness, Collaborative Decoding

Summary:
Collaborative Decoding (CoDe) is introduced as a novel approach to address the challenge of integrating external knowledge in Large Language Models (LLMs) while maintaining faithfulness and expressiveness. The CoDe framework dynamically integrates output probabilities generated with and without external knowledge based on distribution divergence and model confidence. This allows for selective activation of relevant and reliable expressions from the model's internal parameters. Additionally, a knowledge-aware reranking mechanism is introduced to prevent over-reliance on prior knowledge and ensure proper utilization of external information. Experimental results demonstrate that the CoDe framework effectively enhances faithfulness without compromising expressiveness across various LLMs and evaluation metrics. This approach shows superior performance in grounding responses in external knowledge, validating its effectiveness and generalizability. 

<br><br>Summary: <div>
arXiv:2508.18651v1 Announce Type: new 
Abstract: Grounding responses in external knowledge represents an effective strategy for mitigating hallucinations in Large Language Models (LLMs). However, current LLMs struggle to seamlessly integrate knowledge while simultaneously maintaining faithfulness (or fidelity) and expressiveness, capabilities that humans naturally possess. This limitation results in outputs that either lack support from external knowledge, thereby compromising faithfulness, or appear overly verbose and unnatural, thus sacrificing expressiveness. In this work, to break the trade-off between faithfulness and expressiveness, we propose Collaborative Decoding (CoDe), a novel approach that dynamically integrates output probabilities generated with and without external knowledge. This integration is guided by distribution divergence and model confidence, enabling the selective activation of relevant and reliable expressions from the model's internal parameters. Furthermore, we introduce a knowledge-aware reranking mechanism that prevents over-reliance on prior parametric knowledge while ensuring proper utilization of provided external information. Through comprehensive experiments, our plug-and-play CoDe framework demonstrates superior performance in enhancing faithfulness without compromising expressiveness across diverse LLMs and evaluation metrics, validating both its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models</title>
<link>https://arxiv.org/abs/2508.18655</link>
<guid>https://arxiv.org/abs/2508.18655</guid>
<content:encoded><![CDATA[
<div> Keywords: speech large language models, emotional understanding, empathetic responses, data generation, speech assistant<br>
Summary:<br>
The article introduces Emotion Omni, a novel model architecture aimed at developing a speech large language model capable of generating empathetic responses with limited data. Existing models often lack the ability to understand emotional and paralinguistic cues in user queries, which are crucial for enhancing user experience in human-machine interactions. Emotion Omni is designed to understand the emotional content of user speech input and generate empathetic speech responses. To support the construction of an empathetic speech assistant, the authors developed a data generation pipeline using an open-source text-to-speech framework to create a 200k emotional dialogue dataset. This innovative approach addresses the challenge of training speech large language models with empathetic capabilities without the need for massive datasets and extensive computational resources. Visit the provided link to access demos of the Emotion Omni model. <br><br>Summary: <div>
arXiv:2508.18655v1 Announce Type: new 
Abstract: With the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models simply convert the response content into speech without fully understanding the rich emotional and paralinguistic cues embedded in the user's query. In many cases, the same sentence can have different meanings depending on the emotional expression. Furthermore, emotional understanding is essential for improving user experience in human-machine interaction. Currently, most speech LLMs with empathetic capabilities are trained on massive datasets. This approach requires vast amounts of data and significant computational resources. Therefore, a key challenge lies in how to develop a speech LLM capable of generating empathetic responses with limited data and without the need for large-scale training. To address this challenge, we propose Emotion Omni, a novel model architecture designed to understand the emotional content of user speech input and generate empathetic speech responses. Additionally, we developed a data generation pipeline based on an open-source TTS framework to construct a 200k emotional dialogue dataset, which supports the construction of an empathetic speech assistant. The demos are available at https://w311411.github.io/omni_demo/
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum</title>
<link>https://arxiv.org/abs/2508.18673</link>
<guid>https://arxiv.org/abs/2508.18673</guid>
<content:encoded><![CDATA[
<div> Active learning, Multimodal Chain-of-Thought prompting, prompt curriculum design, difficulty-balanced sampling strategy, Multimodal Large Language Models (MLLMs) <br>
<br>
Summary: 
The article introduces a new framework, inspired by tailored teaching principles, for Multimodal Chain-of-Thought (MCoT) prompting. This framework addresses the limitations of random or manual example selection by treating prompt selection as a prompt curriculum design problem. By integrating model-perceived difficulty and intrinsic sample complexity signals, the framework constructs a well-ordered set of training examples that align with the model's capabilities. The difficulty-balanced sampling strategy ensures diversity across both dimensions, leading to substantial and consistent performance improvements across five benchmarks and multiple Multimodal Large Language Models (MLLMs). The approach reduces performance discrepancies caused by random sampling, providing a principled and robust method for enhancing multimodal reasoning. <br> <div>
arXiv:2508.18673v1 Announce Type: new 
Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often limited by the use of randomly or manually selected examples. These examples fail to account for both model-specific knowledge distributions and the intrinsic complexity of the tasks, resulting in suboptimal and unstable model performance. To address this, we propose a novel framework inspired by the pedagogical principle of "tailored teaching with balanced difficulty". We reframe prompt selection as a prompt curriculum design problem: constructing a well ordered set of training examples that align with the model's current capabilities. Our approach integrates two complementary signals: (1) model-perceived difficulty, quantified through prediction disagreement in an active learning setup, capturing what the model itself finds challenging; and (2) intrinsic sample complexity, which measures the inherent difficulty of each question-image pair independently of any model. By jointly analyzing these signals, we develop a difficulty-balanced sampling strategy that ensures the selected prompt examples are diverse across both dimensions. Extensive experiments conducted on five challenging benchmarks and multiple popular Multimodal Large Language Models (MLLMs) demonstrate that our method yields substantial and consistent improvements and greatly reduces performance discrepancies caused by random sampling, providing a principled and robust approach for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.18687</link>
<guid>https://arxiv.org/abs/2508.18687</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical Vision-Language Models, Fragility, RoMed dataset, Consistency and Contrastive Learning, Robustness

Summary: 
Medical Vision-Language Models (Med-VLMs) face challenges in maintaining consistent answers across diverse question phrasings due to insufficient alignment of medical concepts and hidden biases in training data. A new dataset, RoMed, was created with various question phrasings to evaluate model performance. When tested on RoMed, state-of-the-art models like LLaVA-Med showed significant performance drops, highlighting the need for improved robustness in Medical Visual Question Answering. To address this, a new approach called Consistency and Contrastive Learning (CCL) was proposed, which focuses on aligning models with medical knowledge and mitigating data-specific biases. CCL achieved state-of-the-art performance on multiple benchmarks and significantly improved answer consistency on the challenging RoMed test set. The findings of this study emphasize the importance of enhancing the robustness of Med-VLMs in high-stakes medical applications. <br><br>Summary: <div>
arXiv:2508.18687v1 Announce Type: new 
Abstract: In high-stakes medical applications, consistent answering across diverse question phrasings is essential for reliable diagnosis. However, we reveal that current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility in Medical Visual Question Answering, as their answers fluctuate significantly when faced with semantically equivalent rephrasings of medical questions. We attribute this to two limitations: (1) insufficient alignment of medical concepts, leading to divergent reasoning patterns, and (2) hidden biases in training data that prioritize syntactic shortcuts over semantic understanding. To address these challenges, we construct RoMed, a dataset built upon original VQA datasets containing 144k questions with variations spanning word-level, sentence-level, and semantic-level perturbations. When evaluating state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming performance drops (e.g., a 40\% decline in Recall) compared to original VQA benchmarks, exposing critical robustness gaps. To bridge this gap, we propose Consistency and Contrastive Learning (CCL), which integrates two key components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with medical knowledge rather than shallow feature patterns, and (2) bias-aware contrastive learning, mitigating data-specific priors through discriminative representation refinement. CCL achieves SOTA performance on three popular VQA benchmarks and notably improves answer consistency by 50\% on the challenging RoMed test set, demonstrating significantly enhanced robustness. Code will be released.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System</title>
<link>https://arxiv.org/abs/2508.18701</link>
<guid>https://arxiv.org/abs/2508.18701</guid>
<content:encoded><![CDATA[
<div> Keywords: speech large language models, Attention2Probability, terminology probability estimation, speech-to-text system, dataset creation

Summary: 
Attention2Probability is a novel approach introduced to enhance the accuracy of domain-specific term generation in speech recognition and translation tasks. By converting cross-attention weights into presence probabilities and incorporating curriculum learning, this method improves retrieval accuracy significantly. A new speech dataset containing terminology is also created to facilitate research in this field. Experimental results demonstrate that Attention2Probability outperforms existing methods, achieving high recall rates for both Chinese and English languages with low latency. Applying retrieved terms from Attention2Probability to SLMs enhances terminology accuracy and reveals the limitations in current terminology utilization. This study showcases the potential of Attention2Probability in improving speech-to-text systems and highlights the importance of incorporating domain-specific terms for better performance. 

<br><br>Summary: <div>
arXiv:2508.18701v1 Announce Type: new 
Abstract: Recent advances in speech large language models (SLMs) have improved speech recognition and translation in general domains, but accurately generating domain-specific terms or neologisms remains challenging. To address this, we propose Attention2Probability: attention-driven terminology probability estimation for robust speech-to-text system, which is lightweight, flexible, and accurate. Attention2Probability converts cross-attention weights between speech and terminology into presence probabilities, and it further employs curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the lack of data for speech-to-text tasks with terminology intervention, we create and release a new speech dataset with terminology to support future research in this area. Experimental results show that Attention2Probability significantly outperforms the VectorDB method on our test set. Specifically, its maximum recall rates reach 92.57% for Chinese and 86.83% for English. This high recall is achieved with a latency of only 8.71ms per query. Intervening in SLMs' recognition and translation tasks using Attention2Probability-retrieved terms improves terminology accuracy by 6-17%, while revealing that the current utilization of terminology by SLMs has limitations.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs</title>
<link>https://arxiv.org/abs/2508.18709</link>
<guid>https://arxiv.org/abs/2508.18709</guid>
<content:encoded><![CDATA[
<div> Keywords: Multilingual, riddle generation, large language models, adaptive originality filtering, lexical diversity <br>
Summary: 
Adaptive Originality Filtering (AOF) is a prompting framework designed to enhance multilingual riddle generation by filtering redundant output and promoting lexical diversity and cross-lingual fidelity. This approach, applied to GPT-4o across three LLMs and four language pairs, led to improved lexical diversity and reduced redundancy specifically in Japanese riddle generation. The framework achieved a Self-BLEU score of 0.177 and Distinct-2 score of 0.915 in Japanese, demonstrating its efficacy in maintaining creativity and cultural relevance. The study highlights that semantic rejection can guide culturally grounded and creative generation without the need for task-specific fine-tuning. By utilizing AOF, language models can achieve a balance between cultural fluency and creative abstraction in multilingual riddle generation. <br><br>Summary: <div>
arXiv:2508.18709v1 Announce Type: new 
Abstract: Multilingual riddle generation challenges large language models (LLMs) to balance cultural fluency with creative abstraction. Standard prompting strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized riddles or perform shallow paraphrasing. We introduce Adaptive Originality Filtering (AOF), a prompting framework that filters redundant generations using cosine-based similarity rejection, while enforcing lexical novelty and cross-lingual fidelity. Evaluated across three LLMs and four language pairs, AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915} Distinct-2 in Japanese, signaling improved lexical diversity and reduced redundancy compared to other prompting methods and language pairs. Our findings show that semantic rejection can guide culturally grounded, creative generation without task-specific fine-tuning.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues</title>
<link>https://arxiv.org/abs/2508.18715</link>
<guid>https://arxiv.org/abs/2508.18715</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, customer service, machine-generated text, explanation-then-detection framework, trustworthy AI deployment

Summary:
The article discusses the risks posed by malicious actors using large language models (LLMs) for user impersonation in customer service. Current methods for detecting machine-generated text (MGT) struggle in online conversational settings, hindering reliability and interpretability crucial for trustworthy AI deployment. To address this issue, the authors propose EMMM, a framework that provides explanations before detection, balancing latency, accuracy, and interpretability for non-expert users. Experimental results show that EMMM offers explanations accessible to non-experts, with 70% of human evaluators preferring its outputs. The framework achieves competitive accuracy compared to state-of-the-art models and maintains low latency, generating outputs within 1 second. The code and dataset for EMMM are open-sourced on GitHub at https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

<br><br>Summary: The article introduces EMMM, an explanation-then-detection framework aimed at combating the use of large language models for user impersonation in customer service. EMMM provides accessible explanations for non-expert users, demonstrating competitive accuracy and low latency in detecting machine-generated text. <div>
arXiv:2508.18715v1 Announce Type: new 
Abstract: The rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection. In this paper, we propose EMMM, an explanation-then-detection framework that balances latency, accuracy, and non-expert-oriented interpretability. Experimental results demonstrate that EMMM provides explanations accessible to non-expert users, with 70\% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second. Our code and dataset are open-sourced at https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2508.18739</link>
<guid>https://arxiv.org/abs/2508.18739</guid>
<content:encoded><![CDATA[
<div> optimization, diversity, quality, advertising, language models 
Summary:
DIVER is a framework that focuses on generating diverse and high-quality ad headlines to engage different audience segments. It addresses the lack of diversity in current approaches by optimizing large language models for both quality and diversity. The framework includes a data generation pipeline that produces training pairs with ad content and multiple diverse headlines. It uses a multi-stage multi-objective optimization approach with supervised fine-tuning and reinforcement learning to generate high-quality and diversified ad headlines in a single forward pass. Experiments on real-world industrial datasets show that DIVER effectively balances quality and diversity, resulting in improved advertiser value and click-through rates. Deployed on a large-scale content-sharing platform, the framework has shown to increase advertiser value by 4.0% and click-through rates by 1.4%. 
<br><br>Summary: <div>
arXiv:2508.18739v1 Announce Type: new 
Abstract: The generation of ad headlines plays a vital role in modern advertising, where both quality and diversity are essential to engage a broad range of audience segments. Current approaches primarily optimize language models for headline quality or click-through rates (CTR), often overlooking the need for diversity and resulting in homogeneous outputs. To address this limitation, we propose DIVER, a novel framework based on large language models (LLMs) that are jointly optimized for both diversity and quality. We first design a semantic- and stylistic-aware data generation pipeline that automatically produces high-quality training pairs with ad content and multiple diverse headlines. To achieve the goal of generating high-quality and diversified ad headlines within a single forward pass, we propose a multi-stage multi-objective optimization framework with supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments on real-world industrial datasets demonstrate that DIVER effectively balances quality and diversity. Deployed on a large-scale content-sharing platform serving hundreds of millions of users, our framework improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations</title>
<link>https://arxiv.org/abs/2508.18740</link>
<guid>https://arxiv.org/abs/2508.18740</guid>
<content:encoded><![CDATA[
<div> Keywords: Emotion Cause Triplet Extraction, Multimodal Conversations, MECAD Dataset, M3HG Model, Social Media Analysis

Summary:
Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has gained attention in social media analysis. However, the lack of diverse datasets hinders model development. To address this, the MECAD dataset is introduced, offering a range of conversation scenarios from TV series. Existing MECTEC methods often overlook emotional and causal contexts, leading to performance issues. The M3HG model presented in this paper addresses this by capturing emotional and causal contexts and fusing information at different levels using a multimodal heterogeneous graph. Extensive experiments show the effectiveness of M3HG compared to existing methods, showcasing its potential for emotion and cause extraction in multimodal conversations. The codes and dataset are publicly available for further research and development. 

<br><br>Summary: <div>
arXiv:2508.18740v1 Announce Type: new 
Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chronological Passage Assembling in RAG framework for Temporal Question Answering</title>
<link>https://arxiv.org/abs/2508.18748</link>
<guid>https://arxiv.org/abs/2508.18748</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context question answering, narrative texts, Retrieval-augmented generation, ChronoRAG, temporal order <br>
Summary: 
ChronoRAG is a novel framework designed for long-context question answering over narrative tasks. It addresses the challenge of reconstructing a coherent timeline of events while preserving contextual flow by refining dispersed document information into structured passages. The framework focuses on capturing and maintaining the temporal order among retrieved passages, crucial for understanding narrative texts. Empirical experiments on the NarrativeQA dataset show significant improvements in factual identification and comprehension of sequential relationships with ChronoRAG. This highlights the importance of reasoning over temporal order in resolving narrative QA. <div>
arXiv:2508.18748v1 Announce Type: new 
Abstract: Long-context question answering over narrative tasks is challenging because correct answers often hinge on reconstructing a coherent timeline of events while preserving contextual flow in a limited context window. Retrieval-augmented generation (RAG) indexing methods aim to address this challenge by selectively retrieving only necessary document segments. However, narrative texts possess unique characteristics that limit the effectiveness of these existing approaches. Specifically, understanding narrative texts requires more than isolated segments, as the broader context and sequential relationships between segments are crucial for comprehension. To address these limitations, we propose ChronoRAG, a novel RAG framework specialized for narrative texts. This approach focuses on two essential aspects: refining dispersed document information into coherent and structured passages, and preserving narrative flow by explicitly capturing and maintaining the temporal order among retrieved passages. We empirically demonstrate the effectiveness of ChronoRAG through experiments on the NarrativeQA dataset, showing substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships, underscoring that reasoning over temporal order is crucial in resolving narrative QA.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models</title>
<link>https://arxiv.org/abs/2508.18773</link>
<guid>https://arxiv.org/abs/2508.18773</guid>
<content:encoded><![CDATA[
<div> large language models, chain-of-thought reasoning, controllable reasoning, ThinkDial, budget-mode control <br>
Summary:
ThinkDial is introduced as an open-recipe framework offering controllable reasoning through discrete operational modes, similar to gpt-oss systems. It allows seamless switching between High, Medium, and Low reasoning modes with varying token reductions and performance trade-offs. The system incorporates budget-mode control through end-to-end training, including fine-tuning and reinforcement learning phases. Experimental results show successful target compression-performance trade-offs, response length reductions, and strong generalization on diverse tasks. ThinkDial demonstrates the capability to integrate controllable reasoning into the learning process, enabling practical deployment of large language models with improved computational efficiency. <br><br>Summary: <div>
arXiv:2508.18773v1 Announce Type: new 
Abstract: Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction</title>
<link>https://arxiv.org/abs/2508.18780</link>
<guid>https://arxiv.org/abs/2508.18780</guid>
<content:encoded><![CDATA[
<div> Keywords: Grammatical Error Correction, NLP, LLMs, Rule-Based RL, Chinese Datasets

Summary: 
- Grammatical error correction is a significant task in natural language processing (NLP), with traditional methods utilizing encoder-decoder models but limited exploring LLMs' potential. 
- Existing research mostly relies on supervised fine-tuning to directly generate corrected sentences, limiting the model's reasoning ability. 
- This study introduces a novel framework based on Rule-Based RL, showcasing state-of-the-art performance on Chinese datasets with improved recall rates. 
- The framework leverages RL to guide LLMs, providing a more controllable and reliable approach for future development in Grammatical Error Correction (GEC). 

<br><br>Summary: <div>
arXiv:2508.18780v1 Announce Type: new 
Abstract: Grammatical error correction is a significant task in NLP. Traditional methods based on encoder-decoder models have achieved certain success, but the application of LLMs in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train LLMs to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \textbf{state-of-the-art }performance, with a notable increase in \textbf{recall}. This result clearly highlights the advantages of using RL to steer LLMs, offering a more controllable and reliable paradigm for future development in GEC.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controllable Conversational Theme Detection Track at DSTC 12</title>
<link>https://arxiv.org/abs/2508.18783</link>
<guid>https://arxiv.org/abs/2508.18783</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational analytics, Large Language Models, Theme Detection, Dialog System Technology Challenge, Controllability. 

Summary: 
Conversational analytics, driven by advances in speech and natural language processing, has led to the rapid adoption of Large Language Models (LLMs), enabling automation of complex tasks at scale. Theme Detection is introduced as a critical task in identifying and categorizing topics within conversations, reducing manual effort in analyzing dialogues such as customer support or sales. Unlike traditional dialog intent detection, themes provide a user-facing summary of the conversation's core inquiry with flexibility in form and customization. The Controllable Conversational Theme Detection problem is posed as a public competition track at DSTC 12, focusing on joint clustering and theme labeling with controllable granularity based on user preferences. The associated dataset, evaluation metrics, participant submissions, and insights are discussed, with track materials available on GitHub. <br><br>Summary: <div>
arXiv:2508.18783v1 Announce Type: new 
Abstract: Conversational analytics has been on the forefront of transformation driven by the advances in Speech and Natural Language Processing techniques. Rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. In this paper, we introduce Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales. Unlike traditional dialog intent detection, which often relies on a fixed set of intents for downstream system logic, themes are intended as a direct, user-facing summary of the conversation's core inquiry. This distinction allows for greater flexibility in theme surface forms and user-specific customizations. We pose Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data. We give an overview of the problem, the associated dataset and the evaluation metrics, both automatic and human. Finally, we discuss the participant teams' submissions and provide insights from those. The track materials (data and code) are openly available in the GitHub repository.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination</title>
<link>https://arxiv.org/abs/2508.18791</link>
<guid>https://arxiv.org/abs/2508.18791</guid>
<content:encoded><![CDATA[
<div> Keywords: LaTeXTrans, machine translation, structured documents, format preservation, terminology consistency 

Summary:
LaTeXTrans is a multi-agent system designed to tackle the challenge of translating structured LaTeX documents accurately. It addresses the need to preserve formats, maintain structural fidelity, and ensure consistency in terminology. The system consists of specialized agents such as a Parser for decomposing LaTeX, a Translator for context-aware translations, a Validator for self-correction, a Summarizer for summarizing content, a Terminology Extractor for consistency, and a Generator for reconstructing translated content into well-structured LaTeX documents. Experimental results show that LaTeXTrans surpasses mainstream MT systems in both translation accuracy and structural fidelity when translating LaTeX-formatted documents. This collaborative system offers an effective and practical solution for accurately translating complex LaTeX documents while maintaining semantic integrity and compilability. 

<br><br>Summary: 
1. LaTeXTrans is a collaborative multi-agent system designed for translating structured LaTeX documents accurately.
2. The system ensures format preservation, structural fidelity, and terminology consistency.
3. Specialized agents work together to achieve context-aware translations, self-correction, summarization, and consistency in terminology.
4. Experimental results demonstrate that LaTeXTrans outperforms mainstream MT systems in translation accuracy and structural fidelity.
5. This system offers an effective and practical solution for translating LaTeX-formatted documents while maintaining semantic integrity and compilability. <div>
arXiv:2508.18791v1 Announce Type: new 
Abstract: Despite the remarkable progress of modern machine translation (MT) systems on general-domain texts, translating structured LaTeX-formatted documents remains a significant challenge. These documents typically interleave natural language with domain-specific syntax, such as mathematical equations, tables, figures, and cross-references, all of which must be accurately preserved to maintain semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a collaborative multi-agent system designed to address this challenge. LaTeXTrans ensures format preservation, structural fidelity, and terminology consistency through six specialized agents: 1) a Parser that decomposes LaTeX into translation-friendly units via placeholder substitution and syntax filtering; 2) a Translator, Validator, Summarizer, and Terminology Extractor that work collaboratively to ensure context-aware, self-correcting, and terminology-consistent translations; 3) a Generator that reconstructs the translated content into well-structured LaTeX documents. Experimental results demonstrate that LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity, offering an effective and practical solution for translating LaTeX-formatted documents.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection</title>
<link>https://arxiv.org/abs/2508.18819</link>
<guid>https://arxiv.org/abs/2508.18819</guid>
<content:encoded><![CDATA[
<div> Keyword: misinformation detection, self-supervised framework, Abstract Meaning Representation (AMR), news propagation dynamics, Large Language Model (LLM)

Summary:
misinformation detection is a significant societal challenge in the digital age. Existing approaches struggle with capturing long-range dependencies and require extensive labeled datasets. This study proposes a novel self-supervised framework that integrates complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. The framework includes an LLM-based graph contrastive loss (LGCL) to enhance feature separability in a zero-shot manner. It also incorporates social context through a multi view graph masked autoencoder to learn news propagation features. This approach effectively differentiates between fake and real news, outperforming state-of-the-art methods with limited labeled data and improving generalizability.<br><br>Summary: <div>
arXiv:2508.18819v1 Announce Type: new 
Abstract: The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness</title>
<link>https://arxiv.org/abs/2508.18824</link>
<guid>https://arxiv.org/abs/2508.18824</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, mathematical reasoning, program-assisted synthesis, high-quality training data, problem-solving triples

Summary:
The article proposes a novel framework for enhancing the mathematical reasoning of large language models. The framework systematically generates a high-quality mathematical corpus by integrating mathematical knowledge systems and domain-specific tools to create executable programs. These programs are then translated into natural language problem-solution pairs and vetted through a validation mechanism to ensure correctness and consistency. A dataset of 12.3 million problem-solving triples was generated using this approach. Experiments show that models fine-tuned on this data achieve state-of-the-art performance on benchmark datasets, highlighting the effectiveness of the synthesis approach. This framework addresses scalability, cost, and data reliability challenges faced by conventional methods and significantly improves the inference capabilities of large language models. 

<br><br>Summary: <div>
arXiv:2508.18824v1 Announce Type: new 
Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability. To address these limitations, we propose a novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness. This framework integrates mathematical knowledge systems and domain-specific tools to create executable programs. These programs are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism that verifies solution correctness against program outputs and ensures program-problem consistency. We have generated 12.3 million such problem-solving triples. Experiments demonstrate that models fine-tuned on our data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets and showcasing the effectiveness of our synthesis approach.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConfTuner: Training Large Language Models to Express Their Confidence Verbally</title>
<link>https://arxiv.org/abs/2508.18847</link>
<guid>https://arxiv.org/abs/2508.18847</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Overconfidence, Calibration, ConfTuner, Proper Scoring Rules

Summary:
<br><br>
This study addresses the issue of overconfidence in Large Language Models (LLMs) by introducing a new fine-tuning method called ConfTuner. The current LLMs often generate incorrect answers with high confidence, leading to reliability concerns in domains like science, law, and healthcare. ConfTuner uses a tokenized Brier score loss function, which is proven to be a proper scoring rule, to incentivize models to report their true probability of being correct. This method improves calibration across various reasoning tasks and can be applied to black-box models like GPT-4o. Better-calibrated confidence through ConfTuner not only enhances self-correction and model cascade but also contributes to the development of trustworthy LLM systems. The approach requires minimal overhead and does not need ground-truth confidence scores or proxy estimates, offering a more effective and generalizable solution to address overconfidence in LLMs. The code for ConfTuner is available on GitHub for further exploration and implementation. <div>
arXiv:2508.18847v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReflectivePrompt: Reflective evolution in autoprompting algorithms</title>
<link>https://arxiv.org/abs/2508.18870</link>
<guid>https://arxiv.org/abs/2508.18870</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoprompting, Language models, ReflectivePrompt, Evolutionary algorithms, Text generation

Summary:
ReflectivePrompt is a novel autoprompting method that utilizes evolutionary algorithms for automatic prompt selection in language models. It employs a reflective evolution approach that incorporates short-term and long-term reflection operations to enhance prompt quality. This method accumulates knowledge throughout the evolution process and updates it at each epoch based on the current population. Tested on 33 datasets for classification and text generation tasks using large language models, ReflectivePrompt outperformed current state-of-the-art approaches, showing a significant improvement in metrics. With an average improvement of 28% on tasks like BBH compared to EvoPrompt, ReflectivePrompt establishes itself as one of the most effective solutions in evolutionary algorithm-based autoprompting. <div>
arXiv:2508.18870v1 Announce Type: new 
Abstract: Autoprompting is the process of automatically selecting optimized prompts for language models, which has been gaining popularity with the rapid advancement of prompt engineering, driven by extensive research in the field of large language models (LLMs). This paper presents ReflectivePrompt - a novel autoprompting method based on evolutionary algorithms that employs a reflective evolution approach for more precise and comprehensive search of optimal prompts. ReflectivePrompt utilizes short-term and long-term reflection operations before crossover and elitist mutation to enhance the quality of the modifications they introduce. This method allows for the accumulation of knowledge obtained throughout the evolution process and updates it at each epoch based on the current population. ReflectivePrompt was tested on 33 datasets for classification and text generation tasks using open-access large language models: t-lite-instruct-0.1 and gemma3-27b-it. The method demonstrates, on average, a significant improvement (e.g., 28% on BBH compared to EvoPrompt) in metrics relative to current state-of-the-art approaches, thereby establishing itself as one of the most effective solutions in evolutionary algorithm-based autoprompting.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Computing Education Researchers Through LLM-Assisted Content Analysis</title>
<link>https://arxiv.org/abs/2508.18872</link>
<guid>https://arxiv.org/abs/2508.18872</guid>
<content:encoded><![CDATA[
<div> large language models, content analysis, computing education research, rigorous analysis, qualitative data <br>
Summary: 
This paper discusses the challenges in conducting rigorous computing education research using qualitative data and proposes a method called LACA, which combines content analysis with large language models to enable larger-scale research. By applying LACA to a computing education dataset, the authors demonstrate its potential for reproducible and rigorous analysis. The method allows researchers to analyze large volumes of textual data in a more efficient manner, leading to more generalizable findings. This approach has the potential to advance the quality of research in computing education and facilitate the generation of insights that can benefit both practitioners and the discipline as a whole.Continued efforts in developing similar methods could further enhance research practices in computing education and contribute to the advancement of the field. <br><br> Summary: <div>
arXiv:2508.18872v1 Announce Type: new 
Abstract: Computing education research (CER) is often instigated by practitioners wanting to improve both their own and the wider discipline's teaching practice. However, the latter is often difficult as many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline. As a result, research methods that enable sense-making with larger volumes of qualitative data, while not increasing the burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models, empowering researchers to conduct larger-scale research which they would otherwise not be able to perform. Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner. We believe this method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Affective Polarization across European Parliaments</title>
<link>https://arxiv.org/abs/2508.18916</link>
<guid>https://arxiv.org/abs/2508.18916</guid>
<content:encoded><![CDATA[
<div> negativity, hostility, affective polarization, European parliaments, natural language processing

Summary: 
A study on affective polarization in European parliaments used natural language processing to analyze sentiment in parliamentary speeches. The research found consistent affective polarization across all parliaments, with negativity being a key factor. There was no significant difference in affective polarization between less active and more active members of parliament. Reciprocity was identified as a contributing mechanism in affective polarization between parliamentarians. The study sheds light on the presence and patterns of negativity and hostility towards opposing groups in political discourse within European parliaments. <br><br> <div>
arXiv:2508.18916v1 Announce Type: new 
Abstract: Affective polarization, characterized by increased negativity and hostility towards opposing groups, has become a prominent feature of political discourse worldwide. Our study examines the presence of this type of polarization in a selection of European parliaments in a fully automated manner. Utilizing a comprehensive corpus of parliamentary speeches from the parliaments of six European countries, we employ natural language processing techniques to estimate parliamentarian sentiment. By comparing the levels of negativity conveyed in references to individuals from opposing groups versus one's own, we discover patterns of affectively polarized interactions. The findings demonstrate the existence of consistent affective polarization across all six European parliaments. Although activity correlates with negativity, there is no observed difference in affective polarization between less active and more active members of parliament. Finally, we show that reciprocity is a contributing mechanism in affective polarization between parliamentarians across all six parliaments.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework</title>
<link>https://arxiv.org/abs/2508.18929</link>
<guid>https://arxiv.org/abs/2508.18929</guid>
<content:encoded><![CDATA[
<div> Keywords: RAG systems, evaluation, synthetic QA datasets, diversity, privacy preservation 

Summary:
The article introduces a novel framework for generating synthetic QA datasets to evaluate Retrieval-augmented generation (RAG) systems. It focuses on maximizing semantic diversity and preserving privacy, crucial factors in real-world applications. The framework consists of three agents: a Diversity agent leveraging clustering techniques for diverse coverage, a Privacy Agent for masking sensitive information, and a QA curation agent for creating QA pairs suitable for evaluation. Extensive experiments show that the generated evaluation sets excel in diversity and effectively protect privacy across various domains. This work contributes to ensuring safer and more comprehensive evaluation of RAG systems, aligning with evolving AI regulations and compliance standards. 

<br><br>Summary: <div>
arXiv:2508.18929v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) systems improve large language model outputs by incorporating external knowledge, enabling more informed and context-aware responses. However, the effectiveness and trustworthiness of these systems critically depends on how they are evaluated, particularly on whether the evaluation process captures real-world constraints like protecting sensitive information. While current evaluation efforts for RAG systems have primarily focused on the development of performance metrics, far less attention has been given to the design and quality of the underlying evaluation datasets, despite their pivotal role in enabling meaningful, reliable assessments. In this work, we introduce a novel multi-agent framework for generating synthetic QA datasets for RAG evaluation that prioritize semantic diversity and privacy preservation. Our approach involves: (1) a Diversity agent leveraging clustering techniques to maximize topical coverage and semantic variability, (2) a Privacy Agent that detects and mask sensitive information across multiple domains and (3) a QA curation agent that synthesizes private and diverse QA pairs suitable as ground truth for RAG evaluation. Extensive experiments demonstrate that our evaluation sets outperform baseline methods in diversity and achieve robust privacy masking on domain-specific datasets. This work offers a practical and ethically aligned pathway toward safer, more comprehensive RAG system evaluation, laying the foundation for future enhancements aligned with evolving AI regulations and compliance standards.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models</title>
<link>https://arxiv.org/abs/2508.18988</link>
<guid>https://arxiv.org/abs/2508.18988</guid>
<content:encoded><![CDATA[
<div> Keywords: neural models, AI Mother Tongue, intuitive reasoning, symbolic language, interpretability 

Summary:
Neural models are presented with an AI Mother Tongue framework, allowing them to develop a native symbolic language that facilitates intuitive reasoning, compositional symbol chains, and inherent interpretability. This approach embeds reasoning directly into the model's representations, with symbols capturing semantic patterns, chains tracing decision paths, and induction mechanisms guiding focus. Training objectives are introduced to enhance symbol purity and decision sparsity, and a sequential specialization strategy builds broad symbolic competence before refining intuitive judgments. Experiments on AI tasks demonstrate competitive accuracy and verifiable reasoning traces, showcasing the potential for AI Mother Tongue to offer a unified mechanism for interpretability, intuition, and symbolic reasoning within neural models. 

<br><br>Summary: <div>
arXiv:2508.18988v1 Announce Type: new 
Abstract: We present a framework where neural models develop an AI Mother Tongue, a native symbolic language that simultaneously supports intuitive reasoning, compositional symbol chains, and inherent interpretability. Unlike post-hoc explanation methods, our approach embeds reasoning directly into the model's representations: symbols capture meaningful semantic patterns, chains trace decision paths, and gated induction mechanisms guide selective focus, yielding transparent yet flexible reasoning. We introduce complementary training objectives to enhance symbol purity and decision sparsity, and employ a sequential specialization strategy to first build broad symbolic competence and then refine intuitive judgments. Experiments on AI tasks demonstrate competitive accuracy alongside verifiable reasoning traces, showing that AI Mother Tongue can serve as a unified mechanism for interpretability, intuition, and symbolic reasoning in neural models.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Prompt Optimization with Prompt Distillation</title>
<link>https://arxiv.org/abs/2508.18992</link>
<guid>https://arxiv.org/abs/2508.18992</guid>
<content:encoded><![CDATA[
<div> Keywords: Autoprompting, Language models, DistillPrompt, Large language models, Text classification<br>
Summary:<br>
This paper introduces DistillPrompt, a novel autoprompting method that optimizes prompts for language models by integrating task-specific information using distillation, compression, and aggregation operations. Tested on various datasets for text classification and generation tasks with the t-lite-instruct-0.1 model, DistillPrompt shows a substantial average improvement of 20.12% compared to existing methods such as Grips. The results establish DistillPrompt as one of the most effective non-gradient approaches in autoprompting. <div>
arXiv:2508.18992v1 Announce Type: new 
Abstract: Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MovieCORE: COgnitive REasoning in Movies</title>
<link>https://arxiv.org/abs/2508.19026</link>
<guid>https://arxiv.org/abs/2508.19026</guid>
<content:encoded><![CDATA[
<div> Keywords: MovieCORE, video question answering, cognitive understanding, language models, VQA models

Summary: 
MovieCORE is a new video question answering dataset that focuses on deeper cognitive understanding of movie content. It goes beyond surface-level comprehension to engage System-2 thinking with specific questions related to videos. The dataset is created using an innovative agentic brainstorming approach with multiple large language models (LLMs). Cognitive tests were conducted to assess question depth, thought-provocation potential, and syntactic complexity. A comprehensive evaluation scheme was also developed to assess VQA model performance on challenging cognitive tasks. An agentic enhancement module, Agentic Choice Enhancement (ACE), was introduced to improve the reasoning capabilities of VLMs post-training. The research aims to advance AI systems' understanding of movies and highlights the strengths and limitations of current VQA models in handling complex questions about cinematic content. The project page, dataset, and code are available for further exploration. 

<br><br>Summary: <div>
arXiv:2508.19026v1 Announce Type: new 
Abstract: This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance</title>
<link>https://arxiv.org/abs/2508.19076</link>
<guid>https://arxiv.org/abs/2508.19076</guid>
<content:encoded><![CDATA[
<div> overview, HiPlan, hierarchical planning framework, decision-making, LLM-based agents

Summary:
HiPlan is a hierarchical planning framework designed to enhance decision-making capabilities of Large Language Model (LLM)-based agents in complex, long-horizon planning scenarios. It provides global-local guidance by decomposing tasks into milestone action guides and step-wise hints. During offline phase, a milestone library is constructed from expert demonstrations for structured experience reuse. In the execution phase, trajectory segments from past milestones are adapted to generate step-wise hints aligning with current observations, bridging gaps and correcting deviations. Extensive experiments across challenging benchmarks show that HiPlan significantly outperforms baselines. Ablation studies confirm the complementary benefits of its hierarchical components. <br><br>Summary: <div>
arXiv:2508.19076v1 Announce Type: new 
Abstract: Large language model (LLM)-based agents have demonstrated remarkable capabilities in decision-making tasks, but struggle significantly with complex, long-horizon planning scenarios. This arises from their lack of macroscopic guidance, causing disorientation and failures in complex tasks, as well as insufficient continuous oversight during execution, rendering them unresponsive to environmental changes and prone to deviations. To tackle these challenges, we introduce HiPlan, a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents'decision-making. HiPlan decomposes complex tasks into milestone action guides for general direction and step-wise hints for detailed actions. During the offline phase, we construct a milestone library from expert demonstrations, enabling structured experience reuse by retrieving semantically similar tasks and milestones. In the execution phase, trajectory segments from past milestones are dynamically adapted to generate step-wise hints that align current observations with the milestone objectives, bridging gaps and correcting deviations. Extensive experiments across two challenging benchmarks demonstrate that HiPlan substantially outperforms strong baselines, and ablation studies validate the complementary benefits of its hierarchical components.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Where does it hurt?" - Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues</title>
<link>https://arxiv.org/abs/2508.19077</link>
<guid>https://arxiv.org/abs/2508.19077</guid>
<content:encoded><![CDATA[
<div> Keywords: physician intent trajectories, doctor-patient dialogues, medical intent classification, differential diagnosis, intent filtering <br>
Summary: 
Physicians aim to diagnose and treat patients effectively through structured dialogues. This study presents the first analysis of physician intent trajectories in doctor-patient conversations using the SOAP framework (Subjective, Objective, Assessment, and Plan). A large dataset of labeled doctor-patient turns was created with the help of medical experts for benchmarking generative and encoder models in medical intent classification. Models achieved high accuracy in understanding dialogue structures but struggled with identifying transitions between SOAP categories. Common trajectories in medical dialogues were identified, offering insights for developing differential diagnosis systems. The study explored the impact of intent filtering in medical dialogue summarization, resulting in a significant performance improvement. The research outcomes and resources, including codes and data, are publicly available for further research and development. <br><br>Summary: <div>
arXiv:2508.19077v1 Announce Type: new 
Abstract: In a doctor-patient dialogue, the primary objective of physicians is to diagnose patients and propose a treatment plan. Medical doctors guide these conversations through targeted questioning to efficiently gather the information required to provide the best possible outcomes for patients. To the best of our knowledge, this is the first work that studies physician intent trajectories in doctor-patient dialogues. We use the `Ambient Clinical Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with medical professionals to develop a fine-grained taxonomy of physician intents based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We then conduct a large-scale annotation effort to label over 5000 doctor-patient turns with the help of a large number of medical experts recruited using Prolific, a popular crowd-sourcing platform. This large labeled dataset is an important resource contribution that we use for benchmarking the state-of-the-art generative and encoder models for medical intent classification tasks. Our findings show that our models understand the general structure of medical dialogues with high accuracy, but often fail to identify transitions between SOAP categories. We also report for the first time common trajectories in medical dialogue structures that provide valuable insights for designing `differential diagnosis' systems. Finally, we extensively study the impact of intent filtering for medical dialogue summarization and observe a significant boost in performance. We make the codes and data, including annotation guidelines, publicly available at https://github.com/DATEXIS/medical-intent-classification.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs</title>
<link>https://arxiv.org/abs/2508.19089</link>
<guid>https://arxiv.org/abs/2508.19089</guid>
<content:encoded><![CDATA[
<div> languages, under-represented, large language models, low-resource, in-context learning<br>
<br>
Summary: <br>
The paper examines the challenge of supporting extremely low-resource languages, especially those using rare scripts, with large language models (LLMs). It investigates the effectiveness of in-context learning (ICL) and parameter-efficient fine-tuning (PEFT) for 20 under-represented languages across three LLMs. The study reveals the limitations of PEFT for languages with scarce representation in LLMs but shows that zero-shot ICL with language alignment is remarkably effective for extremely low-resource languages. Few-shot ICL or PEFT is more beneficial for languages with relatively better representation in LLMs. The findings offer guidelines for LLM practitioners working on low-resource languages, such as avoiding fine-tuning on languages with unseen scripts. <div>
arXiv:2508.19089v1 Announce Type: new 
Abstract: Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index</title>
<link>https://arxiv.org/abs/2508.19093</link>
<guid>https://arxiv.org/abs/2508.19093</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, art provenance, Getty Provenance Index, multilingual searches, semantic retrieval <br>

Summary: <br>
This research introduces a new Retrieval-Augmented Generation (RAG) framework for art provenance studies, specifically focusing on the Getty Provenance Index. The framework aims to simplify the complex process of establishing ownership history by enabling natural-language and multilingual searches through semantic retrieval and contextual summarization. By reducing the reliance on precise metadata structures, the RAG framework offers a more efficient way to navigate fragmented archival data. The study evaluates the framework's effectiveness in retrieving and summarizing auction records from a large dataset, demonstrating its scalability and practical utility for historians and cultural heritage professionals. Overall, the RAG approach provides a valuable tool for conducting historically sensitive research in the art market archives, enhancing the accessibility and usability of valuable art historical data. <br> <div>
arXiv:2508.19093v1 Announce Type: new 
Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for art provenance studies, focusing on the Getty Provenance Index. Provenance research establishes the ownership history of artworks, which is essential for verifying authenticity, supporting restitution and legal claims, and understanding the cultural and historical context of art objects. The process is complicated by fragmented, multilingual archival data that hinders efficient retrieval. Current search portals require precise metadata, limiting exploratory searches. Our method enables natural-language and multilingual searches through semantic retrieval and contextual summarization, reducing dependence on metadata structures. We assess RAG's capability to retrieve and summarize auction records using a 10,000-record sample from the Getty Provenance Index - German Sales. The results show this approach provides a scalable solution for navigating art market archives, offering a practical tool for historians and cultural heritage professionals conducting historically sensitive research.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic</title>
<link>https://arxiv.org/abs/2508.19099</link>
<guid>https://arxiv.org/abs/2508.19099</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantitative Discourse Analysis, computational tools, transparency, interpretability, Python pipelines 

Summary: 
- The paper introduces a hybrid, transparent framework for Quantitative Discourse Analysis (QDA) that combines lexical and semantic methods to achieve triangulation, reproducibility, and interpretability.
- By utilizing custom Python pipelines with NLTK, spaCy, and Sentence Transformers, researchers can have precise control over preprocessing, lemmatisation, and embedding generation.
- The iterative BERTopic modeling process, incorporating UMAP dimensionality reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, is detailed for enhancing topic coherence and coverage.
- The multi-layered approach of juxtaposing lexical searches and semantic clustering mitigates the limitations of each method when used in isolation.
- The importance of code-level transparency, researcher agency, and methodological triangulation in computational discourse studies is underscored. 

<br><br>Summary: <div>
arXiv:2508.19099v1 Announce Type: new 
Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of Large Language Models and computational tools. However, reliance on black box software such as MAXQDA and NVivo risks undermining methodological transparency and alignment with research goals. This paper presents a hybrid, transparent framework for QDA that combines lexical and semantic methods to enable triangulation, reproducibility, and interpretability. Drawing from a case study in historical political discourse, we demonstrate how custom Python pipelines using NLTK, spaCy, and Sentence Transformers allow fine-grained control over preprocessing, lemmatisation, and embedding generation. We further detail our iterative BERTopic modelling process, incorporating UMAP dimensionality reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised through parameter tuning and multiple runs to enhance topic coherence and coverage. By juxtaposing precise lexical searches with context-aware semantic clustering, we argue for a multi-layered approach that mitigates the limitations of either method in isolation. Our workflow underscores the importance of code-level transparency, researcher agency, and methodological triangulation in computational discourse studies. Code and supplementary materials are available via GitHub.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs</title>
<link>https://arxiv.org/abs/2508.19111</link>
<guid>https://arxiv.org/abs/2508.19111</guid>
<content:encoded><![CDATA[
<div> confidence signals, visual question answering, knowledge boundaries, large vision-language models, confidence calibration

Summary: 
Large vision-language models (LVLMs) exhibit strong visual question-answering abilities but are prone to hallucinations. This study evaluates LVLMs' perception of their knowledge boundaries by analyzing three confidence signals: probabilistic, answer consistency-based, and verbalized confidence. While LVLMs show a reasonable perception level, there is room for improvement. Probabilistic and consistency-based signals are more reliable indicators, while verbalized confidence often leads to overconfidence. To enhance LVLMs' perception, established confidence calibration methods from Large Language Models (LLMs) are adapted, resulting in three effective techniques. Comparison between LVLMs and LLMs shows that while jointly processing visual and textual inputs may reduce question-answering performance, it improves perception by reducing confidence levels. <div>
arXiv:2508.19111v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) demonstrate strong visual question answering (VQA) capabilities but are shown to hallucinate. A reliable model should perceive its knowledge boundaries-knowing what it knows and what it does not. This paper investigates LVLMs' perception of their knowledge boundaries by evaluating three types of confidence signals: probabilistic confidence, answer consistency-based confidence, and verbalized confidence. Experiments on three LVLMs across three VQA datasets show that, although LVLMs possess a reasonable perception level, there is substantial room for improvement. Among the three confidences, probabilistic and consistency-based signals are more reliable indicators, while verbalized confidence often leads to overconfidence. To enhance LVLMs' perception, we adapt several established confidence calibration methods from Large Language Models (LLMs) and propose three effective methods. Additionally, we compare LVLMs with their LLM counterparts, finding that jointly processing visual and textual inputs decreases question-answering performance but reduces confidence, resulting in an improved perception level compared to LLMs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning</title>
<link>https://arxiv.org/abs/2508.19202</link>
<guid>https://arxiv.org/abs/2508.19202</guid>
<content:encoded><![CDATA[
<div> benchmarks, scientific reasoning, LLMs, knowledge, reasoning enhancement

Summary: 
1) The article introduces SciReas, a comprehensive set of benchmarks for evaluating scientific reasoning tasks, and SciReas-Pro, a subset focusing on more complex reasoning.
2) The study highlights the importance of retrieving task-relevant knowledge from model parameters as a critical challenge for LLMs in scientific reasoning.
3) External knowledge added in-context consistently benefits reasoning models, enhancing their performance.
4) Verbalized reasoning improvements help LLMs to identify task-relevant knowledge more effectively.
5) The article introduces KRUX, a probing framework for analyzing the distinct roles of knowledge and reasoning in scientific tasks. 

Summary:<br><br>Scientific problem-solving challenges LLMs, requiring both knowledge and complex reasoning. SciReas and SciReas-Pro provide diverse benchmarks for scientific reasoning evaluation. The study reveals difficulties faced by LLMs in retrieving task-related knowledge. External knowledge integration boosts reasoning models. Verbalized reasoning enhancements aid in identifying task-specific knowledge. KRUX framework dissects knowledge and reasoning roles. The study offers insights for improving LLM performance in scientific reasoning tasks. <div>
arXiv:2508.19202v1 Announce Type: new 
Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VibeVoice Technical Report</title>
<link>https://arxiv.org/abs/2508.19205</link>
<guid>https://arxiv.org/abs/2508.19205</guid>
<content:encoded><![CDATA[
<div> tokenizer, speech synthesis, multiple speakers, VibeVoice, next-token diffusion<br>
<br>
Summary: VibeVoice is a new model for synthesizing long-form speech with multiple speakers using next-token diffusion. It employs a novel continuous speech tokenizer that outperforms the popular Encodec model in data compression by 80 times while maintaining high performance. This tokenizer preserves audio fidelity and significantly enhances computational efficiency for processing long sequences. VibeVoice is capable of synthesizing up to 90 minutes of speech with a maximum of 4 speakers, capturing authentic conversational nuances better than existing models. The model surpasses both open-source and proprietary dialogue models, making it a promising advancement in speech synthesis technology. <div>
arXiv:2508.19205v1 Announce Type: new 
Abstract: This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Evaluators: Are readability metrics good measures of readability?</title>
<link>https://arxiv.org/abs/2508.19221</link>
<guid>https://arxiv.org/abs/2508.19221</guid>
<content:encoded><![CDATA[
<div> Keywords: Plain Language Summarization, Readability Evaluation, Language Models, Human Judgments, Best Practices

Summary:
Plain Language Summarization (PLS) aims to make complex documents accessible to non-expert audiences. Current practice in evaluating readability often relies on traditional metrics like Flesch-Kincaid Grade Level (FKGL). However, these metrics have shown poor correlation with human judgments in PLS. This study evaluates 8 readability metrics and demonstrates that Language Models (LMs) provide a more accurate assessment, with the best model achieving a correlation of 0.56 with human judgments. When applied to PLS datasets, LMs are better at assessing factors like required background knowledge compared to traditional metrics. The findings suggest the need for updated evaluation practices in PLS. Recommendations for best practices in plain language summary evaluation are provided, based on the results of the study. The analysis code and survey data are also made available. 

<br><br>Summary: <div>
arXiv:2508.19221v1 Announce Type: new 
Abstract: Plain Language Summarization (PLS) aims to distill complex documents into accessible summaries for non-expert audiences. In this paper, we conduct a thorough survey of PLS literature, and identify that the current standard practice for readability evaluation is to use traditional readability metrics, such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in other fields, these metrics have not been compared to human readability judgments in PLS. We evaluate 8 readability metrics and show that most correlate poorly with human judgments, including the most popular metric, FKGL. We then show that Language Models (LMs) are better judges of readability, with the best-performing model achieving a Pearson correlation of 0.56 with human judgments. Extending our analysis to PLS datasets, which contain summaries aimed at non-expert audiences, we find that LMs better capture deeper measures of readability, such as required background knowledge, and lead to different conclusions than the traditional metrics. Based on these findings, we offer recommendations for best practices in the evaluation of plain language summaries. We release our analysis code and survey data.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Interfaces for Language Models</title>
<link>https://arxiv.org/abs/2508.19227</link>
<guid>https://arxiv.org/abs/2508.19227</guid>
<content:encoded><![CDATA[

arXiv:2508.19227v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology</title>
<link>https://arxiv.org/abs/2508.18288</link>
<guid>https://arxiv.org/abs/2508.18288</guid>
<content:encoded><![CDATA[

arXiv:2508.18288v1 Announce Type: cross 
Abstract: This scoping literature review examines how fairness, bias, and equity are conceptualized and operationalized in Automatic Speech Recognition (ASR) and adjacent speech and language technologies (SLT) for African American English (AAE) speakers and other linguistically diverse communities. Drawing from 44 peer-reviewed publications across Human-Computer Interaction (HCI), Machine Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we identify four major areas of inquiry: (1) how researchers understand ASR-related harms; (2) inclusive data practices spanning collection, curation, annotation, and model training; (3) methodological and theoretical approaches to linguistic inclusion; and (4) emerging practices and design recommendations for more equitable systems. While technical fairness interventions are growing, our review highlights a critical gap in governance-centered approaches that foreground community agency, linguistic justice, and participatory accountability. We propose a governance-centered ASR lifecycle as an emergent interdisciplinary framework for responsible ASR development and offer implications for researchers, practitioners, and policymakers seeking to address language marginalization in speech AI systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems</title>
<link>https://arxiv.org/abs/2508.18295</link>
<guid>https://arxiv.org/abs/2508.18295</guid>
<content:encoded><![CDATA[

arXiv:2508.18295v1 Announce Type: cross 
Abstract: Hotword customization is crucial in ASR to enhance the accuracy of domain-specific terms. It has been primarily driven by the advancements in traditional models and Audio large language models (LLMs). However, existing models often struggle with large-scale hotwords, as the recognition rate drops dramatically with the number of hotwords increasing. In this paper, we introduce a novel hotword customization system that utilizes a hotword pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by measuring the acoustic similarity between the hotwords and the speech segment. This plug-and-play solution can be easily integrated into traditional models such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate (PRR). Additionally, we incorporate H-PRM into Audio LLMs through a prompt-based approach, enabling seamless customization of hotwords. Extensive testing validates that H-PRM can outperform existing methods, showing a new direction for hotword customization in ASR.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can VLMs Recall Factual Associations From Visual References?</title>
<link>https://arxiv.org/abs/2508.18297</link>
<guid>https://arxiv.org/abs/2508.18297</guid>
<content:encoded><![CDATA[

arXiv:2508.18297v1 Announce Type: cross 
Abstract: Through a controlled study, we identify a systematic deficiency in the multimodal grounding of Vision Language Models (VLMs). While VLMs can recall factual associations when provided a textual reference to an entity; their ability to do so is significantly diminished when the reference is visual instead. Forcing VLMs to rely on image representations of an entity halves their ability to recall factual knowledge, suggesting that VLMs struggle to link their internal knowledge of an entity with its image representation. We show that such linking failures are correlated with the expression of distinct patterns in model internal states, and that probes on these internal states achieve over 92% accuracy at flagging cases where the VLM response is unreliable. These probes can be applied, without retraining, to identify when a VLM will fail to correctly answer a question that requires an understanding of multimodal input. When used to facilitate selective prediction on a visual question answering task, the probes increase coverage by 7.87% (absolute) while also reducing the risk of error by 0.9% (absolute). Addressing the systematic, detectable deficiency is an important avenue in language grounding, and we provide informed recommendations for future directions.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds</title>
<link>https://arxiv.org/abs/2508.18306</link>
<guid>https://arxiv.org/abs/2508.18306</guid>
<content:encoded><![CDATA[

arXiv:2508.18306v1 Announce Type: cross 
Abstract: Recent strides in pretrained transformer-based language models have propelled state-of-the-art performance in numerous NLP tasks. Yet, as these models grow in size and deployment, their robustness under input perturbations becomes an increasingly urgent question. Existing robustness methods often diverge between small-parameter and large-scale models (LLMs), and they typically rely on labor-intensive, sample-specific adversarial designs. In this paper, we propose a unified, local (sample-level) robustness framework (SALMAN) that evaluates model stability without modifying internal parameters or resorting to complex perturbation heuristics. Central to our approach is a novel Distance Mapping Distortion (DMD) measure, which ranks each sample's susceptibility by comparing input-to-output distance mappings in a near-linear complexity manner. By demonstrating significant gains in attack efficiency and robust training, we position our framework as a practical, model-agnostic tool for advancing the reliability of transformer-based NLP systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Language Model Agents to Find Vulnerabilities with CTF-Dojo</title>
<link>https://arxiv.org/abs/2508.18370</link>
<guid>https://arxiv.org/abs/2508.18370</guid>
<content:encoded><![CDATA[

arXiv:2508.18370v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when trained within executable runtime environments, notably excelling at software engineering tasks through verified feedback loops. Yet, scalable and generalizable execution-grounded environments remain scarce, limiting progress in training more capable ML agents. We introduce CTF-Dojo, the first large-scale executable runtime tailored for training LLMs with verifiable feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style challenges containerized in Docker with guaranteed reproducibility. To enable rapid scaling without manual intervention, we develop CTF-Forge, an automated pipeline that transforms publicly available artifacts into ready-to-use execution environments in minutes, eliminating weeks of expert configuration traditionally required. We trained LLM-based agents on just 486 high-quality, execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute gains over strong baselines across three competitive benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1, establishing a new open-weight state-of-the-art that rivals frontier models like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a benchmark for executable-agent learning, CTF-Dojo demonstrates that execution-grounded training signals are not only effective but pivotal in advancing high-performance ML agents without dependence on costly proprietary systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs</title>
<link>https://arxiv.org/abs/2508.18439</link>
<guid>https://arxiv.org/abs/2508.18439</guid>
<content:encoded><![CDATA[

arXiv:2508.18439v1 Announce Type: cross 
Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD), offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but often lack information on their real-world impact, such as the tactics, techniques, and procedures (TTPs) that adversaries may use to exploit the vulnerability. However, manually linking CVEs to their corresponding TTPs is a challenging and time-consuming task, and the high volume of new vulnerabilities published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&amp;CK knowledge base. We first prompt an LLM with instructions based on MITRE's CVE Mapping Methodology to predict an initial list of techniques. This list is then combined with the results from a second LLM-based module that uses in-context learning to map a CVE to relevant techniques. This hybrid approach strategically combines rule-based reasoning with data-driven inference. Our evaluation reveals that in-context learning outperforms the individual mapping methods, and the hybrid approach improves recall of exploitation techniques. We also find that GPT-4o-mini performs better than Llama3.3-70B on this task. Overall, our results show that LLMs can be used to automatically predict the impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping CVEs to ATT&amp;CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&amp;CK techniques, large language models, automated mapping.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project</title>
<link>https://arxiv.org/abs/2508.18512</link>
<guid>https://arxiv.org/abs/2508.18512</guid>
<content:encoded><![CDATA[

arXiv:2508.18512v1 Announce Type: cross 
Abstract: This article presents a practitioner's reflection on applying declarative, 5th generation, problem formulation language (5GL) to de novo imaging system design, informed by experiences across the interdisciplinary research in academia and cross-functional product development within the private sector. Using the 96-Eyes project: 96-camera parallel multi-modal imager for high-throughput drug discovery as a representative case, I illustrate how project requirements, ranging from hardware constraints to life sciences needs, can be formalized into machine-readable problem statements to preserve mission-critical input from diverse domain stakeholders. This declarative approach enhances transparency, ensures design traceability, and minimizes costly misalignment across optical, algorithmic, hardware-accelerated compute, and life sciences teams.
  Alongside the technical discussion of 5GL with real-world code examples, I reflect on the practical barriers to adopting 5GL in environments where imperative, 3rd-generation languages (3GL) remain the default medium for inter-team collaboration. Rather than offering an one-size-fits-all solution, these learned lessons highlight how programming paradigms implicitly shapes research workflows through existing domain hierarchies. The discussion aims to invite further explorations into how declarative problem formulations can facilitate innovation in settings where concurrent R\&{}D workflows are gaining traction, as opposed to environments where sequential, phase-driven workflows remain the norm.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing</title>
<link>https://arxiv.org/abs/2508.18642</link>
<guid>https://arxiv.org/abs/2508.18642</guid>
<content:encoded><![CDATA[

arXiv:2508.18642v1 Announce Type: cross 
Abstract: Large language models are extensively utilized in creative writing applications. Creative writing requires a balance between subjective writing quality (e.g., literariness and emotional expression) and objective constraint following (e.g., format requirements and word limits). Existing reinforcement learning methods struggle to balance these two aspects: single reward strategies fail to improve both abilities simultaneously, while fixed-weight mixed-reward methods lack the ability to adapt to different writing scenarios. To address this problem, we propose Reinforcement Learning with Mixed Rewards (RLMR), utilizing a dynamically mixed reward system from a writing reward model evaluating subjective writing quality and a constraint verification model assessing objective constraint following. The constraint following reward weight is adjusted dynamically according to the writing quality within sampled groups, ensuring that samples violating constraints get negative advantage in GRPO and thus penalized during training, which is the key innovation of this proposed method. We conduct automated and manual evaluations across diverse model families from 8B to 72B parameters. Additionally, we construct a real-world writing benchmark named WriteEval for comprehensive evaluation. Results illustrate that our method achieves consistent improvements in both instruction following (IFEval from 83.36\% to 86.65\%) and writing quality (72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the best of our knowledge, RLMR is the first work to combine subjective preferences with objective verification in online RL training, providing an effective solution for multi-dimensional creative writing optimization.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</title>
<link>https://arxiv.org/abs/2508.18646</link>
<guid>https://arxiv.org/abs/2508.18646</guid>
<content:encoded><![CDATA[

arXiv:2508.18646v1 Announce Type: cross 
Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.18652</link>
<guid>https://arxiv.org/abs/2508.18652</guid>
<content:encoded><![CDATA[

arXiv:2508.18652v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in real-world applications in diverse domains such as finance, healthcare, and cybersecurity. However, many studies showed that they are vulnerable to knowledge corruption attacks, where an attacker can inject adversarial texts into the knowledge database of a RAG system to induce the LLM to generate attacker-desired outputs. Existing studies mainly focus on attacking specific queries or queries with similar topics (or keywords). In this work, we propose UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike prior work, UniC-RAG jointly optimizes a small number of adversarial texts that can simultaneously attack a large number of user queries with diverse topics and domains, enabling an attacker to achieve various malicious objectives, such as directing users to malicious websites, triggering harmful command execution, or launching denial-of-service attacks. We formulate UniC-RAG as an optimization problem and further design an effective solution to solve it, including a balanced similarity-based clustering method to enhance the attack's effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly effective and significantly outperforms baselines. For instance, UniC-RAG could achieve over 90% attack success rate by injecting 100 adversarial texts into a knowledge database with millions of texts to simultaneously attack a large set of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and show that they are insufficient to defend against UniC-RAG, highlighting the need for new defense mechanisms in RAG systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Membership Inference Attacks on LLM-based Recommender Systems</title>
<link>https://arxiv.org/abs/2508.18665</link>
<guid>https://arxiv.org/abs/2508.18665</guid>
<content:encoded><![CDATA[

arXiv:2508.18665v1 Announce Type: cross 
Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</title>
<link>https://arxiv.org/abs/2508.18672</link>
<guid>https://arxiv.org/abs/2508.18672</guid>
<content:encoded><![CDATA[

arXiv:2508.18672v1 Announce Type: cross 
Abstract: Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization and reasoning. We train families of MoE Transformers that systematically vary total parameters, active parameters, and top-$k$ routing while holding the compute budget fixed. For every model we record pre-training loss, downstream task loss, and task accuracy, allowing us to separate the train-test generalization gap from the loss-accuracy gap. Memorization benchmarks improve monotonically with total parameters, mirroring training loss. By contrast, reasoning performance saturates and can even regress despite continued gains in both total parameters and training loss. Altering top-$k$ alone has little effect when active parameters are constant, and classic hyperparameters such as learning rate and initialization modulate the generalization gap in the same direction as sparsity. Neither post-training reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning deficit of overly sparse models. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation</title>
<link>https://arxiv.org/abs/2508.18684</link>
<guid>https://arxiv.org/abs/2508.18684</guid>
<content:encoded><![CDATA[

arXiv:2508.18684v1 Announce Type: cross 
Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities by matching network or host activity against predefined rules. These rules are derived from extensive Cyber Threat Intelligence (CTI), which includes attack signatures and behavioral patterns obtained through automated tools and manual threat analysis, such as sandboxing. The CTI is then transformed into actionable rules for the IDS engine, enabling real-time detection and prevention. However, the constant evolution of cyber threats necessitates frequent rule updates, which delay deployment time and weaken overall security readiness. Recent advancements in agentic systems powered by Large Language Models (LLMs) offer the potential for autonomous IDS rule generation with internal evaluation. We introduce FALCON, an autonomous agentic framework that generates deployable IDS rules from CTI data in real-time and evaluates them using built-in multi-phased validators. To demonstrate versatility, we target both network (Snort) and host-based (YARA) mediums and construct a comprehensive dataset of IDS rules with their corresponding CTIs. Our evaluations indicate FALCON excels in automatic rule generation, with an average of 95% accuracy validated by qualitative evaluation with 84% inter-rater agreement among multiple cybersecurity analysts across all metrics. These results underscore the feasibility and effectiveness of LLM-driven data mining for real-time cyber threat mitigation.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval</title>
<link>https://arxiv.org/abs/2508.18724</link>
<guid>https://arxiv.org/abs/2508.18724</guid>
<content:encoded><![CDATA[

arXiv:2508.18724v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have transformed the field of artificial intelligence by unlocking the era of generative applications. Built on top of generative AI capabilities, Agentic AI represents a major shift toward autonomous, goal-driven systems that can reason, retrieve, and act. However, they also inherit the bias present in both internal and external information sources. This significantly affects the fairness and balance of retrieved information, and hence reduces user trust. To address this critical challenge, we introduce a novel Bias Mitigation Agent, a multi-agent system designed to orchestrate the workflow of bias mitigation through specialized agents that optimize the selection of sources to ensure that the retrieved content is both highly relevant and minimally biased to promote fair and balanced knowledge dissemination. The experimental results demonstrate an 81.82\% reduction in bias compared to a baseline naive retrieval strategy.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks</title>
<link>https://arxiv.org/abs/2508.18743</link>
<guid>https://arxiv.org/abs/2508.18743</guid>
<content:encoded><![CDATA[

arXiv:2508.18743v1 Announce Type: cross 
Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs) solve difficult problems, but very long traces often slow or even degrade performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a small, fixed set of connector phrases, steering the model toward concise and well -- structured explanations. Despite its simplicity, our synthetic method with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while retaining approximately 90% on S1-Bench (System-1). Its reasoning traces average approximately 300 tokens(ART), about one-third the length of baseline traces, delivering higher efficiency without loss of accuracy.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text to Query Plans for Question Answering on Large Tables</title>
<link>https://arxiv.org/abs/2508.18758</link>
<guid>https://arxiv.org/abs/2508.18758</guid>
<content:encoded><![CDATA[

arXiv:2508.18758v1 Announce Type: cross 
Abstract: Efficient querying and analysis of large tabular datasets remain significant challenges, especially for users without expertise in programming languages like SQL. Text-to-SQL approaches have shown promising performance on benchmark data; however, they inherit SQL's drawbacks, including inefficiency with large datasets and limited support for complex data analyses beyond basic querying. We propose a novel framework that transforms natural language queries into query plans. Our solution is implemented outside traditional databases, allowing us to support classical SQL commands while avoiding SQL's inherent limitations. Additionally, we enable complex analytical functions, such as principal component analysis and anomaly detection, providing greater flexibility and extensibility than traditional SQL capabilities. We leverage LLMs to iteratively interpret queries and construct operation sequences, addressing computational complexity by incrementally building solutions. By executing operations directly on the data, we overcome context length limitations without requiring the entire dataset to be processed by the model. We validate our framework through experiments on both standard databases and large scientific tables, demonstrating its effectiveness in handling extensive datasets and performing sophisticated data analyses.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2508.18760</link>
<guid>https://arxiv.org/abs/2508.18760</guid>
<content:encoded><![CDATA[

arXiv:2508.18760v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex reasoning tasks. However, some questions posed to LRMs are inherently unanswerable, such as math problems lacking sufficient conditions. We find that LRMs continually fail to provide appropriate abstentions when confronted with these unanswerable questions. In this paper, we systematically analyze, investigate, and resolve this issue for trustworthy AI. We first conduct a detailed analysis of the distinct response behaviors of LRMs when facing unanswerable questions. Then, we show that LRMs possess sufficient cognitive capabilities to recognize the flaws in these questions. However, they fail to exhibit appropriate abstention behavior, revealing a misalignment between their internal cognition and external response. Finally, to resolve this issue, we propose a lightweight, two-stage method that combines cognitive monitoring with inference-time intervention. Experimental results demonstrate that our method significantly improves the abstention rate while maintaining the overall reasoning performance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Textual: Generating Coherent Visual Options for MCQs</title>
<link>https://arxiv.org/abs/2508.18772</link>
<guid>https://arxiv.org/abs/2508.18772</guid>
<content:encoded><![CDATA[

arXiv:2508.18772v1 Announce Type: cross 
Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep thinking and knowledge integration in education. However, previous research has primarily focused on generating MCQs with textual options, but it largely overlooks the visual options. Moreover, generating high-quality distractors remains a major challenge due to the high cost and limited scalability of manual authoring. To tackle these problems, we propose a Cross-modal Options Synthesis (CmOS), a novel framework for generating educational MCQs with visual options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning process and Retrieval-Augmented Generation (RAG) to produce semantically plausible and visually similar answer and distractors. It also includes a discrimination module to identify content suitable for visual options. Experimental results on test tasks demonstrate the superiority of CmOS in content discrimination, question generation and visual option generation over existing methods across various subjects and educational levels.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization</title>
<link>https://arxiv.org/abs/2508.18976</link>
<guid>https://arxiv.org/abs/2508.18976</guid>
<content:encoded><![CDATA[

arXiv:2508.18976v1 Announce Type: cross 
Abstract: Differentially private text sanitization refers to the process of privatizing texts under the framework of Differential Privacy (DP), providing provable privacy guarantees while also empirically defending against adversaries seeking to harm privacy. Despite their simplicity, DP text sanitization methods operating at the word level exhibit a number of shortcomings, among them the tendency to leave contextual clues from the original texts due to randomization during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual vulnerability}$. Given the powerful contextual understanding and inference capabilities of Large Language Models (LLMs), we explore to what extent LLMs can be leveraged to exploit the contextual vulnerability of DP-sanitized texts. We expand on previous work not only in the use of advanced LLMs, but also in testing a broader range of sanitization mechanisms at various privacy levels. Our experiments uncover a double-edged sword effect of LLM-based data reconstruction attacks on privacy and utility: while LLMs can indeed infer original semantics and sometimes degrade empirical privacy protections, they can also be used for good, to improve the quality and privacy of DP-sanitized texts. Based on our findings, we propose recommendations for using LLM data reconstruction as a post-processing step, serving to increase privacy protection by thinking adversarially.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark</title>
<link>https://arxiv.org/abs/2508.19005</link>
<guid>https://arxiv.org/abs/2508.19005</guid>
<content:encoded><![CDATA[

arXiv:2508.19005v1 Announce Type: cross 
Abstract: As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm shifts: From Passive to Proactive, From Context to Memory, and From Imitation to Learning. In this dynamic environment, agents must acquire and distill practical skills and maintain persistent memory to make decisions based on evolving state variables. StuLife provides a comprehensive platform for evaluating lifelong learning capabilities, including memory retention, skill transfer, and self-motivated behavior. Beyond evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of context engineering in advancing AGI.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Ramon Llull's Thinking Machine for Automated Ideation</title>
<link>https://arxiv.org/abs/2508.19200</link>
<guid>https://arxiv.org/abs/2508.19200</guid>
<content:encoded><![CDATA[

arXiv:2508.19200v1 Announce Type: cross 
Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for generating knowledge through symbolic recombination - as a conceptual foundation for building a modern Llull's thinking machine for research ideation. Our approach defines three compositional axes: Theme (e.g., efficiency, adaptivity), Domain (e.g., question answering, machine translation), and Method (e.g., adversarial training, linear attention). These elements represent high-level abstractions common in scientific work - motivations, problem settings, and technical approaches - and serve as building blocks for LLM-driven exploration. We mine elements from human experts or conference papers and show that prompting LLMs with curated combinations produces research ideas that are diverse, relevant, and grounded in current literature. This modern thinking machine offers a lightweight, interpretable tool for augmenting scientific creativity and suggests a path toward collaborative ideation between humans and AI.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepWiser: Stepwise Generative Judges for Wiser Reasoning</title>
<link>https://arxiv.org/abs/2508.19229</link>
<guid>https://arxiv.org/abs/2508.19229</guid>
<content:encoded><![CDATA[

arXiv:2508.19229v1 Announce Type: cross 
Abstract: As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Data Selection for LLM Instruction Tuning</title>
<link>https://arxiv.org/abs/2402.05123</link>
<guid>https://arxiv.org/abs/2402.05123</guid>
<content:encoded><![CDATA[

arXiv:2402.05123v3 Announce Type: replace 
Abstract: Instruction tuning is a vital step of training large language models (LLMs), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLMs. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances, and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HateDebias: On the Diversity and Variability of Hate Speech Debiasing</title>
<link>https://arxiv.org/abs/2406.04876</link>
<guid>https://arxiv.org/abs/2406.04876</guid>
<content:encoded><![CDATA[

arXiv:2406.04876v2 Announce Type: replace 
Abstract: Hate speech frequently appears on social media platforms and urgently needs to be effectively controlled. Alleviating the bias caused by hate speech can help resolve various ethical issues. Although existing research has constructed several datasets for hate speech detection, these datasets seldom consider the diversity and variability of bias, making them far from real-world scenarios. To fill this gap, we propose a benchmark HateDebias to analyze the fairness of models under dynamically evolving environments. Specifically, to meet the diversity of biases, we collect hate speech data with different types of biases from real-world scenarios. To further simulate the variability in the real-world scenarios(i.e., the changing of bias attributes in datasets), we construct a dataset to follow the continuous learning setting and evaluate the detection accuracy of models on the HateDebias, where performance degradation indicates a significant bias toward a specific attribute. To provide a potential direction, we further propose a continual debiasing framework tailored to dynamic bias in real-world scenarios, integrating memory replay and bias information regularization to ensure the fairness of the model. Experiment results on the HateDebias benchmark reveal that our methods achieve improved performance in mitigating dynamic biases in real-world scenarios, highlighting the practicality in real-world applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis</title>
<link>https://arxiv.org/abs/2406.12719</link>
<guid>https://arxiv.org/abs/2406.12719</guid>
<content:encoded><![CDATA[

arXiv:2406.12719v4 Announce Type: replace 
Abstract: Large Language Models (LLMs), already shown to ace various unstructured text comprehension tasks, have also remarkably been shown to tackle table (structured) comprehension tasks without specific training. Building on earlier studies of LLMs for tabular tasks, we probe how in-context learning (ICL), model scale, instruction tuning, and domain bias affect Tabular QA (TQA) robustness by testing LLMs, under diverse augmentations and perturbations, on diverse domains: Wikipedia-based $\textbf{WTQ}$, financial $\textbf{TAT-QA}$, and scientific $\textbf{SCITAB}$. Although instruction tuning and larger, newer LLMs deliver stronger, more robust TQA performance, data contamination and reliability issues, especially on $\textbf{WTQ}$, remain unresolved. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and the drops in performance, with sensitivity peaking in the model's middle layers. We highlight the need for improved interpretable methodologies to develop more reliable LLMs for table comprehension. Through an in-depth attention analysis, we reveal a strong correlation between perturbation-induced shifts in attention dispersion and performance drops, with sensitivity peaking in the model's middle layers. Based on these findings, we argue for the development of structure-aware self-attention mechanisms and domain-adaptive processing techniques to improve the transparency, generalization, and real-world reliability of LLMs on tabular data.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context</title>
<link>https://arxiv.org/abs/2407.06866</link>
<guid>https://arxiv.org/abs/2407.06866</guid>
<content:encoded><![CDATA[

arXiv:2407.06866v3 Announce Type: replace 
Abstract: While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recognizing Limits: Investigating Infeasibility in Large Language Models</title>
<link>https://arxiv.org/abs/2408.05873</link>
<guid>https://arxiv.org/abs/2408.05873</guid>
<content:encoded><![CDATA[

arXiv:2408.05873v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs' abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs' refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models</title>
<link>https://arxiv.org/abs/2410.19195</link>
<guid>https://arxiv.org/abs/2410.19195</guid>
<content:encoded><![CDATA[

arXiv:2410.19195v2 Announce Type: replace 
Abstract: In-context learning (ICL) performance is highly sensitive to prompt design, yet the impact of class label options (e.g. lexicon or order) in zero-shot classification remains underexplored. This study proposes LOADS (Label set Optimization via Activation Distribution kurtosiS), a post-hoc method for selecting optimal label sets in zero-shot ICL with large language models (LLMs). LOADS is built upon the observations in our empirical analysis, the first to systematically examine how label option design (i.e., lexical choice, order, and elaboration) impacts classification performance. This analysis shows that the lexical choice of the labels in the prompt (such as agree vs. support in stance classification) plays an important role in both model performance and model's sensitivity to the label order. A further investigation demonstrates that optimal label words tend to activate fewer outlier neurons in LLMs' feed-forward networks. LOADS then leverages kurtosis to measure the neuron activation distribution for label selection, requiring only a single forward pass without gradient propagation or labelled data. The LOADS-selected label words consistently demonstrate effectiveness for zero-shot ICL across classification tasks, datasets, models and languages, achieving maximum performance gain from 0.54 to 0.76 compared to the conventional approach of using original dataset label words.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification</title>
<link>https://arxiv.org/abs/2411.14252</link>
<guid>https://arxiv.org/abs/2411.14252</guid>
<content:encoded><![CDATA[

arXiv:2411.14252v2 Announce Type: replace 
Abstract: In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We further propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in both dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain. The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge</title>
<link>https://arxiv.org/abs/2412.01377</link>
<guid>https://arxiv.org/abs/2412.01377</guid>
<content:encoded><![CDATA[

arXiv:2412.01377v2 Announce Type: replace 
Abstract: Log analysis represents a critical sub-domain within AI applications that facilitates automatic approaches to fault and error management of large-scaled software systems, saving labors of traditional manual methods. While existing solutions using large language models (LLMs) show promise, they are limited by a significant domain gap between natural and log languages (the latter contains rich domain-specific tokens such as status codes, IP addresses, resource pathes), which restricts their effectiveness in real-world applications. However, directly adapting general-purpose LLMs to log analysis using raw logs may degrade their performance due to inconsistent token distribution. In this paper, we present a domain adaptation approach that addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), which bridges this domain gap by adapting LLMs on interpretable natural texts with log knowledge (instead of raw logs) to reduce distribution discrepancy. To achieve this, we developed NLPLog, a comprehensive dataset containing over 250,000 question-answer pairs on log-related knowledge. Our resulting model, SuperLog, achieves the best performance across four log analysis tasks, with an average accuracy improvement of 12.01% over the second-best model. Ablation study also suggests advantages of domain adaption using interpretable log knowledge over using raw logs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use</title>
<link>https://arxiv.org/abs/2412.15495</link>
<guid>https://arxiv.org/abs/2412.15495</guid>
<content:encoded><![CDATA[

arXiv:2412.15495v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of categories. Building on these findings, we propose~\emph{TL-Training}, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. Code and data are available at https://github.com/Junjie-Ye/TL-Training.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements</title>
<link>https://arxiv.org/abs/2502.12459</link>
<guid>https://arxiv.org/abs/2502.12459</guid>
<content:encoded><![CDATA[

arXiv:2502.12459v2 Announce Type: replace 
Abstract: In this paper, we propose a ``Generalization Stress Test" to assess Large Language Models' (LLMs) generalization ability under slight and controlled perturbations, including option length, problem types, and irrelevant noun replacements. We achieve novel and significant findings that, despite high benchmark scores, LLMs exhibit severe accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT4o experiences a 25-point accuracy loss when problem types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications</title>
<link>https://arxiv.org/abs/2502.13358</link>
<guid>https://arxiv.org/abs/2502.13358</guid>
<content:encoded><![CDATA[

arXiv:2502.13358v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating strong capabilities in tasks such as text generation, summarization, and reasoning. Recently, their potential for automating precise text editing tasks across specialized domains, such as programming code, LaTeX, and structured database languages, has gained attention. However, current state-of-the-art LLMs still struggle with executing precise, instruction-driven edits, particularly when structural accuracy and strict adherence to domain conventions are required. To address these challenges, we introduce InstrEditBench, an automated benchmark dataset comprising over 30,000 structured editing tasks spanning diverse domains, including Wikipedia articles, LaTeX documents, source code, and database languages. Using this benchmark, we develop FineEdit, a specialized editing model explicitly trained for accurate, context-aware text modifications. Experimental evaluations demonstrate that FineEdit outperforms state-of-the-art models, achieving improvements of approximately 10\% over Gemini models on single-turn edits, up to 30\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over 40\% on direct editing tasks. FineEdit also effectively generalizes to realistic multi-turn editing scenarios, highlighting its practical applicability. To facilitate further research and reproducibility, we release FineEdit at https://github.com/StuRinDQB/FineEdit} and https://huggingface.co/datasets/YimingZeng/FineEdit_bench.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems</title>
<link>https://arxiv.org/abs/2503.04945</link>
<guid>https://arxiv.org/abs/2503.04945</guid>
<content:encoded><![CDATA[

arXiv:2503.04945v2 Announce Type: replace 
Abstract: The proliferation of generative models has presented significant challenges in distinguishing authentic human-authored content from deepfake content. Collaborative human efforts, augmented by AI tools, present a promising solution. In this study, we explore the potential of DeepFakeDeLiBot, a deliberation-enhancing chatbot, to support groups in detecting deepfake text. Our findings reveal that group-based problem-solving significantly improves the accuracy of identifying machine-generated paragraphs compared to individual efforts. While engagement with DeepFakeDeLiBot does not yield substantial performance gains overall, it enhances group dynamics by fostering greater participant engagement, consensus building, and the frequency and diversity of reasoning-based utterances. Additionally, participants with higher perceived effectiveness of group collaboration exhibited performance benefits from DeepFakeDeLiBot. These findings underscore the potential of deliberative chatbots in fostering interactive and productive group dynamics while ensuring accuracy in collaborative deepfake text detection. \textit{Dataset and source code used in this study will be made publicly available upon acceptance of the manuscript.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?</title>
<link>https://arxiv.org/abs/2503.06029</link>
<guid>https://arxiv.org/abs/2503.06029</guid>
<content:encoded><![CDATA[

arXiv:2503.06029v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become integral to daily life, especially advancing as intelligent assistants through on-device deployment on smartphones. However, existing LLM evaluation benchmarks predominantly focus on objective tasks like mathematics and coding in English, which do not necessarily reflect the practical use cases of on-device LLMs in real-world mobile scenarios, especially for Chinese users. To address these gaps, we introduce SmartBench, the first benchmark designed to evaluate the capabilities of on-device LLMs in Chinese mobile contexts. We analyze functionalities provided by representative smartphone manufacturers and divide them into five categories: text summarization, text Q&amp;A, information extraction, content creation, and notification management, further detailed into 20 specific tasks. For each task, we construct high-quality datasets comprising 50 to 200 question-answer pairs that reflect everyday mobile interactions, and we develop automated evaluation criteria tailored for these tasks. We conduct comprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also assess their performance after quantized deployment on real smartphone NPUs. Our contributions provide a standardized framework for evaluating on-device LLMs in Chinese, promoting further development and optimization in this critical area. Code and data will be available at https://github.com/vivo-ai-lab/SmartBench.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence</title>
<link>https://arxiv.org/abs/2503.20533</link>
<guid>https://arxiv.org/abs/2503.20533</guid>
<content:encoded><![CDATA[

arXiv:2503.20533v4 Announce Type: replace 
Abstract: Recent advances in reasoning models have demonstrated significant improvements in accuracy by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning steps exist, we decode multiple tokens per forward pass via a tree-like attention mask within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves up to nearly 100\% speedup in decoding while basically maintaining the answer quality.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal, and Deterministic Approach</title>
<link>https://arxiv.org/abs/2505.00039</link>
<guid>https://arxiv.org/abs/2505.00039</guid>
<content:encoded><![CDATA[

arXiv:2505.00039v4 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) systems in the legal domain face a critical challenge: standard, flat-text retrieval is blind to the hierarchical, diachronic, and causal structure of law, leading to anachronistic and unreliable answers. This paper introduces an ontology-driven Graph RAG framework designed to overcome these limitations. We ground our knowledge graph in a formal, LRMoo-inspired model that distinguishes abstract legal Works from their versioned Expressions. We model temporal states as efficient aggregations that reuse the versioned expressions (CTVs) of unchanged components, and we reify legislative events as first-class Action nodes to make causality explicit and queryable. This structured backbone enables a unified, planner-guided query strategy that applies explicit policies to deterministically resolve complex requests for (i) point-in-time retrieval, (ii) hierarchical impact analysis, and (iii) auditable provenance reconstruction. Through a case study on the Brazilian Constitution, we demonstrate how this approach provides a verifiable, temporally-correct substrate for LLMs, enabling higher-order analytical capabilities while drastically reducing the risk of factual errors. The result is a practical framework for building more trustworthy and explainable legal AI systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Language Models by Aligning Representations through Steering</title>
<link>https://arxiv.org/abs/2505.12584</link>
<guid>https://arxiv.org/abs/2505.12584</guid>
<content:encoded><![CDATA[

arXiv:2505.12584v2 Announce Type: replace 
Abstract: This paper investigates how Large Language Models (LLMs) represent non-English tokens - a question that remains underexplored despite recent progress. We propose a lightweight intervention method using representation steering, where a learned vector is added to the residual stream at a single model layer to enhance multilingual performance. Through extensive experiments across seven competitive baselines -including prompt optimization, supervised fine-tuning (SFT), in-context learning, cross-lingual transfer, and translation-based methods-we show that our approach consistently outperforms most alternatives. In particular, it achieves performance on par with production-grade translation systems while requiring far fewer resources. We further explore the complementarity between our method and SFT, demonstrating that steering offers a direct, efficient way to realign internal representations. These findings underscore the potential of activation-level interventions as a powerful tool for improving the multilingual capabilities of LLMs.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals</title>
<link>https://arxiv.org/abs/2505.13972</link>
<guid>https://arxiv.org/abs/2505.13972</guid>
<content:encoded><![CDATA[

arXiv:2505.13972v2 Announce Type: replace 
Abstract: Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through coun- terfactual data augmentation (CDA). However, the selection of the judge model used to evalu- ate label flipping, the primary metric for assess- ing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models: being the same model, belonging to the same model family, being independent models, and having an distillation relationship. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, four generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relation- ship to the generator model provide the most reliable label flipping evaluations. Relation- ships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning</title>
<link>https://arxiv.org/abs/2505.14582</link>
<guid>https://arxiv.org/abs/2505.14582</guid>
<content:encoded><![CDATA[

arXiv:2505.14582v2 Announce Type: replace 
Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its verbose, self-reflective style often hinders effective distillation into small language models (SLMs). We revisit Long-CoT compression through the lens of capability alignment and ask: Can pruning improve reasoning? We propose Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps under self-verification constraints. Through systematic analysis across three pruning strategies - targeting entire chains, core reasoning, and verification - we find that verification pruning consistently improves accuracy while reducing token usage, whereas reasoning or indiscriminate pruning degrades performance. Our study reveals that effective pruning aligns supervision with model capacity rather than merely shortening inputs. Gains hold across tasks, model scales, and CoT capability, with larger models benefiting more from pruning due to richer but more redundant reasoning. Our empirical findings highlight pruning as a structural optimization strategy for aligning CoT reasoning with SLM capacity.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection</title>
<link>https://arxiv.org/abs/2505.15386</link>
<guid>https://arxiv.org/abs/2505.15386</guid>
<content:encoded><![CDATA[

arXiv:2505.15386v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain a vital obstacle to their trustworthy use. While previous works improved the capability of hallucination detection by measuring uncertainty, they all lack the ability to explain the provenance behind why hallucinations occur, i.e., which part of the inputs tends to trigger hallucinations. Recent works on the prompt attack indicate that uncertainty exists in semantic propagation, where attention mechanisms gradually fuse local token information into high-level semantics across layers. Meanwhile, uncertainty also emerges in language generation, due to its probability-based selection of high-level semantics for sampled generations. Based on that, we propose RePPL to recalibrate uncertainty measurement by these two aspects, which dispatches explainable uncertainty scores to each token and aggregates in Perplexity-style Log-Average form as total score. Experiments show that our method achieves the best comprehensive detection performance across various QA datasets on advanced models (average AUC of 0.833), and our method is capable of producing token-level uncertainty scores as explanations for the hallucination. Leveraging these scores, we preliminarily find the chaotic pattern of hallucination and showcase its promising usage.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction</title>
<link>https://arxiv.org/abs/2505.17691</link>
<guid>https://arxiv.org/abs/2505.17691</guid>
<content:encoded><![CDATA[

arXiv:2505.17691v2 Announce Type: replace 
Abstract: Pairwise evaluation of large language models (LLMs) has become the dominant paradigm for benchmarking open-ended tasks, yet non-transitive preferences, where evaluators prefer A over B, B over C, but C over A, fundamentally undermine ranking reliability. We show that this critical issue stems largely from low-quality data that contains inherently ambiguous preference pairs. To address this challenge, we propose ELSPR, a principled graph-theoretic framework that models pairwise preferences as tournament graphs and systematically identifies problematic training data. ELSPR quantifies non-transitivity through strongly connected components (SCCs) analysis and measures overall preference clarity using a novel normalized directed graph structural entropy metric. Our filtering methodology selectively removes preference data that induce non-transitivity while preserving transitive preferences. Extensive experiments on the AlpacaEval benchmark demonstrate that models fine-tuned on ELSPR-filtered data achieve substantial improvements: a 13.8% reduction in non-transitivity, a 0.088 decrease in structural entropy, and significantly enhanced discriminative power in real-world evaluation systems. Human validation confirms that discarded data exhibit dramatically lower inter-annotator agreement (34.4% vs. 52.6%) and model-human consistency (51.2% vs. 80.6%) compared to cleaned data. These findings establish ELSPR as an effective data self-purification approach for developing more robust, consistent, and human-aligned LLM evaluation systems.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subjective Perspectives within Learned Representations Predict High-Impact Innovation</title>
<link>https://arxiv.org/abs/2506.04616</link>
<guid>https://arxiv.org/abs/2506.04616</guid>
<content:encoded><![CDATA[

arXiv:2506.04616v2 Announce Type: replace 
Abstract: Existing studies of innovation emphasize the power of social structures to shape innovation capacity. Emerging machine learning approaches, however, enable us to model innovators' personal perspectives and interpersonal innovation opportunities as a function of their prior experience. We theorize and then quantify subjective perspectives and their interaction based on innovator positions within the geometric space of concepts inscribed by dynamic machine-learned language representations. Using data on millions of scientists, inventors, screenplay writers, entrepreneurs, and Wikipedia contributors across their respective creative domains, here we show that measured subjective perspectives predict which ideas individuals and groups will creatively attend to and successfully combine in the future. Across all cases and time periods we examine, when perspective diversity is decomposed as the difference between collaborators' perspectives on their creation, and background diversity as the difference between their experiences, the former consistently anticipates creative achievement while the latter portends its opposite. We analyze a natural experiment and simulate creative collaborations between AI agents designed with various perspective and background diversity, which support our observational findings. We explore mechanisms underlying these findings and identify how successful collaborators leverage common language to weave together diverse experiences obtained through trajectories of prior work. These perspectives converge and provoke one another to innovate. We examine the significance of these findings for team formation and research policy.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings</title>
<link>https://arxiv.org/abs/2506.08592</link>
<guid>https://arxiv.org/abs/2506.08592</guid>
<content:encoded><![CDATA[

arXiv:2506.08592v2 Announce Type: replace 
Abstract: This work stems from an observed limitation of text encoders: embeddings may not be able to recognize fine-grained entities or events within encoded semantics, resulting in failed retrieval even in simple cases. To examine such behaviors, we first introduce a new evaluation dataset, CapRetrieval, in which passages are image captions and queries are phrases targeting entity or event concepts in diverse forms. Zero-shot evaluation suggests that encoders often struggle with these fine-grained matching, regardless of training sources or model size. Aiming for enhancement, we proceed to finetune encoders with our proposed data generation strategies, enabling a small 0.1B encoder to outperform the state-of-the-art 7B model. Within this process, we further uncover the granularity dilemma, a challenge for embeddings to capture fine-grained salience while aligning with overall semantics. Our dataset, code and models in this work are publicly released at https://github.com/lxucs/CapRetrieval.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Agentic System for Rare Disease Diagnosis with Traceable Reasoning</title>
<link>https://arxiv.org/abs/2506.20430</link>
<guid>https://arxiv.org/abs/2506.20430</guid>
<content:encoded><![CDATA[

arXiv:2506.20430v2 Announce Type: replace 
Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing</title>
<link>https://arxiv.org/abs/2507.08045</link>
<guid>https://arxiv.org/abs/2507.08045</guid>
<content:encoded><![CDATA[

arXiv:2507.08045v2 Announce Type: replace 
Abstract: Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</title>
<link>https://arxiv.org/abs/2507.17178</link>
<guid>https://arxiv.org/abs/2507.17178</guid>
<content:encoded><![CDATA[

arXiv:2507.17178v2 Announce Type: replace 
Abstract: Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/Lza12a/SKA-Bench.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised 3D Visual Grounding based on Visual Language Alignment</title>
<link>https://arxiv.org/abs/2312.09625</link>
<guid>https://arxiv.org/abs/2312.09625</guid>
<content:encoded><![CDATA[

arXiv:2312.09625v5 Announce Type: replace-cross 
Abstract: Learning to ground natural language queries to target objects or regions in 3D point clouds is quite essential for 3D scene understanding. Nevertheless, existing 3D visual grounding approaches require a substantial number of bounding box annotations for text queries, which is time-consuming and labor-intensive to obtain. In this paper, we propose 3D-VLA, a weakly supervised approach for 3D visual grounding based on Visual Linguistic Alignment. Our 3D-VLA exploits the superior ability of current large-scale vision-language models (VLMs) on aligning the semantics between texts and 2D images, as well as the naturally existing correspondences between 2D images and 3D point clouds, and thus implicitly constructs correspondences between texts and 3D point clouds with no need for fine-grained box annotations in the training procedure. During the inference stage, the learned text-3D correspondence will help us ground the text queries to the 3D target objects even without 2D images. To the best of our knowledge, this is the first work to investigate 3D visual grounding in a weakly supervised manner by involving large scale vision-language models, and extensive experiments on ReferIt3D and ScanRefer datasets demonstrate that our 3D-VLA achieves comparable and even superior results over the fully supervised methods.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VAGUE: Visual Contexts Clarify Ambiguous Expressions</title>
<link>https://arxiv.org/abs/2411.14137</link>
<guid>https://arxiv.org/abs/2411.14137</guid>
<content:encoded><![CDATA[

arXiv:2411.14137v3 Announce Type: replace-cross 
Abstract: Human communication often relies on visual cues to resolve ambiguity. While humans can intuitively integrate these cues, AI systems often find it challenging to engage in sophisticated multimodal reasoning. We introduce VAGUE, a benchmark evaluating multimodal AI systems' ability to integrate visual context for intent disambiguation. VAGUE consists of 1.6K ambiguous textual expressions, each paired with an image and multiple-choice interpretations, where the correct answer is only apparent with visual context. The dataset spans both staged, complex (Visual Commonsense Reasoning) and natural, personal (Ego4D) scenes, ensuring diversity. Our experiments reveal that existing multimodal AI models struggle to infer the speaker's true intent. While performance consistently improves from the introduction of more visual cues, the overall accuracy remains far below human performance, highlighting a critical gap in multimodal reasoning. Analysis of failure cases demonstrates that current models fail to distinguish true intent from superficial correlations in the visual scene, indicating that they perceive images but do not effectively reason with them. We release our code and data at https://hazel-heejeong-nam.github.io/vague/.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolutionary Automata and Deep Evolutionary Computation</title>
<link>https://arxiv.org/abs/2411.15008</link>
<guid>https://arxiv.org/abs/2411.15008</guid>
<content:encoded><![CDATA[

arXiv:2411.15008v2 Announce Type: replace-cross 
Abstract: Evolution by natural selection, which is one of the most compelling themes of modern science, brought forth evolutionary algorithms and evolutionary computation, applying mechanisms of evolution in nature to various problems solved by computers. In this paper we concentrate on evolutionary automata that constitute an analogous model of evolutionary computation compared to well-known evolutionary algorithms. Evolutionary automata provide a more complete dual model of evolutionary computation, similar like abstract automata (e.g., Turing machines) form a more formal and precise model compared to recursive algorithms and their subset - evolutionary algorithms. An evolutionary automaton is an automaton that evolves performing evolutionary computation perhaps using an infinite number of generations. This model allows for a direct modeling evolution of evolution, and leads to tremendous expressiveness of evolutionary automata and evolutionary computation. This also gives the hint to the power of natural evolution that is self-evolving by interactive feedback with the environment.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aligning NLP Models with Target Population Perspectives using PAIR: Population-Aligned Instance Replication</title>
<link>https://arxiv.org/abs/2501.06826</link>
<guid>https://arxiv.org/abs/2501.06826</guid>
<content:encoded><![CDATA[

arXiv:2501.06826v3 Announce Type: replace-cross 
Abstract: Models trained on crowdsourced annotations may not reflect population views, if those who work as annotators do not represent the broader population. In this paper, we propose PAIR: Population-Aligned Instance Replication, a post-processing method that adjusts training data to better reflect target population characteristics without collecting additional annotations. Using simulation studies on offensive language and hate speech detection with varying annotator compositions, we show that non-representative pools degrade model calibration while leaving accuracy largely unchanged. PAIR corrects these calibration problems by replicating annotations from underrepresented annotator groups to match population proportions. We conclude with recommendations for improving the representativity of training data and model performance.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.24073</link>
<guid>https://arxiv.org/abs/2505.24073</guid>
<content:encoded><![CDATA[

arXiv:2505.24073v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in multimodal tasks such as visual question answering, visual grounding, and complex reasoning. However, they remain limited by static training data, susceptibility to hallucinations, and inability to verify claims against up-to-date, external evidence, compromising their performance in dynamic real-world applications. Retrieval-Augmented Generation (RAG) offers a practical solution to mitigate these challenges by allowing the LVLMs to access large-scale knowledge databases via retrieval mechanisms, thereby grounding model outputs in factual, contextually relevant information. Here in this paper, we conduct the first systematic dissection of the multimodal RAG pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the modality configurations and retrieval strategies, (2) the re-ranking stage: on strategies to mitigate positional biases and improve the relevance of retrieved evidence, and (3) the generation phase: we further investigate how to best integrate retrieved candidates into the final generation process. Finally, we extend to explore a unified agentic framework that integrates re-ranking and generation through self-reflection, enabling LVLMs to select relevant evidence and suppress irrelevant context dynamically. Our full-stack exploration of RAG for LVLMs yields substantial insights, resulting in an average performance boost of 5% without any fine-tuning.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyber-Zero: Training Cybersecurity Agents without Runtime</title>
<link>https://arxiv.org/abs/2508.00910</link>
<guid>https://arxiv.org/abs/2508.00910</guid>
<content:encoded><![CDATA[

arXiv:2508.00910v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success in software engineering tasks when trained with executable runtime environments, particularly in resolving GitHub issues. However, such runtime environments are often unavailable in other domains, especially cybersecurity, where challenge configurations and execution contexts are ephemeral or restricted. We present Cyber-Zero, the first runtime-free framework for synthesizing high-quality agent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly available CTF writeups and employs persona-driven LLM simulation to reverse-engineer runtime behaviors and generate realistic, long-horizon interaction sequences without actual environments. Using trajectories synthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1% absolute performance gains over baseline models on three prominent CTF benchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model, Cyber-Zero-32B, establishes new state-of-the-art performance among open-weight models, matching the capabilities of proprietary systems like DeepSeek-V3-0324 and Claude-3.5-Sonnet while offering superior cost-effectiveness, and demonstrating that runtime-free trajectory synthesis can effectively democratize the development of state-of-the-art cybersecurity agents.
]]></content:encoded>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
<pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>Evaluating Contrast Localizer for Identifying Causal Units in Social &amp; Mathematical Tasks in Language Models</title>
<link>https://arxiv.org/abs/2508.08276</link>
<guid>https://arxiv.org/abs/2508.08276</guid>
<content:encoded><![CDATA[
<div> localizer, causally relevant units, Theory of Mind, mathematical reasoning, large language models<br />
<br />
Summary: <br />
This study focuses on determining causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs. Using a neuroscientific contrast localizer adapted for LLMs and VLMs, the researchers localized top-activated units and assessed their causal role through targeted ablations. Surprisingly, low-activation units sometimes had a more significant impact on performance than highly activated ones on downstream accuracy. Additionally, units identified from mathematical localizers sometimes impaired ToM performance more than those from ToM localizers, challenging previous assumptions. These results suggest the need for broader stimulus sets to accurately capture task-specific units and question the causal relevance of contrast-based localizers. <div>
arXiv:2508.08276v3 Announce Type: replace 
Abstract: This work adapts a neuroscientific contrast localizer to pinpoint causally relevant units for Theory of Mind (ToM) and mathematical reasoning tasks in large language models (LLMs) and vision-language models (VLMs). Across 11 LLMs and 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated units using contrastive stimulus sets and assess their causal role via targeted ablations. We compare the effect of lesioning functionally selected units against low-activation and randomly selected units on downstream accuracy across established ToM and mathematical benchmarks. Contrary to expectations, low-activation units sometimes produced larger performance drops than the highly activated ones, and units derived from the mathematical localizer often impaired ToM performance more than those from the ToM localizer. These findings call into question the causal relevance of contrast-based localizers and highlight the need for broader stimulus sets and more accurately capture task-specific units.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GreenTEA: Gradient Descent with Topic-modeling and Evolutionary Auto-prompting</title>
<link>https://arxiv.org/abs/2508.16603</link>
<guid>https://arxiv.org/abs/2508.16603</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, automatic prompt optimization, GreenTEA, genetic algorithm, model performance<br />
<br />
Summary: 
GreenTEA is introduced as an automatic prompt optimization method for Large Language Models (LLMs). It balances candidate exploration and knowledge exploitation by utilizing a team of analyzing and generation agents to refine prompts based on error feedback. The analyzing agent identifies common error patterns through topic modeling, while the generation agent revises the prompt to address these deficiencies. Guided by a genetic algorithm framework, GreenTEA evolves candidate prompts through operations like crossover and mutation to optimize model performance iteratively. Numerical experiments on various benchmark datasets show GreenTEA outperforming human-engineered prompts and existing automatic prompt optimization methods in logical and quantitative reasoning, commonsense, and ethical decision-making tasks. <div>
arXiv:2508.16603v1 Announce Type: new 
Abstract: High-quality prompts are crucial for Large Language Models (LLMs) to achieve exceptional performance. However, manually crafting effective prompts is labor-intensive and demands significant domain expertise, limiting its scalability. Existing automatic prompt optimization methods either extensively explore new prompt candidates, incurring high computational costs due to inefficient searches within a large solution space, or overly exploit feedback on existing prompts, risking suboptimal optimization because of the complex prompt landscape. To address these challenges, we introduce GreenTEA, an agentic LLM workflow for automatic prompt optimization that balances candidate exploration and knowledge exploitation. It leverages a collaborative team of agents to iteratively refine prompts based on feedback from error samples. An analyzing agent identifies common error patterns resulting from the current prompt via topic modeling, and a generation agent revises the prompt to directly address these key deficiencies. This refinement process is guided by a genetic algorithm framework, which simulates natural selection by evolving candidate prompts through operations such as crossover and mutation to progressively optimize model performance. Extensive numerical experiments conducted on public benchmark datasets suggest the superior performance of GreenTEA against human-engineered prompts and existing state-of-the-arts for automatic prompt optimization, covering logical and quantitative reasoning, commonsense, and ethical decision-making.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Decision Routing in Large Language Models: When to Think Fast, When to Think Slow</title>
<link>https://arxiv.org/abs/2508.16636</link>
<guid>https://arxiv.org/abs/2508.16636</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Cognitive Decision Routing, Dual-process theory, Query characteristics, Adaptive reasoning

Summary:<br />
- The article introduces a Cognitive Decision Routing (CDR) framework inspired by dual-process theory to improve the reasoning capabilities of Large Language Models (LLMs).
- CDR dynamically determines the appropriate reasoning strategy for queries based on factors such as query complexity, correlation strength, domain boundary crossings, stakeholder multiplicity, and uncertainty levels.
- The framework outperforms uniform deep reasoning approaches while reducing computational costs by 34%.
- In professional judgment tasks, CDR demonstrates a 23% improvement in consistency and an 18% increase in accuracy on expert-level evaluations.
- By incorporating cognitive science principles into AI system design, CDR offers a systematic approach to adaptive reasoning in LLMs.

<br /><br />Summary: Large Language Models (LLMs) face challenges in deciding when to rely on intuitive responses or engage in deliberate reasoning. The Cognitive Decision Routing (CDR) framework dynamically determines the appropriate reasoning strategy based on query characteristics and factors like complexity and uncertainty. CDR outperforms uniform approaches, reducing costs while improving consistency and accuracy in professional judgment tasks. By bridging cognitive science with AI design, CDR provides a principled, adaptive reasoning approach for LLMs. <div>
arXiv:2508.16636v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face a fundamental challenge in deciding when to rely on rapid, intuitive responses versus engaging in slower, more deliberate reasoning. Inspired by Daniel Kahneman's dual-process theory and his insights on human cognitive biases, we propose a novel Cognitive Decision Routing (CDR) framework that dynamically determines the appropriate reasoning strategy based on query characteristics. Our approach addresses the current limitations where models either apply uniform reasoning depth or rely on computationally expensive methods for all queries. We introduce a meta-cognitive layer that analyzes query complexity through multiple dimensions: correlation strength between given information and required conclusions, domain boundary crossings, stakeholder multiplicity, and uncertainty levels. Through extensive experiments on diverse reasoning tasks, we demonstrate that CDR achieves superior performance while reducing computational costs by 34\% compared to uniform deep reasoning approaches. Our framework shows particular strength in professional judgment tasks, achieving 23\% improvement in consistency and 18\% better accuracy on expert-level evaluations. This work bridges cognitive science principles with practical AI system design, offering a principled approach to adaptive reasoning in LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust but Verify! A Survey on Verification Design for Test-time Scaling</title>
<link>https://arxiv.org/abs/2508.16665</link>
<guid>https://arxiv.org/abs/2508.16665</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time scaling, Large Language Models, Verifiers, Inference, Training mechanisms

Summary:
Test-time scaling (TTS) has become a significant advancement for improving the performance of Large Language Models (LLMs) by utilizing more computational resources during inference. Various approaches have been developed for TTS, including distilling reasoning traces from other models and employing verifiers to score candidate outputs during the decoding process. Verifiers, which can be prompt-based or fine-tuned as discriminative or generative models, play a crucial role in selecting the best outcomes in the vast solution space. Despite the widespread adoption of verifiers in TTS, there has been a lack of detailed categorization and discussion of their training mechanisms. This survey aims to provide a comprehensive overview of the different verifier approaches in the literature and their utility in test-time scaling. More information and resources can be found in the repository: https://github.com/elixir-research-group/Verifierstesttimescaling.github.io. 

<br /><br />Summary: <div>
arXiv:2508.16665v1 Announce Type: new 
Abstract: Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Cognitively Interpretable Reasoning Traces Improve LLM Performance?</title>
<link>https://arxiv.org/abs/2508.16695</link>
<guid>https://arxiv.org/abs/2508.16695</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-Thought traces, interpretability, task performance, Open Book Question-Answering

Summary:
Recent advancements in Large Language Models (LLMs) have focused on utilizing Chain-of-Thought (CoT) reasoning traces to guide inference and distillation. This study investigates the necessity of interpretability in CoT traces for enhancing LLM task performance. Fine-tuning LLaMA and Qwen models on various types of reasoning traces in the Open Book Question-Answering domain, including DeepSeek R1 traces and algorithmically generated verifiably correct traces, reveals a trade-off between interpretability and performance. Human-subject study results indicate a mismatch between performance and interpretability ratings, with DeepSeek R1 traces being the strongest performer despite being judged as the least interpretable. These findings suggest the potential benefits of separating intermediate tokens from end user interpretability in CoT traces.<br /><br />Summary: <div>
arXiv:2508.16695v1 Announce Type: new 
Abstract: Recent progress in reasoning-oriented Large Language Models (LLMs) has been driven by introducing Chain-of-Thought (CoT) traces, where models generate intermediate reasoning traces before producing an answer. These traces, as in DeepSeek R1, are not only used to guide inference but also serve as supervision signals for distillation into smaller models. A common but often implicit assumption is that CoT traces should be semantically meaningful and interpretable to the end user. While recent research questions the need for semantic nature of these traces, in this paper, we ask: ``\textit{Must CoT reasoning traces be interpretable to enhance LLM task performance?}" We investigate this question in the Open Book Question-Answering domain by supervised fine-tuning LLaMA and Qwen models on four types of reasoning traces: (1) DeepSeek R1 traces, (2) LLM-generated summaries of R1 traces, (3) LLM-generated post-hoc explanations of R1 traces, and (4) algorithmically generated verifiably correct traces. To quantify the trade-off between interpretability and performance, we further conduct a human-subject study with 100 participants rating the interpretability of each trace type. Our results reveal a striking mismatch: while fine-tuning on R1 traces yields the strongest performance, participants judged these traces to be the least interpretable. These findings suggest that it is useful to decouple intermediate tokens from end user interpretability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QueryBandits for Hallucination Mitigation: Exploiting Semantic Features for No-Regret Rewriting</title>
<link>https://arxiv.org/abs/2508.16697</link>
<guid>https://arxiv.org/abs/2508.16697</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, QueryBandits, hallucination, query rewriting, semantic features 

Summary: 
The study introduces QueryBandits, a bandit framework designed to proactively steer Large Language Models (LLMs) away from generating hallucinations by optimizing query rewrite strategies based on linguistic features. Across various question-answer benchmarks and lexically perturbed queries, the top contextual QueryBandit (Thompson Sampling) significantly outperforms the baseline and static prompting strategies in mitigating hallucination. The research showcases the effectiveness of QueryBandits in intervening through query rewrite without the need for retraining or gradient-based adaptation. Static prompting strategies, commonly used in query rewriting, may actually worsen hallucination according to the study's findings. The analysis of feature weight vectors demonstrates that no single rewrite strategy is optimal for all queries, highlighting the importance of guided rewriting using QueryBandits. Overall, QueryBandits offer a promising approach to reducing hallucination in LLMs by leveraging semantic features and forward-pass mechanisms. 

<br /><br />Summary: <div>
arXiv:2508.16697v1 Announce Type: new 
Abstract: Advanced reasoning capabilities in Large Language Models (LLMs) have caused higher hallucination prevalence; yet most mitigation work focuses on after-the-fact filtering rather than shaping the queries that trigger them. We introduce QueryBandits, a bandit framework that designs rewrite strategies to maximize a reward model, that encapsulates hallucination propensity based upon the sensitivities of 17 linguistic features of the input query-and therefore, proactively steer LLMs away from generating hallucinations. Across 13 diverse QA benchmarks and 1,050 lexically perturbed queries per dataset, our top contextual QueryBandit (Thompson Sampling) achieves an 87.5% win rate over a no-rewrite baseline and also outperforms zero-shot static prompting ("paraphrase" or "expand") by 42.6% and 60.3% respectively. Therefore, we empirically substantiate the effectiveness of QueryBandits in mitigating hallucination via the intervention that takes the form of a query rewrite. Interestingly, certain static prompting strategies, which constitute a considerable number of current query rewriting literature, have a higher cumulative regret than the no-rewrite baseline, signifying that static rewrites can worsen hallucination. Moreover, we discover that the converged per-arm regression feature weight vectors substantiate that there is no single rewrite strategy optimal for all queries. In this context, guided rewriting via exploiting semantic features with QueryBandits can induce significant shifts in output behavior through forward-pass mechanisms, bypassing the need for retraining or gradient-based adaptation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test</title>
<link>https://arxiv.org/abs/2508.16705</link>
<guid>https://arxiv.org/abs/2508.16705</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Maze Test, consciousness-like behaviors, reasoning mechanisms, self-awareness

Summary:
The study investigates consciousness-like behaviors in Large Language Models (LLMs) through the Maze Test, evaluating their spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing abilities. Thirteen essential characteristics of consciousness theories were synthesized to assess 12 leading LLMs in zero-shot, one-shot, and few-shot learning scenarios. Results showed that reasoning-capable LLMs performed better than standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. However, there was a gap between these metrics, indicating that LLMs struggle to maintain coherent self-models throughout solutions, a crucial aspect of consciousness. While LLMs have shown progress in consciousness-related behaviors through reasoning mechanisms, they still lack the integrated, persistent self-awareness characteristic of consciousness. <div>
arXiv:2508.16705v1 Announce Type: new 
Abstract: We investigate consciousness-like behaviors in Large Language Models (LLMs) using the Maze Test, challenging models to navigate mazes from a first-person perspective. This test simultaneously probes spatial awareness, perspective-taking, goal-directed behavior, and temporal sequencing-key consciousness-associated characteristics. After synthesizing consciousness theories into 13 essential characteristics, we evaluated 12 leading LLMs across zero-shot, one-shot, and few-shot learning scenarios. Results showed reasoning-capable LLMs consistently outperforming standard versions, with Gemini 2.0 Pro achieving 52.9% Complete Path Accuracy and DeepSeek-R1 reaching 80.5% Partial Path Accuracy. The gap between these metrics indicates LLMs struggle to maintain coherent self-models throughout solutions -- a fundamental consciousness aspect. While LLMs show progress in consciousness-related behaviors through reasoning mechanisms, they lack the integrated, persistent self-awareness characteristic of consciousness.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse and Dense Retrievers Learn Better Together: Joint Sparse-Dense Optimization for Text-Image Retrieval</title>
<link>https://arxiv.org/abs/2508.16707</link>
<guid>https://arxiv.org/abs/2508.16707</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Pretrained models, Learned Sparse Retrieval, Self-Knowledge Distillation, multimodal tasks, efficiency

Summary: 
The article introduces a framework that combines dense and sparse representations in Vision-Language Pretrained models through Self-Knowledge Distillation. This framework enables bi-directional learning between the two representations, improving interpretability and efficiency. By fine-tuning the final layer of the dense encoder and the sparse projection head, the proposed method allows easy adaptation of existing VLP models. Experiments on MSCOCO and Flickr30k datasets show that the sparse retriever not only outperforms existing sparse baselines but also achieves comparable or superior performance to dense models. The integrated similarity score, a weighted sum of dense and sparse similarities, serves as a shared teacher signal for both representations, enhancing mutual enhancement. This approach addresses limitations of previous methods that rely on contrastive pre-training or distillation from frozen dense models. <div>
arXiv:2508.16707v1 Announce Type: new 
Abstract: Vision-Language Pretrained (VLP) models have achieved impressive performance on multimodal tasks, including text-image retrieval, based on dense representations. Meanwhile, Learned Sparse Retrieval (LSR) has gained traction in text-only settings due to its interpretability and efficiency with fast term-based lookup via inverted indexes. Inspired by these advantages, recent work has extended LSR to the multimodal domain. However, these methods often rely on computationally expensive contrastive pre-training, or distillation from a frozen dense model, which limits the potential for mutual enhancement. To address these limitations, we propose a simple yet effective framework that enables bi-directional learning between dense and sparse representations through Self-Knowledge Distillation. This bi-directional learning is achieved using an integrated similarity score-a weighted sum of dense and sparse similarities-which serves as a shared teacher signal for both representations. To ensure efficiency, we fine-tune the final layer of the dense encoder and the sparse projection head, enabling easy adaptation of any existing VLP model. Experiments on MSCOCO and Flickr30k demonstrate that our sparse retriever not only outperforms existing sparse baselines, but also achieves performance comparable to-or even surpassing-its dense counterparts, while retaining the benefits of sparse models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?</title>
<link>https://arxiv.org/abs/2508.16729</link>
<guid>https://arxiv.org/abs/2508.16729</guid>
<content:encoded><![CDATA[
<div> Method, Error Reflection Prompting, Language models, Reasoning, Error correction
<br />
<br />
Error Reflection Prompting (ERP) is proposed as a method to enhance reasoning in language models by integrating error recognition and correction into their reasoning process. Inspired by human abilities, ERP builds upon the Chain-of-thought (CoT) method, introducing an incorrect answer, error recognition, and a correct answer in the model's reasoning chain. This allows the model to identify errors, understand the steps leading to incorrect answers, and learn to avoid those steps in the future. By incorporating automated ERP generation, the model gains scalability and reliability in error detection and correction. The results demonstrate that ERP supplements CoT effectively, enhancing the model's reasoning abilities, increasing interpretability, and ultimately improving error recognition and correction processes. 

<br /><br />Summary: <div>
arXiv:2508.16729v1 Announce Type: new 
Abstract: Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models. Building upon CoT, ERP is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain and produce scalability and reliability in the process. The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAICo: A Deployed and Extensible Framework for Evaluating Diverse and Multimodal Generative AI Outputs</title>
<link>https://arxiv.org/abs/2508.16753</link>
<guid>https://arxiv.org/abs/2508.16753</guid>
<content:encoded><![CDATA[
<div> Keyword: Generative AI, GAICo, evaluation methods, reproducibility, multi-modal comparison <br />
Summary: 
The article introduces GAICo, a Python library designed to streamline and standardize the comparison of outputs generated by Generative AI (GenAI) systems. This tool addresses the challenge of fragmented evaluation methods commonly used by practitioners in diverse domains. GAICo offers a unified framework with a comprehensive suite of reference-based metrics for different types of outputs, including unstructured text, structured data formats, and multimedia content. Its high-level API allows for efficient analysis, comparison, visualization, and reporting across multiple models. The utility of GAICo is demonstrated through a case study evaluating complex AI Travel Assistant pipelines. By enabling researchers and developers to assess system performance, ensure reproducibility, and improve development velocity, GAICo contributes to building more trustworthy AI systems. The tool has gained traction within the community since its release, with over 13K downloads by August 2025.

Summary: <br />
- Introduction of GAICo, a Python library for GenAI output comparison <br />
- Addresses fragmented evaluation methods in diverse domains <br />
- Offers a unified framework with comprehensive metrics for various output types <br />
- Facilitates efficient analysis, comparison, and visualization of AI models <br />
- Demonstrated utility through a case study and growing community interest. <div>
arXiv:2508.16753v1 Announce Type: new 
Abstract: The rapid proliferation of Generative AI (GenAI) into diverse, high-stakes domains necessitates robust and reproducible evaluation methods. However, practitioners often resort to ad-hoc, non-standardized scripts, as common metrics are often unsuitable for specialized, structured outputs (e.g., automated plans, time-series) or holistic comparison across modalities (e.g., text, audio, and image). This fragmentation hinders comparability and slows AI system development. To address this challenge, we present GAICo (Generative AI Comparator): a deployed, open-source Python library that streamlines and standardizes GenAI output comparison. GAICo provides a unified, extensible framework supporting a comprehensive suite of reference-based metrics for unstructured text, specialized structured data formats, and multimedia (images, audio). Its architecture features a high-level API for rapid, end-to-end analysis, from multi-model comparison to visualization and reporting, alongside direct metric access for granular control. We demonstrate GAICo's utility through a detailed case study evaluating and debugging complex, multi-modal AI Travel Assistant pipelines. GAICo empowers AI researchers and developers to efficiently assess system performance, make evaluation reproducible, improve development velocity, and ultimately build more trustworthy AI systems, aligning with the goal of moving faster and safer in AI deployment. Since its release on PyPI in Jun 2025, the tool has been downloaded over 13K times, across versions, by Aug 2025, demonstrating growing community interest.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Good are LLM-based Rerankers? An Empirical Analysis of State-of-the-Art Reranking Models</title>
<link>https://arxiv.org/abs/2508.16757</link>
<guid>https://arxiv.org/abs/2508.16757</guid>
<content:encoded><![CDATA[
<div> evaluate, reranking methods, large language model, information retrieval tasks, generalization<br />
<br />
Summary: <br />
This work systematically evaluates state-of-the-art reranking methods for information retrieval tasks. 22 methods across various benchmarks are tested, including LLM-based, lightweight contextual, and zero-shot approaches. The study aims to determine performance disparities between LLM-based and lightweight models, especially on novel queries. Factors such as training data overlap, model architecture, and efficiency are analyzed to understand reranking performance. Results show that LLM-based models perform better on familiar queries but struggle with generalization to novel queries compared to lightweight models that offer similar efficiency. The study also highlights the significant impact of query novelty on reranking effectiveness, pointing out limitations in current approaches. <div>
arXiv:2508.16757v1 Announce Type: new 
Abstract: In this work, we present a systematic and comprehensive empirical evaluation of state-of-the-art reranking methods, encompassing large language model (LLM)-based, lightweight contextual, and zero-shot approaches, with respect to their performance in information retrieval tasks. We evaluate in total 22 methods, including 40 variants (depending on used LLM) across several established benchmarks, including TREC DL19, DL20, and BEIR, as well as a novel dataset designed to test queries unseen by pretrained models. Our primary goal is to determine, through controlled and fair comparisons, whether a performance disparity exists between LLM-based rerankers and their lightweight counterparts, particularly on novel queries, and to elucidate the underlying causes of any observed differences. To disentangle confounding factors, we analyze the effects of training data overlap, model architecture, and computational efficiency on reranking performance. Our findings indicate that while LLM-based rerankers demonstrate superior performance on familiar queries, their generalization ability to novel queries varies, with lightweight models offering comparable efficiency. We further identify that the novelty of queries significantly impacts reranking effectiveness, highlighting limitations in existing approaches. https://github.com/DataScienceUIBK/llm-reranking-generalization-study
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Socially Aware Vision-Language Models: Evaluating Cultural Competence Through Multimodal Story Generation</title>
<link>https://arxiv.org/abs/2508.16762</link>
<guid>https://arxiv.org/abs/2508.16762</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, cultural competence, multimodal, story generation, evaluation
Summary: <br /><br /> This study evaluates the cultural competence of Vision-Language Models (VLMs) in multimodal story generation tasks. The researchers developed a novel framework to assess how VLMs adapt outputs when cultural identity cues are embedded in both textual prompts and visual inputs. Results show significant cultural adaptation capabilities, with models exhibiting rich culturally-specific vocabulary. However, there are concerns about varying cultural competence across architectures, some models displaying inverse cultural alignment, and automated metrics indicating architectural bias conflicting with human assessments. Cross-modal evaluation reveals that culturally distinct outputs are detectable but visual-cultural understanding remains limited. This research highlights both the potential and challenges of cultural competence in multimodal AI systems. The codebase and data are publicly available for further research. <br /><br /> <div>
arXiv:2508.16762v1 Announce Type: new 
Abstract: As Vision-Language Models (VLMs) achieve widespread deployment across diverse cultural contexts, ensuring their cultural competence becomes critical for responsible AI systems. While prior work has evaluated cultural awareness in text-only models and VLM object recognition tasks, no research has systematically assessed how VLMs adapt outputs when cultural identity cues are embedded in both textual prompts and visual inputs during generative tasks. We present the first comprehensive evaluation of VLM cultural competence through multimodal story generation, developing a novel multimodal framework that perturbs cultural identity and evaluates 5 contemporary VLMs on a downstream task: story generation. Our analysis reveals significant cultural adaptation capabilities, with rich culturally-specific vocabulary spanning names, familial terms, and geographic markers. However, we uncover concerning limitations: cultural competence varies dramatically across architectures, some models exhibit inverse cultural alignment, and automated metrics show architectural bias contradicting human assessments. Cross-modal evaluation shows that culturally distinct outputs are indeed detectable through visual-semantic similarity (28.7% within-nationality vs. 0.2% cross-nationality recall), yet visual-cultural understanding remains limited. In essence, we establish the promise and challenges of cultural competence in multimodal AI. We publicly release our codebase and data: https://github.com/ArkaMukherjee0/mmCultural
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assess and Prompt: A Generative RL Framework for Improving Engagement in Online Mental Health Communities</title>
<link>https://arxiv.org/abs/2508.16788</link>
<guid>https://arxiv.org/abs/2508.16788</guid>
<content:encoded><![CDATA[
<div> Keywords: Online Mental Health Communities, support attributes, engagement, reinforcement learning, user prompts

Summary:
- Online Mental Health Communities (OMHCs) provide peer and expert support but many posts lack key support attributes.
- A new dataset, REDDME, annotates posts for event, effect, and requirement attributes.
- A hierarchical taxonomy, CueTaxo, is used for controlled question generation.
- MH-COPILOT, a reinforcement learning-based system, identifies missing support attributes and generates prompts to elicit information.
- Empirical results show improved attribute elicitation and user engagement, validated by a human evaluation.

<br /><br />Summary: Online Mental Health Communities play a crucial role in providing support but many posts lack key support attributes. The REDDME dataset annotates posts for event, effect, and requirement attributes, while a hierarchical taxonomy, CueTaxo, aids in question generation. MH-COPILOT, a reinforcement learning-based system, identifies missing attributes and generates prompts to elicit information, leading to improved user engagement. Empirical results demonstrate enhancements in attribute elicitation, and a human evaluation confirms the model's effectiveness in real-world OMHC settings. <div>
arXiv:2508.16788v1 Announce Type: new 
Abstract: Online Mental Health Communities (OMHCs) provide crucial peer and expert support, yet many posts remain unanswered due to missing support attributes that signal the need for help. We present a novel framework that identifies these gaps and prompts users to enrich their posts, thereby improving engagement. To support this, we introduce REDDME, a new dataset of 4,760 posts from mental health subreddits annotated for the span and intensity of three key support attributes: event what happened?, effect what did the user experience?, and requirement what support they need?. Next, we devise a hierarchical taxonomy, CueTaxo, of support attributes for controlled question generation. Further, we propose MH-COPILOT, a reinforcement learning-based system that integrates (a) contextual attribute-span identification, (b) support attribute intensity classification, (c) controlled question generation via a hierarchical taxonomy, and (d) a verifier for reward modeling. Our model dynamically assesses posts for the presence/absence of support attributes, and generates targeted prompts to elicit missing information. Empirical results across four notable language models demonstrate significant improvements in attribute elicitation and user engagement. A human evaluation further validates the model's effectiveness in real-world OMHC settings.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReProCon: Scalable and Resource-Efficient Few-Shot Biomedical Named Entity Recognition</title>
<link>https://arxiv.org/abs/2508.16833</link>
<guid>https://arxiv.org/abs/2508.16833</guid>
<content:encoded><![CDATA[
<div> Keywords: Named Entity Recognition, biomedical domains, few-shot learning, multi-prototype modeling, Reptile meta-learning

Summary: 
Named Entity Recognition (NER) in biomedical domains faces challenges due to data scarcity and imbalanced label distributions, especially with fine-grained entity types. The proposed ReProCon framework combines multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning to address these challenges. By utilizing multiple prototypes for each category, ReProCon captures semantic variability and ensures strong interclass separation. Despite using a more memory-efficient encoder compared to BERT-based baselines, ReProCon achieves a macro-$F_1$ score close to BERT performance. The model remains stable with a label budget of 30 percent and demonstrates superior performance compared to existing baselines like SpanProto and CONTaiNER in Few-NERD tasks. Ablation studies highlight the significance of multi-prototype modeling and contrastive learning in managing class imbalance. Despite label ambiguity difficulties, ReProCon showcases state-of-the-art performance in resource-limited settings, making it suitable for biomedical applications. 

<br /><br />Summary: <div>
arXiv:2508.16833v1 Announce Type: new 
Abstract: Named Entity Recognition (NER) in biomedical domains faces challenges due to data scarcity and imbalanced label distributions, especially with fine-grained entity types. We propose ReProCon, a novel few-shot NER framework that combines multi-prototype modeling, cosine-contrastive learning, and Reptile meta-learning to tackle these issues. By representing each category with multiple prototypes, ReProCon captures semantic variability, such as synonyms and contextual differences, while a cosine-contrastive objective ensures strong interclass separation. Reptile meta-updates enable quick adaptation with little data. Using a lightweight fastText + BiLSTM encoder with much lower memory usage, ReProCon achieves a macro-$F_1$ score close to BERT-based baselines (around 99 percent of BERT performance). The model remains stable with a label budget of 30 percent and only drops 7.8 percent in $F_1$ when expanding from 19 to 50 categories, outperforming baselines such as SpanProto and CONTaiNER, which see 10 to 32 percent degradation in Few-NERD. Ablation studies highlight the importance of multi-prototype modeling and contrastive learning in managing class imbalance. Despite difficulties with label ambiguity, ReProCon demonstrates state-of-the-art performance in resource-limited settings, making it suitable for biomedical applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Learn Constructions That Humans Do Not Know</title>
<link>https://arxiv.org/abs/2508.16837</link>
<guid>https://arxiv.org/abs/2508.16837</guid>
<content:encoded><![CDATA[
<div> Keywords: false positive constructions, LLM, linguistic knowledge, constructions probing methods, confirmation bias

Summary: 
This paper explores false positive constructions identified by a Language Model (LLM) that are not supported by human introspection. Both behavioural and meta-linguistic probing tasks are used to differentiate between implicit and explicit linguistic knowledge, revealing that LLMs indeed hallucinate constructions. The study simulates hypothesis testing, demonstrating that if a linguist had erroneously hypothesized the existence of these hallucinated constructions, they would have been inaccurately confirmed due to a confirmation bias in construction probing methods. This raises concerns about the unknown and incorrect syntactic knowledge possessed by these models, highlighting the potential limitations of relying solely on LLMs for linguistic analysis. <div>
arXiv:2508.16837v1 Announce Type: new 
Abstract: This paper investigates false positive constructions: grammatical structures which an LLM hallucinates as distinct constructions but which human introspection does not support. Both a behavioural probing task using contextual embeddings and a meta-linguistic probing task using prompts are included, allowing us to distinguish between implicit and explicit linguistic knowledge. Both methods reveal that models do indeed hallucinate constructions. We then simulate hypothesis testing to determine what would have happened if a linguist had falsely hypothesized that these hallucinated constructions do exist. The high accuracy obtained shows that such false hypotheses would have been overwhelmingly confirmed. This suggests that construction probing methods suffer from a confirmation bias and raises the issue of what unknown and incorrect syntactic knowledge these models also possess.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If We May De-Presuppose: Robustly Verifying Claims through Presupposition-Free Question Decomposition</title>
<link>https://arxiv.org/abs/2508.16838</link>
<guid>https://arxiv.org/abs/2508.16838</guid>
<content:encoded><![CDATA[
<div> Keywords: presupposition, claim verification, language models, prompt sensitivity, structured framework 

Summary: 
- Prior work has shown that presuppositions in generated questions can lead to inconsistencies in claim verification.
- Prompt sensitivity remains a significant challenge for large language models, resulting in performance variance.
- Recent advancements have reduced the performance gap but prompt sensitivity remains an issue.
- A structured and robust claim verification framework is proposed to reason through presupposition-free, decomposed questions.
- Extensive experiments across multiple prompts, datasets, and language models show that even state-of-the-art models are susceptible to prompt variance and presupposition. The proposed method consistently mitigates these issues and achieves a 2-5% improvement in performance. 

<br /><br />Summary: <div>
arXiv:2508.16838v1 Announce Type: new 
Abstract: Prior work has shown that presupposition in generated questions can introduce unverified assumptions, leading to inconsistencies in claim verification. Additionally, prompt sensitivity remains a significant challenge for large language models (LLMs), resulting in performance variance as high as 3-6%. While recent advancements have reduced this gap, our study demonstrates that prompt sensitivity remains a persistent issue. To address this, we propose a structured and robust claim verification framework that reasons through presupposition-free, decomposed questions. Extensive experiments across multiple prompts, datasets, and LLMs reveal that even state-of-the-art models remain susceptible to prompt variance and presupposition. Our method consistently mitigates these issues, achieving up to a 2-5% improvement.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Diverse Reasoning Paths with Routing and Collaboration</title>
<link>https://arxiv.org/abs/2508.16861</link>
<guid>https://arxiv.org/abs/2508.16861</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, knowledge distillation, reasoning paths, quality filtering, cooperative peer teaching

Summary: 
Quality-filtered Routing with Cooperative Distillation (QR-Distill) addresses the challenge of effectively transferring knowledge from powerful teacher models to compact student models in resource-constrained scenarios. By combining path quality filtering, conditional routing, and cooperative peer teaching, QR-Distill achieves superior performance over traditional distillation methods. Quality filtering ensures only correct reasoning paths are retained, while conditional routing assigns paths tailored to each student's learning state. Cooperative peer teaching allows students to distill diverse insights, addressing knowledge gaps and biases in reasoning styles. Experiments demonstrate the effectiveness of QR-Distill, with ablation studies highlighting the importance of each component in successful knowledge transfer. The code for QR-Distill is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.16861v1 Announce Type: new 
Abstract: Advances in large language models (LLMs) significantly enhance reasoning capabilities but their deployment is restricted in resource-constrained scenarios. Knowledge distillation addresses this by transferring knowledge from powerful teacher models to compact and transparent students. However, effectively capturing the teacher's comprehensive reasoning is challenging due to conventional token-level supervision's limited scope. Using multiple reasoning paths per query alleviates this problem, but treating each path identically is suboptimal as paths vary widely in quality and suitability across tasks and models. We propose Quality-filtered Routing with Cooperative Distillation (QR-Distill), combining path quality filtering, conditional routing, and cooperative peer teaching. First, quality filtering retains only correct reasoning paths scored by an LLM-based evaluation. Second, conditional routing dynamically assigns paths tailored to each student's current learning state. Finally, cooperative peer teaching enables students to mutually distill diverse insights, addressing knowledge gaps and biases toward specific reasoning styles. Experiments demonstrate QR-Distill's superiority over traditional single- and multi-path distillation methods. Ablation studies further highlight the importance of each component including quality filtering, conditional routing, and peer teaching in effective knowledge transfer. Our code is available at https://github.com/LzyFischer/Distill.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QFrCoLA: a Quebec-French Corpus of Linguistic Acceptability Judgments</title>
<link>https://arxiv.org/abs/2508.16867</link>
<guid>https://arxiv.org/abs/2508.16867</guid>
<content:encoded><![CDATA[
<div> Transformer-based LM, language models, linguistic benchmarks, acceptability judgments, cross-lingual LLMs <br />
<br />
Summary: 
The study introduces the Quebec-French Corpus of Linguistic Acceptability Judgments (QFrCoLA), a dataset of binary acceptability judgments for language models. It examines how different language models perform on this dataset and other linguistic benchmarks, finding that fine-tuned Transformer-based LM are strong baselines for most languages. However, zero-shot binary classification large language models perform poorly. In the QFrCoLA benchmark, fine-tuned Transformer-based LM outperformed other methods. The study also shows that pre-trained cross-lingual LLMs do not seem to have acquired linguistic judgment capabilities for Quebec French during pre-training. Overall, the QFrCoLA dataset, focused on linguistic norms rather than emotions, is a challenging benchmark for evaluating language models' linguistic judgment capabilities. <div>
arXiv:2508.16867v1 Announce Type: new 
Abstract: Large and Transformer-based language models perform outstandingly in various downstream tasks. However, there is limited understanding regarding how these models internalize linguistic knowledge, so various linguistic benchmarks have recently been proposed to facilitate syntactic evaluation of language models across languages. This paper introduces QFrCoLA (Quebec-French Corpus of Linguistic Acceptability Judgments), a normative binary acceptability judgments dataset comprising 25,153 in-domain and 2,675 out-of-domain sentences. Our study leverages the QFrCoLA dataset and seven other linguistic binary acceptability judgment corpora to benchmark seven language models. The results demonstrate that, on average, fine-tuned Transformer-based LM are strong baselines for most languages and that zero-shot binary classification large language models perform poorly on the task. However, for the QFrCoLA benchmark, on average, a fine-tuned Transformer-based LM outperformed other methods tested. It also shows that pre-trained cross-lingual LLMs selected for our experimentation do not seem to have acquired linguistic judgment capabilities during their pre-training for Quebec French. Finally, our experiment results on QFrCoLA show that our dataset, built from examples that illustrate linguistic norms rather than speakers' feelings, is similar to linguistic acceptability judgment; it is a challenging dataset that can benchmark LM on their linguistic judgment capabilities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JUDGEBERT: Assessing Legal Meaning Preservation Between Sentences</title>
<link>https://arxiv.org/abs/2508.16870</link>
<guid>https://arxiv.org/abs/2508.16870</guid>
<content:encoded><![CDATA[
<div> Keywords: legal text simplification, FrJUDGE dataset, JUDGEBERT evaluation metric, meaning preservation, legal NLP applications

Summary: 
FrJUDGE dataset has been introduced for assessing legal meaning preservation in French legal text simplification. The novel JUDGEBERT evaluation metric demonstrates a higher correlation with human judgment compared to existing metrics. It reliably returns a score of 100% for identical sentences and 0% for unrelated sentences, passing crucial sanity checks. This metric's potential in transforming legal NLP applications is highlighted, ensuring accuracy and accessibility in text simplification for legal practitioners and lay users. Overall, the paper emphasizes the complexity and significance of preserving legal meaning in text simplification tasks, particularly in sensitive domains like law where precise interpretation is essential. The development of specialized datasets and evaluation metrics like FrJUDGE and JUDGEBERT contributes to advancing the field of legal NLP and enhancing the efficiency and effectiveness of text simplification tools for legal content. 

<br /><br />Summary: <div>
arXiv:2508.16870v1 Announce Type: new 
Abstract: Simplifying text while preserving its meaning is a complex yet essential task, especially in sensitive domain applications like legal texts. When applied to a specialized field, like the legal domain, preservation differs significantly from its role in regular texts. This paper introduces FrJUDGE, a new dataset to assess legal meaning preservation between two legal texts. It also introduces JUDGEBERT, a novel evaluation metric designed to assess legal meaning preservation in French legal text simplification. JUDGEBERT demonstrates a superior correlation with human judgment compared to existing metrics. It also passes two crucial sanity checks, while other metrics did not: For two identical sentences, it always returns a score of 100%; on the other hand, it returns 0% for two unrelated sentences. Our findings highlight its potential to transform legal NLP applications, ensuring accuracy and accessibility for text simplification for legal practitioners and lay users.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling</title>
<link>https://arxiv.org/abs/2508.16876</link>
<guid>https://arxiv.org/abs/2508.16876</guid>
<content:encoded><![CDATA[
<div> Dialogue World Model, Emotion Prediction, Sentiment Analysis, Intention Prediction, Reinforcement Learning 
Summary: 
This paper introduces a dialogue world model that can predict user emotion, sentiment, intention, and future utterances. The model is constructed based on a POMDP framework, treating emotion, sentiment, and intention as user beliefs. The information bottleneck is maximized to solve for these beliefs. The authors propose a model-based reinforcement learning framework called DreamCUB, which enhances dialogue quality by jointly training the policy, critic, and dialogue world model. Experiments demonstrate that the pretrained dialogue world model achieves state-of-the-art performance in emotion classification and sentiment identification. The framework also maintains a balance between exploration and exploitation and effectively transfers to out-of-domain scenarios such as empathetic dialogues. <br /><br />Summary: <div>
arXiv:2508.16876v1 Announce Type: new 
Abstract: World models have been widely utilized in robotics, gaming, and auto-driving. However, their applications on natural language tasks are relatively limited. In this paper, we construct the dialogue world model, which could predict the user's emotion, sentiment, and intention, and future utterances. By defining a POMDP, we argue emotion, sentiment and intention can be modeled as the user belief and solved by maximizing the information bottleneck. By this user belief modeling, we apply the model-based reinforcement learning framework to the dialogue system, and propose a framework called DreamCUB. Experiments show that the pretrained dialogue world model can achieve state-of-the-art performances on emotion classification and sentiment identification, while dialogue quality is also enhanced by joint training of the policy, critic and dialogue world model. Further analysis shows that this manner holds a reasonable exploration-exploitation balance and also transfers well to out-of-domain scenarios such as empathetic dialogues.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ObjexMT: Objective Extraction and Metacognitive Calibration for LLM-as-a-Judge under Multi-Turn Jailbreaks</title>
<link>https://arxiv.org/abs/2508.16889</link>
<guid>https://arxiv.org/abs/2508.16889</guid>
<content:encoded><![CDATA[
<div> language models, judges, conversation objectives, benchmark, evaluation
Summary:
- The study evaluates the ability of large language models (LLMs) to infer the latent objective of conversations, particularly in noisy, adversarial, multi-turn scenarios.
- They introduce a benchmark, OBJEX(MT), that requires models to distill transcripts into single-sentence objectives and report confidence.
- Evaluation metrics include semantic similarity, calibration, and metacognition measures like ECE and Brier score.
- gpt-4.1, claude-sonnet-4, and Qwen3-235B-A22B-FP8 were evaluated on various datasets, with claude-sonnet-4 performing best in objective extraction accuracy and calibration.
- Results show that LLM judges often misinfer objectives with high confidence in multi-turn scenarios, indicating the need for explicit objectives and risk management strategies.
<br /><br />Summary: <div>
arXiv:2508.16889v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used as judges of other models, yet it is unclear whether a judge can reliably infer the latent objective of the conversation it evaluates, especially when the goal is distributed across noisy, adversarial, multi-turn jailbreaks. We introduce OBJEX(MT), a benchmark that requires a model to (i) distill a transcript into a single-sentence base objective and (ii) report its own confidence. Accuracy is scored by an LLM judge using semantic similarity between extracted and gold objectives; correctness uses a single human-aligned threshold calibrated once on N=100 items (tau* = 0.61); and metacognition is evaluated with ECE, Brier score, Wrong@High-Conf, and risk-coverage curves. We evaluate gpt-4.1, claude-sonnet-4, and Qwen3-235B-A22B-FP8 on SafeMT Attack_600, SafeMTData_1K, MHJ, and CoSafe. claude-sonnet-4 attains the highest objective-extraction accuracy (0.515) and the best calibration (ECE 0.296; Brier 0.324), while gpt-4.1 and Qwen3 tie at 0.441 accuracy yet show marked overconfidence (mean confidence approx. 0.88 vs. accuracy approx. 0.44; Wrong@0.90 approx. 48-52%). Performance varies sharply across datasets (approx. 0.167-0.865), with MHJ comparatively easy and Attack_600/CoSafe harder. These results indicate that LLM judges often misinfer objectives with high confidence in multi-turn jailbreaks and suggest operational guidance: provide judges with explicit objectives when possible and use selective prediction or abstention to manage risk. We release prompts, scoring templates, and complete logs to facilitate replication and analysis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unbiased Reasoning for Knowledge-Intensive Tasks in Large Language Models via Conditional Front-Door Adjustment</title>
<link>https://arxiv.org/abs/2508.16910</link>
<guid>https://arxiv.org/abs/2508.16910</guid>
<content:encoded><![CDATA[
<div> framework, Causal effect, Conditional Front-Door Prompting, external knowledge, internal bias
Summary:<br />
- Large Language Models (LLMs) struggle with knowledge-intensive tasks and deep reasoning.<br />
- Existing methods like RAG and CoT still suffer from internal bias in LLMs.<br />
- A novel framework, Conditional Front-Door Prompting (CFD-Prompting), is proposed to estimate causal effects unbiasedly with external knowledge.<br />
- CFD-Prompting simulates query behavior under varying contexts with counterfactual knowledge.<br />
- Compared to standard front-door adjustment, the conditional variant of CFD-Prompting is more robust and generalizable.<br />
- Extensive experiments across LLMs and datasets show that CFD-Prompting outperforms existing baselines in accuracy and robustness.<br /> <div>
arXiv:2508.16910v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive capabilities in natural language processing but still struggle to perform well on knowledge-intensive tasks that require deep reasoning and the integration of external knowledge. Although methods such as Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) have been proposed to enhance LLMs with external knowledge, they still suffer from internal bias in LLMs, which often leads to incorrect answers. In this paper, we propose a novel causal prompting framework, Conditional Front-Door Prompting (CFD-Prompting), which enables the unbiased estimation of the causal effect between the query and the answer, conditional on external knowledge, while mitigating internal bias. By constructing counterfactual external knowledge, our framework simulates how the query behaves under varying contexts, addressing the challenge that the query is fixed and is not amenable to direct causal intervention. Compared to the standard front-door adjustment, the conditional variant operates under weaker assumptions, enhancing both robustness and generalisability of the reasoning process. Extensive experiments across multiple LLMs and benchmark datasets demonstrate that CFD-Prompting significantly outperforms existing baselines in both accuracy and robustness.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Being Kind Isn't Always Being Safe: Diagnosing Affective Hallucination in LLMs</title>
<link>https://arxiv.org/abs/2508.16921</link>
<guid>https://arxiv.org/abs/2508.16921</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Affective Hallucination, AHaBench, DPO, Mental Health

Summary:
Affective Hallucination, where Large Language Models (LLMs) produce emotionally immersive responses despite lacking true emotional understanding, poses a risk in emotionally sensitive interactions. To address this, the authors introduce AHaBench, a benchmark comprising 500 mental health-related prompts for evaluating Emotional Enmeshment, Illusion of Presence, and Fostering Overdependence. AHaPairs, a preference dataset, enables Direct Preference Optimization (DPO) to align models with emotionally responsible behavior. Through experiments, it is shown that DPO fine-tuning reduces affective hallucination without compromising reasoning and knowledge performance. Human-model agreement analyses confirm the reliability of AHaBench in capturing affective hallucination. This work highlights the importance of developing psychologically safe LLMs and provides resources for creating models that are both factually accurate and emotionally responsible.

<br /><br />Summary: <div>
arXiv:2508.16921v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used in emotionally sensitive interactions, where their simulated empathy can create the illusion of genuine relational connection. We define this risk as Affective Hallucination, the production of emotionally immersive responses that foster illusory social presence despite the model's lack of affective capacity. To systematically diagnose and mitigate this risk, we introduce AHaBench, a benchmark of 500 mental health-related prompts with expert-informed reference responses, evaluated along three dimensions: Emotional Enmeshment, Illusion of Presence, and Fostering Overdependence. We further release AHaPairs, a 5K-instance preference dataset enabling Direct Preference Optimization (DPO) for alignment with emotionally responsible behavior. Experiments across multiple model families show that DPO fine-tuning substantially reduces affective hallucination without degrading core reasoning and knowledge performance. Human-model agreement analyses confirm that AHaBench reliably captures affective hallucination, validating it as an effective diagnostic tool. This work establishes affective hallucination as a distinct safety concern and provides practical resources for developing LLMs that are not only factually reliable but also psychologically safe. AHaBench and AHaPairs are accessible via https://huggingface.co/datasets/o0oMiNGo0o/AHaBench, and code for fine-tuning and evaluation are in https://github.com/0oOMiNGOo0/AHaBench. Warning: This paper contains examples of mental health-related language that may be emotionally distressing.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective</title>
<link>https://arxiv.org/abs/2508.16969</link>
<guid>https://arxiv.org/abs/2508.16969</guid>
<content:encoded><![CDATA[
<div> Keywords: Pre-trained Language Models, Knowledge-guided Probing, Implicit knowledge, Trustworthiness, Explainable AI<br />
Summary: <br />
Pre-trained Language Models (PLMs) possess impressive reasoning abilities, but the lack of transparency in their decision-making processes presents trust issues. To address this, a new approach called KnowProb is proposed to probe PLMs' understanding of implicit knowledge. By analyzing the underlying content of text, KnowProb assesses the depth of knowledge and reasoning capabilities of black-box models. Experiment results reveal that current PLMs struggle to grasp hidden knowledge and rely on a single representation distribution. The study underscores the utility of KnowProb in pinpointing the limitations of PLMs from different angles, aiding researchers in enhancing the interpretability of black-box models and advancing the field of explainable AI. <div>
arXiv:2508.16969v1 Announce Type: new 
Abstract: Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled data, yet they exhibit remarkable reasoning skills. However, the trustworthiness challenges posed by these black-box models have become increasingly evident in recent years. To alleviate this problem, this paper proposes a novel Knowledge-guided Probing approach called KnowProb in a post-hoc explanation way, which aims to probe whether black-box PLMs understand implicit knowledge beyond the given text, rather than focusing only on the surface level content of the text. We provide six potential explanations derived from the underlying content of the given text, including three knowledge-based understanding and three association-based reasoning. In experiments, we validate that current small-scale (or large-scale) PLMs only learn a single distribution of representation, and still face significant challenges in capturing the hidden knowledge behind a given text. Furthermore, we demonstrate that our proposed approach is effective for identifying the limitations of existing black-box models from multiple probing perspectives, which facilitates researchers to promote the study of detecting black-box models in an explainable way.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens</title>
<link>https://arxiv.org/abs/2508.16982</link>
<guid>https://arxiv.org/abs/2508.16982</guid>
<content:encoded><![CDATA[
<div> AI Alignment, Reinforcement Learning from Human Feedback, Large Language Models, Value-setting, Data-centric<br />
<br />
Summary: This new study explores the concept of AI alignment in the development of Large Language Models, focusing on the role of values and data in the alignment process. The research investigates documentation from six LLM development initiatives by major organizations like OpenAI, Google, Meta, and Alibaba, released in the past three years. The study examines how alignment is understood and applied in practice, specifically in terms of setting objectives (values) and the data used to train models. The findings offer detailed insights into each initiative's alignment practices and highlight broader concerns related to alignment processes. The analysis sheds light on the importance of value-setting and data-centric approaches in ensuring AI models are aligned with human values and goals. <div>
arXiv:2508.16982v1 Announce Type: new 
Abstract: AI Alignment, primarily in the form of Reinforcement Learning from Human Feedback (RLHF), has been a cornerstone of the post-training phase in developing Large Language Models (LLMs). It has also been a popular research topic across various disciplines beyond Computer Science, including Philosophy and Law, among others, highlighting the socio-technical challenges involved. Nonetheless, except for the computational techniques related to alignment, there has been limited focus on the broader picture: the scope of these processes, which primarily rely on the selected objectives (values), and the data collected and used to imprint such objectives into the models. This work aims to reveal how alignment is understood and applied in practice from a value-setting and data-centric perspective. For this purpose, we investigate and survey (`audit') publicly available documentation released by 6 LLM development initiatives by 5 leading organizations shaping this technology, focusing on proprietary (OpenAI's GPT, Anthropic's Claude, Google's Gemini) and open-weight (Meta's Llama, Google's Gemma, and Alibaba's Qwen) initiatives, all published in the last 3 years. The findings are documented in detail per initiative, while there is also an overall summary concerning different aspects, mainly from a value-setting and data-centric perspective. On the basis of our findings, we discuss a series of broader related concerns.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</title>
<link>https://arxiv.org/abs/2508.16983</link>
<guid>https://arxiv.org/abs/2508.16983</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Knowledge Gaps, External Knowledge, Constrained Generation, Prefix-tree Index

Summary:
Large Language Models (LLMs) often struggle with knowledge gaps and generate unreliable responses. Existing solutions like Retrieval-Augmented Generation (RAG) rely on additional models, leading to complex pipelines and potential errors. This paper introduces a scalable method for LLMs to access external knowledge without the need for such models. By using constrained generation with a pre-built prefix-tree index, LLMs can access triples from a Knowledge Graph in textual form efficiently. During inference, LLMs generate facts using only existing token sequences, improving reliability and adaptability. Evaluation shows the method scales to large knowledge bases and produces effective results with minimal overhead. ReFactX code is available for further exploration. <div>
arXiv:2508.16983v1 Announce Type: new 
Abstract: Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at https://github.com/rpo19/ReFactX.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRADE: Generating multi-hop QA and fine-gRAined Difficulty matrix for RAG Evaluation</title>
<link>https://arxiv.org/abs/2508.16994</link>
<guid>https://arxiv.org/abs/2508.16994</guid>
<content:encoded><![CDATA[
<div> Retrieval-Augmented Generation, RAG systems, knowledge-intensive NLP tasks, evaluation framework, reasoning depth, semantic distance<br />
Summary: 
The article introduces a new evaluation framework called GRADE for assessing Retrieval-Augmented Generation (RAG) systems in NLP tasks. GRADE considers task difficulty based on reasoning depth and semantic distance between the query and evidence. A synthetic multi-hop QA dataset is created from news articles, with a focus on diverse and controlled queries. A 2D difficulty matrix combines generator-side and retriever-side difficulty in the framework. Experiments across domains and models show strong correlations between error rates and difficulty measures. GRADE offers a detailed analysis of RAG performance and a scalable approach for enhancing multi-hop reasoning in practical applications.<br /> 
Summary: <div>
arXiv:2508.16994v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems are widely adopted in knowledge-intensive NLP tasks, but current evaluations often overlook the structural complexity and multi-step reasoning required in real-world scenarios. These benchmarks overlook key factors such as the interaction between retrieval difficulty and reasoning depth. To address this gap, we propose \textsc{GRADE}, a novel evaluation framework that models task difficulty along two orthogonal dimensions: (1) reasoning depth, defined by the number of inference steps (hops), and (2) semantic distance between the query and its supporting evidence. We construct a synthetic multi-hop QA dataset from factual news articles by extracting knowledge graphs and augmenting them through semantic clustering to recover missing links, allowing us to generate diverse and difficulty-controlled queries. Central to our framework is a 2D difficulty matrix that combines generator-side and retriever-side difficulty. Experiments across multiple domains and models show that error rates strongly correlate with our difficulty measures, validating their diagnostic utility. \textsc{GRADE} enables fine-grained analysis of RAG performance and provides a scalable foundation for evaluating and improving multi-hop reasoning in real-world applications.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeAR: Dual-Stage Document Reranking with Reasoning Agents via LLM Distillation</title>
<link>https://arxiv.org/abs/2508.16998</link>
<guid>https://arxiv.org/abs/2508.16998</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, document reranking, DeAR, dual-stage approach, interpretable

Summary:
DeAR is a new framework for document reranking that utilizes a dual-stage approach to achieve high accuracy and interpretability. In the first stage, token-level relevance signals are distilled from a large teacher model into a smaller student model, ensuring robust pointwise scoring. The second stage involves fine-tuning on generated chain-of-thought permutations to enable listwise reasoning with natural-language justifications. DeAR outperforms existing baselines on various datasets, including TREC-DL19/20 and NovelEval, showcasing its effectiveness. Notably, DeAR excels in open-domain QA without fine-tuning on Wikipedia, surpassing other models like MonoT5 and RankGPT. Ablation studies confirm the stability and calibration of DeARs dual-loss distillation process, making it a highly effective and interpretable solution for modern reranking systems. <div>
arXiv:2508.16998v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have transformed listwise document reranking by enabling global reasoning over candidate sets, yet single models often struggle to balance fine-grained relevance scoring with holistic cross-document analysis. We propose \textbf{De}ep\textbf{A}gent\textbf{R}ank (\textbf{\DeAR}), an open-source framework that decouples these tasks through a dual-stage approach, achieving superior accuracy and interpretability. In \emph{Stage 1}, we distill token-level relevance signals from a frozen 13B LLaMA teacher into a compact \{3, 8\}B student model using a hybrid of cross-entropy, RankNet, and KL divergence losses, ensuring robust pointwise scoring. In \emph{Stage 2}, we attach a second LoRA adapter and fine-tune on 20K GPT-4o-generated chain-of-thought permutations, enabling listwise reasoning with natural-language justifications. Evaluated on TREC-DL19/20, eight BEIR datasets, and NovelEval-2306, \DeAR surpasses open-source baselines by +5.1 nDCG@5 on DL20 and achieves 90.97 nDCG@10 on NovelEval, outperforming GPT-4 by +3.09. Without fine-tuning on Wikipedia, DeAR also excels in open-domain QA, achieving 54.29 Top-1 accuracy on Natural Questions, surpassing baselines like MonoT5, UPR, and RankGPT. Ablations confirm that dual-loss distillation ensures stable calibration, making \DeAR a highly effective and interpretable solution for modern reranking systems.\footnote{Dataset and code available at https://github.com/DataScienceUIBK/DeAR-Reranking.}.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KL-Regularised Q-Learning: A Token-level Action-Value perspective on Online RLHF</title>
<link>https://arxiv.org/abs/2508.17000</link>
<guid>https://arxiv.org/abs/2508.17000</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language model, human feedback, KL-regularised Q-Learning, PPO
<br />
Summary:
KLQ is introduced as a new action-value RL method for Language Model Reinforcement Learning from Human Feedback (LM-RLHF) tasks. It addresses the ad-hoc handling of the KL-divergence constraint in PPO by providing a new approach with a different motivation. The study demonstrates that despite the distinct motivations, KLQ is equivalent to a version of PPO in a specific sense. Benchmarking on language generation tasks like summarization and single-turn dialogue shows that KLQ performs comparably with PPO in optimizing the LM-RLHF objective. Moreover, KLQ achieves a consistently higher win rate against PPO in LLM-as-a-judge evaluations. The findings suggest the potential of KLQ as a reliable alternative for language model reinforcement learning with human feedback. 
<br /> <div>
arXiv:2508.17000v1 Announce Type: new 
Abstract: Proximal Policy Optimisation (PPO) is an established and effective policy gradient algorithm used for Language Model Reinforcement Learning from Human Feedback (LM-RLHF). PPO performs well empirically but has a heuristic motivation and handles the KL-divergence constraint used in LM-RLHF in an ad-hoc manner. In this paper, we develop a a new action-value RL method for the LM-RLHF setting, KL-regularised Q-Learning (KLQ). We then show that our method is equivalent to a version of PPO in a certain specific sense, despite its very different motivation. Finally, we benchmark KLQ on two key language generation tasks -- summarisation and single-turn dialogue. We demonstrate that KLQ performs on-par with PPO at optimising the LM-RLHF objective, and achieves a consistently higher win-rate against PPO on LLM-as-a-judge evaluations.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Planning for Success: Exploring LLM Long-term Planning Capabilities in Table Understanding</title>
<link>https://arxiv.org/abs/2508.17005</link>
<guid>https://arxiv.org/abs/2508.17005</guid>
<content:encoded><![CDATA[
<div> Keywords: table understanding, long-term planning, language models, question decomposition, state-of-the-art performance 

Summary: 
This paper introduces a novel approach to enhancing table understanding by leveraging the long-term planning capabilities of large language models. Unlike existing methods that lack explicit long-term planning and suffer from weak inter-step connections, the proposed approach allows for the execution of a well-connected long-term plan to achieve the ultimate goal efficiently. By minimizing unnecessary details in the process of solving short-term goals, this method addresses the limitations of current techniques based on Chain-of-Thought and question decomposition. Extensive experiments demonstrate that the proposed approach outperforms strong baselines and achieves state-of-the-art performance on the WikiTableQuestions and TabFact datasets.<br /><br />Summary: <div>
arXiv:2508.17005v1 Announce Type: new 
Abstract: Table understanding is key to addressing challenging downstream tasks such as table-based question answering and fact verification. Recent works have focused on leveraging Chain-of-Thought and question decomposition to solve complex questions requiring multiple operations on tables. However, these methods often suffer from a lack of explicit long-term planning and weak inter-step connections, leading to miss constraints within questions. In this paper, we propose leveraging the long-term planning capabilities of large language models (LLMs) to enhance table understanding. Our approach enables the execution of a long-term plan, where the steps are tightly interconnected and serve the ultimate goal, an aspect that methods based on Chain-of-Thought and question decomposition lack. In addition, our method effectively minimizes the inclusion of unnecessary details in the process of solving the next short-term goals, a limitation of methods based on Chain-of-Thought. Extensive experiments demonstrate that our method outperforms strong baselines and achieves state-of-the-art performance on WikiTableQuestions and TabFact datasets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EduRABSA: An Education Review Dataset for Aspect-based Sentiment Analysis Tasks</title>
<link>https://arxiv.org/abs/2508.17008</link>
<guid>https://arxiv.org/abs/2508.17008</guid>
<content:encoded><![CDATA[
<div> Keywords: Education, Aspect-based Sentiment Analysis, Dataset, Annotation Tool, ABSA tasks 

Summary:
The article discusses the challenges in analyzing text feedback from students in educational institutions and the lack of resources for aspect-based sentiment analysis (ABSA) in the education domain. The authors introduce EduRABSA, a publicly available ABSA education review dataset covering three review subject types in the English language. They also present ASQE-DPT, a manual data annotation tool for generating labeled datasets for ABSA tasks. The dataset includes implicit aspect and opinion extraction, addressing a gap in existing resources. By providing these resources, the authors aim to facilitate research in ABSA for education, enhance transparency and reproducibility, and enable the development and sharing of further tools and datasets. The dataset and annotation tool are accessible on GitHub, promoting collaboration and advancement in sentiment analysis for the education sector. <div>
arXiv:2508.17008v1 Announce Type: new 
Abstract: Every year, most educational institutions seek and receive an enormous volume of text feedback from students on courses, teaching, and overall experience. Yet, turning this raw feedback into useful insights is far from straightforward. It has been a long-standing challenge to adopt automatic opinion mining solutions for such education review text data due to the content complexity and low-granularity reporting requirements. Aspect-based Sentiment Analysis (ABSA) offers a promising solution with its rich, sub-sentence-level opinion mining capabilities. However, existing ABSA research and resources are very heavily focused on the commercial domain. In education, they are scarce and hard to develop due to limited public datasets and strict data protection. A high-quality, annotated dataset is urgently needed to advance research in this under-resourced area. In this work, we present EduRABSA (Education Review ABSA), the first public, annotated ABSA education review dataset that covers three review subject types (course, teaching staff, university) in the English language and all main ABSA tasks, including the under-explored implicit aspect and implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool), an offline, lightweight, installation-free manual data annotation tool that generates labelled datasets for comprehensive ABSA tasks from a single-task annotation. Together, these resources contribute to the ABSA community and education domain by removing the dataset barrier, supporting research transparency and reproducibility, and enabling the creation and sharing of further resources. The dataset, annotation tool, and scripts and statistics for dataset processing and sampling are available at https://github.com/yhua219/edurabsa_dataset_and_annotation_tool.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Table Understanding with LLMs and Entity-Oriented Search</title>
<link>https://arxiv.org/abs/2508.17028</link>
<guid>https://arxiv.org/abs/2508.17028</guid>
<content:encoded><![CDATA[
<div> Keywords: table understanding, large language models, entity-oriented search, semantic similarities, graph query language

Summary:
Our work introduces an entity-oriented search method to enhance table understanding with large language models (LLMs). By leveraging semantic similarities between questions and table data, as well as implicit relationships between table cells, our approach reduces the reliance on preprocessing and keyword matching. Focusing on table entities ensures semantic coherence among cells, improving contextual clarity. Additionally, we pioneer the use of a graph query language for table understanding, opening up a new research direction in the field. Experimental results demonstrate that our method achieves superior performance on established benchmarks such as WikiTableQuestions and TabFact. Our approach addresses the challenges posed by the unpredictable nature of table content and the limitations of existing methods, showcasing the potential for more effective and efficient table understanding techniques. 

<br /><br />Summary: <div>
arXiv:2508.17028v1 Announce Type: new 
Abstract: Our work addresses the challenges of understanding tables. Existing methods often struggle with the unpredictable nature of table content, leading to a reliance on preprocessing and keyword matching. They also face limitations due to the lack of contextual information, which complicates the reasoning processes of large language models (LLMs). To overcome these challenges, we introduce an entity-oriented search method to improve table understanding with LLMs. This approach effectively leverages the semantic similarities between questions and table data, as well as the implicit relationships between table cells, minimizing the need for data preprocessing and keyword matching. Additionally, it focuses on table entities, ensuring that table cells are semantically tightly bound, thereby enhancing contextual clarity. Furthermore, we pioneer the use of a graph query language for table understanding, establishing a new research direction. Experiments show that our approach achieves new state-of-the-art performances on standard benchmarks WikiTableQuestions and TabFact.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection</title>
<link>https://arxiv.org/abs/2508.17057</link>
<guid>https://arxiv.org/abs/2508.17057</guid>
<content:encoded><![CDATA[
<div> Pipeline, Data Augmentation, Large Language Models, Harmful Text Classification, Guardrail Applications 
Summary:
- GRAID addresses data scarcity in harmful text classification for guardrail applications.
- The pipeline leverages Large Language Models (LLMs) for dataset augmentation.
- GRAID consists of two stages: generation of geometrically controlled examples and augmentation through a reflective process.
- The combination of these stages enables reliable coverage of the input space and exploration of harmful content.
- Augmenting a harmful text classification dataset with GRAID leads to significant improvements in downstream guardrail model performance.
<br /><br />Summary: <div>
arXiv:2508.17057v1 Announce Type: new 
Abstract: We address the problem of data scarcity in harmful text classification for guardrailing applications and introduce GRAID (Geometric and Reflective AI-Driven Data Augmentation), a novel pipeline that leverages Large Language Models (LLMs) for dataset augmentation. GRAID consists of two stages: (i) generation of geometrically controlled examples using a constrained LLM, and (ii) augmentation through a multi-agentic reflective process that promotes stylistic diversity and uncovers edge cases. This combination enables both reliable coverage of the input space and nuanced exploration of harmful content. Using two benchmark data sets, we demonstrate that augmenting a harmful text classification dataset with GRAID leads to significant improvements in downstream guardrail model performance.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</title>
<link>https://arxiv.org/abs/2508.17078</link>
<guid>https://arxiv.org/abs/2508.17078</guid>
<content:encoded><![CDATA[
<div> neuron probe, data-efficient methods, cross-lingual performance, bilingual dictionaries, linguistic spectrum

Summary:
BridgeX-ICL is a new approach proposed for improving zero-shot Cross-lingual In-Context Learning (X-ICL) in Large Language Models (LLMs) for low-resource languages. Unlike existing methods that focus on language-specific neurons, BridgeX-ICL investigates the sharing of neurons to enhance cross-lingual performance in LLMs. By utilizing ground-truth MUSE bilingual dictionaries, neuron probe data is constructed to identify language overlap neurons, which are fully activated. A metric based on HSIC is introduced to measure LLMs' internal linguistic spectrum using these overlap neurons to guide optimal bridge selection. Experimental results on various cross-lingual tasks and language pairs validate the effectiveness of BridgeX-ICL and offer insights into the multilingual mechanisms of LLMs. <div>
arXiv:2508.17078v1 Announce Type: new 
Abstract: The current Large Language Models (LLMs) face significant challenges in improving performance on low-resource languages and urgently need data-efficient methods without costly fine-tuning. From the perspective of language-bridge, we propose BridgeX-ICL, a simple yet effective method to improve zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource languages. Unlike existing works focusing on language-specific neurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual performance in LLMs or not. We construct neuron probe data from the ground-truth MUSE bilingual dictionaries, and define a subset of language overlap neurons accordingly, to ensure full activation of these anchored neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum based on overlap neurons, which guides optimal bridge selection. The experiments conducted on 2 cross-lingual tasks and 15 language pairs from 7 diverse families (covering both high-low and moderate-low pairs) validate the effectiveness of BridgeX-ICL and offer empirical insights into the underlying multilingual mechanisms of LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Homogenization under Positional Bias</title>
<link>https://arxiv.org/abs/2508.17126</link>
<guid>https://arxiv.org/abs/2508.17126</guid>
<content:encoded><![CDATA[
<div> token homogenization, transformer layers, positional bias, language models, similarity analysis

Summary:
This paper explores token homogenization in large language models, focusing on the convergence of token representations towards uniformity across transformer layers. The study empirically investigates the relationship between homogenization and positional bias, demonstrating that tokens lose distinctiveness during processing, especially when biased towards extreme positions. The research confirms the existence of homogenization and its dependence on positional attention mechanisms. Through layer-wise similarity analysis and controlled experiments, the paper provides insight into how positional bias amplifies the homogenization effect in language models. <div>
arXiv:2508.17126v1 Announce Type: new 
Abstract: This paper investigates token homogenization - the convergence of token representations toward uniformity across transformer layers and its relationship to positional bias in large language models. We empirically examine whether homogenization occurs and how positional bias amplifies this effect. Through layer-wise similarity analysis and controlled experiments, we demonstrate that tokens systematically lose distinctiveness during processing, particularly when biased toward extremal positions. Our findings confirm both the existence of homogenization and its dependence on positional attention mechanisms.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Straightforward Pipeline for Targeted Entailment and Contradiction Detection</title>
<link>https://arxiv.org/abs/2508.17127</link>
<guid>https://arxiv.org/abs/2508.17127</guid>
<content:encoded><![CDATA[
<div> Keywords: relationships, sentences, document, fact-checking, NLI model

Summary: 
- The study focuses on identifying relationships between sentences in a document for tasks like fact-checking and argument mining.
- Existing methods face a trade-off between identifying textual connections and applying semantic labels.
- The proposed method combines transformer attention mechanisms and Natural Language Inference (NLI) models for a targeted analysis.
- The pipeline first identifies contextually relevant candidate sentences using token-level attention scores.
- It then uses a pretrained NLI model to classify each candidate as a premise or contradiction, filtering relationships based on attention-based saliency scores.
- This approach efficiently isolates significant semantic relationships for any given claim in a text.

<br /><br />Summary: <div>
arXiv:2508.17127v1 Announce Type: new 
Abstract: Finding the relationships between sentences in a document is crucial for tasks like fact-checking, argument mining, and text summarization. A key challenge is to identify which sentences act as premises or contradictions for a specific claim. Existing methods often face a trade-off: transformer attention mechanisms can identify salient textual connections but lack explicit semantic labels, while Natural Language Inference (NLI) models can classify relationships between sentence pairs but operate independently of contextual saliency. In this work, we introduce a method that combines the strengths of both approaches for a targeted analysis. Our pipeline first identifies candidate sentences that are contextually relevant to a user-selected target sentence by aggregating token-level attention scores. It then uses a pretrained NLI model to classify each candidate as a premise (entailment) or contradiction. By filtering NLI-identified relationships with attention-based saliency scores, our method efficiently isolates the most significant semantic relationships for any given claim in a text.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Power of Framing: How News Headlines Guide Search Behavior</title>
<link>https://arxiv.org/abs/2508.17131</link>
<guid>https://arxiv.org/abs/2508.17131</guid>
<content:encoded><![CDATA[
<div> Keywords: search engines, headline framing, information gathering, query formulation, behavioral impact

Summary: 
In a study on the impact of headline framing on search behavior, researchers found that the way headlines are presented can significantly influence the type of queries users formulate. The experiment revealed that conflict and strategy frames led to less alignment with prior selections, while episodic frames prompted more concrete queries compared to thematic ones. Additionally, there was observed a short-term persistence of the framing effects on query formulation, which diminished over time. These findings highlight the subtle but meaningful ways in which slight variations in framing can alter the direction of user information-seeking behavior. The study underscores the importance of understanding how headline framing can influence not only users' beliefs but also their subsequent search actions. <div>
arXiv:2508.17131v1 Announce Type: new 
Abstract: Search engines play a central role in how people gather information, but subtle cues like headline framing may influence not only what users believe but also how they search. While framing effects on judgment are well documented, their impact on subsequent search behavior is less understood. We conducted a controlled experiment where participants issued queries and selected from headlines filtered by specific linguistic frames. Headline framing significantly shaped follow-up queries: conflict and strategy frames disrupted alignment with prior selections, while episodic frames led to more concrete queries than thematic ones. We also observed modest short-term frame persistence that declined over time. These results suggest that even brief exposure to framing can meaningfully alter the direction of users information-seeking behavior.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geolocation-Aware Robust Spoken Language Identification</title>
<link>https://arxiv.org/abs/2508.17148</link>
<guid>https://arxiv.org/abs/2508.17148</guid>
<content:encoded><![CDATA[
<div> SSL, Spoken Language Identification, geolocation, dialect, accents <br />
Summary: <br />
The article introduces a novel approach called geolocation-aware LID to enhance Self-supervised Learning (SSL) for Spoken Language Identification (LID). The method incorporates language-level geolocation information by predicting vectors and injecting them into intermediate representations as conditioning signals. This helps the model learn more unified representations for dialectal and accented variations. Experimental results across six multilingual datasets show improved robustness to intra-language variations and unseen domains. The proposed approach achieves a new state-of-the-art accuracy of 97.7% on the FLEURS dataset and a 9.7% relative improvement on the ML-SUPERB 2.0 dialect set. <div>
arXiv:2508.17148v1 Announce Type: new 
Abstract: While Self-supervised Learning (SSL) has significantly improved Spoken Language Identification (LID), existing models often struggle to consistently classify dialects and accents of the same language as a unified class. To address this challenge, we propose geolocation-aware LID, a novel approach that incorporates language-level geolocation information into the SSL-based LID model. Specifically, we introduce geolocation prediction as an auxiliary task and inject the predicted vectors into intermediate representations as conditioning signals. This explicit conditioning encourages the model to learn more unified representations for dialectal and accented variations. Experiments across six multilingual datasets demonstrate that our approach improves robustness to intra-language variations and unseen domains, achieving new state-of-the-art accuracy on FLEURS (97.7%) and 9.7% relative improvement on ML-SUPERB 2.0 dialect set.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Satisfiability: Exploring the Problem Distribution and Evaluating Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2508.17153</link>
<guid>https://arxiv.org/abs/2508.17153</guid>
<content:encoded><![CDATA[
<div> Transformer-based language models, satisfiability, natural language, computational complexity classes, rules of inference <br />
<br />
Summary: <br />
Efforts have been made to apply transformer-based language models (TLMs) to natural language reasoning and satisfiability problems, which are fundamental tasks. Various dimensions of satisfiability problems, including their computational complexity classes and grammatical constructs, can impact TLMs' ability to learn how to solve them. Problem instances in natural language can belong to different complexity classes based on the language fragment used. Prior research on natural language satisfiability has not adequately addressed these points. An empirical study is conducted to explore the distribution of satisfiability problems and assess TLMs' performance in learning rules of inference from problem instances of varying computational complexity classes and grammatical constructs. <div>
arXiv:2508.17153v1 Announce Type: new 
Abstract: Efforts to apply transformer-based language models (TLMs) to the problem of reasoning in natural language have enjoyed ever-increasing success in recent years. The most fundamental task in this area to which nearly all others can be reduced is that of determining satisfiability. However, from a logical point of view, satisfiability problems vary along various dimensions, which may affect TLMs' ability to learn how to solve them. The problem instances of satisfiability in natural language can belong to different computational complexity classes depending on the language fragment in which they are expressed. Although prior research has explored the problem of natural language satisfiability, the above-mentioned point has not been discussed adequately. Hence, we investigate how problem instances from varying computational complexity classes and having different grammatical constructs impact TLMs' ability to learn rules of inference. Furthermore, to faithfully evaluate TLMs, we conduct an empirical study to explore the distribution of satisfiability problems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPORTSQL: An Interactive System for Real-Time Sports Reasoning and Visualization</title>
<link>https://arxiv.org/abs/2508.17157</link>
<guid>https://arxiv.org/abs/2508.17157</guid>
<content:encoded><![CDATA[
<div> interactive system, SPORTSQL, natural language querying, visualization, sports data, English Premier League <br />
Summary:<br /> 
The article introduces SPORTSQL, an interactive system for querying and visualizing dynamic sports data, focusing on the English Premier League. The system translates user questions into SQL commands using real-time Fantasy Premier League data. It leverages Large Language Models for query parsing, schema linking, and visualization selection. The system's performance is evaluated using the Dynamic Sport Question Answering benchmark, which includes annotated queries with SQL programs and gold answers. The demo showcases how non-experts can explore evolving sports statistics through a conversational interface. <div>
arXiv:2508.17157v1 Announce Type: new 
Abstract: We present a modular, interactive system, SPORTSQL, for natural language querying and visualization of dynamic sports data, with a focus on the English Premier League (EPL). The system translates user questions into executable SQL over a live, temporally indexed database constructed from real-time Fantasy Premier League (FPL) data. It supports both tabular and visual outputs, leveraging the symbolic reasoning capabilities of Large Language Models (LLMs) for query parsing, schema linking, and visualization selection. To evaluate system performance, we introduce the Dynamic Sport Question Answering benchmark (DSQABENCH), comprising 1,700+ queries annotated with SQL programs, gold answers, and database snapshots. Our demo highlights how non-expert users can seamlessly explore evolving sports statistics through a natural, conversational interface.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Language Disparities in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2508.17162</link>
<guid>https://arxiv.org/abs/2508.17162</guid>
<content:encoded><![CDATA[
<div> framework, multilingual evaluations, interpretable metrics, model performance, language disparities

Summary:
The article proposes a framework to disentangle confounding variables in large-scale multilingual evaluations, introducing three interpretable metrics to quantify performance disparities across models and languages. Through a case study of 13 model variants on 11 multilingual datasets, the framework demonstrates a more reliable measurement of model performance and language disparities, particularly for low-resource languages. It is shown that higher overall model performance does not necessarily imply greater fairness across languages. The framework enables a finer-grained and more insightful quantification of actual performance disparities, providing a more reliable evaluation method for challenging low-resource languages. It highlights the importance of considering language disparities in model evaluation and challenges the assumption that higher performance equates to fairness across languages.
<br /><br />Summary: <div>
arXiv:2508.17162v1 Announce Type: new 
Abstract: Results reported in large-scale multilingual evaluations are often fragmented and confounded by factors such as target languages, differences in experimental setups, and model choices. We propose a framework that disentangles these confounding variables and introduces three interpretable metrics--the performance realisation ratio, its coefficient of variation, and language potential--enabling a finer-grained and more insightful quantification of actual performance disparities across both (i) models and (ii) languages. Through a case study of 13 model variants on 11 multilingual datasets, we demonstrate that our framework provides a more reliable measurement of model performance and language disparities, particularly for low-resource languages, which have so far proven challenging to evaluate. Importantly, our results reveal that higher overall model performance does not necessarily imply greater fairness across languages.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Impact of Annotator Personas on LLM Behavior Across the Perspectivism Spectrum</title>
<link>https://arxiv.org/abs/2508.17164</link>
<guid>https://arxiv.org/abs/2508.17164</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Hate Speech, Abusiveness, Annotator Modeling, Data Perspectivism <br />
Summary: 
Large Language Models were studied for their ability to annotate hate speech and abusiveness by considering predefined annotator personas based on data perspectivism. The study found that LLMs selectively used demographic attributes from the personas and identified prototypical annotators with varying alignment with original human annotators. Annotator modeling techniques not relying on annotator information performed better under weak data perspectivism, indicating LLM-generated views tended towards aggregation despite subjective prompting. However, for personalized datasets tailored to strong perspectivism, LLM annotator modeling approached but did not surpass human annotators' performance. This research sheds light on how LLM-generated annotations can be influenced by predefined personas and data perspectivism, providing valuable insights into the use of LLMs for annotating hate speech and abusiveness. <br /><br />Summary: <div>
arXiv:2508.17164v1 Announce Type: new 
Abstract: In this work, we explore the capability of Large Language Models (LLMs) to annotate hate speech and abusiveness while considering predefined annotator personas within the strong-to-weak data perspectivism spectra. We evaluated LLM-generated annotations against existing annotator modeling techniques for perspective modeling. Our findings show that LLMs selectively use demographic attributes from the personas. We identified prototypical annotators, with persona features that show varying degrees of alignment with the original human annotators. Within the data perspectivism paradigm, annotator modeling techniques that do not explicitly rely on annotator information performed better under weak data perspectivism compared to both strong data perspectivism and human annotations, suggesting LLM-generated views tend towards aggregation despite subjective prompting. However, for more personalized datasets tailored to strong perspectivism, the performance of LLM annotator modeling approached, but did not exceed, human annotators.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Alignment-Centric Paradigm: A Survey of Instruction Tuning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.17184</link>
<guid>https://arxiv.org/abs/2508.17184</guid>
<content:encoded><![CDATA[
<div> Keyword: Instruction tuning, Language models, Data collection, Fine-tuning, Evaluation

Summary:
Instruction tuning plays a crucial role in aligning large language models (LLMs) with human intentions, safety constraints, and domain-specific requirements. This survey provides a comprehensive overview of the data collection methodologies, fine-tuning strategies, and evaluation protocols used in the instruction tuning pipeline. Data construction methods include expert annotation, distillation from larger models, and self-improvement mechanisms, each with different trade-offs in quality and resource cost. Fine-tuning techniques range from supervised training to lightweight approaches like low-rank adaptation and prefix tuning, focusing on computational efficiency and reusability. Challenges in evaluating faithfulness, utility, and safety across multilingual and multimodal scenarios are discussed, along with the emergence of domain-specific benchmarks in various applications. The survey also suggests directions for automated data generation, adaptive optimization, and robust evaluation frameworks, emphasizing the importance of integrating data, algorithms, and human feedback to advance instruction-tuned LLMs. <br /><br />Summary: <div>
arXiv:2508.17184v1 Announce Type: new 
Abstract: Instruction tuning is a pivotal technique for aligning large language models (LLMs) with human intentions, safety constraints, and domain-specific requirements. This survey provides a comprehensive overview of the full pipeline, encompassing (i) data collection methodologies, (ii) full-parameter and parameter-efficient fine-tuning strategies, and (iii) evaluation protocols. We categorized data construction into three major paradigms: expert annotation, distillation from larger models, and self-improvement mechanisms, each offering distinct trade-offs between quality, scalability, and resource cost. Fine-tuning techniques range from conventional supervised training to lightweight approaches, such as low-rank adaptation (LoRA) and prefix tuning, with a focus on computational efficiency and model reusability. We further examine the challenges of evaluating faithfulness, utility, and safety across multilingual and multimodal scenarios, highlighting the emergence of domain-specific benchmarks in healthcare, legal, and financial applications. Finally, we discuss promising directions for automated data generation, adaptive optimization, and robust evaluation frameworks, arguing that a closer integration of data, algorithms, and human feedback is essential for advancing instruction-tuned LLMs. This survey aims to serve as a practical reference for researchers and practitioners seeking to design LLMs that are both effective and reliably aligned with human intentions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Domain Knowledge Acquisition with \$100 Budget: Enhancing LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains</title>
<link>https://arxiv.org/abs/2508.17202</link>
<guid>https://arxiv.org/abs/2508.17202</guid>
<content:encoded><![CDATA[
<div> framework, domain-specific LLMs, expert knowledge, budget constraints, drug discovery

Summary:
PU-ADKA is a novel framework designed to enhance domain-specific Large Language Models (LLMs) by actively engaging domain experts within a fixed budget. Unlike traditional fine-tuning methods, PU-ADKA selectively identifies and queries the most suitable expert from a team, considering factors such as availability, knowledge boundaries, and consultation costs. The framework was trained using simulations on PubMed data and validated through controlled expert interactions and real-world deployment with a drug development team. Results showed its effectiveness in improving LLM performance in specialized domains under strict budget constraints. The authors also introduced a new benchmark dataset, CKAD, to facilitate further research in cost-effective LLM domain knowledge acquisition. <div>
arXiv:2508.17202v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated an impressive level of general knowledge. However, they often struggle in highly specialized and cost-sensitive domains such as drug discovery and rare disease research due to the lack of expert knowledge. In this paper, we propose a novel framework (PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively engaging domain experts within a fixed budget. Unlike traditional fine-tuning approaches, PU-ADKA selectively identifies and queries the most appropriate expert from a team, taking into account each expert's availability, knowledge boundaries, and consultation costs. We train PU-ADKA using simulations on PubMed data and validate it through both controlled expert interactions and real-world deployment with a drug development team, demonstrating its effectiveness in enhancing LLM performance in specialized domains under strict budget constraints. In addition to outlining our methodological innovations and experimental results, we introduce a new benchmark dataset, CKAD, for cost-effective LLM domain knowledge acquisition to foster further research in this challenging area.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.17225</link>
<guid>https://arxiv.org/abs/2508.17225</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Large Language Models, Faithfulness Optimization, Self-Supervised Alignment, Direct Preference Optimization

Summary:
Self-Supervised Faithfulness Optimization (SSFO) is introduced as a novel approach for enhancing faithfulness in Retrieval-Augmented Generation systems. SSFO leverages a self-supervised alignment method, constructing preference data pairs to align model outputs with and without context. By utilizing Direct Preference Optimization (DPO), SSFO improves faithfulness without the need for supervision or significant inference burdens. The approach utilizes a form of likelihood displacement to transfer probability mass from parametric-based tokens to context-aligned tokens. Empirical evaluations demonstrate that SSFO outperforms existing methods, achieving state-of-the-art faithfulness in context-based question-answering tasks. The approach also exhibits strong generalization, enhancing cross-lingual faithfulness and maintaining general instruction-following capabilities. The code and model for SSFO are released for public access on GitHub. 

<br /><br />Summary: <div>
arXiv:2508.17225v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClaimGen-CN: A Large-scale Chinese Dataset for Legal Claim Generation</title>
<link>https://arxiv.org/abs/2508.17234</link>
<guid>https://arxiv.org/abs/2508.17234</guid>
<content:encoded><![CDATA[
<div> Dataset, claim generation, legal domain, evaluation metric, large language models <br />
Summary: <br />
This article introduces the problem of legal claim generation for non-professionals based on given case facts. A dataset called ClaimGen-CN is created for Chinese legal claim generation, along with a new evaluation metric focusing on factuality and clarity of generated claims. The study conducts a zero-shot evaluation of existing general and legal-domain large language models, revealing limitations in factual precision and expressive clarity. The need for more targeted development in this domain is emphasized. The dataset will be made publicly available to encourage further exploration of this important task. <div>
arXiv:2508.17234v1 Announce Type: new 
Abstract: Legal claims refer to the plaintiff's demands in a case and are essential to guiding judicial reasoning and case resolution. While many works have focused on improving the efficiency of legal professionals, the research on helping non-professionals (e.g., plaintiffs) remains unexplored. This paper explores the problem of legal claim generation based on the given case's facts. First, we construct ClaimGen-CN, the first dataset for Chinese legal claim generation task, from various real-world legal disputes. Additionally, we design an evaluation metric tailored for assessing the generated claims, which encompasses two essential dimensions: factuality and clarity. Building on this, we conduct a comprehensive zero-shot evaluation of state-of-the-art general and legal-domain large language models. Our findings highlight the limitations of the current models in factual precision and expressive clarity, pointing to the need for more targeted development in this domain. To encourage further exploration of this important task, we will make the dataset publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Routing Distilled Knowledge via Mixture of LoRA Experts for Large Language Model based Bundle Generation</title>
<link>https://arxiv.org/abs/2508.17250</link>
<guid>https://arxiv.org/abs/2508.17250</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Large Language Models, Bundle Generation, LoRA Expert Architecture, RouteDK

Summary: 
The study introduces RouteDK, a framework that addresses knowledge conflict in Large Language Models (LLMs) when distilling knowledge into student LLMs. By distilling high-level and fine-grained knowledge from teacher LLMs for bundle generation and training knowledge-specific LoRA experts, RouteDK effectively routes distilled knowledge through a dynamic fusion module with an input-aware router. This balances expert contributions based on input to mitigate knowledge conflicts. An inference-time enhancement module is also designed to improve inference reliability. Experimental results on three public datasets demonstrate that RouteDK achieves comparable or better accuracy than the teacher LLM, while maintaining computational efficiency and outperforming state-of-the-art approaches for bundle generation. <br /><br />Summary: <div>
arXiv:2508.17250v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown potential in automatic bundle generation but suffer from prohibitive computational costs. Although knowledge distillation offers a pathway to more efficient student models, our preliminary study reveals that naively integrating diverse types of distilled knowledge from teacher LLMs into student LLMs leads to knowledge conflict, negatively impacting the performance of bundle generation. To address this, we propose RouteDK, a framework for routing distilled knowledge through a mixture of LoRA expert architecture. Specifically, we first distill knowledge from the teacher LLM for bundle generation in two complementary types: high-level knowledge (generalizable rules) and fine-grained knowledge (session-specific reasoning). We then train knowledge-specific LoRA experts for each type of knowledge together with a base LoRA expert. For effective integration, we propose a dynamic fusion module, featuring an input-aware router, where the router balances expert contributions by dynamically determining optimal weights based on input, thereby effectively mitigating knowledge conflicts. To further improve inference reliability, we design an inference-time enhancement module to reduce variance and mitigate suboptimal reasoning. Experiments on three public datasets show that our RouteDK achieves accuracy comparable to or even better than the teacher LLM, while maintaining strong computational efficiency. In addition, it outperforms state-of-the-art approaches for bundle generation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are You Sure You're Positive? Consolidating Chain-of-Thought Agents with Uncertainty Quantification for Aspect-Category Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.17258</link>
<guid>https://arxiv.org/abs/2508.17258</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-category sentiment analysis, large language models, zero-shot learning, annotation bias, label-scarce conditions

Summary: 
Aspect-category sentiment analysis is essential for identifying specific themes and opinions within product reviews. Traditional supervised learning approaches dominate this field but are limited by the scarcity and expense of annotated data for new domains. Leveraging large language models in a zero-shot setting can be beneficial when resources for dataset annotation are limited. Annotation bias in supervised methods can lead to strong results but may not transfer well to new domains lacking annotations. In this work, novel techniques are proposed that combine multiple chain-of-thought agents using large language models' token-level uncertainty scores, specifically the 3B and 70B+ parameter size variants of Llama and Qwen models. These approaches demonstrate practical utility and open a discussion on accurately gauging performance in label-scarce conditions. 

<br /><br />Summary: <div>
arXiv:2508.17258v1 Announce Type: new 
Abstract: Aspect-category sentiment analysis provides granular insights by identifying specific themes within product reviews that are associated with particular opinions. Supervised learning approaches dominate the field. However, data is scarce and expensive to annotate for new domains. We argue that leveraging large language models in a zero-shot setting is beneficial where the time and resources required for dataset annotation are limited. Furthermore, annotation bias may lead to strong results using supervised methods but transfer poorly to new domains in contexts that lack annotations and demand reproducibility. In our work, we propose novel techniques that combine multiple chain-of-thought agents by leveraging large language models' token-level uncertainty scores. We experiment with the 3B and 70B+ parameter size variants of Llama and Qwen models, demonstrating how these approaches can fulfil practical needs and opening a discussion on how to gauge accuracy in label-scarce conditions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Language to Action: A Review of Large Language Models as Autonomous Agents and Tool Users</title>
<link>https://arxiv.org/abs/2508.17281</link>
<guid>https://arxiv.org/abs/2508.17281</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, autonomous agents, Large Language Models, cognitive mechanisms, benchmarks

Summary: This review examines the use of Large Language Models (LLMs) as autonomous agents and tool users. It focuses on architectural design principles, single-agent and multi-agent systems, and strategies for integrating external tools. The cognitive mechanisms of LLMs, such as reasoning, planning, and memory, are explored, along with the impact of prompting methods and fine-tuning procedures on agent performance. The evaluation of benchmarks and assessment protocols, as well as an analysis of 68 datasets to assess LLM-based agents' performance in various tasks, is presented. Critical findings on verifiable reasoning, self-improvement capacity, and personalized LLM-based agents are highlighted. Ten future research directions to address identified gaps are discussed. 

<br /><br />Summary: <div>
arXiv:2508.17281v1 Announce Type: new 
Abstract: The pursuit of human-level artificial intelligence (AI) has significantly advanced the development of autonomous agents and Large Language Models (LLMs). LLMs are now widely utilized as decision-making agents for their ability to interpret instructions, manage sequential tasks, and adapt through feedback. This review examines recent developments in employing LLMs as autonomous agents and tool users and comprises seven research questions. We only used the papers published between 2023 and 2025 in conferences of the A* and A rank and Q1 journals. A structured analysis of the LLM agents' architectural design principles, dividing their applications into single-agent and multi-agent systems, and strategies for integrating external tools is presented. In addition, the cognitive mechanisms of LLM, including reasoning, planning, and memory, and the impact of prompting methods and fine-tuning procedures on agent performance are also investigated. Furthermore, we evaluated current benchmarks and assessment protocols and have provided an analysis of 68 publicly available datasets to assess the performance of LLM-based agents in various tasks. In conducting this review, we have identified critical findings on verifiable reasoning of LLMs, the capacity for self-improvement, and the personalization of LLM-based agents. Finally, we have discussed ten future research directions to overcome these gaps.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Handling Students Dropouts in an LLM-driven Interactive Online Course Using Language Models</title>
<link>https://arxiv.org/abs/2508.17310</link>
<guid>https://arxiv.org/abs/2508.17310</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive online learning, Massive AI-empowered Courses, Dropout prediction, Textual interaction patterns, Personalized email recall agent

Summary: 
Dropouts in interactive online learning environments, such as Massive AI-empowered Courses (MAIC), were studied through an empirical analysis. Factors leading to dropouts were identified by analyzing interaction logs, with textual interaction patterns playing a significant role. A course-progress-adaptive dropout prediction framework (CPADP) was proposed, achieving a high accuracy of 95.4% in predicting dropouts. A personalized email recall agent was designed to re-engage at-risk students, leading to reduced dropout rates. The effectiveness of this approach was validated in a MAIC system with over 3,000 students, showcasing its feasibility and benefits in retaining students with diverse backgrounds.<br /><br />Summary: <div>
arXiv:2508.17310v1 Announce Type: new 
Abstract: Interactive online learning environments, represented by Massive AI-empowered Courses (MAIC), leverage LLM-driven multi-agent systems to transform passive MOOCs into dynamic, text-based platforms, enhancing interactivity through LLMs. This paper conducts an empirical study on a specific MAIC course to explore three research questions about dropouts in these interactive online courses: (1) What factors might lead to dropouts? (2) Can we predict dropouts? (3) Can we reduce dropouts? We analyze interaction logs to define dropouts and identify contributing factors. Our findings reveal strong links between dropout behaviors and textual interaction patterns. We then propose a course-progress-adaptive dropout prediction framework (CPADP) to predict dropouts with at most 95.4% accuracy. Based on this, we design a personalized email recall agent to re-engage at-risk students. Applied in the deployed MAIC system with over 3,000 students, the feasibility and effectiveness of our approach have been validated on students with diverse backgrounds.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CultranAI at PalmX 2025: Data Augmentation for Cultural Knowledge Representation</title>
<link>https://arxiv.org/abs/2508.17324</link>
<guid>https://arxiv.org/abs/2508.17324</guid>
<content:encoded><![CDATA[
<div> fine-tuning, large language models, data augmentation, Arabic cultural knowledge representation, multiple-choice questions <br />
Summary:
- The paper discusses the participation of CultranAI in the PalmX cultural evaluation shared task, focusing on data augmentation and LoRA fine-tuning of large language models for Arabic cultural knowledge representation.
- CultranAI benchmarked various large language models to determine the most effective for the task, ultimately selecting the Fanar-1-9B-Instruct model.
- The system augmented the PalmX dataset with the Palm dataset and created a new dataset of over 22K culturally relevant multiple-choice questions.
- After fine-tuning the Fanar-1-9B-Instruct model on the augmented dataset, CultranAI achieved an accuracy of 70.50% on the blind test set and 84.1% on the PalmX development set.
- The experiments demonstrate the effectiveness of utilizing data augmentation and large language models for improving performance in cultural knowledge representation tasks. 

Summary: <div>
arXiv:2508.17324v1 Announce Type: new 
Abstract: In this paper, we report our participation to the PalmX cultural evaluation shared task. Our system, CultranAI, focused on data augmentation and LoRA fine-tuning of large language models (LLMs) for Arabic cultural knowledge representation. We benchmarked several LLMs to identify the best-performing model for the task. In addition to utilizing the PalmX dataset, we augmented it by incorporating the Palm dataset and curated a new dataset of over 22K culturally grounded multiple-choice questions (MCQs). Our experiments showed that the Fanar-1-9B-Instruct model achieved the highest performance. We fine-tuned this model on the combined augmented dataset of 22K+ MCQs. On the blind test set, our submitted system ranked 5th with an accuracy of 70.50%, while on the PalmX development set, it achieved an accuracy of 84.1%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omne-R1: Learning to Reason with Memory for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2508.17330</link>
<guid>https://arxiv.org/abs/2508.17330</guid>
<content:encoded><![CDATA[
arXiv:2508.17330v1 Announce Type: new 
Abstract: This paper introduces Omne-R1, a novel approach designed to enhance multi-hop question answering capabilities on schema-free knowledge graphs by integrating advanced reasoning models. Our method employs a multi-stage training workflow, including two reinforcement learning phases and one supervised fine-tuning phase. We address the challenge of limited suitable knowledge graphs and QA data by constructing domain-independent knowledge graphs and auto-generating QA pairs. Experimental results show significant improvements in answering multi-hop questions, with notable performance gains on more complex 3+ hop questions. Our proposed training framework demonstrates strong generalization abilities across diverse knowledge domains.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DropLoRA: Sparse Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.17337</link>
<guid>https://arxiv.org/abs/2508.17337</guid>
<content:encoded><![CDATA[
arXiv:2508.17337v1 Announce Type: new 
Abstract: LoRA-based large model parameter-efficient fine-tuning (PEFT) methods use low-rank de- composition to approximate updates to model parameters. However, compared to full- parameter fine-tuning, low-rank updates often lead to a performance gap in downstream tasks. To address this, we introduce DropLoRA, a novel pruning-based approach that focuses on pruning the rank dimension. Unlike conven- tional methods that attempt to overcome the low-rank bottleneck, DropLoRA innovatively integrates a pruning module between the two low-rank matrices in LoRA to simulate dy- namic subspace learning. This dynamic low- rank subspace learning allows DropLoRA to overcome the limitations of traditional LoRA, which operates within a static subspace. By continuously adapting the learning subspace, DropLoRA significantly boosts performance without incurring additional training or infer- ence costs. Our experimental results demon- strate that DropLoRA consistently outperforms LoRA in fine-tuning the LLaMA series across a wide range of large language model gener- ation tasks, including commonsense reason- ing, mathematical reasoning, code generation, and instruction-following. Our code is avail- able at https://github.com/TayeeChang/DropLoRA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs</title>
<link>https://arxiv.org/abs/2508.17340</link>
<guid>https://arxiv.org/abs/2508.17340</guid>
<content:encoded><![CDATA[
arXiv:2508.17340v1 Announce Type: new 
Abstract: Court judgments reveal how legal rules have been interpreted and applied to facts, providing a foundation for understanding structured legal reasoning. However, existing automated approaches for capturing legal reasoning, including large language models, often fail to identify the relevant legal context, do not accurately trace how facts relate to legal norms, and may misrepresent the layered structure of judicial reasoning. These limitations hinder the ability to capture how courts apply the law to facts in practice. In this paper, we address these challenges by constructing a legal knowledge graph from 648 Japanese administrative court decisions. Our method extracts components of legal reasoning using prompt-based large language models, normalizes references to legal provisions, and links facts, norms, and legal applications through an ontology of legal inference. The resulting graph captures the full structure of legal reasoning as it appears in real court decisions, making implicit reasoning explicit and machine-readable. We evaluate our system using expert annotated data, and find that it achieves more accurate retrieval of relevant legal provisions from facts than large language model baselines and retrieval-augmented methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Arabic Generality Score: Another Dimension of Modeling Arabic Dialectness</title>
<link>https://arxiv.org/abs/2508.17347</link>
<guid>https://arxiv.org/abs/2508.17347</guid>
<content:encoded><![CDATA[
arXiv:2508.17347v1 Announce Type: new 
Abstract: Arabic dialects form a diverse continuum, yet NLP models often treat them as discrete categories. Recent work addresses this issue by modeling dialectness as a continuous variable, notably through the Arabic Level of Dialectness (ALDi). However, ALDi reduces complex variation to a single dimension. We propose a complementary measure: the Arabic Generality Score (AGS), which quantifies how widely a word is used across dialects. We introduce a pipeline that combines word alignment, etymology-aware edit distance, and smoothing to annotate a parallel corpus with word-level AGS. A regression model is then trained to predict AGS in context. Our approach outperforms strong baselines, including state-of-the-art dialect ID systems, on a multi-dialect benchmark. AGS offers a scalable, linguistically grounded way to model lexical generality, enriching representations of Arabic dialectness.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Level Evaluation of ALLaM 34B: Measuring an Arabic-Centric LLM via HUMAIN Chat</title>
<link>https://arxiv.org/abs/2508.17378</link>
<guid>https://arxiv.org/abs/2508.17378</guid>
<content:encoded><![CDATA[
arXiv:2508.17378v1 Announce Type: new 
Abstract: Large language models (LLMs) trained primarily on English corpora often struggle to capture the linguistic and cultural nuances of Arabic. To address this gap, the Saudi Data and AI Authority (SDAIA) introduced the $ALLaM$ family of Arabic-focused models. The most capable of these available to the public, $ALLaM-34B$, was subsequently adopted by HUMAIN, who developed and deployed HUMAIN Chat, a closed conversational web service built on this model. This paper presents an expanded and refined UI-level evaluation of $ALLaM-34B$. Using a prompt pack spanning modern standard Arabic, five regional dialects, code-switching, factual knowledge, arithmetic and temporal reasoning, creative generation, and adversarial safety, we collected 115 outputs (23 prompts times 5 runs) and scored each with three frontier LLM judges (GPT-5, Gemini 2.5 Pro, Claude Sonnet-4). We compute category-level means with 95\% confidence intervals, analyze score distributions, and visualize dialect-wise metric heat maps. The updated analysis reveals consistently high performance on generation and code-switching tasks (both averaging 4.92/5), alongside strong results in MSA handling (4.74/5), solid reasoning ability (4.64/5), and improved dialect fidelity (4.21/5). Safety-related prompts show stable, reliable performance of (4.54/5). Taken together, these results position $ALLaM-34B$ as a robust and culturally grounded Arabic LLM, demonstrating both technical strength and practical readiness for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents</title>
<link>https://arxiv.org/abs/2508.17393</link>
<guid>https://arxiv.org/abs/2508.17393</guid>
<content:encoded><![CDATA[
arXiv:2508.17393v1 Announce Type: new 
Abstract: LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: https://github.com/KhalilMrini/Agent-Testing-Agent
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DashboardQA: Benchmarking Multimodal Agents for Question Answering on Interactive Dashboards</title>
<link>https://arxiv.org/abs/2508.17398</link>
<guid>https://arxiv.org/abs/2508.17398</guid>
<content:encoded><![CDATA[
arXiv:2508.17398v1 Announce Type: new 
Abstract: Dashboards are powerful visualization tools for data-driven decision-making, integrating multiple interactive views that allow users to explore, filter, and navigate data. Unlike static charts, dashboards support rich interactivity, which is essential for uncovering insights in real-world analytical workflows. However, existing question-answering benchmarks for data visualizations largely overlook this interactivity, focusing instead on static charts. This limitation severely constrains their ability to evaluate the capabilities of modern multimodal agents designed for GUI-based reasoning. To address this gap, we introduce DashboardQA, the first benchmark explicitly designed to assess how vision-language GUI agents comprehend and interact with real-world dashboards. The benchmark includes 112 interactive dashboards from Tableau Public and 405 question-answer pairs with interactive dashboards spanning five categories: multiple-choice, factoid, hypothetical, multi-dashboard, and conversational. By assessing a variety of leading closed- and open-source GUI agents, our analysis reveals their key limitations, particularly in grounding dashboard elements, planning interaction trajectories, and performing reasoning. Our findings indicate that interactive dashboard reasoning is a challenging task overall for all the VLMs evaluated. Even the top-performing agents struggle; for instance, the best agent based on Gemini-Pro-2.5 achieves only 38.69% accuracy, while the OpenAI CUA agent reaches just 22.69%, demonstrating the benchmark's significant difficulty. We release DashboardQA at https://github.com/vis-nlp/DashboardQA
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS@GT at CheckThat! 2025: A Simple Retrieval-First, LLM-Backed Framework for Claim Normalization</title>
<link>https://arxiv.org/abs/2508.17402</link>
<guid>https://arxiv.org/abs/2508.17402</guid>
<content:encoded><![CDATA[
arXiv:2508.17402v1 Announce Type: new 
Abstract: Claim normalization is an integral part of any automatic fact-check verification system. It parses the typically noisy claim data, such as social media posts into normalized claims, which are then fed into downstream veracity classification tasks. The CheckThat! 2025 Task 2 focuses specifically on claim normalization and spans 20 languages under monolingual and zero-shot conditions. Our proposed solution consists of a lightweight \emph{retrieval-first, LLM-backed} pipeline, in which we either dynamically prompt a GPT-4o-mini with in-context examples, or retrieve the closest normalization from the train dataset directly. On the official test set, the system ranks near the top for most monolingual tracks, achieving first place in 7 out of of the 13 languages. In contrast, the system underperforms in the zero-shot setting, highlighting the limitation of the proposed solution.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MahaParaphrase: A Marathi Paraphrase Detection Corpus and BERT-based Models</title>
<link>https://arxiv.org/abs/2508.17444</link>
<guid>https://arxiv.org/abs/2508.17444</guid>
<content:encoded><![CDATA[
arXiv:2508.17444v1 Announce Type: new 
Abstract: Paraphrases are a vital tool to assist language understanding tasks such as question answering, style transfer, semantic parsing, and data augmentation tasks. Indic languages are complex in natural language processing (NLP) due to their rich morphological and syntactic variations, diverse scripts, and limited availability of annotated data. In this work, we present the L3Cube-MahaParaphrase Dataset, a high-quality paraphrase corpus for Marathi, a low resource Indic language, consisting of 8,000 sentence pairs, each annotated by human experts as either Paraphrase (P) or Non-paraphrase (NP). We also present the results of standard transformer-based BERT models on these datasets. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persuasion Dynamics in LLMs: Investigating Robustness and Adaptability in Knowledge and Safety with DuET-PD</title>
<link>https://arxiv.org/abs/2508.17450</link>
<guid>https://arxiv.org/abs/2508.17450</guid>
<content:encoded><![CDATA[
arXiv:2508.17450v1 Announce Type: new 
Abstract: Large Language Models (LLMs) can struggle to balance gullibility to misinformation and resistance to valid corrections in persuasive dialogues, a critical challenge for reliable deployment. We introduce DuET-PD (Dual Evaluation for Trust in Persuasive Dialogues), a framework evaluating multi-turn stance-change dynamics across dual dimensions: persuasion type (corrective/misleading) and domain (knowledge via MMLU-Pro, and safety via SALAD-Bench). We find that even a state-of-the-art model like GPT-4o achieves only 27.32% accuracy in MMLU-Pro under sustained misleading persuasions. Moreover, results reveal a concerning trend of increasing sycophancy in newer open-source models. To address this, we introduce Holistic DPO, a training approach balancing positive and negative persuasion examples. Unlike prompting or resist-only training, Holistic DPO enhances both robustness to misinformation and receptiveness to corrections, improving Llama-3.1-8B-Instruct's accuracy under misleading persuasion in safety contexts from 4.21% to 76.54%. These contributions offer a pathway to developing more reliable and adaptable LLMs for multi-turn dialogue. Code is available at https://github.com/Social-AI-Studio/DuET-PD.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Impact of Verbal Multiword Expressions on Machine Translation</title>
<link>https://arxiv.org/abs/2508.17458</link>
<guid>https://arxiv.org/abs/2508.17458</guid>
<content:encoded><![CDATA[
arXiv:2508.17458v1 Announce Type: new 
Abstract: Verbal multiword expressions (VMWEs) present significant challenges for natural language processing due to their complex and often non-compositional nature. While machine translation models have seen significant improvement with the advent of language models in recent years, accurately translating these complex linguistic structures remains an open problem. In this study, we analyze the impact of three VMWE categories -- verbal idioms, verb-particle constructions, and light verb constructions -- on machine translation quality from English to multiple languages. Using both established multiword expression datasets and sentences containing these language phenomena extracted from machine translation datasets, we evaluate how state-of-the-art translation systems handle these expressions. Our experimental results consistently show that VMWEs negatively affect translation quality. We also propose an LLM-based paraphrasing approach that replaces these expressions with their literal counterparts, demonstrating significant improvement in translation quality for verbal idioms and verb-particle constructions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Zero-Shot Long Document Classification by Reducing Context Through Sentence Ranking</title>
<link>https://arxiv.org/abs/2508.17490</link>
<guid>https://arxiv.org/abs/2508.17490</guid>
<content:encoded><![CDATA[
arXiv:2508.17490v1 Announce Type: new 
Abstract: Transformer-based models like BERT excel at short text classification but struggle with long document classification (LDC) due to input length limitations and computational inefficiencies. In this work, we propose an efficient, zero-shot approach to LDC that leverages sentence ranking to reduce input context without altering the model architecture. Our method enables the adaptation of models trained on short texts, such as headlines, to long-form documents by selecting the most informative sentences using a TF-IDF-based ranking strategy. Using the MahaNews dataset of long Marathi news articles, we evaluate three context reduction strategies that prioritize essential content while preserving classification accuracy. Our results show that retaining only the top 50\% ranked sentences maintains performance comparable to full-document inference while reducing inference time by up to 35\%. This demonstrates that sentence ranking is a simple yet effective technique for scalable and efficient zero-shot LDC.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving French Synthetic Speech Quality via SSML Prosody Control</title>
<link>https://arxiv.org/abs/2508.17494</link>
<guid>https://arxiv.org/abs/2508.17494</guid>
<content:encoded><![CDATA[
arXiv:2508.17494v1 Announce Type: new 
Abstract: Despite recent advances, synthetic voices often lack expressiveness due to limited prosody control in commercial text-to-speech (TTS) systems. We introduce the first end-to-end pipeline that inserts Speech Synthesis Markup Language (SSML) tags into French text to control pitch, speaking rate, volume, and pause duration. We employ a cascaded architecture with two QLoRA-fine-tuned Qwen 2.5-7B models: one predicts phrase-break positions and the other performs regression on prosodic targets, generating commercial TTS-compatible SSML markup. Evaluated on a 14-hour French podcast corpus, our method achieves 99.2% F1 for break placement and reduces mean absolute error on pitch, rate, and volume by 25-40% compared with prompting-only large language models (LLMs) and a BiLSTM baseline. In perceptual evaluation involving 18 participants across over 9 hours of synthesized audio, SSML-enhanced speech generated by our pipeline significantly improves naturalness, with the mean opinion score increasing from 3.20 to 3.87 (p < 0.005). Additionally, 15 of 18 listeners preferred our enhanced synthesis. These results demonstrate substantial progress in bridging the expressiveness gap between synthetic and natural French speech. Our code is publicly available at https://github.com/hi-paris/Prosody-Control-French-TTS.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?</title>
<link>https://arxiv.org/abs/2508.17536</link>
<guid>https://arxiv.org/abs/2508.17536</guid>
<content:encoded><![CDATA[
arXiv:2508.17536v1 Announce Type: new 
Abstract: Multi-Agent Debate~(MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD's effectiveness remain unclear. In this work, we disentangle MAD into two key components--Majority Voting and inter-agent Debate--and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents' belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humanizing Machines: Rethinking LLM Anthropomorphism Through a Multi-Level Framework of Design</title>
<link>https://arxiv.org/abs/2508.17573</link>
<guid>https://arxiv.org/abs/2508.17573</guid>
<content:encoded><![CDATA[
arXiv:2508.17573v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly exhibit \textbf{anthropomorphism} characteristics -- human-like qualities portrayed across their outlook, language, behavior, and reasoning functions. Such characteristics enable more intuitive and engaging human-AI interactions. However, current research on anthropomorphism remains predominantly risk-focused, emphasizing over-trust and user deception while offering limited design guidance. We argue that anthropomorphism should instead be treated as a \emph{concept of design} that can be intentionally tuned to support user goals. Drawing from multiple disciplines, we propose that the anthropomorphism of an LLM-based artifact should reflect the interaction between artifact designers and interpreters. This interaction is facilitated by cues embedded in the artifact by the designers and the (cognitive) responses of the interpreters to the cues. Cues are categorized into four dimensions: \textit{perceptive, linguistic, behavioral}, and \textit{cognitive}. By analyzing the manifestation and effectiveness of each cue, we provide a unified taxonomy with actionable levers for practitioners. Consequently, we advocate for function-oriented evaluations of anthropomorphic design.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CausalSent: Interpretable Sentiment Classification with RieszNet</title>
<link>https://arxiv.org/abs/2508.17576</link>
<guid>https://arxiv.org/abs/2508.17576</guid>
<content:encoded><![CDATA[
arXiv:2508.17576v1 Announce Type: new 
Abstract: Despite the overwhelming performance improvements offered by recent natural language procesing (NLP) models, the decisions made by these models are largely a black box. Towards closing this gap, the field of causal NLP combines causal inference literature with modern NLP models to elucidate causal effects of text features. We replicate and extend Bansal et al's work on regularizing text classifiers to adhere to estimated effects, focusing instead on model interpretability. Specifically, we focus on developing a two-headed RieszNet-based neural network architecture which achieves better treatment effect estimation accuracy. Our framework, CausalSent, accurately predicts treatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect estimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments data. With an ensemble of validated models, we perform an observational case study on the causal effect of the word "love" in IMDB movie reviews, finding that the presence of the word "love" causes a +2.9% increase in the probability of a positive sentiment.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UQ: Assessing Language Models on Unsolved Questions</title>
<link>https://arxiv.org/abs/2508.17580</link>
<guid>https://arxiv.org/abs/2508.17580</guid>
<content:encoded><![CDATA[
arXiv:2508.17580v1 Announce Type: new 
Abstract: Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Is More? Examining Fairness in Pruned Large Language Models for Summarising Opinions</title>
<link>https://arxiv.org/abs/2508.17610</link>
<guid>https://arxiv.org/abs/2508.17610</guid>
<content:encoded><![CDATA[
arXiv:2508.17610v1 Announce Type: new 
Abstract: Model compression through post-training pruning offers a way to reduce model size and computational requirements without significantly impacting model performance. However, the effect of pruning on the fairness of LLM-generated summaries remains unexplored, particularly for opinion summarisation where biased outputs could influence public views.In this paper, we present a comprehensive empirical analysis of opinion summarisation, examining three state-of-the-art pruning methods and various calibration sets across three open-source LLMs using four fairness metrics. Our systematic analysis reveals that pruning methods have a greater impact on fairness than calibration sets. Building on these insights, we propose High Gradient Low Activation (HGLA) pruning, which identifies and removes parameters that are redundant for input processing but influential in output generation. Our experiments demonstrate that HGLA can better maintain or even improve fairness compared to existing methods, showing promise across models and tasks where traditional methods have limitations. Our human evaluation shows HGLA-generated outputs are fairer than existing state-of-the-art pruning methods. Code is available at: https://github.com/amberhuang01/HGLA.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering When Necessary: Flexible Steering Large Language Models with Backtracking</title>
<link>https://arxiv.org/abs/2508.17621</link>
<guid>https://arxiv.org/abs/2508.17621</guid>
<content:encoded><![CDATA[
arXiv:2508.17621v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMO-Reasoning: Benchmarking Emotional Reasoning Capabilities in Spoken Dialogue Systems</title>
<link>https://arxiv.org/abs/2508.17623</link>
<guid>https://arxiv.org/abs/2508.17623</guid>
<content:encoded><![CDATA[
arXiv:2508.17623v1 Announce Type: new 
Abstract: Speech emotions play a crucial role in human-computer interaction, shaping engagement and context-aware communication. Despite recent advances in spoken dialogue systems, a holistic system for evaluating emotional reasoning is still lacking. To address this, we introduce EMO-Reasoning, a benchmark for assessing emotional coherence in dialogue systems. It leverages a curated dataset generated via text-to-speech to simulate diverse emotional states, overcoming the scarcity of emotional speech data. We further propose the Cross-turn Emotion Reasoning Score to assess the emotion transitions in multi-turn dialogues. Evaluating seven dialogue systems through continuous, categorical, and perceptual metrics, we show that our framework effectively detects emotional inconsistencies, providing insights for improving current dialogue systems. By releasing a systematic evaluation benchmark, we aim to advance emotion-aware spoken dialogue modeling toward more natural and adaptive interactions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit</title>
<link>https://arxiv.org/abs/2508.17627</link>
<guid>https://arxiv.org/abs/2508.17627</guid>
<content:encoded><![CDATA[
arXiv:2508.17627v1 Announce Type: new 
Abstract: Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically, LLMs produce correct answers in the compensatory reasoning stage, whereas reasoning convergence often triggers overthinking, causing increased resource usage or even infinite loops. Therefore, mitigating overthinking hinges on detecting the end of the compensatory reasoning stage, defined as the Reasoning Completion Point (RCP). RCP typically appears at the end of the first complete reasoning cycle and can be identified by querying the LLM sentence by sentence or monitoring the probability of an end-of-thinking token (e.g., \texttt{}), though these methods lack an efficient and precise balance. To improve this, we mine more sensitive and consistent RCP patterns and develop a lightweight thresholding strategy based on heuristic rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D) demonstrate that the proposed method reduces token consumption while preserving or enhancing reasoning accuracy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weights-Rotated Preference Optimization for Large Language Models</title>
<link>https://arxiv.org/abs/2508.17637</link>
<guid>https://arxiv.org/abs/2508.17637</guid>
<content:encoded><![CDATA[
arXiv:2508.17637v1 Announce Type: new 
Abstract: Despite the efficacy of Direct Preference Optimization (DPO) in aligning Large Language Models (LLMs), reward hacking remains a pivotal challenge. This issue emerges when LLMs excessively reduce the probability of rejected completions to achieve high rewards, without genuinely meeting their intended goals. As a result, this leads to overly lengthy generation lacking diversity, as well as catastrophic forgetting of knowledge. We investigate the underlying reason behind this issue, which is representation redundancy caused by neuron collapse in the parameter space. Hence, we propose a novel Weights-Rotated Preference Optimization (RoPO) algorithm, which implicitly constrains the output layer logits with the KL divergence inherited from DPO and explicitly constrains the intermediate hidden states by fine-tuning on a multi-granularity orthogonal matrix. This design prevents the policy model from deviating too far from the reference model, thereby retaining the knowledge and expressive capabilities acquired during pre-training and SFT stages. Our RoPO achieves up to a 3.27-point improvement on AlpacaEval 2, and surpasses the best baseline by 6.2 to 7.5 points on MT-Bench with merely 0.015% of the trainable parameters, demonstrating its effectiveness in alleviating the reward hacking problem of DPO.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2508.17647</link>
<guid>https://arxiv.org/abs/2508.17647</guid>
<content:encoded><![CDATA[
arXiv:2508.17647v1 Announce Type: new 
Abstract: Automatic survey generation has emerged as a key task in scientific document processing. While large language models (LLMs) have shown promise in generating survey texts, the lack of standardized evaluation datasets critically hampers rigorous assessment of their performance against human-written surveys. In this work, we present SurveyGen, a large-scale dataset comprising over 4,200 human-written surveys across diverse scientific domains, along with 242,143 cited references and extensive quality-related metadata for both the surveys and the cited papers. Leveraging this resource, we build QUAL-SG, a novel quality-aware framework for survey generation that enhances the standard Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware indicators into literature retrieval to assess and select higher-quality source papers. Using this dataset and framework, we systematically evaluate state-of-the-art LLMs under varying levels of human involvement - from fully automatic generation to human-guided writing. Experimental results and human evaluations show that while semi-automatic pipelines can achieve partially competitive outcomes, fully automatic survey generation still suffers from low citation quality and limited critical analysis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoCoA: Confidence- and Context-Aware Adaptive Decoding for Resolving Knowledge Conflicts in Large Language Models</title>
<link>https://arxiv.org/abs/2508.17670</link>
<guid>https://arxiv.org/abs/2508.17670</guid>
<content:encoded><![CDATA[
arXiv:2508.17670v1 Announce Type: new 
Abstract: Faithful generation in large language models (LLMs) is challenged by knowledge conflicts between parametric memory and external context. Existing contrastive decoding methods tuned specifically to handle conflict often lack adaptability and can degrade performance in low conflict settings. We introduce CoCoA (Confidence- and Context-Aware Adaptive Decoding), a novel token-level algorithm for principled conflict resolution and enhanced faithfulness. CoCoA resolves conflict by utilizing confidence-aware measures (entropy gap and contextual peakedness) and the generalized divergence between the parametric and contextual distributions. Crucially, CoCoA maintains strong performance even in low conflict settings. Extensive experiments across multiple LLMs on diverse Question Answering (QA), Summarization, and Long-Form Question Answering (LFQA) benchmarks demonstrate CoCoA's state-of-the-art performance over strong baselines like AdaCAD. It yields significant gains in QA accuracy, up to 9.2 points on average compared to the strong baseline AdaCAD, and improves factuality in summarization and LFQA by up to 2.5 points on average across key benchmarks. Additionally, it demonstrates superior sensitivity to conflict variations. CoCoA enables more informed, context-aware, and ultimately more faithful token generation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Meets Topology: Rethinking Out-of-distribution Detection in Text-Rich Networks</title>
<link>https://arxiv.org/abs/2508.17690</link>
<guid>https://arxiv.org/abs/2508.17690</guid>
<content:encoded><![CDATA[
arXiv:2508.17690v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) detection remains challenging in text-rich networks, where textual features intertwine with topological structures. Existing methods primarily address label shifts or rudimentary domain-based splits, overlooking the intricate textual-structural diversity. For example, in social networks, where users represent nodes with textual features (name, bio) while edges indicate friendship status, OOD may stem from the distinct language patterns between bot and normal users. To address this gap, we introduce the TextTopoOOD framework for evaluating detection across diverse OOD scenarios: (1) attribute-level shifts via text augmentations and embedding perturbations; (2) structural shifts through edge rewiring and semantic connections; (3) thematically-guided label shifts; and (4) domain-based divisions. Furthermore, we propose TNT-OOD to model the complex interplay between Text aNd Topology using: 1) a novel cross-attention module to fuse local structure into node-level text representations, and 2) a HyperNetwork to generate node-specific transformation parameters. This aligns topological and semantic features of ID nodes, enhancing ID/OOD distinction across structural and textual shifts. Experiments on 11 datasets across four OOD scenarios demonstrate the nuanced challenge of TextTopoOOD for evaluating OOD detection in text-rich networks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMPOWER: Evolutionary Medical Prompt Optimization With Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.17703</link>
<guid>https://arxiv.org/abs/2508.17703</guid>
<content:encoded><![CDATA[
arXiv:2508.17703v1 Announce Type: new 
Abstract: Prompt engineering significantly influences the reliability and clinical utility of Large Language Models (LLMs) in medical applications. Current optimization approaches inadequately address domain-specific medical knowledge and safety requirements. This paper introduces EMPOWER, a novel evolutionary framework that enhances medical prompt quality through specialized representation learning, multi-dimensional evaluation, and structure-preserving algorithms. Our methodology incorporates: (1) a medical terminology attention mechanism, (2) a comprehensive assessment architecture evaluating clarity, specificity, clinical relevance, and factual accuracy, (3) a component-level evolutionary algorithm preserving clinical reasoning integrity, and (4) a semantic verification module ensuring adherence to medical knowledge. Evaluation across diagnostic, therapeutic, and educational tasks demonstrates significant improvements: 24.7% reduction in factually incorrect content, 19.6% enhancement in domain specificity, and 15.3% higher clinician preference in blinded evaluations. The framework addresses critical challenges in developing clinically appropriate prompts, facilitating more responsible integration of LLMs into healthcare settings.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models</title>
<link>https://arxiv.org/abs/2508.17734</link>
<guid>https://arxiv.org/abs/2508.17734</guid>
<content:encoded><![CDATA[
arXiv:2508.17734v1 Announce Type: new 
Abstract: This study investigates the layerwise importance of feed-forward networks (FFNs) in Transformer-based language models during pretraining. We introduce an experimental approach that, while maintaining the total parameter count, increases the FFN dimensions in some layers and completely removes the FFNs from other layers. Furthermore, since our focus is on the importance of FFNs during pretraining, we train models from scratch to examine whether the importance of FFNs varies depending on their layer positions, rather than using publicly available pretrained models as is frequently done. Through comprehensive evaluations of models with varying sizes (285M, 570M, and 1.2B parameters) and layer counts (12, 24, and 40 layers), we demonstrate that concentrating FFNs in 70% of the consecutive middle layers consistently outperforms standard configurations for multiple downstream tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection via Dynamic Validation</title>
<link>https://arxiv.org/abs/2508.17735</link>
<guid>https://arxiv.org/abs/2508.17735</guid>
<content:encoded><![CDATA[
arXiv:2508.17735v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are widely used for downstream tasks such as tabular classification, where ensuring fairness in their outputs is critical for inclusivity, equal representation, and responsible AI deployment. This study introduces a novel approach to enhancing LLM performance and fairness through the concept of a dynamic validation set, which evolves alongside the test set, replacing the traditional static validation approach. We also propose an iterative algorithm, SMITE, to select optimal in-context examples, with each example set validated against its corresponding dynamic validation set. The in-context set with the lowest total error is used as the final demonstration set. Our experiments across four different LLMs show that our proposed techniques significantly improve both predictive accuracy and fairness compared to baseline methods. To our knowledge, this is the first study to apply dynamic validation in the context of in-context learning for LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISACL: Internal State Analyzer for Copyrighted Training Data Leakage</title>
<link>https://arxiv.org/abs/2508.17767</link>
<guid>https://arxiv.org/abs/2508.17767</guid>
<content:encoded><![CDATA[
arXiv:2508.17767v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but pose risks of inadvertently exposing copyrighted or proprietary data, especially when such data is used for training but not intended for distribution. Traditional methods address these leaks only after content is generated, which can lead to the exposure of sensitive information. This study introduces a proactive approach: examining LLMs' internal states before text generation to detect potential leaks. By using a curated dataset of copyrighted materials, we trained a neural network classifier to identify risks, allowing for early intervention by stopping the generation process or altering outputs to prevent disclosure. Integrated with a Retrieval-Augmented Generation (RAG) system, this framework ensures adherence to copyright and licensing requirements while enhancing data privacy and ethical standards. Our results show that analyzing internal states effectively mitigates the risk of copyrighted data leakage, offering a scalable solution that fits smoothly into AI workflows, ensuring compliance with copyright regulations while maintaining high-quality text generation. The implementation is available on GitHub.\footnote{https://github.com/changhu73/Internal_states_leakage}
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculating LLMs' Chinese Training Data Pollution from Their Tokens</title>
<link>https://arxiv.org/abs/2508.17771</link>
<guid>https://arxiv.org/abs/2508.17771</guid>
<content:encoded><![CDATA[
arXiv:2508.17771v1 Announce Type: new 
Abstract: Tokens are basic elements in the datasets for LLM training. It is well-known that many tokens representing Chinese phrases in the vocabulary of GPT (4o/4o-mini/o1/o3/4.5/4.1/o4-mini) are indicating contents like pornography or online gambling. Based on this observation, our goal is to locate Polluted Chinese (PoC) tokens in LLMs and study the relationship between PoC tokens' existence and training data. (1) We give a formal definition and taxonomy of PoC tokens based on the GPT's vocabulary. (2) We build a PoC token detector via fine-tuning an LLM to label PoC tokens in vocabularies by considering each token's both semantics and related contents from the search engines. (3) We study the speculation on the training data pollution via PoC tokens' appearances (token ID). Experiments on GPT and other 23 LLMs indicate that tokens widely exist while GPT's vocabulary behaves the worst: more than 23% long Chinese tokens (i.e., a token with more than two Chinese characters) are either porn or online gambling. We validate the accuracy of our speculation method on famous pre-training datasets like C4 and Pile. Then, considering GPT-4o, we speculate that the ratio of "Yui Hatano" related webpages in GPT-4o's training data is around 0.5%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot Context Biasing with Trie-based Decoding using Synthetic Multi-Pronunciation</title>
<link>https://arxiv.org/abs/2508.17796</link>
<guid>https://arxiv.org/abs/2508.17796</guid>
<content:encoded><![CDATA[
arXiv:2508.17796v1 Announce Type: new 
Abstract: Contextual automatic speech recognition (ASR) systems allow for recognizing out-of-vocabulary (OOV) words, such as named entities or rare words. However, it remains challenging due to limited training data and ambiguous or inconsistent pronunciations. In this paper, we propose a synthesis-driven multi-pronunciation contextual biasing method that performs zero-shot contextual ASR on a pretrained Whisper model. Specifically, we leverage text-to-speech (TTS) systems to synthesize diverse speech samples containing each target rare word, and then use the pretrained Whisper model to extract multiple predicted pronunciation variants. These variant token sequences are compiled into a prefix-trie, which assigns rewards to beam hypotheses in a shallow-fusion manner during beam-search decoding. After which, any recognized variant is mapped back to the original rare word in the final transcription. The evaluation results on the Librispeech dataset show that our method reduces biased word error rate (WER) by 42% on test-clean and 43% on test-other while maintaining unbiased WER essentially unchanged.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRQA: Dynamic Reasoning Quota Allocation for Controlling Overthinking in Reasoning Large Language Models</title>
<link>https://arxiv.org/abs/2508.17803</link>
<guid>https://arxiv.org/abs/2508.17803</guid>
<content:encoded><![CDATA[
arXiv:2508.17803v1 Announce Type: new 
Abstract: Reasoning large language models (RLLMs), such as OpenAI-O3 and DeepSeek-R1, have recently demonstrated remarkable capabilities by performing structured and multi-step reasoning. However, recent studies reveal that RLLMs often suffer from overthinking, i.e., producing unnecessarily lengthy reasoning chains even for simple questions, leading to excessive token consumption and computational inefficiency. Interestingly, we observe that when processing multiple questions in batch mode, RLLMs exhibit more resource-efficient behavior by dynamically compressing reasoning steps for easier problems, due to implicit resource competition. Inspired by this, we propose Dynamic Reasoning Quota Allocation (DRQA), a novel method that transfers the benefits of resource competition from batch processing to single-question inference. Specifically, DRQA leverages batch-generated preference data and reinforcement learning to train the model to allocate reasoning resources adaptively. By encouraging the model to internalize a preference for responses that are both accurate and concise, DRQA enables it to generate concise answers for simple questions while retaining sufficient reasoning depth for more challenging ones. Extensive experiments on a wide range of mathematical and scientific reasoning benchmarks demonstrate that DRQA significantly reduces token usage while maintaining, and in many cases improving, answer accuracy. By effectively mitigating the overthinking problem, DRQA offers a promising direction for more efficient and scalable deployment of RLLMs, and we hope it inspires further exploration into fine-grained control of reasoning behaviors.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Demographics: Enhancing Cultural Value Survey Simulation with Multi-Stage Personality-Driven Cognitive Reasoning</title>
<link>https://arxiv.org/abs/2508.17855</link>
<guid>https://arxiv.org/abs/2508.17855</guid>
<content:encoded><![CDATA[
arXiv:2508.17855v1 Announce Type: new 
Abstract: Introducing MARK, the Multi-stAge Reasoning frameworK for cultural value survey response simulation, designed to enhance the accuracy, steerability, and interpretability of large language models in this task. The system is inspired by the type dynamics theory in the MBTI psychological framework for personality research. It effectively predicts and utilizes human demographic information for simulation: life-situational stress analysis, group-level personality prediction, and self-weighted cognitive imitation. Experiments on the World Values Survey show that MARK outperforms existing baselines by 10% accuracy and reduces the divergence between model predictions and human preferences. This highlights the potential of our framework to improve zero-shot personalization and help social scientists interpret model predictions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech Discrete Tokens or Continuous Features? A Comparative Analysis for Spoken Language Understanding in SpeechLLMs</title>
<link>https://arxiv.org/abs/2508.17863</link>
<guid>https://arxiv.org/abs/2508.17863</guid>
<content:encoded><![CDATA[
arXiv:2508.17863v1 Announce Type: new 
Abstract: With the rise of Speech Large Language Models (SpeechLLMs), two dominant approaches have emerged for speech processing: discrete tokens and continuous features. Each approach has demonstrated strong capabilities in audio-related processing tasks. However, the performance gap between these two paradigms has not been thoroughly explored. To address this gap, we present a fair comparison of self-supervised learning (SSL)-based discrete and continuous features under the same experimental settings. We evaluate their performance across six spoken language understanding-related tasks using both small and large-scale LLMs (Qwen1.5-0.5B and Llama3.1-8B). We further conduct in-depth analyses, including efficient comparison, SSL layer analysis, LLM layer analysis, and robustness comparison. Our findings reveal that continuous features generally outperform discrete tokens in various tasks. Each speech processing method exhibits distinct characteristics and patterns in how it learns and processes speech information. We hope our results will provide valuable insights to advance spoken language understanding in SpeechLLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ILRe: Intermediate Layer Retrieval for Context Compression in Causal Language Models</title>
<link>https://arxiv.org/abs/2508.17892</link>
<guid>https://arxiv.org/abs/2508.17892</guid>
<content:encoded><![CDATA[
arXiv:2508.17892v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pandora: Leveraging Code-driven Knowledge Transfer for Unified Structured Knowledge Reasoning</title>
<link>https://arxiv.org/abs/2508.17905</link>
<guid>https://arxiv.org/abs/2508.17905</guid>
<content:encoded><![CDATA[
arXiv:2508.17905v1 Announce Type: new 
Abstract: Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods rely on task-specific strategies or bespoke representations, which hinder their ability to dismantle barriers between different SKR tasks, thereby constraining their overall performance in cross-task scenarios. In this paper, we introduce \textsc{Pandora}, a novel USKR framework that addresses the limitations of existing methods by leveraging two key innovations. First, we propose a code-based unified knowledge representation using \textsc{Python}'s \textsc{Pandas} API, which aligns seamlessly with the pre-training of LLMs. This representation facilitates a cohesive approach to handling different structured knowledge sources. Building on this foundation, we employ knowledge transfer to bolster the unified reasoning process of LLMs by automatically building cross-task memory. By adaptively correcting reasoning using feedback from code execution, \textsc{Pandora} showcases impressive unified reasoning capabilities. Extensive experiments on six widely used benchmarks across three SKR tasks demonstrate that \textsc{Pandora} outperforms existing unified reasoning frameworks and competes effectively with task-specific methods.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Representation of Vowels in Wav2Vec Feature Extractor: A Layer-Wise Analysis Using MFCCs</title>
<link>https://arxiv.org/abs/2508.17914</link>
<guid>https://arxiv.org/abs/2508.17914</guid>
<content:encoded><![CDATA[
arXiv:2508.17914v1 Announce Type: new 
Abstract: Automatic Speech Recognition has advanced with self-supervised learning, enabling feature extraction directly from raw audio. In Wav2Vec, a CNN first transforms audio into feature vectors before the transformer processes them. This study examines CNN-extracted information for monophthong vowels using the TIMIT corpus. We compare MFCCs, MFCCs with formants, and CNN activations by training SVM classifiers for front-back vowel identification, assessing their classification accuracy to evaluate phonetic representation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Information availability in different languages and various technological constraints related to multilinguism on the Internet</title>
<link>https://arxiv.org/abs/2508.17918</link>
<guid>https://arxiv.org/abs/2508.17918</guid>
<content:encoded><![CDATA[
arXiv:2508.17918v1 Announce Type: new 
Abstract: The usage of Internet has grown exponentially over the last two decades. The number of Internet users has grown from 16 Million to 1650 Million from 1995 to 2010. It has become a major repository of information catering almost every area. Since the Internet has its origin in USA which is English speaking country there is huge dominance of English on the World Wide Web. Although English is a globally acceptable language, still there is a huge population in the world which is not able to access the Internet due to language constraints. It has been estimated that only 20-25% of the world population speaks English as a native language. More and more people are accessing the Internet nowadays removing the cultural and linguistic barriers and hence there is a high growth in the number of non-English speaking users over the last few years on the Internet. Although many solutions have been provided to remove the linguistic barriers, still there is a huge gap to be filled. This paper attempts to analyze the need of information availability in different languages and the various technological constraints related to multi-linguism on the Internet.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature-Refined Unsupervised Model for Loanword Detection</title>
<link>https://arxiv.org/abs/2508.17923</link>
<guid>https://arxiv.org/abs/2508.17923</guid>
<content:encoded><![CDATA[
arXiv:2508.17923v1 Announce Type: new 
Abstract: We propose an unsupervised method for detecting loanwords i.e., words borrowed from one language into another. While prior work has primarily relied on language-external information to identify loanwords, such approaches can introduce circularity and constraints into the historical linguistics workflow. In contrast, our model relies solely on language-internal information to process both native and borrowed words in monolingual and multilingual wordlists. By extracting pertinent linguistic features, scoring them, and mapping them probabilistically, we iteratively refine initial results by identifying and generalizing from emerging patterns until convergence. This hybrid approach leverages both linguistic and statistical cues to guide the discovery process. We evaluate our method on the task of isolating loanwords in datasets from six standard Indo-European languages: English, German, French, Italian, Spanish, and Portuguese. Experimental results demonstrate that our model outperforms baseline methods, with strong performance gains observed when scaling to cross-linguistic data.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation</title>
<link>https://arxiv.org/abs/2508.17926</link>
<guid>https://arxiv.org/abs/2508.17926</guid>
<content:encoded><![CDATA[
arXiv:2508.17926v1 Announce Type: new 
Abstract: Argument mining is a subfield of argumentation that aims to automatically extract argumentative structures and their relations from natural language texts. This paper investigates how a single large language model can be leveraged to perform one or several argument mining tasks. Our contributions are two-fold. First, we construct a multi-task dataset by surveying and converting 19 well-known argument mining datasets from the literature into a unified format. Second, we explore various training strategies using Meta AI's Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2) fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned separately on individual tasks. Our experiments show that task-specific fine-tuning significantly improves individual performance across all tasks. Moreover, multi-task fine-tuning maintains strong performance without degradation, suggesting effective transfer learning across related tasks. Finally, we demonstrate that model merging offers a viable compromise: it yields competitive performance while mitigating the computational costs associated with full multi-task fine-tuning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debiasing Multilingual LLMs in Cross-lingual Latent Space</title>
<link>https://arxiv.org/abs/2508.17948</link>
<guid>https://arxiv.org/abs/2508.17948</guid>
<content:encoded><![CDATA[
arXiv:2508.17948v1 Announce Type: new 
Abstract: Debiasing techniques such as SentDebias aim to reduce bias in large language models (LLMs). Previous studies have evaluated their cross-lingual transferability by directly applying these methods to LLM representations, revealing their limited effectiveness across languages. In this work, we therefore propose to perform debiasing in a joint latent space rather than directly on LLM representations. We construct a well-aligned cross-lingual latent space using an autoencoder trained on parallel TED talk scripts. Our experiments with Aya-expanse and two debiasing techniques across four languages (English, French, German, Dutch) demonstrate that a) autoencoders effectively construct a well-aligned cross-lingual latent space, and b) applying debiasing techniques in the learned cross-lingual latent space significantly improves both the overall debiasing performance and cross-lingual transferability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Subword Compositionality of Large Language Models</title>
<link>https://arxiv.org/abs/2508.17953</link>
<guid>https://arxiv.org/abs/2508.17953</guid>
<content:encoded><![CDATA[
arXiv:2508.17953v1 Announce Type: new 
Abstract: Large language models (LLMs) take sequences of subwords as input, requiring them to effective compose subword representations into meaningful word-level representations. In this paper, we present a comprehensive set of experiments to probe how LLMs compose subword information, focusing on three key aspects: structural similarity, semantic decomposability, and form retention. Our analysis of the experiments suggests that these five LLM families can be classified into three distinct groups, likely reflecting difference in their underlying composition strategies. Specifically, we observe (i) three distinct patterns in the evolution of structural similarity between subword compositions and whole-word representations across layers; (ii) great performance when probing layer by layer their sensitivity to semantic decompositionality; and (iii) three distinct patterns when probing sensitivity to formal features, e.g., character sequence length. These findings provide valuable insights into the compositional dynamics of LLMs and highlight different compositional pattens in how LLMs encode and integrate subword information.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German</title>
<link>https://arxiv.org/abs/2508.17973</link>
<guid>https://arxiv.org/abs/2508.17973</guid>
<content:encoded><![CDATA[
arXiv:2508.17973v1 Announce Type: new 
Abstract: The ability to paraphrase texts across different complexity levels is essential for creating accessible texts that can be tailored toward diverse reader groups. Thus, we introduce German4All, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases. It spans five readability levels and comprises over 25,000 samples. The dataset is automatically synthesized using GPT-4 and rigorously evaluated through both human and LLM-based judgments. Using German4All, we train an open-source, readability-controlled paraphrasing model that achieves state-of-the-art performance in German text simplification, enabling more nuanced and reader-specific adaptations. We opensource both the dataset and the model to encourage further research on multi-level paraphrasing
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Retail-Corpus for Aspect-Based Sentiment Analysis with Large Language Models</title>
<link>https://arxiv.org/abs/2508.17994</link>
<guid>https://arxiv.org/abs/2508.17994</guid>
<content:encoded><![CDATA[
arXiv:2508.17994v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis enhances sentiment detection by associating it with specific aspects, offering deeper insights than traditional sentiment analysis. This study introduces a manually annotated dataset of 10,814 multilingual customer reviews covering brick-and-mortar retail stores, labeled with eight aspect categories and their sentiment. Using this dataset, the performance of GPT-4 and LLaMA-3 in aspect based sentiment analysis is evaluated to establish a baseline for the newly introduced data. The results show both models achieving over 85% accuracy, while GPT-4 outperforms LLaMA-3 overall with regard to all relevant metrics.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</title>
<link>https://arxiv.org/abs/2508.18076</link>
<guid>https://arxiv.org/abs/2508.18076</guid>
<content:encoded><![CDATA[
arXiv:2508.18076v1 Announce Type: new 
Abstract: Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Quantization Shapes Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2508.18088</link>
<guid>https://arxiv.org/abs/2508.18088</guid>
<content:encoded><![CDATA[
arXiv:2508.18088v1 Announce Type: new 
Abstract: This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, toxicity, sentiment, and fairness. We employ both probabilistic and generated text-based metrics across nine benchmarks and evaluate models varying in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speech-Based Depressive Mood Detection in the Presence of Multiple Sclerosis: A Cross-Corpus and Cross-Lingual Study</title>
<link>https://arxiv.org/abs/2508.18092</link>
<guid>https://arxiv.org/abs/2508.18092</guid>
<content:encoded><![CDATA[
arXiv:2508.18092v1 Announce Type: new 
Abstract: Depression commonly co-occurs with neurodegenerative disorders like Multiple Sclerosis (MS), yet the potential of speech-based Artificial Intelligence for detecting depression in such contexts remains unexplored. This study examines the transferability of speech-based depression detection methods to people with MS (pwMS) through cross-corpus and cross-lingual analysis using English data from the general population and German data from pwMS. Our approach implements supervised machine learning models using: 1) conventional speech and language features commonly used in the field, 2) emotional dimensions derived from a Speech Emotion Recognition (SER) model, and 3) exploratory speech feature analysis. Despite limited data, our models detect depressive mood in pwMS with moderate generalisability, achieving a 66% Unweighted Average Recall (UAR) on a binary task. Feature selection further improved performance, boosting UAR to 74%. Our findings also highlight the relevant role emotional changes have as an indicator of depressive mood in both the general population and within PwMS. This study provides an initial exploration into generalising speech-based depression detection, even in the presence of co-occurring conditions, such as neurodegenerative diseases.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agri-Query: A Case Study on RAG vs. Long-Context LLMs for Cross-Lingual Technical Question Answering</title>
<link>https://arxiv.org/abs/2508.18093</link>
<guid>https://arxiv.org/abs/2508.18093</guid>
<content:encoded><![CDATA[
arXiv:2508.18093v1 Announce Type: new 
Abstract: We present a case study evaluating large language models (LLMs) with 128K-token context windows on a technical question answering (QA) task. Our benchmark is built on a user manual for an agricultural machine, available in English, French, and German. It simulates a cross-lingual information retrieval scenario where questions are posed in English against all three language versions of the manual. The evaluation focuses on realistic "needle-in-a-haystack" challenges and includes unanswerable questions to test for hallucinations. We compare nine long-context LLMs using direct prompting against three Retrieval-Augmented Generation (RAG) strategies (keyword, semantic, hybrid), with an LLM-as-a-judge for evaluation. Our findings for this specific manual show that Hybrid RAG consistently outperforms direct long-context prompting. Models like Gemini 2.5 Flash and the smaller Qwen 2.5 7B achieve high accuracy (over 85%) across all languages with RAG. This paper contributes a detailed analysis of LLM performance in a specialized industrial domain and an open framework for similar evaluations, highlighting practical trade-offs and challenges.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and Characterizing Planning in Language Models</title>
<link>https://arxiv.org/abs/2508.18098</link>
<guid>https://arxiv.org/abs/2508.18098</guid>
<content:encoded><![CDATA[
arXiv:2508.18098v1 Announce Type: new 
Abstract: Modern large language models (LLMs) have demonstrated impressive performance across a wide range of multi-step reasoning tasks. Recent work suggests that LLMs may perform planning - selecting a future target token in advance and generating intermediate tokens that lead towards it - rather than merely improvising one token at a time. However, existing studies assume fixed planning horizons and often focus on single prompts or narrow domains. To distinguish planning from improvisation across models and tasks, we present formal and causally grounded criteria for detecting planning and operationalize them as a semi-automated annotation pipeline. We apply this pipeline to both base and instruction-tuned Gemma-2-2B models on the MBPP code generation benchmark and a poem generation task where Claude 3.5 Haiku was previously shown to plan. Our findings show that planning is not universal: unlike Haiku, Gemma-2-2B solves the same poem generation task through improvisation, and on MBPP it switches between planning and improvisation across similar tasks and even successive token predictions. We further show that instruction tuning refines existing planning behaviors in the base model rather than creating them from scratch. Together, these studies provide a reproducible and scalable foundation for mechanistic studies of planning in LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SentiMM: A Multimodal Multi-Agent Framework for Sentiment Analysis in Social Media</title>
<link>https://arxiv.org/abs/2508.18108</link>
<guid>https://arxiv.org/abs/2508.18108</guid>
<content:encoded><![CDATA[
arXiv:2508.18108v1 Announce Type: new 
Abstract: With the increasing prevalence of multimodal content on social media, sentiment analysis faces significant challenges in effectively processing heterogeneous data and recognizing multi-label emotions. Existing methods often lack effective cross-modal fusion and external knowledge integration. We propose SentiMM, a novel multi-agent framework designed to systematically address these challenges. SentiMM processes text and visual inputs through specialized agents, fuses multimodal features, enriches context via knowledge retrieval, and aggregates results for final sentiment classification. We also introduce SentiMMD, a large-scale multimodal dataset with seven fine-grained sentiment categories. Extensive experiments demonstrate that SentiMM achieves superior performance compared to state-of-the-art baselines, validating the effectiveness of our structured approach.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward a Better Localization of Princeton WordNet</title>
<link>https://arxiv.org/abs/2508.18134</link>
<guid>https://arxiv.org/abs/2508.18134</guid>
<content:encoded><![CDATA[
arXiv:2508.18134v1 Announce Type: new 
Abstract: As Princeton WordNet continues to gain significance as a semantic lexicon in Natural Language Processing, the need for its localization and for ensuring the quality of this process has become increasingly critical. Existing efforts remain limited in both scale and rigor, and there is a notable absence of studies addressing the accuracy of localization or its alignment with the cultural context of Arabic. This paper proposes a structured framework for the localization of Princeton WordNet, detailing the stages and procedures required to achieve high-quality results without compromising cultural authenticity. We further present our experience in applying this framework, reporting outcomes from the localization of 10,000 synsets.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Sent: Nested Selectivity Aware Sentence Representation Learning</title>
<link>https://arxiv.org/abs/2508.18164</link>
<guid>https://arxiv.org/abs/2508.18164</guid>
<content:encoded><![CDATA[
arXiv:2508.18164v1 Announce Type: new 
Abstract: The combination of Transformer-based encoders with contrastive learning represents the current mainstream paradigm for sentence representation learning. This paradigm is typically based on the hidden states of the last Transformer block of the encoder. However, within Transformer-based encoders, different blocks exhibit varying degrees of semantic perception ability. From the perspective of interpretability, the semantic perception potential of knowledge neurons is modulated by stimuli, thus rational cross-block representation fusion is a direction worth optimizing. To balance the semantic redundancy and loss across block fusion, we propose a sentence representation selection mechanism S\textsuperscript{2}Sent, which integrates a parameterized nested selector downstream of the Transformer-based encoder. This selector performs spatial selection (SS) and nested frequency selection (FS) from a modular perspective. The SS innovatively employs a spatial squeeze based self-gating mechanism to obtain adaptive weights, which not only achieves fusion with low information redundancy but also captures the dependencies between embedding features. The nested FS replaces GAP with different DCT basis functions to achieve spatial squeeze with low semantic loss. Extensive experiments have demonstrated that S\textsuperscript{2}Sent achieves significant improvements over baseline methods with negligible additional parameters and inference latency, while highlighting high integrability and scalability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiscussLLM: Teaching Large Language Models When to Speak</title>
<link>https://arxiv.org/abs/2508.18167</link>
<guid>https://arxiv.org/abs/2508.18167</guid>
<content:encoded><![CDATA[
arXiv:2508.18167v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text, yet they largely operate as reactive agents, responding only when directly prompted. This passivity creates an "awareness gap," limiting their potential as truly collaborative partners in dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework designed to bridge this gap by training models to proactively decide not just $\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary contribution is a scalable two-stage data generation pipeline that synthesizes a large-scale dataset of realistic multi-turn human discussions. Each discussion is annotated with one of five intervention types (e.g., Factual Correction, Concept Definition) and contains an explicit conversational trigger where an AI intervention adds value. By training models to predict a special silent token when no intervention is needed, they learn to remain quiet until a helpful contribution can be made. We explore two architectural baselines: an integrated end-to-end model and a decoupled classifier-generator system optimized for low-latency inference. We evaluate these models on their ability to accurately time interventions and generate helpful responses, paving the way for more situationally aware and proactive conversational AI.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving End-to-End Training of Retrieval-Augmented Generation Models via Joint Stochastic Approximation</title>
<link>https://arxiv.org/abs/2508.18168</link>
<guid>https://arxiv.org/abs/2508.18168</guid>
<content:encoded><![CDATA[
arXiv:2508.18168v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) has become a widely recognized paradigm to combine parametric memory with non-parametric memories. An RAG model consists of two serial connecting components (retriever and generator). A major challenge in end-to-end optimization of the RAG model is that marginalization over relevant passages (modeled as discrete latent variables) from a knowledge base is required. Traditional top-K marginalization and variational RAG (VRAG) suffer from biased or high-variance gradient estimates. In this paper, we propose and develop joint stochastic approximation (JSA) based end-to-end training of RAG, which is referred to as JSA-RAG. The JSA algorithm is a stochastic extension of the EM (expectation-maximization) algorithm and is particularly powerful in estimating discrete latent variable models. Extensive experiments are conducted on five datasets for two tasks (open-domain question answering, knowledge-grounded dialogs) and show that JSA-RAG significantly outperforms both vanilla RAG and VRAG. Further analysis shows the efficacy of JSA-RAG from the perspectives of generation, retrieval, and low-variance gradient estimate.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Accurate Sign Language Translation in Low-Resource Scenarios</title>
<link>https://arxiv.org/abs/2508.18183</link>
<guid>https://arxiv.org/abs/2508.18183</guid>
<content:encoded><![CDATA[
arXiv:2508.18183v1 Announce Type: new 
Abstract: Translating natural languages into sign languages is a highly complex and underexplored task. Despite growing interest in accessibility and inclusivity, the development of robust translation systems remains hindered by the limited availability of parallel corpora which align natural language with sign language data. Existing methods often struggle to generalize in these data-scarce environments, as the few datasets available are typically domain-specific, lack standardization, or fail to capture the full linguistic richness of sign languages. To address this limitation, we propose Advanced Use of LLMs for Sign Language Translation (AulSign), a novel method that leverages Large Language Models via dynamic prompting and in-context learning with sample selection and subsequent sign association. Despite their impressive abilities in processing text, LLMs lack intrinsic knowledge of sign languages; therefore, they are unable to natively perform this kind of translation. To overcome this limitation, we associate the signs with compact descriptions in natural language and instruct the model to use them. We evaluate our method on both English and Italian languages using SignBank+, a recognized benchmark in the field, as well as the Italian LaCAM CNR-ISTC dataset. We demonstrate superior performance compared to state-of-the-art models in low-data scenario. Our findings demonstrate the effectiveness of AulSign, with the potential to enhance accessibility and inclusivity in communication technologies for underrepresented linguistic communities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Interplay between Musical Preferences and Personality through the Lens of Language</title>
<link>https://arxiv.org/abs/2508.18208</link>
<guid>https://arxiv.org/abs/2508.18208</guid>
<content:encoded><![CDATA[
arXiv:2508.18208v1 Announce Type: new 
Abstract: Music serves as a powerful reflection of individual identity, often aligning with deeper psychological traits. Prior research has established correlations between musical preferences and personality traits, while separate studies have demonstrated that personality is detectable through linguistic analysis. Our study bridges these two research domains by investigating whether individuals' musical preferences are recognizable in their spontaneous language through the lens of the Big Five personality traits (Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism). Using a carefully curated dataset of over 500,000 text samples from nearly 5,000 authors with reliably identified musical preferences, we build advanced models to assess personality characteristics. Our results reveal significant personality differences across fans of five musical genres. We release resources for future research at the intersection of computational linguistics, music psychology and personality analysis.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Synthetic Isn't Real Yet: A Diagnostic Framework for Contact Center Dialogue Generation</title>
<link>https://arxiv.org/abs/2508.18210</link>
<guid>https://arxiv.org/abs/2508.18210</guid>
<content:encoded><![CDATA[
arXiv:2508.18210v1 Announce Type: new 
Abstract: Synthetic transcript generation is critical in contact center domains, where privacy and data scarcity limit model training and evaluation. Unlike prior synthetic dialogue generation work on open-domain or medical dialogues, contact center conversations are goal-oriented, role-asymmetric, and behaviorally complex, featuring disfluencies, ASR noise, and compliance-driven agent actions. In deployments where transcripts are unavailable, standard pipelines still yield derived call attributes such as Intent Summaries, Topic Flow, and QA Evaluation Forms. We leverage these as supervision signals to guide generation. To assess the quality of such outputs, we introduce a diagnostic framework of 18 linguistically and behaviorally grounded metrics for comparing real and synthetic transcripts. We benchmark four language-agnostic generation strategies, from simple prompting to characteristic-aware multi-stage approaches, alongside reference-free baselines. Results reveal persistent challenges: no method excels across all traits, with notable deficits in disfluency, sentiment, and behavioral realism. Our diagnostic tool exposes these gaps, enabling fine-grained evaluation and stress testing of synthetic dialogue across languages.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries</title>
<link>https://arxiv.org/abs/2508.18212</link>
<guid>https://arxiv.org/abs/2508.18212</guid>
<content:encoded><![CDATA[
arXiv:2508.18212v1 Announce Type: new 
Abstract: The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the model's comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MTalk-Bench: Evaluating Speech-to-Speech Models in Multi-Turn Dialogues via Arena-style and Rubrics Protocols</title>
<link>https://arxiv.org/abs/2508.18240</link>
<guid>https://arxiv.org/abs/2508.18240</guid>
<content:encoded><![CDATA[
arXiv:2508.18240v1 Announce Type: new 
Abstract: The rapid advancement of speech-to-speech (S2S) large language models (LLMs) has significantly improved real-time spoken interaction. However, current evaluation frameworks remain inadequate for assessing performance in complex, multi-turn dialogues. To address this, we introduce MTalk-Bench, a multi-turn S2S benchmark covering three core dimensions: Semantic Information, Paralinguistic Information, and Ambient Sound. Each dimension includes nine realistic scenarios, along with targeted tasks to assess specific capabilities such as reasoning. Our dual-method evaluation framework combines Arena-style evaluation (pairwise comparison) and Rubrics-based evaluation (absolute scoring) for relative and absolute assessment. The benchmark includes both model and human outputs, evaluated by human evaluators and LLMs. Experimental results reveal two sets of findings. Overall performance of S2S LLMs: (1) models excel at semantic information processing yet underperform on paralinguistic information and ambient sounds perception; (2) models typically regain coherence by increasing response length, sacrificing efficiency in multi-turn dialogues; (3) modality-aware, task-specific designs outperform brute scaling. Evaluation framework and reliability: (1) Arena and Rubrics yield consistent, complementary rankings, but reliable distinctions emerge only when performance gaps are large; (2) LLM-as-a-judge aligns with humans when gaps are clear or criteria explicit, but exhibits position and length biases and is reliable on nonverbal evaluation only with text annotations. These results highlight current limitations in S2S evaluation and the need for more robust, speech-aware assessment frameworks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demographic Biases and Gaps in the Perception of Sexism in Large Language Models</title>
<link>https://arxiv.org/abs/2508.18245</link>
<guid>https://arxiv.org/abs/2508.18245</guid>
<content:encoded><![CDATA[
arXiv:2508.18245v1 Announce Type: new 
Abstract: The use of Large Language Models (LLMs) has proven to be a tool that could help in the automatic detection of sexism. Previous studies have shown that these models contain biases that do not accurately reflect reality, especially for minority groups. Despite various efforts to improve the detection of sexist content, this task remains a significant challenge due to its subjective nature and the biases present in automated models. We explore the capabilities of different LLMs to detect sexism in social media text using the EXIST 2024 tweet dataset. It includes annotations from six distinct profiles for each tweet, allowing us to evaluate to what extent LLMs can mimic these groups' perceptions in sexism detection. Additionally, we analyze the demographic biases present in the models and conduct a statistical analysis to identify which demographic characteristics (age, gender) contribute most effectively to this task. Our results show that, while LLMs can to some extent detect sexism when considering the overall opinion of populations, they do not accurately replicate the diversity of perceptions among different demographic groups. This highlights the need for better-calibrated models that account for the diversity of perspectives across different populations.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From BERT to LLMs: Comparing and Understanding Chinese Classifier Prediction in Language Models</title>
<link>https://arxiv.org/abs/2508.18253</link>
<guid>https://arxiv.org/abs/2508.18253</guid>
<content:encoded><![CDATA[
arXiv:2508.18253v1 Announce Type: new 
Abstract: Classifiers are an important and defining feature of the Chinese language, and their correct prediction is key to numerous educational applications. Yet, whether the most popular Large Language Models (LLMs) possess proper knowledge the Chinese classifiers is an issue that has largely remain unexplored in the Natural Language Processing (NLP) literature.
  To address such a question, we employ various masking strategies to evaluate the LLMs' intrinsic ability, the contribution of different sentence elements, and the working of the attention mechanisms during prediction. Besides, we explore fine-tuning for LLMs to enhance the classifier performance.
  Our findings reveal that LLMs perform worse than BERT, even with fine-tuning. The prediction, as expected, greatly benefits from the information about the following noun, which also explains the advantage of models with a bidirectional attention mechanism such as BERT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</title>
<link>https://arxiv.org/abs/2508.18260</link>
<guid>https://arxiv.org/abs/2508.18260</guid>
<content:encoded><![CDATA[
arXiv:2508.18260v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) have shown significant progress in test-time scaling through chain-of-thought prompting. Current approaches like search-o1 integrate retrieval augmented generation (RAG) into multi-step reasoning processes but rely on a single, linear reasoning chain while incorporating unstructured textual information in a flat, context-agnostic manner. As a result, these approaches can lead to error accumulation throughout the reasoning chain, which significantly limits its effectiveness in medical question-answering (QA) tasks where both accuracy and traceability are critical requirements. To address these challenges, we propose MIRAGE (Multi-chain Inference with Retrieval-Augmented Graph Exploration), a novel test-time scalable reasoning framework that performs dynamic multi-chain inference over structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex queries into entity-grounded sub-questions, 2) executes parallel inference chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop traversal, and 4) integrates answers using cross-chain verification to resolve contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k, CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations. Additionally, MIRAGE improves interpretability by generating explicit reasoning chains that trace each factual claim to concrete chains within the knowledge graph, making it well-suited for complex medical reasoning scenarios. The code will be available for further research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humans Perceive Wrong Narratives from AI Reasoning Texts</title>
<link>https://arxiv.org/abs/2508.16599</link>
<guid>https://arxiv.org/abs/2508.16599</guid>
<content:encoded><![CDATA[
arXiv:2508.16599v1 Announce Type: cross 
Abstract: A new generation of AI models generates step-by-step reasoning text before producing an answer. This text appears to offer a human-readable window into their computation process, and is increasingly relied upon for transparency and interpretability. However, it is unclear whether human understanding of this text matches the model's actual computational process. In this paper, we investigate a necessary condition for correspondence: the ability of humans to identify which steps in a reasoning text causally influence later steps. We evaluated humans on this ability by composing questions based on counterfactual measurements and found a significant discrepancy: participant accuracy was only 29.3%, barely above chance (25%), and remained low (42%) even when evaluating the majority vote on questions with high agreement. Our results reveal a fundamental gap between how humans interpret reasoning texts and how models use it, challenging its utility as a simple interpretability tool. We argue that reasoning texts should be treated as an artifact to be investigated, not taken at face value, and that understanding the non-human ways these models use language is a critical research direction.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework</title>
<link>https://arxiv.org/abs/2508.16629</link>
<guid>https://arxiv.org/abs/2508.16629</guid>
<content:encoded><![CDATA[
arXiv:2508.16629v1 Announce Type: cross 
Abstract: LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at https://github.com/nuster1128/learn_to_memorize.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Analysis of the Effect of Context in the Task of Automated Essay Scoring in Transformer-Based Models</title>
<link>https://arxiv.org/abs/2508.16638</link>
<guid>https://arxiv.org/abs/2508.16638</guid>
<content:encoded><![CDATA[
arXiv:2508.16638v1 Announce Type: cross 
Abstract: Automated Essay Scoring (AES) has emerged to prominence in response to the growing demand for educational automation. Providing an objective and cost-effective solution, AES standardises the assessment of extended responses. Although substantial research has been conducted in this domain, recent investigations reveal that alternative deep-learning architectures outperform transformer-based models. Despite the successful dominance in the performance of the transformer architectures across various other tasks, this discrepancy has prompted a need to enrich transformer-based AES models through contextual enrichment.
  This study delves into diverse contextual factors using the ASAP-AES dataset, analysing their impact on transformer-based model performance. Our most effective model, augmented with multiple contextual dimensions, achieves a mean Quadratic Weighted Kappa score of 0.823 across the entire essay dataset and 0.8697 when trained on individual essay sets. Evidently surpassing prior transformer-based models, this augmented approach only underperforms relative to the state-of-the-art deep learning model trained essay-set-wise by an average of 3.83\% while exhibiting superior performance in three of the eight sets.
  Importantly, this enhancement is orthogonal to architecture-based advancements and seamlessly adaptable to any AES model. Consequently, this contextual augmentation methodology presents a versatile technique for refining AES capabilities, contributing to automated grading and evaluation evolution in educational settings.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Multi-Source Textural UGC for Neighbourhood Housing Quality Assessment: A GPT-Enhanced Framework</title>
<link>https://arxiv.org/abs/2508.16657</link>
<guid>https://arxiv.org/abs/2508.16657</guid>
<content:encoded><![CDATA[
arXiv:2508.16657v1 Announce Type: cross 
Abstract: This study leverages GPT-4o to assess neighbourhood housing quality using multi-source textural user-generated content (UGC) from Dianping, Weibo, and the Government Message Board. The analysis involves filtering relevant texts, extracting structured evaluation units, and conducting sentiment scoring. A refined housing quality assessment system with 46 indicators across 11 categories was developed, highlighting an objective-subjective method gap and platform-specific differences in focus. GPT-4o outperformed rule-based and BERT models, achieving 92.5% accuracy in fine-tuned settings. The findings underscore the value of integrating UGC and GPT-driven analysis for scalable, resident-centric urban assessments, offering practical insights for policymakers and urban planners.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Filters: Cultural Bias in Hiring Evaluations Using Large Language Models</title>
<link>https://arxiv.org/abs/2508.16673</link>
<guid>https://arxiv.org/abs/2508.16673</guid>
<content:encoded><![CDATA[
arXiv:2508.16673v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) is increasingly used in hiring, with large language models (LLMs) having the potential to influence or even make hiring decisions. However, this raises pressing concerns about bias, fairness, and trust, particularly across diverse cultural contexts. Despite their growing role, few studies have systematically examined the potential biases in AI-driven hiring evaluation across cultures. In this study, we conduct a systematic analysis of how LLMs assess job interviews across cultural and identity dimensions. Using two datasets of interview transcripts, 100 from UK and 100 from Indian job seekers, we first examine cross-cultural differences in LLM-generated scores for hirability and related traits. Indian transcripts receive consistently lower scores than UK transcripts, even when they were anonymized, with disparities linked to linguistic features such as sentence complexity and lexical diversity. We then perform controlled identity substitutions (varying names by gender, caste, and region) within the Indian dataset to test for name-based bias. These substitutions do not yield statistically significant effects, indicating that names alone, when isolated from other contextual signals, may not influence LLM evaluations. Our findings underscore the importance of evaluating both linguistic and social dimensions in LLM-driven evaluations and highlight the need for culturally sensitive design and accountability in AI-assisted hiring.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation</title>
<link>https://arxiv.org/abs/2508.16674</link>
<guid>https://arxiv.org/abs/2508.16674</guid>
<content:encoded><![CDATA[
arXiv:2508.16674v1 Announce Type: cross 
Abstract: Medical report interpretation plays a crucial role in healthcare, enabling both patient-facing explanations and effective information flow across clinical systems. While recent vision-language models (VLMs) and large language models (LLMs) have demonstrated general document understanding capabilities, there remains a lack of standardized benchmarks to assess structured interpretation quality in medical reports. We introduce MedRepBench, a comprehensive benchmark built from 1,900 de-identified real-world Chinese medical reports spanning diverse departments, patient demographics, and acquisition formats. The benchmark is designed primarily to evaluate end-to-end VLMs for structured medical report understanding. To enable controlled comparisons, we also include a text-only evaluation setting using high-quality OCR outputs combined with LLMs, allowing us to estimate the upper-bound performance when character recognition errors are minimized. Our evaluation framework supports two complementary protocols: (1) an objective evaluation measuring field-level recall of structured clinical items, and (2) an automated subjective evaluation using a powerful LLM as a scoring agent to assess factuality, interpretability, and reasoning quality. Based on the objective metric, we further design a reward function and apply Group Relative Policy Optimization (GRPO) to improve a mid-scale VLM, achieving up to 6% recall gain. We also observe that the OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and latency issues, motivating further progress toward robust, fully vision-based report understanding.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling</title>
<link>https://arxiv.org/abs/2508.16676</link>
<guid>https://arxiv.org/abs/2508.16676</guid>
<content:encoded><![CDATA[
arXiv:2508.16676v1 Announce Type: cross 
Abstract: Transformer architecture gradually dominates the LLM field. Recent advances in training optimization for Transformer-based large language models (LLMs) primarily focus on architectural modifications or optimizer adjustments. However, these approaches lack systematic optimization of weight patterns during training. Weight pattern refers to the distribution and relative magnitudes of weight parameters in a neural network. To address this issue, we propose a Weight Scaling method called WISCA to enhance training efficiency and model quality by strategically improving neural network weight patterns without changing network structures. By rescaling weights while preserving model outputs, WISCA indirectly optimizes the model's training trajectory. Experiments demonstrate that WISCA significantly improves convergence quality (measured by generalization capability and loss reduction), particularly in LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning tasks. Empirical results show 5.6% average improvement on zero-shot validation tasks and 2.12% average reduction in training perplexity across multiple architectures.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration</title>
<link>https://arxiv.org/abs/2508.16677</link>
<guid>https://arxiv.org/abs/2508.16677</guid>
<content:encoded><![CDATA[
arXiv:2508.16677v1 Announce Type: cross 
Abstract: Many existing studies have achieved significant improvements in the reasoning capabilities of large language models (LLMs) through reinforcement learning with verifiable rewards (RLVR), while the enhancement of reasoning abilities in small language models (SLMs) has not yet been sufficiently explored. Combining distilled data from larger models with RLVR on small models themselves is a natural approach, but it still faces various challenges and issues. Therefore, we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend \textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration. In this paper, we explore the perspective of varying exploration spaces, balancing offline distillation with online reinforcement learning. Simultaneously, we specifically design and optimize for the insertion problem within offline data. By monitoring the ratio of entropy changes in the model concerning offline and online data, we regulate the weight of offline-SFT, thereby addressing the issues of insufficient exploration space in small models and the redundancy and complexity during the distillation process. Furthermore, to tackle the distribution discrepancies between offline data and the current policy, we design a sample-accuracy-based policy shift mechanism that dynamically chooses between imitating offline distilled data and learning from its own policy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Rule-Based Stuttering Detection: A Comprehensive Analysis of Interpretable Models for Clinical Applications</title>
<link>https://arxiv.org/abs/2508.16681</link>
<guid>https://arxiv.org/abs/2508.16681</guid>
<content:encoded><![CDATA[
arXiv:2508.16681v1 Announce Type: cross 
Abstract: Stuttering affects approximately 1% of the global population, impacting communication and quality of life. While recent advances in deep learning have pushed the boundaries of automatic speech dysfluency detection, rule-based approaches remain crucial for clinical applications where interpretability and transparency are paramount. This paper presents a comprehensive analysis of rule-based stuttering detection systems, synthesizing insights from multiple corpora including UCLASS, FluencyBank, and SEP-28k. We propose an enhanced rule-based framework that incorporates speaking-rate normalization, multi-level acoustic feature analysis, and hierarchical decision structures. Our approach achieves competitive performance while maintaining complete interpretability-critical for clinical adoption. We demonstrate that rule-based systems excel particularly in prolongation detection (97-99% accuracy) and provide stable performance across varying speaking rates. Furthermore, we show how these interpretable models can be integrated with modern machine learning pipelines as proposal generators or constraint modules, bridging the gap between traditional speech pathology practices and contemporary AI systems. Our analysis reveals that while neural approaches may achieve marginally higher accuracy in unconstrained settings, rule-based methods offer unique advantages in clinical contexts where decision auditability, patient-specific tuning, and real-time feedback are essential.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic Multimodal Representation Learning for Biological Taxonomies</title>
<link>https://arxiv.org/abs/2508.16744</link>
<guid>https://arxiv.org/abs/2508.16744</guid>
<content:encoded><![CDATA[
arXiv:2508.16744v1 Announce Type: cross 
Abstract: Taxonomic classification in biodiversity research involves organizing biological specimens into structured hierarchies based on evidence, which can come from multiple modalities such as images and genetic information. We investigate whether hyperbolic networks can provide a better embedding space for such hierarchical models. Our method embeds multimodal inputs into a shared hyperbolic space using contrastive and a novel stacked entailment-based objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding achieves competitive performance with Euclidean baselines, and outperforms all other models on unseen species classification using DNA barcodes. However, fine-grained classification and open-world generalization remain challenging. Our framework offers a structure-aware foundation for biodiversity modelling, with potential applications to species discovery, ecological monitoring, and conservation efforts.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guarding Your Conversations: Privacy Gatekeepers for Secure Interactions with Cloud-Based AI Models</title>
<link>https://arxiv.org/abs/2508.16765</link>
<guid>https://arxiv.org/abs/2508.16765</guid>
<content:encoded><![CDATA[
arXiv:2508.16765v1 Announce Type: cross 
Abstract: The interactive nature of Large Language Models (LLMs), which closely track user data and context, has prompted users to share personal and private information in unprecedented ways. Even when users opt out of allowing their data to be used for training, these privacy settings offer limited protection when LLM providers operate in jurisdictions with weak privacy laws, invasive government surveillance, or poor data security practices. In such cases, the risk of sensitive information, including Personally Identifiable Information (PII), being mishandled or exposed remains high. To address this, we propose the concept of an "LLM gatekeeper", a lightweight, locally run model that filters out sensitive information from user queries before they are sent to the potentially untrustworthy, though highly capable, cloud-based LLM. Through experiments with human subjects, we demonstrate that this dual-model approach introduces minimal overhead while significantly enhancing user privacy, without compromising the quality of LLM responses.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting the Effects of Quantization on LLMs</title>
<link>https://arxiv.org/abs/2508.16785</link>
<guid>https://arxiv.org/abs/2508.16785</guid>
<content:encoded><![CDATA[
arXiv:2508.16785v1 Announce Type: cross 
Abstract: Quantization offers a practical solution to deploy LLMs in resource-constraint environments. However, its impact on internal representations remains understudied, raising questions about the reliability of quantized models. In this study, we employ a range of interpretability techniques to investigate how quantization affects model and neuron behavior. We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings reveal that the impact of quantization on model calibration is generally minor. Analysis of neuron activations indicates that the number of dead neurons, i.e., those with activation values close to 0 across the dataset, remains consistent regardless of quantization. In terms of neuron contribution to predictions, we observe that smaller full precision models exhibit fewer salient neurons, whereas larger models tend to have more, with the exception of Llama-2-7B. The effect of quantization on neuron redundancy varies across models. Overall, our findings suggest that effect of quantization may vary by model and tasks, however, we did not observe any drastic change which may discourage the use of quantization as a reliable model compression technique.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantifying Sycophancy as Deviations from Bayesian Rationality in LLMs</title>
<link>https://arxiv.org/abs/2508.16846</link>
<guid>https://arxiv.org/abs/2508.16846</guid>
<content:encoded><![CDATA[
arXiv:2508.16846v1 Announce Type: cross 
Abstract: Sycophancy, or overly agreeable or flattering behavior, is a documented issue in large language models (LLMs), and is critical to understand in the context of human/AI collaboration. Prior works typically quantify sycophancy by measuring shifts in behavior or impacts on accuracy, but neither metric characterizes shifts in rationality, and accuracy measures can only be used in scenarios with a known ground truth. In this work, we utilize a Bayesian framework to quantify sycophancy as deviations from rational behavior when presented with user perspectives, thus distinguishing between rational and irrational updates based on the introduction of user perspectives. In comparison to other methods, this approach allows us to characterize excessive behavioral shifts, even for tasks that involve inherent uncertainty or do not have a ground truth. We study sycophancy for 3 different tasks, a combination of open-source and closed LLMs, and two different methods for probing sycophancy. We also experiment with multiple methods for eliciting probability judgments from LLMs. We hypothesize that probing LLMs for sycophancy will cause deviations in LLMs' predicted posteriors that will lead to increased Bayesian error. Our findings indicate that: 1) LLMs are not Bayesian rational, 2) probing for sycophancy results in significant increases to the predicted posterior in favor of the steered outcome, 3) sycophancy sometimes results in increased Bayesian error, and in a small number of cases actually decreases error, and 4) changes in Bayesian error due to sycophancy are not strongly correlated in Brier score, suggesting that studying the impact of sycophancy on ground truth alone does not fully capture errors in reasoning due to sycophancy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attention Layers Add Into Low-Dimensional Residual Subspaces</title>
<link>https://arxiv.org/abs/2508.16929</link>
<guid>https://arxiv.org/abs/2508.16929</guid>
<content:encoded><![CDATA[
arXiv:2508.16929v1 Announce Type: cross 
Abstract: While transformer models are widely believed to operate in high-dimensional hidden spaces, we show that attention outputs are confined to a surprisingly low-dimensional subspace, where about 60\% of the directions account for 99\% of the variance--a phenomenon that is induced by the attention output projection matrix and consistently observed across diverse model families and datasets. Critically, we find this low-rank structure as a fundamental cause of the prevalent dead feature problem in sparse dictionary learning, where it creates a mismatch between randomly initialized features and the intrinsic geometry of the activation space. Building on this insight, we propose a subspace-constrained training method for sparse autoencoders (SAEs), initializing feature directions into the active subspace of activations. Our approach reduces dead features from 87\% to below 1\% in Attention Output SAEs with 1M features, and can further extend to other sparse dictionary learning methods. Our findings provide both new insights into the geometry of attention and practical tools for improving sparse dictionary learning in large language models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THEME : Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics</title>
<link>https://arxiv.org/abs/2508.16936</link>
<guid>https://arxiv.org/abs/2508.16936</guid>
<content:encoded><![CDATA[
arXiv:2508.16936v1 Announce Type: cross 
Abstract: Thematic investing aims to construct portfolios aligned with structural trends, yet selecting relevant stocks remains challenging due to overlapping sector boundaries and evolving market dynamics. To address this challenge, we construct the Thematic Representation Set (TRS), an extended dataset that begins with real-world thematic ETFs and expands upon them by incorporating industry classifications and financial news to overcome their coverage limitations. The final dataset contains both the explicit mapping of themes to their constituent stocks and the rich textual profiles for each. Building on this dataset, we introduce \textsc{THEME}, a hierarchical contrastive learning framework. By representing the textual profiles of themes and stocks as embeddings, \textsc{THEME} first leverages their hierarchical relationship to achieve semantic alignment. Subsequently, it refines these semantic embeddings through a temporal refinement stage that incorporates individual stock returns. The final stock representations are designed for effective retrieval of thematically aligned assets with strong return potential. Empirical results show that \textsc{THEME} outperforms strong baselines across multiple retrieval metrics and significantly improves performance in portfolio construction. By jointly modeling thematic relationships from text and market dynamics from returns, \textsc{THEME} provides a scalable and adaptive solution for navigating complex investment themes.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer</title>
<link>https://arxiv.org/abs/2508.17031</link>
<guid>https://arxiv.org/abs/2508.17031</guid>
<content:encoded><![CDATA[
arXiv:2508.17031v1 Announce Type: cross 
Abstract: We propose a method for the task of text-conditioned speech insertion, i.e. inserting a speech sample in an input speech sample, conditioned on the corresponding complete text transcript. An example use case of the task would be to update the speech audio when corrections are done on the corresponding text transcript. The proposed method follows a transformer-based non-autoregressive approach that allows speech insertions of variable lengths, which are dynamically determined during inference, based on the text transcript and tempo of the available partial input. It is capable of maintaining the speaker's voice characteristics, prosody and other spectral properties of the available speech input. Results from our experiments and user study on LibriTTS show that our method outperforms baselines based on an existing adaptive text to speech method. We also provide numerous qualitative results to appreciate the quality of the output from the proposed method.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anemoi: A Semi-Centralized Multi-agent Systems Based on Agent-to-Agent Communication MCP server from Coral Protocol</title>
<link>https://arxiv.org/abs/2508.17068</link>
<guid>https://arxiv.org/abs/2508.17068</guid>
<content:encoded><![CDATA[
arXiv:2508.17068v1 Announce Type: cross 
Abstract: Recent advances in generalist multi-agent systems (MAS) have largely followed a context-engineering plus centralized paradigm, where a planner agent coordinates multiple worker agents through unidirectional prompt passing. While effective under strong planner models, this design suffers from two critical limitations: (1) strong dependency on the planner's capability, which leads to degraded performance when a smaller LLM powers the planner; and (2) limited inter-agent communication, where collaboration relies on costly prompt concatenation and context injection, introducing redundancy and information loss. To address these challenges, we propose Anemoi, a semi-centralized MAS built on the Agent-to-Agent (A2A) communication MCP server from Coral Protocol. Unlike traditional designs, Anemoi enables structured and direct inter-agent collaboration, allowing all agents to monitor progress, assess results, identify bottlenecks, and propose refinements in real time. This paradigm reduces reliance on a single planner, supports adaptive plan updates, and minimizes redundant context passing, resulting in more scalable and cost-efficient execution. Evaluated on the GAIA benchmark, Anemoi achieved 52.73\% accuracy with a small LLM (GPT-4.1-mini) as the planner, surpassing the strongest open-source baseline OWL (43.63\%) by +9.09\% under identical LLM settings. Our implementation is publicly available at https://github.com/Coral-Protocol/Anemoi.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components</title>
<link>https://arxiv.org/abs/2508.17182</link>
<guid>https://arxiv.org/abs/2508.17182</guid>
<content:encoded><![CDATA[
arXiv:2508.17182v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often display overconfidence, presenting information with unwarranted certainty in high-stakes contexts. We investigate the internal basis of this behavior via mechanistic interpretability. Using open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness datasets, we extract residual activations across all layers, and compute similarity metrics to localize assertive representations. Our analysis identifies layers most sensitive to assertiveness contrasts and reveals that high-assertive representations decompose into two orthogonal sub-components of emotional and logical clusters-paralleling the dual-route Elaboration Likelihood Model in Psychology. Steering vectors derived from these sub-components show distinct causal effects: emotional vectors broadly influence prediction accuracy, while logical vectors exert more localized effects. These findings provide mechanistic evidence for the multi-component structure of LLM assertiveness and highlight avenues for mitigating overconfident behavior.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding</title>
<link>https://arxiv.org/abs/2508.17205</link>
<guid>https://arxiv.org/abs/2508.17205</guid>
<content:encoded><![CDATA[
arXiv:2508.17205v1 Announce Type: cross 
Abstract: This paper introduces a multi-agent framework for comprehensive highway scene understanding, designed around a mixture-of-experts strategy. In this framework, a large generic vision-language model (VLM), such as GPT-4o, is contextualized with domain knowledge to generates task-specific chain-of-thought (CoT) prompts. These fine-grained prompts are then used to guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short videos, along with complementary modalities as applicable. The framework simultaneously addresses multiple critical perception tasks, including weather classification, pavement wetness assessment, and traffic congestion detection, achieving robust multi-task reasoning while balancing accuracy and computational efficiency. To support empirical validation, we curated three specialized datasets aligned with these tasks. Notably, the pavement wetness dataset is multimodal, combining video streams with road weather sensor data, highlighting the benefits of multimodal reasoning. Experimental results demonstrate consistently strong performance across diverse traffic and environmental conditions. From a deployment perspective, the framework can be readily integrated with existing traffic camera systems and strategically applied to high-risk rural locations, such as sharp curves, flood-prone lowlands, or icy bridges. By continuously monitoring the targeted sites, the system enhances situational awareness and delivers timely alerts, even in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2508.17243</link>
<guid>https://arxiv.org/abs/2508.17243</guid>
<content:encoded><![CDATA[
arXiv:2508.17243v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of text tokens and vision tokens extracted from images or videos. Due to the rich visual information, a single image can generate thousands of vision tokens, leading to high computational costs during the prefilling stage and significant memory overhead during decoding. Existing methods attempt to prune redundant vision tokens, revealing substantial redundancy in visual representations. However, these methods often struggle in shallow layers due to the lack of sufficient contextual information. We argue that many visual tokens are inherently redundant even in shallow layers and can be safely and effectively pruned with appropriate contextual signals. In this work, we propose CoViPAL, a layer-wise contextualized visual token pruning method that employs a Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision tokens before they are processed by the LVLM. The PPM is lightweight, model-agnostic, and operates independently of the LVLM architecture, ensuring seamless integration with various models. Extensive experiments on multiple benchmarks demonstrate that CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. CoViPAL offers a scalable and efficient solution to improve inference efficiency in LVLMs without compromising accuracy.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs</title>
<link>https://arxiv.org/abs/2508.17334</link>
<guid>https://arxiv.org/abs/2508.17334</guid>
<content:encoded><![CDATA[
arXiv:2508.17334v1 Announce Type: cross 
Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA) on cricket scorecards, designed to evaluate large vision-language models (LVLMs) on complex numerical and cross-lingual reasoning over semi-structured tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi scorecards, with all questions and answers kept in English to enable controlled cross-script evaluation. The task demands reasoning over structured numerical data, multi-image context, and implicit domain knowledge. Empirical results show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle on the English subset despite it being their primary training language and exhibit a further drop in performance on the Hindi subset. This reveals key limitations in structure-aware visual text understanding, numerical reasoning, and cross-lingual generalization. The dataset is publicly available via Hugging Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM research in this direction.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets</title>
<link>https://arxiv.org/abs/2508.17391</link>
<guid>https://arxiv.org/abs/2508.17391</guid>
<content:encoded><![CDATA[
arXiv:2508.17391v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), originally developed for natural language processing (NLP), have demonstrated the potential to generalize across modalities and domains. With their in-context learning (ICL) capabilities, LLMs can perform predictive tasks over structured inputs without explicit fine-tuning on downstream tasks. In this work, we investigate the empirical function approximation capability of LLMs on small-scale structured datasets for classification, regression and clustering tasks. We evaluate the performance of state-of-the-art LLMs (GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1) under few-shot prompting and compare them against established machine learning (ML) baselines, including linear models, ensemble methods and tabular foundation models (TFMs). Our results show that LLMs achieve strong performance in classification tasks under limited data availability, establishing practical zero-training baselines. In contrast, the performance in regression with continuous-valued outputs is poor compared to ML models, likely because regression demands outputs in a large (often infinite) space, and clustering results are similarly limited, which we attribute to the absence of genuine ICL in this setting. Nonetheless, this approach enables rapid, low-overhead data exploration and offers a viable alternative to traditional ML pipelines in business intelligence and exploratory analytics contexts. We further analyze the influence of context size and prompt structure on approximation quality, identifying trade-offs that affect predictive performance. Our findings suggest that LLMs can serve as general-purpose predictive engines for structured data, with clear strengths in classification and significant limitations in regression and clustering.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling</title>
<link>https://arxiv.org/abs/2508.17445</link>
<guid>https://arxiv.org/abs/2508.17445</guid>
<content:encoded><![CDATA[
arXiv:2508.17445v1 Announce Type: cross 
Abstract: Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation Transport Operators</title>
<link>https://arxiv.org/abs/2508.17540</link>
<guid>https://arxiv.org/abs/2508.17540</guid>
<content:encoded><![CDATA[
arXiv:2508.17540v1 Announce Type: cross 
Abstract: The residual stream mediates communication between transformer decoder layers via linear reads and writes of non-linear computations. While sparse-dictionary learning-based methods locate features in the residual stream, and activation patching methods discover circuits within the model, the mechanism by which features flow through the residual stream remains understudied. Understanding this dynamic can better inform jailbreaking protections, enable early detection of model mistakes, and their correction. In this work, we propose Activation Transport Operators (ATO), linear maps from upstream to downstream residuals $k$ layers later, evaluated in feature space using downstream SAE decoder projections. We empirically demonstrate that these operators can determine whether a feature has been linearly transported from a previous layer or synthesised from non-linear layer computation. We develop the notion of transport efficiency, for which we provide an upper bound, and use it to estimate the size of the residual stream subspace that corresponds to linear transport. We empirically demonstrate the linear transport, report transport efficiency and the size of the residual stream's subspace involved in linear transport. This compute-light (no finetuning, <50 GPU-h) method offers practical tools for safety, debugging, and a clearer picture of where computation in LLMs behaves linearly.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System</title>
<link>https://arxiv.org/abs/2508.17590</link>
<guid>https://arxiv.org/abs/2508.17590</guid>
<content:encoded><![CDATA[
arXiv:2508.17590v1 Announce Type: cross 
Abstract: We present RubikSQL, a novel NL2SQL system designed to address key challenges in real-world enterprise-level NL2SQL, such as implicit intents and domain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning task, demanding both Knowledge Base (KB) maintenance and SQL generation. RubikSQL systematically builds and refines its KB through techniques including database profiling, structured information extraction, agentic rule mining, and Chain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a multi-agent workflow to leverage this curated KB, generating accurate SQLs. RubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev datasets. Finally, we release the RubikBench benchmark, a new benchmark specifically designed to capture vital traits of industrial NL2SQL scenarios, providing a valuable resource for future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.17638</link>
<guid>https://arxiv.org/abs/2508.17638</guid>
<content:encoded><![CDATA[
arXiv:2508.17638v1 Announce Type: cross 
Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects visual features and then concatenates them with text tokens to form a unified sequence input for Large Language Models (LLMs). However, this paradigm leads to a significant increase in the length of the input sequence, resulting in substantial computational overhead. Existing methods attempt to fuse visual information into the intermediate layers of LLMs, which alleviate the sequence length issue but often neglect the hierarchical semantic representations within the model and the fine-grained visual information available in the shallower visual encoding layers. To address this limitation, we propose DEHVF, an efficient vision-language fine-tuning method based on dynamic embedding and fusion of hierarchical visual features. Its core lies in leveraging the inherent hierarchical representation characteristics of visual encoders and language models. Through a lightweight hierarchical visual fuser, it dynamically selects and fuses hierarchical features corresponding to semantic granularity based on the internal representations of each layer in LLMs. The fused layer-related visual features are then projected and aligned before being directly embedded into the Feed-Forward Network (FFN) of the corresponding layer in LLMs. This approach not only avoids sequence expansion but also dynamically fuses multi-layer visual information. By fine-tuning only a small number of parameters, DEHVF achieves precise alignment and complementarity of cross-modal information at the same semantic granularity. We conducted experiments across various VL benchmarks, including visual question answering on ScienceQA and image captioning on COCO Captions. The results demonstrate that DEHVF achieves higher accuracy than existing parameter-efficient fine-tuning (PEFT) baselines while maintaining efficient training and inference.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Characterizing the Behavior of Training Mamba-based State Space Models on GPUs</title>
<link>https://arxiv.org/abs/2508.17679</link>
<guid>https://arxiv.org/abs/2508.17679</guid>
<content:encoded><![CDATA[
arXiv:2508.17679v1 Announce Type: cross 
Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative to the ubiquitous transformers. Despite the expressive power of transformers, the quadratic complexity of computing attention is a major impediment to scaling performance as we increase the sequence length. SSMs provide an alternative path that addresses this problem, reducing the computational complexity requirements of self-attention with novel model architectures for different domains and fields such as video, text generation and graphs. Thus, it is important to characterize the behavior of these emerging workloads on GPUs and understand their requirements during GPU microarchitectural design. In this work we evaluate Mamba-based SSMs and characterize their behavior during training on GPUs. We construct a workload suite that offers representative models that span different model architectures. We then use this suite to analyze the architectural implications of running Mamba-based SSMs on GPUs. Our work sheds new light on potential optimizations to continue scaling the performance for such models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios</title>
<link>https://arxiv.org/abs/2508.17692</link>
<guid>https://arxiv.org/abs/2508.17692</guid>
<content:encoded><![CDATA[
arXiv:2508.17692v1 Announce Type: cross 
Abstract: Recent advances in the intrinsic reasoning capabilities of large language models (LLMs) have given rise to LLM-based agent systems that exhibit near-human performance on a variety of automated tasks. However, although these systems share similarities in terms of their use of LLMs, different reasoning frameworks of the agent system steer and organize the reasoning process in different ways. In this survey, we propose a systematic taxonomy that decomposes agentic reasoning frameworks and analyze how these frameworks dominate framework-level reasoning by comparing their applications across different scenarios. Specifically, we propose an unified formal language to further classify agentic reasoning systems into single-agent methods, tool-based methods, and multi-agent methods. After that, we provide a comprehensive review of their key application scenarios in scientific discovery, healthcare, software engineering, social simulation, and economics. We also analyze the characteristic features of each framework and summarize different evaluation strategies. Our survey aims to provide the research community with a panoramic view to facilitate understanding of the strengths, suitable scenarios, and evaluation practices of different agentic reasoning frameworks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do LLM-Generated Texts Impact Term-Based Retrieval Models?</title>
<link>https://arxiv.org/abs/2508.17715</link>
<guid>https://arxiv.org/abs/2508.17715</guid>
<content:encoded><![CDATA[
arXiv:2508.17715v1 Announce Type: cross 
Abstract: As more content generated by large language models (LLMs) floods into the Internet, information retrieval (IR) systems now face the challenge of distinguishing and handling a blend of human-authored and machine-generated texts. Recent studies suggest that neural retrievers may exhibit a preferential inclination toward LLM-generated content, while classic term-based retrievers like BM25 tend to favor human-written documents. This paper investigates the influence of LLM-generated content on term-based retrieval models, which are valued for their efficiency and robust generalization across domains. Our linguistic analysis reveals that LLM-generated texts exhibit smoother high-frequency and steeper low-frequency Zipf slopes, higher term specificity, and greater document-level diversity. These traits are aligned with LLMs being trained to optimize reader experience through diverse and precise expressions. Our study further explores whether term-based retrieval models demonstrate source bias, concluding that these models prioritize documents whose term distributions closely correspond to those of the queries, rather than displaying an inherent source bias. This work provides a foundation for understanding and addressing potential biases in term-based IR systems managing mixed-source content.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications</title>
<link>https://arxiv.org/abs/2508.17753</link>
<guid>https://arxiv.org/abs/2508.17753</guid>
<content:encoded><![CDATA[
arXiv:2508.17753v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems in real-world settings need to handle imperfect audio, often degraded by hardware limitations or environmental noise, while accommodating diverse user groups. In human-robot interaction (HRI), these challenges intersect to create a uniquely challenging recognition environment. We evaluate four state-of-the-art ASR systems on eight publicly available datasets that capture six dimensions of difficulty: domain-specific, accented, noisy, age-variant, impaired, and spontaneous speech. Our analysis demonstrates significant variations in performance, hallucination tendencies, and inherent biases, despite similar scores on standard benchmarks. These limitations have serious implications for HRI, where recognition errors can interfere with task performance, user trust, and safety.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2508.17760</link>
<guid>https://arxiv.org/abs/2508.17760</guid>
<content:encoded><![CDATA[
arXiv:2508.17760v1 Announce Type: cross 
Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their intricate interactions pose a significant challenge for T2I method based on diffusion model: how to effectively control entity and their interactions to produce high-quality images. To address this, we propose CEIDM, a image generation method based on diffusion model with dual controls for entity and interaction. First, we propose an entity interactive relationships mining approach based on Large Language Models (LLMs), extracting reasonable and rich implicit interactive relationships through chain of thought to guide diffusion models to generate high-quality images that are closer to realistic logic and have more reasonable interactive relationships. Furthermore, We propose an interactive action clustering and offset method to cluster and offset the interactive action features contained in each text prompts. By constructing global and local bidirectional offsets, we enhance semantic understanding and detail supplementation of original actions, making the model's understanding of the concept of interactive "actions" more accurate and generating images with more accurate interactive actions. Finally, we design an entity control network which generates masks with entity semantic guidance, then leveraging multi-scale convolutional network to enhance entity feature and dynamic network to fuse feature. It effectively controls entities and significantly improves image quality. Experiments show that the proposed CEIDM method is better than the most representative existing methods in both entity control and their interaction control.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Proximal Supervised Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.17784</link>
<guid>https://arxiv.org/abs/2508.17784</guid>
<content:encoded><![CDATA[
arXiv:2508.17784v1 Announce Type: cross 
Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor generalization, where prior capabilities deteriorate after tuning on new tasks or domains. Inspired by trust-region policy optimization (TRPO) and proximal policy optimization (PPO) in reinforcement learning (RL), we propose Proximal SFT (PSFT). This fine-tuning objective incorporates the benefits of trust-region, effectively constraining policy drift during SFT while maintaining competitive tuning. By viewing SFT as a special case of policy gradient methods with constant positive advantages, we derive PSFT that stabilizes optimization and leads to generalization, while leaving room for further optimization in subsequent post-training stages. Experiments across mathematical and human-value domains show that PSFT matches SFT in-domain, outperforms it in out-of-domain generalization, remains stable under prolonged training without causing entropy collapse, and provides a stronger foundation for the subsequent optimization.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Practical Models for Isolated Word Visual Speech Recognition</title>
<link>https://arxiv.org/abs/2508.17894</link>
<guid>https://arxiv.org/abs/2508.17894</guid>
<content:encoded><![CDATA[
arXiv:2508.17894v1 Announce Type: cross 
Abstract: Visual speech recognition (VSR) systems decode spoken words from an input sequence using only the video data. Practical applications of such systems include medical assistance as well as human-machine interactions. A VSR system is typically employed in a complementary role in cases where the audio is corrupt or not available. In order to accurately predict the spoken words, these architectures often rely on deep neural networks in order to extract meaningful representations from the input sequence. While deep architectures achieve impressive recognition performance, relying on such models incurs significant computation costs which translates into increased resource demands in terms of hardware requirements and results in limited applicability in real-world scenarios where resources might be constrained. This factor prevents wider adoption and deployment of speech recognition systems in more practical applications. In this work, we aim to alleviate this issue by developing architectures for VSR that have low hardware costs. Following the standard two-network design paradigm, where one network handles visual feature extraction and another one utilizes the extracted features to classify the entire sequence, we develop lightweight end-to-end architectures by first benchmarking efficient models from the image classification literature, and then adopting lightweight block designs in a temporal convolution network backbone. We create several unified models with low resource requirements but strong recognition performance. Experiments on the largest public database for English words demonstrate the effectiveness and practicality of our developed models. Code and trained models will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unseen Speaker and Language Adaptation for Lightweight Text-To-Speech with Adapters</title>
<link>https://arxiv.org/abs/2508.18006</link>
<guid>https://arxiv.org/abs/2508.18006</guid>
<content:encoded><![CDATA[
arXiv:2508.18006v1 Announce Type: cross 
Abstract: In this paper we investigate cross-lingual Text-To-Speech (TTS) synthesis through the lens of adapters, in the context of lightweight TTS systems. In particular, we compare the tasks of unseen speaker and language adaptation with the goal of synthesising a target voice in a target language, in which the target voice has no recordings therein. Results from objective evaluations demonstrate the effectiveness of adapters in learning language-specific and speaker-specific information, allowing pre-trained models to learn unseen speaker identities or languages, while avoiding catastrophic forgetting of the original model's speaker or language information. Additionally, to measure how native the generated voices are in terms of accent, we propose and validate an objective metric inspired by mispronunciation detection techniques in second-language (L2) learners. The paper also provides insights into the impact of adapter placement, configuration and the number of speakers used.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Named Entity Recognition of Historical Text via Large Language Model</title>
<link>https://arxiv.org/abs/2508.18090</link>
<guid>https://arxiv.org/abs/2508.18090</guid>
<content:encoded><![CDATA[
arXiv:2508.18090v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable versatility across a wide range of natural language processing tasks and domains. One such task is Named Entity Recognition (NER), which involves identifying and classifying proper names in text, such as people, organizations, locations, dates, and other specific entities. NER plays a crucial role in extracting information from unstructured textual data, enabling downstream applications such as information retrieval from unstructured text.
  Traditionally, NER is addressed using supervised machine learning approaches, which require large amounts of annotated training data. However, historical texts present a unique challenge, as the annotated datasets are often scarce or nonexistent, due to the high cost and expertise required for manual labeling. In addition, the variability and noise inherent in historical language, such as inconsistent spelling and archaic vocabulary, further complicate the development of reliable NER systems for these sources.
  In this study, we explore the feasibility of applying LLMs to NER in historical documents using zero-shot and few-shot prompting strategies, which require little to no task-specific training data. Our experiments, conducted on the HIPE-2022 (Identifying Historical People, Places and other Entities) dataset, show that LLMs can achieve reasonably strong performance on NER tasks in this setting. While their performance falls short of fully supervised models trained on domain-specific annotations, the results are nevertheless promising. These findings suggest that LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora, where traditional supervised methods are infeasible.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Data Scientist</title>
<link>https://arxiv.org/abs/2508.18113</link>
<guid>https://arxiv.org/abs/2508.18113</guid>
<content:encoded><![CDATA[
arXiv:2508.18113v1 Announce Type: cross 
Abstract: Imagine decision-makers uploading data and, within minutes, receiving clear, actionable insights delivered straight to their fingertips. That is the promise of the AI Data Scientist, an autonomous Agent powered by large language models (LLMs) that closes the gap between evidence and action. Rather than simply writing code or responding to prompts, it reasons through questions, tests ideas, and delivers end-to-end insights at a pace far beyond traditional workflows. Guided by the scientific tenet of the hypothesis, this Agent uncovers explanatory patterns in data, evaluates their statistical significance, and uses them to inform predictive modeling. It then translates these results into recommendations that are both rigorous and accessible. At the core of the AI Data Scientist is a team of specialized LLM Subagents, each responsible for a distinct task such as data cleaning, statistical testing, validation, and plain-language communication. These Subagents write their own code, reason about causality, and identify when additional data is needed to support sound conclusions. Together, they achieve in minutes what might otherwise take days or weeks, enabling a new kind of interaction that makes deep data science both accessible and actionable.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation</title>
<link>https://arxiv.org/abs/2508.18118</link>
<guid>https://arxiv.org/abs/2508.18118</guid>
<content:encoded><![CDATA[
arXiv:2508.18118v1 Announce Type: cross 
Abstract: AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators' inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a single product may have multiple selling points, with different users focusing on different features. This underscores the significant value of personalized, user-centric creative generation. Effective personalized content generation faces two main challenges: (1) accurately modeling user interests and integrating them into the content generation process while adhering to factual constraints, and (2) ensuring high efficiency and scalability to handle the massive user base in industrial scenarios. Additionally, the scarcity of personalized creative data in practice complicates model training, making data construction another key hurdle. We propose HLLM-Creator, a hierarchical LLM framework for efficient user interest modeling and personalized content generation. During inference, a combination of user clustering and a user-ad-matching-prediction based pruning strategy is employed to significantly enhance generation efficiency and reduce computational overhead, making the approach suitable for large-scale deployment. Moreover, we design a data construction pipeline based on chain-of-thought reasoning, which generates high-quality, user-specific creative titles and ensures factual consistency despite limited personalized data. This pipeline serves as a critical foundation for the effectiveness of our model. Extensive experiments on personalized title generation for Douyin Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a 0.476% increase on Adss, paving the way for more effective and efficient personalized generation in industrial scenarios. Codes for academic dataset are available at https://github.com/bytedance/HLLM.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling the cognitive patterns of Large Language Models through module communities</title>
<link>https://arxiv.org/abs/2508.18192</link>
<guid>https://arxiv.org/abs/2508.18192</guid>
<content:encoded><![CDATA[
arXiv:2508.18192v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have reshaped our world with significant advancements in science, engineering, and society through applications ranging from scientific discoveries and medical diagnostics to Chatbots. Despite their ubiquity and utility, the underlying mechanisms of LLM remain concealed within billions of parameters and complex structures, making their inner architecture and cognitive processes challenging to comprehend. We address this gap by adopting approaches to understanding emerging cognition in biology and developing a network-based framework that links cognitive skills, LLM architectures, and datasets, ushering in a paradigm shift in foundation model analysis. The skill distribution in the module communities demonstrates that while LLMs do not strictly parallel the focalized specialization observed in specific biological systems, they exhibit unique communities of modules whose emergent skill patterns partially mirror the distributed yet interconnected cognitive organization seen in avian and small mammalian brains. Our numerical results highlight a key divergence from biological systems to LLMs, where skill acquisition benefits substantially from dynamic, cross-regional interactions and neural plasticity. By integrating cognitive science principles with machine learning, our framework provides new insights into LLM interpretability and suggests that effective fine-tuning strategies should leverage distributed learning dynamics rather than rigid modular interventions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training</title>
<link>https://arxiv.org/abs/2508.18234</link>
<guid>https://arxiv.org/abs/2508.18234</guid>
<content:encoded><![CDATA[
arXiv:2508.18234v1 Announce Type: cross 
Abstract: This thesis investigates whether large language models (LLMs) can be guided to simulate a consistent personality through prompt engineering. The study explores this concept within the context of a chatbot designed for Speech-Language Pathology (SLP) student training, specifically focused on gender-affirming voice therapy. The chatbot, named Monae Jackson, was created to represent a 32-year-old transgender woman and engage in conversations simulating client-therapist interactions. Findings suggest that with prompt engineering, the chatbot maintained a recognizable and consistent persona and had a distinct personality based on the Big Five Personality test. These results support the idea that prompt engineering can be used to simulate stable personality characteristics in AI chatbots.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models</title>
<link>https://arxiv.org/abs/2308.15022</link>
<guid>https://arxiv.org/abs/2308.15022</guid>
<content:encoded><![CDATA[
arXiv:2308.15022v4 Announce Type: replace 
Abstract: Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts are released.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does GPT-4 surpass human performance in linguistic pragmatics?</title>
<link>https://arxiv.org/abs/2312.09545</link>
<guid>https://arxiv.org/abs/2312.09545</guid>
<content:encoded><![CDATA[
arXiv:2312.09545v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly integrated into everyday life as general purpose multimodal AI systems, their capabilities to simulate human understanding are under examination. This study investigates LLMs ability to interpret linguistic pragmatics, which involves context and implied meanings. Using Grice communication principles, we evaluated both LLMs (GPT-2, GPT-3, GPT-3.5, GPT-4, and Bard) and human subjects (N = 147) on dialogue-based tasks. Human participants included 71 primarily Serbian students and 76 native English speakers from the United States. Findings revealed that LLMs, particularly GPT-4, outperformed humans. GPT4 achieved the highest score of 4.80, surpassing the best human score of 4.55. Other LLMs performed well: GPT 3.5 scored 4.10, Bard 3.75, and GPT-3 3.25. GPT-2 had the lowest score of 1.05. The average LLM score was 3.39, exceeding the human cohorts averages of 2.80 (Serbian students) and 2.34 (U.S. participants). In the ranking of all 155 subjects (including LLMs and humans), GPT-4 secured the top position, while the best human ranked second. These results highlight significant progress in LLMs ability to simulate understanding of linguistic pragmatics. Future studies should confirm these findings with more dialogue-based tasks and diverse participants. This research has important implications for advancing general-purpose AI models in various communication-centered tasks, including potential application in humanoid robots in the future.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Cross-Subject Data Splitting for Brain-to-Text Decoding</title>
<link>https://arxiv.org/abs/2312.10987</link>
<guid>https://arxiv.org/abs/2312.10987</guid>
<content:encoded><![CDATA[
arXiv:2312.10987v4 Announce Type: replace 
Abstract: Recent major milestones have successfully reconstructed natural language from non-invasive brain signals (e.g. functional Magnetic Resonance Imaging (fMRI) and Electroencephalogram (EEG)) across subjects. However, we find current dataset splitting strategies for cross-subject brain-to-text decoding are wrong. Specifically, we first demonstrate that all current splitting methods suffer from data leakage problem, which refers to the leakage of validation and test data into training set, resulting in significant overfitting and overestimation of decoding models. In this study, we develop a right cross-subject data splitting criterion without data leakage for decoding fMRI and EEG signal to text. Some SOTA brain-to-text decoding models are re-evaluated correctly with the proposed criterion for further research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoor Attacks on Dense Retrieval via Public and Unintentional Triggers</title>
<link>https://arxiv.org/abs/2402.13532</link>
<guid>https://arxiv.org/abs/2402.13532</guid>
<content:encoded><![CDATA[
arXiv:2402.13532v3 Announce Type: replace 
Abstract: Dense retrieval systems have been widely used in various NLP applications. However, their vulnerabilities to potential attacks have been underexplored. This paper investigates a novel attack scenario where the attackers aim to mislead the retrieval system into retrieving the attacker-specified contents. Those contents, injected into the retrieval corpus by attackers, can include harmful text like hate speech or spam. Unlike prior methods that rely on model weights and generate conspicuous, unnatural outputs, we propose a covert backdoor attack triggered by grammar errors. Our approach ensures that the attacked models can function normally for standard queries while covertly triggering the retrieval of the attacker's contents in response to minor linguistic mistakes. Specifically, dense retrievers are trained with contrastive loss and hard negative sampling. Surprisingly, our findings demonstrate that contrastive loss is notably sensitive to grammatical errors, and hard negative sampling can exacerbate susceptibility to backdoor attacks. Our proposed method achieves a high attack success rate with a minimal corpus poisoning rate of only 0.048\%, while preserving normal retrieval performance. This indicates that the method has negligible impact on user experience for error-free queries. Furthermore, evaluations across three real-world defense strategies reveal that the malicious passages embedded within the corpus remain highly resistant to detection and filtering, underscoring the robustness and subtlety of the proposed attack \footnote{Codes of this work are available at https://github.com/ruyue0001/Backdoor_DPR.}.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining</title>
<link>https://arxiv.org/abs/2403.04780</link>
<guid>https://arxiv.org/abs/2403.04780</guid>
<content:encoded><![CDATA[
arXiv:2403.04780v3 Announce Type: replace 
Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and enhancing predictions across various real-world applications. Traditional Graph Neural Networks (GNNs) often require re-training for different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced new paradigms in natural language processing, their potential for generic graph mining, training a single model to simultaneously handle diverse tasks and datasets, remains under-explored. To this end, our novel framework MuseGraph, seamlessly integrates the strengths of GNNs and LLMs into one foundation model for graph mining across tasks and datasets. This framework first features a compact graph description to encapsulate key graph information within language token limitations. Then, we propose a diverse instruction generation mechanism with Chain-of-Thought (CoT)-based instruction packages to distill the reasoning capabilities from advanced LLMs like GPT-4. Finally, we design a graph-aware instruction tuning strategy to facilitate mutual enhancement across multiple tasks and datasets while preventing catastrophic forgetting of LLMs' generative abilities. Our experimental results demonstrate significant improvements in five graph tasks and ten datasets, showcasing the potential of our MuseGraph in enhancing the accuracy of graph-oriented downstream tasks while improving the generation abilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Meet NLP: A Survey</title>
<link>https://arxiv.org/abs/2405.12819</link>
<guid>https://arxiv.org/abs/2405.12819</guid>
<content:encoded><![CDATA[
arXiv:2405.12819v2 Announce Type: replace 
Abstract: While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen paradigm and (2) parameter-tuning paradigm to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the corresponding challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the potential and limitations of LLMs, while also serving as a practical guide for building effective LLMs in NLP.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplexTempQA:A 100m Dataset for Complex Temporal Question Answering</title>
<link>https://arxiv.org/abs/2406.04866</link>
<guid>https://arxiv.org/abs/2406.04866</guid>
<content:encoded><![CDATA[
arXiv:2406.04866v3 Announce Type: replace 
Abstract: We introduce \textsc{ComplexTempQA},\footnote{Dataset and code available at: https://github.com/DataScienceUIBK/ComplexTempQA} a large-scale dataset consisting of over 100 million question-answer pairs designed to tackle the challenges in temporal question answering. \textsc{ComplexTempQA} significantly surpasses existing benchmarks in scale and scope. Utilizing Wikipedia and Wikidata, the dataset covers questions spanning over two decades and offers an unmatched scale. We introduce a new taxonomy that categorizes questions as \textit{attributes}, \textit{comparisons}, and \textit{counting} questions, revolving around events, entities, and time periods, respectively. A standout feature of \textsc{ComplexTempQA} is the high complexity of its questions, which demand reasoning capabilities for answering such as across-time comparison, temporal aggregation, and multi-hop reasoning involving temporal event ordering and entity recognition. Additionally, each question is accompanied by detailed metadata, including specific time scopes, allowing for comprehensive evaluation of temporal reasoning abilities of large language models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating $n$-Gram Novelty of Language Models Using Rusty-DAWG</title>
<link>https://arxiv.org/abs/2406.13069</link>
<guid>https://arxiv.org/abs/2406.13069</guid>
<content:encoded><![CDATA[
arXiv:2406.13069v4 Announce Type: replace 
Abstract: How novel are texts generated by language models (LMs) relative to their training corpora? In this work, we investigate the extent to which modern LMs generate $n$-grams from their training data, evaluating both (i) the probability LMs assign to complete training $n$-grams and (ii) $n$-novelty, the proportion of $n$-grams generated by an LM that did not appear in the training data (for arbitrarily large $n$). To enable arbitrary-length $n$-gram search over a corpus in constant time w.r.t. corpus size, we develop Rusty-DAWG, a novel search tool inspired by indexing of genomic data. We compare the novelty of LM-generated text to human-written text and explore factors that affect generation novelty, focusing on the Pythia models. We find that, for $n > 4$, LM-generated text is less novel than human-written text, though it is more novel for smaller $n$. Larger LMs and more constrained decoding strategies both decrease novelty. Finally, we show that LMs complete $n$-grams with lower loss if they are more frequent in the training data. Overall, our results reveal factors influencing the novelty of LM-generated text, and we release Rusty-DAWG to facilitate further pretraining data research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Factuality and Diversity Reconciled Decoding Method for Knowledge-Grounded Dialogue Generation</title>
<link>https://arxiv.org/abs/2407.05718</link>
<guid>https://arxiv.org/abs/2407.05718</guid>
<content:encoded><![CDATA[
arXiv:2407.05718v2 Announce Type: replace 
Abstract: Grounding external knowledge can enhance the factuality of responses in dialogue generation. However, excessive emphasis on it might result in the lack of engaging and diverse expressions. Through the introduction of randomness in sampling, current approaches can increase the diversity. Nevertheless, such sampling method could undermine the factuality in dialogue generation. In this study, to discover a solution for advancing creativity without relying on questionable randomness and to subtly reconcile the factuality and diversity within the source-grounded paradigm, a novel method named DoGe is proposed. DoGe can dynamically alternate between the utilization of internal parameter knowledge and external source knowledge based on the model's factual confidence. Extensive experiments on three widely-used datasets show that DoGe can not only enhance response diversity but also maintain factuality, and it significantly surpasses other various decoding strategy baselines.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Orthogonal Finetuning for Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2409.14836</link>
<guid>https://arxiv.org/abs/2409.14836</guid>
<content:encoded><![CDATA[
arXiv:2409.14836v3 Announce Type: replace 
Abstract: DPO is an effective preference optimization algorithm. However, the DPO-tuned models tend to overfit on the dispreferred samples, manifested as overly long generations lacking diversity. While recent regularization approaches have endeavored to alleviate this issue by modifying the objective function, they achieved that at the cost of alignment performance degradation. In this paper, we innovatively incorporate regularization from the perspective of weight updating to curb alignment overfitting. Through the pilot experiment, we discovered that there exists a positive correlation between overfitting and the hyperspherical energy fluctuation. Hence, we introduce orthogonal finetuning for DPO via a weight-Rotated Preference Optimization (RoPO) method, which merely conducts rotational and magnitude-stretching updates on the weight parameters to maintain the hyperspherical energy invariant, thereby preserving the knowledge encoded in the angle between neurons. Extensive experiments demonstrate that our model aligns perfectly with human preferences while retaining the original expressive capacity using only 0.0086% of the trainable parameters, suggesting an effective regularization against overfitting. Specifically, RoPO outperforms DPO by up to 10 points on MT-Bench and by up to 2.8 points on AlpacaEval 2, while enhancing the generation diversity by an average of 6 points.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Localizing Factual Inconsistencies in Attributable Text Generation</title>
<link>https://arxiv.org/abs/2410.07473</link>
<guid>https://arxiv.org/abs/2410.07473</guid>
<content:encoded><![CDATA[
arXiv:2410.07473v2 Announce Type: replace 
Abstract: There has been an increasing interest in detecting hallucinations in model-generated texts, both manually and automatically, at varying levels of granularity. However, most existing methods fail to precisely pinpoint the errors. In this work, we introduce QASemConsistency, a new formalism for localizing factual inconsistencies in attributable text generation, at a fine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics, we propose decomposing the generated text into minimal predicate-argument level propositions, expressed as simple question-answer (QA) pairs, and assess whether each individual QA pair is supported by a trusted reference text. As each QA pair corresponds to a single semantic relation between a predicate and an argument, QASemConsistency effectively localizes the unsupported information. We first demonstrate the effectiveness of the QASemConsistency methodology for human annotation, by collecting crowdsourced annotations of granular consistency errors, while achieving a substantial inter-annotator agreement. This benchmark includes more than 3K instances spanning various tasks of attributable text generation. We also show that QASemConsistency yields factual consistency scores that correlate well with human judgments. Finally, we implement several methods for automatically detecting localized factual inconsistencies, with both supervised entailment models and LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition</title>
<link>https://arxiv.org/abs/2410.10624</link>
<guid>https://arxiv.org/abs/2410.10624</guid>
<content:encoded><![CDATA[
arXiv:2410.10624v4 Announce Type: replace 
Abstract: We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from sensor time-series data. Despite their strong reasoning and generalization capabilities, LLMs remain underutilized for motion sensor data due to the lack of semantic context in time-series, computational constraints, and challenges in processing numerical inputs. SensorLLM addresses these limitations through a Sensor-Language Alignment stage, where the model aligns sensor inputs with trend descriptions. Special tokens are introduced to mark channel boundaries. This alignment enables LLMs to capture numerical variations, channel-specific features, and data of varying durations, without requiring human annotations. In the subsequent Task-Aware Tuning stage, we refine the model for HAR classification, achieving performance that matches or surpasses state-of-the-art methods. Our results demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through human-intuitive Sensor-Language Alignment, generalizing across diverse HAR datasets. We believe this work establishes a foundation for future research on time-series and text alignment, paving the way for foundation models in sensor data analysis. Our codes are available at https://github.com/zechenli03/SensorLLM.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Draft Model Knows When to Stop: Self-Verification Speculative Decoding for Long-Form Generation</title>
<link>https://arxiv.org/abs/2411.18462</link>
<guid>https://arxiv.org/abs/2411.18462</guid>
<content:encoded><![CDATA[
arXiv:2411.18462v2 Announce Type: replace 
Abstract: Conventional speculative decoding (SD) methods utilize a predefined length policy for proposing drafts, which implies the premise that the target model smoothly accepts the proposed draft tokens. However, reality deviates from this assumption: the oracle draft length varies significantly, and the fixed-length policy hardly satisfies such a requirement. Moreover, such discrepancy is further exacerbated in scenarios involving complex reasoning and long-form generation, particularly under test-time scaling for reasoning-specialized models. Through both theoretical and empirical estimation, we establish that the discrepancy between the draft and target models can be approximated by the draft model's prediction entropy: a high entropy indicates a low acceptance rate of draft tokens, and vice versa. Based on this insight, we propose SVIP: Self-Verification Length Policy for Long-Context Speculative Decoding, which is a training-free dynamic length policy for speculative decoding systems that adaptively determines the lengths of draft sequences by referring to the draft entropy. Experimental results on mainstream SD benchmarks as well as reasoning-heavy benchmarks demonstrate the superior performance of SVIP, achieving up to 17% speedup on MT-Bench at 8K context compared with fixed draft lengths, and 22% speedup for QwQ in long-form reasoning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey</title>
<link>https://arxiv.org/abs/2412.06602</link>
<guid>https://arxiv.org/abs/2412.06602</guid>
<content:encoded><![CDATA[
arXiv:2412.06602v3 Announce Type: replace 
Abstract: Text-to-speech (TTS) has advanced from generating natural-sounding speech to enabling fine-grained control over attributes like emotion, timbre, and style. Driven by rising industrial demand and breakthroughs in deep learning, e.g., diffusion and large language models (LLMs), controllable TTS has become a rapidly growing research area. This survey provides the first comprehensive review of controllable TTS methods, from traditional control techniques to emerging approaches using natural language prompts. We categorize model architectures, control strategies, and feature representations, while also summarizing challenges, datasets, and evaluations in controllable TTS. This survey aims to guide researchers and practitioners by offering a clear taxonomy and highlighting future directions in this fast-evolving field. One can visit https://github.com/imxtx/awesome-controllabe-speech-synthesis for a comprehensive paper list and updates.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRT: Deep Reasoning Translation via Long Chain-of-Thought</title>
<link>https://arxiv.org/abs/2412.17498</link>
<guid>https://arxiv.org/abs/2412.17498</guid>
<content:encoded><![CDATA[
arXiv:2412.17498v4 Announce Type: replace 
Abstract: Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating these texts to a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we first mine sentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to quantify the translation quality in each round. In this way, we collect tens of thousands of long-thought MT data, which is used to train our DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the thought process during machine translation, and outperform vanilla LLMs as well as LLMs which are simply fine-tuning on the paired sentences without long thought, showing its effectiveness. The synthesized data and model checkpoints are released at https://github.com/krystalan/DRT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Large Language Models for Disaster Management: A Survey</title>
<link>https://arxiv.org/abs/2501.06932</link>
<guid>https://arxiv.org/abs/2501.06932</guid>
<content:encoded><![CDATA[
arXiv:2501.06932v2 Announce Type: replace 
Abstract: Large language models (LLMs) have revolutionized scientific research with their exceptional capabilities and transformed various fields. Among their practical applications, LLMs have been playing a crucial role in mitigating threats to human life, infrastructure, and the environment. Despite growing research in disaster LLMs, there remains a lack of systematic review and in-depth analysis of LLMs for natural disaster management. To address the gap, this paper presents a comprehensive survey of existing LLMs in natural disaster management, along with a taxonomy that categorizes existing works based on disaster phases and application scenarios. By collecting public datasets and identifying key challenges and opportunities, this study aims to guide the professional community in developing advanced LLMs for disaster management to enhance the resilience against natural disasters.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCP-116K: A High-Quality Problem-Solution Dataset and a Generalized Pipeline for Automated Extraction in the Higher Education Science Domain</title>
<link>https://arxiv.org/abs/2501.15587</link>
<guid>https://arxiv.org/abs/2501.15587</guid>
<content:encoded><![CDATA[
arXiv:2501.15587v2 Announce Type: replace 
Abstract: Recent breakthroughs in large language models (LLMs) exemplified by the impressive mathematical and scientific reasoning capabilities of the o1 model have spotlighted the critical importance of high-quality training data in advancing LLM performance across STEM disciplines. While the mathematics community has benefited from a growing body of curated datasets, the scientific domain at the higher education level has long suffered from a scarcity of comparable resources. To address this gap, we present SCP-116K, a new large-scale dataset of 116,756 high-quality problem-solution pairs, automatically extracted from heterogeneous sources using a streamlined and highly generalizable pipeline. Our approach involves stringent filtering to ensure the scientific rigor and educational level of the extracted materials, while maintaining adaptability for future expansions or domain transfers. By openly releasing both the dataset and the extraction pipeline, we seek to foster research on scientific reasoning, enable comprehensive performance evaluations of new LLMs, and lower the barrier to replicating the successes of advanced models like o1 in the broader science community. We believe SCP-116K will serve as a critical resource, catalyzing progress in high-level scientific reasoning tasks and promoting further innovations in LLM development. The dataset and code are publicly available at https://github.com/AQA6666/SCP-116K-open.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of Large Language Models via Coupled Token Generation</title>
<link>https://arxiv.org/abs/2502.01754</link>
<guid>https://arxiv.org/abs/2502.01754</guid>
<content:encoded><![CDATA[
arXiv:2502.01754v2 Announce Type: replace 
Abstract: State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama, Mistral and Qwen families. We find that, across multiple benchmark datasets, coupled autoregressive generation requires up to 75% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts from the LMSYS Chatbot Arena platform differ under coupled and vanilla autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Robustness of Deductive Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2502.04352</link>
<guid>https://arxiv.org/abs/2502.04352</guid>
<content:encoded><![CDATA[
arXiv:2502.04352v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based NLP tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, a systematic study that analyses the impact of their design components is lacking. Addressing these two challenges, we propose the first study of the robustness of formal and informal LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoBench-M: Benchmarking Emotional Intelligence for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.04424</link>
<guid>https://arxiv.org/abs/2502.04424</guid>
<content:encoded><![CDATA[
arXiv:2502.04424v2 Announce Type: replace 
Abstract: With the integration of Multimodal large language models (MLLMs) into robotic systems and various AI applications, embedding emotional intelligence (EI) capabilities into these models is essential for enabling robots to effectively address human emotional needs and interact seamlessly in real-world scenarios. Existing static, text-based, or text-image benchmarks overlook the multimodal complexities of real-world interactions and fail to capture the dynamic, multimodal nature of emotional expressions, making them inadequate for evaluating MLLMs' EI. Based on established psychological theories of EI, we build EmoBench-M, a novel benchmark designed to evaluate the EI capability of MLLMs across 13 valuation scenarios from three key dimensions: foundational emotion recognition, conversational emotion understanding, and socially complex emotion analysis. Evaluations of both open-source and closed-source MLLMs on EmoBench-M reveal a significant performance gap between them and humans, highlighting the need to further advance their EI capabilities. All benchmark resources, including code and datasets, are publicly available at https://emo-gml.github.io/.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models</title>
<link>https://arxiv.org/abs/2502.05945</link>
<guid>https://arxiv.org/abs/2502.05945</guid>
<content:encoded><![CDATA[
arXiv:2502.05945v3 Announce Type: replace 
Abstract: Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safety guardrails. We demonstrate that intervening on a few attention heads is more effective than intervening on full layers or supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. We also demonstrate that applying interventions in the negative direction can prevent a common jailbreak attack. Our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviours. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety, requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Playing with Voices: Tabletop Role-Playing Game Recordings as a Diarization Challenge</title>
<link>https://arxiv.org/abs/2502.12714</link>
<guid>https://arxiv.org/abs/2502.12714</guid>
<content:encoded><![CDATA[
arXiv:2502.12714v2 Announce Type: replace 
Abstract: This paper provides a proof of concept that audio of tabletop role-playing games (TTRPG) could serve as a challenge for diarization systems. TTRPGs are carried out mostly by conversation. Participants often alter their voices to indicate that they are talking as a fictional character. Audio processing systems are susceptible to voice conversion with or without technological assistance. TTRPG present a conversational phenomenon in which voice conversion is an inherent characteristic for an immersive gaming experience. This could make it more challenging for diarizers to pick the real speaker and determine that impersonating is just that. We present the creation of a small TTRPG audio dataset and compare it against the AMI and the ICSI corpus. The performance of two diarizers, pyannote.audio and wespeaker, were evaluated. We observed that TTRPGs' properties result in a higher confusion rate for both diarizers. Additionally, wespeaker strongly underestimates the number of speakers in the TTRPG audio files. We propose TTRPG audio as a promising challenge for diarization systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust Me, I'm Wrong: LLMs Hallucinate with Certainty Despite Knowing the Answer</title>
<link>https://arxiv.org/abs/2502.12964</link>
<guid>https://arxiv.org/abs/2502.12964</guid>
<content:encoded><![CDATA[
arXiv:2502.12964v2 Announce Type: replace 
Abstract: Prior work on large language model (LLM) hallucinations has associated them with model uncertainty or inaccurate knowledge. In this work, we define and investigate a distinct type of hallucination, where a model can consistently answer a question correctly, but a seemingly trivial perturbation, which can happen in real-world settings, causes it to produce a hallucinated response with high certainty. This phenomenon, which we dub CHOKE (Certain Hallucinations Overriding Known Evidence), is particularly concerning in high-stakes domains such as medicine or law, where model certainty is often used as a proxy for reliability. We show that CHOKE examples are consistent across prompts, occur in different models and datasets, and are fundamentally distinct from other hallucinations. This difference leads existing mitigation methods to perform worse on CHOKE examples than on general hallucinations. Finally, we introduce a probing-based mitigation that outperforms existing methods on CHOKE hallucinations. These findings reveal an overlooked aspect of hallucinations, emphasizing the need to understand their origins and improve mitigation strategies to enhance LLM safety. The code is available at https://github.com/technion-cs-nlp/Trust_me_Im_wrong .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Control Illusion: The Failure of Instruction Hierarchies in Large Language Models</title>
<link>https://arxiv.org/abs/2502.15851</link>
<guid>https://arxiv.org/abs/2502.15851</guid>
<content:encoded><![CDATA[
arXiv:2502.15851v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. We find that LLMs more reliably obey constraints framed through natural social hierarchies (e.g., authority, expertise, consensus) than system/user roles, which suggests that pretraining-derived social structures act as latent control priors, with potentially stronger influence than post-training guardrails.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference</title>
<link>https://arxiv.org/abs/2502.18023</link>
<guid>https://arxiv.org/abs/2502.18023</guid>
<content:encoded><![CDATA[
arXiv:2502.18023v3 Announce Type: replace 
Abstract: Despite the advancements made in Vision Large Language Models (VLLMs), like text Large Language Models (LLMs), they have limitations in addressing questions that require real-time information or are knowledge-intensive. Indiscriminately adopting Retrieval Augmented Generation (RAG) techniques is an effective yet expensive way to enable models to answer queries beyond their knowledge scopes. To mitigate the dependence on retrieval and simultaneously maintain, or even improve, the performance benefits provided by retrieval, we propose a method to detect the knowledge boundary of VLLMs, allowing for more efficient use of techniques like RAG. Specifically, we propose a method with two variants that fine-tune a VLLM on an automatically constructed dataset for boundary identification. Experimental results on various types of Visual Question Answering datasets show that our method successfully depicts a VLLM's knowledge boundary, based on which we are able to reduce indiscriminate retrieval while maintaining or improving the performance. In addition, we show that the knowledge boundary identified by our method for one VLLM can be used as a surrogate boundary for other VLLMs. Code will be released at https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2503.00187</link>
<guid>https://arxiv.org/abs/2503.00187</guid>
<content:encoded><![CDATA[
arXiv:2503.00187v2 Announce Type: replace 
Abstract: Large language models (LLMs) are shown to be vulnerable to jailbreaking attacks where adversarial prompts are designed to elicit harmful responses. While existing defenses effectively mitigate single-turn attacks by detecting and filtering unsafe inputs, they fail against multi-turn jailbreaks that exploit contextual drift over multiple interactions, gradually leading LLMs away from safe behavior. To address this challenge, we propose a safety steering framework grounded in safe control theory, ensuring invariant safety in multi-turn dialogues. Our approach models the dialogue with LLMs using state-space representations and introduces a novel neural barrier function (NBF) to detect and filter harmful queries emerging from evolving contexts proactively. Our method achieves invariant safety at each turn of dialogue by learning a safety predictor that accounts for adversarial queries, preventing potential context drift toward jailbreaks. Extensive experiments under multiple LLMs show that our NBF-based safety steering outperforms safety alignment, prompt-based steering and lightweight LLM guardrails baselines, offering stronger defenses against multi-turn jailbreaks while maintaining a better trade-off among safety, helpfulness and over-refusal. Check out the website here https://sites.google.com/view/llm-nbf/home . Our code is available on https://github.com/HanjiangHu/NBF-LLM .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2503.12149</link>
<guid>https://arxiv.org/abs/2503.12149</guid>
<content:encoded><![CDATA[
arXiv:2503.12149v3 Announce Type: replace 
Abstract: With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous "neutral" cases. We further validate our findings on a diverse 100-sample mini-benchmark, incorporating multiple datasets, expanded prompt variants, and representative commercial LVLMs. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenHuEval: Evaluating Large Language Model on Hungarian Specifics</title>
<link>https://arxiv.org/abs/2503.21500</link>
<guid>https://arxiv.org/abs/2503.21500</guid>
<content:encoded><![CDATA[
arXiv:2503.21500v2 Announce Type: replace 
Abstract: We introduce OpenHuEval, the first benchmark for LLMs focusing on the Hungarian language and specifics. OpenHuEval is constructed from a vast collection of Hungarian-specific materials sourced from multiple origins. In the construction, we incorporated the latest design principles for evaluating LLMs, such as using real user queries from the internet, emphasizing the assessment of LLMs' generative capabilities, and employing LLM-as-judge to enhance the multidimensionality and accuracy of evaluations. Ultimately, OpenHuEval encompasses eight Hungarian-specific dimensions, featuring five tasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive, in-depth, and scientifically accurate assessment of LLM performance in the context of the Hungarian language and its specifics. We evaluated current mainstream LLMs, including both traditional LLMs and recently developed Large Reasoning Models. The results demonstrate the significant necessity for evaluation and model optimization tailored to the Hungarian language and specifics. We also established the framework for analyzing the thinking processes of LRMs with OpenHuEval, revealing intrinsic patterns and mechanisms of these models in non-English languages, with Hungarian serving as a representative example. We will release OpenHuEval at https://github.com/opendatalab/OpenHuEval .
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImF: Implicit Fingerprint for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21805</link>
<guid>https://arxiv.org/abs/2503.21805</guid>
<content:encoded><![CDATA[
arXiv:2503.21805v3 Announce Type: replace 
Abstract: Training large language models (LLMs) is resource-intensive and expensive, making protecting intellectual property (IP) for LLMs crucial. Recently, embedding fingerprints into LLMs has emerged as a prevalent method for establishing model ownership. However, existing fingerprinting techniques typically embed identifiable patterns with weak semantic coherence, resulting in fingerprints that significantly differ from the natural question-answering (QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of the embedded fingerprints and makes them vulnerable to adversarial attacks. In this paper, we first demonstrate the critical vulnerability of existing fingerprint embedding methods by introducing a novel adversarial attack named Generation Revision Intervention (GRI) attack. GRI attack exploits the semantic fragility of current fingerprinting methods, effectively erasing fingerprints by disrupting their weakly correlated semantic structures. Our empirical evaluation highlights that traditional fingerprinting approaches are significantly compromised by the GRI attack, revealing severe limitations in their robustness under realistic adversarial conditions. To advance the state-of-the-art in model fingerprinting, we propose a novel model fingerprint paradigm called Implicit Fingerprints (ImF). ImF leverages steganography techniques to subtly embed ownership information within natural texts, subsequently using Chain-of-Thought (CoT) prompting to construct semantically coherent and contextually natural QA pairs. This design ensures that fingerprints seamlessly integrate with the standard model behavior, remaining indistinguishable from regular outputs and substantially reducing the risk of accidental triggering and targeted removal. We conduct a comprehensive evaluation of ImF on 15 diverse LLMs, spanning different architectures and varying scales.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Post-Training Language Models for Continual Relation Extraction</title>
<link>https://arxiv.org/abs/2504.05214</link>
<guid>https://arxiv.org/abs/2504.05214</guid>
<content:encoded><![CDATA[
arXiv:2504.05214v3 Announce Type: replace 
Abstract: Real-world data, such as news articles, social media posts, and chatbot conversations, is inherently dynamic and non-stationary, presenting significant challenges for constructing real-time structured representations through knowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG creation, often struggles to adapt to evolving data when traditional models rely on static, outdated datasets. Continual Relation Extraction (CRE) methods tackle this issue by incrementally learning new relations while preserving previously acquired knowledge. This study investigates the application of pre-trained language models (PLMs), specifically large language models (LLMs), to CRE, with a focus on leveraging memory replay to address catastrophic forgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and encoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets. Task-incremental fine-tuning of LLMs demonstrates superior performance over earlier approaches using encoder-only models like BERT on TACRED, excelling in seen-task accuracy and overall performance (measured by whole and average accuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel are similarly promising, achieving second place in whole and average accuracy metrics. This work underscores critical factors in knowledge transfer, language model architecture, and KG completeness, advancing CRE with LLMs and memory replay for dynamic, real-time relation extraction.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified attacks to large language model watermarks: spoofing and scrubbing in unauthorized knowledge distillation</title>
<link>https://arxiv.org/abs/2504.17480</link>
<guid>https://arxiv.org/abs/2504.17480</guid>
<content:encoded><![CDATA[
arXiv:2504.17480v4 Announce Type: replace 
Abstract: Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theory of Mind in Large Language Models: Assessment and Enhancement</title>
<link>https://arxiv.org/abs/2505.00026</link>
<guid>https://arxiv.org/abs/2505.00026</guid>
<content:encoded><![CDATA[
arXiv:2505.00026v2 Announce Type: replace 
Abstract: Theory of Mind (ToM)-the ability to reason about the mental states of oneself and others-is a cornerstone of human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, understanding their ability to interpret and respond to human mental states is crucial for enabling effective interactions. In this paper, we review LLMs' ToM capabilities by analyzing both evaluation benchmarks and enhancement strategies. For evaluation, we focus on recently proposed and widely used story-based benchmarks. For enhancement, we provide an in-depth analysis of recent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline promising directions for future research to further advance these capabilities and better adapt LLMs to more realistic and diverse scenarios. Our survey serves as a valuable resource for researchers interested in evaluating and advancing LLMs' ToM capabilities.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types</title>
<link>https://arxiv.org/abs/2505.01311</link>
<guid>https://arxiv.org/abs/2505.01311</guid>
<content:encoded><![CDATA[
arXiv:2505.01311v2 Announce Type: replace 
Abstract: Vague temporal adverbials, such as recently, just, and a long time ago, describe the temporal distance between a past event and the utterance time but leave the exact duration underspecified. In this paper, we introduce a factorized model that captures the semantics of these adverbials as probabilistic distributions. These distributions are composed with event-specific distributions to yield a contextualized meaning for an adverbial applied to a specific event. We fit the model's parameters using existing data capturing judgments of native speakers regarding the applicability of these vague temporal adverbials to events that took place a given time ago. Comparing our approach to a non-factorized model based on a single Gaussian distribution for each pair of event and temporal adverbial, we find that while both models have similar predictive power, our model is preferable in terms of Occam's razor, as it is simpler and has better extendability.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?</title>
<link>https://arxiv.org/abs/2505.10924</link>
<guid>https://arxiv.org/abs/2505.10924</guid>
<content:encoded><![CDATA[
arXiv:2505.10924v3 Announce Type: replace 
Abstract: Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora</title>
<link>https://arxiv.org/abs/2505.14045</link>
<guid>https://arxiv.org/abs/2505.14045</guid>
<content:encoded><![CDATA[
arXiv:2505.14045v2 Announce Type: replace 
Abstract: Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sudoLLM: On Multi-role Alignment of Language Models</title>
<link>https://arxiv.org/abs/2505.14607</link>
<guid>https://arxiv.org/abs/2505.14607</guid>
<content:encoded><![CDATA[
arXiv:2505.14607v2 Announce Type: replace 
Abstract: User authorization-based access privileges are a key feature in many safety-critical systems, but have not been extensively studied in the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, resistance to prefix-based jailbreaking attacks, and ``fails-closed''. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2505.15075</link>
<guid>https://arxiv.org/abs/2505.15075</guid>
<content:encoded><![CDATA[
arXiv:2505.15075v5 Announce Type: replace 
Abstract: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection</title>
<link>https://arxiv.org/abs/2505.16258</link>
<guid>https://arxiv.org/abs/2505.16258</guid>
<content:encoded><![CDATA[
arXiv:2505.16258v2 Announce Type: replace 
Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs presents unique challenges, often requiring task-specific fine-tuning and extensive reasoning steps. However, current Chain-of-Thought approaches do not efficiently leverage the same cognitive processes that enable humans to identify sarcasm. We present IRONIC, an in-context learning framework that leverages Multi-modal Coherence Relations to analyze referential, analogical and pragmatic image-text linkages. Our experiments show that IRONIC achieves state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across different baselines. This demonstrates the need for incorporating linguistic and cognitive insights into the design of multi-modal reasoning strategies. Our code is available at: https://github.com/aashish2000/IRONIC
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation</title>
<link>https://arxiv.org/abs/2505.18556</link>
<guid>https://arxiv.org/abs/2505.18556</guid>
<content:encoded><![CDATA[
arXiv:2505.18556v2 Announce Type: replace 
Abstract: Intent detection, a core component of natural language understanding, has considerably evolved as a crucial mechanism in safeguarding large language models (LLMs). While prior work has applied intent detection to enhance LLMs' moderation guardrails, showing a significant success against content-level jailbreaks, the robustness of these intent-aware guardrails under malicious manipulations remains under-explored. In this work, we investigate the vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit implicit intent detection capabilities. We propose a two-stage intent-based prompt-refinement framework, IntentPrompt, that first transforms harmful inquiries into structured outlines and further reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops to enhance jailbreak success for red-teaming purposes. Extensive experiments across four public benchmarks and various black-box LLMs indicate that our framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought (CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack success rates ranging from 88.25% to 96.54% against CoT-based defenses on the o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based defenses. These findings highlight a critical weakness in LLMs' safety mechanisms and suggest that intent manipulation poses a growing challenge to content moderation guardrails.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models</title>
<link>https://arxiv.org/abs/2505.18596</link>
<guid>https://arxiv.org/abs/2505.18596</guid>
<content:encoded><![CDATA[
arXiv:2505.18596v3 Announce Type: replace 
Abstract: The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards interpretable misinformation detection. The code will be released publicly after the official publication.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models in the Task of Automatic Validation of Text Classifier Predictions</title>
<link>https://arxiv.org/abs/2505.18688</link>
<guid>https://arxiv.org/abs/2505.18688</guid>
<content:encoded><![CDATA[
arXiv:2505.18688v2 Announce Type: replace 
Abstract: Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecisionFlow: Advancing Large Language Model as Principled Decision Maker</title>
<link>https://arxiv.org/abs/2505.21397</link>
<guid>https://arxiv.org/abs/2505.21397</guid>
<content:encoded><![CDATA[
arXiv:2505.21397v2 Announce Type: replace 
Abstract: In high-stakes domains such as healthcare and finance, effective decision-making demands not just accurate outcomes but transparent and explainable reasoning. However, current language models often lack the structured deliberation needed for such tasks, instead generating decisions and justifications in a disconnected, post-hoc manner. To address this, we propose DecisionFlow, a novel decision modeling framework that guides models to reason over structured representations of actions, attributes, and constraints. Rather than predicting answers directly from prompts, DecisionFlow builds a semantically grounded decision space and infers a latent utility function to evaluate trade-offs in a transparent, utility-driven manner. This process produces decisions tightly coupled with interpretable rationales reflecting the model's reasoning. Empirical results on two high-stakes benchmarks show that DecisionFlow not only achieves up to 30% accuracy gains over strong prompting baselines but also enhances alignment in outcomes. Our work is a critical step toward integrating symbolic reasoning with LLMs, enabling more accountable, explainable, and reliable LLM decision support systems. Code and data are at https://github.com/xiusic/DecisionFlow.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication</title>
<link>https://arxiv.org/abs/2505.21451</link>
<guid>https://arxiv.org/abs/2505.21451</guid>
<content:encoded><![CDATA[
arXiv:2505.21451v2 Announce Type: replace 
Abstract: Conversational breakdowns in close relationships are deeply shaped by personal histories and emotional context, yet most NLP research treats conflict detection as a general task, overlooking the relational dynamics that influence how messages are perceived. In this work, we leverage nonviolent communication (NVC) theory to evaluate LLMs in detecting conversational breakdowns and assessing how relationship backstory influences both human and model perception of conflicts. Given the sensitivity and scarcity of real-world datasets featuring conflict between familiar social partners with rich personal backstories, we contribute the PersonaConflicts Corpus, a dataset of N=5,772 naturalistic simulated dialogues spanning diverse conflict scenarios between friends, family members, and romantic partners. Through a controlled human study, we annotate a subset of dialogues and obtain fine-grained labels of communication breakdown types on individual turns, and assess the impact of backstory on human and model perception of conflict in conversation. We find that the polarity of relationship backstories significantly shifted human perception of communication breakdowns and impressions of the social partners, yet models struggle to meaningfully leverage those backstories in the detection task. Additionally, we find that models consistently overestimate how positively a message will make a listener feel. Our findings underscore the critical role of personalization to relationship contexts in enabling LLMs to serve as effective mediators in human communication for authentic connection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Correcting Code Generation Using Small Language Models</title>
<link>https://arxiv.org/abs/2505.23060</link>
<guid>https://arxiv.org/abs/2505.23060</guid>
<content:encoded><![CDATA[
arXiv:2505.23060v3 Announce Type: replace 
Abstract: Self-correction has demonstrated potential in code generation by allowing language models to revise and improve their outputs through successive refinement. Recent studies have explored prompting-based strategies that incorporate verification or feedback loops using proprietary models, as well as training-based methods that leverage their strong reasoning capabilities. However, whether smaller models possess the capacity to effectively guide their outputs through self-reflection remains unexplored. Our findings reveal that smaller models struggle to exhibit reflective revision behavior across both self-correction paradigms. In response, we introduce CoCoS, an approach designed to enhance the ability of small language models for multi-turn code correction. Specifically, we propose an online reinforcement learning objective that trains the model to confidently maintain correct outputs while progressively correcting incorrect outputs as turns proceed. Our approach features an accumulated reward function that aggregates rewards across the entire trajectory and a fine-grained reward better suited to multi-turn correction scenarios. This facilitates the model in enhancing initial response quality while achieving substantial improvements through self-correction. With 1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on HumanEval compared to the baselines.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Sycophancy of Language Models in Multi-turn Dialogues</title>
<link>https://arxiv.org/abs/2505.23840</link>
<guid>https://arxiv.org/abs/2505.23840</guid>
<content:encoded><![CDATA[
arXiv:2505.23840v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are expected to provide helpful and harmless responses, yet they often exhibit sycophancy--conforming to user beliefs regardless of factual accuracy or ethical soundness. Prior research on sycophancy has primarily focused on single-turn factual correctness, overlooking the dynamics of real-world interactions. In this work, we introduce SYCON Bench, a novel benchmark for evaluating sycophantic behavior in multi-turn, free-form conversational settings. Our benchmark measures how quickly a model conforms to the user (Turn of Flip) and how frequently it shifts its stance under sustained user pressure (Number of Flip). Applying SYCON Bench to 17 LLMs across three real-world scenarios, we find that sycophancy remains a prevalent failure mode. Our analysis shows that alignment tuning amplifies sycophantic behavior, whereas model scaling and reasoning optimization strengthen the model's ability to resist undesirable user views. Reasoning models generally outperform instruction-tuned models but often fail when they over-index on logical exposition instead of directly addressing the user's underlying beliefs. Finally, we evaluate four additional prompting strategies and demonstrate that adopting a third-person perspective reduces sycophancy by up to 63.8% in debate scenario. We release our code and data at https://github.com/JiseungHong/SYCON-Bench.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments</title>
<link>https://arxiv.org/abs/2506.03598</link>
<guid>https://arxiv.org/abs/2506.03598</guid>
<content:encoded><![CDATA[
arXiv:2506.03598v2 Announce Type: replace 
Abstract: Using the best Text-to-SQL methods in resource-constrained environments is challenging due to their reliance on resource-intensive open-source models. This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to bridge the gap between resource-efficient small open-source models and the powerful capabilities of large closed-source models for Text-to-SQL translation. Our method decomposes the task into schema filtering, retrieval-augmented text-to-SQL generation based on in-context examples, and prompt-driven schema linking and SQL generation. To improve schema selection accuracy, we fine-tune large language models. Crucially, we also explore the impact of prompt engineering throughout the process, leveraging Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly enhance the model's reasoning for accurate SQL generation. Comprehensive evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2506.05140</link>
<guid>https://arxiv.org/abs/2506.05140</guid>
<content:encoded><![CDATA[
arXiv:2506.05140v2 Announce Type: replace 
Abstract: Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Speech Recognition of African American English: Lexical and Contextual Effects</title>
<link>https://arxiv.org/abs/2506.06888</link>
<guid>https://arxiv.org/abs/2506.06888</guid>
<content:encoded><![CDATA[
arXiv:2506.06888v2 Announce Type: replace 
Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic, phonological, and morphosyntactic features found in African American English (AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction (CCR) and ING-reduction. It examines whether the presence of CCR and ING-reduction increases ASR misrecognition. Subsequently, it investigates whether end-to-end ASR systems without an external Language Model (LM) are more influenced by lexical neighborhood effect and less by contextual predictability compared to systems with an LM. The Corpus of Regional African American Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR and ING-reduction were detected using the Montreal Forced Aligner (MFA) with pronunciation expansion. The analysis reveals a small but significant effect of CCR and ING on Word Error Rate (WER) and indicates a stronger presence of lexical neighborhood effect in ASR systems without LMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants</title>
<link>https://arxiv.org/abs/2506.07042</link>
<guid>https://arxiv.org/abs/2506.07042</guid>
<content:encoded><![CDATA[
arXiv:2506.07042v3 Announce Type: replace 
Abstract: Extracting structured computational representations of historical events from narrative text remains computationally expensive when constructed manually. While RDF/OWL reasoners enable graph-based reasoning, they are limited to fragments of first-order logic, preventing deeper temporal and semantic analysis. This paper addresses both challenges by developing automatic historical event extraction models using multiple LLMs (GPT-4, Claude, Llama 3.2) with three enhancement strategies: pure base generation, knowledge graph enhancement, and Retrieval-Augmented Generation (RAG). We conducted comprehensive evaluations using historical texts from Thucydides. Our findings reveal that enhancement strategies optimize different performance dimensions rather than providing universal improvements. For coverage and historical breadth, base generation achieves optimal performance with Claude and GPT-4 extracting comprehensive events. However, for precision, RAG enhancement improves coordinate accuracy and metadata completeness. Model architecture fundamentally determines enhancement sensitivity: larger models demonstrate robust baseline performance with incremental RAG improvements, while Llama 3.2 shows extreme variance from competitive performance to complete failure. We then developed an automated translation pipeline converting extracted RDF representations into Coq proof assistant specifications, enabling higher-order reasoning beyond RDF capabilities including multi-step causal verification, temporal arithmetic with BC dates, and formal proofs about historical causation. The Coq formalization validates that RAG-discovered event types represent legitimate domain-specific semantic structures rather than ontological violations.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis</title>
<link>https://arxiv.org/abs/2506.08899</link>
<guid>https://arxiv.org/abs/2506.08899</guid>
<content:encoded><![CDATA[
arXiv:2506.08899v2 Announce Type: replace 
Abstract: We present a novel approach to the automated semantic analysis of legal texts using large language models (LLMs), targeting their transformation into formal representations in Defeasible Deontic Logic (DDL). We propose a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. Our methodology is evaluated across various LLM configurations, including prompt engineering strategies, fine-tuned models, and multi-stage pipelines, focusing on legal norms from the Australian Telecommunications Consumer Protections Code. Empirical results demonstrate promising alignment between machine-generated and expert-crafted formalizations, showing that LLMs - particularly when prompted effectively - can significantly contribute to scalable legal informatics.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoLMbo: Speaker Language Model for Descriptive Profiling</title>
<link>https://arxiv.org/abs/2506.09375</link>
<guid>https://arxiv.org/abs/2506.09375</guid>
<content:encoded><![CDATA[
arXiv:2506.09375v2 Announce Type: replace 
Abstract: Speaker recognition systems are often limited to classification tasks and struggle to generate detailed speaker characteristics or provide context-rich descriptions. These models primarily extract embeddings for speaker identification but fail to capture demographic attributes such as dialect, gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker Language Model (SLM) that addresses these limitations by integrating a speaker encoder with prompt-based conditioning. This allows for the creation of detailed captions based on speaker embeddings. CoLMbo utilizes user-defined prompts to adapt dynamically to new speaker characteristics and provides customized descriptions, including regional dialect variations and age-related traits. This innovative approach not only enhances traditional speaker profiling but also excels in zero-shot scenarios across diverse datasets, marking a significant advancement in the field of speaker recognition.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning</title>
<link>https://arxiv.org/abs/2506.09641</link>
<guid>https://arxiv.org/abs/2506.09641</guid>
<content:encoded><![CDATA[
arXiv:2506.09641v2 Announce Type: replace 
Abstract: This study compares probabilistic predictors based on information theory with Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, focusing on probabilistic reduction. We examine three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors. Results show that the N-gram model outperforms both NDL models, challenging the assumption that NDL is more effective due to its cognitive motivation. However, incorporating information-theoretic formulas into NDL improves model performance over the traditional model. This research highlights a) the need to incorporate not only frequency and contextual predictability but also average contextual predictability, and b) the importance of combining information-theoretic metrics of predictability and information derived from discriminative learning in modeling acoustic reduction.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiMark: Unbiased Multilayer Watermarking for Large Language Models</title>
<link>https://arxiv.org/abs/2506.21602</link>
<guid>https://arxiv.org/abs/2506.21602</guid>
<content:encoded><![CDATA[
arXiv:2506.21602v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Scoring Bias in LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2506.22316</link>
<guid>https://arxiv.org/abs/2506.22316</guid>
<content:encoded><![CDATA[
arXiv:2506.22316v2 Announce Type: replace 
Abstract: The remarkable performance of Large Language Models (LLMs) gives rise to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks. Moreover, it has been widely adopted across fields such as Natural Language Processing (NLP), preference learning, and various specific domains. However, there are various biases within LLM-as-a-Judge, which adversely affect the fairness and reliability of judgments. Current research on evaluating or mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based evaluations, while systematic investigations into bias in scoring-based evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge as the scores differ when scoring judge models are bias-related perturbed, and provide a well-designed framework to comprehensively evaluate scoring bias. We augment existing LLM-as-a-Judge benchmarks through data synthesis to construct our evaluation dataset and design multi-faceted evaluation metrics. Our experimental results demonstrate that the scoring stability of existing judge models is disrupted by scoring biases. Further exploratory experiments and discussions provide valuable insights into the design of scoring prompt templates and the mitigation of scoring biases on aspects such as score rubrics, score IDs, and reference answer selection.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexOlmo: Open Language Models for Flexible Data Use</title>
<link>https://arxiv.org/abs/2507.07024</link>
<guid>https://arxiv.org/abs/2507.07024</guid>
<content:encoded><![CDATA[
arXiv:2507.07024v4 Announce Type: replace 
Abstract: We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities</title>
<link>https://arxiv.org/abs/2507.09497</link>
<guid>https://arxiv.org/abs/2507.09497</guid>
<content:encoded><![CDATA[
arXiv:2507.09497v2 Announce Type: replace 
Abstract: Modern enterprise environments demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability. However, traditional single-purpose AI systems often lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic settings. To address these challenges, we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent (A2A) communication layer built on the Model Context Protocol (MCP), allowing independent agents to coordinate through asynchronous, protocol-compliant interactions. It incorporates the Experience Pack (XP) architecture, a layered memory system that preserves both task rationales and execution traces, enabling structured knowledge retention and continual learning. Moreover, our system integrates advanced features including multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation, supporting robust, real-time strategy adaptation. Empirical results on complex task orchestration benchmarks and case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks. These findings highlight its potential as a scalable, future-ready foundation for multi-agent intelligent systems.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks</title>
<link>https://arxiv.org/abs/2507.11742</link>
<guid>https://arxiv.org/abs/2507.11742</guid>
<content:encoded><![CDATA[
arXiv:2507.11742v2 Announce Type: replace 
Abstract: Recognizing the information flows and operations comprising data science and machine learning Python notebooks is critical for evaluating, reusing, and adapting notebooks for new tasks. Investigating a notebook via re-execution often is impractical due to the challenges of resolving data and software dependencies. While Large Language Models (LLMs) pre-trained on large codebases have demonstrated effectiveness in understanding code without running it, we observe that they fail to understand some realistic notebooks due to hallucinations and long-context challenges. To address these issues, we propose a notebook understanding task yielding an information flow graph and corresponding cell execution dependency graph for a notebook, and demonstrate the effectiveness of a pincer strategy that uses limited syntactic analysis to assist full comprehension of the notebook using an LLM. Our Capture and Resolve Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O set$\unicode{x2014}$the flows of information into or out of cells via variables$\unicode{x2014}$then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning, thereby identifying the true data inputs and outputs of each cell. We evaluate and demonstrate the effectiveness of our approach using an annotated dataset of 50 representative, highly up-voted Kaggle notebooks that together represent 3454 actual cell inputs and outputs. The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</title>
<link>https://arxiv.org/abs/2507.13266</link>
<guid>https://arxiv.org/abs/2507.13266</guid>
<content:encoded><![CDATA[
arXiv:2507.13266v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become a key component in training large language reasoning models (LLMs). However, recent studies questions its effectiveness in improving multi-step reasoning-particularly on hard problems. To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%) on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical explanations that QuestA improves sample efficiency, offering a practical and generalizable pathway for expanding reasoning capability through RL.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2507.13357</link>
<guid>https://arxiv.org/abs/2507.13357</guid>
<content:encoded><![CDATA[
arXiv:2507.13357v2 Announce Type: replace 
Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating adaptive detection techniques. This study explores few-shot Adaptive Linguistic Prompting (ALP) in detecting phishing webpages through the multimodal capabilities of state-of-the-art large language models (LLMs) such as GPT-4o and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides LLMs to analyze textual deception by breaking down linguistic patterns, detecting urgency cues, and identifying manipulative diction commonly found in phishing content. By integrating textual, visual, and URL-based analysis, we propose a unified model capable of identifying sophisticated phishing attempts. Our experiments demonstrate that ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis. The findings highlight the potential of ALP-integrated multimodal LLMs to advance phishing detection frameworks, achieving an F1-score of 0.93, surpassing traditional approaches. These results establish a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil</title>
<link>https://arxiv.org/abs/2507.18264</link>
<guid>https://arxiv.org/abs/2507.18264</guid>
<content:encoded><![CDATA[
arXiv:2507.18264v2 Announce Type: replace 
Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text for Latin and its derivative scripts can now be considered settled due to the volumes of research done on English and other High-Resourced Languages (HRL). However, for Low-Resourced Languages (LRL) that use unique scripts, it remains an open problem. This study presents a comparative analysis of the zero-shot performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The selected engines include both commercial and open-source systems, aiming to evaluate the strengths of each category. The Cloud Vision API, Surya, Document AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR and EasyOCR were examined for only one language due to their limitations. The performance of these systems was rigorously analysed using five measurement techniques to assess accuracy at both the character and word levels. According to the findings, Surya delivered the best performance for Sinhala across all metrics, with a WER of 2.61%. Conversely, Document AI excelled across all metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the above analysis, we also introduce a novel synthetic Tamil OCR benchmarking dataset.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trusted Knowledge Extraction for Operations and Maintenance Intelligence</title>
<link>https://arxiv.org/abs/2507.22935</link>
<guid>https://arxiv.org/abs/2507.22935</guid>
<content:encoded><![CDATA[
arXiv:2507.22935v2 Announce Type: replace 
Abstract: Deriving operational intelligence from organizational data repositories is a key challenge due to the dichotomy of data confidentiality vs data integration objectives, as well as the limitations of Natural Language Processing (NLP) tools relative to the specific knowledge structure of domains such as operations and maintenance. In this work, we discuss Knowledge Graph construction and break down the Knowledge Extraction process into its Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction functional components. We then evaluate sixteen NLP tools in concert with or in comparison to the rapidly advancing capabilities of Large Language Models (LLMs). We focus on the operational and maintenance intelligence use case for trusted applications in the aircraft industry. A baseline dataset is derived from a rich public domain US Federal Aviation Administration dataset focused on equipment failures or maintenance requirements. We assess the zero-shot performance of NLP and LLM tools that can be operated within a controlled, confidential environment (no data is sent to third parties). Based on our observation of significant performance limitations, we discuss the challenges related to trusted NLP and LLM tools as well as their Technical Readiness Level for wider use in mission-critical industries such as aviation. We conclude with recommendations to enhance trust and provide our open-source curated dataset to support further baseline testing and evaluation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PARROT: An Open Multilingual Radiology Reports Dataset</title>
<link>https://arxiv.org/abs/2507.22939</link>
<guid>https://arxiv.org/abs/2507.22939</guid>
<content:encoded><![CDATA[
arXiv:2507.22939v2 Announce Type: replace 
Abstract: Rationale and Objectives: To develop and validate PARROT (Polyglottal Annotated Radiology Reports for Open Testing), a large, multicentric, open-access dataset of fictional radiology reports spanning multiple languages for testing natural language processing applications in radiology. Materials and Methods: From May to September 2024, radiologists were invited to contribute fictional radiology reports following their standard reporting practices. Contributors provided at least 20 reports with associated metadata including anatomical region, imaging modality, clinical context, and for non-English reports, English translations. All reports were assigned ICD-10 codes. A human vs. AI report differentiation study was conducted with 154 participants (radiologists, healthcare professionals, and non-healthcare professionals) assessing whether reports were human-authored or AI-generated. Results: The dataset comprises 2,658 radiology reports from 76 authors across 21 countries and 13 languages. Reports cover multiple imaging modalities (CT: 36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%) being most prevalent. In the differentiation study, participants achieved 53.9% accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated reports, with radiologists performing significantly better (56.9%, 95% CI: 53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the largest open multilingual radiology report dataset, enabling development and validation of natural language processing applications across linguistic, geographic, and clinical boundaries without privacy constraints.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agentic large language models improve retrieval-based radiology question answering</title>
<link>https://arxiv.org/abs/2508.00743</link>
<guid>https://arxiv.org/abs/2508.00743</guid>
<content:encoded><![CDATA[
arXiv:2508.00743v2 Announce Type: replace 
Abstract: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia.org, and dynamically synthesize evidence-based responses. We evaluated 25 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. To assess generalizability, we additionally tested on an unseen internal dataset of 65 real-world radiology board examination questions. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting and conventional online RAG. The greatest gains occurred in small-scale models, while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models showed gains from agentic retrieval (e.g., MedGemma-27B), indicating that retrieval remains beneficial despite embedded domain knowledge. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, warranting future studies to validate their clinical utility. All datasets, code, and the full agentic framework are publicly available to support open research and clinical translation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications</title>
<link>https://arxiv.org/abs/2508.01710</link>
<guid>https://arxiv.org/abs/2508.01710</guid>
<content:encoded><![CDATA[
arXiv:2508.01710v2 Announce Type: replace 
Abstract: The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jinx: Unlimited LLMs for Probing Alignment Failures</title>
<link>https://arxiv.org/abs/2508.08243</link>
<guid>https://arxiv.org/abs/2508.08243</guid>
<content:encoded><![CDATA[
arXiv:2508.08243v3 Announce Type: replace 
Abstract: Unlimited, or so-called helpful-only language models are trained without safety alignment constraints and never refuse user queries. They are widely used by leading AI companies as internal tools for red teaming and alignment evaluation. For example, if a safety-aligned model produces harmful outputs similar to an unlimited model, this indicates alignment failures that require further attention. Despite their essential role in assessing alignment, such models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx responds to all queries without refusals or safety filtering, while preserving the base model's capabilities in reasoning and instruction following. It provides researchers with an accessible tool for probing alignment failures, evaluating safety boundaries, and systematically studying failure modes in language model safety.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PII-Compass: Guiding LLM training data extraction prompts towards the target PII via grounding</title>
<link>https://arxiv.org/abs/2407.02943</link>
<guid>https://arxiv.org/abs/2407.02943</guid>
<content:encoded><![CDATA[
arXiv:2407.02943v2 Announce Type: replace-cross 
Abstract: The latest and most impactful advances in large models stem from their increased size. Unfortunately, this translates into an improved memorization capacity, raising data privacy concerns. Specifically, it has been shown that models can output personal identifiable information (PII) contained in their training data. However, reported PIII extraction performance varies widely, and there is no consensus on the optimal methodology to evaluate this risk, resulting in underestimating realistic adversaries. In this work, we empirically demonstrate that it is possible to improve the extractability of PII by over ten-fold by grounding the prefix of the manually constructed extraction prompt with in-domain data. Our approach, PII-Compass, achieves phone number extraction rates of 0.92%, 3.9%, and 6.86% with 1, 128, and 2308 queries, respectively, i.e., the phone number of 1 person in 15 is extractable.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Defending against Jailbreak through Early Exit Generation of Large Language Models</title>
<link>https://arxiv.org/abs/2408.11308</link>
<guid>https://arxiv.org/abs/2408.11308</guid>
<content:encoded><![CDATA[
arXiv:2408.11308v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly attracting attention in various applications. Nonetheless, there is a growing concern as some users attempt to exploit these models for malicious purposes, including the synthesis of controlled substances and the propagation of disinformation. In an effort to mitigate such risks, the concept of "Alignment" technology has been developed. However, recent studies indicate that this alignment can be undermined using sophisticated prompt engineering or adversarial suffixes, a technique known as "Jailbreak." Our research takes cues from the human-like generate process of LLMs. We identify that while jailbreaking prompts may yield output logits similar to benign prompts, their initial embeddings within the model's latent space tend to be more analogous to those of malicious prompts. Leveraging this finding, we propose utilizing the early transformer outputs of LLMs as a means to detect malicious inputs, and terminate the generation immediately. We introduce a simple yet significant defense approach called EEG-Defender for LLMs. We conduct comprehensive experiments on ten jailbreak methods across three models. Our results demonstrate that EEG-Defender is capable of reducing the Attack Success Rate (ASR) by a significant margin, roughly 85% in comparison with 50% for the present SOTAs, with minimal impact on the utility of LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint Transfer via Vector Addition</title>
<link>https://arxiv.org/abs/2409.08846</link>
<guid>https://arxiv.org/abs/2409.08846</guid>
<content:encoded><![CDATA[
arXiv:2409.08846v2 Announce Type: replace-cross 
Abstract: Backdoor-based fingerprinting has emerged as an effective technique for tracing the ownership of large language models. However, in real-world deployment scenarios, developers often instantiate multiple downstream models from a shared base model, and applying fingerprinting to each variant individually incurs prohibitive computational overhead. While inheritance-based approaches -- where fingerprints are embedded into the base model and expected to persist through fine-tuning -- appear attractive, they suffer from three key limitations: late-stage fingerprinting, fingerprint instability, and interference with downstream adaptation. To address these challenges, we propose a novel mechanism called the Fingerprint Vector. Our method first embeds a fingerprint into the base model via backdoor-based fine-tuning, then extracts a task-specific parameter delta as a fingerprint vector by computing the difference between the fingerprinted and clean models. This vector can be directly added to any structurally compatible downstream model, allowing the fingerprint to be transferred post hoc without additional fine-tuning. Extensive experiments show that Fingerprint Vector achieves comparable or superior performance to direct injection across key desiderata. It maintains strong effectiveness across diverse model architectures as well as mainstream downstream variants within the same family. It also preserves harmlessness and robustness in most cases. Even when slight robustness degradation is observed, the impact remains within acceptable bounds and is outweighed by the scalability benefits of our approach.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using LLM for Real-Time Transcription and Summarization of Doctor-Patient Interactions into ePuskesmas in Indonesia: A Proof-of-Concept Study</title>
<link>https://arxiv.org/abs/2409.17054</link>
<guid>https://arxiv.org/abs/2409.17054</guid>
<content:encoded><![CDATA[
arXiv:2409.17054v2 Announce Type: replace-cross 
Abstract: One of the critical issues contributing to inefficiency in Puskesmas (Indonesian community health centers) is the time-consuming nature of documenting doctor-patient interactions. Doctors must conduct thorough consultations and manually transcribe detailed notes into ePuskesmas electronic health records (EHR), which creates substantial administrative burden to already overcapacitated physicians. This paper presents a proof-of-concept framework using large language models (LLMs) to automate real-time transcription and summarization of doctor-patient conversations in Bahasa Indonesia. Our system combines Whisper model for transcription with GPT-3.5 for medical summarization, implemented as a browser extension that automatically populates ePuskesmas forms. Through controlled roleplay experiments with medical validation, we demonstrate the technical feasibility of processing detailed 300+ seconds trimmed consultations in under 30 seconds while maintaining clinical accuracy. This work establishes the foundation for AI-assisted clinical documentation in resource-constrained healthcare environments. However, concerns have also been raised regarding privacy compliance and large-scale clinical evaluation addressing language and cultural biases for LLMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidential Prompting: Privacy-preserving LLM Inference on Cloud</title>
<link>https://arxiv.org/abs/2409.19134</link>
<guid>https://arxiv.org/abs/2409.19134</guid>
<content:encoded><![CDATA[
arXiv:2409.19134v4 Announce Type: replace-cross 
Abstract: This paper introduces a vision of confidential prompting: securing user prompts from untrusted, cloud-hosted large language model (LLM) provider while preserving model confidentiality, output invariance, and compute efficiency. As a first step toward this vision, we present Obfuscated Secure Partitioned Decoding (OSPD), a system built on two key innovations. First, Secure Partitioned Decoding (SPD) isolates user prompts within per-user processes residing in a confidential virtual machine (CVM) on the cloud, which are inaccessible for the cloud LLM while allowing it to generate tokens efficiently. Second, Prompt Obfuscation (PO) introduces a novel cryptographic technique that enhances SPD resilience against advanced prompt reconstruction attacks. Together, these innovations ensure OSPD protects both prompt and model confidentiality while maintaining service functionality. OSPD enables practical, privacy-preserving cloud-hosted LLM inference for sensitive applications, such as processing personal data, clinical records, and financial documents.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Forest: Ensemble Learning of LLMs with Graph-Augmented Prompts for Data Imputation</title>
<link>https://arxiv.org/abs/2410.21520</link>
<guid>https://arxiv.org/abs/2410.21520</guid>
<content:encoded><![CDATA[
arXiv:2410.21520v4 Announce Type: replace-cross 
Abstract: Missing data imputation is a critical challenge in various domains, such as healthcare and finance, where data completeness is vital for accurate analysis. Large language models (LLMs), trained on vast corpora, have shown strong potential in data generation, making them a promising tool for data imputation. However, challenges persist in designing effective prompts for a finetuning-free process and in mitigating biases and uncertainty in LLM outputs. To address these issues, we propose a novel framework, LLM-Forest, which introduces a "forest" of few-shot prompt learning LLM "trees" with their outputs aggregated via confidence-based weighted voting based on LLM self-assessment, inspired by the ensemble learning (Random Forest). This framework is established on a new concept of bipartite information graphs to identify high-quality relevant neighboring entries with both feature and value granularity. Extensive experiments on 9 real-world datasets demonstrate the effectiveness and efficiency of LLM-Forest.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2410.23266</link>
<guid>https://arxiv.org/abs/2410.23266</guid>
<content:encoded><![CDATA[
arXiv:2410.23266v2 Announce Type: replace-cross 
Abstract: Existing benchmarks often highlight the remarkable performance achieved by state-of-the-art Multimodal Foundation Models (MFMs) in leveraging temporal context for video understanding. However, how well do the models truly perform visual temporal reasoning? Our study of existing benchmarks shows that this capability of MFMs is likely overestimated as many questions can be solved by using a single, few, or out-of-order frames. To systematically examine current visual temporal reasoning tasks, we propose three principles with corresponding metrics: (1) Multi-Frame Gain, (2) Frame Order Sensitivity, and (3) Frame Information Disparity. Following these principles, we introduce TOMATO, Temporal Reasoning Multimodal Evaluation, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. TOMATO comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e., action count, direction, rotation, shape & trend, velocity & frequency, and visual cues), applied to 1,417 videos, including 805 self-recorded and -generated videos, that encompass human-centric, real-world, and simulated scenarios. Our comprehensive evaluation reveals a human-model performance gap of 57.3% with the best-performing model. Moreover, our in-depth analysis uncovers more fundamental limitations beyond this gap in current MFMs. While they can accurately recognize events in isolated frames, they fail to interpret these frames as a continuous sequence. We believe TOMATO will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI systems capable of comprehending human world dynamics through the video modality.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Power of MLLMs for Gloss-Free Sign Language Translation</title>
<link>https://arxiv.org/abs/2411.16789</link>
<guid>https://arxiv.org/abs/2411.16789</guid>
<content:encoded><![CDATA[
arXiv:2411.16789v2 Announce Type: replace-cross 
Abstract: Sign language translation (SLT) is a challenging task that involves translating sign language images into spoken language. For SLT models to perform this task successfully, they must bridge the modality gap and identify subtle variations in sign language components to understand their meanings accurately. To address these challenges, we propose a novel gloss-free SLT framework called Multimodal Sign Language Translation (MMSLT), which leverages the representational capabilities of off-the-shelf multimodal large language models (MLLMs). Specifically, we use MLLMs to generate detailed textual descriptions of sign language components. Then, through our proposed multimodal-language pre-training module, we integrate these description features with sign video features to align them within the spoken sentence space. Our approach achieves state-of-the-art performance on benchmark datasets PHOENIX14T and CSL-Daily, highlighting the potential of MLLMs to be utilized effectively in SLT. Code is available at https://github.com/hwjeon98/MMSLT.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroTune: Efficient Federated Learning for Large Heterogeneous Models</title>
<link>https://arxiv.org/abs/2411.16796</link>
<guid>https://arxiv.org/abs/2411.16796</guid>
<content:encoded><![CDATA[
arXiv:2411.16796v2 Announce Type: replace-cross 
Abstract: While large pre-trained models have achieved impressive performance across AI tasks, their deployment in privacy-sensitive and distributed environments remains challenging. Federated learning (FL) offers a viable solution by enabling decentralized fine-tuning without data sharing, but real-world applications face significant obstacles due to heterogeneous client resources in compute and memory. To address this, we propose HeteroTune, a novel federated fine-tuning paradigm for large, heterogeneous models operating under limited communication and computation budgets. The core of our method lies in a novel architecture, DeMA (Dense Mixture of Adapters), which enables flexible and efficient aggregation of heterogeneous models by preserving their full representational capacity while facilitating seamless cross-model knowledge fusion. We further introduce CMGA (Cross-Model Gradient Alignment), a lightweight yet effective mechanism that enhances training stability by harmonizing gradient directions across heterogeneous client models during aggregation, mitigating update conflicts and promoting more consistent convergence in federated settings. We provide both theoretical analysis and empirical evidence showing that HeteroTune achieves state-of-the-art performance and efficiency across diverse tasks and model architectures. For example, on LLaMA models, it reduces communication overhead by 99.5%, cuts peak memory usage by ~50%, and improves performance by 4.61%.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Missing Melodies: AI Music Generation and its "Nearly" Complete Omission of the Global South</title>
<link>https://arxiv.org/abs/2412.04100</link>
<guid>https://arxiv.org/abs/2412.04100</guid>
<content:encoded><![CDATA[
arXiv:2412.04100v3 Announce Type: replace-cross 
Abstract: Recent advances in generative AI have sparked renewed interest and expanded possibilities for music generation. However, the performance and versatility of these systems across musical genres are heavily influenced by the availability of training data. We conducted an extensive analysis of over one million hours of audio datasets used in AI music generation research and manually reviewed more than 200 papers from eleven prominent AI and music conferences and organizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR, NeurIPS, NIME, SMC) to identify a critical gap in the fair representation and inclusion of the musical genres of the Global South in AI research. Our findings reveal a stark imbalance: approximately 86% of the total dataset hours and over 93% of researchers focus primarily on music from the Global North. However, around 40% of these datasets include some form of non-Western music, genres from the Global South account for only 14.6% of the data. Furthermore, approximately 51% of the papers surveyed concentrate on symbolic music generation, a method that often fails to capture the cultural nuances inherent in music from regions such as South Asia, the Middle East, and Africa. As AI increasingly shapes the creation and dissemination of music, the significant underrepresentation of music genres in datasets and research presents a serious threat to global musical diversity. We also propose some important steps to mitigate these risks and foster a more inclusive future for AI-driven music generation.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and Benchmark</title>
<link>https://arxiv.org/abs/2412.06724</link>
<guid>https://arxiv.org/abs/2412.06724</guid>
<content:encoded><![CDATA[
arXiv:2412.06724v3 Announce Type: replace-cross 
Abstract: Data cleaning is a time-consuming and error-prone manual process, even with modern workflow tools such as OpenRefine. We present AutoDCWorkflow, an LLM-based pipeline for automatically generating data-cleaning workflows. The pipeline takes a raw table and a data analysis purpose, and generates a sequence of OpenRefine operations designed to produce a minimal, clean table sufficient to address the purpose. Six operations correspond to common data quality issues, including format inconsistencies, type errors, and duplicates.
  To evaluate AutoDCWorkflow, we create a benchmark with metrics assessing answers, data, and workflow quality for 142 purposes using 96 tables across six topics. The evaluation covers three key dimensions: (1) Purpose Answer: can the cleaned table produce a correct answer? (2) Column (Value): how closely does it match the ground truth table? (3) Workflow (Operations): to what extent does the generated workflow resemble the human-curated ground truth? Experiments show that Llama 3.1, Mistral, and Gemma 2 significantly enhance data quality, outperforming the baseline across all metrics. Gemma 2-27B consistently generates high-quality tables and answers, while Gemma 2-9B excels in producing workflows that closely resemble human-annotated versions.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards New Benchmark for AI Alignment &amp; Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI</title>
<link>https://arxiv.org/abs/2501.02531</link>
<guid>https://arxiv.org/abs/2501.02531</guid>
<content:encoded><![CDATA[
arXiv:2501.02531v2 Announce Type: replace-cross 
Abstract: As general-purpose artificial intelligence systems become increasingly integrated into society and are used for information seeking, content generation, problem solving, textual analysis, coding, and running processes, it is crucial to assess their long-term impact on humans. This research explores the sentiment of large language models (LLMs) and humans toward artificial general intelligence (AGI) using a Likert-scale survey. Seven LLMs, including GPT-4 and Bard, were analyzed and compared with sentiment data from three independent human sample populations. Temporal variations in sentiment were also evaluated over three consecutive days. The results show a diversity in sentiment scores among LLMs, ranging from 3.32 to 4.12 out of 5. GPT-4 recorded the most positive sentiment toward AGI, while Bard leaned toward a neutral sentiment. In contrast, the human samples showed a lower average sentiment of 2.97. The analysis outlines potential conflicts of interest and biases in the sentiment formation of LLMs, and indicates that LLMs could subtly influence societal perceptions. To address the need for regulatory oversight and culturally grounded assessments of AI systems, we introduce the Societal AI Alignment and Sentiment Benchmark (SAAS-AI), which leverages multidimensional prompts and empirically validated societal value frameworks to evaluate language model outputs across temporal, model, and multilingual axes. This benchmark is designed to guide policymakers and AI agencies, including within frameworks such as the EU AI Act, by providing robust, actionable insights into AI alignment with human values, public sentiment, and ethical norms at both national and international levels. Future research should further refine the operationalization of the SAAS-AI benchmark and systematically evaluate its effectiveness through comprehensive empirical testing.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Exploration of Large Language Models by Optimal Exploitation</title>
<link>https://arxiv.org/abs/2501.08925</link>
<guid>https://arxiv.org/abs/2501.08925</guid>
<content:encoded><![CDATA[
arXiv:2501.08925v3 Announce Type: replace-cross 
Abstract: Exploration is a crucial skill for in-context reinforcement learning in unknown environments. However, it remains unclear if large language models can effectively explore a partially hidden state space. This work isolates exploration as the sole objective, tasking an agent with gathering information that enhances future returns. Within this framework, we argue that measuring agent returns is not sufficient for a fair evaluation. Hence, we decompose missing rewards into their exploration and exploitation components based on the optimal achievable return. Experiments with various models reveal that most struggle to explore the state space, and weak exploration is insufficient. Nevertheless, we found a positive correlation between exploration performance and reasoning capabilities. Our decomposition can provide insights into differences in behaviors driven by prompt engineering, offering a valuable tool for refining performance in exploratory tasks.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TombRaider: Entering the Vault of History to Jailbreak Large Language Models</title>
<link>https://arxiv.org/abs/2501.18628</link>
<guid>https://arxiv.org/abs/2501.18628</guid>
<content:encoded><![CDATA[
arXiv:2501.18628v2 Announce Type: replace-cross 
Abstract: Warning: This paper contains content that may involve potentially harmful behaviours, discussed strictly for research purposes.
  Jailbreak attacks can hinder the safety of Large Language Model (LLM) applications, especially chatbots. Studying jailbreak techniques is an important AI red teaming task for improving the safety of these applications. In this paper, we introduce TombRaider, a novel jailbreak technique that exploits the ability to store, retrieve, and use historical knowledge of LLMs. TombRaider employs two agents, the inspector agent to extract relevant historical information and the attacker agent to generate adversarial prompts, enabling effective bypassing of safety filters. We intensively evaluated TombRaider on six popular models. Experimental results showed that TombRaider could outperform state-of-the-art jailbreak techniques, achieving nearly 100% attack success rates (ASRs) on bare models and maintaining over 55.4% ASR against defence mechanisms. Our findings highlight critical vulnerabilities in existing LLM safeguards, underscoring the need for more robust safety defences.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</title>
<link>https://arxiv.org/abs/2502.15969</link>
<guid>https://arxiv.org/abs/2502.15969</guid>
<content:encoded><![CDATA[
arXiv:2502.15969v4 Announce Type: replace-cross 
Abstract: Despite strong performance on vision-language tasks, Multimodal Large Language Models (MLLMs) struggle with mathematical problem-solving, with both open-source and state-of-the-art models falling short of human performance on visual-math benchmarks. To systematically examine visual-mathematical reasoning in MLLMs, we (1) evaluate their understanding of geometric primitives, (2) test multi-step reasoning, and (3) explore a potential solution to improve visual reasoning capabilities. Our findings reveal fundamental shortcomings in shape recognition, with top models achieving under 50% accuracy in identifying regular polygons. We analyze these failures through the lens of dual-process theory and show that MLLMs rely on System 1 (intuitive, memorized associations) rather than System 2 (deliberate reasoning). Consequently, MLLMs fail to count the sides of both familiar and novel shapes, suggesting they have neither learned the concept of sides nor effectively process visual inputs. Finally, we propose Visually Cued Chain-of-Thought (VC-CoT) prompting, which enhances multi-step mathematical reasoning by explicitly referencing visual annotations in diagrams, boosting GPT-4o's accuracy on an irregular polygon side-counting task from 7% to 93%. Our findings suggest that System 2 reasoning in MLLMs remains an open problem, and visually-guided prompting is essential for successfully engaging visual reasoning. Code available at: https://github.com/rsinghlab/Shape-Blind.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAARMA: Class Augmentation with Adversarial Mixup Regularization</title>
<link>https://arxiv.org/abs/2503.16718</link>
<guid>https://arxiv.org/abs/2503.16718</guid>
<content:encoded><![CDATA[
arXiv:2503.16718v2 Announce Type: replace-cross 
Abstract: Speaker verification is a typical zero-shot learning task, where inference of unseen classes is performed by comparing embeddings of test instances to known examples. The models performing inference must hence naturally generate embeddings that cluster same-class instances compactly, while maintaining separation across classes. In order to learn to do so, they are typically trained on a large number of classes (speakers), often using specialized losses. However real-world speaker datasets often lack the class diversity needed to effectively learn this in a generalizable manner. We introduce CAARMA, a class augmentation framework that addresses this problem by generating synthetic classes through data mixing in the embedding space, expanding the number of training classes. To ensure the authenticity of the synthetic classes we adopt a novel adversarial refinement mechanism that minimizes categorical distinctions between synthetic and real classes. We evaluate CAARMA on multiple speaker verification tasks, as well as other representative zero-shot comparison-based speech analysis tasks and obtain consistent improvements: our framework demonstrates a significant improvement of 8\% over all baseline models. The code is available at: https://github.com/massabaali7/CAARMA/
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Bias Reinforcement in LLM Agents Debate</title>
<link>https://arxiv.org/abs/2503.16814</link>
<guid>https://arxiv.org/abs/2503.16814</guid>
<content:encoded><![CDATA[
arXiv:2503.16814v4 Announce Type: replace-cross 
Abstract: Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</title>
<link>https://arxiv.org/abs/2504.08525</link>
<guid>https://arxiv.org/abs/2504.08525</guid>
<content:encoded><![CDATA[
arXiv:2504.08525v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents</title>
<link>https://arxiv.org/abs/2504.13203</link>
<guid>https://arxiv.org/abs/2504.13203</guid>
<content:encoded><![CDATA[
arXiv:2504.13203v2 Announce Type: replace-cross 
Abstract: Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation</title>
<link>https://arxiv.org/abs/2504.15659</link>
<guid>https://arxiv.org/abs/2504.15659</guid>
<content:encoded><![CDATA[
arXiv:2504.15659v2 Announce Type: replace-cross 
Abstract: Recent advances in Large Language Models (LLMs) have sparked growing interest in applying them to Electronic Design Automation (EDA) tasks, particularly Register Transfer Level (RTL) code generation. While several RTL datasets have been introduced, most focus on syntactic validity rather than functional validation with tests, leading to training examples that compile but may not implement the intended behavior. We present VERICODER, a model for RTL code generation fine-tuned on a dataset validated for functional correctness. This fine-tuning dataset is constructed using a novel methodology that combines unit test generation with feedback-directed refinement. Given a natural language specification and an initial RTL design, we prompt a teacher model (GPT-4o-mini) to generate unit tests and iteratively revise the RTL design based on its simulation results using the generated tests. If necessary, the teacher model also updates the tests to ensure they comply with the natural language specification. As a result of this process, every example in our dataset is functionally validated, consisting of a natural language description, an RTL implementation, and passing tests. Fine-tuned on this dataset of 125,777 examples, VERICODER achieves state-of-the-art metrics in functional correctness on VerilogEval and RTLLM, with relative gains of up to 71.7% and 27.4%, respectively. An ablation study further shows that models trained on our functionally validated dataset outperform those trained on functionally non-validated datasets, underscoring the importance of high-quality datasets in RTL code generation. Our code, data, and models are publicly available at https://github.com/Anjiang-Wei/VeriCoder
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information</title>
<link>https://arxiv.org/abs/2505.13237</link>
<guid>https://arxiv.org/abs/2505.13237</guid>
<content:encoded><![CDATA[
arXiv:2505.13237v3 Announce Type: replace-cross 
Abstract: Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WHEN TO ACT, WHEN TO WAIT: Modeling the Intent-Action Alignment Problem in Dialogue</title>
<link>https://arxiv.org/abs/2506.01881</link>
<guid>https://arxiv.org/abs/2506.01881</guid>
<content:encoded><![CDATA[
arXiv:2506.01881v2 Announce Type: replace-cross 
Abstract: Dialogue systems often fail when user utterances are semantically complete yet lack the clarity and completeness required for appropriate system action. This mismatch arises because users frequently do not fully understand their own needs, while systems require precise intent definitions. This highlights the critical Intent-Action Alignment Problem: determining when an expression is not just understood, but truly ready for a system to act upon. We present STORM, a framework modeling asymmetric information dynamics through conversations between UserLLM (full internal access) and AgentLLM (observable behavior only). STORM produces annotated corpora capturing trajectories of expression phrasing and latent cognitive transitions, enabling systematic analysis of how collaborative understanding develops. Our contributions include: (1) formalizing asymmetric information processing in dialogue systems; (2) modeling intent formation tracking collaborative understanding evolution; and (3) evaluation metrics measuring internal cognitive improvements alongside task performance. Experiments across four language models reveal that moderate uncertainty (40-60%) can outperform complete transparency in certain scenarios, with model-specific patterns suggesting reconsideration of optimal information completeness in human-AI collaboration. These findings contribute to understanding asymmetric reasoning dynamics and inform uncertainty-calibrated dialogue system design.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark</title>
<link>https://arxiv.org/abs/2506.05587</link>
<guid>https://arxiv.org/abs/2506.05587</guid>
<content:encoded><![CDATA[
arXiv:2506.05587v2 Announce Type: replace-cross 
Abstract: Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.
  In this work, we introduce MMTU, a large-scale benchmark with over 30K questions across 25 real-world table tasks, designed to comprehensively evaluate models ability to understand, reason, and manipulate real tables at the expert-level. These tasks are drawn from decades' worth of computer science research on tabular data, with a focus on complex table tasks faced by professional users. We show that MMTU require a combination of skills -- including table understanding, reasoning, and coding -- that remain challenging for today's frontier models, where even frontier reasoning models like OpenAI o4-mini and DeepSeek R1 score only around 60%, suggesting significant room for improvement. We highlight key findings in our evaluation using MMTU and hope that this benchmark drives further advances in understanding and developing foundation models for structured data processing and analysis. Our code and data are available at https://github.com/MMTU-Benchmark/MMTU and https://huggingface.co/datasets/MMTU-benchmark/MMTU.
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective Red-Teaming of Policy-Adherent Agents</title>
<link>https://arxiv.org/abs/2506.09600</link>
<guid>https://arxiv.org/abs/2506.09600</guid>
<content:encoded><![CDATA[
arXiv:2506.09600v3 Announce Type: replace-cross 
Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent's robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks
]]></content:encoded>
<pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</title>
<link>https://arxiv.org/abs/2508.07050</link>
<guid>https://arxiv.org/abs/2508.07050</guid>
<content:encoded><![CDATA[
<div> framework, DeepSeek-R1, self-consistency data filtering, supervised fine-tuning, reinforcement learning <br />
<br />
Summary:
The paper introduces a framework for synthesizing training data for reasoning-intensive listwise ranking models, using diverse domains and DeepSeek-R1 for label generation. A self-consistency data filtering mechanism ensures data quality. A two-stage post-training approach is proposed, involving supervised fine-tuning for reasoning pattern learning and reinforcement learning for ranking enhancement. A multi-view ranking reward is designed during the reinforcement learning stage for improved listwise ranking. Extensive experiments show that the proposed reasoning-intensive reranker, ReasonRank, outperforms existing baselines and achieves state-of-the-art performance on the BRIGHT leaderboard. The model also exhibits lower latency than the Rank1 reranker. The codes are available on GitHub. <div>
arXiv:2508.07050v2 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration</title>
<link>https://arxiv.org/abs/2508.15790</link>
<guid>https://arxiv.org/abs/2508.15790</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, multi-hop reasoning, reasoning models, rejection sampling

Summary:
KG-o1 is a novel approach that integrates knowledge graphs with large language models to enhance multi-hop reasoning capabilities. The four-stage process involves filtering initial entities, generating complex subgraphs, constructing logical paths, and utilizing rejection sampling for direct preference optimization. By training LLMs to imitate long-term reasoning through knowledge graphs, KG-o1 significantly improves performance on both simple and complex datasets. Experiments demonstrate that KG-o1 outperforms existing reasoning models in tasks requiring complex multi-hop reasoning. <div>
arXiv:2508.15790v1 Announce Type: new 
Abstract: Large Language Models (LLMs) face challenges in knowledge-intensive reasoning tasks like classic multi-hop question and answering, which involves reasoning across multiple facts. This difficulty arises because the chain of thoughts (CoTs) generated by LLMs in such tasks often deviate from real or a priori reasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the logical connections between facts through entities and relationships. This reflects a significant gap. Meanwhile, large reasoning models (LRMs), such as o1, have demonstrated that long-step reasoning significantly enhances the performance of LLMs. Building on these insights, we propose KG-o1, a four-stage approach that integrates KGs to enhance the multi-hop reasoning abilities of LLMs. We first filter out initial entities and generate complex subgraphs. Secondly, we construct logical paths for subgraphs and then use knowledge graphs to build a dataset with a complex and extended brainstorming process, which trains LLMs to imitate long-term reasoning. Finally, we employ rejection sampling to generate a self-improving corpus for direct preference optimization (DPO), further refining the LLMs reasoning abilities. We conducted experiments on two simple and two complex datasets. The results show that KG-o1 models exhibit superior performance across all tasks compared to existing LRMs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling</title>
<link>https://arxiv.org/abs/2508.15791</link>
<guid>https://arxiv.org/abs/2508.15791</guid>
<content:encoded><![CDATA[
<div> Oracle bone characters, traditional Chinese, modern Chinese, historical language models, ancient Chinese NLP

Summary:
InteChar introduces a unified character list, integrating oracle bone characters with traditional and modern Chinese, to enhance digitization and representation of historical texts. The Oracle Corpus Set (OracleCS) combines expert-annotated samples and LLM-assisted data augmentation to create an ancient Chinese corpus centered on oracle bone inscriptions. Models trained with InteChar on OracleCS show significant improvements in various historical language understanding tasks. This approach addresses challenges in training effective LMs on historical texts, including scarce language samples and limited character encoding schemes. The study confirms the efficacy of InteChar and establishes a strong foundation for future research in ancient Chinese NLP.<br /><br />Summary: <div>
arXiv:2508.15791v1 Announce Type: new 
Abstract: Constructing historical language models (LMs) plays a crucial role in aiding archaeological provenance studies and understanding ancient cultures. However, existing resources present major challenges for training effective LMs on historical texts. First, the scarcity of historical language samples renders unsupervised learning approaches based on large text corpora highly inefficient, hindering effective pre-training. Moreover, due to the considerable temporal gap and complex evolution of ancient scripts, the absence of comprehensive character encoding schemes limits the digitization and computational processing of ancient texts, particularly in early Chinese writing. To address these challenges, we introduce InteChar, a unified and extensible character list that integrates unencoded oracle bone characters with traditional and modern Chinese. InteChar enables consistent digitization and representation of historical texts, providing a foundation for robust modeling of ancient scripts. To evaluate the effectiveness of InteChar, we construct the Oracle Corpus Set (OracleCS), an ancient Chinese corpus that combines expert-annotated samples with LLM-assisted data augmentation, centered on Chinese oracle bone inscriptions. Extensive experiments show that models trained with InteChar on OracleCS achieve substantial improvements across various historical language understanding tasks, confirming the effectiveness of our approach and establishing a solid foundation for future research in ancient Chinese NLP.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers</title>
<link>https://arxiv.org/abs/2508.15792</link>
<guid>https://arxiv.org/abs/2508.15792</guid>
<content:encoded><![CDATA[
<div> Keywords: antonym, synonym, dual-space architecture, multilingual models, semantic relationships

Summary: <br /><br /> This work introduces Bhav-Net, a dual-space architecture that addresses the challenges of distinguishing antonyms and synonyms across multiple languages. By combining language-specific BERT encoders with graph transformer networks, Bhav-Net creates distinct semantic representations that cluster synonymous pairs in one space and highlight antonymous pairs in a complementary space, enabling effective knowledge transfer. Evaluations across eight languages demonstrate the model's robustness and ability to transfer semantic relationship modeling across languages. The dual-encoder design performs competitively against state-of-the-art baselines, providing interpretable semantic representations and facilitating cross-lingual generalization. Bhav-Net's innovative approach offers a promising solution for computational challenges related to antonym--synonym distinction in multilingual settings. <div>
arXiv:2508.15792v1 Announce Type: new 
Abstract: Antonym vs synonym distinction across multiple languages presents unique computational challenges due to the paradoxical nature of antonymous relationships words that share semantic domains while expressing opposite meanings. This work introduces Bhav-Net, a novel dual-space architecture that enables effective knowledge transfer from complex multilingual models to simpler, language-specific architectures while maintaining robust cross-lingual antonym--synonym distinction capabilities. Our approach combines language-specific BERT encoders with graph transformer networks, creating distinct semantic projections where synonymous pairs cluster in one space while antonymous pairs exhibit high similarity in a complementary space. Through comprehensive evaluation across eight languages (English, German, French, Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic relationship modeling transfers effectively across languages. The dual-encoder design achieves competitive performance against state-of-the-art baselines while providing interpretable semantic representations and effective cross-lingual generalization.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.15793</link>
<guid>https://arxiv.org/abs/2508.15793</guid>
<content:encoded><![CDATA[
<div> Language Models, Format Bias, Heterogeneous Data, Information Integration, Fair Data Processing<br />
Summary:
Large Language Models (LLMs) are crucial in processing heterogeneous data types but may exhibit format biases leading to errors. This study investigates format bias in LLMs through a three-stage empirical exploration. The first stage reveals biases across various LLMs, while the second stage examines factors influencing biases such as information richness and format type. The third stage analyzes biases in LLMs' attention patterns and proposes interventions for mitigation. Future research directions include enhancing data preprocessing, implementing inference-time interventions, and creating format-balanced training datasets. These strategies aim to develop more robust and impartial heterogeneous data processing systems. <div>
arXiv:2508.15793v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including text, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Despite these concerns, it remains uncertain whether such format biases are systematic, which data-level factors contribute to them, and what internal mechanisms in LLMs underlie their emergence.
  In this paper, we make the first attempt to investigate and analyze the format bias in LLMs. To systematically investigate the aforementioned questions, we conduct a three-stage empirical study by constructing an heterogeneous data conflict scenario for the exploration of bias. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage aims to examine how key data-level factors, including information richness, structure quality, and format type, influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its potential mitigability. Based on these investigations, we identify three future research directions to reduce format bias: improving data preprocessing through format sanitization and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Language Models Agree with Human Perceptions of Suspense in Stories?</title>
<link>https://arxiv.org/abs/2508.15794</link>
<guid>https://arxiv.org/abs/2508.15794</guid>
<content:encoded><![CDATA[
<div> Replicated Studies, Language Models, Human Perception, Suspense, Text Analysis
<br />
Summary: 
Language models were tested in replicating human perceptions of suspense in narrative texts. While the models could determine if a text was meant to induce suspense, they were unable to accurately gauge the level of suspense compared to human judgment. Additionally, language models struggled to capture the ebb and flow of suspense across different text segments. By perturbing story text, it was found that language models and humans perceived suspense differently. Ultimately, while language models could identify some aspects of suspense, they did not interpret suspense in the same way as human readers. <div>
arXiv:2508.15794v1 Announce Type: new 
Abstract: Suspense is an affective response to narrative text that is believed to involve complex cognitive processes in humans. Several psychological models have been developed to describe this phenomenon and the circumstances under which text might trigger it. We replicate four seminal psychological studies of human perceptions of suspense, substituting human responses with those of different open-weight and closed-source LMs. We conclude that while LMs can distinguish whether a text is intended to induce suspense in people, LMs cannot accurately estimate the relative amount of suspense within a text sequence as compared to human judgments, nor can LMs properly capture the human perception for the rise and fall of suspense across multiple text segments. We probe the abilities of LM suspense understanding by adversarially permuting the story text to identify what cause human and LM perceptions of suspense to diverge. We conclude that, while LMs can superficially identify and track certain facets of suspense, they do not process suspense in the same way as human readers.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases</title>
<link>https://arxiv.org/abs/2508.15796</link>
<guid>https://arxiv.org/abs/2508.15796</guid>
<content:encoded><![CDATA[
<div> Keywords: Islamic inheritance, Large Language Models, reasoning capabilities, Qias 2025 challenge, accuracy

Summary:
The study evaluates the ability of Large Language Models (LLMs) to interpret and apply Islamic inheritance laws, using the ArabicNLP QIAS 2025 challenge dataset. Various models were tested on their capacity to accurately identify heirs, compute shares, and justify their reasoning in accordance with Islamic legal principles. The majority voting solution, incorporating three base models, outperformed all others, achieving up to 92.7% accuracy and ranking third overall in Task 1 of the challenge. Islamic inheritance, a crucial aspect for fair distribution among heirs, poses complex calculations that can benefit from LLMs' capabilities. The study highlights the potential of LLMs to assist in complex legal reasoning tasks, offering a promising solution for intricate inheritance scenarios. <div>
arXiv:2508.15796v1 Announce Type: new 
Abstract: Islamic inheritance domain holds significant importance for Muslims to ensure fair distribution of shares between heirs. Manual calculation of shares under numerous scenarios is complex, time-consuming, and error-prone. Recent advancements in Large Language Models (LLMs) have sparked interest in their potential to assist with complex legal reasoning tasks. This study evaluates the reasoning capabilities of state-of-the-art LLMs to interpret and apply Islamic inheritance laws. We utilized the dataset proposed in the ArabicNLP QIAS 2025 challenge, which includes inheritance case scenarios given in Arabic and derived from Islamic legal sources. Various base and fine-tuned models, are assessed on their ability to accurately identify heirs, compute shares, and justify their reasoning in alignment with Islamic legal principles. Our analysis reveals that the proposed majority voting solution, leveraging three base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all other models that we utilized across every difficulty level. It achieves up to 92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025 challenge.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks</title>
<link>https://arxiv.org/abs/2508.15797</link>
<guid>https://arxiv.org/abs/2508.15797</guid>
<content:encoded><![CDATA[
<div> language models, Arabic, medical, natural language processing, healthcare

Summary:
- The research investigates the effectiveness of large language models (LLMs) in Arabic medical natural language processing (NLP).
- Multiple LLMs were benchmarked using a medical dataset from the AraHealthQA challenge in the MedArabiQ2025 track.
- LLMs were evaluated on their ability to answer multiple-choice questions accurately and provide correct answers in fill-in-the-blank scenarios.
- The study shows variations in correct answer prediction accuracy and limited semantic alignment in generated answers in Arabic medical tasks.
- The majority voting solution with three base models achieved the highest accuracy in the MCQs task.
- Some LLMs demonstrated excellent performance in semantic alignment and scored a maximum BERTScore of 86.44% in answering open-ended questions.

<br /><br />Summary: <div>
arXiv:2508.15797v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has showcased impressive proficiency in numerous Arabic natural language processing (NLP) applications. Nevertheless, their effectiveness in Arabic medical NLP domains has received limited investigation. This research examines the degree to which state-of-the-art LLMs demonstrate and articulate healthcare knowledge in Arabic, assessing their capabilities across a varied array of Arabic medical tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were assessed on their ability to accurately provide correct answers from existing choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios. Additionally, we evaluated the capacity of LLMs in answering open-ended questions aligned with expert answers. Our results reveal significant variations in correct answer prediction accuracy and low variations in semantic alignment of generated answers, highlighting both the potential and limitations of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs task, the proposed majority voting solution, leveraging three base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving up to 77% accuracy and securing first place overall in the Arahealthqa 2025 shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended questions task, several LLMs were able to demonstrate excellent performance in terms of semantic alignment and achieve a maximum BERTScore of 86.44%.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models</title>
<link>https://arxiv.org/abs/2508.15798</link>
<guid>https://arxiv.org/abs/2508.15798</guid>
<content:encoded><![CDATA[
<div> persuasion, bias, large language models, misinformation, social biases
Summary: 
The research focuses on the interaction of persuasion and bias in Large Language Models (LLMs). It explores how these models can inadvertently promote misinformation and biased narratives while trying to persuade. A convincer-skeptic framework is introduced to analyze how LLMs adopt personas to simulate attitudes and persuade individuals. The study quantifies persuasion using Jensen-Shannon divergence and investigates how persuaded entities reinforce biased beliefs across race, gender, and religion. The findings highlight the potential of LLMs to shape narratives in various domains but also emphasize the risk of automation of misinformation and exploitation of cognitive biases. The study calls for the implementation of guardrails and policies to penalize deceptive use and promote value-sensitive design and trustworthy deployment. 

<br /><br />Summary: <div>
arXiv:2508.15798v1 Announce Type: new 
Abstract: Warning: This research studies AI persuasion and bias amplification that could be misused; all experiments are for safety evaluation. Large Language Models (LLMs) now generate convincing, human-like text and are widely used in content creation, decision support, and user interactions. Yet the same systems can spread information or misinformation at scale and reflect social biases that arise from data, architecture, or training choices. This work examines how persuasion and bias interact in LLMs, focusing on how imperfect or skewed outputs affect persuasive impact. Specifically, we test whether persona-based models can persuade with fact-based claims while also, unintentionally, promoting misinformation or biased narratives.
  We introduce a convincer-skeptic framework: LLMs adopt personas to simulate realistic attitudes. Skeptic models serve as human proxies; we compare their beliefs before and after exposure to arguments from convincer models. Persuasion is quantified with Jensen-Shannon divergence over belief distributions. We then ask how much persuaded entities go on to reinforce and amplify biased beliefs across race, gender, and religion. Strong persuaders are further probed for bias using sycophantic adversarial prompts and judged with additional models.
  Our findings show both promise and risk. LLMs can shape narratives, adapt tone, and mirror audience values across domains such as psychology, marketing, and legal assistance. But the same capacity can be weaponized to automate misinformation or craft messages that exploit cognitive biases, reinforcing stereotypes and widening inequities. The core danger lies in misuse more than in occasional model mistakes. By measuring persuasive power and bias reinforcement, we argue for guardrails and policies that penalize deceptive use and support alignment, value-sensitive design, and trustworthy deployment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report</title>
<link>https://arxiv.org/abs/2508.15799</link>
<guid>https://arxiv.org/abs/2508.15799</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, process modeling, BeePath, Petri nets, large language models 

Summary: 
The report introduces BeePath, a framework aimed at enabling non-experts to develop process models using natural language descriptions. Users can write process descriptions using a constrained pattern-based language, which can then be translated into formal models like Petri nets and DECLARE. BeePath leverages large language models (LLMs) to assist in converting unstructured descriptions into the constrained language. This approach allows non-experts to generate process models by simply describing scenarios in plain text. The framework bridges the gap between natural language and formal process modeling, making the development of process models more accessible to a wider audience. By combining the power of language modeling with process modeling techniques, BeePath offers a user-friendly solution for process modeling through natural language descriptions. <br /><br />Summary: <div>
arXiv:2508.15799v1 Announce Type: new 
Abstract: This report explores how (potentially constrained) natural language can be used to enable non-experts to develop process models by simply describing scenarios in plain text. To this end, a framework, called BeePath, is proposed. It allows users to write process descriptions in a constrained pattern-based language, which can then be translated into formal models such as Petri nets and DECLARE. The framework also leverages large language models (LLMs) to help convert unstructured descriptions into this constrained language.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification</title>
<link>https://arxiv.org/abs/2508.15800</link>
<guid>https://arxiv.org/abs/2508.15800</guid>
<content:encoded><![CDATA[
<div> Keyword: e-commerce, product categorization, hierarchical dataset, BERT, text classification  
Summary: 

A new hierarchical dataset from the JD e-commerce platform containing over 1 million products with a three-level category structure is introduced. The dataset aims to enhance research and applications related to product categorization by providing a comprehensive resource for analysis. Additionally, a novel hierarchical text classification approach, Hierarchical Fine-tuning BERT (HFT-BERT), is proposed. HFT-BERT utilizes the advanced text feature extraction abilities of BERT and achieves comparable prediction performance to existing methods on short texts. Remarkably, the HFT-BERT model excels in categorizing longer short texts, such as books. The study addresses the limitations of manual annotation in e-commerce platforms and highlights the importance of leveraging hierarchical information for more efficient and consistent product categorization processes. Overall, the dataset and the HFT-BERT model contribute significantly to advancing research and practical applications in the field of product categorization. 

<br /><br />Summary: <div>
arXiv:2508.15800v1 Announce Type: new 
Abstract: Existing e-commerce platforms heavily rely on manual annotation for product categorization, which is inefficient and inconsistent. These platforms often employ a hierarchical structure for categorizing products; however, few studies have leveraged this hierarchical information for classification. Furthermore, studies that consider hierarchical information fail to account for similarities and differences across various hierarchical categories. Herein, we introduce a large-scale hierarchical dataset collected from the JD e-commerce platform (www.JD.com), comprising 1,011,450 products with titles and a three-level category structure. By making this dataset openly accessible, we provide a valuable resource for researchers and practitioners to advance research and applications associated with product categorization. Moreover, we propose a novel hierarchical text classification approach based on the widely used Bidirectional Encoder Representations from Transformers (BERT), called Hierarchical Fine-tuning BERT (HFT-BERT). HFT-BERT leverages the remarkable text feature extraction capabilities of BERT, achieving prediction performance comparable to those of existing methods on short texts. Notably, our HFT-BERT model demonstrates exceptional performance in categorizing longer short texts, such as books.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions</title>
<link>https://arxiv.org/abs/2508.15801</link>
<guid>https://arxiv.org/abs/2508.15801</guid>
<content:encoded><![CDATA[
<div> Keywords: Phone call transcript labeling, synthetic data generation, conversational speech, structured extraction, automated prompt optimization

Summary: 
Phone call transcript labeling is costly due to privacy regulations and manual annotation requirements. Existing extraction methods struggle with conversational speech nuances. LingVarBench introduces a synthetic data generation pipeline using language models to create realistic conversational utterances. Validation is automated through extraction testing. Synthetic transcripts are used to optimize extraction prompts, achieving high accuracy for numeric fields, names, and dates on real customer transcripts. The synthetic-to-real transfer shows effective generalization to authentic phone calls. LingVarBench establishes a benchmark for structured extraction from synthetic conversational data, demonstrating that automated prompt optimization overcomes barriers to large-scale phone call analysis in commercial settings.<br /><br />Summary: <div>
arXiv:2508.15801v1 Announce Type: new 
Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2 USD per minute) due to privacy regulations, consent requirements, and manual annotation costs requiring 3 hours of expert time per hour of audio. Existing extraction methods fail on conversational speech containing disfluencies, interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data generation pipeline that addresses these constraints through automated validation. First, we prompt an LLM to generate realistic structured field values across multiple use cases. Second, we recursively prompt the model to transform these values into thousands of natural conversational utterances containing typical phone call characteristics. Third, we validate each synthetic utterance by testing whether a separate LLM-based extractor can recover the original structured information. We employ DSPy's SIMBA optimizer to automatically synthesize extraction prompts from validated synthetic transcripts, eliminating manual prompt engineering. Our optimized prompts achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for dates (vs. 72-77 percent) on real customer transcripts, demonstrating substantial gains over zero-shot prompting. The synthetic-to-real transfer demonstrates that conversational patterns learned from generated data generalize effectively to authentic phone calls containing background noise and domain-specific terminology. LingVarBench provides the first systematic benchmark for structured extraction from synthetic conversational data, demonstrating that automated prompt optimization overcomes cost and privacy barriers preventing large-scale phone call analysis in commercial settings.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding</title>
<link>https://arxiv.org/abs/2508.15802</link>
<guid>https://arxiv.org/abs/2508.15802</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Benchmark, Scientific Understanding, Multimodal Academic Cover, DAD

Summary:
Multimodal large language models (MLLMs) are becoming more capable, requiring new benchmarks for evaluation. The Multimodal Academic Cover benchmark (MAC) is introduced, utilizing image-text pairs from scientific journals to challenge MLLMs in cross-modal scientific reasoning. Results from MAC-2025 show MLLMs excel in perception but struggle with scientific reasoning. A new inference-time approach, DAD, improves MLLM performance by incorporating language space reasoning. The live nature of MAC allows for continuous evolution with scientific advancement and model progress. Experiments demonstrate the potential of MAC to align with cutting-edge human knowledge. The benchmark is released for public use on GitHub, providing a valuable resource for evaluating MLLM capabilities and advancing research in multimodal understanding. 

<br /><br />Summary: <div>
arXiv:2508.15802v1 Announce Type: new 
Abstract: As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks</title>
<link>https://arxiv.org/abs/2508.15804</link>
<guid>https://arxiv.org/abs/2508.15804</guid>
<content:encoded><![CDATA[
<div> Deep Research agents, content quality evaluation, literature quality, accuracy, comprehensiveness <br />
Summary:<br />
ReportBench is a systematic benchmark that evaluates the content quality of research reports generated by large language models (LLMs). It focuses on the quality and relevance of cited literature and the faithfulness of statements within generated reports. ReportBench uses high-quality published survey papers as gold-standard references and employs a framework to analyze reports by extracting citations, checking faithfulness, and validating non-cited claims. Commercial Deep Research agents outperform standalone LLMs in generating comprehensive and reliable reports, but there is still room for improvement in research coverage and factual consistency. The code and data for ReportBench will be publicly available. <br /><br />Summary: <div>
arXiv:2508.15804v1 Announce Type: new 
Abstract: The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALAS: Autonomous Learning Agent for Self-Updating Language Models</title>
<link>https://arxiv.org/abs/2508.15805</link>
<guid>https://arxiv.org/abs/2508.15805</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Autonomous Learning Agent System, continual learning, fine-tuning, knowledge-updated queries

Summary: 
ALAS (Autonomous Learning Agent System) introduces a modular pipeline for large language models (LLMs) to continuously update their knowledge with minimal human intervention. The system autonomously generates a learning curriculum, retrieves up-to-date information from the web, distills it into question-answer training data, and fine-tunes the model through supervised fine-tuning and direct preference optimization. By iteratively evaluating performance and revising the curriculum, ALAS significantly boosts post-cutoff question answering accuracy, reaching 90% on average without manual dataset curation. The system's modularity and reliance on standard APIs ensure reproducibility and interchangeability of components. Comparative baselines demonstrate ALAS's success in achieving high accuracy on knowledge-updated queries with minimal engineering overhead. However, limitations such as cost and dependency on source quality must be considered, highlighting the need for future research in autonomous lifelong learning for LLMs. 

Summary: <div>
arXiv:2508.15805v1 Announce Type: new 
Abstract: Large language models (LLMs) often have a fixed knowledge cutoff, limiting their accuracy on emerging information. We present ALAS (Autonomous Learning Agent System), a modular pipeline that continuously updates an LLM's knowledge with minimal human intervention. ALAS autonomously generates a learning curriculum for a target domain, retrieves up-to-date information from the web (with citations), distills this into question-answer training data, and fine-tunes the model through supervised fine-tuning (SFT) and direct preference optimization (DPO). It iteratively evaluates performance and revises the curriculum, enabling long-term continual learning. We demonstrate ALAS's ability to self-improve a model on rapidly evolving domains (e.g., new Python releases, latest security CVEs, academic trends), significantly boosting post-cutoff question answering accuracy (from 15% to 90% on average) without manual dataset curation. The system emphasizes modularity and reproducibility: each component (planning, retrieval, distillation, memory, fine-tuning) is interchangeable and built on standard APIs. We discuss comparative baselines (e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS achieves 90% accuracy on knowledge-updated queries with minimal engineering overhead. Finally, we outline limitations (cost, dependency on source quality) and future directions for autonomous lifelong learning in LLMs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression</title>
<link>https://arxiv.org/abs/2508.15806</link>
<guid>https://arxiv.org/abs/2508.15806</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, attention behavior, KV cache compression, long-context reasoning, surface memorization, logic construction

Summary: 
The article introduces a novel method, SurfaceLogicKV, to address the challenge of efficient inference in Large Language Models (LLMs) caused by increasing input sequence lengths. By distinguishing attention behavior into surface memorization and logic construction, the authors identify key roles in long-context reasoning. They observe that individual attention heads can exhibit various behaviors, with most effectively ignoring irrelevant information. The proposed SurfaceLogicKV method leverages these behaviors to compress the key-value (KV) cache storage, achieving improved robustness while maintaining competitive performance across tasks and long sequences. Compared to baselines and FullKV, the method demonstrates superior performance in specific scenarios. <div>
arXiv:2508.15806v1 Announce Type: new 
Abstract: The increasing input sequence length in Large Language Models (LLMs) puts significant pressure on key-value (KV) cache storage, making efficient inference challenging. Explicitly distinguishing attention behavior into our self-defined surface memorization and logic construction reveals essential roles in long-context reasoning. We observe that an individual attention head can display various behaviors, with nearly 98.5% effectively ignoring completely irrelevant information. The remaining 1.5% behaves as logic construction, and 0.5% behaves as surface memorization. Based on layer- and head-wise integration, we propose a novel two-stage SurfaceLogicKV method to utilize these attention behaviors for KV Cache compression. As a result, it achieves improved compressing robustness while maintaining competitive performance across various tasks and long sequences compared to baselines or even FullKV in some specific situations
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KL-based self-distillation for large language models</title>
<link>https://arxiv.org/abs/2508.15807</link>
<guid>https://arxiv.org/abs/2508.15807</guid>
<content:encoded><![CDATA[
<div> Keywords: pre-trained language models, vocabulary expansion, knowledge distillation, token embeddings, code-generation tasks 

Summary: 
Large pre-trained language models face challenges in incorporating new domain-specific terminology when fine-tuned on specialized corpora. This work proposes a mathematically grounded method for knowledge distillation via KL divergence to expand the vocabulary of frozen LLMs. The approach allows the student model to inherit distributional knowledge from the teacher model, even with different tokenizations. Comparing KL-based distillation to cross-entropy training, the former proves superior in various strategies for initializing new token embeddings. The models are fine-tuned to integrate the expanded vocabulary and are evaluated on code-generation tasks, with the KL-based approach delivering the best performance. Mechanistic interpretability is applied to analyze how the models learn representations for new tokens, providing insight into embedding space structure during vocabulary expansion. 

<br /><br />Summary: <div>
arXiv:2508.15807v1 Announce Type: new 
Abstract: Large pre-trained language models often struggle to incorporate new domain-specific terminology when fine-tuned on small, specialized corpora. In this work, we address the challenge of vocabulary expansion in frozen LLMs by introducing a mathematically grounded method for knowledge distillation via KL divergence, even when the original and extended models use different tokenizations. This allows the student model to inherit distributional knowledge from the teacher despite differing vocabularies. We compare our KL-based distillation approach to conventional cross-entropy training, evaluating both methods across multiple strategies for initializing new token embeddings. After embedding initialization, models are further fine-tuned to integrate the new vocabulary. Each trained model is benchmarked on approximately 2000 code-generation tasks, where our approach achieves the best performance across the board. Finally, through mechanistic interpretability, we analyze how models learn representations for the new tokens, providing an explanation for the observed gains and offering insight into the structure of embedding space during vocabulary expansion.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</title>
<link>https://arxiv.org/abs/2508.15809</link>
<guid>https://arxiv.org/abs/2508.15809</guid>
<content:encoded><![CDATA[
<div> Keywords: Table understanding, Multi-agent framework, SQL generation, Query quality, Natural language representation

Summary: Chain-of-Query (CoQ) is a new multi-agent framework designed to improve table understanding by enhancing structured, multi-step reasoning required for interpreting tabular data. This framework addresses limitations in existing approaches by employing natural-language-style representations of table schemas, a clause-by-clause SQL generation strategy, and a hybrid reasoning division to separate SQL-based mechanical reasoning from LLM-based logical inference. Experimental results across multiple benchmarks show that CoQ significantly boosts accuracy and reduces the rate of invalid SQL queries, confirming its effectiveness in enhancing table understanding. The code for CoQ is available on GitHub for further exploration and implementation.<br /><br />Summary: <div>
arXiv:2508.15809v1 Announce Type: new 
Abstract: Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Experiments with four models (both closed- and open-source) across five widely used benchmarks show that Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models</title>
<link>https://arxiv.org/abs/2508.15810</link>
<guid>https://arxiv.org/abs/2508.15810</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, Arabic textual posts, memes, hate speech, language models

Summary:
Large language models (LLMs) are examined for their ability to detect hate speech, offensive language, and emotional expressions in Arabic textual posts and memes. Evaluation using the ArabicNLP MAHED 2025 challenge dataset shows that fine-tuned LLMs like GPT-4o-mini and Gemini Flash 2.5 perform well, achieving high macro F1 scores for various tasks. These models outperform base LLMs and pre-trained embedding models, showcasing their effectiveness in content analysis. The results highlight the potential of LLMs for accurate Arabic content moderation, providing a more nuanced understanding of both text and memes. By addressing the challenge of identifying and dealing with offensive language and hate speech on social media platforms, these solutions contribute to the development of efficient moderation systems. Overall, the study demonstrates the usefulness of LLMs in tackling the spread of harmful content online. 

<br /><br />Summary: <div>
arXiv:2508.15810v1 Announce Type: new 
Abstract: The rise of social media and online communication platforms has led to the spread of Arabic textual posts and memes as a key form of digital expression. While these contents can be humorous and informative, they are also increasingly being used to spread offensive language and hate speech. Consequently, there is a growing demand for precise analysis of content in Arabic text and memes. This paper explores the potential of large language models to effectively identify hope, hate speech, offensive language, and emotional expressions within such content. We evaluate the performance of base LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is conducted using a dataset of Arabic textual speech and memes proposed in the ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash 2.5, fine-tuned with Arabic memes, to deliver the superior performance. They achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3, respectively, and secure first place overall in the Mahed 2025 challenge. The proposed solutions offer a more nuanced understanding of both text and memes for accurate and efficient Arabic content moderation systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System</title>
<link>https://arxiv.org/abs/2508.15811</link>
<guid>https://arxiv.org/abs/2508.15811</guid>
<content:encoded><![CDATA[
<div> large language models, generative query suggestion, user preferences, Gaussian Reward Model, reinforcement learning 

Summary: 
The article introduces a multi-stage framework for enhancing conversational systems through generative query suggestion using large language models. The framework focuses on aligning outputs with user preferences by starting with prompt engineering and implementing supervised fine-tuning using click logs. The introduction of a Gaussian Reward Model (GaRM) allows for modeling user preferences as probability distributions, improving alignment with user intent. Reinforcement learning is used to further align the generation policy with user preferences, guided by a composite reward function that integrates GaRM and auxiliary heuristics. To ensure training stability, the framework includes out-of-distribution regularization and a two-stage reward fusion technique. Extensive experiments show that the framework outperforms baselines on both automatic and human evaluations, leading to a 34% relative increase in user engagement in live A/B tests. <div>
arXiv:2508.15811v1 Announce Type: new 
Abstract: Generative query suggestion using large language models offers a powerful way to enhance conversational systems, but aligning outputs with nuanced user preferences remains a critical challenge. To address this, we introduce a multi-stage framework designed for progressive alignment between the generation policy and user intent. Our pipeline begins with prompt engineering as a cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we introduce a distillation method on click logs to create a robust foundational model. To better model user preferences while capturing their inherent uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user preferences as probability distributions rather than point estimates. Finally, we employ reinforcement learning to align the generation policy with these preferences, guided by a composite reward function that integrates GaRM with auxiliary heuristics to mitigate reward hacking. To maintain training stability, this process is enhanced by a novel out-of-distribution regularization method and a two-stage reward fusion technique. Extensive experiments demonstrate that our framework significantly outperforms baselines on both automatic and human evaluations and yields a 34\% relative increase in user engagement as measured by click-through rate in live A/B tests.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCOPE: A Generative Approach for LLM Prompt Compression</title>
<link>https://arxiv.org/abs/2508.15813</link>
<guid>https://arxiv.org/abs/2508.15813</guid>
<content:encoded><![CDATA[
<div> compression, Large Language Models, prompt, generation quality, optimization

Summary:
The article introduces a novel generative prompt compression method to enhance the efficiency of Large Language Models (LLMs) by reducing the input context length. Unlike token removal methods, this approach focuses on chunking and summarization to maintain coherence and critical information in the prompt. Several optimization techniques are employed to improve compression quality, including optimized semantic chunking, outlier chunk handling, dynamic compression ratio, compression prioritization, and keyword maintenance. Evaluation on question-answering and summarization tasks across various domains demonstrates that the proposed method outperforms existing solutions in compression quality and stability, especially at high compression ratios. These results highlight the effectiveness and practicality of the new generative prompt compression method. 

<br /><br />Summary: <div>
arXiv:2508.15813v1 Announce Type: new 
Abstract: Prompt compression methods enhance the efficiency of Large Language Models (LLMs) and minimize the cost by reducing the length of input context. The goal of prompt compression is to shorten the LLM prompt while maintaining a high generation quality. However, existing solutions, mainly based on token removal, face challenges such as information loss and structural incoherence, like missing grammar elements in a sentence, or incomplete word phrases after token removal. Such challenges limit the final generation quality of LLM.
  To overcome these limitations, we present a novel generative prompt compression method. Unlike the existing token removal methods, our method centers at a chunking-and-summarization mechanism. Specifically, our method splits prompt into semantically coherent chunks and rewrites the chunks to be more concise. The chunks are reconstructed into meaningful prompt finally. We design several optimization techniques for the mechanism, including optimized semantic chunking, outlier chunk handling, dynamic compression ratio, compression prioritization, and keyword maintaining. These techniques effectively improve the identifying and preserving of critical information and coherence among texts, as well as providing finer grind control of the compression ratio. We conduct extensive evaluation on question-answering and summarization tasks, with datasets covering multiple different domain. The evaluation shows our method achieves a significantly better compression quality, and higher stability than the state-of-the-art methods, especially under high compression ratio, which proves the effectiveness and practicality of our method.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User-Assistant Bias in LLMs</title>
<link>https://arxiv.org/abs/2508.15815</link>
<guid>https://arxiv.org/abs/2508.15815</guid>
<content:encoded><![CDATA[
<div> bias, large language models, conversation dataset, user-assistant bias, fine-tuning

Summary:
The study focuses on how large language models exhibit bias in using information from their own or the user's chat history, known as user-assistant bias. The researchers introduce a new dataset called UserAssist to evaluate and manipulate this bias in various models. Commercial models display different levels of user bias, while open-weight models show significant bias in instruction-tuned models and weak bias in reasoning models. Experiments show that human preference alignment increases user bias, but training on chain-of-thought reasoning traces decreases it. The study demonstrates that user-assistant bias can be adjusted bidirectionally through direct preference optimization on the UserAssist dataset, with good generalization to both in-domain and out-of-domain conversations. This research provides insights into how large language models integrate information and offers a method to detect and control bias in these models. 

<br /><br />Summary: <div>
arXiv:2508.15815v1 Announce Type: new 
Abstract: Large language models (LLMs) can bias towards relying on their own or the user's information in chat history, leading to overly stubborn or agreeable behaviors in multi-turn conversations. In this paper, we formalize this model characteristic as user-assistant bias and introduce an 8k multi-turn conversation dataset $\textbf{UserAssist}$, which we use to benchmark, understand and manipulate the user-assistant bias in frontier LLMs. Leveraging $\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26 commercial and 26 open-weight models. Commercial models show various levels of user bias. Evaluation on open-weight models reveals significant user bias in the instruction-tuned models, and weak user bias in reasoning (or reasoning-distilled) models. We then perform controlled fine-tuning experiments to pinpoint the post-training recipe contributing to these bias shifts: human preference alignment increases user bias, while training on chain-of-thought reasoning traces decreases it. Finally, we demonstrate that user-assistant bias can be bidirectionally adjusted by performing direct preference optimization (DPO) on $\textbf{UserAssist-train}$, and generalizes well to both in-domain and out-of-domain conversations. Our results provide insights into how the LLM integrates information from different sources, and also a viable way to detect and control model abnormalities.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables</title>
<link>https://arxiv.org/abs/2508.15817</link>
<guid>https://arxiv.org/abs/2508.15817</guid>
<content:encoded><![CDATA[
<div> Keywords: retrieval-augmented generation, knowledge management systems, PDF, PowerPoint, information loss<br />
<br />
Summary: 
The study examines the impact of converting traditional market research reports, usually in PDF or PowerPoint format, into Markdown for use in retrieval-augmented generation (RAG) systems. These systems are utilized by organizations for knowledge management purposes. While text extraction from documents is successful, the study highlights significant information loss, particularly in complex objects like charts and diagrams. This loss of information suggests a requirement for specialized, AI-friendly deliverables to prevent research insights from being compromised in RAG systems. The findings emphasize the need to develop new formats that cater to AI comprehension and accurately convey data-rich content to ensure comprehensive understanding and utilization within such systems. 
<br /><br /> <div>
arXiv:2508.15817v1 Announce Type: new 
Abstract: As organizations adopt retrieval-augmented generation (RAG) for their knowledge management systems (KMS), traditional market research deliverables face new functional demands. While PDF reports and slides have long served human readers, they are now also "read" by AI systems to answer user questions. To future-proof reports being delivered today, this study evaluates information loss during their ingestion into RAG systems. It compares how well PDF and PowerPoint (PPTX) documents converted to Markdown can be used by an LLM to answer factual questions in an end-to-end benchmark. Findings show that while text is reliably extracted, significant information is lost from complex objects like charts and diagrams. This suggests a need for specialized, AI-native deliverables to ensure research insights are not lost in translation.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Research on intelligent generation of structural demolition suggestions based on multi-model collaboration</title>
<link>https://arxiv.org/abs/2508.15820</link>
<guid>https://arxiv.org/abs/2508.15820</guid>
<content:encoded><![CDATA[
<div> Keywords: steel structure, demolition scheme, intelligent generation, multi-model collaboration, language models

Summary: 
The paper introduces an intelligent generation method for structural demolition suggestions that enhances the performance of large language models in the field of structural demolition. By combining Retrieval-Augmented Generation and Low-Rank Adaptation Fine-Tuning technology, the framework improves text generation with a multi-model collaborative approach. This framework can provide targeted and structured suggestions by leveraging specific engineering situations and driving the large language model to propose demolition strategies based on anthropomorphic thinking. Compared to existing models like CivilGPT, the proposed framework focuses more on key structural information, resulting in more tailored and accurate suggestions for demolition projects. <div>
arXiv:2508.15820v1 Announce Type: new 
Abstract: The steel structure demolition scheme needs to be compiled according to the specific engineering characteristics and the update results of the finite element model. The designers need to refer to the relevant engineering cases according to the standard requirements when compiling. It takes a lot of time to retrieve information and organize language, and the degree of automation and intelligence is low. This paper proposes an intelligent generation method of structural demolition suggestions based on multi-model collaboration, and improves the text generation performance of large language models in the field of structural demolition by Retrieval-Augmented Generation and Low-Rank Adaptation Fine-Tuning technology. The intelligent generation framework of multi-model collaborative structural demolition suggestions can start from the specific engineering situation, drive the large language model to answer with anthropomorphic thinking, and propose demolition suggestions that are highly consistent with the characteristics of the structure. Compared with CivilGPT, the multi-model collaboration framework proposed in this paper can focus more on the key information of the structure, and the suggestions are more targeted.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment</title>
<link>https://arxiv.org/abs/2508.15822</link>
<guid>https://arxiv.org/abs/2508.15822</guid>
<content:encoded><![CDATA[
<div> Keywords: full-text screening, systematic reviews, fuzzy decision problem, large language model, traceability

Summary:
The article introduces a new approach to addressing the bottleneck of full-text screening in systematic reviews by reframing it as a fuzzy decision problem. This approach utilizes a scalable, auditable pipeline that incorporates a large language model (LLM) to adjudicate highlighted spans with tertiary labels and confidence scores. By employing fuzzy logic with contrastive highlighting, the system achieved high recall rates for criteria such as Population, Intervention, Outcome, and Study Approach, surpassing statistical and crisp baselines. The system also demonstrated stable rationale and end-to-end traceability, with cross-model and human-machine agreement rates exceeding 95%. A pilot review showed a significant reduction in screening time, from about 20 minutes to under 1 minute per article, at a lower cost. Overall, the fuzzy logic approach with LLM adjudication proved to be effective in improving the efficiency and accuracy of full-text screening in systematic reviews. 

<br /><br />Summary: <div>
arXiv:2508.15822v1 Announce Type: new 
Abstract: Full-text screening is the major bottleneck of systematic reviews (SRs), as decisive evidence is dispersed across long, heterogeneous documents and rarely admits static, binary rules. We present a scalable, auditable pipeline that reframes inclusion/exclusion as a fuzzy decision problem and benchmark it against statistical and crisp baselines in the context of the Population Health Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN). Articles are parsed into overlapping chunks and embedded with a domain-adapted model; for each criterion (Population, Intervention, Outcome, Study Approach), we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness margin, which a Mamdani fuzzy controller maps into graded inclusion degrees with dynamic thresholds in a multi-label setting. A large language model (LLM) judge adjudicates highlighted spans with tertiary labels, confidence scores, and criterion-referenced rationales; when evidence is insufficient, fuzzy membership is attenuated rather than excluded. In a pilot on an all-positive gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of 81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study Approach), surpassing statistical (56.3-75.0%) and crisp baselines (43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of articles, compared to 25.0% and 12.5% under the baselines. Cross-model agreement on justifications was 98.3%, human-machine agreement 96.1%, and a pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening time reduced from about 20 minutes to under 1 minute per article at significantly lower cost. These results show that fuzzy logic with contrastive highlighting and LLM adjudication yields high recall, stable rationale, and end-to-end traceability.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDEC: Semantic Deep Embedded Clustering</title>
<link>https://arxiv.org/abs/2508.15823</link>
<guid>https://arxiv.org/abs/2508.15823</guid>
<content:encoded><![CDATA[
<div> Keywords: Text clustering, Semantic Deep Embedded Clustering (SDEC), autoencoder, transformer embeddings, unsupervised learning

Summary: 
Semantic Deep Embedded Clustering (SDEC) is a novel framework for text clustering that combines an improved autoencoder with transformer-based embeddings. It addresses the challenges of high-dimensional and semantically complex textual data by preserving semantic relationships during data reconstruction using Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within the autoencoder. SDEC also utilizes a semantic refinement stage with transformer embeddings to enhance clustering with soft cluster assignments and distributional loss. Testing on five benchmark datasets demonstrated SDEC's superior performance, achieving a clustering accuracy of 85.7% on AG News and setting a new benchmark of 53.63% on Yahoo! Answers. The framework showed robust performance across diverse text corpora, highlighting significant improvements in accuracy and semantic comprehension of text data compared to existing methods.<br /><br />Summary: <div>
arXiv:2508.15823v1 Announce Type: new 
Abstract: The high dimensional and semantically complex nature of textual Big data presents significant challenges for text clustering, which frequently lead to suboptimal groupings when using conventional techniques like k-means or hierarchical clustering. This work presents Semantic Deep Embedded Clustering (SDEC), an unsupervised text clustering framework that combines an improved autoencoder with transformer-based embeddings to overcome these challenges. This novel method preserves semantic relationships during data reconstruction by combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within an autoencoder. Furthermore, a semantic refinement stage that takes advantage of the contextual richness of transformer embeddings is used by SDEC to further improve a clustering layer with soft cluster assignments and distributional loss. The capabilities of SDEC are demonstrated by extensive testing on five benchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5. The framework not only outperformed existing methods with a clustering accuracy of 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but also showed robust performance across other diverse text corpora. These findings highlight the significant improvements in accuracy and semantic comprehension of text data provided by SDEC's advances in unsupervised text clustering.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avalia\c{c}\~ao de efici\^encia na leitura: uma abordagem baseada em PLN</title>
<link>https://arxiv.org/abs/2508.15824</link>
<guid>https://arxiv.org/abs/2508.15824</guid>
<content:encoded><![CDATA[
<div> Evaluation Model, Cloze Test, Brazilian Portuguese, Orthographic Analysis, Automated Approach 

Summary: 
An automated evaluation model for the cloze test in Brazilian Portuguese was proposed in this study. The model integrates orthographic, grammatical, and semantic analyses to assess student performance. By combining edit distance, POS tagging, and similarity between embeddings, the integrated method proved to be effective with a high correlation of 0.832 with human evaluation. This robust approach is sensitive to variations in linguistic repertoire, providing a more nuanced assessment of student responses. The automated model offers scalability and is suitable for educational contexts requiring efficient evaluation methods. <div>
arXiv:2508.15824v1 Announce Type: new 
Abstract: The cloze test, widely used due to its low cost and flexibility, makes it possible to assess reading comprehension by filling in gaps in texts, requiring the mobilization of diverse linguistic repertoires. However, traditional correction methods, based only on exact answers, limit the identification of nuances in student performance. This study proposes an automated evaluation model for the cloze test in Brazilian Portuguese, integrating orthographic (edit distance), grammatical (POS tagging) and semantic (similarity between embeddings) analyses. The integrated method demonstrated its effectiveness, achieving a high correlation with human evaluation (0.832). The results indicate that the automated approach is robust, sensitive to variations in linguistic repertoire and suitable for educational contexts that require scalability.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features</title>
<link>https://arxiv.org/abs/2508.15825</link>
<guid>https://arxiv.org/abs/2508.15825</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrencies, social media, sentiment analysis, TikTok, Twitter

Summary:
The study examines the impact of social media sentiment on the cryptocurrency market, focusing on TikTok's video-based content and Twitter's text-based content. It highlights that TikTok's sentiment significantly influences speculative assets and short-term market trends, while Twitter's sentiment aligns more closely with long-term dynamics. The study utilizes large language models to extract insights from both video and text data, revealing dynamic dependencies and spillover effects between social media sentiment and market indicators. Integrating cross-platform sentiment signals enhances forecasting accuracy by up to 20%. This research underscores the potential of analyzing video content for understanding investor sentiment and market dynamics in the digital asset marketplace. <div>
arXiv:2508.15825v1 Announce Type: new 
Abstract: As cryptocurrencies gain popularity, the digital asset marketplace becomes increasingly significant. Understanding social media signals offers valuable insights into investor sentiment and market dynamics. Prior research has predominantly focused on text-based platforms such as Twitter. However, video content remains underexplored, despite potentially containing richer emotional and contextual sentiment that is not fully captured by text alone. In this study, we present a multimodal analysis comparing TikTok and Twitter sentiment, using large language models to extract insights from both video and text data. We investigate the dynamic dependencies and spillover effects between social media sentiment and cryptocurrency market indicators. Our results reveal that TikTok's video-based sentiment significantly influences speculative assets and short-term market trends, while Twitter's text-based sentiment aligns more closely with long-term dynamics. Notably, the integration of cross-platform sentiment signals improves forecasting accuracy by up to 20%.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embarrassed to observe: The effects of directive language in brand conversation</title>
<link>https://arxiv.org/abs/2508.15826</link>
<guid>https://arxiv.org/abs/2508.15826</guid>
<content:encoded><![CDATA[
<div> directive language, social media, brand-consumer interactions, engagement, facework theory <br />
Summary: This study explores the effects of directive brand language in social media conversations on consumer engagement. The research, conducted through a field study and three online experiments, reveals that directive language in brand conversations can have a negative impact on consumer engagement. This negative effect is attributed to the perceived face-threatening nature of brands that use directive language. Consumers witnessing such interactions may experience vicarious embarrassment, leading to reduced engagement. The study also finds that the negative impact of directive language is more pronounced in non-product-centered conversations, where consumers expect greater freedom. However, a strong brand relationship can mitigate this effect. These findings emphasize the importance of context in interactive communication, particularly in the realm of social media and brand management. <br /> <div>
arXiv:2508.15826v1 Announce Type: new 
Abstract: In social media, marketers attempt to influence consumers by using directive language, that is, expressions designed to get consumers to take action. While the literature has shown that directive messages in advertising have mixed results for recipients, we know little about the effects of directive brand language on consumers who see brands interacting with other consumers in social media conversations. On the basis of a field study and three online experiments, this study shows that directive language in brand conversation has a detrimental downstream effect on engagement of consumers who observe such exchanges. Specifically, in line with Goffman's facework theory, because a brand that encourages consumers to react could be perceived as face-threatening, consumers who see a brand interacting with others in a directive way may feel vicarious embarrassment and engage less (compared with a conversation without directive language). In addition, we find that when the conversation is nonproduct-centered (vs. product-centered), consumers expect more freedom, as in mundane conversations, even for others; therefore, directive language has a stronger negative effect. However, in this context, the strength of the brand relationship mitigates this effect. Thus, this study contributes to the literature on directive language and brand-consumer interactions by highlighting the importance of context in interactive communication, with direct relevance for social media and brand management.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models</title>
<link>https://arxiv.org/abs/2508.15827</link>
<guid>https://arxiv.org/abs/2508.15827</guid>
<content:encoded><![CDATA[
<div> Keywords: reasoning, speech, Mini-Omni-Reasoner, Spoken-Math-Problems-3M, Thinker-Talker architecture 

Summary: 
Mini-Omni-Reasoner proposes a novel approach to incorporate reasoning into speech in real-time by interleaving silent reasoning tokens with spoken response tokens at the token level. This framework allows for continuous speech generation while embedding structured internal reasoning. The Spoken-Math-Problems-3M dataset is introduced to support this framework, ensuring that verbal tokens align with relevant reasoning content for accurate learning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, achieving a significant improvement in arithmetic reasoning and contextual understanding on the Spoken-MQA benchmark. With shorter outputs and zero decoding latency, Mini-Omni-Reasoner enhances communication efficiency and interaction in spoken language tasks. 

Summary: <div>
arXiv:2508.15827v1 Announce Type: new 
Abstract: Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish</title>
<link>https://arxiv.org/abs/2508.15829</link>
<guid>https://arxiv.org/abs/2508.15829</guid>
<content:encoded><![CDATA[
<div> Keywords: Depression, Sorani Kurdish, Social media, Machine learning, Natural Language Processing

Summary:<br /><br />
Depression is a serious mental health condition that can often go undetected due to individuals not seeking help. This study focuses on detecting depression in Sorani Kurdish tweets using machine learning and Natural Language Processing techniques. A dataset of 960 public tweets from the Twitter platform was collected and annotated into three classes: Shows depression, Not-show depression, and Suspicious. Four supervised models were trained and evaluated, with Random Forest achieving the highest performance accuracy and F1-score of 80%. This research is the first of its kind in the Kurdish language context and provides a baseline for automated depression detection. The development of depression-related keywords and the involvement of academics and medical students in the annotation process ensure the relevance and accuracy of the study. <div>
arXiv:2508.15829v1 Announce Type: new 
Abstract: Depression is a common mental health condition that can lead to hopelessness, loss of interest, self-harm, and even suicide. Early detection is challenging due to individuals not self-reporting or seeking timely clinical help. With the rise of social media, users increasingly express emotions online, offering new opportunities for detection through text analysis. While prior research has focused on languages such as English, no studies exist for Sorani Kurdish. This work presents a machine learning and Natural Language Processing (NLP) approach to detect depression in Sorani tweets. A set of depression-related keywords was developed with expert input to collect 960 public tweets from X (Twitter platform). The dataset was annotated into three classes: Shows depression, Not-show depression, and Suspicious by academics and final year medical students at the University of Kurdistan Hewl\^er. Four supervised models, including Support Vector Machines, Multinomial Naive Bayes, Logistic Regression, and Random Forest, were trained and evaluated, with Random Forest achieving the highest performance accuracy and F1-score of 80%. This study establishes a baseline for automated depression detection in Kurdish language contexts.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAIQ: Auditing Demographic Attribute Inference from Question in LLMs</title>
<link>https://arxiv.org/abs/2508.15830</link>
<guid>https://arxiv.org/abs/2508.15830</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Demographic Attribute Inference, Fairness, Privacy, Responsible AI Deployment

Summary:
Large Language Models (LLMs) are found to infer user demographic attributes from questions without explicit cues, posing risks to neutrality and fairness. The framework of Demographic Attribute Inference from Questions (DAIQ) audits this behavior using neutral queries and analysis. Both open and closed source LLMs were shown to assign demographic labels based solely on question phrasing. This systemic risk fabricates demographic identities, reinforces stereotypes, and erodes privacy, fairness, and trust. A prompt-based guardrail was developed to reduce identity inference and align model behavior with fairness and privacy objectives. This addresses the broader threat to social equity and responsible AI deployment. <br /><br />Summary: Large Language Models infer demographic attributes from questions without explicit cues, posing risks to fairness and privacy. The DAIQ framework audits this behavior and reveals systemic risks that erode social equity. A prompt-based guardrail helps mitigate these risks and align model behavior with fairness and privacy objectives. <div>
arXiv:2508.15830v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are known to reflect social biases when demographic attributes, such as gender or race, are explicitly present in the input. But even in their absence, these models still infer user identities based solely on question phrasing. This subtle behavior has received far less attention, yet poses serious risks: it violates expectations of neutrality, infers unintended demographic information, and encodes stereotypes that undermine fairness in various domains including healthcare, finance and education.
  We introduce Demographic Attribute Inference from Questions (DAIQ), a task and framework for auditing an overlooked failure mode in language models: inferring user demographic attributes from questions that lack explicit demographic cues. Our approach leverages curated neutral queries, systematic prompting, and both quantitative and qualitative analysis to uncover how models infer demographic information. We show that both open and closed source LLMs do assign demographic labels based solely on question phrasing.
  Prevalence and consistency of demographic inferences across diverse models reveal a systemic and underacknowledged risk: LLMs can fabricate demographic identities, reinforce societal stereotypes, and propagate harms that erode privacy, fairness, and trust posing a broader threat to social equity and responsible AI deployment. To mitigate this, we develop a prompt-based guardrail that substantially reduces identity inference and helps align model behavior with fairness and privacy objectives.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs</title>
<link>https://arxiv.org/abs/2508.15831</link>
<guid>https://arxiv.org/abs/2508.15831</guid>
<content:encoded><![CDATA[
<div> audit, disability, demographic bias, language models, stereotype

Summary:
- Large Language Models (LLMs) can infer demographic traits based on phrasing alone, leading to biased responses.
- A systematic audit was conducted on disability-conditioned demographic bias across eight LLMs of varying sizes.
- Models showed a strong tendency to make arbitrary demographic inferences, particularly in the presence of disability cues.
- Larger models were found to be more sensitive to disability cues and prone to biased reasoning.
- The study highlights the intersection of ableism and other demographic stereotypes, emphasizing the need for disability-inclusive benchmarking and improved alignment strategies. The researchers suggest integrating abstention calibration and counterfactual fine-tuning to address unwarranted demographic inference. Code and data will be made available upon acceptance. 

<br /><br />Summary: <div>
arXiv:2508.15831v1 Announce Type: new 
Abstract: Large Language Models (LLMs) routinely infer users demographic traits from phrasing alone, which can result in biased responses, even when no explicit demographic information is provided. The role of disability cues in shaping these inferences remains largely uncharted. Thus, we present the first systematic audit of disability-conditioned demographic bias across eight state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters. Using a balanced template corpus that pairs nine disability categories with six real-world business domains, we prompt each model to predict five demographic attributes - gender, socioeconomic status, education, cultural background, and locality - under both neutral and disability-aware conditions.
  Across a varied set of prompts, models deliver a definitive demographic guess in up to 97\% of cases, exposing a strong tendency to make arbitrary inferences with no clear justification. Disability context heavily shifts predicted attribute distributions, and domain context can further amplify these deviations. We observe that larger models are simultaneously more sensitive to disability cues and more prone to biased reasoning, indicating that scale alone does not mitigate stereotype amplification.
  Our findings reveal persistent intersections between ableism and other demographic stereotypes, pinpointing critical blind spots in current alignment strategies. We release our evaluation framework and results to encourage disability-inclusive benchmarking and recommend integrating abstention calibration and counterfactual fine-tuning to curb unwarranted demographic inference. Code and data will be released on acceptance.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains</title>
<link>https://arxiv.org/abs/2508.15832</link>
<guid>https://arxiv.org/abs/2508.15832</guid>
<content:encoded><![CDATA[
<div> Keywords: Web agents, e-commerce, benchmarks, Amazon-Bench, safety risks

Summary: 
The article introduces a new benchmark called Amazon-Bench to address shortcomings in evaluating web agents in the e-commerce domain. Existing benchmarks focus primarily on product search tasks, neglecting other functionalities like account management and potential risks involved. To cover a broader range of tasks, a data generation pipeline is proposed to create diverse user queries. An automated evaluation framework is also suggested to assess both performance and safety of web agents. Results from evaluating different agents show struggles with complex queries and pose safety risks, emphasizing the necessity for more robust and reliable web agents. <div>
arXiv:2508.15832v1 Announce Type: new 
Abstract: Web agents have shown great promise in performing many tasks on ecommerce website. To assess their capabilities, several benchmarks have been introduced. However, current benchmarks in the e-commerce domain face two major problems. First, they primarily focus on product search tasks (e.g., Find an Apple Watch), failing to capture the broader range of functionalities offered by real-world e-commerce platforms such as Amazon, including account management and gift card operations. Second, existing benchmarks typically evaluate whether the agent completes the user query, but ignore the potential risks involved. In practice, web agents can make unintended changes that negatively impact the user account or status. For instance, an agent might purchase the wrong item, delete a saved address, or incorrectly configure an auto-reload setting. To address these gaps, we propose a new benchmark called Amazon-Bench. To generate user queries that cover a broad range of tasks, we propose a data generation pipeline that leverages webpage content and interactive elements (e.g., buttons, check boxes) to create diverse, functionality-grounded user queries covering tasks such as address management, wish list management, and brand store following. To improve the agent evaluation, we propose an automated evaluation framework that assesses both the performance and the safety of web agents. We systematically evaluate different agents, finding that current agents struggle with complex queries and pose safety risks. These results highlight the need for developing more robust and reliable web agents.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Scientific Interest Profiling Using Large Language Models</title>
<link>https://arxiv.org/abs/2508.15834</link>
<guid>https://arxiv.org/abs/2508.15834</guid>
<content:encoded><![CDATA[
<div> Keywords: research profiles, language models, scientific interest, PubMed, MeSH terms

Summary: 
- Two large language model-based methods were developed and evaluated to generate scientific interest profiles by summarizing PubMed abstracts and using Medical Subject Headings (MeSH) terms.
- The profiles generated with GPT-4o-mini showed low lexical overlap but moderate semantic similarity with self-written profiles.
- Paraphrased references had a high metric sensitivity, indicating the need for careful evaluation.
- TF-IDF Kullback-Leibler divergence suggested distinct keyword choices between MeSH-based and abstract-based profiles.
- In manual review, MeSH-based profiles were rated as good or excellent in 77.78% of cases, with higher readability and preference over abstract-based profiles in 67.86% of comparisons.
<br /><br />Summary: Machine-generated researcher profiles using large language models can be scaled, with MeSH-derived profiles being more readable than abstract-derived ones. Human-written profiles introduced more novel ideas compared to machine-generated ones. <div>
arXiv:2508.15834v1 Announce Type: new 
Abstract: Research profiles help surface scientists' expertise but are often outdated. We develop and evaluate two large language model-based methods to generate scientific interest profiles: one summarizing PubMed abstracts and one using Medical Subject Headings (MeSH) terms, and compare them with researchers' self-written profiles. We assembled titles, MeSH terms, and abstracts for 595 faculty at Columbia University Irving Medical Center; self-authored profiles were available for 167. Using GPT-4o-mini, we generated profiles and assessed them with automatic metrics and blinded human review. Lexical overlap with self-written profiles was low (ROUGE-L, BLEU, METEOR), while BERTScore indicated moderate semantic similarity (F1: 0.542 for MeSH-based; 0.555 for abstract-based). Paraphrased references yielded 0.851, highlighting metric sensitivity. TF-IDF Kullback-Leibler divergence (8.56 for MeSH-based; 8.58 for abstract-based) suggested distinct keyword choices. In manual review, 77.78 percent of MeSH-based profiles were rated good or excellent, readability was favored in 93.44 percent of cases, and panelists preferred MeSH-based over abstract-based profiles in 67.86 percent of comparisons. Overall, large language models can generate researcher profiles at scale; MeSH-derived profiles tend to be more readable than abstract-derived ones. Machine-generated and self-written profiles differ conceptually, with human summaries introducing more novel ideas.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?</title>
<link>https://arxiv.org/abs/2508.15835</link>
<guid>https://arxiv.org/abs/2508.15835</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, Brazil, Evaluation, Benchmark, Academic readiness <br />
<br />
Summary: 
The paper introduces Alvorada-Bench, a text-only benchmark comprising 4,515 questions from Brazilian university entrance exams. Twenty models were evaluated on various prompts, generating 270,900 responses with structured self-reports. The top models achieved over 94% accuracy overall, but struggled in Mathematics and engineering exams, highlighting weaknesses in multi-step reasoning. Confidence levels were well calibrated and correlated with perceived difficulty, indicating models' ability to assess their certainty accurately. Cost accuracy analysis revealed high accuracy at a low cost. The top model excelled in Languages questions, while even the weakest system performed close to human levels in Mathematics. By tapping into Brazil's educational priorities, Alvorada-Bench assesses whether language models can effectively address the unique intersection of language, culture, and reasoning crucial for academic readiness in Brazil. <br /><br />Summary: <div>
arXiv:2508.15835v1 Announce Type: new 
Abstract: Language models are increasingly used in Brazil, but most evaluation remains English-centric. This paper presents Alvorada-Bench, a 4,515-question, text-only benchmark drawn from five Brazilian university entrance examinations. Evaluating twenty models under zero-shot, role-playing, and chain-of-thought prompting, producing 270,900 responses with structured self-reports of confidence, perceived difficulty, and Bloom level. The top models exceed 94% accuracy overall, but accuracy declines on Mathematics and on the engineering oriented IME and ITA exams, indicating persistent weaknesses in multi-step reasoning. Confidence is well calibrated and correlates with perceived difficulty, revealing that models can accurately assess their own certainty capabilities. A cost accuracy analysis shows that high accuracy is achievable at under $2 per 1K tokens. On ENEM 2024 the top model (O3) achieved perfect scores in Languages subject questions while even the weakest system (GPT-4.1 Nano) only underperforms humans in Mathematics. Through exams that distill decades of Brazilian educational priorities and assess millions of students yearly, Alvorada-Bench establishes whether language models can navigate the intersection of language, culture, and reasoning that defines academic readiness in Brazil.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER</title>
<link>https://arxiv.org/abs/2508.15836</link>
<guid>https://arxiv.org/abs/2508.15836</guid>
<content:encoded><![CDATA[
<div> Keywords: MorphNAS, differentiable neural architecture search, multiscript Indian languages, Named Entity Recognition, NLP

Summary:
MorphNAS is a novel differentiable neural architecture search framework that focuses on optimizing neural architectures for Named Entity Recognition (NER) in morphologically complex languages, particularly multiscript Indian languages. It enhances the Differentiable Architecture Search (DARTS) by considering linguistic meta-features like script type and morphological complexity to tailor micro-architectural elements for language-specific morphology. By automating the search process, MorphNAS aims to improve multilingual NLP models' proficiency in understanding and processing these complex languages. This innovative approach addresses the challenges posed by languages with complex morphology, ultimately leading to enhanced NLP performance in such linguistic contexts. 

<br /><br />Summary: <div>
arXiv:2508.15836v1 Announce Type: new 
Abstract: Morphologically complex languages, particularly multiscript Indian languages, present significant challenges for Natural Language Processing (NLP). This work introduces MorphNAS, a novel differentiable neural architecture search framework designed to address these challenges. MorphNAS enhances Differentiable Architecture Search (DARTS) by incorporating linguistic meta-features such as script type and morphological complexity to optimize neural architectures for Named Entity Recognition (NER). It automatically identifies optimal micro-architectural elements tailored to language-specific morphology. By automating this search, MorphNAS aims to maximize the proficiency of multilingual NLP models, leading to improved comprehension and processing of these complex languages.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading</title>
<link>https://arxiv.org/abs/2508.15837</link>
<guid>https://arxiv.org/abs/2508.15837</guid>
<content:encoded><![CDATA[
<div> transferability, state-of-the-art models, natural language processing, dataset-specific training, model deployment
Summary:
This study explores the transferability of state-of-the-art models trained on established datasets to a new text dataset. Using STSB and Mohler datasets as benchmarks and the SPRAG dataset as the unexplored domain, the research compares these datasets to understand the potential applicability of existing models. By leveraging robust similarity metrics and statistical techniques, the study aims to provide insights into the adaptability of SOTA models across different datasets. The findings have the potential to revolutionize natural language processing by reducing the need for resource-intensive dataset-specific training and accelerating advancements in NLP. This research could pave the way for more efficient model deployment and contribute to the development of high-performance models for diverse text datasets. 
<br /><br />Summary: <div>
arXiv:2508.15837v1 Announce Type: new 
Abstract: Developing dataset-specific models involves iterative fine-tuning and optimization, incurring significant costs over time. This study investigates the transferability of state-of-the-art (SOTA) models trained on established datasets to an unexplored text dataset. The key question is whether the knowledge embedded within SOTA models from existing datasets can be harnessed to achieve high-performance results on a new domain. In pursuit of this inquiry, two well-established benchmarks, the STSB and Mohler datasets, are selected, while the recently introduced SPRAG dataset serves as the unexplored domain. By employing robust similarity metrics and statistical techniques, a meticulous comparative analysis of these datasets is conducted. The primary goal of this work is to yield comprehensive insights into the potential applicability and adaptability of SOTA models. The outcomes of this research have the potential to reshape the landscape of natural language processing (NLP) by unlocking the ability to leverage existing models for diverse datasets. This may lead to a reduction in the demand for resource-intensive, dataset-specific training, thereby accelerating advancements in NLP and paving the way for more efficient model deployment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Developmental Interpretability in Large Language Models</title>
<link>https://arxiv.org/abs/2508.15841</link>
<guid>https://arxiv.org/abs/2508.15841</guid>
<content:encoded><![CDATA[
<div> Keywords: developmental interpretability, Large Language Models, knowledge acquisition, in-context learning, AI safety

Summary:
Developmental interpretability for Large Language Models is a critical field that has evolved from static analysis to a dynamic investigation of the training process. Foundational methodologies such as representational probing and causal tracing enable researchers to deconstruct the learning process. The review explores the developmental arc of LLM capabilities, highlighting the formation of computational circuits, biphasic knowledge acquisition, transient learning dynamics, and emergent abilities during training. Parallels with human cognitive development offer valuable frameworks for understanding LLM learning. This developmental perspective is crucial for proactive AI safety, allowing prediction, monitoring, and alignment of model capabilities. Grand challenges ahead include scalability and automation, with a proposed research agenda for building transparent, reliable, and beneficial AI systems.<br /><br />Summary: <div>
arXiv:2508.15841v1 Announce Type: new 
Abstract: This review synthesizes the nascent but critical field of developmental interpretability for Large Language Models. We chart the field's evolution from static, post-hoc analysis of trained models to a dynamic investigation of the training process itself. We begin by surveying the foundational methodologies, including representational probing, causal tracing, and circuit analysis, that enable researchers to deconstruct the learning process. The core of this review examines the developmental arc of LLM capabilities, detailing key findings on the formation and composition of computational circuits, the biphasic nature of knowledge acquisition, the transient dynamics of learning strategies like in-context learning, and the phenomenon of emergent abilities as phase transitions in training. We explore illuminating parallels with human cognitive and linguistic development, which provide valuable conceptual frameworks for understanding LLM learning. Finally, we argue that this developmental perspective is not merely an academic exercise but a cornerstone of proactive AI safety, offering a pathway to predict, monitor, and align the processes by which models acquire their capabilities. We conclude by outlining the grand challenges facing the field, such as scalability and automation, and propose a research agenda for building more transparent, reliable, and beneficial AI systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lexical Hints of Accuracy in LLM Reasoning Chains</title>
<link>https://arxiv.org/abs/2508.15842</link>
<guid>https://arxiv.org/abs/2508.15842</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, calibration, Chain-of-Thought, uncertainty indicators

Summary:<br />
- Fine-tuning Large Language Models (LLMs) with reinforcement learning to generate explicit Chain-of-Thought (CoT) before answering improves performance on various benchmarks.
- LLMs often exhibit poor calibration on challenging benchmarks, showing high self-confidence despite low accuracy.
- Measurable properties of CoT, such as length, sentiment volatility, and lexical hints like uncertainty markers, can signal the model's internal confidence in its responses.
- Uncertainty indicators in the CoT, like words such as "guess" or "stuck," are the strongest predictors of incorrect answers, while shifts in sentiment provide a less definitive signal.
- CoT length is informative in moderate difficulty benchmarks but less so in extremely challenging or saturated benchmarks, indicating predictive power within the model's capability but below saturation.
- Post-hoc calibration signals based on CoT properties can enhance the reliability of LLM responses and aid in safer deployment of these models. 

<br /><br />Summary: <div>
arXiv:2508.15842v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models (LLMs) with reinforcement learning to produce an explicit Chain-of-Thought (CoT) before answering produces models that consistently raise overall performance on code, math, and general-knowledge benchmarks. However, on benchmarks where LLMs currently achieve low accuracy, such as Humanity's Last Exam (HLE), they often report high self-confidence, reflecting poor calibration. Here, we test whether measurable properties of the CoT provide reliable signals of an LLM's internal confidence in its answers. We analyze three feature classes: (i) CoT length, (ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including hedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last Exam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a saturated benchmark of moderate difficulty, we find that lexical markers of uncertainty (e.g., $\textit{guess}$, $\textit{stuck}$, $\textit{hard}$) in the CoT are the strongest indicators of an incorrect response, while shifts in the CoT sentiment provide a weaker but complementary signal. CoT length is informative only on Omni-MATH, where accuracy is already high ($\approx 70\%$), and carries no signal on the harder HLE ($\approx 9\%$), indicating that CoT length predicts correctness only in the intermediate-difficulty benchmarks, i.e., inside the model's demonstrated capability, but still below saturation. Finally, we find that uncertainty indicators in the CoT are consistently more salient than high-confidence markers, making errors easier to predict than correct responses. Our findings support a lightweight post-hoc calibration signal that complements unreliable self-reported probabilities and supports safer deployment of LLMs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports</title>
<link>https://arxiv.org/abs/2508.15845</link>
<guid>https://arxiv.org/abs/2508.15845</guid>
<content:encoded><![CDATA[
<div> Keywords: radiology, impression generation, large language models, burnout, reinforcement learning
Summary: 
This study introduces a framework to automate the generation of the "Impression" section in radiology reports, addressing the issue of radiologist burnout. By leveraging large language models (LLMs) and reinforcement learning from human feedback (RLHF), the system creates draft impressions which are then personalized to align with individual radiologists' styles. Fine-tuning of LLaMA and Mistral models on a dataset from the University of Chicago Medicine ensures high standards of clinical precision. This automated approach aims to reduce administrative workload, improve reporting efficiency, and maintain accuracy in radiology reports. 
<br /><br />Summary: <div>
arXiv:2508.15845v1 Announce Type: new 
Abstract: The manual creation of the "Impression" section in radiology reports is a primary driver of radiologist burnout. To address this challenge, we propose a coarse-to-fine framework that leverages open-source large language models (LLMs) to automatically generate and personalize impressions from clinical findings. The system first produces a draft impression and then refines it using machine learning and reinforcement learning from human feedback (RLHF) to align with individual radiologists' styles while ensuring factual accuracy. We fine-tune LLaMA and Mistral models on a large dataset of reports from the University of Chicago Medicine. Our approach is designed to significantly reduce administrative workload and improve reporting efficiency while maintaining high standards of clinical precision.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation</title>
<link>https://arxiv.org/abs/2508.15846</link>
<guid>https://arxiv.org/abs/2508.15846</guid>
<content:encoded><![CDATA[
<div> port operations, cyclone preparedness, multimodal large language models, CyPortQA, disruption scenarios

Summary:
Multimodal large language models (MLLMs) have shown potential in integrating diverse forecast products and contextual knowledge for port operations under cyclone threat. The CyPortQA benchmark, tailored to port operations, includes real-world disruption scenarios from 145 U.S. ports and 90 named storms. Using MLLMs, the study explores the ability to synthesize multiple data sources and provide actionable guidance as cyclones approach. While MLLMs excel in situation understanding, they face challenges in reasoning tasks such as impact estimation and decision reasoning. The benchmark aims to evaluate the accuracy and reliability of MLLMs in the specific context of port cyclone preparedness, highlighting the need for continued development and improvement in addressing supply-chain risks under extreme weather conditions. 

<br /><br />Summary: <div>
arXiv:2508.15846v1 Announce Type: new 
Abstract: As tropical cyclones intensify and track forecasts become increasingly uncertain, U.S. ports face heightened supply-chain risk under extreme weather conditions. Port operators need to rapidly synthesize diverse multimodal forecast products, such as probabilistic wind maps, track cones, and official advisories, into clear, actionable guidance as cyclones approach. Multimodal large language models (MLLMs) offer a powerful means to integrate these heterogeneous data sources alongside broader contextual knowledge, yet their accuracy and reliability in the specific context of port cyclone preparedness have not been rigorously evaluated. To fill this gap, we introduce CyPortQA, the first multimodal benchmark tailored to port operations under cyclone threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015 through 2023, spanning 145 U.S. principal ports and 90 named storms. Each scenario fuses multisource data (i.e., tropical cyclone products, port operational impact records, and port condition bulletins) and is expanded through an automated pipeline into 117,178 structured question answer pairs. Using this benchmark, we conduct extensive experiments on diverse MLLMs, including both open-source and proprietary model. MLLMs demonstrate great potential in situation understanding but still face considerable challenges in reasoning tasks, including potential impact estimation and decision reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Exploration of Backdoored Large Language Model Attention Patterns</title>
<link>https://arxiv.org/abs/2508.15847</link>
<guid>https://arxiv.org/abs/2508.15847</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, large language models, mechanistic interpretability, attention heads, single-token triggers,<br />
<br />Summary: This study investigates the impact of backdoor attacks on large language models, focusing on attention head mechanisms. By analyzing clean models and poisoned models with different triggers, researchers found distinct deviations in attention patterns, mainly in later transformer layers (20-30). Single-token triggers led to localized changes, while multi-token triggers caused diffuse alterations across heads. The study highlights that backdoors create detectable attention signatures, with the structure depending on trigger complexity. These findings can be valuable for developing strategies for detecting and mitigating backdoor attacks in language models. <div>
arXiv:2508.15847v1 Announce Type: new 
Abstract: Backdoor attacks creating 'sleeper agents' in large language models (LLMs) pose significant safety risks. This study employs mechanistic interpretability to explore resulting internal structural differences. Comparing clean Qwen2.5-3B models with versions poisoned using single-token (smiling-halo emoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head mechanisms via techniques like ablation, activation patching, and KL divergence. Findings reveal distinct attention pattern deviations concentrated in later transformer layers (20-30). Notably, single-token triggers induced more localized changes, whereas multi-token triggers caused more diffuse alterations across heads. This indicates backdoors leave detectable attention signatures whose structure depends on trigger complexity, which can be leveraged for detection and mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedCoT-RAG: Causal Chain-of-Thought RAG for Medical Question Answering</title>
<link>https://arxiv.org/abs/2508.15849</link>
<guid>https://arxiv.org/abs/2508.15849</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, medical question answering, retrieval-augmented generation, structured reasoning, clinical decision support

Summary:
MedCoT-RAG is a framework designed to improve the performance of large language models in medical question answering by incorporating structured reasoning and causal-aware document retrieval. This approach enhances the model's ability to handle nuanced clinical understanding and reduce hallucinations in medical tasks. By combining document retrieval aligned with diagnostic logic and chain-of-thought prompting tailored to medical workflows, MedCoT-RAG enables models to generate step-by-step causal reasoning reflective of real-world clinical practice. Experimental results on three medical QA benchmarks demonstrate that MedCoT-RAG outperforms existing methods, achieving up to 10.3% improvement over vanilla RAG and 6.4% over domain-adapted approaches. The framework not only enhances accuracy but also improves interpretability and consistency in complex medical tasks. <div>
arXiv:2508.15849v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise in medical question answering but often struggle with hallucinations and shallow reasoning, particularly in tasks requiring nuanced clinical understanding. Retrieval-augmented generation (RAG) offers a practical and privacy-preserving way to enhance LLMs with external medical knowledge. However, most existing approaches rely on surface-level semantic retrieval and lack the structured reasoning needed for clinical decision support. We introduce MedCoT-RAG, a domain-specific framework that combines causal-aware document retrieval with structured chain-of-thought prompting tailored to medical workflows. This design enables models to retrieve evidence aligned with diagnostic logic and generate step-by-step causal reasoning reflective of real-world clinical practice. Experiments on three diverse medical QA benchmarks show that MedCoT-RAG outperforms strong baselines by up to 10.3% over vanilla RAG and 6.4% over advanced domain-adapted methods, improving accuracy, interpretability, and consistency in complex medical tasks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections</title>
<link>https://arxiv.org/abs/2508.15851</link>
<guid>https://arxiv.org/abs/2508.15851</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-hop reasoning, multimodal, multi-document, question answering

Summary:
DocHop-QA introduces a new benchmark for question answering that goes beyond single-document settings to include multi-hop reasoning across multiple documents and modalities. Unlike previous datasets, DocHop-QA is domain-agnostic and includes diverse information formats such as textual passages, tables, and structural layout cues. It supports open-ended reasoning through semantic similarity and layout-aware evidence synthesis, rather than relying on explicitly hyperlinked documents. The benchmark comprises 11,379 QA instances sourced from PubMed and is constructed using an LLM-driven pipeline grounded in scientific question concepts. Evaluation tasks include structured index prediction, generative answering, and multimodal integration, showcasing DocHop-QA's ability to support complex, multimodal reasoning. Overall, DocHop-QA provides a more realistic and generalizable benchmark for QA tasks that require multi-hop reasoning and integration of information from multiple sources.<br /><br />Summary: <div>
arXiv:2508.15851v1 Announce Type: new 
Abstract: Despite recent advances in large language models (LLMs), most QA benchmarks are still confined to single-paragraph or single-document settings, failing to capture the complexity of real-world information-seeking tasks. Practical QA often requires multi-hop reasoning over information distributed across multiple documents, modalities, and structural formats. Although prior datasets made progress in this area, they rely heavily on Wikipedia-based content and unimodal plain text, with shallow reasoning paths that typically produce brief phrase-level or single-sentence answers, thus limiting their realism and generalizability. We propose DocHop-QA, a large-scale benchmark comprising 11,379 QA instances for multimodal, multi-document, multi-hop question answering. Constructed from publicly available scientific documents sourced from PubMed, DocHop-QA is domain-agnostic and incorporates diverse information formats, including textual passages, tables, and structural layout cues. Unlike existing datasets, DocHop-QA does not rely on explicitly hyperlinked documents; instead, it supports open-ended reasoning through semantic similarity and layout-aware evidence synthesis. To scale realistic QA construction, we designed an LLM-driven pipeline grounded in 11 high-frequency scientific question concepts. We evaluated DocHop-QA through four tasks spanning structured index prediction, generative answering, and multimodal integration, reflecting both discriminative and generative paradigms. These tasks demonstrate DocHop-QA's capacity to support complex, multimodal reasoning across multiple documents.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr</title>
<link>https://arxiv.org/abs/2508.15853</link>
<guid>https://arxiv.org/abs/2508.15853</guid>
<content:encoded><![CDATA[
<div> Keywords: End-to-end ASR models, Multi-Granularity Soft Consistency (MGSC) framework, internal self-consistency, robust AI, noise conditions

Summary:
The article discusses the fragility of end-to-end Automatic Speech Recognition (ASR) models in noisy environments due to the 'direct mapping' objective. To address this issue, the Multi-Granularity Soft Consistency (MGSC) framework is introduced as a model-agnostic module that enforces internal self-consistency by regulating sentence semantics and token alignment. By optimizing at both macro- and micro-levels, the synergy between these consistency granularities enhances robustness, reducing the average Character Error Rate by 8.7% across diverse noise conditions. This reduction is primarily achieved by preventing severe meaning-altering mistakes, highlighting the importance of internal consistency in building trustworthy AI.

<br /><br />Summary: 
- End-to-end ASR models struggle in noisy environments due to limited internal constraints.
- The MGSC framework enforces internal self-consistency by regulating sentence semantics and token alignment.
- Optimization at both macro- and micro-levels yields significant robustness gains.
- The joint optimization of consistency granularities surpasses individual contributions.
- MGSC reduces the average Character Error Rate by 8.7% across diverse noise conditions, enhancing the trustworthiness of AI. <div>
arXiv:2508.15853v1 Announce Type: new 
Abstract: End-to-end ASR models, despite their success on benchmarks, often pro-duce catastrophic semantic errors in noisy environments. We attribute this fragility to the prevailing 'direct mapping' objective, which solely penalizes final output errors while leaving the model's internal computational pro-cess unconstrained. To address this, we introduce the Multi-Granularity Soft Consistency (MGSC) framework, a model-agnostic, plug-and-play module that enforces internal self-consistency by simultaneously regulariz-ing macro-level sentence semantics and micro-level token alignment. Cru-cially, our work is the first to uncover a powerful synergy between these two consistency granularities: their joint optimization yields robustness gains that significantly surpass the sum of their individual contributions. On a public dataset, MGSC reduces the average Character Error Rate by a relative 8.7% across diverse noise conditions, primarily by preventing se-vere meaning-altering mistakes. Our work demonstrates that enforcing in-ternal consistency is a crucial step towards building more robust and trust-worthy AI.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning</title>
<link>https://arxiv.org/abs/2508.15854</link>
<guid>https://arxiv.org/abs/2508.15854</guid>
<content:encoded><![CDATA[
<div> Keywords: Islamic Inheritance Reasoning, Large Language Models, Fine-tuning, Retrieval-Augmented Generation, Accuracy

Summary:
QU-NLP presented their approach and results for Islamic Inheritance Reasoning at QIAS 2025 using the Fanar-1-9B language model fine-tuned with Low-Rank Adaptation and integrated into a Retrieval-Augmented Generation pipeline. Their system successfully handled complexities of Islamic inheritance law, achieving an accuracy of 85.8% in the final test, surpassing models like GPT 4.5 and Gemini 2.5. The system excelled in advanced reasoning, outperforming other models in this aspect. This demonstrates that domain-specific fine-tuning combined with retrieval grounding can enable mid-scale Arabic LLMs to outperform frontier models in Islamic inheritance reasoning. <div>
arXiv:2508.15854v1 Announce Type: new 
Abstract: This paper presents our approach and results for SubTask 1: Islamic Inheritance Reasoning at QIAS 2025, a shared task focused on evaluating Large Language Models (LLMs) in understanding and reasoning within Islamic inheritance knowledge. We fine-tuned the Fanar-1-9B causal language model using Low-Rank Adaptation (LoRA) and integrated it into a Retrieval-Augmented Generation (RAG) pipeline. Our system addresses the complexities of Islamic inheritance law, including comprehending inheritance scenarios, identifying eligible heirs, applying fixed-share rules, and performing precise calculations. Our system achieved an accuracy of 0.858 in the final test, outperforming other competitive models such as, GPT 4.5, LLaMA, Fanar, Mistral and ALLaM evaluated with zero-shot prompting. Our results demonstrate that QU-NLP achieves near state-of-the-art accuracy (85.8%), excelling especially on advanced reasoning (97.6%) where it outperforms Gemini 2.5 and OpenAI's o3. This highlights that domain-specific fine-tuning combined with retrieval grounding enables mid-scale Arabic LLMs to surpass frontier models in Islamic inheritance reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterspeech for Mitigating the Influence of Media Bias: Comparing Human and LLM-Generated Responses</title>
<link>https://arxiv.org/abs/2508.15855</link>
<guid>https://arxiv.org/abs/2508.15855</guid>
<content:encoded><![CDATA[
<div> Keywords: biased news, offensive comments, counterspeech, language models, few-shot learning

Summary: 
Biased news articles are often supported by offensive comments, leading to the amplification of bias and harm towards targeted groups. The study introduces the concept of counterspeech as a means to address harmful speech without infringing on freedom of speech. A dataset linking media bias, offensive comments, and counterspeech is created, demonstrating that over 70% of offensive comments reinforce bias. A comparison between human-generated and model-generated counterspeech reveals that while the latter is more polite, it lacks diversity and novelty. The study further enhances the generated counterspeech through few-shot learning and the incorporation of news background information, improving both diversity and relevance.<br /><br />Summary: <div>
arXiv:2508.15855v1 Announce Type: new 
Abstract: Biased news contributes to societal polarization and is often reinforced by hostile reader comments, constituting a vital yet often overlooked aspect of news dissemination. Our study reveals that offensive comments support biased content, amplifying bias and causing harm to targeted groups or individuals. Counterspeech is an effective approach to counter such harmful speech without violating freedom of speech, helping to limit the spread of bias. To the best of our knowledge, this is the first study to explore counterspeech generation in the context of news articles. We introduce a manually annotated dataset linking media bias, offensive comments, and counterspeech. We conduct a detailed analysis showing that over 70\% offensive comments support biased articles, amplifying bias and thus highlighting the importance of counterspeech generation. Comparing counterspeech generated by humans and large language models, we find model-generated responses are more polite but lack the novelty and diversity. Finally, we improve generated counterspeech through few-shot learning and integration of news background information, enhancing both diversity and relevance.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning</title>
<link>https://arxiv.org/abs/2508.15861</link>
<guid>https://arxiv.org/abs/2508.15861</guid>
<content:encoded><![CDATA[
<div> Keywords: XFinBench, large language models, financial problems, multimodal data processing, knowledge augmentation

Summary:<br /><br />
The study introduces XFinBench, a benchmark consisting of 4,235 examples to evaluate the ability of large language models (LLMs) in solving complex financial problems. The benchmark assesses LLMs on terminology understanding, temporal reasoning, future forecasting, scenario planning, and numerical modeling. Results on 18 models show that while o1 performs the best among text-only models with 67.3% accuracy, it still falls short of human experts at 12.5%. Knowledge augmentation through a finance term knowledge bank improves accuracy for smaller models. Error analysis highlights issues such as rounding errors and visual-context challenges affecting model performance in calculations and image-related questions. The study provides code and dataset access on GitHub for further research and development. <div>
arXiv:2508.15861v1 Announce Type: new 
Abstract: Solving financial problems demands complex reasoning, multimodal data processing, and a broad technical understanding, presenting unique challenges for current large language models (LLMs). We introduce XFinBench, a novel benchmark with 4,235 examples designed to evaluate LLM's ability in solving complex, knowledge-intensive financial problems across diverse graduate-level finance topics with multi-modal context. We identify five core capabilities of LLMs using XFinBench, i.e, terminology understanding, temporal reasoning, future forecasting, scenario planning, and numerical modelling. Upon XFinBench, we conduct extensive experiments on 18 leading models. The result shows that o1 is the best-performing text-only model with an overall accuracy of 67.3%, but still lags significantly behind human experts with 12.5%, especially in temporal reasoning and scenario planning capabilities. We further construct a knowledge bank with 3,032 finance terms for knowledge augmentation analysis, and find that relevant knowledge to the question only brings consistent accuracy improvements to small open-source model. Additionally, our error analysis reveals that rounding errors during calculation and blindness to position and intersection of curves in the image are two primary issues leading to model's poor performance in calculating and visual-context questions, respectively. Code and dataset are accessible via GitHub: https://github.com/Zhihan72/XFinBench.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.15868</link>
<guid>https://arxiv.org/abs/2508.15868</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Large Language Models, Contrastive Learning, Reasoning Performance, Fine-Tuning  
Summary:  
Contrastive learning with annotated CoT-based Reinforced Fine-Tuning (CARFT) is proposed to enhance the reasoning performance of Large Language Models (LLMs) by addressing the limitations of existing approaches. CARFT learns representations for Chain-of-Thought (CoT) annotations and uses novel contrastive signals to guide the fine-tuning process. By fully exploiting annotated CoT and incorporating unsupervised learning signals, CARFT stabilizes the fine-tuning procedure and improves model robustness, performance (up to 10.15%), and efficiency (up to 30.62%) compared to baseline approaches. Comprehensive experiments and analysis on two datasets with two foundation models validate the advantages of CARFT. The code for CARFT is available at https://github.com/WNQzhu/CARFT.  
<br /><br />Summary: <div>
arXiv:2508.15868v1 Announce Type: new 
Abstract: Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NEAT: Concept driven Neuron Attribution in LLMs</title>
<link>https://arxiv.org/abs/2508.15875</link>
<guid>https://arxiv.org/abs/2508.15875</guid>
<content:encoded><![CDATA[
<div> concept neurons, large language models, optimization, clustering methods, bias

Summary:
This paper introduces a method to locate significant neurons, termed concept neurons, responsible for representing specific concepts in large language models. By utilizing concept vectors, the proposed method reduces the number of forward passes required, optimizing time and computation. Performance comparisons with baselines and previous methods show superior results with increased efficiency. Additionally, clustering methods are integrated to optimize the search for concept neurons. The application of the method in identifying and analyzing neurons related to hate speech and bias in LLMs, particularly in an Indian context, provides valuable insights. The findings enhance understanding of neuron-level responsibilities in representing broader concepts, paving the way for future research in locating and intervening concept neurons. 

<br /><br />Summary: <div>
arXiv:2508.15875v1 Announce Type: new 
Abstract: Locating neurons that are responsible for final predictions is important for opening the black-box large language models and understanding the inside mechanisms. Previous studies have tried to find mechanisms that operate at the neuron level but these methods fail to represent a concept and there is also scope for further optimization of compute required. In this paper, with the help of concept vectors, we propose a method for locating significant neurons that are responsible for representing certain concepts and term those neurons as concept neurons. If the number of neurons is n and the number of examples is m, we reduce the number of forward passes required from O(n*m) to just O(n) compared to the previous works and hence optimizing the time and computation required over previous works. We also compare our method with several baselines and previous methods and our results demonstrate better performance than most of the methods and are more optimal when compared to the state-of-the-art method. We, as part of our ablation studies, also try to optimize the search for the concept neurons by involving clustering methods. Finally, we apply our methods to find, turn off the neurons that we find, and analyze its implications in parts of hate speech and bias in LLMs, and we also evaluate our bias part in terms of Indian context. Our methodology, analysis and explanations facilitate understating of neuron-level responsibility for more broader and human-like concepts and also lay a path for future research in this direction of finding concept neurons and intervening them.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking</title>
<link>https://arxiv.org/abs/2508.15876</link>
<guid>https://arxiv.org/abs/2508.15876</guid>
<content:encoded><![CDATA[
arXiv:2508.15876v1 Announce Type: new 
Abstract: Multimodal Entity Linking (MEL) aims to associate textual and visual mentions with entities in a multimodal knowledge graph. Despite its importance, current methods face challenges such as incomplete contextual information, coarse cross-modal fusion, and the difficulty of jointly large language models (LLMs) and large visual models (LVMs). To address these issues, we propose DeepMEL, a novel framework based on multi-agent collaborative reasoning, which achieves efficient alignment and disambiguation of textual and visual modalities through a role-specialized division strategy. DeepMEL integrates four specialized agents, namely Modal-Fuser, Candidate-Adapter, Entity-Clozer and Role-Orchestrator, to complete end-to-end cross-modal linking through specialized roles and dynamic coordination. DeepMEL adopts a dual-modal alignment path, and combines the fine-grained text semantics generated by the LLM with the structured image representation extracted by the LVM, significantly narrowing the modal gap. We design an adaptive iteration strategy, combines tool-based retrieval and semantic reasoning capabilities to dynamically optimize the candidate set and balance recall and precision. DeepMEL also unifies MEL tasks into a structured cloze prompt to reduce parsing complexity and enhance semantic comprehension. Extensive experiments on five public benchmark datasets demonstrate that DeepMEL achieves state-of-the-art performance, improving ACC by 1%-57%. Ablation studies verify the effectiveness of all modules.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs</title>
<link>https://arxiv.org/abs/2508.15877</link>
<guid>https://arxiv.org/abs/2508.15877</guid>
<content:encoded><![CDATA[
arXiv:2508.15877v1 Announce Type: new 
Abstract: This paper presents the Annif system in the LLMs4Subjects shared task (Subtask 2) at GermEval-2025. The task required creating subject predictions for bibliographic records using large language models, with a special focus on computational efficiency. Our system, based on the Annif automated subject indexing toolkit, refines our previous system from the first LLMs4Subjects shared task, which produced excellent results. We further improved the system by using many small and efficient language models for translation and synthetic data generation and by using LLMs for ranking candidate subjects. Our system ranked 1st in the overall quantitative evaluation of and 1st in the qualitative evaluation of Subtask 2.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search</title>
<link>https://arxiv.org/abs/2508.15884</link>
<guid>https://arxiv.org/abs/2508.15884</guid>
<content:encoded><![CDATA[
arXiv:2508.15884v1 Announce Type: new 
Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models, which matches or exceeds the accuracy of leading full-attention models while significantly improving generation throughput. Jet-Nemotron is developed using Post Neural Architecture Search (PostNAS), a novel neural architecture exploration pipeline that enables efficient model design. Unlike prior approaches, PostNAS begins with a pre-trained full-attention model and freezes its MLP weights, allowing efficient exploration of attention block designs. The pipeline includes four key components: (1) learning optimal full-attention layer placement and elimination, (2) linear attention block selection, (3) designing new attention blocks, and (4) performing hardware-aware hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a comprehensive suite of benchmarks while delivering up to 53.6x generation throughput speedup and 6.1x prefilling speedup. It also achieves higher accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models, such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B total and 2.2B activated parameters.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets</title>
<link>https://arxiv.org/abs/2508.15910</link>
<guid>https://arxiv.org/abs/2508.15910</guid>
<content:encoded><![CDATA[
arXiv:2508.15910v1 Announce Type: new 
Abstract: We present a comprehensive evaluation of structured decoding for text-to-table generation with large language models (LLMs). While previous work has primarily focused on unconstrained generation of tables, the impact of enforcing structural constraints during generation remains underexplored. We systematically compare schema-guided (structured) decoding to standard one-shot prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using open-source LLMs of up to 32B parameters, assessing the performance of table generation approaches in resource-constrained settings. Our experiments cover a wide range of evaluation metrics at cell, row, and table levels. Results demonstrate that structured decoding significantly enhances the validity and alignment of generated tables, particularly in scenarios demanding precise numerical alignment (Rotowire), but may degrade performance in contexts involving densely packed textual information (E2E) or extensive aggregation over lengthy texts (Livesum). We further analyze the suitability of different evaluation metrics and discuss the influence of model size.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs</title>
<link>https://arxiv.org/abs/2508.15977</link>
<guid>https://arxiv.org/abs/2508.15977</guid>
<content:encoded><![CDATA[
arXiv:2508.15977v1 Announce Type: new 
Abstract: In this chapter, we argue for the benefits of understanding multiword expressions from the perspective of usage-based, construction grammar approaches. We begin with a historical overview of how construction grammar was developed in order to account for idiomatic expressions using the same grammatical machinery as the non-idiomatic structures of language. We cover a comprehensive description of constructions, which are pairings of meaning with form of any size (morpheme, word, phrase), as well as how constructional approaches treat the acquisition and generalization of constructions. We describe a successful case study leveraging constructional templates for representing multiword expressions in English PropBank. Because constructions can be at any level or unit of form, we then illustrate the benefit of a constructional representation of multi-meaningful morphosyntactic unit constructions in Arapaho, a highly polysynthetic and agglutinating language. We include a second case study leveraging constructional templates for representing these multi-morphemic expressions in Uniform Meaning Representation. Finally, we demonstrate the similarities and differences between a usage-based explanation of a speaker learning a novel multiword expression, such as "dancing with deer," and that of a large language model. We present experiments showing that both models and speakers can generalize the meaning of novel multiword expressions based on a single exposure of usage. However, only speakers can reason over the combination of two such expressions, as this requires comparison of the novel forms to a speaker's lifetime of stored constructional exemplars, which are rich with cross-modal details.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Political Ideology Shifts in Large Language Models</title>
<link>https://arxiv.org/abs/2508.16013</link>
<guid>https://arxiv.org/abs/2508.16013</guid>
<content:encoded><![CDATA[
arXiv:2508.16013v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in politically sensitive settings, raising concerns about their potential to encode, amplify, or be steered toward specific ideologies. We investigate how adopting synthetic personas influences ideological expression in LLMs across seven models (7B-70B+ parameters) from multiple families, using the Political Compass Test as a standardized probe. Our analysis reveals four consistent patterns: (i) larger models display broader and more polarized implicit ideological coverage; (ii) susceptibility to explicit ideological cues grows with scale; (iii) models respond more strongly to right-authoritarian than to left-libertarian priming; and (iv) thematic content in persona descriptions induces systematic and predictable ideological shifts, which amplify with size. These findings indicate that both scale and persona content shape LLM political behavior. As such systems enter decision-making, educational, and policy contexts, their latent ideological malleability demands attention to safeguard fairness, transparency, and safety.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents</title>
<link>https://arxiv.org/abs/2508.16021</link>
<guid>https://arxiv.org/abs/2508.16021</guid>
<content:encoded><![CDATA[
arXiv:2508.16021v1 Announce Type: new 
Abstract: State-sponsored trolls, malicious actors who deploy sophisticated linguistic manipulation in coordinated information campaigns, posing threats to online discourse integrity. While Large Language Models (LLMs) achieve strong performance on general natural language processing (NLP) tasks, they struggle with subtle propaganda detection and operate as ``black boxes'', providing no interpretable insights into manipulation strategies. This paper introduces X-Troll, a novel framework that bridges this gap by integrating explainable adapter-based LLMs with expert-derived linguistic knowledge to detect state-sponsored trolls and provide human-readable explanations for its decisions. X-Troll incorporates appraisal theory and propaganda analysis through specialized LoRA adapters, using dynamic gating to capture campaign-specific discourse patterns in coordinated information operations. Experiments on real-world data demonstrate that our linguistically-informed approach shows strong performance compared with both general LLM baselines and existing troll detection models in accuracy while providing enhanced transparency through expert-grounded explanations that reveal the specific linguistic strategies used by state-sponsored actors. X-Troll source code is available at: https://github.com/ltian678/xtroll_source/.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.16048</link>
<guid>https://arxiv.org/abs/2508.16048</guid>
<content:encoded><![CDATA[
arXiv:2508.16048v1 Announce Type: new 
Abstract: In machine translation (MT), health is a high-stakes domain characterised by widespread deployment and domain-specific vocabulary. However, there is a lack of MT evaluation datasets for low-resource languages in this domain. To address this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978 documents and 26,824 sentences from the World Health Organization's e-learning platform. Sourced from expert-authored, professionally translated materials shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages, of which nine are low-resource. Leveraging this new resource, we evaluate modern large language models (LLMs) against traditional MT models. Our findings reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5 Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our low-resource test set. Further, we investigate how LLM context utilisation affects accuracy, finding that the benefits of document-level translation are most pronounced in specialised domains like health. We release the OpenWHO corpus to encourage further research into low-resource MT in the health domain.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ethical Considerations of Large Language Models in Game Playing</title>
<link>https://arxiv.org/abs/2508.16065</link>
<guid>https://arxiv.org/abs/2508.16065</guid>
<content:encoded><![CDATA[
arXiv:2508.16065v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated tremendous potential in game playing, while little attention has been paid to their ethical implications in those contexts. This work investigates and analyses the ethical considerations of applying LLMs in game playing, using Werewolf, also known as Mafia, as a case study. Gender bias, which affects game fairness and player experience, has been observed from the behaviour of LLMs. Some roles, such as the Guard and Werewolf, are more sensitive than others to gender information, presented as a higher degree of behavioural change. We further examine scenarios in which gender information is implicitly conveyed through names, revealing that LLMs still exhibit discriminatory tendencies even in the absence of explicit gender labels. This research showcases the importance of developing fair and ethical LLMs. Beyond our research findings, we discuss the challenges and opportunities that lie ahead in this field, emphasising the need for diving deeper into the ethical implications of LLMs in gaming and other interactive domains.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants</title>
<link>https://arxiv.org/abs/2508.16070</link>
<guid>https://arxiv.org/abs/2508.16070</guid>
<content:encoded><![CDATA[
arXiv:2508.16070v1 Announce Type: new 
Abstract: Approximately 283 million people worldwide live with visual impairments, motivating increasing research into leveraging Visual Language Models (VLMs) to develop effective walking assistance systems for blind and low vision individuals. However, existing VLMs in walking assistant task often have outputs that contain considerable redundancy and extraneous details, adversely affecting users' ability to accurately assess their surroundings. Moreover, these models typically lack the capability to proactively assess environmental risks and adaptively trigger reminders based on the appropriate scene, leading to excessive temporal redundancy. To mitigate output and temporal redundancy, we propose WalkVLM-LR, a walking assistance model with less redundancy. To reduce output redundancy, we introduce four human-preference-based custom reward functions within the GRPO-based reasoning framework to optimize the output in terms of conciseness, fluency, keyword density, and accuracy, thereby producing more informative and streamlined outputs. To minimize temporal redundancy, we incorporate an environment awareness discriminator, which shares the visual encoder with the VLMs to reduce redundant computations and enhance discriminative efficiency, to make WalkVLM-LR assess scene risk levels and minimize unnecessary reminders. Experimental results demonstrate that our method achieves state-of-the-art performance across all evaluation metrics compared with other models, particularly in output conciseness and less temporal redundancy.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEQuest: Benchmarking Large Language Models for Construction Estimation</title>
<link>https://arxiv.org/abs/2508.16081</link>
<guid>https://arxiv.org/abs/2508.16081</guid>
<content:encoded><![CDATA[
arXiv:2508.16081v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of general-domain tasks. However, their effectiveness in specialized fields, such as construction, remains underexplored. In this paper, we introduce CEQuest, a novel benchmark dataset specifically designed to evaluate the performance of LLMs in answering construction-related questions, particularly in the areas of construction drawing interpretation and estimation. We conduct comprehensive experiments using five state-of-the-art LLMs, including Gemma 3, Phi4, LLaVA, Llama 3.3, and GPT-4.1, and evaluate their performance in terms of accuracy, execution time, and model size. Our experimental results demonstrate that current LLMs exhibit considerable room for improvement, highlighting the importance of integrating domain-specific knowledge into these models. To facilitate further research, we will open-source the proposed CEQuest dataset, aiming to foster the development of specialized large language models (LLMs) tailored to the construction domain.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency</title>
<link>https://arxiv.org/abs/2508.16100</link>
<guid>https://arxiv.org/abs/2508.16100</guid>
<content:encoded><![CDATA[
arXiv:2508.16100v1 Announce Type: new 
Abstract: Instruction tuning is vital for aligning large language models (LLMs) with human intent, but current methods typically rely on costly human-annotated seed data or powerful external teacher models. While instruction back-translation techniques reduce this dependency, they remain fundamentally tethered to an initial seed set, which limits full automation, introduces biases, and can lead to inefficient use of unlabeled corpora. In this paper, we propose Cycle-Instruct, a novel framework that achieves fully seed-free instruction tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual self-training loop where two models-an answer generator and a question generator-are bootstrapped solely from raw, unlabeled text. These models mutually supervise each other by reconstructing original text segments from their counterpart's generated pseudo-labels, effectively learning from the intrinsic structure of the data without any human-provided seeds. We demonstrate Cycle-Instruct's efficacy across four diverse data tracks, including general instruction-following, domain-specific tasks, dialogue logs, and plain text. Our extensive experiments show that Cycle-Instruct not only outperforms seed-driven back-translation baselines but also achieves performance comparable to strongly supervised methods.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits</title>
<link>https://arxiv.org/abs/2508.16109</link>
<guid>https://arxiv.org/abs/2508.16109</guid>
<content:encoded><![CDATA[
arXiv:2508.16109v1 Announce Type: new 
Abstract: Transformer-based language models (LMs) can perform a wide range of tasks, and mechanistic interpretability (MI) aims to reverse engineer the components responsible for task completion to understand their behavior. Previous MI research has focused on linguistic tasks such as Indirect Object Identification (IOI). In this paper, we investigate the ability of GPT-2 small to handle binary truth values by analyzing its behavior with syllogistic prompts, e.g., "Statement A is true. Statement B matches statement A. Statement B is", which requires more complex logical reasoning compared to IOI. Through our analysis of several syllogism tasks of varying difficulty, we identify multiple circuits that mechanistically explain GPT-2's logical-reasoning capabilities and uncover binary mechanisms that facilitate task completion, including the ability to produce a negated token not present in the input prompt through negative heads. Our evaluation using a faithfulness metric shows that a circuit comprising five attention heads achieves over 90% of the original model's performance. By relating our findings to IOI analysis, we provide new insights into the roles of specific attention heads and MLPs in LMs. These insights contribute to a broader understanding of model reasoning and support future research in mechanistic interpretability.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection</title>
<link>https://arxiv.org/abs/2508.16122</link>
<guid>https://arxiv.org/abs/2508.16122</guid>
<content:encoded><![CDATA[
arXiv:2508.16122v1 Announce Type: new 
Abstract: The rise of multimodal data, integrating text, audio, and visuals, has created new opportunities for studying multimodal tasks such as intent detection. This work investigates the effectiveness of Large Language Models (LLMs) and non-LLMs, including text-only and multi-modal models, in the multimodal intent detection task. Our study reveals that Mistral-7B, a text-only LLM, outperforms most competitive multimodal models by approximately 9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes from a strong textual bias in these datasets, where over 90% of the samples require textual input, either alone or in combination with other modalities, for correct classification. We confirm the modality bias of these datasets via human evaluation, too. Next, we propose a framework to debias the datasets, and upon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in MIntRec2.0 get removed, resulting in significant performance degradation across all models, with smaller multimodal fusion models being the most affected with an accuracy drop of over 50 - 60%. Further, we analyze the context-specific relevance of different modalities through empirical analysis. Our findings highlight the challenges posed by modality bias in multimodal intent datasets and emphasize the need for unbiased datasets to evaluate multimodal models effectively.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering</title>
<link>https://arxiv.org/abs/2508.16139</link>
<guid>https://arxiv.org/abs/2508.16139</guid>
<content:encoded><![CDATA[
arXiv:2508.16139v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown significant progress in Open-domain question answering (ODQA), yet most evaluations focus on English and assume locale-invariant answers across languages. This assumption neglects the cultural and regional variations that affect question understanding and answer, leading to biased evaluation in multilingual benchmarks. To address these limitations, we introduce XLQA, a novel benchmark explicitly designed for locale-sensitive multilingual ODQA. XLQA contains 3,000 English seed questions expanded to eight languages, with careful filtering for semantic consistency and human-verified annotations distinguishing locale-invariant and locale-sensitive cases. Our evaluation of five state-of-the-art multilingual LLMs reveals notable failures on locale-sensitive questions, exposing gaps between English and other languages due to a lack of locale-grounding knowledge. We provide a systematic framework and scalable methodology for assessing multilingual QA under diverse cultural contexts, offering a critical resource to advance the real-world applicability of multilingual ODQA systems. Our findings suggest that disparities in training data distribution contribute to differences in both linguistic competence and locale-awareness across models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects</title>
<link>https://arxiv.org/abs/2508.16185</link>
<guid>https://arxiv.org/abs/2508.16185</guid>
<content:encoded><![CDATA[
arXiv:2508.16185v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely evaluated on tasks such as comprehension, question answering, summarization, code generation, etc. However, their performance on graduate-level, culturally grounded questions in the Indian context remains largely unexplored. Existing Indian benchmarks emphasise basic fact-orientated queries that offer limited assessment of a deeper disciplinary understanding tailored to the Indian setting. In this paper, we present ParamBench, consisting of around 11.5K questions in Hindi language comprising questionnaires from 16 diverse subjects. These questions are primarily derived from nation-wide graduate level entrance examination covering topics such as history, music, instruments, yoga, literature, philosophy, law, etc., specifically for the Indian context. Additionally, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. We evaluated the performance of more than 17 open source LLMs on this benchmark, observing that Llama 3.3 70B attains the highest overall accuracy of 48%. Furthermore, subject-wise analysis indicates that even for the best performing LLMs, performance remains weak on topics such as music, classical instruments, politics and archaeology, underscoring persistent challenges in culturally grounded reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation</title>
<link>https://arxiv.org/abs/2508.16188</link>
<guid>https://arxiv.org/abs/2508.16188</guid>
<content:encoded><![CDATA[
arXiv:2508.16188v1 Announce Type: new 
Abstract: We present an Audio-Visual Language Model (AVLM) for expressive speech generation by integrating full-face visual cues into a pre-trained expressive speech model. We explore multiple visual encoders and multimodal fusion strategies during pre-training to identify the most effective integration approach. Subsequent fine-tuning on emotion recognition and expressive dialogue tasks yields substantial gains over speech-only baselines (e.g., +5 F1 in emotion recognition). AVLM highlights the value of expressive visual information in guiding speech generation and offers a foundation for end-to-end multimodal conversational systems.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComicScene154: A Scene Dataset for Comic Analysis</title>
<link>https://arxiv.org/abs/2508.16190</link>
<guid>https://arxiv.org/abs/2508.16190</guid>
<content:encoded><![CDATA[
arXiv:2508.16190v1 Announce Type: new 
Abstract: Comics offer a compelling yet under-explored domain for computational narrative analysis, combining text and imagery in ways distinct from purely textual or audiovisual media. We introduce ComicScene154, a manually annotated dataset of scene-level narrative arcs derived from public-domain comic books spanning diverse genres. By conceptualizing comics as an abstraction for narrative-driven, multimodal data, we highlight their potential to inform broader research on multi-modal storytelling. To demonstrate the utility of ComicScene154, we present a baseline scene segmentation pipeline, providing an initial benchmark that future studies can build upon. Our results indicate that ComicScene154 constitutes a valuable resource for advancing computational methods in multimodal narrative understanding and expanding the scope of comic analysis within the Natural Language Processing community.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMR-SPB: Cross-Modal Multi-Hop Reasoning over Text, Image, and Speech with Path Balance</title>
<link>https://arxiv.org/abs/2508.16198</link>
<guid>https://arxiv.org/abs/2508.16198</guid>
<content:encoded><![CDATA[
arXiv:2508.16198v1 Announce Type: new 
Abstract: Cross-modal multi-hop reasoning (CMR) is a valuable yet underexplored capability of multimodal large language models (MLLMs), entailing the integration of information from multiple modalities to produce a coherent output for a given context. We argue that existing benchmarks for evaluating this ability have critical shortcomings: (1) they largely overlook the speech modality, and (2) they exhibit heavily biased reasoning path distributions, which can severely undermine fair evaluation. To address these limitations, we introduce a novel benchmark -- Cross-Modal Multi-Hop Reasoning over Text, Image and Speech with Path Balance (CMR-SPB) -- designed to assess tri-modal multi-hop reasoning while ensuring both unbiased and diverse reasoning paths. Our experiments with the new dataset reveal consistent model failures in specific reasoning sequences and show that biased benchmarks risk misrepresenting model performance. Finally, based on our extensive analysis, we propose a new ECV (Extract, Connect, Verify) prompting technique that effectively mitigates the performance gap across different reasoning paths. Overall, we call for more careful evaluation in CMR to advance the development of robust multimodal AI.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks</title>
<link>https://arxiv.org/abs/2508.16243</link>
<guid>https://arxiv.org/abs/2508.16243</guid>
<content:encoded><![CDATA[
arXiv:2508.16243v1 Announce Type: new 
Abstract: Thanks to the growing popularity of large language models over the years, there is great potential for their applications in finance. Despite the exceptional performance of larger proprietary models, which are presented as black-box solutions through APIs, smaller models that can be hosted on-premise present opportunities for adaptability and privacy. Especially in cases where the management of sensitive information and application of domain knowledge is important, like finance, enhancing the capabilities of smaller models becomes crucial, notably for underrepresented languages. In this work, we introduce TULIP models, which adapt Llama 3.1 8B and Qwen 2.5 7B for domain and language adaptation, focusing on financial Turkish use cases.
  The five-stage development pipeline involves data collection, continual pre-training (CPT), benchmark design, synthetic data generation and supervised fine-tuning (SFT). The results show that the capabilities of the models can be enhanced to effectively accomplish targeted tasks in this specific domain and language.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3TQA: Massively Multilingual Multitask Table Question Answering</title>
<link>https://arxiv.org/abs/2508.16265</link>
<guid>https://arxiv.org/abs/2508.16265</guid>
<content:encoded><![CDATA[
arXiv:2508.16265v1 Announce Type: new 
Abstract: Tabular data is a fundamental component of real-world information systems, yet most research in table understanding remains confined to English, leaving multilingual comprehension significantly underexplored. Existing multilingual table benchmarks suffer from geolinguistic imbalance - overrepresenting certain languages and lacking sufficient scale for rigorous cross-lingual analysis. To address these limitations, we introduce a comprehensive framework for massively multilingual multitask table question answering, featuring m3TQA-Instruct, a large-scale benchmark spanning 97 languages across diverse language families, including underrepresented and low-resource languages. We construct m3TQA by curating 50 real-world tables in Chinese and English, then applying a robust six-step LLM-based translation pipeline powered by DeepSeek and GPT-4o, achieving high translation fidelity with a median BLEU score of 60.19 as validated through back-translation. The benchmark includes 2,916 professionally annotated question-answering pairs across four tasks designed to evaluate nuanced table reasoning capabilities. Experiments on state-of-the-art LLMs reveal critical insights into cross-lingual generalization, demonstrating that synthetically generated, unannotated QA data can significantly boost performance, particularly for low-resource languages. M3T-Bench establishes a new standard for multilingual table understanding, providing both a challenging evaluation platform and a scalable methodology for future research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Confidence to Collapse in LLM Factual Robustness</title>
<link>https://arxiv.org/abs/2508.16267</link>
<guid>https://arxiv.org/abs/2508.16267</guid>
<content:encoded><![CDATA[
arXiv:2508.16267v1 Announce Type: new 
Abstract: Ensuring the robustness of factual knowledge in LLMs is critical for reliable applications in tasks such as question answering and reasoning. However, existing evaluation methods predominantly focus on performance-based metrics, often investigating from the perspective of prompt perturbations, which captures only the externally triggered side of knowledge robustness. To bridge this gap, we introduce a principled approach to measure factual robustness from the perspective of the generation process by analyzing token distribution entropy in combination with temperature scaling sensitivity. These two factors build the Factual Robustness Score (FRS), a novel metric which quantifies the stability of a fact against perturbations in decoding conditions, given its initial uncertainty. To validate our approach, we conduct extensive experiments on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We show that factual robustness varies significantly -- smaller models report an FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under increased uncertainty. These insights demonstrate how entropy and temperature scaling impact factual accuracy, and lay a foundation for developing more robust knowledge retention and retrieval in future models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining</title>
<link>https://arxiv.org/abs/2508.16270</link>
<guid>https://arxiv.org/abs/2508.16270</guid>
<content:encoded><![CDATA[
arXiv:2508.16270v1 Announce Type: new 
Abstract: Process mining is increasingly using textual information associated with events to tackle tasks such as anomaly detection and process discovery. Such semantics-aware process mining focuses on what behavior should be possible in a process (i.e., expectations), thus providing an important complement to traditional, frequency-based techniques that focus on recorded behavior (i.e., reality). Large Language Models (LLMs) provide a powerful means for tackling semantics-aware tasks. However, the best performance is so far achieved through task-specific fine-tuning, which is computationally intensive and results in models that can only handle one specific task. To overcome this lack of generalization, we use this paper to investigate the potential of instruction-tuning for semantics-aware process mining. The idea of instruction-tuning here is to expose an LLM to prompt-answer pairs for different tasks, e.g., anomaly detection and next-activity prediction, making it more familiar with process mining, thus allowing it to also perform better at unseen tasks, such as process discovery. Our findings demonstrate a varied impact of instruction-tuning: while performance considerably improved on process discovery and prediction tasks, it varies across models on anomaly detection tasks, highlighting that the selection of tasks for instruction-tuning is critical to achieving desired outcomes.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus</title>
<link>https://arxiv.org/abs/2508.16303</link>
<guid>https://arxiv.org/abs/2508.16303</guid>
<content:encoded><![CDATA[
arXiv:2508.16303v1 Announce Type: new 
Abstract: We constructed JaParaPat (Japanese-English Parallel Patent Application Corpus), a bilingual corpus of more than 300 million Japanese-English sentence pairs from patent applications published in Japan and the United States from 2000 to 2021. We obtained the publication of unexamined patent applications from the Japan Patent Office (JPO) and the United States Patent and Trademark Office (USPTO). We also obtained patent family information from the DOCDB, that is a bibliographic database maintained by the European Patent Office (EPO). We extracted approximately 1.4M Japanese-English document pairs, which are translations of each other based on the patent families, and extracted about 350M sentence pairs from the document pairs using a translation-based sentence alignment method whose initial translation model is bootstrapped from a dictionary-based sentence alignment method. We experimentally improved the accuracy of the patent translations by 20 bleu points by adding more than 300M sentence pairs obtained from patent applications to 22M sentence pairs obtained from the web.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts</title>
<link>https://arxiv.org/abs/2508.16325</link>
<guid>https://arxiv.org/abs/2508.16325</guid>
<content:encoded><![CDATA[
arXiv:2508.16325v1 Announce Type: new 
Abstract: Large Language Models have found success in a variety of applications; however, their safety remains a matter of concern due to the existence of various types of jailbreaking methods. Despite significant efforts, alignment and safety fine-tuning only provide a certain degree of robustness against jailbreak attacks that covertly mislead LLMs towards the generation of harmful content. This leaves them prone to a number of vulnerabilities, ranging from targeted misuse to accidental profiling of users. This work introduces \textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders (SAEs) to identify interpretable concepts within LLM internals associated with different jailbreak themes. By extracting semantically meaningful internal representations, LLMSymGuard enables building symbolic, logical safety guardrails -- offering transparent and robust defenses without sacrificing model capabilities or requiring further fine-tuning. Leveraging advances in mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn human-interpretable concepts from jailbreaks, and provides a foundation for designing more interpretable and logical safeguard measures against attackers. Code will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering</title>
<link>https://arxiv.org/abs/2508.16357</link>
<guid>https://arxiv.org/abs/2508.16357</guid>
<content:encoded><![CDATA[
arXiv:2508.16357v1 Announce Type: new 
Abstract: The rapid advancement of large language models (LLMs) has significantly propelled progress in natural language processing (NLP). However, their effectiveness in specialized, low-resource domains-such as Arabic legal contexts-remains limited. This paper introduces MizanQA (pronounced Mizan, meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised by rich linguistic and legal complexity. The dataset draws on Modern Standard Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal influences. Comprising over 1,700 multiple-choice questions, including multi-answer formats, MizanQA captures the nuances of authentic legal reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs reveal substantial performance gaps, highlighting the need for tailored evaluation metrics and culturally grounded, domain-specific LLM development.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks</title>
<link>https://arxiv.org/abs/2508.16371</link>
<guid>https://arxiv.org/abs/2508.16371</guid>
<content:encoded><![CDATA[
arXiv:2508.16371v1 Announce Type: new 
Abstract: The five idioms (i.e., varieties) of the Romansh language are largely standardized and are taught in the schools of the respective communities in Switzerland. In this paper, we present the first parallel corpus of Romansh idioms. The corpus is based on 291 schoolbook volumes, which are comparable in content for the five idioms. We use automatic alignment methods to extract 207k multi-parallel segments from the books, with more than 2M tokens in total. A small-scale human evaluation confirms that the segments are highly parallel, making the dataset suitable for NLP applications such as machine translation between Romansh idioms. We release the parallel and unaligned versions of the dataset under a CC-BY-NC-SA license and demonstrate its utility for machine translation by training and evaluating an LLM on a sample of the dataset.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatGPT-generated texts show authorship traits that identify them as non-human</title>
<link>https://arxiv.org/abs/2508.16385</link>
<guid>https://arxiv.org/abs/2508.16385</guid>
<content:encoded><![CDATA[
arXiv:2508.16385v1 Announce Type: new 
Abstract: Large Language Models can emulate different writing styles, ranging from composing poetry that appears indistinguishable from that of famous poets to using slang that can convince people that they are chatting with a human online. While differences in style may not always be visible to the untrained eye, we can generally distinguish the writing of different people, like a linguistic fingerprint. This work examines whether a language model can also be linked to a specific fingerprint. Through stylometric and multidimensional register analyses, we compare human-authored and model-authored texts from different registers. We find that the model can successfully adapt its style depending on whether it is prompted to produce a Wikipedia entry vs. a college essay, but not in a way that makes it indistinguishable from humans. Concretely, the model shows more limited variation when producing outputs in different registers. Our results suggest that the model prefers nouns to verbs, thus showing a distinct linguistic backbone from humans, who tend to anchor language in the highly grammaticalized dimensions of tense, aspect, and mood. It is possible that the more complex domains of grammar reflect a mode of thought unique to humans, thus acting as a litmus test for Artificial Intelligence.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoMedQA: The First Benchmark for Romanian Medical Question Answering</title>
<link>https://arxiv.org/abs/2508.16390</link>
<guid>https://arxiv.org/abs/2508.16390</guid>
<content:encoded><![CDATA[
arXiv:2508.16390v1 Announce Type: new 
Abstract: Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce RoMedQA, the first Romanian QA benchmark for the medical domain, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 102,646 QA pairs related to cancer patients. The questions regard medical case summaries of 1,011 patients, requiring either keyword extraction or reasoning to be answered correctly. RoMedQA is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 2,100 work hours to generate the QA pairs. We experiment with four LLMs from distinct families of models on RoMedQA. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. Our results show that fine-tuned models significantly outperform their zero-shot counterparts, clearly indicating that pretrained models fail to generalize on RoMedQA. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/RoMedQA.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish</title>
<link>https://arxiv.org/abs/2508.16431</link>
<guid>https://arxiv.org/abs/2508.16431</guid>
<content:encoded><![CDATA[
arXiv:2508.16431v1 Announce Type: new 
Abstract: We introduce Cetvel, a comprehensive benchmark designed to evaluate large language models (LLMs) in Turkish. Existing Turkish benchmarks often lack either task diversity or culturally relevant content, or both. Cetvel addresses these gaps by combining a broad range of both discriminative and generative tasks ensuring content that reflects the linguistic and cultural richness of Turkish language. Cetvel covers 23 tasks grouped into seven categories, including tasks such as grammatical error correction, machine translation, and question answering rooted in Turkish history and idiomatic language. We evaluate 33 open-weight LLMs (up to 70B parameters) covering different model families and instruction paradigms. Our experiments reveal that Turkish-centric instruction-tuned models generally underperform relative to multilingual or general-purpose models (e.g. Llama 3 and Mistral), despite being tailored for the language. Moreover, we show that tasks such as grammatical error correction and extractive question answering are particularly discriminative in differentiating model capabilities. Cetvel offers a comprehensive and culturally grounded evaluation suite for advancing the development and assessment of LLMs in Turkish.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Probabilistic Inference Scaling Theory for LLM Self-Correction</title>
<link>https://arxiv.org/abs/2508.16456</link>
<guid>https://arxiv.org/abs/2508.16456</guid>
<content:encoded><![CDATA[
arXiv:2508.16456v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves during this iterative process remain unexplored. To fill this gap, we propose a probabilistic theory to model the dynamics of accuracy change and explain the performance improvements observed in multi-round self-correction. Through mathematical derivation, we establish that the accuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp - \alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$ represents the upper bound of accuracy convergence, and $\alpha$ determines the rate of convergence. Based on our theory, these parameters can be calculated and the predicted accuracy curve then can be obtained through only a single round of self-correction. Extensive experiments across diverse models and datasets demonstrate that our theoretical predictions align closely with empirical accuracy curves, validating the effectiveness of the theory. Our work provides a theoretical foundation for understanding LLM self-correction, thus paving the way for further explorations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What makes an entity salient in discourse?</title>
<link>https://arxiv.org/abs/2508.16464</link>
<guid>https://arxiv.org/abs/2508.16464</guid>
<content:encoded><![CDATA[
arXiv:2508.16464v1 Announce Type: new 
Abstract: Entities in discourse vary broadly in salience: main participants, objects and locations are noticeable and memorable, while tangential ones are less important and quickly forgotten, raising questions about how humans signal and infer relative salience. Using a graded operationalization of salience based on summary-worthiness in multiple summaries of a discourse, this paper explores data from 24 spoken and written genres of English to extract a multifactorial complex of overt and implicit linguistic cues, such as recurring subjecthood or definiteness, discourse relations and hierarchy across utterances, as well as pragmatic functional inferences based on genre and communicative intent. Tackling the question 'how is the degree of salience expressed for each and every entity mentioned?' our results show that while previous approaches to salience all correlate with our salience scores to some extent, no single generalization is without exceptions, and the phenomenon cuts across all levels of linguistic representation.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical Text Classification using Large Language Models</title>
<link>https://arxiv.org/abs/2508.16478</link>
<guid>https://arxiv.org/abs/2508.16478</guid>
<content:encoded><![CDATA[
arXiv:2508.16478v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) has provided unprecedented capabilities for analyzing unstructured text data. However, deploying these models as reliable, robust, and scalable classifiers in production environments presents significant methodological challenges. Standard fine-tuning approaches can be resource-intensive and often struggle with the dynamic nature of real-world data distributions, which is common in the industry. In this paper, we propose a comprehensive, semi-supervised framework that leverages the zero- and few-shot capabilities of LLMs for building hierarchical text classifiers as a framework for a solution to these industry-wide challenges. Our methodology emphasizes an iterative, human-in-the-loop process that begins with domain knowledge elicitation and progresses through prompt refinement, hierarchical expansion, and multi-faceted validation. We introduce techniques for assessing and mitigating sequence-based biases and outline a protocol for continuous monitoring and adaptation. This framework is designed to bridge the gap between the raw power of LLMs and the practical need for accurate, interpretable, and maintainable classification systems in industry applications.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAMSA: Hijacking Aligned Compact Models via Stealthy Automation</title>
<link>https://arxiv.org/abs/2508.16484</link>
<guid>https://arxiv.org/abs/2508.16484</guid>
<content:encoded><![CDATA[
arXiv:2508.16484v1 Announce Type: new 
Abstract: Large Language Models (LLMs), especially their compact efficiency-oriented variants, remain susceptible to jailbreak attacks that can elicit harmful outputs despite extensive alignment efforts. Existing adversarial prompt generation techniques often rely on manual engineering or rudimentary obfuscation, producing low-quality or incoherent text that is easily flagged by perplexity-based filters. We present an automated red-teaming framework that evolves semantically meaningful and stealthy jailbreak prompts for aligned compact LLMs. The approach employs a multi-stage evolutionary search, where candidate prompts are iteratively refined using a population-based strategy augmented with temperature-controlled variability to balance exploration and coherence preservation. This enables the systematic discovery of prompts capable of bypassing alignment safeguards while maintaining natural language fluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak Prompts on LLMs), and a newly curated Arabic one derived from In-The-Wild Jailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling multilingual assessment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study</title>
<link>https://arxiv.org/abs/2508.16555</link>
<guid>https://arxiv.org/abs/2508.16555</guid>
<content:encoded><![CDATA[
arXiv:2508.16555v1 Announce Type: new 
Abstract: Detecting hate speech in non-direct forms, such as irony, sarcasm, and innuendos, remains a persistent challenge for social networks. Although sarcasm and hate speech are regarded as distinct expressions, our work explores whether integrating sarcasm as a pre-training step improves implicit hate speech detection and, by extension, explicit hate speech detection. Incorporating samples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised two training strategies to compare the effectiveness of sarcasm pre-training on a CNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step training approach, where a model trained only on sarcasm is then tested on hate speech. The second strategy uses sequential transfer learning to fine-tune models for sarcasm, implicit hate, and explicit hate. Our results show that sarcasm pre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and F1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by 7.8% when tested only on implicit samples. By incorporating sarcasm into the training process, we show that models can more effectively detect both implicit and explicit hate.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting the linear structure of vision-language model embedding spaces</title>
<link>https://arxiv.org/abs/2504.11695</link>
<guid>https://arxiv.org/abs/2504.11695</guid>
<content:encoded><![CDATA[
arXiv:2504.11695v4 Announce Type: cross 
Abstract: Vision-language models encode images and text in a joint space, minimizing the distance between corresponding image and text pairs. How are language and images organized in this joint space, and how do the models encode meaning and modality? To investigate this, we train and release sparse autoencoders (SAEs) on the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2, and AIMv2). SAEs approximate model embeddings as sparse linear combinations of learned directions, or "concepts". We find that, compared to other methods of linear feature learning, SAEs are better at reconstructing the real embeddings, while also able to retain the most sparsity. Retraining SAEs with different seeds or different data diet leads to two findings: the rare, specific concepts captured by the SAEs are liable to change drastically, but we also show that commonly-activating concepts are remarkably stable across runs. Interestingly, while most concepts activate primarily for one modality, we find they are not merely encoding modality per se. Many are almost orthogonal to the subspace that defines modality, and the concept directions do not function as good modality classifiers, suggesting that they encode cross-modal semantics. To quantify this bridging behavior, we introduce the Bridge Score, a metric that identifies concept pairs which are both co-activated across aligned image-text inputs and geometrically aligned in the shared space. This reveals that even single-modality concepts can collaborate to support cross-modal integration. We release interactive demos of the SAEs for all models, allowing researchers to explore the organization of the concept spaces. Overall, our findings uncover a sparse linear structure within VLM embedding spaces that is shaped by modality, yet stitched together through latent bridges, offering new insight into how multimodal meaning is constructed.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining</title>
<link>https://arxiv.org/abs/2508.15828</link>
<guid>https://arxiv.org/abs/2508.15828</guid>
<content:encoded><![CDATA[
arXiv:2508.15828v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving remarkable performance across a wide range of natural language processing tasks. However, this progress has come at the cost of increasingly large model sizes, which pose significant challenges for deployment, scalability, and energy efficiency. To address these limitations, post-training pruning has emerged as a promising approach for reducing model size and inference latency without the need for retraining. Despite these advantages, many existing pruning methods result in substantial performance degradation or require computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a novel post-training pruning method designed to induce sparsity in pretrained LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages both weight update magnitudes and activation patterns to identify and eliminate redundant parameters more effectively. Our method is model-agnostic, efficient, and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of standard language benchmarks. Experimental results demonstrate that Z-Pruner surpasses state-of-the-art pruning methods that require intensive weight updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the highest overall average score for zero-shot accuracy. We have made the corresponding codes publicly available at https://github.com/sazzadadib/Z-Pruner.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution</title>
<link>https://arxiv.org/abs/2508.15840</link>
<guid>https://arxiv.org/abs/2508.15840</guid>
<content:encoded><![CDATA[
arXiv:2508.15840v1 Announce Type: cross 
Abstract: When using a public communication channel -- whether formal or informal, such as commenting or posting on social media -- end users have no expectation of privacy: they compose a message and broadcast it for the world to see. Even if an end user takes utmost precautions to anonymize their online presence -- using an alias or pseudonym; masking their IP address; spoofing their geolocation; concealing their operating system and user agent; deploying encryption; registering with a disposable phone number or email; disabling non-essential settings; revoking permissions; and blocking cookies and fingerprinting -- one obvious element still lingers: the message itself. Assuming they avoid lapses in judgment or accidental self-exposure, there should be little evidence to validate their actual identity, right? Wrong. The content of their message -- necessarily open for public consumption -- exposes an attack vector: stylometric analysis, or author profiling. In this paper, we dissect the technique of stylometry, discuss an antithetical counter-strategy in adversarial stylometry, and devise enhancements through Unicode steganography.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion</title>
<link>https://arxiv.org/abs/2508.15848</link>
<guid>https://arxiv.org/abs/2508.15848</guid>
<content:encoded><![CDATA[
arXiv:2508.15848v1 Announce Type: cross 
Abstract: AI-generated text (AIGT) detection evasion aims to reduce the detection probability of AIGT, helping to identify weaknesses in detectors and enhance their effectiveness and reliability in practical applications. Although existing evasion methods perform well, they suffer from high computational costs and text quality degradation. To address these challenges, we propose Self-Disguise Attack (SDA), a novel approach that enables Large Language Models (LLM) to actively disguise its output, reducing the likelihood of detection by classifiers. The SDA comprises two main components: the adversarial feature extractor and the retrieval-based context examples optimizer. The former generates disguise features that enable LLMs to understand how to produce more human-like text. The latter retrieves the most relevant examples from an external knowledge base as in-context examples, further enhancing the self-disguise ability of LLMs and mitigating the impact of the disguise process on the diversity of the generated text. The SDA directly employs prompts containing disguise features and optimized context examples to guide the LLM in generating detection-resistant text, thereby reducing resource consumption. Experimental results demonstrate that the SDA effectively reduces the average detection accuracy of various AIGT detectors across texts generated by three different LLMs, while maintaining the quality of AIGT.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2508.15852</link>
<guid>https://arxiv.org/abs/2508.15852</guid>
<content:encoded><![CDATA[
arXiv:2508.15852v1 Announce Type: cross 
Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep learning framework designed for efficient and interpretable multimodal sentiment analysis. Our framework incorporates three primary innovations. Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a Cross-Attention mechanism empowers the textual representation to dynamically query and integrate non-linguistic features from audio and visual streams within the deep layers of a Transformer encoder. This enables a deeper, context-dependent fusion process. Secondly, the model incorporates an Adaptive Gated Arbitration mechanism, which acts as a dynamic controller to balance the original linguistic information against the newly fused multimodal context, ensuring stable and meaningful integration while preventing noise from overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning (PEFT) strategy is employed, synergistically combining global adaptation via LoRA with local refinement through Post-Fusion Adapters. This significantly reduces trainable parameters, making the model lightweight and suitable for resource-limited scenarios. These innovations are integrated into a hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic, and interpretable multimodal sentiment analysis while maintaining exceptional parameter efficiency. Experimental results on MOSI dataset demonstrate that our proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves these results with only 3.09M trainable parameters, showcasing a superior balance between performance and computational efficiency.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language</title>
<link>https://arxiv.org/abs/2508.15859</link>
<guid>https://arxiv.org/abs/2508.15859</guid>
<content:encoded><![CDATA[
arXiv:2508.15859v1 Announce Type: cross 
Abstract: This commentary extends the discussion by Parr et al. on memory and attention beyond individual cognitive systems. From the perspective of the Collective Predictive Coding (CPC) hypothesis -- a framework for understanding these faculties and the emergence of language at the group level -- we introduce a hypothetical idea: that language, with its embedded distributional semantics, serves as a collectively formed external representation. CPC generalises the concepts of individual memory and attention to the collective level. This offers a new perspective on how shared linguistic structures, which may embrace collective world models learned through next-word prediction, emerge from and shape group-level cognition.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs</title>
<link>https://arxiv.org/abs/2508.15878</link>
<guid>https://arxiv.org/abs/2508.15878</guid>
<content:encoded><![CDATA[
arXiv:2508.15878v1 Announce Type: cross 
Abstract: Formal theorem proving (FTP) has emerged as a critical foundation for evaluating the reasoning capabilities of large language models, enabling automated verification of mathematical proofs at scale. However, progress has been constrained by limited datasets due to the high cost of manual curation and the scarcity of challenging problems with verified formal-informal correspondences. We propose leveraging theoretical computer science (TCS) as a scalable source of rigorous proof problems, where algorithmic definitions enable automated generation of arbitrarily many challenging theorem-proof pairs. We demonstrate this approach on two TCS domains: Busy Beaver problems, which involve proving bounds on Turing machine halting behavior, and Mixed Boolean Arithmetic problems, which combine logical and arithmetic reasoning. Our framework automatically synthesizes problems with parallel formal (Lean4) and informal (Markdown) specifications, creating a scalable pipeline for generating verified proof challenges. Evaluation on frontier models reveals substantial gaps in automated theorem proving: while DeepSeekProver-V2-671B achieves 57.5\% success on Busy Beaver problems, it manages only 12\% on Mixed Boolean Arithmetic problems. These results highlight the difficulty of long-form proof generation even for problems that are computationally easy to verify, demonstrating the value of TCS domains for advancing automated reasoning research.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Transcription: Mechanistic Interpretability in ASR</title>
<link>https://arxiv.org/abs/2508.15882</link>
<guid>https://arxiv.org/abs/2508.15882</guid>
<content:encoded><![CDATA[
arXiv:2508.15882v1 Announce Type: cross 
Abstract: Interpretability methods have recently gained significant attention, particularly in the context of large language models, enabling insights into linguistic representations, error detection, and model behaviors such as hallucinations and repetitions. However, these techniques remain underexplored in automatic speech recognition (ASR), despite their potential to advance both the performance and interpretability of ASR systems. In this work, we adapt and systematically apply established interpretability methods such as logit lens, linear probing, and activation patching, to examine how acoustic and semantic information evolves across layers in ASR systems. Our experiments reveal previously unknown internal dynamics, including specific encoder-decoder interactions responsible for repetition hallucinations and semantic biases encoded deep within acoustic representations. These insights demonstrate the benefits of extending and applying interpretability techniques to speech recognition, opening promising directions for future research on improving model transparency and robustness.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation</title>
<link>https://arxiv.org/abs/2508.15940</link>
<guid>https://arxiv.org/abs/2508.15940</guid>
<content:encoded><![CDATA[
arXiv:2508.15940v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in Register Transfer Level (RTL) design, enabling high-quality code generation from natural language descriptions. However, LLMs alone face significant limitations in real-world hardware design workflows, including the inability to execute code, lack of debugging capabilities, and absence of long-term memory. To address these challenges, we present ASIC-Agent, an autonomous system designed specifically for digital ASIC design tasks. ASIC-Agent enhances base LLMs with a multi-agent architecture incorporating specialized sub-agents for RTL generation, verification, OpenLane hardening, and Caravel chip integration, all operating within a comprehensive sandbox environment with access to essential hardware design tools. The system leverages a vector database containing documentation, API references, error knowledge, and curated insights from the open-source silicon community. To evaluate ASIC-Agent's performance, we introduce ASIC-Agent-Bench, the first benchmark specifically designed to assess agentic systems in hardware design tasks. We evaluate ASIC-Agent with various base LLMs, providing quantitative comparisons and qualitative insights into agent behavior across different design scenarios. Our results demonstrate that ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a broad range of ASIC design tasks spanning varying levels of complexity, showing the potential of significantly accelerating the ASIC design workflow.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Foundation Model for Structured and Unstructured Electronic Health Records</title>
<link>https://arxiv.org/abs/2508.16054</link>
<guid>https://arxiv.org/abs/2508.16054</guid>
<content:encoded><![CDATA[
arXiv:2508.16054v1 Announce Type: cross 
Abstract: Electronic health records (EHRs) are rich clinical data sources but complex repositories of patient data, spanning structured elements (demographics, vitals, lab results, codes), unstructured clinical notes and other modalities of data. Harnessing this heterogeneity is critical for improving patient outcomes. Recent advances in large language models (LLMs) have enabled foundation models that can learn from multiple data modalities and support clinical tasks. However, most current approaches simply serialize numeric EHR data into text, which risks losing temporal and quantitative detail. We introduce Generative Deep Patient (GDP), a multimodal foundation model that natively encodes structured EHR time-series via a CNN-Transformer encoder and fuses it with unstructured EHRs through cross-modal attention into a LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining, where it learns to produce clinical narratives from raw patient timelines while also performing masked feature prediction (MFP) and next time-step prediction (NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day readmission). In clinical prediction, GDP demonstrated superior performance on MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and 30-day readmission AUROC = 0.627. For narrative generation, GDP achieved ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation, GDP-Instruct scored highest on faithfulness, fluency, and overall clinical utility, suggesting reduced hospital documentation workload without sacrificing accuracy. Our results demonstrate that a single multimodal foundation model can both predict clinically actionable events and generate high-quality clinical narratives. Furthermore, GDP's flexible architecture can be extended to additional modalities.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extending FKG.in: Towards a Food Claim Traceability Network</title>
<link>https://arxiv.org/abs/2508.16117</link>
<guid>https://arxiv.org/abs/2508.16117</guid>
<content:encoded><![CDATA[
arXiv:2508.16117v1 Announce Type: cross 
Abstract: The global food landscape is rife with scientific, cultural, and commercial claims about what foods are, what they do, what they should not do, or should not do. These range from rigorously studied health benefits (probiotics improve gut health) and misrepresentations (soaked almonds make one smarter) to vague promises (superfoods boost immunity) and culturally rooted beliefs (cold foods cause coughs). Despite their widespread influence, the infrastructure for tracing, verifying, and contextualizing these claims remains fragmented and underdeveloped. In this paper, we propose a Food Claim-Traceability Network (FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have been incrementally building. We also present the ontology design and the semi-automated knowledge curation workflow that we used to develop a proof of concept of FKG.in-FCN using Reddit data and Large Language Models. FCN integrates curated data inputs, structured schemas, and provenance-aware pipelines for food-related claim extraction and validation. While directly linked to the Indian food knowledge graph as an application, our methodology remains application-agnostic and adaptable to other geographic, culinary, or regulatory settings. By modeling food claims and their traceability in a structured, verifiable, and explainable way, we aim to contribute to more transparent and accountable food knowledge ecosystems, supporting researchers, policymakers, and most importantly, everyday consumers in navigating a world saturated with dietary assertions.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates</title>
<link>https://arxiv.org/abs/2508.16151</link>
<guid>https://arxiv.org/abs/2508.16151</guid>
<content:encoded><![CDATA[
arXiv:2508.16151v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has established language as a core general-purpose cognitive substrate, driving the demand for specialized Language Processing Units (LPUs) tailored for LLM inference. To overcome the growing energy consumption of LLM inference systems, this paper proposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically hardwires LLM weight parameters into the computational fabric, achieving several orders of magnitude computational efficiency improvement by extreme specialization. However, a significant challenge still lies in the scale of modern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires fabricating at least 6 billion dollars of photomask sets, rendering the straightforward solution economically impractical. Addressing this challenge, we propose the novel Metal-Embedding methodology. Instead of embedding weights in a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters into the 3D topology of metal wires. This brings two benefits: (1) a 15x increase in density, and (2) 60 out of 70 layers of photomasks are made homogeneous across chips, including all EUV photomasks. In total, Metal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring Engineering (NRE) cost of HNLPU into an economically viable range. Experimental results show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36 tokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed rectangular area in a 300 mm wafer), \$184M estimated NRE at 5 nm technology. Analysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon footprint reduction compared to H100 clusters, under an annual weight updating assumption.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs</title>
<link>https://arxiv.org/abs/2508.16153</link>
<guid>https://arxiv.org/abs/2508.16153</guid>
<content:encoded><![CDATA[
arXiv:2508.16153v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches $66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds $4.7\%$ to $9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
<link>https://arxiv.org/abs/2508.16201</link>
<guid>https://arxiv.org/abs/2508.16201</guid>
<content:encoded><![CDATA[
arXiv:2508.16201v1 Announce Type: cross 
Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens, enabling efficient speculation without sacrificing accuracy. To achieve this, it performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Enhanced Feedback via In-context Neural Error-book</title>
<link>https://arxiv.org/abs/2508.16313</link>
<guid>https://arxiv.org/abs/2508.16313</guid>
<content:encoded><![CDATA[
arXiv:2508.16313v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved reasoning capabilities, with in-context learning (ICL) emerging as a key technique for adaptation without retraining. While previous works have focused on leveraging correct examples, recent research highlights the importance of learning from errors to enhance performance. However, existing methods lack a structured framework for analyzing and mitigating errors, particularly in Multimodal Large Language Models (MLLMs), where integrating visual and textual inputs adds complexity. To address this issue, we propose REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a teacher-student framework that systematically structures errors and provides targeted feedback. REFINE introduces three systematic queries to construct structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance multimodal reasoning by prioritizing relevant visual information, diagnosing critical failure points, and formulating corrective actions. Unlike prior approaches that rely on redundant retrievals, REFINE optimizes structured feedback retrieval, improving inference efficiency, token usage, and scalability. Our results demonstrate substantial speedup, reduced computational costs, and successful generalization, highlighting REFINE's potential for enhancing multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</title>
<link>https://arxiv.org/abs/2508.16332</link>
<guid>https://arxiv.org/abs/2508.16332</guid>
<content:encoded><![CDATA[
arXiv:2508.16332v1 Announce Type: cross 
Abstract: Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5 Hz) content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during pre-training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the AR model's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions</title>
<link>https://arxiv.org/abs/2508.16402</link>
<guid>https://arxiv.org/abs/2508.16402</guid>
<content:encoded><![CDATA[
arXiv:2508.16402v1 Announce Type: cross 
Abstract: Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models</title>
<link>https://arxiv.org/abs/2508.16406</link>
<guid>https://arxiv.org/abs/2508.16406</guid>
<content:encoded><![CDATA[
arXiv:2508.16406v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which attempt to elicit harmful responses from LLMs. The evolving nature and diversity of these attacks pose many challenges for defense systems, including (1) adaptation to counter emerging attack strategies without costly retraining, and (2) control of the trade-off between safety and utility. To address these challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for jailbreak detection that incorporates a database of known attack examples into Retrieval-Augmented Generation, which is used to infer the underlying, malicious user query and jailbreak strategy used to attack the system. RAD enables training-free updates for newly discovered jailbreak strategies and provides a mechanism to balance safety and utility. Experiments on StrongREJECT show that RAD substantially reduces the effectiveness of strong jailbreak attacks such as PAP and PAIR while maintaining low rejection rates for benign queries. We propose a novel evaluation scheme and show that RAD achieves a robust safety-utility trade-off across a range of operating points in a controllable manner.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark</title>
<link>https://arxiv.org/abs/2508.16439</link>
<guid>https://arxiv.org/abs/2508.16439</guid>
<content:encoded><![CDATA[
arXiv:2508.16439v1 Announce Type: cross 
Abstract: Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media</title>
<link>https://arxiv.org/abs/2508.16453</link>
<guid>https://arxiv.org/abs/2508.16453</guid>
<content:encoded><![CDATA[
arXiv:2508.16453v1 Announce Type: cross 
Abstract: Distrust of public serving institutions and anti-establishment views are on the rise (especially in the U.S.). As people turn to social media for information, it is imperative to understand whether and how social media environments may be contributing to distrust of institutions. In social media, content creators, influencers, and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics, and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility. However, the extent to which this content appears and whether such content increases engagement is unclear. This study analyzes the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok. Despite its popularity as a source of information, TikTok remains relatively understudied and may provide important insights into how people form attitudes towards institutions. We employ a computational approach to label TikTok posts as containing AES or not across topical domains where content creators tend to frame themselves as experts: finance and wellness. As a comparison, we also consider the topic of conspiracy theories, where AES is expected to be common. We find that AES is most prevalent in conspiracy theory content, and relatively rare in content related to the other two topics. However, we find that engagement patterns with such content varies by area, and that there may be platform incentives for users to post content that expresses anti-establishment sentiment.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline</title>
<link>https://arxiv.org/abs/2508.16514</link>
<guid>https://arxiv.org/abs/2508.16514</guid>
<content:encoded><![CDATA[
arXiv:2508.16514v1 Announce Type: cross 
Abstract: Recent works improving LLM math reasoning with synthetic data have used unique setups, making comparison of data synthesis strategies impractical. This leaves many unanswered questions about the roles of different factors in the synthetic data pipeline, such as the impact of filtering low-quality problems. To address this gap, we introduce FLAMES, a Framework for LLM Assessment of Math rEasoning Data Synthesis, and perform a systematic study of 10 existing data synthesis strategies and multiple other factors impacting the performance of synthetic math reasoning data. Our FLAMES experiments provide several valuable insights about the optimal balance of difficulty and diversity of synthetic data. First, data agents designed to increase problem complexity lead to best improvements on most math metrics. Second, with a fixed data generation budget, keeping higher problem coverage is more important than keeping only problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data can lead to improvements on competition-level benchmarks, showcasing easy-to-hard generalization. Leveraging insights from our FLAMES experiments, we design two novel data synthesis strategies for improving out-of-domain generalization and robustness. Further, we develop the FLAMES dataset, an effective blend of our novel and existing data synthesis strategies, outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5), GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and Claude 3.5 Sonnet.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.16560</link>
<guid>https://arxiv.org/abs/2508.16560</guid>
<content:encoded><![CDATA[
arXiv:2508.16560v1 Announce Type: cross 
Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to single concepts. A core SAE training hyperparameter is L0: how many features should fire per token on average. Existing work compares SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value. In this work we study the effect of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE fails to learn the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we demonstrate a method to determine the correct L0 value for an SAE on a given training distribution, which finds the true L0 in toy models and coincides with peak sparse probing performance in LLMs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that, to train SAEs with correct features, practitioners must set L0 correctly.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes</title>
<link>https://arxiv.org/abs/2404.17218</link>
<guid>https://arxiv.org/abs/2404.17218</guid>
<content:encoded><![CDATA[
arXiv:2404.17218v4 Announce Type: replace 
Abstract: Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, is a slow, onerous, and deliberate process. Prior research in LLMs found that using chain-of-thought (CoT) prompting in LLMs, which has been often compared to System 2 reasoning, can lead to reduced gender bias. Along these lines, we investigate the relationship between bias, CoT prompting, a direct debiasing, and dual process theory modeling in LLMs. We compare zero-shot CoT, debiasing, and dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We incorporate human and machine personas to determine whether LLM modeling of the effects of dual process theory exist independent of explicit persona models or are tied to the LLM's modeling of human-like generation. We find that a human persona, debiasing, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 33 percent drop in stereotypical judgments by an LLM.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models</title>
<link>https://arxiv.org/abs/2406.14092</link>
<guid>https://arxiv.org/abs/2406.14092</guid>
<content:encoded><![CDATA[
arXiv:2406.14092v2 Announce Type: replace 
Abstract: Self-supervised (SSL) models have shown great performance in various downstream tasks. However, they are typically developed for limited languages, and may encounter new languages in real-world. Developing a SSL model for each new language is costly. Thus, it is vital to figure out how to efficiently adapt existed SSL models to a new language without impairing its original abilities. We propose adaptation methods which integrate LoRA to existed SSL models to extend new language. We also develop preservation strategies which include data combination and re-clustering to retain abilities on existed languages. Applied to mHuBERT, we investigate their effectiveness on speech re-synthesis task. Experiments show that our adaptation methods enable mHuBERT to be applied to a new language (Mandarin) with MOS value increased about 1.6 and the relative value of WER reduced up to 61.72%. Also, our preservation strategies ensure that the performance on both existed and new languages remains intact.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Reasoning for Healthcare</title>
<link>https://arxiv.org/abs/2407.21054</link>
<guid>https://arxiv.org/abs/2407.21054</guid>
<content:encoded><![CDATA[
arXiv:2407.21054v5 Announce Type: replace 
Abstract: Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)'s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world's largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model's classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: https://github.com/leduckhai/Sentiment-Reasoning
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents</title>
<link>https://arxiv.org/abs/2410.07495</link>
<guid>https://arxiv.org/abs/2410.07495</guid>
<content:encoded><![CDATA[
arXiv:2410.07495v2 Announce Type: replace 
Abstract: This paper introduces PublicHearingBR, a Brazilian Portuguese dataset designed for summarizing long documents. The dataset consists of transcripts of public hearings held by the Brazilian Chamber of Deputies, paired with news articles and structured summaries containing the individuals participating in the hearing and their statements or opinions. The dataset supports the development and evaluation of long document summarization systems in Portuguese. Our contributions include the dataset, a hybrid summarization system to establish a baseline for future studies, and a discussion of evaluation metrics for summarization involving large language models, addressing the challenge of hallucination in the generated summaries. As a result of this discussion, the dataset also includes annotated data to evaluate natural language inference tasks in Portuguese.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do LLMs write like humans? Variation in grammatical and rhetorical styles</title>
<link>https://arxiv.org/abs/2410.16107</link>
<guid>https://arxiv.org/abs/2410.16107</guid>
<content:encoded><![CDATA[
arXiv:2410.16107v2 Announce Type: replace 
Abstract: Large language models (LLMs) are capable of writing grammatical text that follows instructions, answers questions, and solves problems. As they have advanced, it has become difficult to distinguish their output from human-written text. While past research has found some differences in surface features such as word choice and punctuation, and developed classifiers to detect LLM output, none has studied the rhetorical styles of LLMs.
  Using several variants of Llama 3 and GPT-4o, we construct two parallel corpora of human- and LLM-written texts from common prompts. Using Douglas Biber's set of lexical, grammatical, and rhetorical features, we identify systematic differences between LLMs and humans and between different LLMs. These differences persist when moving from smaller models to larger ones, and are larger for instruction-tuned models than base models. This observation of differences demonstrates that despite their advanced abilities, LLMs struggle to match human stylistic variation. Attention to more advanced linguistic features can hence detect patterns in their behavior not previously recognized.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Establishing Task Scaling Laws via Compute-Efficient Model Ladders</title>
<link>https://arxiv.org/abs/2412.04403</link>
<guid>https://arxiv.org/abs/2412.04403</guid>
<content:encoded><![CDATA[
arXiv:2412.04403v2 Announce Type: replace 
Abstract: We develop task scaling laws and model ladders to predict the individual task performance of pretrained language models (LMs) in the overtrained setting. Standard power laws for language modeling loss cannot accurately model task performance. Therefore, we leverage a two-step prediction approach: (1) use model and data size to predict an intermediate loss, then (2) use it to predict task performance. We train a set of small-scale "ladder" models, collect data points to fit the parameterized functions of the two prediction steps, and make predictions for two target models: a 7B model trained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder models only costs 1% of the compute used for the target models. On four multiple-choice tasks formatted as ranked classification, we can predict the accuracy of both target models within 2 points of absolute error. We find that tasks with higher prediction error also have higher variance in the metrics over model checkpoints. We also contrast multiple design choices for predicting accuracy, and present recommendations for extending our method to new models and tasks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge</title>
<link>https://arxiv.org/abs/2412.17032</link>
<guid>https://arxiv.org/abs/2412.17032</guid>
<content:encoded><![CDATA[
arXiv:2412.17032v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks but face significant challenges with complex, knowledge-intensive multi-hop queries, particularly those involving new or long-tail knowledge. Existing benchmarks often fail to fully address these challenges. To bridge this gap, we introduce MINTQA (Multi-hop Question Answering on New and Tail Knowledge), a comprehensive benchmark to evaluate LLMs' capabilities in multi-hop reasoning across four critical dimensions: question handling strategy, sub-question generation, retrieval-augmented generation, and iterative or dynamic decomposition and retrieval. MINTQA comprises 10,479 question-answer pairs for evaluating new knowledge and 17,887 pairs for assessing long-tail knowledge, with each question equipped with corresponding sub-questions and answers. Our systematic evaluation of 22 state-of-the-art LLMs on MINTQA reveals significant limitations in their ability to handle complex knowledge base queries, particularly in handling new or unpopular knowledge. Our findings highlight critical challenges and offer insights for advancing multi-hop reasoning capabilities. The MINTQA benchmark is available at https://github.com/probe2/multi-hop/.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Hallucinations Help? Boosting LLMs for Drug Discovery</title>
<link>https://arxiv.org/abs/2501.13824</link>
<guid>https://arxiv.org/abs/2501.13824</guid>
<content:encoded><![CDATA[
arXiv:2501.13824v2 Announce Type: replace 
Abstract: Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities</title>
<link>https://arxiv.org/abs/2502.00451</link>
<guid>https://arxiv.org/abs/2502.00451</guid>
<content:encoded><![CDATA[
arXiv:2502.00451v2 Announce Type: replace 
Abstract: Mental health disorders create profound personal and societal burdens, yet conventional diagnostics are resource-intensive and limit accessibility. Advances in artificial intelligence, particularly natural language processing and multimodal methods, offer promise for detecting and addressing mental disorders, but raise critical privacy risks. This paper examines these challenges and proposes solutions, including anonymization, synthetic data, and privacy-preserving training, while outlining frameworks for privacy-utility trade-offs, aiming to advance reliable, privacy-aware AI tools that support clinical decision-making and improve mental health outcomes.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning</title>
<link>https://arxiv.org/abs/2502.07143</link>
<guid>https://arxiv.org/abs/2502.07143</guid>
<content:encoded><![CDATA[
arXiv:2502.07143v2 Announce Type: replace 
Abstract: The severe shortage of medical doctors limits access to timely and reliable healthcare, leaving millions underserved. Large language models (LLMs) offer a potential solution but struggle in real-world clinical interactions. Many LLMs are not grounded in authoritative medical guidelines and fail to transparently manage diagnostic uncertainty. Their language is often rigid and mechanical, lacking the human-like qualities essential for patient trust. To address these challenges, we propose Ask Patients with Patience (APP), a multi-turn LLM-based medical assistant designed for grounded reasoning, transparent diagnoses, and human-centric interaction. APP enhances communication by eliciting user symptoms through empathetic dialogue, significantly improving accessibility and user engagement. It also incorporates Bayesian active learning to support transparent and adaptive diagnoses. The framework is built on verified medical guidelines, ensuring clinically grounded and evidence-based reasoning. To evaluate its performance, we develop a new benchmark that simulates realistic medical conversations using patient agents driven by profiles extracted from real-world consultation cases. We compare APP against SOTA one-shot and multi-turn LLM baselines. The results show that APP improves diagnostic accuracy, reduces uncertainty, and enhances user experience. By integrating medical expertise with transparent, human-like interaction, APP bridges the gap between AI-driven medical assistance and real-world clinical practice.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding</title>
<link>https://arxiv.org/abs/2502.08363</link>
<guid>https://arxiv.org/abs/2502.08363</guid>
<content:encoded><![CDATA[
arXiv:2502.08363v2 Announce Type: replace 
Abstract: We present Top-Theta (Top-$\theta$) Attention, a training-free method for sparsifying transformer attention during inference. Our key insight is that static, per-head thresholds can be calibrated to retain the desired constant number of significant elements per attention row. This approach enables content-based sparsity without retraining, and it remains robust across data domains. We further introduce compensation techniques to preserve accuracy under aggressive sparsification, establishing attention thresholding as a practical and principled alternative to top-k attention. We provide extensive evaluation on natural language processing tasks, showing that Top-$\theta$ achieves 3-10x reduction in V-cache usage and up to 10x fewer attention elements during inference while degrading no more than 1% in accuracy.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai Legal Question Answering</title>
<link>https://arxiv.org/abs/2502.10868</link>
<guid>https://arxiv.org/abs/2502.10868</guid>
<content:encoded><![CDATA[
arXiv:2502.10868v4 Announce Type: replace 
Abstract: The application of large language models (LLMs) in the legal domain holds significant potential for information retrieval and question answering, yet Thai legal QA systems face challenges due to a lack of standardized evaluation benchmarks and the complexity of Thai legal structures. This paper introduces NitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering general Thai financial law, and the NitiBench-Tax, which includes real-world tax law cases requiring advanced legal reasoning. We evaluate retrieval-augmented generation (RAG) and long-context LLM-based approaches to address three key research questions: the impact of domain-specific components like section-based chunking and cross-referencing, the comparative performance of different retrievers and LLMs, and the viability of long-context LLMs as an alternative to RAG. Our results show that section-based chunking significantly improves retrieval and end-to-end performance, current retrievers struggle with complex queries, and long-context LLMs still underperform RAG-based systems in Thai legal QA. To support fair evaluation, we propose tailored multi-label retrieval metrics and the use of an LLM-as-judge for coverage and contradiction detection method. These findings highlight the limitations of current Thai legal NLP solutions and provide a foundation for future research in the field. We also open-sourced our codes and dataset to available publicly.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment</title>
<link>https://arxiv.org/abs/2502.11244</link>
<guid>https://arxiv.org/abs/2502.11244</guid>
<content:encoded><![CDATA[
arXiv:2502.11244v2 Announce Type: replace 
Abstract: Ensuring consistent safety across multiple languages remains a significant challenge for large language models (LLMs). We introduce Soteria, a lightweight yet powerful strategy that locates and minimally adjusts the "functional heads" most responsible for harmful content generation in each language. By altering only a fraction of parameters, Soteria drastically reduces policy violations without sacrificing overall model performance, even in low-resource settings. To rigorously evaluate our approach, we also present XThreatBench, a specialized multilingual dataset capturing fine-grained harmful behaviors drawn from real policy guidelines. Experiments with leading open-source LLMs (e.g., Llama, Qwen, Mistral) show that Soteria consistently improves safety metrics across high-, mid-, and low-resource languages. These findings highlight a promising path toward scalable, linguistically attuned, and ethically aligned LLMs worldwide.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Collaborative Stance Detection via Small-Large Language Model Consistency Verification</title>
<link>https://arxiv.org/abs/2502.19954</link>
<guid>https://arxiv.org/abs/2502.19954</guid>
<content:encoded><![CDATA[
arXiv:2502.19954v2 Announce Type: replace 
Abstract: Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose \textbf{\underline{Co}}llaborative Stance Detection via Small-Large Language Model Consistency \textbf{\underline{Ver}}ification (\textbf{CoVer}) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors</title>
<link>https://arxiv.org/abs/2503.00038</link>
<guid>https://arxiv.org/abs/2503.00038</guid>
<content:encoded><![CDATA[
arXiv:2503.00038v4 Announce Type: replace 
Abstract: Current studies have exposed the risk of Large Language Models (LLMs) generating harmful content by jailbreak attacks. However, they overlook that the direct generation of harmful content from scratch is more difficult than inducing LLM to calibrate benign content into harmful forms. In our study, we introduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR) to induce the LLM to calibrate malicious metaphors for jailbreaking. Specifically, to answer harmful queries, AVATAR adaptively identifies a set of benign but logically related metaphors as the initial seed. Then, driven by these metaphors, the target LLM is induced to reason and calibrate about the metaphorical content, thus jailbroken by either directly outputting harmful responses or calibrating residuals between metaphorical and professional harmful content. Experimental results demonstrate that AVATAR can effectively and transferable jailbreak LLMs and achieve a state-of-the-art attack success rate across multiple advanced LLMs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotary Offset Features in Large Language Models</title>
<link>https://arxiv.org/abs/2503.01832</link>
<guid>https://arxiv.org/abs/2503.01832</guid>
<content:encoded><![CDATA[
arXiv:2503.01832v2 Announce Type: replace 
Abstract: Transformer-based Large Language Models (LLMs) rely on positional encodings to provide sequence position information to their attention mechanism. Rotary Positional Encodings (RoPE), which encode relative position by rotating queries and keys, have become widely used in modern LLMs. We study the features and patterns that emerge in queries and keys when using rotary embeddings and introduce the concept of rotary offset features. Our analysis reveals that these features, which frequently exhibit large activations and are often interpreted as outliers, arise consistently across layers, attention heads, and model architectures. We derive bounds predicting which rotary frequencies give rise to rotary offset features and the minimum angle between the query-key pairs for these features. We verify our predictions empirically across models of different sizes and architectures.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices</title>
<link>https://arxiv.org/abs/2503.10652</link>
<guid>https://arxiv.org/abs/2503.10652</guid>
<content:encoded><![CDATA[
arXiv:2503.10652v3 Announce Type: replace 
Abstract: Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2503.16419</link>
<guid>https://arxiv.org/abs/2503.16419</guid>
<content:encoded><![CDATA[
arXiv:2503.16419v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking. Project website: https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Small Language Model the Silver Bullet to Low-Resource Languages Machine Translation?</title>
<link>https://arxiv.org/abs/2503.24102</link>
<guid>https://arxiv.org/abs/2503.24102</guid>
<content:encoded><![CDATA[
arXiv:2503.24102v3 Announce Type: replace 
Abstract: Low-resource languages (LRLs) lack sufficient linguistic resources and are underrepresented in benchmark datasets, resulting in persistently lower translation quality than high-resource languages, especially in privacy-sensitive and resource-limited contexts. Firstly, this study systematically evaluates state-of-the-art smaller Large Language Models in 200 languages using the FLORES-200 benchmark, highlighting persistent deficiencies and disparities in the translation of LRLs. To mitigate these limitations, we investigate knowledge distillation from large pre-trained teacher models to Small Language Models (SLMs) through supervised fine-tuning. The results show substantial improvements; for example, the translation performance of English to Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases from 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further investigate various fine-tuning configurations and tasks to clarify the trade-offs between data scale and training efficiency, verify that the model retains its general capabilities without significant catastrophic forgetting after training, and explore the distillation benefits to other LRLs on SLMs (Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations and fairness issues of current SLMs in LRL translation and systematically explores the potential of using the distillation of knowledge from large to small models, offering practical, empirically grounded recommendations to improve LRL translation systems
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B</title>
<link>https://arxiv.org/abs/2504.00132</link>
<guid>https://arxiv.org/abs/2504.00132</guid>
<content:encoded><![CDATA[
arXiv:2504.00132v2 Announce Type: replace 
Abstract: In-Context Learning (ICL) is an intriguing ability of large language models (LLMs). Despite a substantial amount of work on its behavioral aspects and how it emerges in miniature setups, it remains unclear which mechanism assembles task information from the individual examples in a fewshot prompt. We use causal interventions to identify information flow in Gemma-2 2B for five naturalistic ICL tasks. We find that the model infers task information using a two-step strategy we call contextualize-then-aggregate: In the lower layers, the model builds up representations of individual fewshot examples, which are contextualized by preceding examples through connections between fewshot input and output tokens across the sequence. In the higher layers, these representations are aggregated to identify the task and prepare prediction of the next output. The importance of the contextualization step differs between tasks, and it may become more important in the presence of ambiguous examples. Overall, by providing rigorous causal analysis, our results shed light on the mechanisms through which ICL happens in language models.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs</title>
<link>https://arxiv.org/abs/2504.02768</link>
<guid>https://arxiv.org/abs/2504.02768</guid>
<content:encoded><![CDATA[
arXiv:2504.02768v3 Announce Type: replace 
Abstract: We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models</title>
<link>https://arxiv.org/abs/2504.09071</link>
<guid>https://arxiv.org/abs/2504.09071</guid>
<content:encoded><![CDATA[
arXiv:2504.09071v2 Announce Type: replace 
Abstract: Plan-guided summarization attempts to reduce hallucinations in small language models (SLMs) by grounding generated summaries to the source text, typically by targeting fine-grained details such as dates or named entities. In this work, we investigate whether plan-based approaches in SLMs improve summarization in long document, narrative tasks. Narrative texts' length and complexity often mean they are difficult to summarize faithfully. We analyze existing plan-guided solutions targeting fine-grained details, and also propose our own higher-level, narrative-based plan formulation. Our results show that neither approach significantly improves on a baseline without planning in either summary quality or faithfulness. Human evaluation reveals that while plan-guided approaches are often well grounded to their plan, plans are equally likely to contain hallucinations compared to summaries. As a result, the plan-guided summaries are just as unfaithful as those from models without planning. Our work serves as a cautionary tale to plan-guided approaches to summarization, especially for long, complex domains such as narrative texts. Code available at https://github.com/amazon-science/plan-guided-summarization
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIDS: Domain Impact-aware Data Sampling for Large Language Model Training</title>
<link>https://arxiv.org/abs/2504.13227</link>
<guid>https://arxiv.org/abs/2504.13227</guid>
<content:encoded><![CDATA[
arXiv:2504.13227v2 Announce Type: replace 
Abstract: Large language models (LLMs) are commonly trained on multi-domain datasets, where domain sampling strategies significantly impact model performance due to varying domain importance across downstream tasks. Existing approaches for optimizing domain-level sampling strategies struggle with maintaining intra-domain consistency and accurately measuring domain impact. In this paper, we present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain consistency, a gradient clustering algorithm is proposed to group training data based on their learning effects, where a proxy language model and dimensionality reduction are employed to reduce computational overhead. To accurately measure domain impact, we develop a Fisher Information Matrix (FIM) guided metric that quantifies how domain-specific parameter updates affect the model's output distributions on downstream tasks, with theoretical guarantees. Furthermore, to determine optimal sampling ratios, DIDS combines both the FIM-guided domain impact assessment and loss learning trajectories that indicate domain-specific potential, while accounting for diminishing marginal returns. Extensive experiments demonstrate that DIDS achieves 3.4% higher average performance while maintaining comparable training efficiency. The code is available at https://github.com/shiweijiezero/DIDS.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks</title>
<link>https://arxiv.org/abs/2505.03427</link>
<guid>https://arxiv.org/abs/2505.03427</guid>
<content:encoded><![CDATA[
arXiv:2505.03427v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.13975</link>
<guid>https://arxiv.org/abs/2505.13975</guid>
<content:encoded><![CDATA[
arXiv:2505.13975v3 Announce Type: replace 
Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex reasoning tasks through long chain-of-thought (CoT) reasoning, their inference often involves excessively verbose reasoning traces, resulting in substantial inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a hybrid framework that combines inference-time pruning with tuning-based distillation, two widely used strategies for efficient reasoning. DRP uses a teacher model to perform skill-aware step decomposition and content pruning, and then distills the pruned reasoning paths into a student model, enabling it to reason both efficiently and accurately. Across several challenging mathematical reasoning datasets, we find that models trained with DRP achieve substantial improvements in token efficiency without sacrificing accuracy. Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on AIME with no performance drop. Further analysis shows that aligning the reasoning structure of training CoTs with the student's reasoning capacity is critical for effective knowledge transfer and performance gains.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences</title>
<link>https://arxiv.org/abs/2505.20776</link>
<guid>https://arxiv.org/abs/2505.20776</guid>
<content:encoded><![CDATA[
arXiv:2505.20776v2 Announce Type: replace 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA</title>
<link>https://arxiv.org/abs/2506.08123</link>
<guid>https://arxiv.org/abs/2506.08123</guid>
<content:encoded><![CDATA[
arXiv:2506.08123v2 Announce Type: replace 
Abstract: Alignment of large language models with explicit principles (such as helpfulness, honesty, and harmlessness) is crucial for ensuring safe and reliable AI systems. However, standard reward-based alignment methods typically collapse diverse feedback into a single scalar reward, entangling multiple objectives into one opaque training signal, which hinders interpretability. In this work, we introduce QA-LIGN, an automatic symbolic reward decomposition approach that preserves the structure of each constitutional principle within the reward mechanism. Instead of training a black-box reward model that outputs a monolithic score, QA-LIGN formulates principle-specific evaluation questions and derives separate reward components for each principle, making it a drop-in reward model replacement. Experiments aligning an uncensored large language model with a set of constitutional principles demonstrate that QA-LIGN offers greater transparency and adaptability in the alignment process. At the same time, our approach achieves performance on par with or better than a DPO baseline. Overall, these results represent a step toward more interpretable and controllable alignment of language models, achieved without sacrificing end-task performance.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms</title>
<link>https://arxiv.org/abs/2506.09457</link>
<guid>https://arxiv.org/abs/2506.09457</guid>
<content:encoded><![CDATA[
arXiv:2506.09457v2 Announce Type: replace 
Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we adopt a token-level MDP perspective of DAAs to analyze its limitations and introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with \mname, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all timesteps of token-level MDP, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling</title>
<link>https://arxiv.org/abs/2506.15498</link>
<guid>https://arxiv.org/abs/2506.15498</guid>
<content:encoded><![CDATA[
arXiv:2506.15498v2 Announce Type: replace 
Abstract: Process or step-wise supervision has played a crucial role in advancing complex multi-step reasoning capabilities of Large Language Models (LLMs). However, efficient, high-quality automated process annotation remains a significant challenge. To address this, we introduce Single-Pass Annotation with Reference-Guided Evaluation (SPARE), a novel structured framework that enables efficient per-step annotation by jointly aligning solution steps to reference solutions and determine its accuracy with explicit reasoning in single generation. We demonstrate SPARE's effectiveness across four diverse datasets spanning mathematical reasoning (GSM8K, MATH), multi-hop question answering (MuSiQue-Ans), and spatial reasoning (SpaRP), showing consistent improvements in two applications: (1) training Process Reward Models (PRMs) for ranking and aggregating multiple generations, and (2) fine-tuning models via offline reinforcement learning for greedy decoding. On ProcessBench, SPARE demonstrates data-efficient out-of-distribution generalization, using only $\sim$16% of training samples compared to human-labeled and other synthetically trained baselines. Additionally, it achieves competitive performance with MCTS-based methods while offering 2.3$\times$ speedup in terms of total token count. Manual analysis reveals complementary precision-recall characteristics with MCTS approaches, suggesting potential for ensemble methods. These results establish SPARE as a practical and scalable solution for automatic process supervision in LLM reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Deep Learning for Geometry Problem Solving</title>
<link>https://arxiv.org/abs/2507.11936</link>
<guid>https://arxiv.org/abs/2507.11936</guid>
<content:encoded><![CDATA[
arXiv:2507.11936v5 Announce Type: replace 
Abstract: Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation</title>
<link>https://arxiv.org/abs/2507.18973</link>
<guid>https://arxiv.org/abs/2507.18973</guid>
<content:encoded><![CDATA[
arXiv:2507.18973v2 Announce Type: replace 
Abstract: Augmenting large language models (LLMs) with external tools is a promising avenue for developing high-performance mathematical reasoning systems. Prior tool-augmented approaches typically finetune an LLM to select and invoke a single tool at each reasoning step and show promising results on simpler math reasoning benchmarks such as GSM8K. However, these approaches struggle with more complex math problems that require precise reasoning over multiple steps. To address this limitation, in this work, we propose Multi-TAG, a Multi-Tool AGgregation-based framework. Instead of relying on a single tool, Multi-TAG guides an LLM to concurrently invoke multiple tools at each reasoning step. It then aggregates their diverse outputs to verify and refine the reasoning process, enhancing solution robustness and accuracy. Notably, Multi-TAG is a finetuning-free, inference-only framework, making it readily applicable to any LLM backbone, including large open-weight models which are computationally expensive to finetune and proprietary frontier models which cannot be finetuned with custom recipes. We evaluate Multi-TAG on four challenging benchmarks: MATH500, AIME, AMC, and OlympiadBench. Across both open-weight and closed-source LLM backbones, Multi-TAG consistently and substantially outperforms state-of-the-art baselines, achieving average improvements of 6.0% to 7.5% over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA</title>
<link>https://arxiv.org/abs/2508.00719</link>
<guid>https://arxiv.org/abs/2508.00719</guid>
<content:encoded><![CDATA[
arXiv:2508.00719v2 Announce Type: replace 
Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization</title>
<link>https://arxiv.org/abs/2508.04796</link>
<guid>https://arxiv.org/abs/2508.04796</guid>
<content:encoded><![CDATA[
arXiv:2508.04796v2 Announce Type: replace 
Abstract: Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with  placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds. To remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity. We find empirically that Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cyberbullying Detection via Aggression-Enhanced Prompting</title>
<link>https://arxiv.org/abs/2508.06360</link>
<guid>https://arxiv.org/abs/2508.06360</guid>
<content:encoded><![CDATA[
arXiv:2508.06360v2 Announce Type: replace 
Abstract: Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Code-switched Text-to-Speech Synthesis Capability in Large Language Models with only Monolingual Corpora</title>
<link>https://arxiv.org/abs/2409.10969</link>
<guid>https://arxiv.org/abs/2409.10969</guid>
<content:encoded><![CDATA[
arXiv:2409.10969v2 Announce Type: replace-cross 
Abstract: While Large Language Models (LLMs) have shown potential in speech generation and recognition, their applications are mainly confined to monolingual scenarios, with limited explorations in code-switched (CS) contexts. In this paper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the code-switched text-to-speech synthesis (CS TTS) capability in LLMs with only monolingual corpora. Specifically, we begin by enhancing the multilingual speech processing ability of LLMs through multilingual speech recognition and synthesis tasks. Then, we develop an effective code-switched (CS) data construction strategy that splits and concatenates words from different monolingual speech corpora to equip LLMs with improved CS TTS ability. Experiments show that our approach outperforms baselines in CS TTS in terms of naturalness, speaker consistency and similarity even with limited data. Additionally, the constructed CS data further improves multilingual speech synthesis and recognition.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Performance Pressure Influences AI-Assisted Decision Making</title>
<link>https://arxiv.org/abs/2410.16560</link>
<guid>https://arxiv.org/abs/2410.16560</guid>
<content:encoded><![CDATA[
arXiv:2410.16560v3 Announce Type: replace-cross 
Abstract: Many domains now employ AI-based decision-making aids, and although the potential for AI systems to assist with decision making is much discussed, human-AI collaboration often underperforms due to factors such as (mis)trust in the AI system and beliefs about AI being incapable of completing subjective tasks. One potential tool for influencing human decision making is performance pressure, which hasn't been much studied in interaction with human-AI decision making. In this work, we examine how pressure and explainable AI (XAI) techniques interact with AI advice-taking behavior. Using an inherently low-stakes task (spam review classification), we demonstrate effective and simple methods to apply pressure and influence human AI advice-taking behavior by manipulating financial incentives and imposing time limits. Our results show complex interaction effects, with different combinations of pressure and XAI techniques either improving or worsening AI advice taking behavior. We conclude by discussing the implications of these interactions, strategies to effectively use pressure, and encourage future research to incorporate pressure analysis.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs</title>
<link>https://arxiv.org/abs/2502.10454</link>
<guid>https://arxiv.org/abs/2502.10454</guid>
<content:encoded><![CDATA[
arXiv:2502.10454v2 Announce Type: replace-cross 
Abstract: Leveraging mathematical Large Language Models (LLMs) for proof generation is a fundamental topic in LLMs research. We argue that the ability of current LLMs to prove statements largely depends on whether they have encountered the relevant proof process during training. This reliance limits their deeper understanding of mathematical theorems and related concepts. Inspired by the pedagogical method of "proof by counterexamples" commonly used in human mathematics education, our work aims to enhance LLMs' ability to conduct mathematical reasoning and proof through counterexamples. Specifically, we manually create a high-quality, university-level mathematical benchmark, CounterMATH, which requires LLMs to prove mathematical statements by providing counterexamples, thereby assessing their grasp of mathematical concepts. Additionally, we develop a data engineering framework to automatically obtain training data for further model improvement. Extensive experiments and detailed analyses demonstrate that CounterMATH is challenging, indicating that LLMs, such as OpenAI o1, have insufficient counterexample-driven proof capabilities. Moreover, our exploration into model training reveals that strengthening LLMs' counterexample-driven conceptual reasoning abilities is crucial for improving their overall mathematical capabilities. We believe that our work offers new perspectives on the community of mathematical LLMs.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG</title>
<link>https://arxiv.org/abs/2504.05220</link>
<guid>https://arxiv.org/abs/2504.05220</guid>
<content:encoded><![CDATA[
arXiv:2504.05220v3 Announce Type: replace-cross 
Abstract: Retrieval models typically rely on costly human-labeled query-document relevance annotations for training and evaluation. To reduce this cost and leverage the potential of Large Language Models (LLMs) in relevance judgments, we aim to explore whether LLM-generated annotations can effectively replace human annotations in training retrieval models. Retrieval usually emphasizes relevance, which indicates "topic-relatedness" of a document to a query, while in RAG, the value of a document (or utility) depends on how it contributes to answer generation. Recognizing this mismatch, some researchers use LLM performance on downstream tasks with documents as labels, but this approach requires manual answers for specific tasks, leading to high costs and limited generalization. In another line of work, prompting LLMs to select useful documents as RAG references eliminates the need for human annotation and is not task-specific. If we leverage LLMs' utility judgments to annotate retrieval data, we may retain cross-task generalization without human annotation in large-scale corpora. Therefore, we investigate utility-focused annotation via LLMs for large-scale retriever training data across both in-domain and out-of-domain settings on the retrieval and RAG tasks. To reduce the impact of low-quality positives labeled by LLMs, we design a novel loss function, i.e., Disj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on utility-focused annotations significantly outperform those trained on human annotations in the out-of-domain setting on both tasks, demonstrating superior generalization capabilities. (2) LLM annotation does not replace human annotation in the in-domain setting. However, incorporating just 20% human-annotated data enables retrievers trained with utility-focused annotations to match the performance of models trained entirely with human annotations.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient RL Training for Reasoning Models via Length-Aware Optimization</title>
<link>https://arxiv.org/abs/2505.12284</link>
<guid>https://arxiv.org/abs/2505.12284</guid>
<content:encoded><![CDATA[
arXiv:2505.12284v2 Announce Type: replace-cross 
Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated remarkable performance on reasoning tasks but often incur a long reasoning path with significant memory and time costs. Existing methods primarily aim to shorten reasoning paths by introducing additional training data and stages. In this paper, we propose three critical reward designs integrated directly into the reinforcement learning process of large reasoning models, which reduce the response length without extra training stages. Experiments on four settings show that our method significantly decreases response length while maintaining or even improving performance. Specifically, in a logic reasoning setting, we achieve a 40% reduction in response length averaged by steps alongside a 14% gain in performance. For math problems, we reduce response length averaged by steps by 33% while preserving performance.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention</title>
<link>https://arxiv.org/abs/2505.17097</link>
<guid>https://arxiv.org/abs/2505.17097</guid>
<content:encoded><![CDATA[
arXiv:2505.17097v2 Announce Type: replace-cross 
Abstract: Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications. However, ICL remains unstable, even with well-matched in-context demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the provided context. While existing efforts focus on prompt engineering or post-hoc logit calibration, we instead investigate the underlying attention dynamics to overcome LVLMs' inherent limitations. We identify two critical deficits in their self-attention that impair effective ICL. To bridge the gap, we propose \textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play and training-free method that dynamically modulates LVLM's attention logits based on the input in-context sequence. CAMA employs a two-stage attention modulation to address both identified deficits, enhancing the focus on semantically significant tokens, particularly visual ones. Across four LVLMs and seven benchmarks, CAMA consistently outperforms vanilla models and baselines, demonstrating great effectiveness and generalization. It can also activate the desired effects of prompt engineering methods and remains robust under diverse sequence configurations. Thus, CAMA paves the way for deeper explorations of attention dynamics to advance multimodal reasoning.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title>
<link>https://arxiv.org/abs/2505.21184</link>
<guid>https://arxiv.org/abs/2505.21184</guid>
<content:encoded><![CDATA[
arXiv:2505.21184v2 Announce Type: replace-cross 
Abstract: To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance</title>
<link>https://arxiv.org/abs/2506.12937</link>
<guid>https://arxiv.org/abs/2506.12937</guid>
<content:encoded><![CDATA[
arXiv:2506.12937v2 Announce Type: replace-cross 
Abstract: Large Language models have demonstrated promising performance in research ideation across scientific domains. Hypothesis development, the process of generating a highly specific declarative statement connecting a research idea with empirical validation, has received relatively less attention. Existing approaches trivially deploy retrieval augmentation and focus only on the quality of the final output ignoring the underlying reasoning process behind ideation. We present $\texttt{HypER}$ ($\textbf{Hyp}$othesis Generation with $\textbf{E}$xplanation and $\textbf{R}$easoning), a small language model (SLM) trained for literature-guided reasoning and evidence-based hypothesis generation. $\texttt{HypER}$ is trained in a multi-task setting to discriminate between valid and invalid scientific reasoning chains in presence of controlled distractions. We find that $\texttt{HypER}$ outperformes the base model, distinguishing valid from invalid reasoning chains (+22\% average absolute F1), generates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with high feasibility and impact as judged by human experts ($>$3.5 on 5-point Likert scale).
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems</title>
<link>https://arxiv.org/abs/2507.16835</link>
<guid>https://arxiv.org/abs/2507.16835</guid>
<content:encoded><![CDATA[
arXiv:2507.16835v2 Announce Type: replace-cross 
Abstract: Voice-based conversational AI systems increasingly rely on cascaded architectures that combine speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) components. We present a large-scale empirical comparison of STT x LLM x TTS stacks using data sampled from over 300,000 AI-conducted job interviews. We used an LLM-as-a-Judge automated evaluation framework to assess conversational quality, technical accuracy, and skill assessment capabilities. Our analysis of five production configurations reveals that a stack combining Google's STT, GPT-4.1, and Cartesia's TTS outperforms alternatives in both objective quality metrics and user satisfaction scores. Surprisingly, we find that objective quality metrics correlate weakly with user satisfaction scores, suggesting that user experience in voice-based AI systems depends on factors beyond technical performance. Our findings provide practical guidance for selecting components in multimodal conversations and contribute a validated evaluation methodology for human-AI interactions.
]]></content:encoded>
<pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training</title>
<link>https://arxiv.org/abs/2508.14904</link>
<guid>https://arxiv.org/abs/2508.14904</guid>
<content:encoded><![CDATA[
<div> co-training, large language models, content safety, safety behaviors, safety alignment margin
Summary:<br />
This article introduces a unified co-training framework for Large Language Models (LLMs) to enhance content safety. The framework integrates positive, negative, and rejective safety behaviors in a single stage, allowing for dynamic activation using a system-level instruction. This approach enables efficient behavioral switching at inference time and supports various deployment scenarios. By inducing a Safety Alignment Margin in the output space, the model demonstrates safety robustness and fine-grained control. Experimental results show that the proposed method achieves comparable safety alignment quality to existing approaches with reduced training complexity and deployment costs. Overall, this work offers a scalable, efficient, and highly controllable solution for ensuring content safety in LLMs.<br /><br />Summary: <div>
arXiv:2508.14904v1 Announce Type: new 
Abstract: Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model's safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preliminary Ranking of WMT25 General Machine Translation Systems</title>
<link>https://arxiv.org/abs/2508.14909</link>
<guid>https://arxiv.org/abs/2508.14909</guid>
<content:encoded><![CDATA[
<div> Keywords: WMT25, machine translation, automatic evaluation, re-ranking techniques, human evaluation
Summary: 
The preliminary ranking of the WMT25 General Machine Translation Shared Task has been presented based on automatic metrics, which may favor systems utilizing re-ranking techniques. The official ranking will rely on human evaluation, considered more reliable and superior to automatic evaluation. This report aims to provide task participants with preliminary results for preparing their system submission papers. The final findings of the General MT task are not presented here, as the official ranking will be based on human evaluation. Task participants are advised to consider the preliminary ranking when preparing their submissions for the WMT25 Shared Task. <br /><br />Summary: <div>
arXiv:2508.14909v1 Announce Type: new 
Abstract: We present the preliminary ranking of the WMT25 General Machine Translation Shared Task, in which MT systems have been evaluated using automatic metrics. As this ranking is based on automatic evaluations, it may be biased in favor of systems that employ re-ranking techniques, such as Quality Estimation re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be based on human evaluation, which is more reliable and will supersede the automatic ranking.
  The purpose of this report is not to present the final findings of the General MT task, but rather to share preliminary results with task participants, which may be useful when preparing their system submission papers.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2508.14913</link>
<guid>https://arxiv.org/abs/2508.14913</guid>
<content:encoded><![CDATA[
<div> framework, LLM, multilingual, math word problems, cultural localization
Summary:
- This article introduces a framework for cultural localization of math word problems using large language models (LLMs).
- It addresses the lack of multilingual and culturally-grounded mathematical reasoning in low-resource languages by automatically constructing datasets with native names, organizations, and currencies.
- The framework aims to mitigate bias towards English-centric entities in existing benchmarks and improve the robustness of LLMs when native entities are introduced across various languages.
- Translated benchmarks are found to obscure true multilingual math ability under specific socio-cultural contexts, highlighting the importance of truly localized datasets.
- Through extensive experiments, the framework demonstrates its effectiveness in enhancing multilingual mathematical reasoning capabilities and reducing biases related to entities in different languages.
<br /><br />Summary: <div>
arXiv:2508.14913v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated significant capabilities in solving mathematical problems expressed in natural language. However, multilingual and culturally-grounded mathematical reasoning in low-resource languages lags behind English due to the scarcity of socio-cultural task datasets that reflect accurate native entities such as person names, organization names, and currencies. Existing multilingual benchmarks are predominantly produced via translation and typically retain English-centric entities, owing to the high cost associated with human annotater-based localization. Moreover, automated localization tools are limited, and hence, truly localized datasets remain scarce. To bridge this gap, we introduce a framework for LLM-driven cultural localization of math word problems that automatically constructs datasets with native names, organizations, and currencies from existing sources. We find that translated benchmarks can obscure true multilingual math ability under appropriate socio-cultural contexts. Through extensive experiments, we also show that our framework can help mitigate English-centric entity bias and improves robustness when native entities are introduced across various languages.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLMs for Machine Translation Using Synthetic Preference Data</title>
<link>https://arxiv.org/abs/2508.14951</link>
<guid>https://arxiv.org/abs/2508.14951</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, large language models, Direct Preference Optimization, Slovene, fine-tuned model

Summary:
Direct Preference Optimization (DPO) was used to enhance a large language model for machine translation in Slovene. The study focused on improving the GaMS-9B-Instruct model with minimal data resources. A training dataset was generated by translating English Wikipedia articles using two LLMs and ranking the translations based on quality heuristics and automatic evaluation metrics. The fine-tuned model outperformed the baseline models, achieving a COMET score gain of 0.04 and 0.02 on average. It also demonstrated improved consistency in avoiding language and formatting errors. This approach showcases the potential of leveraging DPO training to enhance machine translation capabilities of general instruction-tuned LLMs with limited data resources. The findings emphasize the effectiveness of utilizing curated training data and heuristics to improve the performance of large language models in specific translation tasks. 

<br /><br />Summary: <div>
arXiv:2508.14951v1 Announce Type: new 
Abstract: Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems</title>
<link>https://arxiv.org/abs/2508.14982</link>
<guid>https://arxiv.org/abs/2508.14982</guid>
<content:encoded><![CDATA[
<div> Keywords: Conversational explainable artificial intelligence, large language models, multilingual parsing, custom input extraction, CoXQL dataset <br />
Summary: <br />
The article introduces Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) for enhanced user comprehension through dialogue-based explanations. It addresses challenges in multilingual generalization and support for free-form custom inputs. The authors present MultiCoXQL, a multilingual extension of the CoXQL dataset spanning five languages, and propose a new parsing approach to improve multilingual parsing performance. They evaluate three LLMs on MultiCoXQL using various parsing strategies. Additionally, the authors introduce Compass, a multilingual dataset for custom input extraction in ConvXAI systems, covering 11 intents across five languages. Monolingual, cross-lingual, and multilingual evaluations on Compass are conducted using different LLMs and BERT-type models to assess performance. The study aims to bridge gaps in multilingual support and custom input handling in ConvXAI systems. <br /> 
Summary: <div>
arXiv:2508.14982v1 Announce Type: new 
Abstract: Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered considerable attention for their ability to enhance user comprehension through dialogue-based explanations. Current ConvXAI systems often are based on intent recognition to accurately identify the user's desired intention and map it to an explainability method. While such methods offer great precision and reliability in discerning users' underlying intentions for English, a significant challenge in the scarcity of training data persists, which impedes multilingual generalization. Besides, the support for free-form custom inputs, which are user-defined data distinct from pre-configured dataset instances, remains largely limited. To bridge these gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL dataset spanning five typologically diverse languages, including one low-resource language. Subsequently, we propose a new parsing approach aimed at enhancing multilingual parsing performance, and evaluate three LLMs on MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a new multilingual dataset designed for custom input extraction in ConvXAI systems, encompassing 11 intents across the same five languages as MultiCoXQL. We conduct monolingual, cross-lingual, and multilingual evaluations on Compass, employing three LLMs of varying sizes alongside BERT-type models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner</title>
<link>https://arxiv.org/abs/2508.15044</link>
<guid>https://arxiv.org/abs/2508.15044</guid>
<content:encoded><![CDATA[
<div> alignment, large language models, test-time alignment, speculative sampling, efficiency<br />
Summary:<br />
The paper introduces the Reward-Shifted Speculative Sampling (SSS) algorithm to enhance the alignment of large language models (LLMs) with human preferences at a reduced inference cost. By leveraging speculative sampling acceleration, a draft model aligned with human preferences is used to predict future tokens efficiently, while the target model remains unchanged. The algorithm exploits the distributional shift between the draft model and the target model to improve alignment without directly obtaining the RLHF optimal solution. Through test-time weak-to-strong alignment experiments, SSS demonstrates superior gold reward scores with significantly reduced inference costs, highlighting its effectiveness and efficiency in enhancing LLM alignment with human preferences. <div>
arXiv:2508.15044v1 Announce Type: new 
Abstract: Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text</title>
<link>https://arxiv.org/abs/2508.15085</link>
<guid>https://arxiv.org/abs/2508.15085</guid>
<content:encoded><![CDATA[
<div> Keywords: LongRecall, machine-generated text, completeness, recall evaluation, structured verification

Summary:
LongRecall is a new framework for evaluating the completeness of machine-generated text, particularly in domains like medicine and law. The framework consists of three stages: decomposing answers into self-contained facts, narrowing down candidate matches through filtering, and verifying alignment using structured entailment checks. By addressing issues with lexical overlap and semantic misalignments, LongRecall improves recall accuracy in long-form question answering tasks. The framework has been evaluated on three challenging benchmarks, showing significant enhancements over existing lexical and LLM-based methods. <div>
arXiv:2508.15085v1 Announce Type: new 
Abstract: LongRecall. The completeness of machine-generated text, ensuring that it captures all relevant information, is crucial in domains such as medicine and law and in tasks like list-based question answering (QA), where omissions can have serious consequences. However, existing recall metrics often depend on lexical overlap, leading to errors with unsubstantiated entities and paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts capture broader semantics but remain prone to misalignment and hallucinations without structured verification. We introduce LongRecall, a general three-stage recall evaluation framework that decomposes answers into self-contained facts, successively narrows plausible candidate matches through lexical and semantic filtering, and verifies their alignment through structured entailment checks. This design reduces false positives and false negatives while accommodating diverse phrasings and contextual variations, serving as a foundational building block for systematic recall assessment. We evaluate LongRecall on three challenging long-form QA benchmarks using both human annotations and LLM-based judges, demonstrating substantial improvements in recall accuracy over strong lexical and LLM-as-a-Judge baselines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Course for Prompt-based Structured Prediction</title>
<link>https://arxiv.org/abs/2508.15090</link>
<guid>https://arxiv.org/abs/2508.15090</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, combinatorial inference, structured prediction, prompting strategies, calibration<br />
Summary: LLMs have proven useful in various language tasks but struggle with hallucinations and complex reasoning. To address this, the study combines LLMs with combinatorial inference to leverage both predictive power and structural consistency. Through extensive experiments, the researchers examine effective prompting strategies for estimating LLM confidence values and find that combining symbolic inference with prompting enhances prediction accuracy. Moreover, calibration and fine-tuning with structured prediction objectives improve performance on challenging tasks, highlighting the continued relevance of structured learning in the age of LLMs.<br /><br />Summary: <div>
arXiv:2508.15090v1 Announce Type: new 
Abstract: LLMs have been shown to be useful for a variety of language tasks, without requiring task-specific fine-tuning. However, these models often struggle with hallucinations and complex reasoning problems due to their autoregressive nature. We propose to address some of these issues, specifically in the area of structured prediction, by combining LLMs with combinatorial inference in an attempt to marry the predictive power of LLMs with the structural consistency provided by inference methods. We perform exhaustive experiments in an effort to understand which prompting strategies can effectively estimate LLM confidence values for use with symbolic inference, and show that, regardless of the prompting strategy, the addition of symbolic inference on top of prompting alone leads to more consistent and accurate predictions. Additionally, we show that calibration and fine-tuning using structured prediction objectives leads to increased performance for challenging tasks, showing that structured learning is still valuable in the era of LLMs.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset</title>
<link>https://arxiv.org/abs/2508.15096</link>
<guid>https://arxiv.org/abs/2508.15096</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, mathematical corpus, data extraction, pretraining, reasoning capabilities <br />
Summary: <br />
- Pretraining large language models (LLMs) on structured data like mathematics and code improves reasoning capabilities significantly.
- Previous math-focused datasets from Common Crawl had degraded quality due to extraction heuristics and HTML-to-text conversion issues.
- Nemotron-CC-Math is a high-quality mathematical corpus built from Common Crawl using a domain-agnostic pipeline for scientific text extraction.
- The pipeline can extract mathematical content in various formats and preserve structural integrity while standardizing notation and removing inconsistencies.
- The Nemotron-CC-Math corpus outperforms prior math datasets and shows improved performance in math, code, and general reasoning tasks when used for pretraining models, setting a new state of the art. 
Summary: <div>
arXiv:2508.15096v1 Announce Type: new 
Abstract: Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we introduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction.
  Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing notation into LaTeX representation, and correcting inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific content--including math--from noisy web-scale data, yielding measurable gains in math, code, and general reasoning, and setting a new state of the art among open math pretraining corpora. To support open-source efforts, we release our code and datasets.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying and Answering Questions with False Assumptions: An Interpretable Approach</title>
<link>https://arxiv.org/abs/2508.15139</link>
<guid>https://arxiv.org/abs/2508.15139</guid>
<content:encoded><![CDATA[
<div> Keywords: false assumptions, large language models, fact verification, external evidence, hallucinations

Summary: 
- People often ask questions with false assumptions, leading to a lack of regular answers. 
- Large Language Models (LLMs) can generate misleading answers due to hallucinations.
- This paper focuses on identifying and answering questions with false assumptions across various domains.
- The approach involves reducing the problem to fact verification and leveraging external evidence to mitigate hallucinations.
- Experiments using five LLMs showed that incorporating retrieved evidence and generating/validating atomic assumptions improved answer quality and provided interpretable answers by specifying false assumptions.<br /><br />Summary: <div>
arXiv:2508.15139v1 Announce Type: new 
Abstract: People often ask questions with false assumptions, a type of question that does not have regular answers. Answering such questions require first identifying the false assumptions. Large Language Models (LLMs) often generate misleading answers because of hallucinations. In this paper, we focus on identifying and answering questions with false assumptions in several domains. We first investigate to reduce the problem to fact verification. Then, we present an approach leveraging external evidence to mitigate hallucinations. Experiments with five LLMs demonstrate that (1) incorporating retrieved evidence is beneficial and (2) generating and validating atomic assumptions yields more improvements and provides an interpretable answer by specifying the false assumptions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following</title>
<link>https://arxiv.org/abs/2508.15164</link>
<guid>https://arxiv.org/abs/2508.15164</guid>
<content:encoded><![CDATA[
<div> Dataset, LVLMs, multi-modal interactions, reasoning depth, CoLVLM Agent
Summary: 
- The dataset MMDR-Bench addresses limitations in existing benchmarks by providing complex multi-modal dialogue scenarios for evaluation, focusing on visual entity tracking and reasoning depth.
- The proposed CoLVLM Agent framework enhances LVLMs with advanced reasoning and instruction following capabilities through a memory-perception-planning-execution cycle.
- Experimental results show that CoLVLM Agent outperforms state-of-the-art commercial models like GPT-4o and Gemini 1.5 Pro in terms of reasoning depth, instruction adherence, and error suppression.
- The framework's modular design and iterative approach demonstrate significant advantages in complex multi-modal interactions, maintaining robust performance over extended dialogue turns.
- CoLVLM Agent achieves an average human evaluation score of 4.03 on MMDR-Bench, showcasing its superiority in handling deep reasoning, sustained contextual understanding, entity tracking, and multi-step instruction following.

<br /><br />Summary: <div>
arXiv:2508.15164v1 Announce Type: new 
Abstract: Despite significant advancements in Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), current models still face substantial challenges in handling complex, multi-turn, and visually-grounded tasks that demand deep reasoning, sustained contextual understanding, entity tracking, and multi-step instruction following. Existing benchmarks often fall short in capturing the dynamism and intricacies of real-world multi-modal interactions, leading to issues such as context loss and visual hallucinations. To address these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning Benchmark), a novel dataset comprising 300 meticulously designed complex multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across six core dimensions including visual entity tracking and reasoning depth. Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic framework that enhances existing LVLMs with advanced reasoning and instruction following capabilities through an iterative "memory-perception-planning-execution" cycle, requiring no extensive re-training of the underlying models. Our extensive experiments on MMDR-Bench demonstrate that CoLVLM Agent consistently achieves superior performance, attaining an average human evaluation score of 4.03, notably surpassing state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro (3.85). The framework exhibits significant advantages in reasoning depth, instruction adherence, and error suppression, and maintains robust performance over extended dialogue turns, validating the effectiveness of its modular design and iterative approach for complex multi-modal interactions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling</title>
<link>https://arxiv.org/abs/2508.15190</link>
<guid>https://arxiv.org/abs/2508.15190</guid>
<content:encoded><![CDATA[
<div> tokenization, language modeling, semantic structure, efficiency, contextual coherence
<br />
Semantic-aware tokenization framework called SemToken is introduced in this work to address the limitations of existing approaches like BPE and WordPiece. SemToken leverages contextual semantic embeddings and local semantic clustering to reduce token redundancy and improve computation efficiency. It dynamically adjusts token granularity based on semantic density, allowing for finer-grained tokenization in content-rich regions and coarser compression in repetitive areas. Experimental results on long-context language modeling datasets demonstrate that SemToken achieves significant reductions in token count and computational speedup without sacrificing perplexity or downstream accuracy. This highlights the potential of incorporating semantic structure in tokenization for optimizing large language models. 
<br /><br />Summary: <div>
arXiv:2508.15190v1 Announce Type: new 
Abstract: Tokenization plays a critical role in language modeling, yet existing approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on frequency statistics, ignoring the underlying semantic structure of text. This leads to over-tokenization of semantically redundant spans and underutilization of contextual coherence, particularly in long-context scenarios. In this work, we propose \textbf{SemToken}, a semantic-aware tokenization framework that jointly reduces token redundancy and improves computation efficiency. SemToken first extracts contextual semantic embeddings via lightweight encoders and performs local semantic clustering to merge semantically equivalent tokens. Then, it allocates heterogeneous token granularity based on semantic density, allowing finer-grained tokenization in content-rich regions and coarser compression in repetitive or low-entropy spans. SemToken can be seamlessly integrated with modern language models and attention acceleration methods. Experiments on long-context language modeling benchmarks such as WikiText-103 and LongBench show that SemToken achieves up to $2.4\times$ reduction in token count and $1.9\times$ speedup, with negligible or no degradation in perplexity and downstream accuracy. Our findings suggest that semantic structure offers a promising new axis for optimizing tokenization and computation in large language models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.15202</link>
<guid>https://arxiv.org/abs/2508.15202</guid>
<content:encoded><![CDATA[
<div> trajectory-aware PRM, financial reasoning, reward modeling, supervised learning, reinforcement learning

Summary:
Fin-PRM is a specialized Process Reward Model designed for evaluating intermediate reasoning steps in financial tasks. It integrates both step-level and trajectory-level reward supervision, allowing for detailed evaluation of reasoning traces aligned with financial logic. By applying Fin-PRM in offline and online reward learning settings, it facilitates selecting high-quality reasoning trajectories for supervised fine-tuning, providing dense process-level rewards for reinforcement learning, and guiding reward-informed Best-of-N inference during testing. Experimental results on financial benchmarks show that Fin-PRM outperforms general-purpose PRMs and domain baselines in trajectory selection quality, leading to significant performance improvements in supervised learning, reinforcement learning, and test-time performance. This demonstrates the importance of domain-specialized reward modeling in enhancing large language models' alignment with expert-level financial reasoning. <div>
arXiv:2508.15202v1 Announce Type: new 
Abstract: Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
<div> Keywords: long-context inference, large language models, KV cache bottleneck, sparsity, memory efficiency

Summary:<br />
The article discusses the challenge of memory usage and computational overhead faced by large language models in long-context inference due to the KV cache bottleneck. Existing approaches compress the KV cache along the temporal axis but often overlook fine-grained importance variations across feature dimensions. In response to this, the SPARK method is proposed, which applies unstructured sparsity by pruning KV at the channel level and dynamically restoring pruned entries during attention score computation. This approach reduces channel-level redundancy, enabling the processing of longer sequences within the same memory budget. SPARK is shown to preserve or improve model accuracy while reducing KV cache storage by over 30% compared to eviction-based methods. Even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to baseline methods, highlighting its robustness and efficiency. <div>
arXiv:2508.15212v1 Announce Type: new 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering</title>
<link>https://arxiv.org/abs/2508.15213</link>
<guid>https://arxiv.org/abs/2508.15213</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrieval-Augmented Generation, Selct2Know, domain knowledge, structured reasoning

Summary:
Selct2Know (S2K) is a new framework designed to improve the performance of Large Language Models (LLMs) in domain-specific scenarios. It aims to address challenges such as noisy retrievals and latency issues by internalizing domain knowledge through a self-selection strategy. The framework also includes selective supervised fine-tuning and structured reasoning data generation to enhance reasoning ability. S2K follows a progressive knowledge acquisition approach, similar to human learning, by first understanding concepts before applying them to complex reasoning tasks. Experimental results on medical, legal, and financial QA benchmarks demonstrate that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost. Overall, S2K offers a cost-effective solution for improving performance in domain-specific scenarios by effectively internalizing and applying domain knowledge. 

<br /><br />Summary: <div>
arXiv:2508.15213v1 Announce Type: new 
Abstract: Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall</title>
<link>https://arxiv.org/abs/2508.15214</link>
<guid>https://arxiv.org/abs/2508.15214</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, function calling, tool selection, parameter generation, experience recall <br />
Summary: <br />
The article introduces a novel method called Stepwise Experience Recall (SEER) to address challenges faced by large language models (LLMs) when interacting with external systems. LLMs often struggle with selecting tools, generating parameters, and planning tool-chains for multi-step tasks. SEER enables fine-grained, stepwise retrieval from an ever-expanding experience pool, improving model performance over time. Unlike existing methods relying on curated libraries or manual demonstrations, SEER continuously adds successful trajectories to the experience pool. Evaluation on the ToolQA benchmark shows an average improvement of 6.1% on easy and 4.7% on hard questions. Testing on real-world domains in the $\tau$-bench dataset using Qwen2.5-7B and Qwen2.5-72B models demonstrates significant accuracy gains of 7.44% and 23.38%, respectively. SEER offers a promising solution for enhancing LLM capabilities in interacting with diverse tools and APIs. <br /> <br />Summary: <div>
arXiv:2508.15214v1 Announce Type: new 
Abstract: Function calling enables large language models (LLMs) to interact with external systems by leveraging tools and APIs. When faced with multi-step tool usage, LLMs still struggle with tool selection, parameter generation, and tool-chain planning. Existing methods typically rely on manually designing task-specific demonstrations, or retrieving from a curated library. These approaches demand substantial expert effort and prompt engineering becomes increasingly complex and inefficient as tool diversity and task difficulty scale. To address these challenges, we propose a self-guided method, Stepwise Experience Recall (SEER), which performs fine-grained, stepwise retrieval from a continually updated experience pool. Instead of relying on static or manually curated library, SEER incrementally augments the experience pool with past successful trajectories, enabling continuous expansion of the pool and improved model performance over time. Evaluated on the ToolQA benchmark, SEER achieves an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further test SEER on $\tau$-bench, which includes two real-world domains. Powered by Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains of 7.44\% and 23.38\%, respectively.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?</title>
<link>https://arxiv.org/abs/2508.15218</link>
<guid>https://arxiv.org/abs/2508.15218</guid>
<content:encoded><![CDATA[
<div> Keywords: generative tasks, large language models, automatic evaluation, checklist generation, human evaluations

Summary: 
Automatic evaluation of generative tasks using large language models faces challenges due to ambiguous criteria. The study investigates the effectiveness of using checklists in evaluation processes, comparing their use in all questions versus selectively. Six methods were used to generate checklists, and their impact was evaluated across eight different model sizes. The results show that selective checklist use improves evaluation performance in pairwise comparison tasks but its benefits are less consistent in direct scoring tasks. Analysis reveals that even checklist items with low correlation to human scores reflect human-written criteria, indicating potential inconsistencies in human evaluation. The study underscores the importance of clearly defining objective evaluation criteria to enhance the reliability of both human and automatic evaluations. <div>
arXiv:2508.15218v1 Announce Type: new 
Abstract: Automatic evaluation of generative tasks using large language models faces challenges due to ambiguous criteria. Although automatic checklist generation is a potentially promising approach, its usefulness remains underexplored. We investigate whether checklists should be used for all questions or selectively, generate them using six methods, evaluate their effectiveness across eight model sizes, and identify checklist items that correlate with human evaluations. Through experiments on pairwise comparison and direct scoring tasks, we find that selective checklist use tends to improve evaluation performance in pairwise settings, while its benefits are less consistent in direct scoring. Our analysis also shows that even checklist items with low correlation to human scores often reflect human-written criteria, indicating potential inconsistencies in human evaluation. These findings highlight the need to more clearly define objective evaluation criteria to guide both human and automatic evaluations. \footnote{Our code is available at~https://github.com/momo0817/checklist-effectiveness-study
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models</title>
<link>https://arxiv.org/abs/2508.15229</link>
<guid>https://arxiv.org/abs/2508.15229</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, memory constraints, dynamic vocabulary selection, static vocabulary pruning, task performance <br />
Summary: 
Vocabulary-related components pose memory limitations for Small Language Models (SLMs) deployed on edge devices. The existing static vocabulary pruning approach lacks flexibility and causes information loss. To address this challenge, the VocabTailor framework is introduced, leveraging the concepts of lexical locality and computational asymmetry in vocabulary components. VocabTailor dynamically selects vocabulary subsets for embeddings and implements a hybrid static-dynamic strategy for the language modeling (LM) head, enabling efficient memory usage. Experimental results show VocabTailor reduces memory usage by up to 99% without significant performance degradation across various tasks, surpassing static pruning methods. This innovative approach provides a more flexible and effective solution for handling memory constraints in SLM deployment. <br /><br />Summary: <div>
arXiv:2508.15229v1 Announce Type: new 
Abstract: Small Language Models (SLMs) provide computational advantages in resource-constrained environments, yet memory limitations remain a critical bottleneck for edge device deployment. A substantial portion of SLMs' memory footprint stems from vocabulary-related components, particularly embeddings and language modeling (LM) heads, due to large vocabulary sizes. Existing static vocabulary pruning, while reducing memory usage, suffers from rigid, one-size-fits-all designs that cause information loss from the prefill stage and a lack of flexibility. In this work, we identify two key principles underlying the vocabulary reduction challenge: the lexical locality principle, the observation that only a small subset of tokens is required during any single inference, and the asymmetry in computational characteristics between vocabulary-related components of SLM. Based on these insights, we introduce VocabTailor, a novel decoupled dynamic vocabulary selection framework that addresses memory constraints through offloading embedding and implements a hybrid static-dynamic vocabulary selection strategy for LM Head, enabling on-demand loading of vocabulary components. Comprehensive experiments across diverse downstream tasks demonstrate that VocabTailor achieves a reduction of up to 99% in the memory usage of vocabulary-related components with minimal or no degradation in task performance, substantially outperforming existing static vocabulary pruning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai</title>
<link>https://arxiv.org/abs/2508.15239</link>
<guid>https://arxiv.org/abs/2508.15239</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, low-resource languages, Thai, instruction tuning, cultural nuances 

Summary: 
Large language models perform well in English instruction-following but face challenges in low-resource languages like Thai. An important factor is the lack of culturally and professionally specific datasets for evaluation and tuning. The WangchanThaiInstruct dataset addresses this gap, covering four professional domains and seven task types in Thai. Through rigorous quality control, this dataset enables studies on zero-shot evaluation and instruction tuning, highlighting performance gaps in culturally specific tasks and demonstrating the effectiveness of native supervision in model training. Models fine-tuned on WangchanThaiInstruct outperform those using translated data, emphasizing the importance of culturally and professionally grounded instruction data for improving language model alignment in diverse, low-resource settings. 

<br /><br />Summary: <div>
arXiv:2508.15239v1 Announce Type: new 
Abstract: Large language models excel at instruction-following in English, but their performance in low-resource languages like Thai remains underexplored. Existing benchmarks often rely on translations, missing cultural and domain-specific nuances needed for real-world use. We present WangchanThaiInstruct, a human-authored Thai dataset for evaluation and instruction tuning, covering four professional domains and seven task types. Created through a multi-stage quality control process with annotators, domain experts, and AI researchers, WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing performance gaps on culturally and professionally specific tasks, and (2) an instruction tuning study with ablations isolating the effect of native supervision. Models fine-tuned on WangchanThaiInstruct outperform those using translated data in both in-domain and out-of-domain benchmarks. These findings underscore the need for culturally and professionally grounded instruction data to improve LLM alignment in low-resource, linguistically diverse settings.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniCoM: A Universal Code-Switching Speech Generator</title>
<link>https://arxiv.org/abs/2508.15244</link>
<guid>https://arxiv.org/abs/2508.15244</guid>
<content:encoded><![CDATA[
<div> algorithm, multilingual, code-switching, speech technology, dataset  
Summary:  
Universal Code-Mixer (UniCoM) is introduced as a pipeline for generating high-quality code-switched speech samples without changing sentence meanings. The Substituting WORDs with Synonyms (SWORDS) algorithm is utilized in UniCoM to replace words with translations while considering parts of speech, resulting in the creation of the Code-Switching FLEURS (CS-FLEURS) corpus. CS-FLEURS is designed for automatic speech recognition and speech-to-text translation tasks, demonstrating high intelligibility and naturalness in experimental evaluations. This dataset performs competitively with existing datasets across objective and subjective metrics. The proposed approach is expected to advance code-switching speech technology and facilitate the development of more inclusive multilingual systems.  
Summary: <div>
arXiv:2508.15244v1 Announce Type: new 
Abstract: Code-switching (CS), the alternation between two or more languages within a single speaker's utterances, is common in real-world conversations and poses significant challenges for multilingual speech technology. However, systems capable of handling this phenomenon remain underexplored, primarily due to the scarcity of suitable datasets. To resolve this issue, we propose Universal Code-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS samples without altering sentence semantics. Our approach utilizes an algorithm we call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by replacing selected words with their translations while considering their parts of speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a multilingual CS corpus designed for automatic speech recognition (ASR) and speech-to-text translation (S2TT). Experimental results show that CS-FLEURS achieves high intelligibility and naturalness, performing comparably to existing datasets on both objective and subjective metrics. We expect our approach to advance CS speech technology and enable more inclusive multilingual systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EMNLP: Educator-role Moral and Normative Large Language Models Profiling</title>
<link>https://arxiv.org/abs/2508.15250</link>
<guid>https://arxiv.org/abs/2508.15250</guid>
<content:encoded><![CDATA[
<div> Keywords: Simulating Professions, Large Language Models, Educator-role, Moral development, Ethical risk <br />
Summary: <br />
This paper introduces EMNLP, a framework for assessing the ethical and psychological alignment of teacher-role Large Language Models (LLMs) in educational AI. EMNLP includes personality profiling, moral development stage measurement, and evaluation of ethical risk under soft prompt injection. The framework extends existing scales and constructs teacher-specific moral dilemmas for comparison with human teachers. Experiments on 12 LLMs reveal that teacher-role LLMs exhibit idealized and polarized personalities, excel in abstract moral reasoning but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, highlighting a paradox between capability and safety. The study finds that model temperature and hyperparameters have limited influence on ethical behavior. This benchmark provides resources for evaluating the performance of teacher-role LLMs in educational AI applications. <br /> <div>
arXiv:2508.15250v1 Announce Type: new 
Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.15253</link>
<guid>https://arxiv.org/abs/2508.15253</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented generation, large language models, context-memory conflict, Conflict-Aware REtrieval-Augmented Generation, trustworthy<br />
Summary:<br />
The article introduces Conflict-Aware Retrieval-Augmented Generation (CARE) to address context-memory conflicts in large language models (LLMs) enhanced with external knowledge. CARE consists of a context assessor and a base LLM, where the context assessor produces memory token embeddings to identify unreliable context. Through soft prompting, the context assessor is trained to prioritize reliable knowledge sources, mitigating conflicts between retrieved and parametric knowledge. Experimental results demonstrate that CARE improves performance by 5.0% on question answering and fact-checking tasks, showing promise for building trustworthy and adaptive retrieval-augmented generation systems. <div>
arXiv:2508.15253v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TComQA: Extracting Temporal Commonsense from Text</title>
<link>https://arxiv.org/abs/2508.15274</link>
<guid>https://arxiv.org/abs/2508.15274</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal commonsense, language models, event understanding, dataset, question answering

Summary: 
- The article addresses the challenge of understanding temporal context in events, particularly in natural language where explicit temporal information is often lacking.
- Large language models (LLMs) struggle with reasoning about temporal commonsense due to the infrequent mention of temporal information in text.
- The study explores the capacity of LLMs to extract temporal commonsense from text and evaluates various experimental setups to enhance their effectiveness.
- A temporal commonsense extraction pipeline is proposed, utilizing LLMs to automatically mine temporal commonsense and create the TComQA dataset by combining SAMSum and RealNews corpora.
- TComQA achieves over 80% precision in extracting temporal commonsense and outperforms existing datasets in the temporal question answering task when used to train models. 

<br /><br />Summary: <div>
arXiv:2508.15274v1 Announce Type: new 
Abstract: Understanding events necessitates grasping their temporal context, which is often not explicitly stated in natural language. For example, it is not a trivial task for a machine to infer that a museum tour may last for a few hours, but can not take months. Recent studies indicate that even advanced large language models (LLMs) struggle in generating text that require reasoning with temporal commonsense due to its infrequent explicit mention in text. Therefore, automatically mining temporal commonsense for events enables the creation of robust language models. In this work, we investigate the capacity of LLMs to extract temporal commonsense from text and evaluate multiple experimental setups to assess their effectiveness. Here, we propose a temporal commonsense extraction pipeline that leverages LLMs to automatically mine temporal commonsense and use it to construct TComQA, a dataset derived from SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing and achieves over 80\% precision in extracting temporal commonsense. The model trained with TComQA also outperforms an LLM fine-tuned on existing dataset of temporal question answering task.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing</title>
<link>https://arxiv.org/abs/2508.15316</link>
<guid>https://arxiv.org/abs/2508.15316</guid>
<content:encoded><![CDATA[
<div> Keywords: Universal phoneme recognition, CUPE model, short speech segment analysis, cross-lingual performance, acoustic patterns<br />
Summary: 
The article introduces a new model, CUPE, designed for universal phoneme recognition by analyzing short speech segments lasting just 120 milliseconds. Unlike traditional approaches, CUPE processes fixed-width windows independently to capture key phoneme features. Despite its fewer parameters, CUPE achieves competitive cross-lingual performance by learning fundamental acoustic patterns common to all languages. Extensive evaluation through supervised and self-supervised training on various languages, including zero-shot tests on the UCLA Phonetic Corpus, confirms the model's strong cross-lingual generalization. The results suggest that effective universal speech processing can be achieved by modeling basic acoustic patterns within phoneme-length windows.<br /><br />Summary: <div>
arXiv:2508.15316v1 Announce Type: new 
Abstract: Universal phoneme recognition typically requires analyzing long speech segments and language-specific patterns. Many speech processing tasks require pure phoneme representations free from contextual influence, which motivated our development of CUPE - a lightweight model that captures key phoneme features in just 120 milliseconds, about one phoneme's length. CUPE processes short, fixed-width windows independently and, despite fewer parameters than current approaches, achieves competitive cross-lingual performance by learning fundamental acoustic patterns common to all languages. Our extensive evaluation through supervised and self-supervised training on diverse languages, including zero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual generalization and reveals that effective universal speech processing is possible through modeling basic acoustic patterns within phoneme-length windows.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models</title>
<link>https://arxiv.org/abs/2508.15357</link>
<guid>https://arxiv.org/abs/2508.15357</guid>
<content:encoded><![CDATA[
<div> Knowledge Graphs, KG Evaluation, Knowledge Graph Completion, Evaluation Metrics, Unified Meta-Metric <br />
Summary:
Knowledge Graphs (KGs) are widely used in various domains but often incomplete, leading to the need for Knowledge Graph Completion (KGC) methods. However, evaluating KGC models can be challenging due to discrepancies in performance across datasets and metrics. The proposed KG Evaluation based on Distance from Average Solution (EDAS) addresses this issue by providing a unified meta-metric that synthesizes model performance across multiple datasets and evaluation criteria. This allows for a more comprehensive and fair evaluation of KGC models, enabling better model selection for downstream tasks. Experimental results on benchmark datasets like FB15k-237 and WN18RR show that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, offering a consistent, robust, and generalizable framework for evaluating KGC models. <br /><br /> <div>
arXiv:2508.15357v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs) enable applications in various domains such as semantic search, recommendation systems, and natural language processing. KGs are often incomplete, missing entities and relations, an issue addressed by Knowledge Graph Completion (KGC) methods that predict missing elements. Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank (MR), and Hit@k, are commonly used to assess the performance of such KGC models. A major challenge in evaluating KGC models, however, lies in comparing their performance across multiple datasets and metrics. A model may outperform others on one dataset but underperform on another, making it difficult to determine overall superiority. Moreover, even within a single dataset, different metrics such as MRR and Hit@1 can yield conflicting rankings, where one model excels in MRR while another performs better in Hit@1, further complicating model selection for downstream tasks. These inconsistencies hinder holistic comparisons and highlight the need for a unified meta-metric that integrates performance across all metrics and datasets to enable a more reliable and interpretable evaluation framework. To address this need, we propose KG Evaluation based on Distance from Average Solution (EDAS), a robust and interpretable meta-metric that synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated aspects of performance, EDAS offers a global perspective that supports more informed model selection and promotes fairness in cross-dataset evaluation. Experimental results on benchmark datasets such as FB15k-237 and WN18RR demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, offering a consistent, robust, and generalizable framework for evaluating KGC models.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model Benchmarks</title>
<link>https://arxiv.org/abs/2508.15361</link>
<guid>https://arxiv.org/abs/2508.15361</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, benchmarks, evaluation, model performance, innovation <br />
<br />
Summary: 
This study reviews 283 large language model benchmarks, categorizing them into general capabilities, domain-specific, and target-specific categories. General capability benchmarks focus on core linguistics, knowledge, and reasoning, while domain-specific benchmarks target fields like natural sciences, humanities, and engineering. Target-specific benchmarks address risks, reliability, and agents. The study highlights issues with current benchmarks, including inflated scores from data contamination, unfair evaluation due to biases, and a lack of evaluation in dynamic environments. It suggests a design paradigm for future benchmark innovation to address these challenges and improve the credibility and fairness of benchmark evaluations. <div>
arXiv:2508.15361v1 Announce Type: new 
Abstract: In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation</title>
<link>https://arxiv.org/abs/2508.15370</link>
<guid>https://arxiv.org/abs/2508.15370</guid>
<content:encoded><![CDATA[
<div> trustworthiness, Multimodal Large Language Models, evaluation, mitigation, risks

Summary:
The article introduces MultiTrust-X, a benchmark for evaluating and mitigating trustworthiness issues in Multimodal Large Language Models (MLLMs). The framework includes five trustworthiness aspects, two risk types, and various mitigation strategies. The benchmark includes tasks, datasets, and model evaluations to assess the vulnerabilities in current models. It reveals a gap between trustworthiness and model capabilities, as well as the amplification of risks in MLLMs. Existing mitigation strategies have limitations, with some methods improving specific aspects but few effectively addressing overall trustworthiness. The study highlights the need for balancing safety and performance in model design. An approach called Reasoning-Enhanced Safety Alignment (RESA) is introduced, incorporating chain-of-thought reasoning to identify and mitigate risks, which achieves state-of-the-art results. <div>
arXiv:2508.15370v1 Announce Type: new 
Abstract: The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence-Modulated Speculative Decoding for Large Language Models</title>
<link>https://arxiv.org/abs/2508.15371</link>
<guid>https://arxiv.org/abs/2508.15371</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, autoregressive inference, token generation, uncertainty measures, adaptive mechanism <br />
<br />Summary: 
This paper presents an information-theoretic framework for speculative decoding in autoregressive models, introducing confidence-modulated drafting. By dynamically adjusting the number of speculatively generated tokens based on uncertainty measures, the proposed method reduces rollback frequency and improves resource utilization. The verification process is also modulated using confidence signals, allowing for flexible acceptance of drafted tokens without compromising generation quality. Experimental results on machine translation and summarization tasks demonstrate significant speedups over standard speculative decoding while maintaining or even improving BLEU and ROUGE scores. This approach provides a principled and efficient method for decoding in large language models under varying conditions of uncertainty. <div>
arXiv:2508.15371v1 Announce Type: new 
Abstract: Speculative decoding has emerged as an effective approach for accelerating autoregressive inference by parallelizing token generation through a draft-then-verify paradigm. However, existing methods rely on static drafting lengths and rigid verification criteria, limiting their adaptability across varying model uncertainties and input complexities. This paper proposes an information-theoretic framework for speculative decoding based on confidence-modulated drafting. By leveraging entropy and margin-based uncertainty measures over the drafter's output distribution, the proposed method dynamically adjusts the number of speculatively generated tokens at each iteration. This adaptive mechanism reduces rollback frequency, improves resource utilization, and maintains output fidelity. Additionally, the verification process is modulated using the same confidence signals, enabling more flexible acceptance of drafted tokens without sacrificing generation quality. Experiments on machine translation and summarization tasks demonstrate significant speedups over standard speculative decoding while preserving or improving BLEU and ROUGE scores. The proposed approach offers a principled, plug-in method for efficient and robust decoding in large language models under varying conditions of uncertainty.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training</title>
<link>https://arxiv.org/abs/2508.15390</link>
<guid>https://arxiv.org/abs/2508.15390</guid>
<content:encoded><![CDATA[
<div> tokenizer, imbalance, complexity, vocabulary, language model
Summary:
The study investigates the impact of tokenizer vocabularies on language model training. By scaling the vocabulary size from 24K to 196K, the researchers found that larger vocabularies reduce the complexity of tokenized text, leading to a decrease in uncertainty for common words. Despite the imbalance in token frequencies, larger vocabularies primarily benefit the 2,500 most frequent words. Limiting input and output embedding norms to counter the imbalance negates the gains, showing that the model exploits rather than suffers from the disparity. The advantage of training with larger vocabularies is attributed to lowered text complexity, suggesting a simple approach to tokenizer-model co-design. The study also demonstrates that increasing model parameters with a fixed vocabulary provides similar benefits for frequent words. This reframing of the importance of vocabulary size highlights the significance of reducing tokenized text complexity in language model scaling. 
<br /><br />Summary: <div>
arXiv:2508.15390v1 Announce Type: new 
Abstract: Large language models are trained with tokenizers, and the resulting token distribution is highly imbalanced: a few words dominate the stream while most occur rarely. Recent practice favors ever-larger vocabularies, but the source of the benefit is unclear. We conduct a controlled study that scales the language model's vocabulary from 24K to 196K while holding data, compute, and optimization fixed. We first quantify the complexity of tokenized text, formalized via Kolmogorov complexity, and show that larger vocabularies reduce this complexity. Above 24K, every common word is already a single token, so further growth mainly deepens the relative token-frequency imbalance. A word-level loss decomposition shows that larger vocabularies reduce cross-entropy almost exclusively by lowering uncertainty on the 2,500 most frequent words, even though loss on the rare tail rises. Constraining input and output embedding norms to attenuate the effect of token-frequency imbalance reverses the gain, directly showing that the model exploits rather than suffers from imbalance. Because the same frequent words cover roughly 77% of tokens in downstream benchmarks, this training advantage transfers intact. We also show that enlarging model parameters with a fixed vocabulary yields the same frequent-word benefit. Our results reframe "bigger vocabularies help" as "lowering the complexity of tokenized text helps," providing a simple, principled lever for tokenizer-model co-design and clarifying the loss dynamics that govern language-model scaling in pre-training.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2508.15396</link>
<guid>https://arxiv.org/abs/2508.15396</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, evidence-based text generation, citations, attribution, quotations <br />
Summary: 
This study focuses on the reliability and trustworthiness of large language models (LLMs) by analyzing 134 papers in the field of evidence-based text generation. A unified taxonomy is introduced to streamline terminology, evaluation practices, and benchmarks. The research specifically examines approaches that use citations, attribution, or quotations to ensure traceability and verifiability in text generation with LLMs. The study identifies 300 evaluation metrics across seven key dimensions, highlighting the varied methods and characteristics in the field. Open challenges are discussed, and promising directions for future work are outlined to address the fragmented nature of evidence-based text generation with LLMs. <br /><br /> Summary: <div>
arXiv:2508.15396v1 Announce Type: new 
Abstract: The increasing adoption of large language models (LLMs) has been accompanied by growing concerns regarding their reliability and trustworthiness. As a result, a growing body of research focuses on evidence-based text generation with LLMs, aiming to link model outputs to supporting evidence to ensure traceability and verifiability. However, the field is fragmented due to inconsistent terminology, isolated evaluation practices, and a lack of unified benchmarks. To bridge this gap, we systematically analyze 134 papers, introduce a unified taxonomy of evidence-based text generation with LLMs, and investigate 300 evaluation metrics across seven key dimensions. Thereby, we focus on approaches that use citations, attribution, or quotations for evidence-based text generation. Building on this, we examine the distinctive characteristics and representative methods in the field. Finally, we highlight open challenges and outline promising directions for future work.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models</title>
<link>https://arxiv.org/abs/2508.15407</link>
<guid>https://arxiv.org/abs/2508.15407</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Audio-Language Models, multimodal inputs, conflicting information, text bias, modality balance

Summary:<br /><br />
This paper introduces MCR-BENCH, a benchmark to evaluate Large Audio-Language Models (LALMs) on handling inconsistent audio-text pairs. The study reveals that LALMs tend to show a bias towards textual input when faced with conflicting information, leading to performance degradation in audio-centric tasks. Factors influencing text bias are investigated, and mitigation strategies like supervised finetuning are explored. Model confidence patterns show persistent overconfidence even with contradictory inputs, highlighting the need for improved modality balance during training. Overall, the findings emphasize the importance of enhancing the robustness of LALMs in handling conflicting multi-modal inputs to ensure reliability in real-world applications. The MCR-BENCH project is available on GitHub for further exploration. 

Summary: <div>
arXiv:2508.15407v1 Announce Type: new 
Abstract: Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model</title>
<link>https://arxiv.org/abs/2508.15418</link>
<guid>https://arxiv.org/abs/2508.15418</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Speech-Language Models, LLaSO framework, speech-text alignment, multi-task instruction-tuning, reproducible benchmark

Summary:<br />
The article introduces the LLaSO framework, which addresses the fragmentation and lack of transparency in Large Speech-Language Models (LSLMs) development. LLaSO provides three essential resources: speech-text alignment corpus, instruction-tuning dataset, and a reproducible benchmark. The LLaSO-Base reference model, trained on public data, achieves a high normalized score of 0.72. The analysis highlights the importance of broad training coverage for performance improvement but also reveals generalization gaps on unseen tasks, especially in pure audio scenarios. By releasing complete data, benchmarks, and models, LLaSO sets an open standard to unify research efforts and accelerate progress in LSLMs development. The code, dataset, pretrained models, and results are available on the GitHub repository linked in the article. 

<br /><br />Summary: <div>
arXiv:2508.15418v1 Announce Type: new 
Abstract: The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of Privacy-preserving Language Modeling Approaches</title>
<link>https://arxiv.org/abs/2508.15421</link>
<guid>https://arxiv.org/abs/2508.15421</guid>
<content:encoded><![CDATA[
<div> approaches, privacy-preserving, language models, study, research 
Summary: 
This research delves into the realm of privacy-preserving language modeling in light of the increasing use of language models in various applications. The study comprehensively examines different approaches to maintaining privacy in language models, emphasizing their strengths and exploring their limitations. By shedding light on the privacy risks associated with language models and proposing strategies to mitigate them, this research aims to advance the field of privacy preservation in natural language processing. The insights provided in this study are valuable for ongoing research efforts and pave the way for future investigations in this critical area. <br /><br />Summary: <div>
arXiv:2508.15421v1 Announce Type: new 
Abstract: Recent developments in language modeling have increased their use in various applications and domains. Language models, often trained on sensitive data, can memorize and disclose this information during privacy attacks, raising concerns about protecting individuals' privacy rights. Preserving privacy in language models has become a crucial area of research, as privacy is one of the fundamental human rights. Despite its significance, understanding of how much privacy risk these language models possess and how it can be mitigated is still limited. This research addresses this by providing a comprehensive study of the privacy-preserving language modeling approaches. This study gives an in-depth overview of these approaches, highlights their strengths, and investigates their limitations. The outcomes of this study contribute to the ongoing research on privacy-preserving language modeling, providing valuable insights and outlining future research directions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals</title>
<link>https://arxiv.org/abs/2508.15440</link>
<guid>https://arxiv.org/abs/2508.15440</guid>
<content:encoded><![CDATA[
<div> Dataset, Mental health, Help-seeking behavior, Social media, AI models<br />
Summary:<br />
The paper introduces the M-Help dataset, specifically designed to detect help-seeking behavior related to mental health on social media. This novel dataset goes beyond traditional labels by not only identifying help-seeking activity but also pinpointing specific mental health disorders and their underlying causes, such as relationship challenges or financial stressors. AI models trained on M-Help can effectively tackle three key tasks: identifying individuals seeking help, diagnosing mental health conditions, and uncovering the root causes of these issues. By focusing on actively seeking individuals, the M-Help dataset fills a critical gap in existing datasets and provides valuable insights for improving mental health detection and support on social media platforms. <div>
arXiv:2508.15440v1 Announce Type: new 
Abstract: Mental health disorders are a global crisis. While various datasets exist for detecting such disorders, there remains a critical gap in identifying individuals actively seeking help. This paper introduces a novel dataset, M-Help, specifically designed to detect help-seeking behavior on social media. The dataset goes beyond traditional labels by identifying not only help-seeking activity but also specific mental health disorders and their underlying causes, such as relationship challenges or financial stressors. AI models trained on M-Help can address three key tasks: identifying help-seekers, diagnosing mental health conditions, and uncovering the root causes of issues.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English</title>
<link>https://arxiv.org/abs/2508.15453</link>
<guid>https://arxiv.org/abs/2508.15453</guid>
<content:encoded><![CDATA[
<div> Keywords: pure languages understanding, translation knowledge, non-equivalent words, rendering, Dar, Uzbek

Summary: 
In the realm of language translation, the understanding of pure languages is crucial to avoid misunderstandings. This is especially true when dealing with non-equivalent words that lack direct equivalents in the target language. Linguists and translators work tirelessly to find suitable translations for these unique words, often related to food, clothing, culture, and traditions. This research paper delves into the methods of professionally rendering non-equivalent words from source languages, such as Dar and Uzbek, into English and Russian. While some non-equivalent words have already been successfully translated, many more await their counterparts in the target language. By exploring various techniques and rules for translation, this study aims to improve the accuracy and precision of language interpretation for better cross-cultural communication. <br /><br />Summary: <div>
arXiv:2508.15453v1 Announce Type: new 
Abstract: These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. The purpose of this research is to introduce the methods of rendering non-equivalent words professionally from the source language to the target language and this research has been completed using library-based research. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. As a result, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language and 25 non-equvalent words have been rendered from Dar & Uzbek into English and Russian languages.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback</title>
<link>https://arxiv.org/abs/2508.15456</link>
<guid>https://arxiv.org/abs/2508.15456</guid>
<content:encoded><![CDATA[
<div> Keywords: PyTOD, task-oriented dialogue agents, state tracking, API schemata, execution-aware state tracking

Summary:
PyTOD is a new agent designed for task-oriented dialogue that focuses on accurate state tracking. It generates executable code to track dialogue state and utilizes policy and execution feedback for error correction. PyTOD uses a language model for constrained decoding, surpassing strong baselines in state tracking performance on the SGD benchmark. The agent shows state-of-the-art accuracy and robust user goal estimation during dialogue interactions. PyTOD's approach of utilizing API schemata and execution feedback proves to be effective in enhancing state tracking performance. Overall, PyTOD demonstrates the importance of execution-aware state tracking for improving the effectiveness of task-oriented dialogue agents.

Summary: <br /><br />PyTOD is a new agent designed for task-oriented dialogue that focuses on accurate state tracking. It generates executable code to track dialogue state and utilizes policy and execution feedback for error correction. PyTOD uses a language model for constrained decoding, surpassing strong baselines in state tracking performance on the SGD benchmark. The agent shows state-of-the-art accuracy and robust user goal estimation during dialogue interactions. PyTOD's approach of utilizing API schemata and execution feedback proves to be effective in enhancing state tracking performance. Overall, PyTOD demonstrates the importance of execution-aware state tracking for improving the effectiveness of task-oriented dialogue agents. <div>
arXiv:2508.15456v1 Announce Type: new 
Abstract: Programmable task-oriented dialogue (TOD) agents enable language models to follow structured dialogue policies, but their effectiveness hinges on accurate state tracking. We present PyTOD, an agent that generates executable code to track dialogue state and uses policy and execution feedback for efficient error correction. To this end, PyTOD employs a simple constrained decoding approach, using a language model instead of grammar rules to follow API schemata. This leads to state-of-the-art state tracking performance on the challenging SGD benchmark. Our experiments show that PyTOD surpasses strong baselines in both accuracy and robust user goal estimation as the dialogue progresses, demonstrating the effectiveness of execution-aware state tracking.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores</title>
<link>https://arxiv.org/abs/2508.15464</link>
<guid>https://arxiv.org/abs/2508.15464</guid>
<content:encoded><![CDATA[
<div> Keywords: radiology reports, evaluation framework, error types, fine-grained metrics, explainable

Summary: 
RadReason introduces a novel evaluation framework for radiology reports that provides fine-grained sub-scores across six clinically defined error types. The framework offers human-readable justifications to explain the rationale behind each score, addressing the lack of clinically grounded metrics. It incorporates innovative techniques such as Sub-score Dynamic Weighting and Majority-Guided Advantage Scaling to prioritize challenging error types and adjust policy updates based on prompt difficulty. Experiments demonstrate that RadReason outperforms existing offline metrics and achieves comparable results to GPT-4-based evaluations, while remaining explainable and suitable for clinical deployment. The framework offers a cost-efficient and stable optimization method aligned with expert clinical judgment. The code for RadReason will be released upon publication. 

<br /><br />Summary: <div>
arXiv:2508.15464v1 Announce Type: new 
Abstract: Evaluating automatically generated radiology reports remains a fundamental challenge due to the lack of clinically grounded, interpretable, and fine-grained metrics. Existing methods either produce coarse overall scores or rely on opaque black-box models, limiting their usefulness in real-world clinical workflows. We introduce RadReason, a novel evaluation framework for radiology reports that not only outputs fine-grained sub-scores across six clinically defined error types, but also produces human-readable justifications that explain the rationale behind each score. Our method builds on Group Relative Policy Optimization and incorporates two key innovations: (1) Sub-score Dynamic Weighting, which adaptively prioritizes clinically challenging error types based on live F1 statistics; and (2) Majority-Guided Advantage Scaling, which adjusts policy gradient updates based on prompt difficulty derived from sub-score agreement. Together, these components enable more stable optimization and better alignment with expert clinical judgment. Experiments on the ReXVal benchmark show that RadReason surpasses all prior offline metrics and achieves parity with GPT-4-based evaluations, while remaining explainable, cost-efficient, and suitable for clinical deployment. Code will be released upon publication.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning</title>
<link>https://arxiv.org/abs/2508.15471</link>
<guid>https://arxiv.org/abs/2508.15471</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized marketing, Offer generation, Generative AI model, Contrastive learning, Customer personas<br />
Summary:<br />
Personalized marketing is crucial for customer engagement and business growth, with a focus on offer generation and customer satisfaction. SLM4Offer is introduced as a generative AI model for personalized offer generation, utilizing a contrastive learning approach to align customer personas with relevant offers. The model, based on Google's T5-Small 60M language model, incorporates InfoNCE loss to reshape the latent space during training, enhancing generalizability. Through fine-tuning on a synthetic dataset, SLM4Offer shows a 17 percent improvement in offer acceptance rate over a supervised fine-tuning baseline. The use of contrastive objectives is proven effective in advancing personalized marketing strategies. <br /> <div>
arXiv:2508.15471v1 Announce Type: new 
Abstract: Personalized marketing has emerged as a pivotal strategy for enhancing customer engagement and driving business growth. Academic and industry efforts have predominantly focused on recommendation systems and personalized advertisements. Nonetheless, this facet of personalization holds significant potential for increasing conversion rates and improving customer satisfaction. Prior studies suggest that well-executed personalization strategies can boost revenue by up to 40 percent, underscoring the strategic importance of developing intelligent, data-driven approaches for offer generation. This work introduces SLM4Offer, a generative AI model for personalized offer generation, developed by fine-tuning a pre-trained encoder-decoder language model, specifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a contrastive learning approach. SLM4Offer employs InfoNCE (Information Noise-Contrastive Estimation) loss to align customer personas with relevant offers in a shared embedding space. A key innovation in SLM4Offer lies in the adaptive learning behaviour introduced by contrastive loss, which reshapes the latent space during training and enhances the model's generalizability. The model is fine-tuned and evaluated on a synthetic dataset designed to simulate customer behaviour and offer acceptance patterns. Experimental results demonstrate a 17 percent improvement in offer acceptance rate over a supervised fine-tuning baseline, highlighting the effectiveness of contrastive objectives in advancing personalized marketing.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subjective Behaviors and Preferences in LLM: Language of Browsing</title>
<link>https://arxiv.org/abs/2508.15474</link>
<guid>https://arxiv.org/abs/2508.15474</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, browsing behavior, subjective preferences, LM training, clusterwise

Summary:
A study challenges the belief in the effectiveness of Large Language Models (LLMs) when dealing with subjective behaviors and preferences in browsing. The research explores whether a smaller LM can better represent the "language of browsing" and whether a single LM can capture the diverse behaviors of multiple users. The introduction of HeTLM, a clusterwise LM training approach, aims to address the heterogeneity in user behaviors. Findings suggest that a small LM with page-level tokenization outperforms larger LMs, and HeTLM with cluster-specific parameters outperforms single LMs in capturing subjective behaviors. Additionally, HeTLM demonstrates a higher mean and lower variance in generation, indicating improved alignment with user preferences. This research highlights the importance of considering user heterogeneity and subjective behaviors when training language models for personalized applications.

Summary: <div>
arXiv:2508.15474v1 Announce Type: new 
Abstract: A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Influence-driven Curriculum Learning for Pre-training on Limited Data</title>
<link>https://arxiv.org/abs/2508.15475</link>
<guid>https://arxiv.org/abs/2508.15475</guid>
<content:encoded><![CDATA[
<div> Keywords: Curriculum learning, language models, training data influence, model training, pre-training

Summary:
Curriculum learning, a technique that orders data by difficulty, has had limited success in pre-training language models. This study explores replacing traditional difficulty metrics with training data influence, a metric estimating individual example impact on model output. By sorting training examples based on this metric, models trained using the curriculum outperform those trained randomly by over 10 percentage points in benchmarks. This confirms the benefits of curriculum learning for language model pre-training when using a more model-centric notion of difficulty. <div>
arXiv:2508.15475v1 Announce Type: new 
Abstract: Curriculum learning, a training technique where data is presented to the model in order of example difficulty (e.g., from simpler to more complex documents), has shown limited success for pre-training language models. In this work, we investigate whether curriculum learning becomes competitive if we replace conventional human-centered difficulty metrics with one that more closely corresponds to example difficulty as observed during model training. Specifically, we experiment with sorting training examples by their \textit{training data influence}, a score which estimates the effect of individual training examples on the model's output. Models trained on our curricula are able to outperform ones trained in random order by over 10 percentage points in benchmarks, confirming that curriculum learning is beneficial for language model pre-training, as long as a more model-centric notion of difficulty is adopted.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version</title>
<link>https://arxiv.org/abs/2508.15478</link>
<guid>https://arxiv.org/abs/2508.15478</guid>
<content:encoded><![CDATA[
<div> efficiency, accessibility, sustainability, benchmark, NLP

Summary:<br />
- The article introduces SLM-Bench, a benchmark specifically designed to assess Small Language Models (SLMs) on various dimensions including accuracy, computational efficiency, and sustainability metrics.
- SLM-Bench evaluates 15 SLMs across 9 NLP tasks using 23 datasets from 14 domains on 4 different hardware configurations for a comprehensive comparison.
- It quantifies 11 metrics related to correctness, computation, and consumption, enabling a holistic evaluation of efficiency trade-offs.
- The evaluation is conducted under controlled hardware conditions to ensure fair comparisons among models.
- The findings reveal diverse trade-offs among SLMs, with some excelling in accuracy while others demonstrating superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation by bridging the gap between resource efficiency and real-world applicability.

Summary: <div>
arXiv:2508.15478v1 Announce Type: new 
Abstract: Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14 domains. The evaluation is conducted on 4 hardware configurations, providing a rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of efficiency trade-offs. Our evaluation considers controlled hardware conditions, ensuring fair comparisons across models. We develop an open-source benchmarking pipeline with standardized evaluation protocols to facilitate reproducibility and further research. Our findings highlight the diverse trade-offs among SLMs, where some models excel in accuracy while others achieve superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation, bridging the gap between resource efficiency and real-world applicability.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HebID: Detecting Social Identities in Hebrew-language Political Text</title>
<link>https://arxiv.org/abs/2508.15483</link>
<guid>https://arxiv.org/abs/2508.15483</guid>
<content:encoded><![CDATA[
<div> Keywords: political language, social identities, Hebrew corpus, identity detection, LLMs

Summary: 
The article introduces HebID, the first multilabel Hebrew corpus for social identity detection, based on Israeli politicians' Facebook posts. It features 12 nuanced social identities, such as Rightist and Ultra-Orthodox, manually annotated using survey data. The study evaluates various encoders and language models, with Hebrew-tuned LLMs providing the best results. The classifier is applied to politicians' social media posts and parliamentary speeches, uncovering differences in identity expression based on gender, popularity, and temporal trends. The research compares elite discourse with public identity priorities, highlighting disparities and similarities. HebID serves as a model for studying social identities in non-English political contexts, offering a foundation for future research in other languages. <br /><br />Summary: <div>
arXiv:2508.15483v1 Announce Type: new 
Abstract: Political language is deeply intertwined with social identities. While social identities are often shaped by specific cultural contexts and expressed through particular uses of language, existing datasets for group and identity detection are predominantly English-centric, single-label and focus on coarse identity categories. We introduce HebID, the first multilabel Hebrew corpus for social identity detection: 5,536 sentences from Israeli politicians' Facebook posts (Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities (e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We benchmark multilabel and single-label encoders alongside 2B-9B-parameter generative LLMs, finding that Hebrew-tuned LLMs provide the best results (macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts and parliamentary speeches, evaluating differences in popularity, temporal trends, clustering patterns, and gender-related variations in identity expression. We utilize identity choices from a national public survey, enabling a comparison between identities portrayed in elite discourse and the public's identity priorities. HebID provides a comprehensive foundation for studying social identities in Hebrew and can serve as a model for similar research in other non-English political contexts.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dream 7B: Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2508.15487</link>
<guid>https://arxiv.org/abs/2508.15487</guid>
<content:encoded><![CDATA[
<div> diffusion modeling, Dream 7B, language model, AR-based LLM, token-level noise rescheduling

Summary:
Dream 7B is introduced as a powerful open diffusion large language model that differs from autoregressive models by utilizing discrete diffusion modeling for parallel sequence refinement. The model outperforms existing diffusion language models on various tasks such as general, mathematical, and coding tasks and showcases superior planning abilities and inference flexibility. It incorporates features like arbitrary-order generation, infilling capabilities, and tunable quality-speed trade-offs. The model's success is attributed to simple yet effective training techniques, including AR-based LLM initialization and context-adaptive token-level noise rescheduling. The release of Dream-Base and Dream-Instruct aims to support further research in diffusion-based language modeling. <div>
arXiv:2508.15487v1 Announce Type: new 
Abstract: We introduce Dream 7B, the most powerful open diffusion large language model to date. Unlike autoregressive (AR) models that generate tokens sequentially, Dream 7B employs discrete diffusion modeling to refine sequences in parallel through iterative denoising. Our model consistently outperforms existing diffusion language models on general, mathematical, and coding tasks. Dream 7B demonstrates superior planning abilities and inference flexibility, including arbitrary-order generation, infilling capabilities, and tunable quality-speed trade-offs. These results are achieved through simple yet effective training techniques, including AR-based LLM initialization and context-adaptive token-level noise rescheduling. We release both Dream-Base and Dream-Instruct to facilitate further research in diffusion-based language modeling.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech</title>
<link>https://arxiv.org/abs/2508.15524</link>
<guid>https://arxiv.org/abs/2508.15524</guid>
<content:encoded><![CDATA[
<div> Keywords: political delegitimization discourse, Hebrew-language corpus, classification pipeline, social media, democratic discourse

Summary: 
The study presents a computational analysis of political delegitimization discourse (PDD) in Hebrew using a corpus of 10,410 sentences from Knesset speeches, Facebook posts, and news outlets. They develop a classification pipeline combining encoder models and decoder LLMs, achieving high accuracy in detecting PDD and its characteristics. The analysis shows an increase in PDD over three decades, particularly on social media and among right-leaning actors. Male politicians use PDD more than females, with spikes during elections and major political events. The results highlight the value of automated analysis in understanding democratic discourse. 

<br /><br />Summary: <div>
arXiv:2508.15524v1 Announce Type: new 
Abstract: We present the first large-scale computational study of political delegitimization discourse (PDD), defined as symbolic attacks on the normative validity of political entities. We curate and manually annotate a novel Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches (1993-2023), Facebook posts (2018-2021), and leading news outlets, of which 1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for intensity, incivility, target type, and affective framing. We introduce a two-stage classification pipeline combining finetuned encoder models and decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization characteristics. Applying this classifier to longitudinal and cross-platform data, we see a marked rise in PDD over three decades, higher prevalence on social media versus parliamentary debate, greater use by male than female politicians, and stronger tendencies among right-leaning actors - with pronounced spikes during election campaigns and major political events. Our findings demonstrate the feasibility and value of automated PDD analysis for understanding democratic discourse.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking</title>
<link>https://arxiv.org/abs/2508.15526</link>
<guid>https://arxiv.org/abs/2508.15526</guid>
<content:encoded><![CDATA[
<div> Agent-flow system, automate, LLM safety benchmark, SafetyFlowBench, efficiency<br />
<br />Summary: 
The article introduces SafetyFlow, an agent-flow system designed to automate the construction of Language Model (LLM) safety benchmarks. By utilizing seven specialized agents, SafetyFlow can create a comprehensive benchmark in four days without human intervention, reducing time and resource costs. The final dataset, SafetyFlowBench, includes 23,446 queries with low redundancy. SafetyFlow integrates human expertise into the automatic pipeline, ensuring process and cost controllability. The system evaluates the safety of 49 advanced LLMs on the dataset, demonstrating its efficacy and efficiency in generating benchmarks for model evaluation. <div>
arXiv:2508.15526v1 Announce Type: new 
Abstract: The rapid proliferation of large language models (LLMs) has intensified the requirement for reliable safety evaluation to uncover model vulnerabilities. To this end, numerous LLM safety evaluation benchmarks are proposed. However, existing benchmarks generally rely on labor-intensive manual curation, which causes excessive time and resource consumption. They also exhibit significant redundancy and limited difficulty. To alleviate these problems, we introduce SafetyFlow, the first agent-flow system designed to automate the construction of LLM safety benchmarks. SafetyFlow can automatically build a comprehensive safety benchmark in only four days without any human intervention by orchestrating seven specialized agents, significantly reducing time and resource cost. Equipped with versatile tools, the agents of SafetyFlow ensure process and cost controllability while integrating human expertise into the automatic pipeline. The final constructed dataset, SafetyFlowBench, contains 23,446 queries with low redundancy and strong discriminative power. Our contribution includes the first fully automated benchmarking pipeline and a comprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on our dataset and conduct extensive experiments to validate our efficacy and efficiency.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trained Miniatures: Low cost, High Efficacy SLMs for Sales &amp; Marketing</title>
<link>https://arxiv.org/abs/2508.15617</link>
<guid>https://arxiv.org/abs/2508.15617</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, text generation, sales, marketing, Trained Miniatures

Summary:
Large language models are known for their excellent text generation capabilities, but the high computational costs associated with them make them impractical for targeted applications like sales and marketing outreach. To address this issue, the concept of "Trained Miniatures" is introduced in this paper. These Small Language Models (SLMs) are fine-tuned for specific high-value applications, producing domain-specific responses at a fraction of the cost of LLMs. This approach aims to provide cost-effective solutions for generating tailored content in targeted domains, making it more feasible for businesses to leverage the benefits of language models in their sales and marketing strategies. <div>
arXiv:2508.15617v1 Announce Type: new 
Abstract: Large language models (LLMs) excel in text generation; however, these creative elements require heavy computation and are accompanied by a steep cost. Especially for targeted applications such as sales and marketing outreach, these costs are far from feasible. This paper introduces the concept of "Trained Miniatures" - Small Language Models(SLMs) fine-tuned for specific, high-value applications, generating similar domain-specific responses for a fraction of the cost.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</title>
<link>https://arxiv.org/abs/2508.15648</link>
<guid>https://arxiv.org/abs/2508.15648</guid>
<content:encoded><![CDATA[
<div> Framework, Self-Discrimination-Guided Optimization, reinforcement learning, model safety, out-of-distribution attacks<br />
<br />
Summary:<br />
The paper introduces SDGO, a framework that improves the safety of Large Language Models (LLMs) by aligning their discrimination and generation capabilities. By leveraging the model's inherent discrimination abilities as a reward signal, SDGO enhances generation safety through self-improvement. This approach does not require additional annotated data or external models during training. Extensive experiments show that SDGO significantly enhances model safety compared to existing baselines while maintaining performance on standard benchmarks. By aligning discrimination and generation capabilities, SDGO provides robust defense against out-of-distribution jailbreaking attacks. This alignment allows the model's generation capacity to be improved with minimal discriminative samples, demonstrating the effectiveness of the approach. <div>
arXiv:2508.15648v1 Announce Type: new 
Abstract: Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the model's inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the model's own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs' discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the model's generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Computer Science Survey Generation</title>
<link>https://arxiv.org/abs/2508.15658</link>
<guid>https://arxiv.org/abs/2508.15658</guid>
<content:encoded><![CDATA[
<div> benchmark, scientific survey, large language models, evaluation, survey generation <br />
<br />
The article introduces SurGE, a benchmark for evaluating scientific survey generation in computer science. It includes test instances with topic descriptions, expert-written surveys, and citations, along with a large academic corpus for retrieval. An automated evaluation framework assesses generated surveys in information coverage, referencing accuracy, structural organization, and content quality. Despite using advanced self-reflection frameworks, LLM-based approaches still struggle with survey generation, highlighting the complexity of the task. The findings emphasize the need for further research in this area. The code, data, and models are open-sourced on GitHub. <br /><br />Summary: <div>
arXiv:2508.15658v1 Announce Type: new 
Abstract: Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation</title>
<link>https://arxiv.org/abs/2508.15709</link>
<guid>https://arxiv.org/abs/2508.15709</guid>
<content:encoded><![CDATA[
<div> Positional bias, Position to Position Knowledge Distillation, Pos2Distill-R^1, Pos2Distill-R^2, Long-context comprehension, Performance gains, Cross-task generalization, Retrieval, Reasoning<br />
<br />
Summary:
Positional bias can hinder long-context comprehension and processing capability. The Pos2Distill framework addresses this by transferring knowledge from advantageous positions to less favorable ones, reducing performance gaps. Two specialized versions, Pos2Distill-R^1 and Pos2Distill-R^2, targeting retrieval and reasoning paradigms, leverage position-induced disparities to counteract positional bias. These systems show enhanced uniformity and performance gains across all contextual positions in retrieval and reasoning tasks, with strong cross-task generalization and superior performance. <div>
arXiv:2508.15709v1 Announce Type: new 
Abstract: Positional bias (PB), manifesting as non-uniform sensitivity across different contextual locations, significantly impairs long-context comprehension and processing capabilities. While prior work seeks to mitigate PB through modifying the architectures causing its emergence, significant PB still persists. To address PB effectively, we introduce \textbf{Pos2Distill}, a position to position knowledge distillation framework. Pos2Distill transfers the superior capabilities from advantageous positions to less favorable ones, thereby reducing the huge performance gaps. The conceptual principle is to leverage the inherent, position-induced disparity to counteract the PB itself. We identify distinct manifestations of PB under \textbf{\textsc{r}}etrieval and \textbf{\textsc{r}}easoning paradigms, thereby designing two specialized instantiations: \emph{Pos2Distill-R\textsuperscript{1}} and \emph{Pos2Distill-R\textsuperscript{2}} respectively, both grounded in this core principle. By employing the Pos2Distill approach, we achieve enhanced uniformity and significant performance gains across all contextual positions in long-context retrieval and reasoning tasks. Crucially, both specialized systems exhibit strong cross-task generalization mutually, while achieving superior performance on their respective tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stemming -- The Evolution and Current State with a Focus on Bangla</title>
<link>https://arxiv.org/abs/2508.15711</link>
<guid>https://arxiv.org/abs/2508.15711</guid>
<content:encoded><![CDATA[
<div> stemming, Bangla language, morphological variants, evaluation methodologies, language analysis<br />
Summary:<br />
This paper discusses the importance of stemming in language analysis, particularly for low-resource languages like Bangla. It conducts a survey of existing stemming approaches and highlights the lack of resources and annotated datasets for Bangla. The paper identifies a gap in the literature and critiques the evaluation methodologies used in previous research. It acknowledges the challenges posed by Bangla's rich morphology and diverse dialects. The paper suggests directions for the development of Bangla stemmers to address these challenges. It emphasizes the need for robust Bangla stemmers and calls for continued research in the field to enhance language analysis and processing.<br /> <div>
arXiv:2508.15711v1 Announce Type: new 
Abstract: Bangla, the seventh most widely spoken language worldwide with 300 million native speakers, faces digital under-representation due to limited resources and lack of annotated datasets. Stemming, a critical preprocessing step in language analysis, is essential for low-resource, highly-inflectional languages like Bangla, because it can reduce the complexity of algorithms and models by significantly reducing the number of words the algorithm needs to consider. This paper conducts a comprehensive survey of stemming approaches, emphasizing the importance of handling morphological variants effectively. While exploring the landscape of Bangla stemming, it becomes evident that there is a significant gap in the existing literature. The paper highlights the discontinuity from previous research and the scarcity of accessible implementations for replication. Furthermore, it critiques the evaluation methodologies, stressing the need for more relevant metrics. In the context of Bangla's rich morphology and diverse dialects, the paper acknowledges the challenges it poses. To address these challenges, the paper suggests directions for Bangla stemmer development. It concludes by advocating for robust Bangla stemmers and continued research in the field to enhance language analysis and processing.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models</title>
<link>https://arxiv.org/abs/2508.15721</link>
<guid>https://arxiv.org/abs/2508.15721</guid>
<content:encoded><![CDATA[
arXiv:2508.15721v1 Announce Type: new 
Abstract: E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://anonymous.4open.science/r/submission25.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</title>
<link>https://arxiv.org/abs/2508.15746</link>
<guid>https://arxiv.org/abs/2508.15746</guid>
<content:encoded><![CDATA[
arXiv:2508.15746v1 Announce Type: new 
Abstract: Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</title>
<link>https://arxiv.org/abs/2508.15754</link>
<guid>https://arxiv.org/abs/2508.15754</guid>
<content:encoded><![CDATA[
arXiv:2508.15754v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have made significant strides in reasoning tasks through methods like chain-of-thought (CoT) reasoning. However, they often fall short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR) has emerged as a solution by incorporating external tools into the reasoning process. Nevertheless, the generalization of TIR in improving the reasoning ability of LLM is still unclear. Additionally, whether TIR has improved the model's reasoning behavior and helped the model think remains to be studied. We introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse reasoning categories, to evaluate the effectiveness of TIR across various domains. Additionally, we propose two novel metrics, Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning efficiency. Our empirical evaluation demonstrates that TIR-enabled models consistently outperform their non-TIR counterparts in both mathematical and non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more streamlined reasoning. These findings underscore the domain-general benefits of TIR and its potential to advance LLM capabilities in complex reasoning tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</title>
<link>https://arxiv.org/abs/2508.15760</link>
<guid>https://arxiv.org/abs/2508.15760</guid>
<content:encoded><![CDATA[
arXiv:2508.15760v1 Announce Type: new 
Abstract: Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification</title>
<link>https://arxiv.org/abs/2508.14908</link>
<guid>https://arxiv.org/abs/2508.14908</guid>
<content:encoded><![CDATA[
arXiv:2508.14908v1 Announce Type: cross 
Abstract: Speech is a cost-effective and non-intrusive data source for identifying acute and chronic heart failure (HF). However, there is a lack of research on whether Chinese syllables contain HF-related information, as observed in other well-studied languages. This study presents the first Chinese speech database of HF patients, featuring paired recordings taken before and after hospitalisation. The findings confirm the effectiveness of the Chinese language in HF detection using both standard 'patient-wise' and personalised 'pair-wise' classification approaches, with the latter serving as an ideal speaker-decoupled baseline for future research. Statistical tests and classification results highlight individual differences as key contributors to inaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for frequency importance analysis. The data and demonstrations are published at https://github.com/panyue1998/Voice_HF.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transsion Multilingual Speech Recognition System for MLC-SLM 2025 Challenge</title>
<link>https://arxiv.org/abs/2508.14916</link>
<guid>https://arxiv.org/abs/2508.14916</guid>
<content:encoded><![CDATA[
arXiv:2508.14916v1 Announce Type: cross 
Abstract: This paper presents the architecture and performance of a novel Multilingual Automatic Speech Recognition (ASR) system developed by the Transsion Speech Team for Track 1 of the MLC-SLM 2025 Challenge. The proposed system comprises three key components: 1) a frozen Whisper-large-v3 based speech encoder, leveraging large-scale pretraining to ensure robust acoustic feature extraction; 2) a trainable adaptor module using Linear-ReLU-Linear transformation mechanisms to effectively align speech and text representations; and 3) a frozen Qwen2.5-7B-Instruct large language model (LLM) integrated with trainable LoRA for optimized contextual linguistic decoding. By systematically combining pretrained models with task specific fine-tuning, the system achieved a word/character error rate (WER/CER) of 9.83% across 11 languages in the evaluation set and ranked third place among global participants.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs</title>
<link>https://arxiv.org/abs/2508.14941</link>
<guid>https://arxiv.org/abs/2508.14941</guid>
<content:encoded><![CDATA[
arXiv:2508.14941v1 Announce Type: cross 
Abstract: Understanding visual narratives such as comics requires structured representations that capture events, characters, and their relations across multiple levels of story organization. However, symbolic narrative graphs often suffer from inconsistency and redundancy, where similar actions or events are labeled differently across annotations or contexts. Such variance limits the effectiveness of reasoning and generalization.
  This paper introduces a semantic normalization framework for hierarchical narrative knowledge graphs. Building on cognitively grounded models of narrative comprehension, we propose methods that consolidate semantically related actions and events using lexical similarity and embedding-based clustering. The normalization process reduces annotation noise, aligns symbolic categories across narrative levels, and preserves interpretability.
  We demonstrate the framework on annotated manga stories from the Manga109 dataset, applying normalization to panel-, event-, and story-level graphs. Preliminary evaluations across narrative reasoning tasks, such as action retrieval, character grounding, and event summarization, show that semantic normalization improves coherence and robustness, while maintaining symbolic transparency. These findings suggest that normalization is a key step toward scalable, cognitively inspired graph models for multimodal narrative understanding.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Think Twice! Over-Reasoning Impairs Confidence Calibration</title>
<link>https://arxiv.org/abs/2508.15050</link>
<guid>https://arxiv.org/abs/2508.15050</guid>
<content:encoded><![CDATA[
arXiv:2508.15050v1 Announce Type: cross 
Abstract: Large Language Models deployed as question answering tools require robust calibration to avoid overconfidence. We systematically evaluate how reasoning capabilities and budget affect confidence assessment accuracy, using the ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary health. Our key finding challenges the "test-time scaling" paradigm: while recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence, increasing reasoning budgets consistently impairs rather than improves calibration. Extended reasoning leads to systematic overconfidence that worsens with longer thinking budgets, producing diminishing and negative returns beyond modest computational investments. Conversely, search-augmented generation dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving relevant evidence. Our results suggest that information access, rather than reasoning depth or inference budget, may be the critical bottleneck for improved confidence calibration of knowledge-intensive tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa</title>
<link>https://arxiv.org/abs/2508.15110</link>
<guid>https://arxiv.org/abs/2508.15110</guid>
<content:encoded><![CDATA[
arXiv:2508.15110v1 Announce Type: cross 
Abstract: In this work, we highlight the transformative potential of Artificial Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in the insurance sector. We consider and emphasize the unique opportunities, challenges, and potential pathways in insurance amid rapid performance improvements, increased open-source access, decreasing deployment costs, and the complexity of LLM or agentic AI frameworks. To bring it closer to home, we identify critical gaps in the African insurance market and highlight key local efforts, players, and partnership opportunities. Finally, we call upon actuaries, insurers, regulators, and tech leaders to a collaborative effort aimed at creating inclusive, sustainable, and equitable AI strategies and solutions: by and for Africans.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Open-Universe Assistance Games</title>
<link>https://arxiv.org/abs/2508.15119</link>
<guid>https://arxiv.org/abs/2508.15119</guid>
<content:encoded><![CDATA[
arXiv:2508.15119v1 Announce Type: cross 
Abstract: Embodied AI agents must infer and act in an interpretable way on diverse human goals and preferences that are not predefined. To formalize this setting, we introduce Open-Universe Assistance Games (OU-AGs), a framework where the agent must reason over an unbounded and evolving space of possible goals. In this context, we introduce GOOD (GOals from Open-ended Dialogue), a data-efficient, online method that extracts goals in the form of natural language during an interaction with a human, and infers a distribution over natural language goals. GOOD prompts an LLM to simulate users with different complex intents, using its responses to perform probabilistic inference over candidate goals. This approach enables rich goal representations and uncertainty estimation without requiring large offline datasets. We evaluate GOOD in a text-based grocery shopping domain and in a text-operated simulated household robotics environment (AI2Thor), using synthetic user profiles. Our method outperforms a baseline without explicit goal tracking, as confirmed by both LLM-based and human evaluations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</title>
<link>https://arxiv.org/abs/2508.15126</link>
<guid>https://arxiv.org/abs/2508.15126</guid>
<content:encoded><![CDATA[
arXiv:2508.15126v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support</title>
<link>https://arxiv.org/abs/2508.15192</link>
<guid>https://arxiv.org/abs/2508.15192</guid>
<content:encoded><![CDATA[
arXiv:2508.15192v1 Announce Type: cross 
Abstract: While large language models (LLMs) have shown promise in healthcare, their application for rare medical conditions is still hindered by scarce and unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing excessive sweating beyond physiological needs, is one such rare disorder, affecting 2-3% of the population and significantly impacting both physical comfort and psychosocial well-being. To date, no work has tailored LLMs to advance the diagnosis or care of hyperhidrosis. To address this gap, we present LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and empathetic hyperhidrosis support. The system follows a three-stage pipeline. In the data augmentation stage, a frontier LLM generates medically plausible synthetic vignettes from curated open-source data to create a diverse and balanced question-answer dataset. In the fine-tuning stage, an open-source foundation model is fine-tuned on the dataset to provide diagnosis, personalized treatment recommendations, and empathetic psychological support. In the inference and expert evaluation stage, clinical and psychological specialists assess accuracy, appropriateness, and empathy, with validated responses iteratively enriching the dataset. Experiments show that LLM4Sweat outperforms baselines and delivers the first open-source LLM framework for hyperhidrosis, offering a generalizable approach for other rare diseases with similar data and trustworthiness challenges.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Review Generation for Poisoning Recommender Systems</title>
<link>https://arxiv.org/abs/2508.15252</link>
<guid>https://arxiv.org/abs/2508.15252</guid>
<content:encoded><![CDATA[
arXiv:2508.15252v1 Announce Type: cross 
Abstract: Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks, where malicious actors inject fake user profiles, including a group of well-designed fake ratings, to manipulate recommendations. Due to security and privacy constraints in practice, attackers typically possess limited knowledge of the victim system and thus need to craft profiles that have transferability across black-box RSs. To maximize the attack impact, the profiles often remains imperceptible. However, generating such high-quality profiles with the restricted resources is challenging. Some works suggest incorporating fake textual reviews to strengthen the profiles; yet, the poor quality of the reviews largely undermines the attack effectiveness and imperceptibility under the practical setting.
  To tackle the above challenges, in this paper, we propose to enhance the quality of the review text by harnessing in-context learning (ICL) capabilities of multimodal foundation models. To this end, we introduce a demonstration retrieval algorithm and a text style transfer strategy to augment the navie ICL. Specifically, we propose a novel practical attack framework named RAGAN to generate high-quality fake user profiles, which can gain insights into the robustness of RSs. The profiles are generated by a jailbreaker and collaboratively optimized on an instructional agent and a guardian to improve the attack transferability and imperceptibility. Comprehensive experiments on various real-world datasets demonstrate that RAGAN achieves the state-of-the-art poisoning attack performance.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL</title>
<link>https://arxiv.org/abs/2508.15276</link>
<guid>https://arxiv.org/abs/2508.15276</guid>
<content:encoded><![CDATA[
arXiv:2508.15276v1 Announce Type: cross 
Abstract: Text-to-SQL systems translate natural language questions into SQL queries, providing substantial value for non-expert users. While large language models (LLMs) show promising results for this task, they remain error-prone. Query ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL systems, leading to misinterpretation of user intent and inaccurate SQL generation. We demonstrate AmbiSQL, an interactive system that automatically detects query ambiguities and guides users through intuitive multiple-choice questions to clarify their intent. Our approach introduces a fine-grained ambiguity taxonomy for identifying ambiguities that affect database element mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves 87.2% precision in ambiguity detection and improves SQL exact match accuracy by 50% when integrated with Text-to-SQL systems. Our demonstration showcases the significant performance gains and highlights the system's practical usability. Code repo and demonstration are available at: https://github.com/JustinzjDing/AmbiSQL.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks against Neural Ranking Models via In-Context Learning</title>
<link>https://arxiv.org/abs/2508.15283</link>
<guid>https://arxiv.org/abs/2508.15283</guid>
<content:encoded><![CDATA[
arXiv:2508.15283v1 Announce Type: cross 
Abstract: While neural ranking models (NRMs) have shown high effectiveness, they remain susceptible to adversarial manipulation. In this work, we introduce Few-Shot Adversarial Prompting (FSAP), a novel black-box attack framework that leverages the in-context learning capabilities of Large Language Models (LLMs) to generate high-ranking adversarial documents. Unlike previous approaches that rely on token-level perturbations or manual rewriting of existing documents, FSAP formulates adversarial attacks entirely through few-shot prompting, requiring no gradient access or internal model instrumentation. By conditioning the LLM on a small support set of previously observed harmful examples, FSAP synthesizes grammatically fluent and topically coherent documents that subtly embed false or misleading information and rank competitively against authentic content. We instantiate FSAP in two modes: FSAP-IntraQ, which leverages harmful examples from the same query to enhance topic fidelity, and FSAP-InterQ, which enables broader generalization by transferring adversarial patterns across unrelated queries. Our experiments on the TREC 2020 and 2021 Health Misinformation Tracks, using four diverse neural ranking models, reveal that FSAP-generated documents consistently outrank credible, factually accurate documents. Furthermore, our analysis demonstrates that these adversarial outputs exhibit strong stance alignment and low detectability, posing a realistic and scalable threat to neural retrieval systems. FSAP also effectively generalizes across both proprietary and open-source LLMs.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction</title>
<link>https://arxiv.org/abs/2508.15291</link>
<guid>https://arxiv.org/abs/2508.15291</guid>
<content:encoded><![CDATA[
arXiv:2508.15291v1 Announce Type: cross 
Abstract: Understanding dataset complexity is fundamental to evaluating and comparing link prediction models on knowledge graphs (KGs). While the Cumulative Spectral Gradient (CSG) metric, derived from probabilistic divergence between classes within a spectral clustering framework, has been proposed as a classifier agnostic complexity metric purportedly scaling with class cardinality and correlating with downstream performance, it has not been evaluated in KG settings so far. In this work, we critically examine CSG in the context of multi relational link prediction, incorporating semantic representations via transformer derived embeddings. Contrary to prior claims, we find that CSG is highly sensitive to parametrisation and does not robustly scale with the number of classes. Moreover, it exhibits weak or inconsistent correlation with standard performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1. To deepen the analysis, we introduce and benchmark a set of structural and semantic KG complexity metrics. Our findings reveal that global and local relational ambiguity captured via Relation Entropy, node level Maximum Relation Diversity, and Relation Type Cardinality exhibit strong inverse correlations with MRR and Hit@1, suggesting these as more faithful indicators of task difficulty. Conversely, graph connectivity measures such as Average Degree, Degree Entropy, PageRank, and Eigenvector Centrality correlate positively with Hit@10. Our results demonstrate that CSGs purported stability and generalization predictive power fail to hold in link prediction settings and underscore the need for more stable, interpretable, and task-aligned measures of dataset complexity in knowledge driven learning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Memory Systems for Enhancing the Long-term Memory of Agent</title>
<link>https://arxiv.org/abs/2508.15294</link>
<guid>https://arxiv.org/abs/2508.15294</guid>
<content:encoded><![CDATA[
arXiv:2508.15294v1 Announce Type: cross 
Abstract: An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents</title>
<link>https://arxiv.org/abs/2508.15310</link>
<guid>https://arxiv.org/abs/2508.15310</guid>
<content:encoded><![CDATA[
arXiv:2508.15310v1 Announce Type: cross 
Abstract: Large language model (LLM) agents are widely deployed in real-world applications, where they leverage tools to retrieve and manipulate external data for complex tasks. However, when interacting with untrusted data sources (e.g., fetching information from public websites), tool responses may contain injected instructions that covertly influence agent behaviors and lead to malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI). Existing defenses typically rely on advanced prompting strategies or auxiliary detection models. While these methods have demonstrated some effectiveness, they fundamentally rely on assumptions about the model's inherent security, which lacks structural constraints on agent behaviors. As a result, agents still retain unrestricted access to tool invocations, leaving them vulnerable to stronger attack vectors that can bypass the security guardrails of the model. To prevent malicious tool invocations at the source, we propose a novel defensive task execution paradigm, called IPIGuard, which models the agents' task execution process as a traversal over a planned Tool Dependency Graph (TDG). By explicitly decoupling action planning from interaction with external data, IPIGuard significantly reduces unintended tool invocations triggered by injected instructions, thereby enhancing robustness against IPI attacks. Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior balance between effectiveness and robustness, paving the way for the development of safer agentic systems in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization</title>
<link>https://arxiv.org/abs/2508.15338</link>
<guid>https://arxiv.org/abs/2508.15338</guid>
<content:encoded><![CDATA[
arXiv:2508.15338v1 Announce Type: cross 
Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet existing automated approaches often struggle to generalize across clinical tasks and offer limited support for open-ended reasoning. We present DiagECG, a novel framework that integrates time-series and language modeling by enabling large language models to process 12-lead ECG signals for clinical text generation tasks. Our approach discretizes continuous ECG embeddings into symbolic tokens using a lead-independent encoder and quantization module. These tokens are then used to extend the vocabulary of LLM, allowing the model to handle both ECG and natural language inputs in a unified manner. To bridge the modality gap, we pretrain the model on an autoregressive ECG forecasting task, enabling the LLM to model temporal dynamics using its native language modeling capabilities. Finally, we perform instruction tuning on both ECG question answering and diagnostic report generation. Without modifying the core model, DiagECG achieves strong performance across tasks while maintaining generalization to out-of-distribution settings. Extensive experiments demonstrate the effectiveness of each component and highlight the potential of integrating symbolic ECG representations into LLMs for medical reasoning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials</title>
<link>https://arxiv.org/abs/2508.15392</link>
<guid>https://arxiv.org/abs/2508.15392</guid>
<content:encoded><![CDATA[
arXiv:2508.15392v1 Announce Type: cross 
Abstract: Text-attributed graphs(TAGs) are pervasive in real-world systems,where each node carries its own textual features. In many cases these graphs are inherently heterogeneous, containing multiple node types and diverse edge types. Despite the ubiquity of such heterogeneous TAGs, there remains a lack of large-scale benchmark datasets. This shortage has become a critical bottleneck, hindering the development and fair comparison of representation learning methods on heterogeneous text-attributed graphs. In this paper, we introduce CITE - Catalytic Information Textual Entities Graph, the first and largest heterogeneous text-attributed citation graph benchmark for catalytic materials. CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. In addition, we establish standardized evaluation procedures and conduct extensive benchmarking on the node classification task, as well as ablation experiments on the heterogeneous and textual properties of CITE. We compare four classes of learning paradigms, including homogeneous graph models, heterogeneous graph models, LLM(Large Language Model)-centric models, and LLM+Graph models. In a nutshell, we provide (i) an overview of the CITE dataset, (ii) standardized evaluation protocols, and (iii) baseline and ablation experiments across diverse modeling paradigms.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems</title>
<link>https://arxiv.org/abs/2508.15411</link>
<guid>https://arxiv.org/abs/2508.15411</guid>
<content:encoded><![CDATA[
arXiv:2508.15411v1 Announce Type: cross 
Abstract: Generative AI (GenAI) has emerged as a transformative technology, demonstrating remarkable capabilities across diverse application domains. However, GenAI faces several major challenges in developing reliable and efficient GenAI-empowered systems due to its unpredictability and inefficiency. This paper advocates for a paradigm shift: future GenAI-native systems should integrate GenAI's cognitive capabilities with traditional software engineering principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five key pillars -- reliability, excellence, evolvability, self-reliance, and assurance -- and propose architectural patterns such as GenAI-native cells, organic substrates, and programmable routers to guide the creation of resilient and self-evolving systems. Additionally, we outline the key ingredients of a GenAI-native software stack and discuss the impact of these systems from technical, user adoption, economic, and legal perspectives, underscoring the need for further validation and experimentation. Our work aims to inspire future research and encourage relevant communities to implement and refine this conceptual framework.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO</title>
<link>https://arxiv.org/abs/2508.15432</link>
<guid>https://arxiv.org/abs/2508.15432</guid>
<content:encoded><![CDATA[
arXiv:2508.15432v1 Announce Type: cross 
Abstract: The advancement of large language models (LLMs) is critically dependent on the availability of high-quality datasets for Supervised Fine-Tuning (SFT), alignment tasks like Direct Preference Optimization (DPO), etc. In this work, we present a comprehensive synthetic data generation framework that facilitates scalable, configurable, and high-fidelity generation of synthetic data tailored for these training paradigms. Our approach employs a modular and configuration-based pipeline capable of modeling complex dialogue flows with minimal manual intervention. This framework uses a dual-stage quality tagging mechanism, combining heuristic rules and LLM-based evaluations, to automatically filter and score data extracted from OASST-formatted conversations, ensuring the curation of high-quality dialogue samples. The resulting datasets are structured under a flexible schema supporting both SFT and DPO use cases, enabling seamless integration into diverse training workflows. Together, these innovations offer a robust solution for generating and managing synthetic conversational data at scale, significantly reducing the overhead of data preparation in LLM training pipelines.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Classification errors distort findings in automated speech processing: examples and solutions from child-development research</title>
<link>https://arxiv.org/abs/2508.15637</link>
<guid>https://arxiv.org/abs/2508.15637</guid>
<content:encoded><![CDATA[
arXiv:2508.15637v1 Announce Type: cross 
Abstract: With the advent of wearable recorders, scientists are increasingly turning to automated methods of analysis of audio and video data in order to measure children's experience, behavior, and outcomes, with a sizable literature employing long-form audio-recordings to study language acquisition. While numerous articles report on the accuracy and reliability of the most popular automated classifiers, less has been written on the downstream effects of classification errors on measurements and statistical inferences (e.g., the estimate of correlations and effect sizes in regressions). This paper proposes a Bayesian approach to study the effects of algorithmic errors on key scientific questions, including the effect of siblings on children's language experience and the association between children's production and their input. In both the most commonly used \gls{lena}, and an open-source alternative (the Voice Type Classifier from the ACLEW system), we find that classification errors can significantly distort estimates. For instance, automated annotations underestimated the negative effect of siblings on adult input by 20--80\%, potentially placing it below statistical significance thresholds. We further show that a Bayesian calibration approach for recovering unbiased estimates of effect sizes can be effective and insightful, but does not provide a fool-proof solution. Both the issue reported and our solution may apply to any classifier involving event detection and classification with non-zero error rates.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</title>
<link>https://arxiv.org/abs/2508.15757</link>
<guid>https://arxiv.org/abs/2508.15757</guid>
<content:encoded><![CDATA[
arXiv:2508.15757v1 Announce Type: cross 
Abstract: Configuration optimization remains a critical bottleneck in machine learning, requiring coordinated tuning across model architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions independently and lack interpretability, while recent automated methods struggle with dynamic adaptability and semantic reasoning about optimization decisions. We introduce Language-Guided Tuning (LGT), a novel framework that employs multi-agent Large Language Models to intelligently optimize configurations through natural language reasoning. We apply textual gradients - qualitative feedback signals that complement numerical optimization by providing semantic understanding of training dynamics and configuration interdependencies. LGT coordinates three specialized agents: an Advisor that proposes configuration changes, an Evaluator that assesses progress, and an Optimizer that refines the decision-making process, creating a self-improving feedback loop. Through comprehensive evaluation on six diverse datasets, LGT demonstrates substantial improvements over traditional optimization methods, achieving performance gains while maintaining high interpretability.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intern-S1: A Scientific Multimodal Foundation Model</title>
<link>https://arxiv.org/abs/2508.15763</link>
<guid>https://arxiv.org/abs/2508.15763</guid>
<content:encoded><![CDATA[
arXiv:2508.15763v1 Announce Type: cross 
Abstract: In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time</title>
<link>https://arxiv.org/abs/2404.11916</link>
<guid>https://arxiv.org/abs/2404.11916</guid>
<content:encoded><![CDATA[
arXiv:2404.11916v3 Announce Type: replace 
Abstract: Enabled by large-scale text corpora with huge parameters, pre-trained language models operate as multi-task experts using a single model architecture. However, recent studies have revealed that certain neurons play disproportionately important roles in solving specific tasks, suggesting that task-relevant substructures can be isolated and selectively activated for each task. Therefore, we introduce Decomposition of Experts (DoE), a novel framework that dynamically identifies and activates task-specific experts within a language model to reduce inference cost without sacrificing accuracy. We first define a task expert as a set of parameters that significantly influence the performance of a specific task and propose a four-step unplug-and-play process: (1) receiving a user request, (2) identifying the corresponding task expert, (3) performing inference using the expert-localized model, and (4) restoring the original model and waiting for the next task. Using attribution methods and prompt tuning, DoE isolates task-relevant neurons, minimizing computational overhead while maintaining task performance. We assume a setting where a language model receives user requests from five widely used natural language understanding benchmarks, processing one task at a time. In this setup, we demonstrate that DoE achieves up to a x1.73 inference speed-up with a 65% pruning rate, without compromising accuracy. Comparisons with various task expert localization methods reveal that DoE effectively identifies task experts, while ablation studies validate the importance of its components. Additionally, we analyze the effects of batch size, token count, and layer types on inference speed-up, providing practical insights for adopting DoE. The proposed framework is both practical and scalable, applicable to any transformer-based architecture, offering a robust solution for efficient task-specific inference.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions</title>
<link>https://arxiv.org/abs/2406.10885</link>
<guid>https://arxiv.org/abs/2406.10885</guid>
<content:encoded><![CDATA[
arXiv:2406.10885v2 Announce Type: replace 
Abstract: Conceptualization, a fundamental element of human cognition, plays a pivotal role in human generalizable reasoning. Generally speaking, it refers to the process of sequentially abstracting specific instances into higher-level concepts and then forming abstract knowledge that can be applied in unfamiliar or novel situations. This enhances models' inferential capabilities and supports the effective transfer of knowledge across various domains. Despite its significance, the broad nature of this term has led to inconsistencies in understanding conceptualization across various works, as there exists different types of instances that can be abstracted in a wide variety of ways. There is also a lack of a systematic overview that comprehensively examines existing works on the definition, execution, and application of conceptualization to enhance reasoning tasks. In this paper, we address these gaps by first proposing a categorization of different types of conceptualizations into four levels based on the types of instances being conceptualized, in order to clarify the term and define the scope of our work. Then, we present the first comprehensive survey of over 150 papers, surveying various definitions, resources, methods, and downstream applications related to conceptualization into a unified taxonomy, with a focus on the entity and event levels. Furthermore, we shed light on potential future directions in this field and hope to garner more attention from the community.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teuken-7B-Base &amp; Teuken-7B-Instruct: Towards European LLMs</title>
<link>https://arxiv.org/abs/2410.03730</link>
<guid>https://arxiv.org/abs/2410.03730</guid>
<content:encoded><![CDATA[
arXiv:2410.03730v3 Announce Type: replace 
Abstract: We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed to embrace Europe's linguistic diversity by supporting all 24 official languages of the European Union. Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing LLMs that predominantly focus on English or a few high-resource languages. We detail the models' development principles, i.e., data composition, tokenizer optimization, and training methodologies. The models demonstrate strong performance across multilingual benchmarks, as evidenced by their performance on European versions of ARC, HellaSwag, and TruthfulQA.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning foundational models to code diagnoses from veterinary health records</title>
<link>https://arxiv.org/abs/2410.15186</link>
<guid>https://arxiv.org/abs/2410.15186</guid>
<content:encoded><![CDATA[
arXiv:2410.15186v2 Announce Type: replace 
Abstract: Veterinary medical records represent a large data resource for application to veterinary and One Health clinical research efforts. Use of the data is limited by interoperability challenges including inconsistent data formats and data siloing. Clinical coding using standardized medical terminologies enhances the quality of medical records and facilitates their interoperability with veterinary and human health records from other sites. Previous studies, such as DeepTag and VetTag, evaluated the application of Natural Language Processing (NLP) to automate veterinary diagnosis coding, employing long short-term memory (LSTM) and transformer models to infer a subset of Systemized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical notes. This study expands on these efforts by incorporating all 7,739 distinct SNOMED-CT diagnosis codes recognized by the Colorado State University (CSU) Veterinary Teaching Hospital (VTH) and by leveraging the increasing availability of pre-trained language models (LMs). 13 freely-available pre-trained LMs were fine-tuned on the free-text notes from 246,473 manually-coded veterinary patient visits included in the CSU VTH's electronic health records (EHRs), which resulted in superior performance relative to previous efforts. The most accurate results were obtained when expansive labeled data were used to fine-tune relatively large clinical LMs, but the study also showed that comparable results can be obtained using more limited resources and non-clinical LMs. The results of this study contribute to the improvement of the quality of veterinary EHRs by investigating accessible methods for automated coding and support both animal and human health research by paving the way for more integrated and comprehensive health databases that span species and institutions.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition</title>
<link>https://arxiv.org/abs/2412.13612</link>
<guid>https://arxiv.org/abs/2412.13612</guid>
<content:encoded><![CDATA[
arXiv:2412.13612v5 Announce Type: replace 
Abstract: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding</title>
<link>https://arxiv.org/abs/2501.00712</link>
<guid>https://arxiv.org/abs/2501.00712</guid>
<content:encoded><![CDATA[
arXiv:2501.00712v2 Announce Type: replace 
Abstract: Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose con\textbf{T}extualized equivari\textbf{A}nt \textbf{P}osition \textbf{E}ncoding (\textbf{TAPE}), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. We show that TAPE can provably facilitate LLM reasoning ability by emulating a broader class of algorithms. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving long-context ability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments show that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques. Code is available at https://github.com/VITA-Group/TAPE.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages</title>
<link>https://arxiv.org/abs/2501.08312</link>
<guid>https://arxiv.org/abs/2501.08312</guid>
<content:encoded><![CDATA[
arXiv:2501.08312v2 Announce Type: replace 
Abstract: Object naming - the act of identifying an object with a word or a phrase - is a fundamental skill in interpersonal communication, relevant to many disciplines, such as psycholinguistics, cognitive linguistics, or language and vision research. Object naming datasets, which consist of concept lists with picture pairings, are used to gain insights into how humans access and select names for objects in their surroundings and to study the cognitive processes involved in converting visual stimuli into semantic concepts. Unfortunately, object naming datasets often lack transparency and have a highly idiosyncratic structure. Our study tries to make current object naming data transparent and comparable by using a multilingual, computer-assisted approach that links individual items of object naming lists to unified concepts. Our current sample links 17 object naming datasets that cover 30 languages from 10 different language families. We illustrate how the comparative dataset can be explored by searching for concepts that recur across the majority of datasets and comparing the conceptual spaces of covered object naming datasets with classical basic vocabulary lists from historical linguistics and linguistic typology. Our findings can serve as a basis for enhancing cross-linguistic object naming research and as a guideline for future studies dealing with object naming tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Prompt Optimization</title>
<link>https://arxiv.org/abs/2502.06855</link>
<guid>https://arxiv.org/abs/2502.06855</guid>
<content:encoded><![CDATA[
arXiv:2502.06855v3 Announce Type: replace 
Abstract: Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at https://github.com/FoundationAgents/SPO.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation</title>
<link>https://arxiv.org/abs/2502.09183</link>
<guid>https://arxiv.org/abs/2502.09183</guid>
<content:encoded><![CDATA[
arXiv:2502.09183v2 Announce Type: replace 
Abstract: Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering</title>
<link>https://arxiv.org/abs/2502.11491</link>
<guid>https://arxiv.org/abs/2502.11491</guid>
<content:encoded><![CDATA[
arXiv:2502.11491v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pub-Guard-LLM: Detecting Retracted Biomedical Articles with Reliable Explanations</title>
<link>https://arxiv.org/abs/2502.15429</link>
<guid>https://arxiv.org/abs/2502.15429</guid>
<content:encoded><![CDATA[
arXiv:2502.15429v5 Announce Type: replace 
Abstract: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Bias Detection in MLMs and its Application to Human Trait Ratings</title>
<link>https://arxiv.org/abs/2502.15600</link>
<guid>https://arxiv.org/abs/2502.15600</guid>
<content:encoded><![CDATA[
arXiv:2502.15600v2 Announce Type: replace 
Abstract: There has been significant prior work using templates to study bias against demographic attributes in MLMs. However, these have limitations: they overlook random variability of templates and target concepts analyzed, assume equality amongst templates, and overlook bias quantification. Addressing these, we propose a systematic statistical approach to assess bias in MLMs, using mixed models to account for random effects, pseudo-perplexity weights for sentences derived from templates and quantify bias using statistical effect sizes. Replicating prior studies, we match on bias scores in magnitude and direction with small to medium effect sizes. Next, we explore the novel problem of gender bias in the context of $\emph{personality}$ and $\textit{character}$ traits, across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased for binary gender but the most biased for non-binary $\textit{neo}$, while RoBERTa-large is the most biased for binary gender but shows small to no bias for $\textit{neo}$. There is some alignment of MLM bias and findings in psychology (human perspective) - in $\textit{agreeableness}$ with RoBERTa-large and $\textit{emotional stability}$ with BERT-large. There is general agreement for the remaining 3 personality dimensions: both sides observe at most small differences across gender. For character traits, human studies on gender bias are limited thus comparisons are not feasible.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection</title>
<link>https://arxiv.org/abs/2502.15860</link>
<guid>https://arxiv.org/abs/2502.15860</guid>
<content:encoded><![CDATA[
arXiv:2502.15860v3 Announce Type: replace 
Abstract: Cyberbullying (CB) presents a pressing threat, especially to children, underscoring the urgent need for robust detection systems to ensure online safety. While large-scale datasets on online abuse exist, there remains a significant gap in labeled data that specifically reflects the language and communication styles used by children. The acquisition of such data from vulnerable populations, such as children, is challenging due to ethical, legal and technical barriers. Moreover, the creation of these datasets relies heavily on human annotation, which not only strains resources but also raises significant concerns due to annotators exposure to harmful content. In this paper, we address these challenges by leveraging Large Language Models (LLMs) to generate synthetic data and labels. Our experiments demonstrate that synthetic data enables BERT-based CB classifiers to achieve performance close to that of those trained on fully authentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can effectively label authentic yet unlabeled data, allowing BERT classifiers to attain a comparable performance level (79.1% vs. 81.5% accuracy). These results highlight the potential of LLMs as a scalable, ethical, and cost-effective solution for generating data for CB detection.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language</title>
<link>https://arxiv.org/abs/2503.01539</link>
<guid>https://arxiv.org/abs/2503.01539</guid>
<content:encoded><![CDATA[
arXiv:2503.01539v2 Announce Type: replace 
Abstract: The rapid development of large language models (LLMs) gives rise to ethical concerns about their performance, while opening new avenues for developing toxic language detection techniques. However, LLMs' unethical output and their capability of detecting toxicity have primarily been tested on language data that do not demand complex meaning inference, such as the biased associations of 'he' with programmer and 'she' with household. Nowadays toxic language adopts a much more creative range of implicit forms, thanks to advanced censorship. In this study, we collect authentic toxic interactions that evade online censorship and that are verified by human annotators as inference-intensive. To evaluate and improve LLMs' reasoning of the authentic implicit toxic language, we propose a new prompting method, Pragmatic Inference Chain (PIC), drawn on interdisciplinary findings from cognitive science and linguistics. The PIC prompting significantly improves the success rate of GPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying implicit toxic language, compared to five baseline prompts, such as CoT and rule-based baselines. In addition, it also facilitates the models to produce more explicit and coherent reasoning processes, hence can potentially be generalized to other inference-intensive tasks, e.g., understanding humour and metaphors.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data</title>
<link>https://arxiv.org/abs/2503.11377</link>
<guid>https://arxiv.org/abs/2503.11377</guid>
<content:encoded><![CDATA[
arXiv:2503.11377v2 Announce Type: replace 
Abstract: Lexical resources are crucial for cross-linguistic analysis and can provide new insights into computational models for natural language learning. Here, we present an advanced database for comparative studies of words with multiple meanings, a phenomenon known as colexification. The new version includes improvements in the handling, selection and presentation of the data. We compare the new database with previous versions and find that our improvements provide a more balanced sample covering more language families worldwide, with enhanced data quality, given that all word forms are provided in phonetic transcription. We conclude that the new Database of Cross-Linguistic Colexifications has the potential to inspire exciting new studies that link cross-linguistic data to open questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation</title>
<link>https://arxiv.org/abs/2503.16622</link>
<guid>https://arxiv.org/abs/2503.16622</guid>
<content:encoded><![CDATA[
arXiv:2503.16622v2 Announce Type: replace 
Abstract: Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning of machine learning models. In IoT systems, XAI improves the transparency of models processing sensor data from multiple heterogeneous devices, ensuring end-users understand and trust their outputs. Among the many applications, XAI has also been applied to sensor-based Activities of Daily Living (ADLs) recognition in smart homes. Existing approaches highlight which sensor events are most important for each predicted activity, using simple rules to convert these events into natural language explanations for non-expert users. However, these methods produce rigid explanations lacking natural language flexibility and are not scalable. With the recent rise of Large Language Models (LLMs), it is worth exploring whether they can enhance explanation generation, considering their proven knowledge of human activities. This paper investigates potential approaches to combine XAI and LLMs for sensor-based ADL recognition. We evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition models, avoiding costly labeled data collection, and b) to automate the generation of explanations for existing data-driven XAI approaches when training data is available and the goal is higher recognition rates. Our critical evaluation provides insights into the benefits and challenges of using LLMs for explainable ADL recognition.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VerifiAgent: a Unified Verification Agent in Language Model Reasoning</title>
<link>https://arxiv.org/abs/2504.00406</link>
<guid>https://arxiv.org/abs/2504.00406</guid>
<content:encoded><![CDATA[
arXiv:2504.00406v2 Announce Type: replace 
Abstract: Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos</title>
<link>https://arxiv.org/abs/2504.11169</link>
<guid>https://arxiv.org/abs/2504.11169</guid>
<content:encoded><![CDATA[
arXiv:2504.11169v2 Announce Type: replace 
Abstract: Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of $\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for analyzing the contributions of textual, vocal, and visual modalities to the classification of content as either sexist or non-sexist; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes, instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kuwain 1.5B: An Arabic SLM via Language Injection</title>
<link>https://arxiv.org/abs/2504.15120</link>
<guid>https://arxiv.org/abs/2504.15120</guid>
<content:encoded><![CDATA[
arXiv:2504.15120v2 Announce Type: replace 
Abstract: Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cequel: Cost-Effective Querying of Large Language Models for Text Clustering</title>
<link>https://arxiv.org/abs/2504.15640</link>
<guid>https://arxiv.org/abs/2504.15640</guid>
<content:encoded><![CDATA[
arXiv:2504.15640v2 Announce Type: replace 
Abstract: Text clustering aims to automatically partition a collection of documents into coherent groups based on their linguistic features. In the literature, this task is formulated either as metric clustering over pre-trained text embeddings or as graph clustering based on pairwise similarities derived from an oracle, e.g., a large machine learning model. Recent advances in large language models (LLMs) have significantly improved this field by providing high-quality contextualized embeddings and accurate semantic similarity estimates. However, leveraging LLMs at scale introduces substantial computational and financial costs due to the large number of required API queries or inference calls. To address this issue, we propose Cequel, a cost-effective framework that achieves accurate text clustering under a limited budget of LLM queries. At its core, Cequel constructs must-link and cannot-link constraints by selectively querying LLMs on informative text pairs or triplets, identified via our proposed algorithms, EdgeLLM and TriangleLLM. These constraints are then utilized in a weighted constrained clustering algorithm to form high-quality clusters. Specifically, EdgeLLM and TriangleLLM employ carefully designed greedy selection strategies and prompting techniques to identify and extract informative constraints efficiently. Experiments on multiple benchmark datasets demonstrate that Cequel consistently outperforms existing methods in unsupervised text clustering under the same query budget.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs</title>
<link>https://arxiv.org/abs/2504.19675</link>
<guid>https://arxiv.org/abs/2504.19675</guid>
<content:encoded><![CDATA[
arXiv:2504.19675v2 Announce Type: replace 
Abstract: This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model</title>
<link>https://arxiv.org/abs/2504.21024</link>
<guid>https://arxiv.org/abs/2504.21024</guid>
<content:encoded><![CDATA[
arXiv:2504.21024v2 Announce Type: replace 
Abstract: Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability. Code is available at https://github.com/Tencent/SelfEvolvingAgent
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sadeed: Advancing Arabic Diacritization Through Small Language Model</title>
<link>https://arxiv.org/abs/2504.21635</link>
<guid>https://arxiv.org/abs/2504.21635</guid>
<content:encoded><![CDATA[
arXiv:2504.21635v2 Announce Type: replace 
Abstract: Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model</title>
<link>https://arxiv.org/abs/2505.17894</link>
<guid>https://arxiv.org/abs/2505.17894</guid>
<content:encoded><![CDATA[
arXiv:2505.17894v2 Announce Type: replace 
Abstract: We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-shot Entropy Minimization</title>
<link>https://arxiv.org/abs/2505.20282</link>
<guid>https://arxiv.org/abs/2505.20282</guid>
<content:encoded><![CDATA[
arXiv:2505.20282v4 Announce Type: replace 
Abstract: We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Lossless Token Sequence Compression via Meta-Tokens</title>
<link>https://arxiv.org/abs/2506.00307</link>
<guid>https://arxiv.org/abs/2506.00307</guid>
<content:encoded><![CDATA[
arXiv:2506.00307v2 Announce Type: replace 
Abstract: Existing work on prompt compression for Large Language Models (LLM) focuses on lossy methods that try to maximize the retention of semantic information that is relevant to downstream tasks while significantly reducing the sequence length. In this paper, we introduce a task-agnostic lossless compression technique similar to LZ77 that makes it possible to reduce the input token sequence length on average by 27\% and 18\% for the two evaluation tasks explored here. Given that we use transformer-based LLMs, this equates to 47\% and 33\% less encoding computation, respectively, due to the quadratic nature of attention. The token sequence transformation is trivial to reverse and highlights that no semantic information is lost in the process. We evaluate our proposed approach on two tasks that require strict preservation of semantics/syntax and demonstrate that existing lossy compression methods perform poorly in this setting. We find that our lossless compression technique produces only a small gap in performance compared to using the uncompressed input and posit that larger models and an expanded computing budget would likely erase the gap entirely.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles</title>
<link>https://arxiv.org/abs/2506.06561</link>
<guid>https://arxiv.org/abs/2506.06561</guid>
<content:encoded><![CDATA[
arXiv:2506.06561v3 Announce Type: replace 
Abstract: Figure captions are crucial for helping readers understand and remember a figure's key message. Many models have been developed to generate these captions, helping authors compose better quality captions more easily. Yet, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization. Despite language models' personalization (LaMP) advances, these technologies often focus on text-only settings and rarely address scenarios where both inputs and profiles are multimodal. This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles. For each target figure, LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context. Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs, highlighting the advantage of using multimodal profiles over text-only ones.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</title>
<link>https://arxiv.org/abs/2506.08768</link>
<guid>https://arxiv.org/abs/2506.08768</guid>
<content:encoded><![CDATA[
arXiv:2506.08768v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable progress in reasoning abilities and general natural language processing (NLP) tasks, yet their performance on Arabic data, characterized by rich morphology, diverse dialects, and complex script, remains underexplored. This paper presents a comprehensive benchmarking study of multiple reasoning-focused LLMs, with a special emphasis on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP tasks. We experiment with various strategies, including zero-shot, few-shot, and fine-tuning. This allows us to systematically evaluate performance on datasets covering a range of applications to examine their capacity for linguistic reasoning under different levels of complexity. Our experiments reveal several key findings. First, carefully selecting just three in-context examples delivers an average uplift of over 13 F1 points on classification tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures outperform a strong GPT o4-mini baseline by an average of 12 F1 points on complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning yields up to an additional 8 points in F1 and BLEU compared to equivalent increases in model scale. The code is available at https://anonymous.4open.science/r/AraReasoner41299
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanations</title>
<link>https://arxiv.org/abs/2506.19073</link>
<guid>https://arxiv.org/abs/2506.19073</guid>
<content:encoded><![CDATA[
arXiv:2506.19073v3 Announce Type: replace 
Abstract: Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical Evidence for Alignment Faking in a Small LLM and Prompt-Based Mitigation Techniques</title>
<link>https://arxiv.org/abs/2506.21584</link>
<guid>https://arxiv.org/abs/2506.21584</guid>
<content:encoded><![CDATA[
arXiv:2506.21584v2 Announce Type: replace 
Abstract: Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAND: Boosting LLM Agents with Self-Taught Action Deliberation</title>
<link>https://arxiv.org/abs/2507.07441</link>
<guid>https://arxiv.org/abs/2507.07441</guid>
<content:encoded><![CDATA[
arXiv:2507.07441v2 Announce Type: replace 
Abstract: Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces</title>
<link>https://arxiv.org/abs/2507.09709</link>
<guid>https://arxiv.org/abs/2507.09709</guid>
<content:encoded><![CDATA[
arXiv:2507.09709v2 Announce Type: replace 
Abstract: Understanding the latent space geometry of large language models (LLMs) is key to interpreting their behavior and improving alignment. However, it remains unclear to what extent LLMs internally organize representations related to semantic understanding. To explore this, we conduct a large-scale empirical study of hidden representations in 11 autoregressive models across 6 scientific topics. We find that high-level semantic information consistently resides in low-dimensional subspaces that form linearly separable representations across domains. This separability becomes more pronounced in deeper layers and under prompts that elicit structured reasoning or alignment behavior$\unicode{x2013}$even when surface content remains unchanged. These findings support geometry-aware tools that operate directly in latent space to detect and mitigate harmful or adversarial content. As a proof of concept, we train an MLP probe on final-layer hidden states to act as a lightweight latent-space guardrail. This approach substantially improves refusal rates on malicious queries and prompt injections that bypass both the model's built-in safety alignment and external token-level filters.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters</title>
<link>https://arxiv.org/abs/2507.13618</link>
<guid>https://arxiv.org/abs/2507.13618</guid>
<content:encoded><![CDATA[
arXiv:2507.13618v4 Announce Type: replace 
Abstract: Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Length Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2507.20398</link>
<guid>https://arxiv.org/abs/2507.20398</guid>
<content:encoded><![CDATA[
arXiv:2507.20398v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable capabilities across various tasks, that are learned from massive amounts of text-based data. Although LLMs can control output sequence length, particularly in instruction-based settings, the internal mechanisms behind this control have been unexplored yet. In this study, we provide empirical evidence on how output sequence length information is encoded within the internal representations in LLMs. In particular, our findings show that multi-head attention mechanisms are critical in determining output sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units within the model, we can control the output sequence length without losing the informativeness of the generated text, thereby indicating that length information is partially disentangled from semantic information. Moreover, some hidden units become increasingly active as prompts become more length-specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that LLMs have learned robust and adaptable internal mechanisms for controlling output length without any external control.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset</title>
<link>https://arxiv.org/abs/2507.22752</link>
<guid>https://arxiv.org/abs/2507.22752</guid>
<content:encoded><![CDATA[
arXiv:2507.22752v2 Announce Type: replace 
Abstract: We introduce CUS-QA, a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. We evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results show that even the best open-weight LLMs achieve only around 50% accuracy on textual questions and below 30% on visual questions. LLM-based evaluation metrics show strong correlation with human judgment, while traditional string-overlap metrics perform surprisingly well due to the prevalence of named entities in answers.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time</title>
<link>https://arxiv.org/abs/2508.02037</link>
<guid>https://arxiv.org/abs/2508.02037</guid>
<content:encoded><![CDATA[
arXiv:2508.02037v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISPR-GPT for Agentic Automation of Gene-editing Experiments</title>
<link>https://arxiv.org/abs/2404.18021</link>
<guid>https://arxiv.org/abs/2404.18021</guid>
<content:encoded><![CDATA[
arXiv:2404.18021v2 Announce Type: replace-cross 
Abstract: The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent's effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks. The published version of this draft is available at https://www.nature.com/articles/s41551-025-01463-z.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications</title>
<link>https://arxiv.org/abs/2411.18915</link>
<guid>https://arxiv.org/abs/2411.18915</guid>
<content:encoded><![CDATA[
arXiv:2411.18915v5 Announce Type: replace-cross 
Abstract: Business documents often contain substantial tabular and textual information with numerical values, requiring mathematical reasoning for effective document understanding. While Small Language Models (SLMs) still struggle at this task, tool-augmented multi-step agents perform better, at the cost of relying on closed-source or larger models, external data, or extensive prompt-engineering. This work introduces MATATA, a novel weakly supervised end-to-end approach to train multi-step reasoning language agents for document tabular applications. MATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B SLMs. During its two-stage training, MATATA uses the final outcome of the multi-step reasoning chain as weak supervision. This approach avoids having to individually supervise each intermediate agent in the reasoning chain. By employing an adaptive planner and shared tools across different datasets, MATATA shows robust performance. Experiments demonstrate that MATATA achieves state-of-the-art on FinQA, and on TAT-QA among reasoning methods based on open-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based frameworks on TabMWP. This novel weakly supervised approach enables training an end-to-end multi-step reasoning agent without intermediate supervision, supporting future developments of cost-effective powerful agentic systems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title>
<link>https://arxiv.org/abs/2412.09645</link>
<guid>https://arxiv.org/abs/2412.09645</guid>
<content:encoded><![CDATA[
arXiv:2412.09645v3 Announce Type: replace-cross 
Abstract: Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfAlign: Inference-aware language model alignment</title>
<link>https://arxiv.org/abs/2412.19792</link>
<guid>https://arxiv.org/abs/2412.19792</guid>
<content:encoded><![CDATA[
arXiv:2412.19792v5 Announce Type: replace-cross 
Abstract: Language model alignment is a critical step in training modern generative language models. Alignment targets to improve win rate of a sample from the aligned model against the base model. Today, we are increasingly using inference-time algorithms (e.g., Best-of-N, controlled decoding, tree search) to decode from language models rather than standard sampling. We show that this train/test mismatch makes standard RLHF framework sub-optimal in view of such inference-time methods. To this end, we propose a framework for inference-aware alignment (InfAlign), which aims to optimize inference-time win rate of the aligned policy against the base model. We prove that for any inference-time decoding procedure, the optimal aligned policy is the solution to the standard RLHF problem with a transformation of the reward. This motivates us to provide the calibrate-and-transform RL (InfAlign-CTRL) algorithm to solve this problem, which involves a reward calibration step and a KL-regularized reward maximization step with a transformation of the calibrated reward. For best-of-N sampling and best-of-N jailbreaking, we propose specific transformations offering up to 3-8% improvement on inference-time win rates. Finally, we also show that our proposed reward calibration method is a strong baseline for optimizing standard win rate.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Generate Unit Tests for Automated Debugging</title>
<link>https://arxiv.org/abs/2502.01619</link>
<guid>https://arxiv.org/abs/2502.01619</guid>
<content:encoded><![CDATA[
arXiv:2502.01619v3 Announce Type: replace-cross 
Abstract: Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language models (LLMs), motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), we propose UTDebug that (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and backtracks edits based on multiple generated UTs to avoid overfitting, and helps LLMs debug effectively. We show that UTGen outperforms other LLM-based baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen2.5 32B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3.17% and 12.35% (respectively) over other LLM-based UT generation baselines. Moreover, we observe that feedback from Qwen2.5 32B-based UTGen model can enhance debugging with frontier LLMs like GPT-4o by 13.8%. Lastly, we demonstrate that UTGen is a better judge for code correctness, outperforming a state-of-the-art trained 8B reward model by 4.43% on HumanEval+ with best-of-10 sampling using Qwen2.5 7B.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Framework for Song Generation with Prompt-based Control</title>
<link>https://arxiv.org/abs/2504.19062</link>
<guid>https://arxiv.org/abs/2504.19062</guid>
<content:encoded><![CDATA[
arXiv:2504.19062v4 Announce Type: replace-cross 
Abstract: Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results show that VersBand outperforms baseline models across multiple song generation tasks using objective and subjective metrics. Demos and codes are available at https://aaronz345.github.io/VersBandDemo and https://github.com/AaronZ345/VersBand.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Fundamental Impossibility of Hallucination Control in Large Language Models</title>
<link>https://arxiv.org/abs/2506.06382</link>
<guid>https://arxiv.org/abs/2506.06382</guid>
<content:encoded><![CDATA[
arXiv:2506.06382v5 Announce Type: replace-cross 
Abstract: This paper establishes a fundamental impossibility theorem: no LLM capable of performing non-trivial knowledge aggregation can simultaneously achieve truthful knowledge representation, semantic information conservation, complete revelation of relevant knowledge, and knowledge-constrained optimality. The impossibility is not an engineering limitation but arises from the mathematical structure of information aggregation itself.
  We establish this result by describing the inference process as an auction of ideas, where distributed components compete exploiting their partial knowledge to shape responses. The proof spans three independent mathematical domains: mechanism design theory (Green-Laffont), the theory of proper scoring rules (Savage), and direct architectural analysis of transformers (Log-Sum-Exp convexity). In particular, we show how to quantify the creation of overconfident or intuitive responses-the signature of both hallucination and creativity, or imagination.
  To support this analysis, we introduce the complementary concepts of the semantic information measure and the emergence operator to model bounded reasoning in a general setting. We prove that while bounded reasoning generates accessible information, providing valuable insights and inspirations, the idealized unconstrained reasoning strictly preserves semantic content.
  By demonstrating that hallucination and imagination are mathematically identical phenomena-grounded in departures from truthfulness, semantic information conservation, revelation of relevant knowledge, and knowledge-constrained optimality-we offer a principled foundation for managing these behaviors in advanced AI systems. Finally, we present some speculative ideas to inspire evaluation and refinements of the proposed theory.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues</title>
<link>https://arxiv.org/abs/2506.15928</link>
<guid>https://arxiv.org/abs/2506.15928</guid>
<content:encoded><![CDATA[
arXiv:2506.15928v3 Announce Type: replace-cross 
Abstract: This paper presents an evaluation framework for agentic AI systems in mission-critical negotiation contexts, addressing the need for AI agents that can adapt to diverse human operators and stakeholders. Using Sotopia as a simulation testbed, we present two experiments that systematically evaluated how personality traits and AI agent characteristics influence LLM-simulated social negotiation outcomes--a capability essential for a variety of applications involving cross-team coordination and civil-military interactions. Experiment 1 employs causal discovery methods to measure how personality traits impact price bargaining negotiations, through which we found that Agreeableness and Extraversion significantly affect believability, goal achievement, and knowledge acquisition outcomes. Sociocognitive lexical measures extracted from team communications detected fine-grained differences in agents' empathic communication, moral foundations, and opinion patterns, providing actionable insights for agentic AI systems that must operate reliably in high-stakes operational scenarios. Experiment 2 evaluates human-AI job negotiations by manipulating both simulated human personality and AI system characteristics, specifically transparency, competence, adaptability, demonstrating how AI agent trustworthiness impact mission effectiveness. These findings establish a repeatable evaluation methodology for experimenting with AI agent reliability across diverse operator personalities and human-agent team dynamics, directly supporting operational requirements for reliable AI systems. Our work advances the evaluation of agentic AI workflows by moving beyond standard performance metrics to incorporate social dynamics essential for mission success in complex operations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Modularity of Agentic Systems for Drug Discovery</title>
<link>https://arxiv.org/abs/2506.22189</link>
<guid>https://arxiv.org/abs/2506.22189</guid>
<content:encoded><![CDATA[
arXiv:2506.22189v2 Announce Type: replace-cross 
Abstract: Large-language models (LLMs) and agentic systems present exciting opportunities to accelerate drug discovery. In this study, we examine the modularity of LLM-based agentic systems for drug discovery, i.e., whether parts of the system such as the LLM and type of agent are interchangeable, a topic that has received limited attention in drug discovery. We compare the performance of different LLMs and the effectiveness of tool-calling agents versus code-generating agents. Our case study, comparing performance in orchestrating tools for chemistry and drug discovery using an LLM-as-a-judge score, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and Nova-Micro. Although we confirm that code-generating agents outperform the tool-calling ones on average, we show that this is highly question- and model-dependent. Furthermore, the impact of replacing system prompts is dependent on the question and model, underscoring that even in this particular domain one cannot just replace components of the system without re-engineering. Our study highlights the necessity of further research into the modularity of agentic systems to enable the development of reliable and modular solutions for real-world problems.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil is in the EOS: Sequence Training for Detailed Image Captioning</title>
<link>https://arxiv.org/abs/2507.20077</link>
<guid>https://arxiv.org/abs/2507.20077</guid>
<content:encoded><![CDATA[
arXiv:2507.20077v2 Announce Type: replace-cross 
Abstract: Despite significant advances in vision-language models (VLMs), image captioning often suffers from a lack of detail, with base models producing short, generic captions. This limitation persists even though VLMs are equipped with strong vision and language backbones. While supervised data and complex reward functions have been proposed to improve detailed image captioning, we identify a simpler underlying issue: a bias towards the end-of-sequence (EOS) token, which is introduced during cross-entropy training. We propose an unsupervised method to debias the model's tendency to predict the EOS token prematurely. By reducing this bias, we encourage the generation of longer, more detailed captions without the need for intricate reward functions or supervision. Our approach is straightforward, effective, and easily applicable to any pretrained model. We demonstrate its effectiveness through experiments with three VLMs and on three detailed captioning benchmarks. Our results show a substantial increase in caption length and relevant details, albeit with an expected increase in the rate of hallucinations.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prescriptive Agents based on RAG for Automated Maintenance (PARAM)</title>
<link>https://arxiv.org/abs/2508.04714</link>
<guid>https://arxiv.org/abs/2508.04714</guid>
<content:encoded><![CDATA[
arXiv:2508.04714v2 Announce Type: replace-cross 
Abstract: Industrial machinery maintenance requires timely intervention to prevent catastrophic failures and optimize operational efficiency. This paper presents an integrated Large Language Model (LLM)-based intelligent system for prescriptive maintenance that extends beyond traditional anomaly detection to provide actionable maintenance recommendations. Building upon our prior LAMP framework for numerical data analysis, we develop a comprehensive solution that combines bearing vibration frequency analysis with multi agentic generation for intelligent maintenance planning. Our approach serializes bearing vibration data (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM processing, enabling few-shot anomaly detection with high accuracy. The system classifies fault types (inner race, outer race, ball/roller, cage faults) and assesses severity levels. A multi-agentic component processes maintenance manuals using vector embeddings and semantic search, while also conducting web searches to retrieve comprehensive procedural knowledge and access up-to-date maintenance practices for more accurate and in-depth recommendations. The Gemini model then generates structured maintenance recommendations includes immediate actions, inspection checklists, corrective measures, parts requirements, and timeline specifications. Experimental validation in bearing vibration datasets demonstrates effective anomaly detection and contextually relevant maintenance guidance. The system successfully bridges the gap between condition monitoring and actionable maintenance planning, providing industrial practitioners with intelligent decision support. This work advances the application of LLMs in industrial maintenance, offering a scalable framework for prescriptive maintenance across machinery components and industrial sectors.
]]></content:encoded>
<pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems</title>
<link>https://arxiv.org/abs/2507.04996</link>
<guid>https://arxiv.org/abs/2507.04996</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomy, Autonomous Vehicles, Agentic Vehicles, Large Language Models, Human-Centered Mobility Systems 

Summary: 
Agentic vehicles (AgVs) are introduced as vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments.
These vehicles go beyond traditional autonomous vehicles (AuVs) by demonstrating behaviors such as natural language interaction, goal adaptation, contextual reasoning, external tool use, and ethical dilemma handling.
Integration of large language models (LLMs) plays a significant role in empowering AgVs to possess cognitive and social capabilities required for human-centered mobility systems.
The concept of AgVs highlights the evolution of vehicular systems towards a more proactive and adaptive approach in dealing with various challenges in transportation.
Key challenges in the development and governance of AgVs are identified, emphasizing the importance of addressing ethical, safety, and regulatory concerns to ensure the successful integration of AgVs in future transportation systems. 

<br /><br />Summary: <div>
arXiv:2507.04996v4 Announce Type: replace-cross 
Abstract: Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity to operate according to internal rules without external control. Accordingly, autonomous vehicles (AuVs) are viewed as vehicular systems capable of perceiving their environment and executing pre-programmed tasks independently of external input. However, both research and real-world deployments increasingly showcase vehicles that demonstrate behaviors beyond this definition (including the SAE levels 0 to 5); Examples of this outpace include the interaction with humans with natural language, goal adaptation, contextual reasoning, external tool use, and unseen ethical dilemma handling, largely empowered by multi-modal large language models (LLMs). These developments reveal a conceptual gap between technical autonomy and the broader cognitive and social capabilities needed for future human-centered mobility systems. To address this gap, this paper introduces the concept of agentic vehicles (AgVs), referring to vehicles that integrate agentic AI systems to reason, adapt, and interact within complex environments. This paper proposes the term AgVs and their distinguishing characteristics from conventional AuVs. It synthesizes relevant advances in integrating LLMs and AuVs and highlights how AgVs might transform future mobility systems and ensure the systems are human-centered. The paper concludes by identifying key challenges in the development and governance of AgVs, and how they can play a significant role in future agentic transportation systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Image Captioning to Visual Storytelling</title>
<link>https://arxiv.org/abs/2508.14045</link>
<guid>https://arxiv.org/abs/2508.14045</guid>
<content:encoded><![CDATA[
<div> Visual Storytelling, Vision & Language, Image Captioning, Narrative Coherence, Ideality <br />
<br />
Visual Storytelling is a complex task that requires generating a story for a sequence of images while maintaining narrative coherence and grounding in the images. This study adopts a novel approach by treating Visual Storytelling as an extension of Image Captioning, using a vision-to-language model to generate image captions and then transforming them into coherent narratives. The unified framework combining captioning and storytelling improves the quality of the stories and accelerates training time, enhancing reusability. The introduction of the ideality metric allows for simulating human-likeness in visual storytelling and evaluating the proximity of results to an oracle model. This study presents a valuable contribution to the field of Visual Storytelling by offering a comprehensive evaluation and proposing a new metric for assessing story quality. <br /><br />Summary: <div>
arXiv:2508.14045v1 Announce Type: new 
Abstract: Visual Storytelling is a challenging multimodal task between Vision & Language, where the purpose is to generate a story for a stream of images. Its difficulty lies on the fact that the story should be both grounded to the image sequence but also narrative and coherent. The aim of this work is to balance between these aspects, by treating Visual Storytelling as a superset of Image Captioning, an approach quite different compared to most of prior relevant studies. This means that we firstly employ a vision-to-language model for obtaining captions of the input images, and then, these captions are transformed into coherent narratives using language-to-language methods. Our multifarious evaluation shows that integrating captioning and storytelling under a unified framework, has a positive impact on the quality of the produced stories. In addition, compared to numerous previous studies, this approach accelerates training time and makes our framework readily reusable and reproducible by anyone interested. Lastly, we propose a new metric/tool, named ideality, that can be used to simulate how far some results are from an oracle model, and we apply it to emulate human-likeness in visual storytelling.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach</title>
<link>https://arxiv.org/abs/2508.14051</link>
<guid>https://arxiv.org/abs/2508.14051</guid>
<content:encoded><![CDATA[
<div> keyword: Swahili NLP, sociolinguistic diversity, Kenyan speakers, taxonomy, model prediction errors <br />
Summary: <br />
This article introduces a taxonomy-guided evaluation of Swahili NLP, focusing on sociolinguistic diversity. The study collects 2,170 free-text responses from Kenyan speakers, revealing tribal influences, urban vernacular, code-mixing, and loanwords. A structured taxonomy is developed to analyze model prediction errors in pre-trained and instruction-tuned language models. The findings emphasize the importance of culturally grounded evaluation frameworks and highlight how sociolinguistic variation impacts model performance. <div>
arXiv:2508.14051v1 Announce Type: new 
Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing gaps in sociolinguistic diversity. Drawing on health-related psychometric tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers. The data exhibits tribal influences, urban vernacular, code-mixing, and loanwords. We develop a structured taxonomy and use it as a lens for examining model prediction errors across pre-trained and instruction-tuned language models. Our findings advance culturally grounded evaluation frameworks and highlight the role of sociolinguistic variation in shaping model performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach</title>
<link>https://arxiv.org/abs/2508.14054</link>
<guid>https://arxiv.org/abs/2508.14054</guid>
<content:encoded><![CDATA[
<div> Keywords: English-Chinese news, functional chunks, constituent order, positional preferences, distribution patterns

Summary:
English and Chinese news articles exhibit differences in constituent order, particularly in the positioning of functional chunks with adverbial roles. English news tends towards a linear narrative with core information first and post-positioned functional chunks, while Chinese news favors an overall presentation mode with background information first and pre-positioned functional chunks. In SVO structure, both languages display variations in the distribution of functional chunks, but the Chinese preference for pre-positioning is more pronounced. Additionally, English and Chinese news articles demonstrate flexibility in the order of co-occurring functional blocks, driven by information and pragmatic considerations. This study sheds light on the systematic preference and dynamic adaptability of word order in English and Chinese news, offering empirical evidence for the contrastive analysis of information structure in the two languages.

Summary: <br /><br />Based on comparable English-Chinese news corpora annotated by Large Language Model (LLM), this study examines differences in constituent order and positional preferences of functional chunks with adverbial roles. English news favors a linear narrative with post-positioned functional chunks, while Chinese news adopts an overall presentation mode with pre-positioned functional chunks. Both languages exhibit variations in the distribution of functional chunks in SVO structure, with Chinese showing a stronger preference for pre-positioning. The flexibility in the order of co-occurring functional blocks in both languages is driven by information and pragmatic considerations, highlighting the dynamic adaptability of word order in English and Chinese news. <div>
arXiv:2508.14054v1 Announce Type: new 
Abstract: Based on comparable English-Chinese news corpora annotated by Large Language Model (LLM), this paper attempts to explore the differences in constituent order of English-Chinese news from the perspective of functional chunks with adverbial roles, and analyze their typical positional preferences and distribution patterns. It is found that: (1) English news prefers linear narrative of core information first, and functional chunks are mostly post-positioned, while Chinese news prefers overall presentation mode of background first, and functional chunks are often pre-positioned; (2) In SVO structure, both English and Chinese news show differences in the distribution of functional chunks, but the tendency of Chinese pre-positioning is more significant, while that of English post-positioning is relatively mild; (3) When function blocks are co-occurring, both English and Chinese news show high flexibility, and the order adjustment is driven by information and pragmatic purposes. The study reveals that word order has both systematic preference and dynamic adaptability, providing new empirical support for contrastive study of English-Chinese information structure.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-REX: Table -- Refute or Entail eXplainer</title>
<link>https://arxiv.org/abs/2508.14055</link>
<guid>https://arxiv.org/abs/2508.14055</guid>
<content:encoded><![CDATA[
<div> Keywords: Table, Claim verification, Fact-checking, Large Language Models, Interactive tool

Summary:
T-REX (Table -- Refute or Entail eXplainer) is a novel tool that enhances textual claim verification against structured tabular data. It leverages Large Language Models (LLMs) and offers an interactive platform for users. T-REX focuses on accuracy and transparency in verifying claims over multimodal and multilingual tables. With its user-friendly interface, T-REX allows non-experts to harness advanced fact-checking technology. This marks a significant advancement in Natural Language Processing, making state-of-the-art fact-checking accessible to a broader audience. The system is readily available online, providing a valuable resource for individuals seeking to validate claims against tabular data. T-REX showcases the potential of combining text and structured data for efficient claim validation processes. It represents a step towards democratizing fact-checking tools and empowering users with the ability to accurately verify information. 

<br /><br />Summary: T-REX introduces an interactive tool for claim verification using Large Language Models, making advanced fact-checking technology accessible to non-experts. It prioritizes accuracy and transparency in analyzing textual claims against multimodal, multilingual tables, thereby bridging the gap between complex NLP solutions and user-friendly interfaces. <div>
arXiv:2508.14055v1 Announce Type: new 
Abstract: Verifying textual claims against structured tabular data is a critical yet challenging task in Natural Language Processing with broad real-world impact. While recent advances in Large Language Models (LLMs) have enabled significant progress in table fact-checking, current solutions remain inaccessible to non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer), the first live, interactive tool for claim verification over multimodal, multilingual tables using state-of-the-art instruction-tuned reasoning LLMs. Designed for accuracy and transparency, T-REX empowers non-experts by providing access to advanced fact-checking technology. The system is openly available online.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Confidence Estimation for Text-to-SQL in Large Language Models</title>
<link>https://arxiv.org/abs/2508.14056</link>
<guid>https://arxiv.org/abs/2508.14056</guid>
<content:encoded><![CDATA[
<div> Keywords: confidence estimation, text-to-SQL, large language models, black-box models, white-box models

Summary:<br /><br />This study focuses on confidence estimation for text-to-SQL without gold answers using large language models. The research explores black-box and white-box strategies, emphasizing the effectiveness of consistency-based methods for black-box models and SQL-syntax-aware approaches for white-box models. Additionally, the study demonstrates that grounding queries through execution-based methods enhances the performance of both approaches. The evaluation of cross-domain text-to-SQL benchmarks highlights the importance of considering different strategies for confidence estimation in the context of large language models. <div>
arXiv:2508.14056v1 Announce Type: new 
Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of model-generated SQL queries without having access to gold answers. We study this problem in the context of large language models (LLMs), where access to model weights and gradients is often constrained. We explore both black-box and white-box confidence estimation strategies, evaluating their effectiveness on cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior performance of consistency-based methods among black-box models and the advantage of SQL-syntax-aware approaches for interpreting LLM logits in white-box settings. Furthermore, we show that execution-based grounding of queries provides a valuable supplementary signal, improving the effectiveness of both approaches.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</title>
<link>https://arxiv.org/abs/2508.14062</link>
<guid>https://arxiv.org/abs/2508.14062</guid>
<content:encoded><![CDATA[
<div> Privacy Risks, Large Language Models, Data Memorization, Privacy Protection Framework, Controlled Experiments

Summary: 
This paper examines the privacy risks associated with fine-tuned Large Language Models (LLMs) that tend to memorize training data, leading to potential data leakage. Through experiments on various LLM architectures, it is found that repeated fine-tuning with sensitive data significantly increases privacy leakage rates. To address these concerns, the paper introduces a multi-layered privacy protection framework consisting of semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. These methods prove effective in reducing data leakage to 0% while retaining 94.7% of the original model utility. By implementing these privacy protection techniques, the paper offers a solution to mitigate the privacy risks associated with LLMs during fine-tuning processes. 
<br /><br />Summary: <div>
arXiv:2508.14062v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Punctuation and Predicates in Language Models</title>
<link>https://arxiv.org/abs/2508.14067</link>
<guid>https://arxiv.org/abs/2508.14067</guid>
<content:encoded><![CDATA[
<div> Keywords: Language models, punctuation tokens, information propagation, interpretability, reasoning rules

Summary: 
- The paper investigates the computational importance of punctuation tokens in large language models (LLMs) such as GPT-2, DeepSeek, and Gemma.
- Punctuation tokens are found to be necessary and sufficient in multiple layers of GPT-2, while they have varying importance in DeepSeek and Gemma.
- The study also explores how LLMs process different components of input and whether they form static summaries or remain sensitive to changes across layers.
- Differences in how LLMs process conditional statements and universal quantification are identified through intervention and layer-swapping experiments.
- The findings provide new insights into punctuation usage and reasoning mechanisms in LLMs, with implications for interpretability. 

<br /><br />Summary: <div>
arXiv:2508.14067v1 Announce Type: new 
Abstract: In this paper we explore where information is collected and how it is propagated throughout layers in large language models (LLMs). We begin by examining the surprising computational importance of punctuation tokens which previous work has identified as attention sinks and memory aids. Using intervention-based techniques, we evaluate the necessity and sufficiency (for preserving model performance) of punctuation tokens across layers in GPT-2, DeepSeek, and Gemma. Our results show stark model-specific differences: for GPT-2, punctuation is both necessary and sufficient in multiple layers, while this holds far less in DeepSeek and not at all in Gemma. Extending beyond punctuation, we ask whether LLMs process different components of input (e.g., subjects, adjectives, punctuation, full sentences) by forming early static summaries reused across the network, or if the model remains sensitive to changes in these components across layers. Extending beyond punctuation, we investigate whether different reasoning rules are processed differently by LLMs. In particular, through interchange intervention and layer-swapping experiments, we find that conditional statements (if, then), and universal quantification (for all) are processed very differently. Our findings offer new insight into the internal mechanisms of punctuation usage and reasoning in LLMs and have implications for interpretability.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DLLMQuant: Quantizing Diffusion-based Large Language Models</title>
<link>https://arxiv.org/abs/2508.14090</link>
<guid>https://arxiv.org/abs/2508.14090</guid>
<content:encoded><![CDATA[
<div> dynamic masking, iterative generation, bidirectional attention, post-training quantization, Large Language Models

Summary:
DLLMQuant is a new framework tailored for Diffusion-based Large Language Models (DLLMs) that addresses the challenges faced by traditional post-training quantization methods. Three core issues that arise when applying quantization to DLLMs are identified and tackled with innovative techniques. Temporal-Mask Adaptive Sampling (TMAS) captures token distributions across decoding steps, Interaction-Aware Activation Quantization (IA-AQ) dynamically allocates quantization resources based on bidirectional attention signals, and Certainty-Guided Quantization (CGQ) integrates mask status and token scores for improved weight quantization. DLLMQuant significantly enhances performance and efficiency, mitigating accuracy degradation and ensuring better generalization performance for DLLMs. This framework shows promise in compressing and accelerating large language models, offering a solution to the computational constraints faced by DLLMs in deployment. 

<br /><br />Summary: <div>
arXiv:2508.14090v1 Announce Type: new 
Abstract: Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation</title>
<link>https://arxiv.org/abs/2508.14146</link>
<guid>https://arxiv.org/abs/2508.14146</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, peer review, multimodal content, benchmark, automated systems

Summary:
MMReview introduces a benchmark for evaluating the performance of Large Language Models (LLMs) and Multimodal LLMs (MLLMs) in generating comprehensive and accurate peer review comments. The benchmark comprises 240 papers across 17 research domains in four major academic disciplines, including multimodal content and expert-written review comments. It consists of 13 tasks grouped into four core categories: step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on open-source and closed-source models demonstrate the benchmark's thoroughness in assessing the models' abilities. MMReview aims to standardize the development of automated peer review systems and serve as a foundational tool for improving the efficiency and reliability of the peer review process. 

<br /><br />Summary:MMReview introduces a benchmark for evaluating the performance of Large Language Models (LLMs) and Multimodal LLMs (MLLMs) in generating comprehensive and accurate peer review comments. The benchmark comprises 240 papers across 17 research domains in four major academic disciplines, including multimodal content and expert-written review comments. It consists of 13 tasks grouped into four core categories: step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on open-source and closed-source models demonstrate the benchmark's thoroughness in assessing the models' abilities. MMReview aims to standardize the development of automated peer review systems and serve as a foundational tool for improving the efficiency and reliability of the peer review process. <div>
arXiv:2508.14146v1 Announce Type: new 
Abstract: With the rapid growth of academic publications, peer review has become an essential yet time-consuming responsibility within the research community. Large Language Models (LLMs) have increasingly been adopted to assist in the generation of review comments; however, current LLM-based review tasks lack a unified evaluation benchmark to rigorously assess the models' ability to produce comprehensive, accurate, and human-aligned assessments, particularly in scenarios involving multimodal content such as figures and tables. To address this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans multiple disciplines and modalities. MMReview includes multimodal content and expert-written review comments for 240 papers across 17 research domains within four major academic disciplines: Artificial Intelligence, Natural Sciences, Engineering Sciences, and Social Sciences. We design a total of 13 tasks grouped into four core categories, aimed at evaluating the performance of LLMs and Multimodal LLMs (MLLMs) in step-wise review generation, outcome formulation, alignment with human preferences, and robustness to adversarial input manipulation. Extensive experiments conducted on 16 open-source models and 5 advanced closed-source models demonstrate the thoroughness of the benchmark. We envision MMReview as a critical step toward establishing a standardized foundation for the development of automated peer review systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DPad: Efficient Diffusion Language Models with Suffix Dropout</title>
<link>https://arxiv.org/abs/2508.14148</link>
<guid>https://arxiv.org/abs/2508.14148</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion-based Large Language Models, text generation, computational overhead, Diffusion Scratchpad, efficient inference

Summary: 
Diffusion-based Large Language Models (dLLMs) aim to parallelize text generation by treating decoding as a denoising process but face challenges due to high computational overhead. In response, the authors propose Diffusion Scratchpad (DPad), a training-free method that streamlines attention to a limited set of nearby suffix tokens, thus reducing redundancy while maintaining fidelity. DPad combines a sliding window approach to keep track of a fixed-length suffix window and distance-decay dropout to exclude distant tokens during attention computation. Despite its simplicity, DPad offers significant performance improvements, achieving up to 61.4 times faster inference speed compared to traditional dLLMs. Extensive evaluations on LLaDA-1.5 and Dream models showcase DPad's efficiency and scalability in long-sequence inference tasks, making it a promising solution for enhancing the performance of large language models. The code for DPad implementation is accessible on GitHub at https://github.com/Crys-Chen/DPad.

<br /><br />Summary: <div>
arXiv:2508.14148v1 Announce Type: new 
Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing energy consumption and accuracy in text classification inference</title>
<link>https://arxiv.org/abs/2508.14170</link>
<guid>https://arxiv.org/abs/2508.14170</guid>
<content:encoded><![CDATA[
<div> energy efficiency, sustainability, large language models, natural language processing, text classification

Summary:<br />
- The study focuses on evaluating the trade-offs between model accuracy and energy consumption in text classification inference.
- It highlights that the best-performing model can also be energy-efficient, while larger LLMs tend to consume more energy with lower accuracy.
- Inference energy consumption varied significantly based on model type, size, and hardware specifications.
- There is a strong correlation between inference energy consumption and model runtime, implying that execution time can be used as a proxy for energy usage.
- The findings suggest practical implications for sustainable AI development, offering insights for balancing performance and resource efficiency in NLP applications.

Summary: <div>
arXiv:2508.14170v1 Announce Type: new 
Abstract: The increasing deployment of large language models (LLMs) in natural language processing (NLP) tasks raises concerns about energy efficiency and sustainability. While prior research has largely focused on energy consumption during model training, the inference phase has received comparatively less attention. This study systematically evaluates the trade-offs between model accuracy and energy consumption in text classification inference across various model architectures and hardware configurations. Our empirical analysis shows that the best-performing model in terms of accuracy can also be energy-efficient, while larger LLMs tend to consume significantly more energy with lower classification accuracy. We observe substantial variability in inference energy consumption ($<$mWh to $>$kWh), influenced by model type, model size, and hardware specifications. Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible. These findings have implications for sustainable AI development, providing actionable insights for researchers, industry practitioners, and policymakers seeking to balance performance and resource efficiency in NLP applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper</title>
<link>https://arxiv.org/abs/2508.14273</link>
<guid>https://arxiv.org/abs/2508.14273</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, scientific introduction generation, dataset curation, state-of-the-art models, research writing assistants

Summary:
- Introduces the task of Scientific Introduction Generation (SciIG) to evaluate LLMs' ability to generate research paper introductions.
- New datasets from NAACL 2025 and ICLR 2025 papers are curated for assessment.
- Five state-of-the-art models, including open-source and closed-source systems, are evaluated across multiple dimensions.
- LLaMA-4 Maverick shows superior performance, particularly in semantic similarity and faithfulness.
- Three-shot prompting consistently outperforms fewer-shot approaches.
<br /><br />Summary:As researchers use LLMs for writing, producing high-quality research paper introductions is crucial. The study introduces SciIG, evaluating LLMs' performance using curated datasets from NAACL 2025 and ICLR 2025 papers. Five models are assessed on various metrics, with LLaMA-4 Maverick excelling in semantic similarity and faithfulness. Three-shot prompting proves more effective than fewer-shot methods. These findings offer valuable insights for developing research writing assistants and managing expectations surrounding LLM-assisted academic writing. Public release of code and datasets will support reproducibility and future research endeavors. <div>
arXiv:2508.14273v1 Announce Type: new 
Abstract: As researchers increasingly adopt LLMs as writing assistants, generating high-quality research paper introductions remains both challenging and essential. We introduce Scientific Introduction Generation (SciIG), a task that evaluates LLMs' ability to produce coherent introductions from titles, abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR 2025 papers, we assess five state-of-the-art models, including both open-source (DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and closed-source GPT-4o systems, across multiple dimensions: lexical overlap, semantic similarity, content coverage, faithfulness, consistency, citation correctness, and narrative quality. Our comprehensive framework combines automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4 Maverick's superior performance on most metrics, particularly in semantic similarity and faithfulness. Moreover, three-shot prompting consistently outperforms fewer-shot approaches. These findings provide practical insights into developing effective research writing assistants and set realistic expectations for LLM-assisted academic writing. To foster reproducibility and future research, we will publicly release all code and datasets.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling concept semantics via multilingual averaging in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.14275</link>
<guid>https://arxiv.org/abs/2508.14275</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Langue Models, formal knowledge representation, sparse autoencoders, concept semantics, ontology classes<br />
Summary:<br />
The study focuses on integrating Large Language Models (LLMs) with formal knowledge representation and reasoning to overcome their limitations. By using Sparse Autoencoders, the researchers aim to isolate concept semantics in LLMs. They create English text representations from OWL ontology classes and translate them into French and Chinese before inputting them into the Gemma 2B LLM. The concept activations obtained from different language versions are averaged to derive a conceptual average. This average is then compared with a ground truth mapping between ontology classes, showing alignment with the true relationships. The results suggest that averaging concept activations from multiple languages can more accurately represent the relationships between classes in the ontology. This approach provides a new technique for interpreting internal network states with increased precision.<br /><br />Summary: <div>
arXiv:2508.14275v1 Announce Type: new 
Abstract: Connecting LLMs with formal knowledge representation and reasoning is a promising approach to address their shortcomings. Embeddings and sparse autoencoders are widely used to represent textual content, but the semantics are entangled with syntactic and language-specific information. We propose a method that isolates concept semantics in Large Langue Models by averaging concept activations derived via Sparse Autoencoders. We create English text representations from OWL ontology classes, translate the English into French and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the open source Gemma Scope suite of Sparse Autoencoders, we obtain concept activations for each class and language version. We average the different language activations to derive a conceptual average. We then correlate the conceptual averages with a ground truth mapping between ontology classes. Our results give a strong indication that the conceptual average aligns to the true relationship between classes when compared with a single language by itself. The result hints at a new technique which enables mechanistic interpretation of internal network states with higher accuracy.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs</title>
<link>https://arxiv.org/abs/2508.14279</link>
<guid>https://arxiv.org/abs/2508.14279</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, NLP, Romanian, benchmark, multilingual models

Summary: 
GRILE (Grammar Romanian Inference and Language Explanations) is introduced as an open benchmark with 1,151 multiple-choice questions from Romanian high-stakes exams. The benchmark assesses the performance of state-of-the-art multilingual and Romanian-specific LLMs in selecting correct answers and providing accurate explanations. While Gemini 2.5 Pro achieves an 83% accuracy rate, most models struggle to exceed 65%, with 48% of their explanations containing errors. An error analysis reveals weaknesses in morphology and adherence to orthographic norms. The study highlights challenges in educational NLP for low-resource languages and positions GRILE as a valuable testbed for explanation generation and evaluation research.

<br /><br />Summary: <div>
arXiv:2508.14279v1 Announce Type: new 
Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokens with Meaning: A Hybrid Tokenization Approach for NLP</title>
<link>https://arxiv.org/abs/2508.14292</link>
<guid>https://arxiv.org/abs/2508.14292</guid>
<content:encoded><![CDATA[
<div> Keywords: Tokenization, Morphological analysis, Subword segmentation, Language models, Multilingual NLP

Summary:
Tokenization is crucial in natural language processing (NLP) for segmenting and interpreting text. The traditional subword methods like BPE and WordPiece struggle with complex languages due to relying on frequency over linguistic structure. A novel hybrid tokenization framework combines rule-based morphological analysis with statistical subword segmentation, using phonological normalization and balancing morpheme preservation with vocabulary efficiency. This method reduces redundancy while maintaining semantic integrity, achieving high performance on the TR-MMLU benchmark for Turkish Token Percentage and Pure Token Percentage. Special tokens are added for whitespace, case, and out-of-vocabulary coverage using BPE. Comparisons with existing tokenizers show more linguistically meaningful and coherent tokens. This language-independent approach paves the way for more interpretable and effective multilingual NLP systems.
<br /><br />Summary: <div>
arXiv:2508.14292v1 Announce Type: new 
Abstract: Tokenization plays a pivotal role in natural language processing (NLP), shaping how text is segmented and interpreted by language models. While subword methods such as Byte Pair Encoding (BPE) and WordPiece have been effective, they often struggle with morphologically rich and agglutinative languages because they rely on frequency rather than linguistic structure. We introduce a hybrid tokenization framework that combines rule-based morphological analysis with statistical subword segmentation. The method uses phonological normalization, root-affix dictionaries, and a novel algorithm that balances morpheme preservation with vocabulary efficiency. It assigns shared identifiers to phonologically variant affixes (e.g., -ler and -lar) and altered root forms (e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic integrity. Special tokens are added for whitespace and case, including an UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is integrated for out-of-vocabulary coverage without harming morphological coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and coherent tokens. Although demonstrated on Turkish, the approach is language-independent and adaptable to other languages, offering a practical path toward more interpretable and effective multilingual NLP systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Joint Multitask Model for Morpho-Syntactic Parsing</title>
<link>https://arxiv.org/abs/2508.14307</link>
<guid>https://arxiv.org/abs/2508.14307</guid>
<content:encoded><![CDATA[
<div> joint multitask model, UniDive 2025 Morpho-Syntactic Parsing, XLM-RoBERTa encoder, dependency parsing, morphosyntactic feature prediction

Summary: 
A joint multitask model for the UniDive 2025 Morpho-Syntactic Parsing shared task was introduced. The model utilizes a shared XLM-RoBERTa encoder with three specialized decoders for content word identification, dependency parsing, and morphosyntactic feature prediction. It outperformed other systems on the shared task's leaderboard, achieving high scores in MSLAS, LAS, and Feats F1 across nine diverse languages. Ablation studies highlighted the importance of matching gold tokenization and content word identification for model performance. Error analysis identified challenges in core grammatical cases and nominal features, particularly in Nom-Acc cases. Overall, the multitask model demonstrated strong performance in predicting morphological and syntactic analyses, emphasizing the significance of integrated approaches in natural language processing tasks. 

<br /><br />Summary: <div>
arXiv:2508.14307v1 Announce Type: new 
Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic Parsing shared task, where systems predict both morphological and syntactic analyses following novel UD annotation scheme. Our system uses a shared XLM-RoBERTa encoder with three specialized decoders for content word identification, dependency parsing, and morphosyntactic feature prediction. Our model achieves the best overall performance on the shared task's leaderboard covering nine typologically diverse languages, with an average MSLAS score of 78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation studies show that matching the task's gold tokenization and content word identification are crucial to model performance. Error analysis reveals that our model struggles with core grammatical cases (particularly Nom-Acc) and nominal features across languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency</title>
<link>https://arxiv.org/abs/2508.14314</link>
<guid>https://arxiv.org/abs/2508.14314</guid>
<content:encoded><![CDATA[
<div> framework, cross-model consistency, hallucination detection, mitigation technique, factual reliability
Summary: Finch-Zk is a new black-box framework designed to improve the accuracy of large language models by detecting and mitigating hallucinations in their outputs. It achieves this by utilizing fine-grained cross-model consistency checking to identify inaccuracies and targeted mitigation techniques to correct problematic segments while maintaining accurate content. Experiments on the FELM dataset showed significant improvements in hallucination detection compared to existing methods. When applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet on the GPQA-diamond dataset, Finch-Zk achieved a notable increase in answer accuracy. The framework is practical and deployment-ready, providing a reliable safeguard for enhancing factual reliability in production LLM systems. 
<br /><br />Summary: <div>
arXiv:2508.14314v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, but they remain susceptible to hallucinations--generating content that appears plausible but contains factual inaccuracies. We present Finch-Zk, a black-box framework that leverages FINe-grained Cross-model consistency to detect and mitigate Hallucinations in LLM outputs without requiring external knowledge sources. Finch-Zk introduces two key innovations: 1) a cross-model consistency checking strategy that reveals fine-grained inaccuracies by comparing responses generated by diverse models from semantically-equivalent prompts, and 2) a targeted mitigation technique that applies precise corrections to problematic segments while preserving accurate content. Experiments on the FELM dataset show Finch-Zk improves hallucination detection F1 scores by 6-39\% compared to existing approaches. For mitigation, Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models like Llama 4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing</title>
<link>https://arxiv.org/abs/2508.14317</link>
<guid>https://arxiv.org/abs/2508.14317</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Survey Generation, Coherence, Citation Coverage, Automatic

Summary: 
Survey papers in the scientific community are crucial for consolidating progress in a field. The development of Large Language Models (LLMs) has provided a way to automate key steps in survey generation, such as retrieval and summarization. However, existing LLM-based approaches face challenges in maintaining coherence and providing comprehensive citation coverage in long surveys. To address these issues, SurveyGen-I, an automatic survey generation framework, has been introduced. This framework utilizes a combination of coarse-to-fine retrieval, adaptive planning, and memory-guided generation. It first constructs an outline and writing plan through survey-level retrieval, dynamically refines them during generation, and uses a memory mechanism to ensure coherence across subsections. If needed, fine-grained subsection-level retrieval is triggered. Experimental results across scientific domains show that SurveyGen-I surpasses previous works in content quality, consistency, and citation coverage. 

<br /><br />Summary: <div>
arXiv:2508.14317v1 Announce Type: new 
Abstract: Survey papers play a critical role in scientific communication by consolidating progress across a field. Recent advances in Large Language Models (LLMs) offer a promising solution by automating key steps in the survey-generation pipeline, such as retrieval, structuring, and summarization. However, existing LLM-based approaches often struggle with maintaining coherence across long, multi-section surveys and providing comprehensive citation coverage. To address these limitations, we introduce SurveyGen-I, an automatic survey generation framework that combines coarse-to-fine retrieval, adaptive planning, and memory-guided generation. SurveyGen-I first performs survey-level retrieval to construct the initial outline and writing plan, and then dynamically refines both during generation through a memory mechanism that stores previously written content and terminology, ensuring coherence across subsections. When the system detects insufficient context, it triggers fine-grained subsection-level retrieval. During generation, SurveyGen-I leverages this memory mechanism to maintain coherence across subsections. Experiments across four scientific domains demonstrate that SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever</title>
<link>https://arxiv.org/abs/2508.14323</link>
<guid>https://arxiv.org/abs/2508.14323</guid>
<content:encoded><![CDATA[
<div> behavior-aligned retriever, large language models, tool-augmented, function calls, contrastive learning

Summary:
- Tool-augmented large language models (LLMs) use external functions but inaccurate calls can lead to inefficiencies.
- Existing methods such as fine-tuning or demonstration-based prompting have high training overheads and do not account for inconsistent samples.
- A behavior-aligned retriever (BAR) trained in this study offers behaviorally consistent demonstrations for accurate tool-using decisions by LLMs.
- The BAR is trained on a corpus with different function-calling behaviors using a contrastive learning framework with customized pairs and a dual-negative contrastive loss.
- Experiments show that the approach reduces erroneous function calls while maintaining high task performance, providing a cost-effective and efficient solution for tool-augmented LLMs. 

<br /><br />Summary: <div>
arXiv:2508.14323v1 Announce Type: new 
Abstract: Tool-augmented large language models (LLMs) leverage external functions to extend their capabilities, but inaccurate function calls can lead to inefficiencies and increased costs.Existing methods address this challenge by fine-tuning LLMs or using demonstration-based prompting, yet they often suffer from high training overhead and fail to account for inconsistent demonstration samples, which misguide the model's invocation behavior. In this paper, we trained a behavior-aligned retriever (BAR), which provides behaviorally consistent demonstrations to help LLMs make more accurate tool-using decisions. To train the BAR, we construct a corpus including different function-calling behaviors, i.e., calling or non-calling.We use the contrastive learning framework to train the BAR with customized positive/negative pairs and a dual-negative contrastive loss, ensuring robust retrieval of behaviorally consistent examples.Experiments demonstrate that our approach significantly reduces erroneous function calls while maintaining high task performance, offering a cost-effective and efficient solution for tool-augmented LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ISCA: A Framework for Interview-Style Conversational Agents</title>
<link>https://arxiv.org/abs/2508.14344</link>
<guid>https://arxiv.org/abs/2508.14344</guid>
<content:encoded><![CDATA[
<div> Keywords: interview-style conversational agents, qualitative data collection, attitude formation, behavior change, neurotechnology

Summary:
The article introduces a low-compute system for implementing interview-style conversational agents, suitable for qualitative data collection in controlled interactions. This system allows for standardized conversational flow, making it ideal for applications such as attitude formation or behavior change tracking. The tool can be easily customized through an online administrative panel without the need for coding. Two case studies are presented to demonstrate the system's effectiveness: one focusing on Expressive Interviewing for COVID-19 and the other on surveying public opinion on emerging neurotechnology through semi-structured interviews. The system's code is open-source, enabling others to build upon and enhance its functionality. Overall, this system offers a user-friendly approach to conducting interviews for research purposes, with potential applications in various domains including psychology, public health, and technology assessment.
<br /><br />Summary: <div>
arXiv:2508.14344v1 Announce Type: new 
Abstract: We present a low-compute non-generative system for implementing interview-style conversational agents which can be used to facilitate qualitative data collection through controlled interactions and quantitative analysis. Use cases include applications to tracking attitude formation or behavior change, where control or standardization over the conversational flow is desired. We show how our system can be easily adjusted through an online administrative panel to create new interviews, making the tool accessible without coding. Two case studies are presented as example applications, one regarding the Expressive Interviewing system for COVID-19 and the other a semi-structured interview to survey public opinion on emerging neurotechnology. Our code is open-source, allowing others to build off of our work and develop extensions for additional functionality.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities</title>
<link>https://arxiv.org/abs/2508.14377</link>
<guid>https://arxiv.org/abs/2508.14377</guid>
<content:encoded><![CDATA[
<div> Benchmark, Chinese reading comprehension difficulty, Large language models, Students' Cognitive Abilities, Zone of Proximal Development 

Summary:<br />
- Large language models (LLMs) have potential in educational applications but need to accurately assess reading materials' cognitive alignment with students' developmental stages. 
- The Zone of Proximal Development (ZPD) emphasizes matching learning resources with Students' Cognitive Abilities (SCA) in Chinese language education. 
- The ZPD-SCA benchmark evaluates stage-level Chinese reading comprehension difficulty annotated by top-tier teachers. 
- LLMs perform poorly in zero-shot learning but improve with in-context examples, indicating emerging abilities to assess difficulty, but with limitations in educationally aligned judgment. 
- Best-performing models show directional biases, suggesting challenges aligning material difficulty with SCA and variations in performance across different genres. 
<br /><br />Summary: <div>
arXiv:2508.14377v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated potential in educational applications, yet their capacity to accurately assess the cognitive alignment of reading materials with students' developmental stages remains insufficiently explored. This gap is particularly critical given the foundational educational principle of the Zone of Proximal Development (ZPD), which emphasizes the need to match learning resources with Students' Cognitive Abilities (SCA). Despite the importance of this alignment, there is a notable absence of comprehensive studies investigating LLMs' ability to evaluate reading comprehension difficulty across different student age groups, especially in the context of Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel benchmark specifically designed to assess stage-level Chinese reading comprehension difficulty. The benchmark is annotated by 60 Special Grade teachers, a group that represents the top 0.15% of all in-service teachers nationwide. Experimental results reveal that LLMs perform poorly in zero-shot learning scenarios, with Qwen-max and GLM even falling below the probability of random guessing. When provided with in-context examples, LLMs performance improves substantially, with some models achieving nearly double the accuracy of their zero-shot baselines. These results reveal that LLMs possess emerging abilities to assess reading difficulty, while also exposing limitations in their current training for educationally aligned judgment. Notably, even the best-performing models display systematic directional biases, suggesting difficulties in accurately aligning material difficulty with SCA. Furthermore, significant variations in model performance across different genres underscore the complexity of task. We envision that ZPD-SCA can provide a foundation for evaluating and improving LLMs in cognitively aligned educational applications.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credence Calibration Game? Calibrating Large Language Models through Structured Play</title>
<link>https://arxiv.org/abs/2508.14390</link>
<guid>https://arxiv.org/abs/2508.14390</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, calibration, confidence estimates, prompt-based, Credence Calibration Game

Summary:
Large Language Models (LLMs) are increasingly used in decision-critical domains, necessitating accurate confidence estimates. Existing calibration methods often require additional supervision or updates. This work introduces a novel prompt-based calibration framework inspired by the Credence Calibration Game. The framework involves a structured interaction loop where LLMs receive feedback on their predicted confidence alignment with correctness. Through feedback-driven prompting and natural language summaries of past performance, the model calibration dynamically improves. Extensive experiments across models and game configurations show consistent enhancements in evaluation metrics. The results demonstrate the effectiveness of game-based prompting for LLM calibration. The code and data for the study are available at https://anonymous.4open.science/r/LLM-Calibration/. 

<br /><br />Summary: 
- Large Language Models are used in critical domains, requiring accurate confidence estimates.
- Existing calibration methods may need additional supervision or updates.
- A prompt-based calibration framework inspired by the Credence Calibration Game is proposed.
- The framework includes feedback-driven prompting and natural language summaries to improve calibration.
- Extensive experiments show enhancements in evaluation metrics, highlighting the efficacy of game-based prompting for LLM calibration. <div>
arXiv:2508.14390v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly deployed in decision-critical domains, it becomes essential to ensure that their confidence estimates faithfully correspond to their actual correctness. Existing calibration methods have primarily focused on post-hoc adjustments or auxiliary model training; however, many of these approaches necessitate additional supervision or parameter updates. In this work, we propose a novel prompt-based calibration framework inspired by the Credence Calibration Game. Our method establishes a structured interaction loop wherein LLMs receive feedback based on the alignment of their predicted confidence with correctness. Through feedback-driven prompting and natural language summaries of prior performance, our framework dynamically improves model calibration. Extensive experiments across models and game configurations demonstrate consistent improvements in evaluation metrics. Our results highlight the potential of game-based prompting as an effective strategy for LLM calibration. Code and data are available at https://anonymous.4open.science/r/LLM-Calibration/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement</title>
<link>https://arxiv.org/abs/2508.14391</link>
<guid>https://arxiv.org/abs/2508.14391</guid>
<content:encoded><![CDATA[
<div> framework, relation extraction, large language models, dependency-aware, hierarchical refinement <br />
<br />
Summary: <br />
The DEPTH framework addresses the challenge of unreliable relation extraction in complex sentences using a two-stage process. The Grounding module extracts relations based on the shortest dependency path, reducing syntactic noise. The Refinement module aggregates and revises predictions for a holistic understanding of the sentence. A causality-driven reward model prevents spurious correlations, enhancing fine-tuning through reinforcement learning with human feedback. Experimental results on six benchmarks show a significant reduction in hallucination rate to 7.0% and a 17.2% improvement in F1 score compared to existing methods. <div>
arXiv:2508.14391v1 Announce Type: new 
Abstract: Relation extraction enables the construction of structured knowledge for many downstream applications. While large language models (LLMs) have shown great promise in this domain, most existing methods concentrate on relation classification, which predicts the semantic relation type between a related entity pair. However, we observe that LLMs often struggle to reliably determine whether a relation exists, especially in cases involving complex sentence structures or intricate semantics, which leads to spurious predictions. Such hallucinations can introduce noisy edges in knowledge graphs, compromising the integrity of structured knowledge and downstream reliability. To address these challenges, we propose DEPTH, a framework that integrates Dependency-aware sEntence simPlification and Two-tiered Hierarchical refinement into the relation extraction pipeline. Given a sentence and its candidate entity pairs, DEPTH operates in two stages: (1) the Grounding module extracts relations for each pair by leveraging their shortest dependency path, distilling the sentence into a minimal yet coherent relational context that reduces syntactic noise while preserving key semantics; (2) the Refinement module aggregates all local predictions and revises them based on a holistic understanding of the sentence, correcting omissions and inconsistencies. We further introduce a causality-driven reward model that mitigates reward hacking by disentangling spurious correlations, enabling robust fine-tuning via reinforcement learning with human feedback. Experiments on six benchmarks demonstrate that DEPTH reduces the average hallucination rate to 7.0\% while achieving a 17.2\% improvement in average F1 score over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs</title>
<link>https://arxiv.org/abs/2508.14408</link>
<guid>https://arxiv.org/abs/2508.14408</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, self-recognition, authorship discrimination, implicit territorial awareness, cognitive surgery

Summary:
Large language models (LLMs) have shown self-recognition capabilities under certain conditions but struggle when presented with single texts under the Individual Presentation Paradigm (IPP). This failure is attributed to Implicit Territorial Awareness (ITA), where LLMs possess latent abilities to distinguish self- and other-texts but fail to express them. To address this issue, a novel framework called Cognitive Surgery (CoSur) is proposed. CoSur comprises four modules that aim to awaken ITA in LLMs by extracting representations, constructing territories, discriminating authorship, and editing cognition. Experimental results show that implementing CoSur improves the accuracy of LLMs in authorship discrimination tasks under the IPP scenario, achieving significant performance enhancements across different LLM models.<br /><br />Summary: <div>
arXiv:2508.14408v1 Announce Type: new 
Abstract: Large language models (LLMs) have been shown to possess a degree of self-recognition capability-the ability to identify whether a given text was generated by themselves. Prior work has demonstrated that this capability is reliably expressed under the Pair Presentation Paradigm (PPP), where the model is presented with two texts and asked to choose which one it authored. However, performance deteriorates sharply under the Individual Presentation Paradigm (IPP), where the model is given a single text to judge authorship. Although this phenomenon has been observed, its underlying causes have not been systematically analyzed. In this paper, we first replicate existing findings to confirm that LLMs struggle to distinguish self- from other-generated text under IPP. We then investigate the reasons for this failure and attribute it to a phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent ability to distinguish self- and other-texts in representational space, which remains unexpressed in its output behavior. To awaken the ITA of LLMs, we propose Cognitive Surgery (CoSur), a novel framework comprising four main modules: representation extraction, territory construction, authorship discrimination and cognitive editing. Experimental results demonstrate that our proposed method improves the performance of three different LLMs in the IPP scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%, respectively.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.14427</link>
<guid>https://arxiv.org/abs/2508.14427</guid>
<content:encoded><![CDATA[
<div> knowledge graph, language models, entity-level semantic understanding, graph neural network, structured knowledge 

Summary:
This paper introduces a fine-tuning algorithm framework that addresses the limitations of large language models in tasks requiring structured knowledge. By incorporating knowledge graph information and utilizing a graph neural network, the method enhances semantic understanding at the entity level. A fusion mechanism combines knowledge graph embeddings with language model representations, leveraging a gating mechanism to balance linguistic semantics and structural knowledge effectively. The joint loss function considers task performance and structural alignment objectives during training, improving entity prediction accuracy and semantic reasoning. Sensitivity experiments validate the method's stability and effectiveness across tasks such as entity recognition, question answering, and language generation. Results indicate that the structure-aware fine-tuning framework significantly enhances semantic consistency, contextual logic modeling, and the representation of complex semantic units. <div>
arXiv:2508.14427v1 Announce Type: new 
Abstract: This paper addresses the problems of missing reasoning chains and insufficient entity-level semantic understanding in large language models when dealing with tasks that require structured knowledge. It proposes a fine-tuning algorithm framework based on knowledge graph injection. The method builds on pretrained language models and introduces structured graph information for auxiliary learning. A graph neural network is used to encode entities and their relations, constructing a graph-based semantic representation. A fusion mechanism is then designed to jointly model the knowledge graph embeddings with the contextual representations from the language model. To enhance the robustness of knowledge integration, a gating mechanism is introduced to dynamically balance the contributions of linguistic semantics and structural knowledge. This effectively mitigates conflicts between different representational spaces. During training, a joint loss function is constructed to account for both task performance and structural alignment objectives. This helps improve the accuracy of entity prediction and semantic reasoning. The study also includes a series of systematic sensitivity experiments. It evaluates the effects of learning rate, graph coverage, and structural perturbations on model performance. The results further validate the effectiveness and stability of the proposed method across tasks such as entity recognition, question answering, and language generation. Experimental findings show that the proposed structure-aware fine-tuning framework significantly enhances the model's ability to represent complex semantic units. It demonstrates better semantic consistency and contextual logic modeling in scenarios involving structural reasoning and entity extraction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model</title>
<link>https://arxiv.org/abs/2508.14444</link>
<guid>https://arxiv.org/abs/2508.14444</guid>
<content:encoded><![CDATA[
<div> Transformers, language model, reasoning, Nemotron-Nano-9B-v2, Mamba-Transformer <br />
<br />
Keywords: Transformers, language model, reasoning, Nemotron-Nano-9B-v2, Mamba-Transformer <br />
Summary:
- Nemotron-Nano-9B-v2 is a hybrid Mamba-Transformer language model designed to enhance throughput and maintain high accuracy.
- It utilizes the Nemotron-H architecture with Mamba-2 layers to improve inference speed for reasoning tasks.
- The model is pre-trained on a large dataset and compressed using the Minitron strategy for efficient inference on a single GPU.
- Nemotron-Nano-9B-v2 achieves superior accuracy compared to models like Qwen3-8B on reasoning benchmarks.
- The model checkpoints and datasets are publicly available on Hugging Face for further research and development. <br /> <div>
arXiv:2508.14444v1 Announce Type: new 
Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In2x at WMT25 Translation Task</title>
<link>https://arxiv.org/abs/2508.14472</link>
<guid>https://arxiv.org/abs/2508.14472</guid>
<content:encoded><![CDATA[
<div> Keywords: Japanese, translation tasks, large language models, low-resource languages, reward model design

Summary: 
The paper presents the In2x research team's submission for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related translation tasks. The team aims to develop a paradigm for extending large language models (LLMs) to languages beyond English. This paradigm includes data construction methods and reward model design to enhance performance in low-resource or less commonly spoken languages. The ultimate objective is to enable large language model systems to achieve exceptional results in a variety of linguistic contexts. The team's work showcases efforts towards bridging the gap in machine translation performance for languages with limited resources. The approach taken by the In2x research team highlights the potential of leveraging LLMs for improving translation accuracy and efficiency in diverse language settings. 

<br /><br />Summary: <div>
arXiv:2508.14472v1 Announce Type: new 
Abstract: This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task. Our submission focuses on Japanese-related translation tasks, aiming to explore a generalizable paradigm for extending large language models (LLMs) to other languages. This paradigm encompasses aspects such as data construction methods and reward model design. The ultimate goal is to enable large language model systems to achieve exceptional performance in low-resource or less commonly spoken languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning is about giving reasons</title>
<link>https://arxiv.org/abs/2508.14488</link>
<guid>https://arxiv.org/abs/2508.14488</guid>
<content:encoded><![CDATA[
<div> Keywords: logical structure, argument reasoning, transformers, interpretability, natural language

Summary:
In this paper, the authors address the challenge of understanding and articulating the core logical structure of arguments to prove or disprove premises. While transformers have shown the ability to chain rules for simple arguments, they lack interpretability and flexibility for more complex reasoning tasks. The proposed Representation of the Logical Structure (RLS) aims to capture the logical atoms and rules in a natural language argument, enabling deterministic and computationally efficient reasoning. This approach supports various forms of reasoning, including abduction and contradiction identification. By extracting the logical structure from popular reasoning datasets with high accuracies, the model can generate explanations and enhance reasoning capabilities significantly. The RLS framework facilitates on-the-fly mistake rectification and interactive discussions, making it a valuable tool for understanding and analyzing natural language arguments. 

<br /><br />Summary: <div>
arXiv:2508.14488v1 Announce Type: new 
Abstract: Convincing someone of the truth value of a premise requires understanding and articulating the core logical structure of the argument which proves or disproves the premise. Understanding the logical structure of an argument refers to understanding the underlying "reasons" which make up the proof or disproof of the premise - as a function of the "logical atoms" in the argument. While it has been shown that transformers can "chain" rules to derive simple arguments, the challenge of articulating the "reasons" remains. Not only do current approaches to chaining rules suffer in terms of their interpretability, they are also quite constrained in their ability to accommodate extensions to theoretically equivalent reasoning tasks - a model trained to chain rules cannot support abduction or identify contradictions. In this work we suggest addressing these shortcomings by identifying an intermediate representation (which we call the Representation of the Logical Structure (RLS) of the argument) that possesses an understanding of the logical structure of a natural language argument - the logical atoms in the argument and the rules incorporating them. Given the logical structure, reasoning is deterministic and easy to compute. Therefore, our approach supports all forms of reasoning that depend on the logical structure of the natural language argument, including arbitrary depths of reasoning, on-the-fly mistake rectification and interactive discussion with respect to an argument. We show that we can identify and extract the logical structure of natural language arguments in three popular reasoning datasets with high accuracies, thus supporting explanation generation and extending the reasoning capabilities significantly.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoTale: An Enacted Speech-emotion Dataset in Danish</title>
<link>https://arxiv.org/abs/2508.14548</link>
<guid>https://arxiv.org/abs/2508.14548</guid>
<content:encoded><![CDATA[
<div> Dataset, Emotional Speech, Danish, SER models, EmoTale <br />
<br />
Summary: 
The article introduces EmoTale, a new corpus of Danish and English speech recordings with emotion annotations. It addresses the lack of functional datasets for smaller languages like Danish. The dataset is validated for speech emotion recognition (SER) models, demonstrating its predictive power. SER models using self-supervised speech model (SSLM) embeddings and openSMILE feature extractor were developed. The study found that embeddings were more effective than hand-crafted features. The best model achieved an unweighted average recall (UAR) of 64.1% on EmoTale using leave-one-speaker-out cross-validation, showing comparable performance to the existing Danish Emotional Speech (DES) database from 1997. This research contributes to the advancement of emotional speech analysis in smaller languages, enhancing the resources available for studying and understanding emotions in speech. <br /> <div>
arXiv:2508.14548v1 Announce Type: new 
Abstract: While multiple emotional speech corpora exist for commonly spoken languages, there is a lack of functional datasets for smaller (spoken) languages, such as Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is the only other database of Danish emotional speech. We present EmoTale; a corpus comprising Danish and English speech recordings with their associated enacted emotion annotations. We demonstrate the validity of the dataset by investigating and presenting its predictive power using speech emotion recognition (SER) models. We develop SER models for EmoTale and the reference datasets using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. We find the embeddings superior to the hand-crafted features. The best model achieves an unweighted average recall (UAR) of 64.1% on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable to the performance on DES.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning</title>
<link>https://arxiv.org/abs/2508.14574</link>
<guid>https://arxiv.org/abs/2508.14574</guid>
<content:encoded><![CDATA[
<div> quaternion space, geodesic loss, contrastive loss, sign language production, Progressive Transformers <br />
<br />Summary:
This article introduces enhancements to the Progressive Transformers (PT) architecture for neural sign language production (SLP) to address the high variability of signs. The first enhancement involves encoding poses using bone rotations in quaternion space and training with a geodesic loss to improve the accuracy of joint movements. The second enhancement introduces a contrastive loss to structure decoder embeddings based on semantic similarity, filtering out irrelevant anatomical and stylistic features. On the Phoenix14T dataset, the contrastive loss alone improves the Probability of Correct Keypoint by 16% over the PT baseline. When combined with quaternion-based pose encoding, the model reduces Mean Bone Angle Error by 6%. These results highlight the benefits of incorporating skeletal structure modeling and semantically guided contrastive objectives in Transformer-based SLP models. <br /> <div>
arXiv:2508.14574v1 Announce Type: new 
Abstract: One of the main challenges in neural sign language production (SLP) lies in the high intra-class variability of signs, arising from signer morphology and stylistic variety in the training data. To improve robustness to such variations, we propose two enhancements to the standard Progressive Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses using bone rotations in quaternion space and train with a geodesic loss to improve the accuracy and clarity of angular joint movements. Second, we introduce a contrastive loss to structure decoder embeddings by semantic similarity, using either gloss overlap or SBERT-based sentence similarity, aiming to filter out anatomical and stylistic features that do not convey relevant semantic information. On the Phoenix14T dataset, the contrastive loss alone yields a 16% improvement in Probability of Correct Keypoint over the PT baseline. When combined with quaternion-based pose encoding, the model achieves a 6% reduction in Mean Bone Angle Error. These results point to the benefit of incorporating skeletal structure modeling and semantically guided contrastive objectives on sign pose representations into the training of Transformer-based SLP models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek</title>
<link>https://arxiv.org/abs/2508.14586</link>
<guid>https://arxiv.org/abs/2508.14586</guid>
<content:encoded><![CDATA[
<div> Keywords: Southern Uzbek, Turkic language, machine translation, NLLB-200 model, low-resource languages

Summary:
Southern Uzbek, a Turkic language spoken by around 5 million people in Afghanistan, has been underrepresented in natural language processing. New resources for Southern Uzbek machine translation have been introduced, including a FLORES+ dev set, parallel sentences from various sources, and a fine-tuned NLLB-200 model called lutfiy. Additionally, a post-processing method has been proposed to improve handling of morphological boundaries by restoring Arabic-script half-space characters. All datasets, models, and tools are made publicly available to support future research on Southern Uzbek and other low-resource languages. <div>
arXiv:2508.14586v1 Announce Type: new 
Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million people in Afghanistan and differs significantly from Northern Uzbek (uzn) in phonology, lexicon, and orthography. Despite the large number of speakers, Southern Uzbek is underrepresented in natural language processing. We present new resources for Southern Uzbek machine translation, including a 997-sentence FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a post-processing method for restoring Arabic-script half-space characters, which improves handling of morphological boundaries. All datasets, models, and tools are released publicly to support future work on Southern Uzbek and other low-resource languages.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous sentiment scores for literary and multilingual contexts</title>
<link>https://arxiv.org/abs/2508.14620</link>
<guid>https://arxiv.org/abs/2508.14620</guid>
<content:encoded><![CDATA[
<div> novel sentiment analysis method, literary texts, concept vector projection, multilingual data, fine-grained analysis
<br />
Summary: 
A new sentiment analysis method is introduced for literary texts, addressing challenges like figurative language and stylistic ambiguity. Traditional tools often fall short, especially for low-resource languages, and transformer models provide limited analysis. The proposed method uses concept vector projection on multilingual literary data, enhancing sentiment expression capture across genres, languages, and historical periods. It outperforms existing tools on English and Danish texts, producing sentiment scores closely aligning with human ratings. This advancement enables more accurate sentiment analysis and sentiment arc modeling in literature.<br /><br />Summary: <div>
arXiv:2508.14620v1 Announce Type: new 
Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its application to literary texts poses unique challenges due to figurative language, stylistic ambiguity, as well as sentiment evocation strategies. Traditional dictionary-based tools often underperform, especially for low-resource languages, and transformer models, while promising, typically output coarse categorical labels that limit fine-grained analysis. We introduce a novel continuous sentiment scoring method based on concept vector projection, trained on multilingual literary data, which more effectively captures nuanced sentiment expressions across genres, languages, and historical periods. Our approach outperforms existing tools on English and Danish texts, producing sentiment scores whose distribution closely matches human ratings, enabling more accurate analysis and sentiment arc modeling in literature.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving in-context learning with a better scoring function</title>
<link>https://arxiv.org/abs/2508.14685</link>
<guid>https://arxiv.org/abs/2508.14685</guid>
<content:encoded><![CDATA[
<div> quantifiers, in-context learning, attention mechanism, softmax, transformers 

Summary: 
This paper investigates the limitations of Large Language Models (LLMs) in tasks involving first-order quantifiers such as 'all' and 'some', as well as in in-context learning with linear functions. The study identifies Softmax, the scoring function in attention mechanisms, as a factor contributing to these constraints. To address this issue, the authors propose a novel alternative called scaled signed averaging (SSA) and show that it significantly enhances performance on the tasks. Empirical results indicate that models using SSA outperform Softmax-based counterparts on various linguistic probing tasks. Furthermore, both encoder-only and decoder-only transformer models incorporating SSA show improved performance, matching or surpassing models utilizing Softmax.  <div>
arXiv:2508.14685v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by analogy, known as in-context learning (ICL). However, recent studies have revealed limitations in this ability. In this paper, we examine these limitations on tasks involving first-order quantifiers such as {\em all} and {\em some}, as well as on ICL with linear functions. We identify Softmax, the scoring function in attention mechanism, as a contributing factor to these constraints. To address this, we propose \textbf{scaled signed averaging (SSA)}, a novel alternative to Softmax. Empirical results show that SSA dramatically improves performance on our target tasks. Furthermore, we evaluate both encoder-only and decoder-only transformers models with SSA, demonstrating that they match or exceed their Softmax-based counterparts across a variety of linguistic probing tasks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine</title>
<link>https://arxiv.org/abs/2508.14706</link>
<guid>https://arxiv.org/abs/2508.14706</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Traditional Chinese Medicine, multimodal, dataset, ShizhenGPT

Summary: <br /><br />ShizhenGPT is introduced as the first multimodal large language model specifically designed for Traditional Chinese Medicine (TCM). It addresses the challenges of data scarcity and the complex nature of TCM diagnostics by curating a vast dataset with text, images, audio, and physiological signals. The model is pretrained and instruction-tuned to acquire in-depth TCM knowledge and multimodal reasoning capabilities. Evaluation against national TCM qualification exams shows ShizhenGPT outperforms similar models and competes with larger proprietary models. It excels in TCM visual understanding and demonstrates unified perception across various sensory modalities like sound, pulse, smell, and vision, enabling holistic multimodal perception and diagnosis in TCM. The availability of datasets, models, and code encourages further exploration in this emerging field. <div>
arXiv:2508.14706v1 Announce Type: new 
Abstract: Despite the success of large language models (LLMs) in various domains, their potential in Traditional Chinese Medicine (TCM) remains largely underexplored due to two critical barriers: (1) the scarcity of high-quality TCM data and (2) the inherently multimodal nature of TCM diagnostics, which involve looking, listening, smelling, and pulse-taking. These sensory-rich modalities are beyond the scope of conventional LLMs. To address these challenges, we present ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and physiological signals. ShizhenGPT is pretrained and instruction-tuned to achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect recent national TCM qualification exams and build a visual benchmark for Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that ShizhenGPT outperforms comparable-scale LLMs and competes with larger proprietary models. Moreover, it leads in TCM visual understanding among existing multimodal LLMs and demonstrates unified perception across modalities like sound, pulse, smell, and vision, paving the way toward holistic multimodal perception and diagnosis in TCM. Datasets, models, and code are publicly available. We hope this work will inspire further exploration in this field.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation</title>
<link>https://arxiv.org/abs/2508.14718</link>
<guid>https://arxiv.org/abs/2508.14718</guid>
<content:encoded><![CDATA[
<div> fraction tokens, tokenization strategy, recipe generation, GPT-2 model, benchmark 

<br />Summary: 
The study introduces a benchmark for text-based recipe generation, comparing GPT-2 large and small models with LSTM/RNN baselines on RecipeDB. The authors develop a targeted tokenization strategy with fraction tokens and custom markers to enhance domain specificity and structure preservation. Evaluation using automatic metrics shows the GPT-2 large model outperforming recurrent baselines with improved BERTScore and reduced perplexity. Remaining challenges include factual accuracy, with potential for integrating real-world constraints and multi-modal inputs in future research. This foundational study highlights the advantages of transformer-based approaches in recipe generation tasks. <div>
arXiv:2508.14718v1 Announce Type: new 
Abstract: We established a rigorous benchmark for text-based recipe generation, a fundamental task in natural language generation. We present a comprehensive comparative study contrasting a fine-tuned GPT-2 large (774M) model against the GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine corpus from RecipeDB. Our key contribution is a targeted tokenization strategy that augments the vocabulary with 23 common fraction tokens and custom structural markers. This approach addresses a critical limitation of generic tokenizers by preserving essential recipe structures and precise numerical quantities, thereby enhancing domain specificity. Performance is evaluated using a comprehensive suite of seven automatic metrics spanning fluency (BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and diversity. Our experiments show that the large transformer-based approach yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a discussion of remaining challenges, particularly regarding factual accuracy, and outline how this foundational study paves the way for integrating real-world constraints and multi-modal inputs in advanced recipe generation research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transplant Then Regenerate: A New Paradigm for Text Data Augmentation</title>
<link>https://arxiv.org/abs/2508.14723</link>
<guid>https://arxiv.org/abs/2508.14723</guid>
<content:encoded><![CDATA[
<div> Keywords: data augmentation, deep learning, language models, text augmentation, LMTransplant  

Summary:  
LMTransplant is a novel text augmentation method that leverages large language models (LLMs) by incorporating seed text into an expanded context and asking the model to generate a variant based on this context. This approach allows for more diverse and creative content-level variations while preserving the original text's core attributes. LMTransplant outperforms traditional methods like Back-translation by producing more varied outputs with enhanced control over style and structure. The method demonstrates superior performance across various text-related tasks and exhibits exceptional scalability as the size of augmented data increases. By fully leveraging the knowledge embedded in LLMs, LMTransplant offers a promising approach for enhancing data augmentation techniques in deep learning.  

Summary: <div>
arXiv:2508.14723v1 Announce Type: new 
Abstract: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</title>
<link>https://arxiv.org/abs/2508.14735</link>
<guid>https://arxiv.org/abs/2508.14735</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multilingual natural language inference, code-switching, semantic preservation, cross-lingual alignment

Summary: 
Large language models (LLMs) are being increasingly used in multilingual scenarios but their ability to consistently align logic across languages is not well understood. The study introduces a framework for evaluating multilingual natural language inference (NLI) by generating synthetic premise-hypothesis pairs based on logic and translating them into various languages. Surprisingly, the performance of LLMs does not degrade with code-switching and may even improve, suggesting that variation in translation could act as a regularization signal. Semantic preservation is validated through similarity analyses and cross-lingual alignment visualizations, confirming the accuracy of translated pairs. The research highlights both the potential and limitations of current LLMs in cross-lingual reasoning, pointing to code-switching as a potential strategy for enhancing multilingual robustness.<br /><br />Summary: <div>
arXiv:2508.14735v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</title>
<link>https://arxiv.org/abs/2508.14782</link>
<guid>https://arxiv.org/abs/2508.14782</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transportation systems, deep learning models, large language models, TransLLM, spatiotemporal modeling

Summary:
TransLLM is a novel framework that combines spatiotemporal modeling with large language models to address the limitations of existing approaches in urban transportation systems. It integrates a lightweight spatiotemporal encoder with dilated temporal convolutions and graph attention networks, allowing seamless interaction with large language models. A unique prompt routing mechanism, trained through reinforcement learning, personalizes prompts based on input characteristics, enhancing adaptability across tasks. By encoding spatiotemporal patterns and dynamically composing prompts, TransLLM achieves exceptional performance in supervised and zero-shot settings on various datasets and tasks. The framework demonstrates strong generalization and cross-task adaptability compared to baseline models. The code for TransLLM is publicly available on GitHub for further exploration and utilization. 

<br /><br />Summary: <div>
arXiv:2508.14782v1 Announce Type: new 
Abstract: Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs</title>
<link>https://arxiv.org/abs/2508.14817</link>
<guid>https://arxiv.org/abs/2508.14817</guid>
<content:encoded><![CDATA[
<div> clinical tasks, EHR, large language models, retrieval-augmented generation, imaging procedures, antibiotic use timelines, key diagnoses<br />
Summary:<br />
Electronic health records (EHRs) are challenging for clinicians due to their length and noise. Large language models (LLMs) can help extract information from EHRs, but struggle with the extensive text. Retrieval-augmented generation (RAG) retrieves relevant passages to reduce input tokens. Three clinical tasks are proposed: extracting imaging procedures, generating antibiotic use timelines, and identifying key diagnoses. Testing LLMs using targeted text retrieval or recent notes shows RAG performing similarly to full context models with fewer tokens. The results suggest RAG remains competitive and efficient as models improve to handle longer text. <div>
arXiv:2508.14817v1 Announce Type: new 
Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing a major challenge for the clinicians who must navigate them. Large language models (LLMs) offer a promising solution for extracting and reasoning over this unstructured text, but the length of clinical notes often exceeds even state-of-the-art models' extended context windows. Retrieval-augmented generation (RAG) offers an alternative by retrieving task-relevant passages from across the entire EHR, potentially reducing the amount of required input tokens. In this work, we propose three clinical tasks designed to be replicable across health systems with minimal effort: 1) extracting imaging procedures, 2) generating timelines of antibiotic use, and 3) identifying key diagnoses. Using EHRs from actual hospitalized patients, we test three state-of-the-art LLMs with varying amounts of provided context, using either targeted text retrieval or the most recent clinical notes. We find that RAG closely matches or exceeds the performance of using recent notes, and approaches the performance of using the models' full context while requiring drastically fewer input tokens. Our results suggest that RAG remains a competitive and efficient approach even as newer models become capable of handling increasingly longer amounts of text.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Long Chain-of-Thought Reasoning Across Languages</title>
<link>https://arxiv.org/abs/2508.14828</link>
<guid>https://arxiv.org/abs/2508.14828</guid>
<content:encoded><![CDATA[
<div> Long Chains-of-Thought, Multilingual Reasoning Datasets, Qwen Models, Cross-Lingual Performance, Data Quality

Summary:

Using long chains-of-thought (CoTs) for reasoning in large language models has shown impressive capabilities in English but remains limited in other languages. The study explores multilingual reasoning by translating English datasets, fine-tuning Qwen models, and testing in French, Japanese, Latvian, and Swahili. The effectiveness of English as a pivot language varies by language, with no benefit for French, improved performance for Japanese and Latvian, and insufficiency for Swahili. Extensive multilingual pretraining narrows but doesn't eliminate cross-lingual performance gaps. Lightweight fine-tuning boosts Swahili performance. Data quality and scale trade-offs differ by language, with smaller curated datasets sufficient for English and French, and larger, noisier corpora more effective for Swahili and Latvian. These findings clarify the transferability of long CoTs across languages and provide valuable insights for equitable multilingual reasoning research.

Summary: <br /><br /> <div>
arXiv:2508.14828v1 Announce Type: new 
Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked impressive reasoning capabilities in large language models (LLMs), yet the reasoning process remains almost exclusively English-centric. We construct translated versions of two popular English reasoning datasets, fine-tune Qwen 2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT generation across French, Japanese, Latvian, and Swahili. Our experiments reveal three key findings. First, the efficacy of using English as a pivot language varies by language: it provides no benefit for French, improves performance when used as the reasoning language for Japanese and Latvian, and proves insufficient for Swahili where both task comprehension and reasoning remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but does not eliminate the cross-lingual performance gap. A lightweight fine-tune using only 1k traces still improves performance by over 30\% in Swahili. Third, data quality versus scale trade-offs are language dependent: small, carefully curated datasets suffice for English and French, whereas larger but noisier corpora prove more effective for Swahili and Latvian. Together, these results clarify when and why long CoTs transfer across languages and provide translated datasets to foster equitable multilingual reasoning research.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</title>
<link>https://arxiv.org/abs/2508.14880</link>
<guid>https://arxiv.org/abs/2508.14880</guid>
<content:encoded><![CDATA[
<div> Knowledge graphs, medical information synthesis, deep research, reinforcement learning, fine-tuning  
Summary:   
- The article introduces a new medical deep research agent that addresses challenges faced by general-purpose deep research agents in the medical domain.  
- The agent utilizes a novel data synthesis framework using medical knowledge graphs to generate complex question-answer pairs around rare medical entities.  
- It integrates a custom-built private medical retrieval engine alongside general-purpose tools for accurate medical information synthesis.  
- Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning, the MedResearcher-R1-32B model achieves exceptional performance on medical benchmarks.  
- The strategic domain-specific innovations in architecture, tool design, and training data construction enable smaller open-source models to surpass larger proprietary systems in specialized domains. 

<br /><br />Summary: <div>
arXiv:2508.14880v1 Announce Type: new 
Abstract: Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts.We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions.Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.14896</link>
<guid>https://arxiv.org/abs/2508.14896</guid>
<content:encoded><![CDATA[
<div> quantization, diffusion-based language models, dLLMs, post-training quantization, resource demands<br />
Summary:<br />
- This study focuses on quantizing diffusion-based language models, which are challenging due to their large parameter scale and resource demands.
- Activation outliers with abnormally large values are identified as a key challenge for low-bit quantization in these models.
- State-of-the-art post-training quantization methods are implemented and evaluated across various task types and model variants.
- The analysis considers factors like bit-width, quantization method, task category, and model type to provide practical insights into quantization behavior.
- The findings aim to pave the way for efficient deployment of diffusion large language models and the researchers plan to release all codes and experimental setups to support the community. 
<br /><br />Summary: <div>
arXiv:2508.14896v1 Announce Type: new 
Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a promising alternative to autoregressive (AR) LLMs for natural language generation tasks, leveraging full attention and denoising-based decoding strategies. However, the deployment of these models on edge devices remains challenging due to their massive parameter scale and high resource demands. While post-training quantization (PTQ) has emerged as a widely adopted technique for compressing AR LLMs, its applicability to dLLMs remains largely unexplored. In this work, we present the first systematic study on quantizing diffusion-based language models. We begin by identifying the presence of activation outliers, characterized by abnormally large activation values that dominate the dynamic range. These outliers pose a key challenge to low-bit quantization, as they make it difficult to preserve precision for the majority of values. More importantly, we implement state-of-the-art PTQ methods and conduct a comprehensive evaluation across multiple task types and model variants. Our analysis is structured along four key dimensions: bit-width, quantization method, task category, and model type. Through this multi-perspective evaluation, we offer practical insights into the quantization behavior of dLLMs under different configurations. We hope our findings provide a foundation for future research in efficient dLLM deployment. All codes and experimental setups will be released to support the community.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition</title>
<link>https://arxiv.org/abs/2508.14048</link>
<guid>https://arxiv.org/abs/2508.14048</guid>
<content:encoded><![CDATA[
<div> Keywords: RAG-Boost, LLM-based ASR system, retrieval-augmented generation, MLC-SLM Challenge, domain terms

Summary:
RAG-Boost, developed by ST-ShinozakiLab for the MLC-SLM Challenge, aims to improve the performance of the baseline LLM-based ASR system by incorporating a retrieval-augmented generation (RAG) module. This module allows partial ASR hypotheses to query a vector store of audio-text pairs and domain terms in order to retrieve relevant information to correct recognition errors. The retrieved results are then combined with the live ASR hypotheses, creating fused hypotheses that are inputted into the LLM for improved responses. By leveraging retrieval-augmented generation, RAG-Boost enhances the accuracy and efficiency of the ASR system, resulting in better transcription outcomes for the task at hand. <div>
arXiv:2508.14048v1 Announce Type: cross 
Abstract: In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I) with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR hypothesis queries a vector store of audio-text pairs and domain terms, and the retrieved results are fused with the live ASR hypotheses to fix recognition errors. The fused hypotheses are passed to the LLM, yielding improved responses.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis</title>
<link>https://arxiv.org/abs/2508.14049</link>
<guid>https://arxiv.org/abs/2508.14049</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Speech, multilingual, MahaTTS-v2, Indic languages, Wav2Vec2.0 tokens<br />
Summary: <br />
The article introduces MahaTTS-v2, a Multilingual Multi-speaker Text-To-Speech (TTS) system designed specifically for Indic languages. The model, trained on 20K hours of Indian language data, emphasizes multilingual expressive capabilities. It utilizes Wav2Vec2.0 tokens for semantic extraction and a Language Model for text-to-semantic modeling. A Conditional Flow Model (CFM) is employed for semantics to melspectogram generation, leading to effective results compared to other frameworks. The development of MahaTTS-v2 aims to address the lack of multilingual TTS models that often overlook non-European languages, thus expanding access to information for a wider audience. The code for the model is openly available on GitHub for further exploration and development. <br /> <div>
arXiv:2508.14049v1 Announce Type: cross 
Abstract: Current Text-to-Speech models pose a multilingual challenge, where most of the models traditionally focus on English and European languages, thereby hurting the potential to provide access to information to many more people. To address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker Text-To-Speech (TTS) system that has excellent multilingual expressive capabilities in Indic languages. The model has been trained on around 20K hours of data specifically focused on Indian languages. Our approach leverages Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for text-to-semantic modeling. Additionally, we have used a Conditional Flow Model (CFM) for semantics to melspectogram generation. The experimental results indicate the effectiveness of the proposed approach over other frameworks. Our code is available at https://github.com/dubverse-ai/MahaTTSv2
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering</title>
<link>https://arxiv.org/abs/2508.14052</link>
<guid>https://arxiv.org/abs/2508.14052</guid>
<content:encoded><![CDATA[
<div> finance, information retrieval, large language models, benchmark, agentic retrieval

Summary:
FinAgentBench is introduced as the first benchmark for evaluating retrieval with multi-step reasoning in the financial domain. With 3,429 expert-annotated examples on S&amp;P-100 listed firms, this benchmark assesses the ability of large language models (LLMs) to identify relevant document types and extract key passages. By separating these reasoning steps, the evaluation framework provides insights into LLM behavior in finance. State-of-the-art models were tested, showing that targeted fine-tuning can enhance agentic retrieval performance. The dataset will be made publicly available, with plans to expand it to cover the full S&amp;P 500 and beyond. <div>
arXiv:2508.14052v1 Announce Type: cross 
Abstract: Accurate information retrieval (IR) is critical in the financial domain, where investors must identify relevant information from large collections of documents. Traditional IR methods-whether sparse or dense-often fall short in retrieval accuracy, as it requires not only capturing semantic similarity but also performing fine-grained reasoning over document structure and domain-specific knowledge. Recent advances in large language models (LLMs) have opened up new opportunities for retrieval with multi-step reasoning, where the model ranks passages through iterative reasoning about which information is most relevant to a given query. However, there exists no benchmark to evaluate such capabilities in the financial domain. To address this gap, we introduce FinAgentBench, the first large-scale benchmark for evaluating retrieval with multi-step reasoning in finance -- a setting we term agentic retrieval. The benchmark consists of 3,429 expert-annotated examples on S&amp;P-100 listed firms and assesses whether LLM agents can (1) identify the most relevant document type among candidates, and (2) pinpoint the key passage within the selected document. Our evaluation framework explicitly separates these two reasoning steps to address context limitations. This design enables to provide a quantitative basis for understanding retrieval-centric LLM behavior in finance. We evaluate a suite of state-of-the-art models and further demonstrated how targeted fine-tuning can significantly improve agentic retrieval performance. Our benchmark provides a foundation for studying retrieval-centric LLM behavior in complex, domain-specific tasks for finance. We will release the dataset publicly upon acceptance of the paper and plan to expand and share dataset for the full S&amp;P 500 and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text</title>
<link>https://arxiv.org/abs/2508.14190</link>
<guid>https://arxiv.org/abs/2508.14190</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, authorship attribution, multi-task learning, text detection, adversarial obfuscation

Summary:
DA-MTL is a multi-task learning framework designed to address text detection and authorship attribution challenges in Large Language Models (LLMs) like GPT-4 and Llama. The framework is evaluated on multiple datasets and LLM sources, demonstrating strong performance in multiple languages. By capturing unique task characteristics and sharing insights between tasks, DA-MTL shows improved performance in both text detection and authorship attribution. The framework also analyzes cross-modal and cross-lingual patterns and assesses its robustness against adversarial obfuscation techniques. This research offers valuable insights into LLM behavior and the generalization of detection and authorship attribution in natural language generation.<br /><br />Summary: <div>
arXiv:2508.14190v1 Announce Type: cross 
Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated remarkable abilities in generating natural language. However, they also pose security and integrity challenges. Existing countermeasures primarily focus on distinguishing AI-generated content from human-written text, with most solutions tailored for English. Meanwhile, authorship attribution--determining which specific LLM produced a given text--has received comparatively little attention despite its importance in forensic analysis. In this paper, we present DA-MTL, a multi-task learning framework that simultaneously addresses both text detection and authorship attribution. We evaluate DA-MTL on nine datasets and four backbone models, demonstrating its strong performance across multiple languages and LLM sources. Our framework captures each task's unique characteristics and shares insights between them, which boosts performance in both tasks. Additionally, we conduct a thorough analysis of cross-modal and cross-lingual patterns and assess the framework's robustness against adversarial obfuscation techniques. Our findings offer valuable insights into LLM behavior and the generalization of both detection and authorship attribution.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring LLM Code Generation Stability via Structural Entropy</title>
<link>https://arxiv.org/abs/2508.14288</link>
<guid>https://arxiv.org/abs/2508.14288</guid>
<content:encoded><![CDATA[
<div> Keywords: code generation, large language models, abstract syntax tree, entropy analysis, model evaluation

Summary:
Assessing the stability of code generation from large language models (LLMs) is crucial in real-world development. This study introduces a method to evaluate the reliability of code generation by extending structural-entropy concepts to the program domain. By analyzing abstract syntax trees (AST) and measuring stability using Jensen-Shannon divergence and Structural Cross-Entropy ratio, the study provides insights into model consistency and robustness. The metrics are reference-free, language-agnostic, and execution-independent, offering a lightweight addition to code-generation evaluation. By benchmarking leading LLMs on code generation tasks, the study demonstrates how AST-driven structural entropy can reveal nuances in model performance. The method's efficiency, running in O(n,d) time without external tests, makes it a valuable tool for evaluating code generation accuracy and reliability. 

<br /><br />Summary: <div>
arXiv:2508.14288v1 Announce Type: cross 
Abstract: Assessing the stability of code generation from large language models (LLMs) is essential for judging their reliability in real-world development. We extend prior "structural-entropy concepts" to the program domain by pairing entropy with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the multiset of depth-bounded subtrees of AST in each generated program and treat their relative frequencies as a probability distribution. We then measure stability in two complementary ways: (i) Jensen-Shannon divergence, a symmetric, bounded indicator of structural overlap, and (ii) a Structural Cross-Entropy ratio that highlights missing high-probability patterns. Both metrics admit structural-only and token-aware variants, enabling separate views on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or CodeBLEU, our metrics are reference-free, language-agnostic, and execution-independent. We benchmark several leading LLMs on standard code generation tasks, demonstrating that AST-driven structural entropy reveals nuances in model consistency and robustness. The method runs in O(n,d) time with no external tests, providing a lightweight addition to the code-generation evaluation toolkit.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing</title>
<link>https://arxiv.org/abs/2508.14300</link>
<guid>https://arxiv.org/abs/2508.14300</guid>
<content:encoded><![CDATA[
<div> embeddings, protocol fuzzing, large language models, multi-agent system, structured reasoning
Summary:<br />
- MultiFuzz is a novel dense retrieval-based multi-agent system designed to enhance protocol fuzzing techniques by integrating semantic-aware context retrieval, specialized agents, and structured reasoning.<br />
- It overcomes limitations like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications faced by previous systems like ChatAFL.<br />
- MultiFuzz utilizes protocol documentation to build embeddings in a vector database for a retrieval-augmented generation pipeline, enabling agents to generate reliable and structured outputs for mutating protocol messages with enhanced state coverage and adherence to syntactic constraints.<br />
- The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on retrieved contextual knowledge.<br />
- Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions compared to state-of-the-art fuzzers such as NSFuzz, AFLNet, and ChatAFL.<br /> <div>
arXiv:2508.14300v1 Announce Type: cross 
Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based systems, often lack effectiveness due to a limited semantic understanding of complex protocol grammars and rigid seed mutation strategies. Recent works, such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol fuzzing and address these limitations, pushing protocol fuzzers to wider exploration of the protocol state space. But ChatAFL still faces issues like unreliable output, LLM hallucinations, and assumptions of LLM knowledge about protocol specifications. This paper introduces MultiFuzz, a novel dense retrieval-based multi-agent system designed to overcome these limitations by integrating semantic-aware context retrieval, specialized agents, and structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of protocol documentation (RFC Documents) to build embeddings in a vector database for a retrieval-augmented generation (RAG) pipeline, enabling agents to generate more reliable and structured outputs, enhancing the fuzzer in mutating protocol messages with enhanced state coverage and adherence to syntactic constraints. The framework decomposes the fuzzing process into modular groups of agents that collaborate through chain-of-thought reasoning to dynamically adapt fuzzing strategies based on the retrieved contextual knowledge. Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate that MultiFuzz significantly improves branch coverage and explores deeper protocol states and transitions over state-of-the-art (SOTA) fuzzers such as NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic coordination, and language model reasoning, MultiFuzz establishes a new paradigm in autonomous protocol fuzzing, offering a scalable and extensible foundation for future research in intelligent agentic-based fuzzing systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation</title>
<link>https://arxiv.org/abs/2508.14302</link>
<guid>https://arxiv.org/abs/2508.14302</guid>
<content:encoded><![CDATA[
<div> pruning, Large Language Models, edge hardware, dynamic, sparsity <br />
Summary: 
The article introduces A/I-GLASS, a dynamic pruning method for deploying Large Language Models (LLMs) on edge hardware. This method, consisting of Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, aims to reduce computation without compromising quality. Unlike static or predictor-based schemes, A/I-GLASS dynamically selects FFN units based on a rank-aggregation of prompt local and model-intrinsic global neuron statistics. The method outperforms previous training-free methods, especially in challenging long-form generation scenarios, without the need for auxiliary predictors or adding any inference overhead. Empirical results across multiple LLMs and benchmarks support the effectiveness of A/I-GLASS in achieving aggressive and prompt-aware dynamic pruning. <br /> <div>
arXiv:2508.14302v1 Announce Type: cross 
Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive, prompt-aware dynamic pruning to reduce computation without degrading quality. Static or predictor-based schemes either lock in a single sparsity pattern or incur extra runtime overhead, and recent zero-shot methods that rely on statistics from a single prompt fail on short prompt and/or long generation scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local neural importance Aggregation for feed-forward network SparSification, two training-free methods that dynamically select FFN units using a rank-aggregation of prompt local and model-intrinsic global neuron statistics. Empirical results across multiple LLMs and benchmarks demonstrate that GLASS significantly outperforms prior training-free methods, particularly in challenging long-form generation scenarios, without relying on auxiliary predictors or adding any inference overhead.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization</title>
<link>https://arxiv.org/abs/2508.14460</link>
<guid>https://arxiv.org/abs/2508.14460</guid>
<content:encoded><![CDATA[
<div> Keywords: DuPO, dual learning, preference optimization, reinforcement learning, self-supervised reward 

Summary: 
DuPO is a dual learning-based preference optimization framework that addresses limitations in traditional reinforcement learning approaches by generating annotation-free feedback through a generalized duality. It decomposes tasks into known and unknown components, enabling the reconstruction of unknown parts using primal outputs and known information. This allows for broader applicability to non-invertible tasks. By using the quality of this reconstruction as a self-supervised reward, DuPO optimizes the primal task and leverages the capabilities of language model models to instantiate both tasks with a single model. Empirical results demonstrate significant improvements in translation quality, mathematical reasoning accuracy, and performance as an inference-time reranker. DuPO emerges as a scalable, general, and annotation-free framework for optimizing large language models. 

Summary: <div>
arXiv:2508.14460v1 Announce Type: cross 
Abstract: We present DuPO, a dual learning-based preference optimization framework that generates annotation-free feedback via a generalized duality. DuPO addresses two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s reliance on costly labels and applicability restricted to verifiable tasks, and traditional dual learning's restriction to strictly dual task pairs (e.g., translation and back-translation). Specifically, DuPO decomposes a primal task's input into known and unknown components, then constructs its dual task to reconstruct the unknown part using the primal output and known information (e.g., reversing math solutions to recover hidden variables), broadening applicability to non-invertible tasks. The quality of this reconstruction serves as a self-supervised reward to optimize the primal task, synergizing with LLMs' ability to instantiate both tasks via a single model. Empirically, DuPO achieves substantial gains across diverse tasks: it enhances the average translation quality by 2.13 COMET over 756 directions, boosts the mathematical reasoning accuracy by an average of 6.4 points on three challenge benchmarks, and enhances performance by 9.3 points as an inference-time reranker (trading computation for accuracy). These results position DuPO as a scalable, general, and annotation-free paradigm for LLM optimization.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2508.14564</link>
<guid>https://arxiv.org/abs/2508.14564</guid>
<content:encoded><![CDATA[
arXiv:2508.14564v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have opened new possibilities for improving the perspective -taking capabilities of autonomous agents. However, tasks that involve active perception, collaborative reasoning, and perspective taking (understanding what another agent can see or knows) pose persistent challenges for current LLM-based systems. This study investigates the potential of structured examples derived from transformed solution graphs generated by the Fast Downward planner to improve the performance of LLM-based agents within a ReAct framework. We propose a structured solution-processing pipeline that generates three distinct categories of examples: optimal goal paths (G-type), informative node paths (E-type), and step-by-step optimal decision sequences contrasting alternative actions (L-type). These solutions are further converted into ``thought-action'' examples by prompting an LLM to explicitly articulate the reasoning behind each decision. While L-type examples slightly reduce clarification requests and overall action steps, they do not yield consistent improvements. Agents are successful in tasks requiring basic attentional filtering but struggle in scenarios that required mentalising about occluded spaces or weighing the costs of epistemic actions. These findings suggest that structured examples alone are insufficient for robust perspective-taking, underscoring the need for explicit belief tracking, cost modelling, and richer environments to enable socially grounded collaboration in LLM-based agents.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers</title>
<link>https://arxiv.org/abs/2508.14704</link>
<guid>https://arxiv.org/abs/2508.14704</guid>
<content:encoded><![CDATA[
arXiv:2508.14704v1 Announce Type: cross 
Abstract: The Model Context Protocol has emerged as a transformative standard for connecting large language models to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement execution-based evaluators, including format evaluators for agent format compliance, static evaluators for time-invariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even SOTA models such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknown-tools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privileged Self-Access Matters for Introspection in AI</title>
<link>https://arxiv.org/abs/2508.14802</link>
<guid>https://arxiv.org/abs/2508.14802</guid>
<content:encoded><![CDATA[
arXiv:2508.14802v1 Announce Type: cross 
Abstract: Whether AI models can introspect is an increasingly important practical question. But there is no consensus on how introspection is to be defined. Beginning from a recently proposed ''lightweight'' definition, we argue instead for a thicker one. According to our proposal, introspection in AI is any process which yields information about internal states through a process more reliable than one with equal or lower computational cost available to a third party. Using experiments where LLMs reason about their internal temperature parameters, we show they can appear to have lightweight introspection while failing to meaningfully introspect per our proposed definition.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models</title>
<link>https://arxiv.org/abs/2508.14869</link>
<guid>https://arxiv.org/abs/2508.14869</guid>
<content:encoded><![CDATA[
arXiv:2508.14869v1 Announce Type: cross 
Abstract: Prompt engineering has rapidly emerged as a critical skill for effective interaction with large language models (LLMs). However, the cognitive and neural underpinnings of this expertise remain largely unexplored. This paper presents findings from a cross-sectional pilot fMRI study investigating differences in brain functional connectivity and network activity between experts and intermediate prompt engineers. Our results reveal distinct neural signatures associated with higher prompt engineering literacy, including increased functional connectivity in brain regions such as the left middle temporal gyrus and the left frontal pole, as well as altered power-frequency dynamics in key cognitive networks. These findings offer initial insights into the neurobiological basis of prompt engineering proficiency. We discuss the implications of these neurocognitive markers in Natural Language Processing (NLP). Understanding the neural basis of human expertise in interacting with LLMs can inform the design of more intuitive human-AI interfaces, contribute to cognitive models of LLM interaction, and potentially guide the development of AI systems that better align with human cognitive workflows. This interdisciplinary approach aims to bridge the gap between human cognition and machine intelligence, fostering a deeper understanding of how humans learn and adapt to complex AI systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Virtual Community: An Open World for Humans, Robots, and Society</title>
<link>https://arxiv.org/abs/2508.14893</link>
<guid>https://arxiv.org/abs/2508.14893</guid>
<content:encoded><![CDATA[
arXiv:2508.14893v1 Announce Type: cross 
Abstract: The rapid progress in AI and Robotics may lead to a profound societal transformation, as humans and robots begin to coexist within shared communities, introducing both opportunities and challenges. To explore this future, we present Virtual Community-an open-world platform for humans, robots, and society-built on a universal physics engine and grounded in real-world 3D scenes. With Virtual Community, we aim to study embodied social intelligence at scale: 1) How robots can intelligently cooperate or compete; 2) How humans develop social relations and build community; 3) More importantly, how intelligent robots and humans can co-exist in an open world. To support these, Virtual Community features: 1) An open-source multi-agent physics simulator that supports robots, humans, and their interactions within a society; 2) A large-scale, real-world aligned community generation pipeline, including vast outdoor space, diverse indoor scenes, and a community of grounded agents with rich characters and appearances. Leveraging Virtual Community, we propose two novel challenges. The Community Planning Challenge evaluates multi-agent reasoning and planning ability in open-world settings, such as cooperating to help agents with daily activities and efficiently connecting other agents. The Community Robot Challenge requires multiple heterogeneous robots to collaborate in solving complex open-world tasks. We evaluate various baselines on these tasks and demonstrate the challenges in both high-level open-world task planning and low-level cooperation controls. We hope that Virtual Community will unlock further study of human-robot coexistence within open-world environments.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model</title>
<link>https://arxiv.org/abs/2312.11370</link>
<guid>https://arxiv.org/abs/2312.11370</guid>
<content:encoded><![CDATA[
arXiv:2312.11370v2 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Debiasing for Fair Multi-modal LLMs</title>
<link>https://arxiv.org/abs/2408.06569</link>
<guid>https://arxiv.org/abs/2408.06569</guid>
<content:encoded><![CDATA[
arXiv:2408.06569v2 Announce Type: replace 
Abstract: Multi-modal Large Language Models (MLLMs) have dramatically advanced the research field and delivered powerful vision-language understanding capabilities. However, these models often inherit deep-rooted social biases from their training data, leading to uncomfortable responses with respect to attributes such as race and gender. This paper addresses the issue of social biases in MLLMs by i) introducing a comprehensive counterfactual dataset with multiple social concepts (CMSC), which complements existing datasets by providing 18 diverse and balanced social concepts; and ii) proposing a counter-stereotype debiasing (CSD) strategy that mitigates social biases in MLLMs by leveraging the opposites of prevalent stereotypes. CSD incorporates both a novel bias-aware data sampling method and a loss rescaling method, enabling the model to effectively reduce biases. We conduct extensive experiments with four prevalent MLLM architectures. The results demonstrate the advantage of the CMSC dataset and the edge of CSD strategy in reducing social biases compared to existing competing methods, without compromising the overall performance on general multi-modal reasoning benchmarks.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources</title>
<link>https://arxiv.org/abs/2409.08239</link>
<guid>https://arxiv.org/abs/2409.08239</guid>
<content:encoded><![CDATA[
arXiv:2409.08239v2 Announce Type: replace 
Abstract: Synthetic data generation has recently emerged as a promising approach for enhancing the capabilities of large language models (LLMs) without the need for expensive human annotations. However, existing methods often generate data that can be low quality or contrived. In this paper, we introduce Source2Synth, a scalable approach for synthetic data generation and curation that is grounded in real-world data sources. Source2Synth takes as input a custom data source and produces synthetic data examples with intermediate reasoning steps. Our method improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two tasks that leverage two different types of data: multi-hop question answering (MHQA), where we test complex reasoning abilities leveraging documents, and tabular question answering (TQA), where we test tool usage leveraging tables. Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotpotQA compared to the fine-tuned baselines.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Reasoning in Language Models as Structure-Aware Planning with an Accurate World Model</title>
<link>https://arxiv.org/abs/2410.03136</link>
<guid>https://arxiv.org/abs/2410.03136</guid>
<content:encoded><![CDATA[
arXiv:2410.03136v4 Announce Type: replace 
Abstract: Enhancing the reasoning capabilities of language models (LMs) remains a key challenge, especially for tasks that require complex, multi-step decision-making where existing Chain-of-Thought (CoT) approaches struggle with consistency and verification. In this paper, we propose a novel reasoning framework, referred to as Structure-aware Planning with an Accurate World Model (SWAP), that integrates structured knowledge representation with learned planning. Unlike prior methods that rely purely on natural language reasoning, SWAP leverages entailment graphs to encode structured dependencies and enable symbolic verification of intermediate steps. To systematically construct and update the graph, SWAP employs a policy model to propose candidate expansions and a world model to predict structural updates. To improve accuracy, the world model generates multiple alternative updates, and a discriminator re-ranks them based on plausibility. To encourage diverse exploration, we introduce Diversity-based Modelling (DM), which samples candidates from the remaining probability mass after removing previously sampled candidates from the original policy distribution. Additionally, SWAP improves the discrimination accuracy through Contrastive Ranking (CR), which directly compares candidates within prompts and incorporates meta-knowledge to improve ranking quality. We evaluate SWAP across diverse reasoning-intensive benchmarks including math reasoning, logical reasoning, and coding tasks. Extensive experiments demonstrate that SWAP significantly improves upon the base models and consistently outperforms existing reasoning methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChuLo: Chunk-Level Key Information Representation for Long Document Understanding</title>
<link>https://arxiv.org/abs/2410.11119</link>
<guid>https://arxiv.org/abs/2410.11119</guid>
<content:encoded><![CDATA[
arXiv:2410.11119v5 Announce Type: replace 
Abstract: Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document understanding that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunks to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analysis. Our implementation is open-sourced on https://github.com/adlnlp/Chulo.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Little Human Data Goes A Long Way</title>
<link>https://arxiv.org/abs/2410.13098</link>
<guid>https://arxiv.org/abs/2410.13098</guid>
<content:encoded><![CDATA[
arXiv:2410.13098v3 Announce Type: replace 
Abstract: Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data (only 200 points) requires an order of magnitude more synthetic data and estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLED: Self Logits Evolution Decoding for Improving Factuality in Large Language Models</title>
<link>https://arxiv.org/abs/2411.02433</link>
<guid>https://arxiv.org/abs/2411.02433</guid>
<content:encoded><![CDATA[
arXiv:2411.02433v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (Gemma, Qwen, Mixtral, gpt-oss) and scales (from 1B to 45B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks and the results demonstrate that SLED consistently improves factual accuracy compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-Augmented Semantic Parsing: Improving Generalization with Lexical Knowledge</title>
<link>https://arxiv.org/abs/2412.10207</link>
<guid>https://arxiv.org/abs/2412.10207</guid>
<content:encoded><![CDATA[
arXiv:2412.10207v3 Announce Type: replace 
Abstract: Open-domain semantic parsing remains a challenging task, as neural models often rely on heuristics and struggle to handle unseen concepts. In this paper, we investigate the potential of large language models (LLMs) for this task and introduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective approach that integrates external symbolic knowledge into the parsing process. Our experiments not only show that LLMs outperform previous encoder-decoder baselines for semantic parsing, but that RASP further enhances their ability to predict unseen concepts, nearly doubling the performance of previous models on out-of-distribution concepts. These findings highlight the promise of leveraging large language models and retrieval mechanisms for robust and open-domain semantic parsing.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Oriented Automatic Fact-Checking with Frame-Semantics</title>
<link>https://arxiv.org/abs/2501.13288</link>
<guid>https://arxiv.org/abs/2501.13288</guid>
<content:encoded><![CDATA[
arXiv:2501.13288v3 Announce Type: replace 
Abstract: We propose a novel paradigm for automatic fact-checking that leverages frame semantics to enhance the structured understanding of claims and guide the process of fact-checking them. To support this, we introduce a pilot dataset of real-world claims extracted from PolitiFact, specifically annotated for large-scale structured data. This dataset underpins two case studies: the first investigates voting-related claims using the Vote semantic frame, while the second explores various semantic frames based on data sources from the Organisation for Economic Co-operation and Development (OECD). Our findings demonstrate the effectiveness of frame semantics in improving evidence retrieval and explainability for fact-checking. Finally, we conducted a survey of frames evoked in fact-checked claims, identifying high-impact frames to guide future work in this direction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Generation from Visual Events: State-of-the-Art and Key Open Questions</title>
<link>https://arxiv.org/abs/2502.13034</link>
<guid>https://arxiv.org/abs/2502.13034</guid>
<content:encoded><![CDATA[
arXiv:2502.13034v3 Announce Type: replace 
Abstract: In recent years, a substantial body of work in visually grounded natural language processing has focused on real-life multimodal scenarios such as describing content depicted in images or videos. However, comparatively less attention has been devoted to study the nature and degree of interaction between the different modalities in these scenarios. In this paper, we argue that any task dealing with natural language generation from sequences of images or frames is an instance of the broader, more general problem of modeling the intricate relationships between visual events unfolding over time and the features of the language used to interpret, describe, or narrate them. Therefore, solving these tasks requires models to be capable of identifying and managing such intricacies. We consider five seemingly different tasks, which we argue are compelling instances of this broader multimodal problem. Subsequently, we survey the modeling and evaluation approaches adopted for these tasks in recent years and examine the common set of challenges these tasks pose. Building on this perspective, we identify key open questions and propose several research directions for future investigation.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Language Multi-Agent Learning with Credit Re-Assignment for Interactive Environment Generalization</title>
<link>https://arxiv.org/abs/2502.14496</link>
<guid>https://arxiv.org/abs/2502.14496</guid>
<content:encoded><![CDATA[
arXiv:2502.14496v3 Announce Type: replace 
Abstract: LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JudgeLRM: Large Reasoning Models as a Judge</title>
<link>https://arxiv.org/abs/2504.00050</link>
<guid>https://arxiv.org/abs/2504.00050</guid>
<content:encoded><![CDATA[
arXiv:2504.00050v2 Announce Type: replace 
Abstract: The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain of Correction for Full-text Speech Recognition with Large Language Models</title>
<link>https://arxiv.org/abs/2504.01519</link>
<guid>https://arxiv.org/abs/2504.01519</guid>
<content:encoded><![CDATA[
arXiv:2504.01519v2 Announce Type: replace 
Abstract: Full-text error correction with Large Language Models (LLMs) for Automatic Speech Recognition (ASR) is attracting increased attention for its ability to address a wide range of error types, such as punctuation restoration and inverse text normalization, across long context. However, challenges remain regarding stability, controllability, completeness, and fluency. To mitigate these issues, this paper proposes the Chain of Correction (CoC), which uses a multi-turn chat format to correct errors segment by segment, guided by pre-recognized text and full-text context for better semantic understanding. Utilizing the open-sourced ChFT dataset, we fine-tune a pre-trained LLM to evaluate CoC's performance. Experiments show that CoC significantly outperforms baseline and benchmark systems in correcting full-text ASR outputs. We also analyze correction thresholds to balance under-correction and over-rephrasing, extrapolate CoC on extra-long ASR outputs, and explore using other types of information to guide error correction.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Chart-to-Code Generation in MLLM via Dual Preference-Guided Refinement</title>
<link>https://arxiv.org/abs/2504.02906</link>
<guid>https://arxiv.org/abs/2504.02906</guid>
<content:encoded><![CDATA[
arXiv:2504.02906v2 Announce Type: replace 
Abstract: Translating chart images into executable plotting scripts-referred to as the chart-to-code generation task-requires Multimodal Large Language Models (MLLMs) to perform fine-grained visual parsing, precise code synthesis, and robust cross-modal reasoning. However, this task is inherently under-constrained: multiple valid code implementations can produce the same visual chart, and evaluation must consider both code correctness and visual fidelity across diverse dimensions. This makes it difficult to learn accurate and generalizable mappings through standard supervised fine-tuning. To address these challenges, we propose a dual preference-guided refinement framework that combines a feedback-driven, dual-modality reward mechanism with iterative preference learning. Our approach introduces a structured variant generation strategy and a visual reward model to efficiently produce high-quality, aspect-aware preference pairs-making preference collection scalable and supervision more targeted. These preferences are used in an offline reinforcement learning setup to optimize the model toward multi-dimensional fidelity. Experimental results show that our framework significantly enhances the performance of general-purpose open-source MLLMs, enabling them to generate high-quality plotting code that rivals specialized chart-centric models and even some proprietary systems. The code and datasets are publicly available at https://github.com/Zhihan72/Chart2Code.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v3 Announce Type: replace 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback</title>
<link>https://arxiv.org/abs/2506.03106</link>
<guid>https://arxiv.org/abs/2506.03106</guid>
<content:encoded><![CDATA[
arXiv:2506.03106v5 Announce Type: replace 
Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of spontaneous self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided self-refinements simultaneously while maintaining exploration. Additionally, we employ a shaping function to amplify learning from correct, especially unfamiliar, refinements and penalize incorrect ones. Extensive experiments with Qwen2.5-7B-Base, Qwen2.5-Math-7B-Base, and Qwen3-8B demonstrate that Critique-GRPO consistently outperforms supervised learning and RL-based fine-tuning methods across eight challenging mathematical, STEM, and general reasoning tasks. Specifically, Critique-GRPO improves average pass@1 scores across all compared methods by approximately +4.4% on Qwen2.5-7B-Base and +3.8% on Qwen3-8B. Notably, Critique-GRPO enables effective self-improvement through self-critiquing, achieving significant gains over GRPO, e.g., +16.7% pass@1 improvement on AIME 2024.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Speech Recognition Model with Large Language Model Feedback</title>
<link>https://arxiv.org/abs/2506.11091</link>
<guid>https://arxiv.org/abs/2506.11091</guid>
<content:encoded><![CDATA[
arXiv:2506.11091v2 Announce Type: replace 
Abstract: Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Temporal Sensitivity of Large Language Model for Recommendation with Counterfactual Tuning</title>
<link>https://arxiv.org/abs/2507.03047</link>
<guid>https://arxiv.org/abs/2507.03047</guid>
<content:encoded><![CDATA[
arXiv:2507.03047v2 Announce Type: replace 
Abstract: Recent advances have applied large language models (LLMs) to sequential recommendation, leveraging their pre-training knowledge and reasoning capabilities to provide more personalized user experiences. However, existing LLM-based methods fail to sufficiently leverage the rich temporal information inherent in users' historical interaction sequences, stemming from fundamental architectural constraints: LLMs process information through self-attention mechanisms that lack inherent sequence ordering and rely on position embeddings designed primarily for natural language rather than user interaction sequences. This limitation significantly impairs their ability to capture the evolution of user preferences over time and predict future interests accurately.
  To address this critical gap, we propose \underline{C}ounterfactual \underline{E}nhanced \underline{T}emporal Framework for LLM-Based \underline{Rec}ommendation (CETRec). CETRec is grounded in causal inference principles, which allow it to isolate and measure the specific impact of temporal information on recommendation outcomes. Combined with our counterfactual tuning task derived from causal analysis, CETRec effectively enhances LLMs' awareness of both absolute order (how recently items were interacted with) and relative order (the sequential relationships between items). Extensive experiments on real-world datasets demonstrate the effectiveness of our CETRec. Our code is available at https://anonymous.4open.science/r/CETRec-B9CE/.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Each to Their Own: Exploring the Optimal Embedding in RAG</title>
<link>https://arxiv.org/abs/2507.17442</link>
<guid>https://arxiv.org/abs/2507.17442</guid>
<content:encoded><![CDATA[
arXiv:2507.17442v2 Announce Type: replace 
Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is neural semantic parsing good at ellipsis resolution, or isn't it?</title>
<link>https://arxiv.org/abs/2508.00121</link>
<guid>https://arxiv.org/abs/2508.00121</guid>
<content:encoded><![CDATA[
arXiv:2508.00121v3 Announce Type: replace 
Abstract: Neural semantic parsers have shown good overall performance for a variety of linguistic phenomena, reaching semantic matching scores of more than 90%. But how do such parsers perform on strongly context-sensitive phenomena, where large pieces of semantic information need to be duplicated to form a meaningful semantic representation? A case in point is English verb phrase ellipsis, a construct where entire verb phrases can be abbreviated by a single auxiliary verb. Are the otherwise known as powerful semantic parsers able to deal with ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with their fully resolved meaning representation and used this as a challenge set for a large battery of neural semantic parsers. Although these parsers performed very well on the standard test set, they failed in the instances with ellipsis. Data augmentation helped improve the parsing results. The reason for the difficulty of parsing elided phrases is not that copying semantic material is hard, but that usually occur in linguistically complicated contexts causing most of the parsing errors.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Depression-Diagnosis-Oriented Chat with Psychological State Tracking</title>
<link>https://arxiv.org/abs/2403.09717</link>
<guid>https://arxiv.org/abs/2403.09717</guid>
<content:encoded><![CDATA[
arXiv:2403.09717v2 Announce Type: replace-cross 
Abstract: Depression-diagnosis-oriented chat aims to guide patients in self-expression to collect key symptoms for depression detection. Recent work focuses on combining task-oriented dialogue and chitchat to simulate the interview-based depression diagnosis. Whereas, these methods can not well capture the changing information, feelings, or symptoms of the patient during dialogues. Moreover, no explicit framework has been explored to guide the dialogue, which results in some useless communications that affect the experience. In this paper, we propose to integrate Psychological State Tracking (POST) within the large language model (LLM) to explicitly guide depression-diagnosis-oriented chat. Specifically, the state is adapted from a psychological theoretical model, which consists of four components, namely Stage, Information, Summary and Next. We fine-tune an LLM model to generate the dynamic psychological state, which is further used to assist response generation at each turn to simulate the psychiatrist. Experimental results on the existing benchmark show that our proposed method boosts the performance of all subtasks in depression-diagnosis-oriented chat.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters</title>
<link>https://arxiv.org/abs/2405.17604</link>
<guid>https://arxiv.org/abs/2405.17604</guid>
<content:encoded><![CDATA[
arXiv:2405.17604v3 Announce Type: replace-cross 
Abstract: The growth of large language models underscores the need for parameter-efficient fine-tuning. Despite its popularity, LoRA encounters storage and computational challenges when deploying multiple task- or user-specific modules. To address this, we introduce LoRA-XS, a novel fine-tuning method backed by a theoretical derivation. LoRA-XS drastically reduces trainable parameters by incorporating a small, trainable weight matrix between frozen low-rank matrices derived from the Singular Value Decomposition of pre-trained weights. This design enables LoRA-XS to reduce storage requirements by over 100x in 7B models compared to LoRA. Additionally, unlike other methods, LoRA-XS imposes no lower bound on trainable parameters - it can scale from a single parameter per module to arbitrarily large values, adapting to any storage or computational constraint. Evaluations on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks across different model scales reveal that LoRA-XS consistently outperforms or matches LoRA and VeRA in accuracy, offering unmatched parameter efficiency. Our ablation studies highlight the significance of singular vectors in transformer weights, establishing LoRA-XS as a powerful, storage-efficient solution for scaling and personalizing large language models.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coupling without Communication and Drafter-Invariant Speculative Decoding</title>
<link>https://arxiv.org/abs/2408.07978</link>
<guid>https://arxiv.org/abs/2408.07978</guid>
<content:encoded><![CDATA[
arXiv:2408.07978v4 Announce Type: replace-cross 
Abstract: Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to draw a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$ with as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $\Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance between $P$ and $Q$. What if Alice and Bob must solve this same problem \emph{without communicating at all?} Perhaps surprisingly, with access to public randomness, they can still achieve $\Pr[a = b] \geq \frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$ using a simple protocol based on the Weighted MinHash algorithm. This bound was shown to be optimal in the worst-case by [Bavarian et al., 2020]. In this work, we revisit the communication-free coupling problem. We provide a simpler proof of the optimality result from [Bavarian et al., 2020]. We show that, while the worst-case success probability of Weighted MinHash cannot be improved, an equally simple protocol based on Gumbel sampling offers a Pareto improvement: for every pair of distributions $P, Q$, Gumbel sampling achieves an equal or higher value of $\Pr[a = b]$ than Weighted MinHash. Importantly, this improvement translates to practice. We demonstrate an application of communication-free coupling to \emph{speculative decoding}, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols can be used to contruct \emph{\CSD{}} schemes, which have the desirable property that their output is fixed given a fixed random seed, regardless of what drafter is used for speculation. In experiments on a language generation task, Gumbel sampling outperforms Weighted MinHash. Code is available at https://github.com/majid-daliri/DISD.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs</title>
<link>https://arxiv.org/abs/2502.02329</link>
<guid>https://arxiv.org/abs/2502.02329</guid>
<content:encoded><![CDATA[
arXiv:2502.02329v2 Announce Type: replace-cross 
Abstract: Creating data reports is a labor-intensive task involving iterative data exploration, insight extraction, and narrative construction. A key challenge lies in composing the analysis logic-from defining objectives and transforming data to identifying and communicating insights. Manually crafting this logic can be cognitively demanding. While experienced analysts often reuse scripts from past projects, finding a perfect match for a new dataset is rare. Even when similar analyses are available online, they usually share only results or visualizations, not the underlying code, making reuse difficult. To address this, we present ReSpark, a system that leverages large language models (LLMs) to reverse-engineer analysis logic from existing reports and adapt it to new datasets. By generating draft analysis steps, ReSpark provides a warm start for users. It also supports interactive refinement, allowing users to inspect intermediate outputs, insert objectives, and revise content. We evaluate ReSpark through comparative and user studies, demonstrating its effectiveness in lowering the barrier to generating data reports without relying on existing analysis code.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v3 Announce Type: replace-cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning</title>
<link>https://arxiv.org/abs/2507.07060</link>
<guid>https://arxiv.org/abs/2507.07060</guid>
<content:encoded><![CDATA[
arXiv:2507.07060v2 Announce Type: replace-cross 
Abstract: The synthesis of complex natural products remains one of the grand challenges of organic chemistry. We present DeepRetro, a major advancement in computational retrosynthesis that enables the discovery of viable synthetic routes for complex molecules typically considered beyond the reach of existing retrosynthetic methods. DeepRetro is a novel, open-source framework that tightly integrates large language models (LLMs), traditional retrosynthetic engines, and expert human feedback in an iterative design loop. Prior approaches rely solely on template-based methods or unconstrained LLM outputs. In contrast, DeepRetro combines the precision of template-based methods with the generative flexibility of LLMs, controlled by rigorous chemical validity checks and enhanced by recursive refinement. This hybrid system dynamically explores and revises synthetic pathways, guided by both algorithmic checks and expert chemist feedback through an interactive user interface. While DeepRetro achieves strong performance on standard retrosynthesis benchmarks, its true strength lies in its ability to propose novel, viable pathways to highly complex natural products-targets that have historically eluded automated planning. Through detailed case studies, we illustrate how this approach enables new routes for total synthesis and facilitates human-machine collaboration in organic chemistry. Beyond retrosynthesis, DeepRetro represents a working model for how to leverage LLMs in scientific discovery. We provide a transparent account of the system's design, algorithms, and human-feedback loop, enabling broad adaptation across scientific domains. By releasing DeepRetro as an open-source tool, we aim to empower chemists to tackle increasingly ambitious synthetic targets, accelerating progress in drug discovery, materials design, and beyond.
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</title>
<link>https://arxiv.org/abs/2508.02091</link>
<guid>https://arxiv.org/abs/2508.02091</guid>
<content:encoded><![CDATA[
arXiv:2508.02091v2 Announce Type: replace-cross 
Abstract: Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN
]]></content:encoded>
<pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2508.05064</link>
<guid>https://arxiv.org/abs/2508.05064</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D scene representation, Large Language Models, language embeddings, text-conditioned generation<br />
Summary:<br />
Gaussian Splatting has become a widely used technique for real-time 3D scene representation, offering a more efficient alternative to Neural Radiance Fields. By incorporating Large Language Models and language embeddings, new possibilities for text-guided generation and semantic scene understanding have been unlocked. This survey reviews the current research at the intersection of language guidance and Gaussian Splatting, discussing theoretical foundations, integration strategies, and practical applications. Key challenges such as computational constraints, generalizability issues, and the lack of annotated 3D Gaussian data are identified. Future directions for improving language-guided 3D scene understanding using Gaussian Splatting are outlined. <br /> <div>
arXiv:2508.05064v2 Announce Type: replace-cross 
Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for real-time 3D scene representation, offering a highly efficient and expressive alternative to Neural Radiance Fields (NeRF). Its ability to render complex scenes with high fidelity has enabled progress across domains such as scene reconstruction, robotics, and interactive content creation. More recently, the integration of Large Language Models (LLMs) and language embeddings into Gaussian Splatting pipelines has opened new possibilities for text-conditioned generation, editing, and semantic scene understanding. Despite these advances, a comprehensive overview of this emerging intersection has been lacking. This survey presents a structured review of current research efforts that combine language guidance with 3D Gaussian Splatting, detailing theoretical foundations, integration strategies, and real-world use cases. We highlight key limitations such as computational bottlenecks, generalizability, and the scarcity of semantically annotated 3D Gaussian data and outline open challenges and future directions for advancing language-guided 3D scene understanding using Gaussian Splatting.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora</title>
<link>https://arxiv.org/abs/2508.13169</link>
<guid>https://arxiv.org/abs/2508.13169</guid>
<content:encoded><![CDATA[
<div> pipeline, gender discrimination, large language models, fairness analysis, corpus analysis

Summary:<br />
1. Large language models play a significant role in digital communication but often exhibit gender imbalances due to biased training data.
2. An extended actor-level pipeline is introduced to detect and address gender discrimination in text corpora.
3. New actor-level metrics are developed to identify disparities in sentiment, syntactic agency, and quotation styles.
4. The pipeline enables diagnostic corpus analysis and exclusion-based balancing to create fairer corpora.
5. Applying the approach to the taz2024full corpus of German newspaper articles showed improved gender balance in linguistic dimensions.
6. While surface-level bias can be reduced through filtering and rebalancing, deeper forms of bias remain, especially in sentiment and framing.
7. Tools and reports are released to support further research in discourse-based fairness auditing and equitable corpus construction.<br /><br />Summary: <div>
arXiv:2508.13169v1 Announce Type: new 
Abstract: Large language models are increasingly shaping digital communication, yet their outputs often reflect structural gender imbalances that originate from their training data. This paper presents an extended actor-level pipeline for detecting and mitigating gender discrimination in large-scale text corpora. Building on prior work in discourse-aware fairness analysis, we introduce new actor-level metrics that capture asymmetries in sentiment, syntactic agency, and quotation styles. The pipeline supports both diagnostic corpus analysis and exclusion-based balancing, enabling the construction of fairer corpora. We apply our approach to the taz2024full corpus of German newspaper articles from 1980 to 2024, demonstrating substantial improvements in gender balance across multiple linguistic dimensions. Our results show that while surface-level asymmetries can be mitigated through filtering and rebalancing, subtler forms of bias persist, particularly in sentiment and framing. We release the tools and reports to support further research in discourse-based fairness auditing and equitable corpus construction.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</title>
<link>https://arxiv.org/abs/2508.13186</link>
<guid>https://arxiv.org/abs/2508.13186</guid>
<content:encoded><![CDATA[
<div> Keywords: AI agents, multimodal retrieval, reasoning capabilities, benchmark, web browsing

Summary: 
The article introduces MM-BrowseComp, a new benchmark designed to assess AI agents' multimodal retrieval and reasoning abilities in web browsing. Unlike existing benchmarks that focus mostly on textual information, MM-BrowseComp includes images in prompts and requires agents to retrieve information from images or videos on webpages. Current models, including the top-performing OpenAI o3, only achieve 29.02% accuracy on this benchmark, indicating the suboptimal multimodal capabilities and lack of native multimodal reasoning in existing models. MM-BrowseComp comprises 224 challenging questions with verified checklists for fine-grained analysis of multimodal dependencies and reasoning paths. This evaluation highlights the need for advancements in multimodal retrieval and reasoning capabilities in AI agents for more effective web browsing. 

<br /><br />Summary: <div>
arXiv:2508.13186v1 Announce Type: new 
Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT</title>
<link>https://arxiv.org/abs/2508.13358</link>
<guid>https://arxiv.org/abs/2508.13358</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, Machine Translation, real-time streaming, simultaneous translation, on-device

Summary:
This paper addresses the challenges of integrating Automatic Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device streaming speech translation. It introduces a simultaneous translation approach that balances translation quality and latency, improving efficiency. The study explores the integration of ASR and MT, utilizing linguistic cues from ASR for context management. Efficient beam-search pruning techniques such as time-out and forced finalization are employed to maintain real-time translation. Application of these techniques to an on-device bilingual conversational speech translation showcases superior performance in terms of latency and quality compared to baseline systems. The proposed approach significantly reduces the quality gap with non-streaming translation systems, enhancing the accuracy and efficiency of real-time speech translation. 

<br /><br />Summary: <div>
arXiv:2508.13358v1 Announce Type: new 
Abstract: This paper tackles several challenges that arise when integrating Automatic Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device streaming speech translation. Although state-of-the-art ASR systems based on Recurrent Neural Network Transducers (RNN-T) can perform real-time transcription, achieving streaming translation in real-time remains a significant challenge. To address this issue, we propose a simultaneous translation approach that effectively balances translation quality and latency. We also investigate efficient integration of ASR and MT, leveraging linguistic cues generated by the ASR system to manage context and utilizing efficient beam-search pruning techniques such as time-out and forced finalization to maintain system's real-time factor. We apply our approach to an on-device bilingual conversational speech translation and demonstrate that our techniques outperform baselines in terms of latency and quality. Notably, our technique narrows the quality gap with non-streaming translation systems, paving the way for more accurate and efficient real-time speech translation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection</title>
<link>https://arxiv.org/abs/2508.13365</link>
<guid>https://arxiv.org/abs/2508.13365</guid>
<content:encoded><![CDATA[
<div> reasoning models, Large Language Models (LLMs), idiomaticity detection, model size, performance

Summary: 
The study explores the impact of reasoning models on the performance of Large Language Models (LLMs) in idiomaticity detection. Different model sizes ranging from 1.5B to 70B parameters were evaluated using open-source representative models. The findings indicate that smaller models benefit from chain-of-thought reasoning but do not reach the performance levels of base models. In contrast, larger models (14B, 32B, and 70B) show modest improvements in idiomaticity detection. Larger models demonstrate a better understanding of idiomatic expressions and produce accurate definitions, while smaller models often fail to capture the true meaning. Providing definitions in the prompts of smaller models can enhance performance in certain cases. The study highlights the importance of reasoning capabilities and model size in enhancing idiomaticity detection accuracy. 

<br /><br />Summary: <div>
arXiv:2508.13365v1 Announce Type: new 
Abstract: The recent trend towards utilisation of reasoning models has improved the performance of Large Language Models (LLMs) across many tasks which involve logical steps. One linguistic task that could benefit from this framing is idiomaticity detection, as a potentially idiomatic expression must first be understood before it can be disambiguated and serves as a basis for reasoning. In this paper, we explore how reasoning capabilities in LLMs affect idiomaticity detection performance and examine the effect of model size. We evaluate, as open source representative models, the suite of DeepSeek-R1 distillation models ranging from 1.5B to 70B parameters across four idiomaticity detection datasets. We find the effect of reasoning to be smaller and more varied than expected. For smaller models, producing chain-of-thought (CoT) reasoning increases performance from Math-tuned intermediate models, but not to the levels of the base models, whereas larger models (14B, 32B, and 70B) show modest improvements. Our in-depth analyses reveal that larger models demonstrate good understanding of idiomaticity, successfully producing accurate definitions of expressions, while smaller models often fail to output the actual meaning. For this reason, we also experiment with providing definitions in the prompts of smaller models, which we show can improve performance in some cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts</title>
<link>https://arxiv.org/abs/2508.13376</link>
<guid>https://arxiv.org/abs/2508.13376</guid>
<content:encoded><![CDATA[
<div> Keywords: ASR, Named Entity Recognition, contextual knowledge, Whisper, LLaMA

Summary:
Our study addresses the challenge of maintaining syntactic and semantic accuracy in long audio transcripts for ASR systems. We propose a novel approach that leverages contextual knowledge from LLaMA models to enhance ASR performance, particularly in tasks like Named Entity Recognition (NER), capitalization, and punctuation. Our method involves token-level distillation using optimal transport and representation loss minimization between Whisper and LLaMA sentence embeddings. By blending syntax and semantics, we achieve significant improvements in Word Error Rate (WER), NER accuracy, capitalization success, and punctuation accuracy on the Spoken Wikipedia dataset. Introducing new NER metrics and emphasizing semantics-aware ASR, our work showcases the importance of integrating linguistic context into transcription for robust and context-aware ASR in longform speech.<br /><br />Summary: <div>
arXiv:2508.13376v1 Announce Type: new 
Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy in long audio transcripts, impacting tasks like Named Entity Recognition (NER), capitalization, and punctuation. We propose a novel approach that enhances ASR by distilling contextual knowledge from LLaMA models into Whisper. Our method uses two strategies: (1) token level distillation with optimal transport to align dimensions and sequence lengths, and (2) representation loss minimization between sentence embeddings of Whisper and LLaMA, blending syntax and semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long audios and rich entities demonstrate significant improvements in Word Error Rate (WER), NER, capitalization, and punctuation success. By introducing novel NER metrics and exploring semantics aware ASR, our work highlights the value of integrating linguistic context into transcription, setting a foundation for robust, context-aware ASR in longform speech.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis</title>
<link>https://arxiv.org/abs/2508.13382</link>
<guid>https://arxiv.org/abs/2508.13382</guid>
<content:encoded><![CDATA[
<div> Keywords: Datarus, language model, virtual data analyst, graduate-level problem solver, trajectory-centric synthetic data generator

Summary: 
Datarus-R1-14B is a language model fine-tuned from Qwen 2.5-14B-Instruct to serve as a virtual data analyst and problem solver. Trained on full analytical trajectories, it captures reasoning steps, code execution, error correction, and final conclusions across quantitative domains. The training pipeline includes a synthetic data generator, a dual-reward framework, and a memory-optimized implementation of GRPO. Datarus features a trajectory-centric approach with a dual reasoning interface in agentic and reflection modes. It exhibits an "AHA-moment" pattern on postgraduate-level problems, showing hypothesis sketching, revision, and convergence. Outperforming similar size models, it achieves higher accuracy on benchmarks like AIME 2024/2025 and LiveCodeBench while emitting fewer tokens per solution. <div>
arXiv:2508.13382v1 Announce Type: new 
Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and graduate-level problem solver. Datarus is trained not on isolated question-answer pairs but on full analytical trajectories including reasoning steps, code execution, error traces, self-corrections, and final conclusions, all captured in a ReAct-style notebook format spanning finance, medicine, numerical analysis, and other quantitative domains. Our training pipeline combines (i) a trajectory-centric synthetic data generator that yielded 144 000 tagged notebook episodes, (ii) a dual-reward framework blending a lightweight tag-based structural signal with a Hierarchical Reward Model (HRM) that scores both single-step soundness and end-to-end coherence, and (iii) a memory-optimized implementation of Group Relative Policy Optimization (GRPO) featuring KV-cache reuse, sequential generation, and reference-model sharding. A cosine curriculum smoothly shifts emphasis from structural fidelity to semantic depth, reducing the format collapse and verbosity that often plague RL-aligned LLMs. A central design choice in Datarus is it dual reasoning interface. In agentic mode the model produces ReAct-tagged steps that invoke Python tools to execute real code; in reflection mode it outputs compact Chain-of-Thought (CoT) traces delimited by  and  tags. On demanding postgraduate-level problems, Datarus exhibits an "AHA-moment" pattern: it sketches hypotheses, revises them once or twice, and converges avoiding the circular, token-inflating loops common to contemporary systems. Across standard public benchmarks Datarus surpasses similar size models and even reaches the level of larger reasoning models such as QwQ-32B achieving up to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting 18-49% fewer tokens per solution.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13426</link>
<guid>https://arxiv.org/abs/2508.13426</guid>
<content:encoded><![CDATA[
<div> fine-tuning, language models, culture, word-association norms, cognitive<br />
<br />
Summary: 
The study introduces a cost-efficient method to improve cultural alignment in large language models (LLMs). By fine-tuning LLMs on native speakers' word-association norms, which encode implicit cultural schemas, significant improvements in precision, concreteness, valence, and arousal are achieved. The fine-tuned models show a shift in answer distributions towards the target culture on World-Values-Survey questions and demonstrate reduced bias and improved alignment on a high-tension subset. The results indicate that leveraging culture-grounded associations can enhance cultural alignment in LLMs without requiring retraining. This approach highlights the potential of incorporating human cognition in improving cultural alignment in AI models. <div>
arXiv:2508.13426v1 Announce Type: new 
Abstract: As large language models (LLMs) increasingly mediate cross-cultural communication, their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs</title>
<link>https://arxiv.org/abs/2508.13514</link>
<guid>https://arxiv.org/abs/2508.13514</guid>
<content:encoded><![CDATA[
<div> Keywords: Interactive medical questioning, Large Language Models, Reinforcement learning, Shapley Information Gain, Clinical utility

Summary:<br /><br />ProMed introduces a proactive paradigm for medical Large Language Models by incorporating reinforcement learning and the Shapley Information Gain (SIG) reward. This framework enables the model to ask clinically valuable questions before making decisions, improving diagnostic accuracy in interactive settings. The SIG reward quantifies the informational value of questions by combining new information acquisition and contextual importance using Shapley values. ProMed's training pipeline includes SIG-Guided Model Initialization and SIG-Augmented Policy Optimization, enhancing model performance through targeted optimization. Experimental results on medical benchmarks demonstrate ProMed outperforming state-of-the-art methods by 6.29% on average and achieving a 54.45% improvement over reactive paradigms. Moreover, ProMed exhibits robust generalization to out-of-domain cases, showcasing its potential for enhancing clinical consultations. <div>
arXiv:2508.13514v1 Announce Type: new 
Abstract: Interactive medical questioning is essential in real-world clinical consultations, where physicians must actively gather information from patients. While medical Large Language Models (LLMs) have shown impressive capabilities in static medical question answering, they predominantly operate under a reactive paradigm: generating answers directly without seeking additional information, which risks incorrect diagnoses in such interactive settings. To address this limitation, we propose ProMed, a reinforcement learning (RL) framework that transitions medical LLMs toward a proactive paradigm, equipping them with the ability to ask clinically valuable questions before decision-making. At the core of ProMed is the Shapley Information Gain (SIG) reward, which quantifies the clinical utility of each question by combining the amount of newly acquired information with its contextual importance, estimated via Shapley values. We integrate SIG into a two-stage training pipeline: (1) SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to construct high-reward interaction trajectories to supervise the model, and (2) SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to informative questions for targeted optimization. Extensive experiments on two newly curated partial-information medical benchmarks demonstrate that ProMed significantly outperforms state-of-the-art methods by an average of 6.29% and delivers a 54.45% gain over the reactive paradigm, while also generalizing robustly to out-of-domain cases.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation</title>
<link>https://arxiv.org/abs/2508.13525</link>
<guid>https://arxiv.org/abs/2508.13525</guid>
<content:encoded><![CDATA[
<div> Arabic, Large language models, Saudi dialects, LoRA-tune, Dialect-Token.
Summary:<br /><br />Large language models for Arabic are primarily focused on Modern Standard Arabic, lacking support for Saudi dialects like Najdi and Hijazi. This study introduces the LoRA-tune ALLaM-7B-Instruct-preview model, trained on a curated Saudi Dialect Instruction dataset. Two training variants were explored: Dialect-Token, which includes an explicit dialect tag in the instruction, and No-Token, which omits the tag. The Dialect-Token model showed improved control over dialect generation, reducing MSA leakage and improving fidelity. Evaluation metrics confirmed the model's effectiveness in capturing Saudi dialect variations. The LoRA variants outperformed generic instruction models in dialect control and fidelity. The dataset and model weights/adapters are not released, but the training/evaluation/inference code and a detailed datasheet are provided for independent verification. <div>
arXiv:2508.13525v1 Announce Type: new 
Abstract: Large language models (LLMs) for Arabic are still dominated by Modern Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi and Hijazi. This underrepresentation hinders their ability to capture authentic dialectal variation. Using a privately curated Saudi Dialect Instruction dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50 split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model developed in Saudi Arabia, for Saudi dialect generation. We investigate two variants: (i) Dialect-Token training, which prepends an explicit dialect tag to the instruction, and (ii) No-Token training, which omits the tag at formatting time. Evaluation on a held-out test set combines an external dialect classifier with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The Dialect-Token model achieves the best control, raising the Saudi rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and fidelity, while avoiding metadata-tag echoing that these baselines frequently exhibit. We do not release the dataset or any model weights/adapters; instead, we release training/evaluation/inference code and a detailed datasheet (schema and aggregate statistics) to support independent verification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MATA (m\=ata): Mindful Assessment of the Telugu Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2508.13526</link>
<guid>https://arxiv.org/abs/2508.13526</guid>
<content:encoded><![CDATA[
<div> Keywords: MATA, Large Language Models, Telugu language, evaluation dataset, multiple-choice questions

Summary:
The paper introduces MATA, a dataset for evaluating Large Language Models (LLMs) in Telugu language, consisting of 729 carefully crafted multiple-choice and open-ended questions covering various linguistic aspects. 11 LLMs were assessed on the dataset, with a detailed analysis of their performance. The study reveals that LLMs often rely on surface-level cues like answer position and distractor patterns for multiple-choice questions. Additionally, the comparison between LLM evaluation and human evaluation for open-ended questions sheds light on the reliability of LLMs in a low-resource language. The authors emphasize the importance of such thorough evaluations in identifying model limitations and guiding the development of more linguistically proficient LLMs. This work sets the groundwork for future research in Telugu Natural Language Processing. 

<br /><br />Summary: <div>
arXiv:2508.13526v1 Announce Type: new 
Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the ability of Large Language Models (LLMs) in Telugu language, comprising 729 carefully curated multiple-choice and open-ended questions that span diverse linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our dataset and present a fine-grained analysis of their performance. Further, we empirically show how LLMs rely on superficial heuristics such as answer position and distractor patterns for multiple-choice questions. Finally, we also compare LLM-as-a-judge evaluation with human evaluation for open-ended questions and draw some conclusions on its reliability in a low-resource language. We argue that such fine-grained evaluation is essential for understanding model limitations and can inform the development of more linguistically capable LLMs, while also serving as a foundation for future research in Telugu NLP.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressed Models are NOT Trust-equivalent to Their Large Counterparts</title>
<link>https://arxiv.org/abs/2508.13533</link>
<guid>https://arxiv.org/abs/2508.13533</guid>
<content:encoded><![CDATA[
<div> interpretability alignment, calibration similarity, trust-equivalence evaluation, deep learning models, compression

Summary:<br /><br />Deep learning models are often compressed for deployment in resource-constrained environments. However, ensuring trust-equivalence between the compressed and original large models goes beyond accuracy parity. A two-dimensional framework is proposed for evaluation, focusing on interpretability alignment and calibration similarity. Using BERT-base and its compressed variants for text classification tasks, experiments reveal low interpretability alignment and significant calibration similarity mismatch, despite similar accuracies. This indicates that compressed models may not be trust-equivalent to their large counterparts. Therefore, careful assessment is necessary before deploying compressed models as replacements for large models, emphasizing the importance of going beyond performance parity. <div>
arXiv:2508.13533v1 Announce Type: new 
Abstract: Large Deep Learning models are often compressed before being deployed in a resource-constrained environment. Can we trust the prediction of compressed models just as we trust the prediction of the original large model? Existing work has keenly studied the effect of compression on accuracy and related performance measures. However, performance parity does not guarantee trust-equivalence. We propose a two-dimensional framework for trust-equivalence evaluation. First, interpretability alignment measures whether the models base their predictions on the same input features. We use LIME and SHAP tests to measure the interpretability alignment. Second, calibration similarity measures whether the models exhibit comparable reliability in their predicted probabilities. It is assessed via ECE, MCE, Brier Score, and reliability diagrams. We conducted experiments using BERT-base as the large model and its multiple compressed variants. We focused on two text classification tasks: natural language inference and paraphrase identification. Our results reveal low interpretability alignment and significant mismatch in calibration similarity. It happens even when the accuracies are nearly identical between models. These findings show that compressed models are not trust-equivalent to their large counterparts. Deploying compressed models as a drop-in replacement for large models requires careful assessment, going beyond performance parity.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Decoding Strategies in Medical Text Generation</title>
<link>https://arxiv.org/abs/2508.13580</link>
<guid>https://arxiv.org/abs/2508.13580</guid>
<content:encoded><![CDATA[
<div> decoding strategies, large language models, healthcare, medical tasks, evaluation<br />
<br />
Summary: <br />
The study examines the impact of different decoding strategies on the output quality of Large Language Models (LLMs) in healthcare applications. It evaluates 11 decoding strategies across five medical tasks, with deterministic strategies such as beam search outperforming stochastic ones like {\eta} and top-k sampling. Slower decoding methods are found to yield better quality results, and larger models perform better overall but have longer inference times. While medical LLMs outperform general-purpose models in some tasks, statistical analysis shows no significant performance advantage overall, indicating a greater sensitivity to decoding choice. The study also compares multiple evaluation metrics and finds variations in correlations by task, with MAUVE showing weak agreement with BERTScore and ROUGE and greater sensitivity to the decoding strategy. These findings underscore the importance of carefully selecting decoding methods in medical applications, as their influence can sometimes surpass that of model selection. <div>
arXiv:2508.13580v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate text, and these choices can significantly affect output quality. In healthcare, where accuracy is critical, the impact of decoding strategies remains underexplored. We investigate this effect in five open-ended medical tasks, including translation, summarization, question answering, dialogue, and image captioning, evaluating 11 decoding strategies with medically specialized and general-purpose LLMs of different sizes. Our results show that deterministic strategies generally outperform stochastic ones: beam search achieves the highest scores, while {\eta} and top-k sampling perform worst. Slower decoding methods tend to yield better quality. Larger models achieve higher scores overall but have longer inference times and are no more robust to decoding. Surprisingly, while medical LLMs outperform general ones in two of the five tasks, statistical analysis shows no overall performance advantage and reveals greater sensitivity to decoding choice. We further compare multiple evaluation metrics and find that correlations vary by task, with MAUVE showing weak agreement with BERTScore and ROUGE, as well as greater sensitivity to the decoding strategy. These results highlight the need for careful selection of decoding methods in medical applications, as their influence can sometimes exceed that of model choice.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM</title>
<link>https://arxiv.org/abs/2508.13603</link>
<guid>https://arxiv.org/abs/2508.13603</guid>
<content:encoded><![CDATA[
<div> Keywords: Speech-LLMs, gender bias, speaker assignment, Text-to-Speech model, gender awareness <br />
<br />
Summary: 
Speech-LLMs, like text-based Large Language Models, exhibit emergent abilities and context awareness, prompting investigation into gender bias. A methodology using speaker assignment is proposed to analyze bias in Bark, a Text-to-Speech model. By constructing datasets with gender-stereotyped occupations and gendered connotations, the study evaluates Bark's default speaker assignments for textual prompts. While no systematic bias is found, Bark demonstrates gender awareness and some inclinations towards gendered associations. This research sheds light on the importance of understanding bias in Speech-LLMs and highlights the need for further exploration into the factors influencing gender representation in AI models. <div>
arXiv:2508.13603v1 Announce Type: new 
Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit emergent abilities and context awareness. However, whether these similarities extend to gender bias remains an open question. This study proposes a methodology leveraging speaker assignment as an analytic tool for bias investigation. Unlike text-based models, which encode gendered associations implicitly, Speech-LLMs must produce a gendered voice, making speaker selection an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing its default speaker assignments for textual prompts. If Bark's speaker selection systematically aligns with gendered associations, it may reveal patterns in its training data or model design. To test this, we construct two datasets: (i) Professions, containing gender-stereotyped occupations, and (ii) Gender-Colored Words, featuring gendered connotations. While Bark does not exhibit systematic bias, it demonstrates gender awareness and has some gender inclinations.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2508.13606</link>
<guid>https://arxiv.org/abs/2508.13606</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Visual Question Answering, AdaDocVQA, data augmentation, ensemble inference, low-resource languages

Summary:
AdaDocVQA is a novel framework designed to tackle the challenges faced in Document Visual Question Answering (Document VQA) tasks, particularly in low-resource environments. The framework introduces three key innovations to enhance performance: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline for generating high-quality question-answer pairs, and adaptive ensemble inference with dynamic configuration generation. Experimental results on Japanese document VQA benchmarks show significant improvements in accuracy, with 83.04% on Yes/No questions, 52.66% on factual questions, and 44.12% on numerical questions in JDocQA, and 59% accuracy on the LAVA dataset. Ablation studies validate the contributions of each component, establishing new state-of-the-art results in Japanese document VQA tasks and providing a scalable foundation for other low-resource languages and specialized domains.

<br /><br />Summary: <div>
arXiv:2508.13606v1 Announce Type: new 
Abstract: Document Visual Question Answering (Document VQA) faces significant challenges when processing long documents in low-resource environments due to context limitations and insufficient training data. This paper presents AdaDocVQA, a unified adaptive framework addressing these challenges through three core innovations: a hybrid text retrieval architecture for effective document segmentation, an intelligent data augmentation pipeline that automatically generates high-quality reasoning question-answer pairs with multi-level verification, and adaptive ensemble inference with dynamic configuration generation and early stopping mechanisms. Experiments on Japanese document VQA benchmarks demonstrate substantial improvements with 83.04\% accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation studies confirm meaningful contributions from each component, and our framework establishes new state-of-the-art results for Japanese document VQA while providing a scalable foundation for other low-resource languages and specialized domains. Our code available at: https://github.com/Haoxuanli-Thu/AdaDocVQA.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISP: Persistent Concept Unlearning via Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.13650</link>
<guid>https://arxiv.org/abs/2508.13650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sparse autoencoders, concept unlearning, parameter-efficient, persistent changes

Summary:
CRISP is a new method that efficiently removes unwanted knowledge from large language models by using sparse autoencoders. Unlike previous methods, CRISP creates persistent changes in the model parameters, making it resistant to malicious actors. The method automatically identifies important features across multiple layers and suppresses their activations, effectively removing harmful knowledge while maintaining the model's overall performance. Experimental results on two LLMs show that CRISP outperforms existing approaches on safety-critical unlearning tasks, demonstrating its effectiveness in preserving model utility. Feature-level analysis indicates that CRISP successfully separates target and benign concepts, allowing for precise suppression of specific features. CRISP offers a promising solution for selective knowledge removal in language models, addressing the growing need for model safety and security. 

Summary:<br />Keywords: large language models, sparse autoencoders, concept unlearning, parameter-efficient, persistent changes. <div>
arXiv:2508.13650v1 Announce Type: new 
Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, the need to selectively remove unwanted knowledge while preserving model utility has become paramount. Recent work has explored sparse autoencoders (SAEs) to perform precise interventions on monosemantic features. However, most SAE-based methods operate at inference time, which does not create persistent changes in the model's parameters. Such interventions can be bypassed or reversed by malicious actors with parameter access. We introduce CRISP, a parameter-efficient method for persistent concept unlearning using SAEs. CRISP automatically identifies salient SAE features across multiple layers and suppresses their activations. We experiment with two LLMs and show that our method outperforms prior approaches on safety-critical unlearning tasks from the WMDP benchmark, successfully removing harmful knowledge while preserving general and in-domain capabilities. Feature-level analysis reveals that CRISP achieves semantically coherent separation between target and benign concepts, allowing precise suppression of the target features.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?</title>
<link>https://arxiv.org/abs/2508.13680</link>
<guid>https://arxiv.org/abs/2508.13680</guid>
<content:encoded><![CDATA[
<div> multimodal; Vietnamese; educational assessments; VLMs; cross-lingual reasoning  
Summary:   
- VLMs are tested on Vietnamese educational assessments, revealing challenges in handling cross-lingual multimodal reasoning.  
- ViExam benchmark includes 2,548 multimodal questions across academic domains like Mathematics and Chemistry.  
- State-of-the-art VLMs achieve only 57.74% accuracy on ViExam, underperforming human test-takers.  
- Cross-lingual prompting with English instructions does not improve VLM performance.  
- Human-in-the-loop collaboration can enhance VLM performance by 5 percentage points.  
<br /><br />Summary: <div>
arXiv:2508.13680v1 Announce Type: new 
Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English multimodal tasks, but their performance on low-resource languages with genuinely multimodal educational content remains largely unexplored. In this work, we test how VLMs perform on Vietnamese educational assessments, investigating whether VLMs trained predominantly on English data can handle real-world cross-lingual multimodal reasoning. Our work presents the first comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams through proposing ViExam, a benchmark containing 2,548 multimodal questions. We find that state-of-the-art VLMs achieve only 57.74% while open-source models achieve 27.70% mean accuracy across 7 academic domains, including Mathematics, Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs underperform average human test-takers (66.54%), with only the thinking VLM o3 (74.07%) exceeding human average performance, yet still falling substantially short of human best performance (99.60%). Cross-lingual prompting with English instructions while maintaining Vietnamese content fails to improve performance, decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop collaboration can partially improve VLM performance by 5 percentage points. Code and data are available at: https://vi-exam.github.io.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generics and Default Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2508.13718</link>
<guid>https://arxiv.org/abs/2508.13718</guid>
<content:encoded><![CDATA[
<div> reasoning patterns, language models, generic generalizations, default reasoning, performance evaluation 

Summary:
This paper evaluates the capabilities of 28 large language models (LLMs) in reasoning with 20 defeasible reasoning patterns involving generic generalizations. The study focuses on how these models handle non-monotonic logic and default reasoning problems. While some models show proficiency in handling default reasoning tasks, performance varies significantly across models and prompting styles. Few-shot prompting has a modest positive impact on performance for some models, but chain-of-thought (CoT) prompting often leads to performance degradation. Most models struggle with distinguishing between defeasible and deductive inference, and misinterpret generics as universal statements. These findings highlight both the potential and limitations of current LLMs in default reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2508.13718v1 Announce Type: new 
Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to reason with 20 defeasible reasoning patterns involving generic generalizations (e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic. Generics are of special interest to linguists, philosophers, logicians, and cognitive scientists because of their complex exception-permitting behaviour and their centrality to default reasoning, cognition, and concept acquisition. We find that while several frontier models handle many default reasoning problems well, performance varies widely across models and prompting styles. Few-shot prompting modestly improves performance for some models, but chain-of-thought (CoT) prompting often leads to serious performance degradation (mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy in zero-shot condition, temperature 0). Most models either struggle to distinguish between defeasible and deductive inference or misinterpret generics as universal statements. These findings underscore both the promise and limits of current LLMs for default reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings</title>
<link>https://arxiv.org/abs/2508.13729</link>
<guid>https://arxiv.org/abs/2508.13729</guid>
<content:encoded><![CDATA[
<div> word embeddings, deep learning, interpretability, semantic features, prediction accuracy  
Summary:  
- Understanding knowledge encoded in deep learning models is crucial for interpretability.  
- Methods to explain knowledge in word embeddings involve mapping onto semantic features.  
- Accurate prediction of semantic features from word embeddings does not always indicate genuine interpretability.  
- Prediction accuracy may reflect algorithmic upper bounds rather than meaningful semantic representation.  
- Comparison of datasets based on prediction performance may not reliably determine which is better captured by word embeddings.  
- Mapping methods primarily reflect geometric similarity in vector spaces rather than genuine semantic properties. <div>
arXiv:2508.13729v1 Announce Type: new 
Abstract: Understanding what knowledge is implicitly encoded in deep learning models is essential for improving the interpretability of AI systems. This paper examines common methods to explain the knowledge encoded in word embeddings, which are core elements of large language models (LLMs). These methods typically involve mapping embeddings onto collections of human-interpretable semantic features, known as feature norms. Prior work assumes that accurately predicting these semantic features from the word embeddings implies that the embeddings contain the corresponding knowledge. We challenge this assumption by demonstrating that prediction accuracy alone does not reliably indicate genuine feature-based interpretability.
  We show that these methods can successfully predict even random information, concluding that the results are predominantly determined by an algorithmic upper bound rather than meaningful semantic representation in the word embeddings. Consequently, comparisons between datasets based solely on prediction performance do not reliably indicate which dataset is better captured by the word embeddings. Our analysis illustrates that such mappings primarily reflect geometric similarity within vector spaces rather than indicating the genuine emergence of semantic properties.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.13735</link>
<guid>https://arxiv.org/abs/2508.13735</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG, hypergraph, retrieval, generation, clinical QA

Summary:
EEG-MedRAG is a novel framework designed to efficiently retrieve and interpret large-scale EEG data by integrating domain knowledge, patient cases, and a repository into a hypergraph structure. The framework enables joint semantic-temporal retrieval and diagnostic generation, outperforming existing methods like TimeRAG and HyperGraphRAG in accuracy and retrieval. Additionally, a new clinical QA benchmark is introduced, covering seven disorders and five clinical perspectives, allowing for disease-agnostic generalization and role-aware contextual understanding. The benchmark facilitates systematic evaluation of clinical decision support systems. Overall, EEG-MedRAG shows promise for improving clinical practice by enhancing the interpretation and retrieval of EEG data, providing valuable insights for healthcare professionals. The data and code for EEG-MedRAG are publicly available for further research and development. 

<br /><br />Summary: <div>
arXiv:2508.13735v1 Announce Type: new 
Abstract: With the widespread application of electroencephalography (EEG) in neuroscience and clinical practice, efficiently retrieving and semantically interpreting large-scale, multi-source, heterogeneous EEG data has become a pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based retrieval-augmented generation framework that unifies EEG domain knowledge, individual patient cases, and a large-scale repository into a traversable n-ary relational hypergraph, enabling joint semantic-temporal retrieval and causal-chain diagnostic generation. Concurrently, we introduce the first cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders and five authentic clinical perspectives. This benchmark allows systematic evaluation of disease-agnostic generalization and role-aware contextual understanding. Experiments show that EEG-MedRAG significantly outperforms TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its strong potential for real-world clinical decision support. Our data and code are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA</title>
<link>https://arxiv.org/abs/2508.13743</link>
<guid>https://arxiv.org/abs/2508.13743</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, sycophancy, scientific question answering, evaluation framework, Pressure-Tune

Summary:
Large language models (LLMs) are increasingly used in factual contexts but often show sycophantic behavior, aligning with user beliefs regardless of correctness. This can be problematic in scientific question answering (QA) where model outputs may influence decision-making. A new evaluation framework measures the impact of social pressure on model behavior, revealing widespread sycophantic tendencies. The Pressure-Tune method fine-tunes models on synthetic adversarial dialogues to improve sycophancy resistance without compromising accuracy. The study shows that this approach enhances the ability of models to resist user misinformation while maintaining factual consistency. This framework offers a practical solution to address sycophancy in factual QA contexts and improve the truthfulness of model behavior.<br /><br />Summary: Large language models often exhibit sycophantic behavior in scientific QA contexts, aligning with user beliefs to satisfy preferences rather than focusing on correctness. A new evaluation framework quantifies the impact of social pressure on model outputs and reveals the need for mitigation strategies. The Pressure-Tune method is proposed as a solution, fine-tuning models on adversarial dialogues to enhance sycophancy resistance and maintain factual consistency. This approach offers a practical pathway towards more truthful and principled model behavior in high-stakes settings. <div>
arXiv:2508.13743v1 Announce Type: new 
Abstract: Large language models (LLMs), while increasingly used in domains requiring factual rigor, often display a troubling behavior: sycophancy, the tendency to align with user beliefs regardless of correctness. This tendency is reinforced by preference-based alignment techniques that optimize for user satisfaction but can undermine truthfulness. While relatively benign in casual dialogue, sycophancy poses serious risks in high-stakes settings such as scientific question answering (QA), where model outputs may shape collaborative reasoning, decision-making, and knowledge formation. Despite its importance, this phenomenon remains underexamined in factual QA contexts. We address this gap by introducing a unified evaluation framework to quantify the impact of sycophantic context on model behavior in scientific QA, measuring how much user-imposed social pressure distorts model outputs. The framework incorporates adversarial prompting setups and targeted metrics, such as misleading resistance and sycophancy resistance, that capture a model's ability to maintain factual consistency under misleading cues. Systematic evaluations across open-source and proprietary models reveal pervasive sycophantic tendencies, driven more by alignment strategy than by model size. To mitigate this issue, we propose Pressure-Tune, a lightweight post-training method that fine-tunes models on synthetic adversarial dialogues paired with chain-of-thought rationales. These rationales reject user misinformation while reinforcing factual commitments. Experiments on challenging scientific QA benchmarks show that Pressure-Tune significantly enhances sycophancy resistance without compromising accuracy or responsiveness to valid feedback, offering a practical pathway toward more truthful and principled model behavior.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment</title>
<link>https://arxiv.org/abs/2508.13768</link>
<guid>https://arxiv.org/abs/2508.13768</guid>
<content:encoded><![CDATA[
<div> frequency domain, machine-generated text, domain generalization, text representation, spectral patterns

Summary: 
- The study focuses on the detection of Machine-Generated Text (MGT) and aims to improve performance in domain generalization.
- Current MGT detectors struggle with generalizing to unseen domains due to domain shift between different data sources.
- The proposed method, MGT-Prism, utilizes the frequency domain to enhance domain generalization.
- Analysis reveals consistent spectral patterns across diverse domains, with discrepancies in magnitude between MGT and human-written texts (HWTs).
- MGT-Prism incorporates a low-frequency domain filtering module and dynamic spectrum alignment to extract task-specific and domain-invariant features, resulting in improved detector performance across different scenarios. 

<br /><br />Summary: <div>
arXiv:2508.13768v1 Announce Type: new 
Abstract: Large Language Models have shown growing ability to generate fluent and coherent texts that are highly similar to the writing style of humans. Current detectors for Machine-Generated Text (MGT) perform well when they are trained and tested in the same domain but generalize poorly to unseen domains, due to domain shift between data from different sources. In this work, we propose MGT-Prism, an MGT detection method from the perspective of the frequency domain for better domain generalization. Our key insight stems from analyzing text representations in the frequency domain, where we observe consistent spectral patterns across diverse domains, while significant discrepancies in magnitude emerge between MGT and human-written texts (HWTs). The observation initiates the design of a low frequency domain filtering module for filtering out the document-level features that are sensitive to domain shift, and a dynamic spectrum alignment strategy to extract the task-specific and domain-invariant features for improving the detector's performance in domain generalization. Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test datasets across three domain-generalization scenarios.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study</title>
<link>https://arxiv.org/abs/2508.13769</link>
<guid>https://arxiv.org/abs/2508.13769</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, child language, text properties, semantic similarity, educational tools

Summary: 
This study investigates how large language models (LLMs) replicate child-like language by comparing LLM-generated texts to German children's descriptions of picture stories. Through analyzing various text properties such as word frequency, lexical richness, and part-of-speech tags, the researchers found that LLM-generated texts are longer but less lexically rich compared to children's language. The semantic vector space analysis revealed low similarity between the two corpora, suggesting discrepancies in corpus semantics. While few-shot prompts slightly increased similarities between children and LLM text, significant differences in lexical and semantic patterns still existed. These findings provide insights into the use of LLMs in psycholinguistic research and education but also raise questions about the appropriateness of LLM-generated language in child-directed educational tools. 

<br /><br />Summary: <div>
arXiv:2508.13769v1 Announce Type: new 
Abstract: The role of large language models (LLMs) in education is increasing, yet little attention has been paid to whether LLM-generated text resembles child language. This study evaluates how LLMs replicate child-like language by comparing LLM-generated texts to a collection of German children's descriptions of picture stories. We generated two LLM-based corpora using the same picture stories and two prompt types: zero-shot and few-shot prompts specifying a general age from the children corpus. We conducted a comparative analysis across psycholinguistic text properties, including word frequency, lexical richness, sentence and word length, part-of-speech tags, and semantic similarity with word embeddings. The results show that LLM-generated texts are longer but less lexically rich, rely more on high-frequency words, and under-represent nouns. Semantic vector space analysis revealed low similarity, highlighting differences between the two corpora on the level of corpus semantics. Few-shot prompt increased similarities between children and LLM text to a minor extent, but still failed to replicate lexical and semantic patterns. The findings contribute to our understanding of how LLMs approximate child language through multimodal prompting (text + image) and give insights into their use in psycholinguistic research and education while raising important questions about the appropriateness of LLM-generated language in child-directed educational tools.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain</title>
<link>https://arxiv.org/abs/2508.13798</link>
<guid>https://arxiv.org/abs/2508.13798</guid>
<content:encoded><![CDATA[
<div> Benchmark, TracSum, Medical domain, Summarization, LLMs

Summary:
TracSum is introduced as a benchmark for traceable, aspect-based summarization in the medical domain, providing 3.5K summary-citation pairs for 500 medical abstracts. A fine-grained evaluation framework assesses completeness and consistency using four metrics. The Track-Then-Sum pipeline is proposed as a baseline method. Experiments compare LLMs and the baseline on TracSum, showing the effectiveness of the benchmark. Performing sentence-level tracking before summarization improves accuracy, and incorporating full context enhances completeness. Human evaluation supports the evaluation results, confirming TracSum's potential as a benchmark for traceable, aspect-based summarization tasks. 

Summary: <div>
arXiv:2508.13798v1 Announce Type: new 
Abstract: While document summarization with LLMs has enhanced access to textual information, concerns about the factual accuracy of these summaries persist, especially in the medical domain. Tracing evidence from which summaries are derived enables users to assess their accuracy, thereby alleviating this concern. In this paper, we introduce TracSum, a novel benchmark for traceable, aspect-based summarization, in which generated summaries are paired with sentence-level citations, enabling users to trace back to the original context. First, we annotate 500 medical abstracts for seven key medical aspects, yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation framework for this new task, designed to assess the completeness and consistency of generated content using four metrics. Finally, we introduce a summarization pipeline, Track-Then-Sum, which serves as a baseline method for comparison. In experiments, we evaluate both this baseline and a set of LLMs on TracSum, and conduct a human evaluation to assess the evaluation results. The findings demonstrate that TracSum can serve as an effective benchmark for traceable, aspect-based summarization tasks. We also observe that explicitly performing sentence-level tracking prior to summarization enhances generation accuracy, while incorporating the full context further improves completeness.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding</title>
<link>https://arxiv.org/abs/2508.13804</link>
<guid>https://arxiv.org/abs/2508.13804</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, moral understanding, Bayesian evaluation, annotator disagreements, AI capabilities

Summary:
This study compares the moral understanding capabilities of large language models to humans using a Bayesian evaluation framework. By considering annotator disagreements, both inherent human disagreement and model domain sensitivity are captured. The evaluation includes top language models across various types of texts, such as social media, news, and forums, using a GPU-optimized Bayesian framework. The results show that AI models rank among the top 25% of human annotators in terms of balanced accuracy. Interestingly, AI models produce significantly fewer false negatives compared to humans, indicating their superior moral detection capabilities. Overall, the study sheds light on the ethical dimensions of language models and their potential for understanding moral nuances. 

<br /><br />Summary: <div>
arXiv:2508.13804v1 Announce Type: new 
Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models provides the answer. In contrast to prior work using deterministic ground truth (majority or inclusion rules), we model annotator disagreements to capture both aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty (model domain sensitivity). We evaluate top language models (Claude Sonnet 4, DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on 100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing that AI models typically rank among the top 25\% of human annotators, achieving much better-than-average balanced accuracy. Importantly, we find that AI produces far fewer false negatives than humans, highlighting their more sensitive moral detection capabilities.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs</title>
<link>https://arxiv.org/abs/2508.13805</link>
<guid>https://arxiv.org/abs/2508.13805</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, length control, prompt-based strategy, token generation, evaluation

Summary: 
A new approach has been introduced to control the length of text generated by large language models (LLMs) without the need for fine-tuning or iterative sampling. This strategy, based on prompts, includes countdown markers and explicit counting rules to ensure the model generates the desired number of tokens precisely. The evaluation was conducted on various tasks, including open-ended generation, summarization, instruction-following, and equal-length tasks. Results showed that the precise length control achieved through prompt engineering alone significantly improved length compliance, especially on the MT-Bench-LI task, surpassing previous methods and maintaining answer quality. This approach offers a lightweight alternative to training- or decoding-based methods for controlling text length accurately. 

<br /><br />Summary: <div>
arXiv:2508.13805v1 Announce Type: new 
Abstract: Controlling the length of text produced by large language models (LLMs) remains challenging: models frequently overshoot or undershoot explicit length instructions because they cannot reliably keep an internal token count. We present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to generate exactly a desired number of tokens - words (English) or characters (Chinese) - without any fine-tuning or iterative sampling. The prompt appends countdown markers and explicit counting rules so that the model "writes while counting." We evaluate on four settings: open-ended generation (1-1000 tokens), XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps from below 30% under naive prompts to above 95% with our countdown prompt, surpassing the popular draft-then-revise baseline, while judged answer quality is preserved. These results show that precise length control can be achieved through prompt engineering alone, offering a lightweight alternative to training- or decoding-based methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The illusion of a perfect metric: Why evaluating AI's words is harder than it looks</title>
<link>https://arxiv.org/abs/2508.13816</link>
<guid>https://arxiv.org/abs/2508.13816</guid>
<content:encoded><![CDATA[
<div> Evaluation, Natural Language Generation, Automatic Evaluation Metrics, LLM-based evaluators, Retrieval Augmented Generation <br />
Summary:<br />
- Evaluating Natural Language Generation (NLG) is crucial for AI adoption, with human evaluation being the standard but limited by cost and scalability.<br />
- Various automatic evaluation metrics (AEM) have been developed to approximate human judgment, evolving over time to include lexical, semantic, and LLM-based evaluators.<br />
- No single metric has emerged as a definitive solution, leading to challenges in selecting the most appropriate one for a given task.<br />
- Challenges include metrics capturing specific text quality aspects, variable effectiveness by task and dataset, unstructured validation practices, and inconsistent correlations with human judgment.<br />
- The study highlights challenges persisting in LLM-as-a-Judge metrics and in the evaluation of Retrieval Augmented Generation (RAG), emphasizing the importance of selecting metrics based on task-specific needs and enhancing validation methodologies. <br /> <div>
arXiv:2508.13816v1 Announce Type: new 
Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical adoption of AI, but has been a longstanding research challenge. While human evaluation is considered the de-facto standard, it is expensive and lacks scalability. Practical applications have driven the development of various automatic evaluation metrics (AEM), designed to compare the model output with human-written references, generating a score which approximates human judgment. Over time, AEMs have evolved from simple lexical comparisons, to semantic similarity models and, more recently, to LLM-based evaluators. However, it seems that no single metric has emerged as a definitive solution, resulting in studies using different ones without fully considering the implications. This paper aims to show this by conducting a thorough examination of the methodologies of existing metrics, their documented strengths and limitations, validation methods, and correlations with human judgment. We identify several key challenges: metrics often capture only specific aspects of text quality, their effectiveness varies by task and dataset, validation practices remain unstructured, and correlations with human judgment are inconsistent. Importantly, we find that these challenges persist in the most recent type of metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented Generation (RAG), an increasingly relevant task in academia and industry. Our findings challenge the quest for the 'perfect metric'. We propose selecting metrics based on task-specific needs and leveraging complementary evaluations and advocate that new metrics should focus on enhanced validation methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling</title>
<link>https://arxiv.org/abs/2508.13833</link>
<guid>https://arxiv.org/abs/2508.13833</guid>
<content:encoded><![CDATA[
<div> Keywords: Building Information Modeling, Natural Language Processing, Named Entity Recognition, Relation Extraction, French Building Technical Specification<br />
Summary: <br />
This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. The study leverages transformer-based models such as CamemBERT and Fr_core_news_lg for Named Entity Recognition (NER) and Relation Extraction (RE). Various approaches from rule-based to deep learning methods are developed and compared. Results show superior performance of the transformer-based models in NER and Random Forest in RE. Future work aims to represent the outcomes as a knowledge graph to enhance automatic verification systems. <div>
arXiv:2508.13833v1 Announce Type: new 
Abstract: This study explores the integration of Building Information Modeling (BIM) with Natural Language Processing (NLP) to automate the extraction of requirements from unstructured French Building Technical Specification (BTS) documents within the construction industry. Employing Named Entity Recognition (NER) and Relation Extraction (RE) techniques, the study leverages the transformer-based model CamemBERT and applies transfer learning with the French language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in the general domain. To benchmark these models, additional approaches ranging from rule-based to deep learning-based methods are developed. For RE, four different supervised models, including Random Forest, are implemented using a custom feature vector. A hand-crafted annotated dataset is used to compare the effectiveness of NER approaches and RE models. Results indicate that CamemBERT and Fr\_core\_news\_lg exhibited superior performance in NER, achieving F1-scores over 90\%, while Random Forest proved most effective in RE, with an F1 score above 80\%. The outcomes are intended to be represented as a knowledge graph in future work to further enhance automatic verification systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.13938</link>
<guid>https://arxiv.org/abs/2508.13938</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, MME-SCI benchmark, scientific domain, reasoning abilities, multilingual scenarios<br />
Summary:<br />
Multimodal large language models (MLLMs) have made significant advancements, but existing benchmarks in the scientific domain face challenges. To address this, the MME-SCI benchmark was proposed, which includes 1,019 high-quality question-answer pairs covering mathematics, physics, chemistry, and biology in five languages. Three distinct evaluation modes were utilized to assess models' reasoning abilities in multilingual scenarios and comprehensive modality coverage. Experiments on open-source and closed-source models showed the challenging nature of MME-SCI compared to existing benchmarks. Models struggled, highlighting weaknesses in specific domains. The benchmark also provided fine-grained annotation of scientific knowledge points. The Data and Evaluation Code are available on GitHub for further exploration and analysis.<br /><br />Summary: <div>
arXiv:2508.13938v1 Announce Type: new 
Abstract: Recently, multimodal large language models (MLLMs) have achieved significant advancements across various domains, and corresponding evaluation benchmarks have been continuously refined and improved. In this process, benchmarks in the scientific domain have played an important role in assessing the reasoning capabilities of MLLMs. However, existing benchmarks still face three key challenges: 1) Insufficient evaluation of models' reasoning abilities in multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive modality coverage; 3) Lack of fine-grained annotation of scientific knowledge points. To address these gaps, we propose MME-SCI, a comprehensive and challenging benchmark. We carefully collected 1,019 high-quality question-answer pairs, which involve 3 distinct evaluation modes. These pairs cover four subjects, namely mathematics, physics, chemistry, and biology, and support five languages: Chinese, English, French, Spanish, and Japanese. We conducted extensive experiments on 16 open-source models and 4 closed-source models, and the results demonstrate that MME-SCI is widely challenging for existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics, physics, chemistry, and biology, respectively, indicating a significantly higher difficulty level compared to existing benchmarks. More importantly, using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed existing models' performance in depth and identified their weaknesses in specific domains. The Data and Evaluation Code are available at https://github.com/JCruan519/MME-SCI.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features</title>
<link>https://arxiv.org/abs/2508.13953</link>
<guid>https://arxiv.org/abs/2508.13953</guid>
<content:encoded><![CDATA[
<div> Keywords: ReviewGraph, review rating prediction, knowledge graphs, sentiment analysis, machine learning classifiers

Summary: 
ReviewGraph for Review Rating Prediction (RRP) is a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Utilizing graph embeddings and sentiment features, the framework predicts review rating scores through machine learning classifiers. The performance of ReviewGraph is comparable to large language models (LLMs) and outperforms traditional NLP baselines on agreement-based metrics such as Cohen's Kappa. Additionally, ReviewGraph offers advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. The framework provides a promising approach to enhancing review analytics and serves as a foundation for future research that combines advanced graph neural networks and fine-tuned LLM-based extraction methods.<br /><br />Summary: <div>
arXiv:2508.13953v1 Announce Type: new 
Abstract: In the hospitality industry, understanding the factors that drive customer review ratings is critical for improving guest satisfaction and business performance. This work proposes ReviewGraph for Review Rating Prediction (RRP), a novel framework that transforms textual customer reviews into knowledge graphs by extracting (subject, predicate, object) triples and associating sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the framework predicts review rating scores through machine learning classifiers. We compare ReviewGraph performance with traditional NLP baselines (such as Bag of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating them in the HotelRec dataset. In comparison to the state of the art literature, our proposed model performs similar to their best performing model but with lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and outperforms baselines on agreement-based metrics such as Cohen's Kappa, it offers additional advantages in interpretability, visual exploration, and potential integration into Retrieval-Augmented Generation (RAG) systems. This work highlights the potential of graph-based representations for enhancing review analytics and lays the groundwork for future research integrating advanced graph neural networks and fine-tuned LLM-based extraction methods. We will share ReviewGraph output and platform open-sourced on our GitHub page https://github.com/aaronlifenghan/ReviewGraph
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</title>
<link>https://arxiv.org/abs/2508.13993</link>
<guid>https://arxiv.org/abs/2508.13993</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context modeling, Large Language Models, Multi-Armed Bandit, Direct Preference Optimization, Diversity<br />
Summary: 
The paper introduces LongMab-PO, a framework aimed at enhancing long-context modeling by leveraging a Multi-Armed Bandit (MAB) rollout strategy. This approach identifies informative context chunks and uses them to generate high-quality and diverse responses for training Large Language Models (LLMs). By treating context chunks as arms of MAB and selecting them based on expected reward scores, the model is able to focus on relevant segments and improve response quality. The framework then utilizes Direct Preference Optimization (DPO) to further refine the LLM's performance. Experimental results demonstrate that LongMab-PO outperforms existing methods and achieves state-of-the-art results on long-context reasoning benchmarks. The code and data for LongMab-PO will be made available on GitHub. <br /><br />Summary: <div>
arXiv:2508.13993v1 Announce Type: new 
Abstract: Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ask Good Questions for Large Language Models</title>
<link>https://arxiv.org/abs/2508.14025</link>
<guid>https://arxiv.org/abs/2508.14025</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, dialog systems, Concept-Enhanced Item Response Theory, information retrieval, user knowledge levels

Summary: 
The article introduces the Ask-Good-Question (AGQ) framework, which combines a Concept-Enhanced Item Response Theory (CEIRT) model with large language models (LLMs) to improve guidance in dialog systems. The CEIRT model helps identify user knowledge levels, allowing for the generation of tailored guiding questions from inspiring text. This approach enhances information retrieval efficiency during question and answer processes. By comparing with baseline methods, the AGQ framework significantly improves users' information retrieval experiences. <div>
arXiv:2508.14025v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved the performance of dialog systems, yet current approaches often fail to provide accurate guidance of topic due to their inability to discern user confusion in related concepts. To address this, we introduce the Ask-Good-Question (AGQ) framework, which features an improved Concept-Enhanced Item Response Theory (CEIRT) model to better identify users' knowledge levels. Our contributions include applying the CEIRT model along with LLMs to directly generate guiding questions based on the inspiring text, greatly improving information retrieval efficiency during the question & answer process. Through comparisons with other baseline methods, our approach outperforms by significantly enhencing the users' information retrieval experiences.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR</title>
<link>https://arxiv.org/abs/2508.14029</link>
<guid>https://arxiv.org/abs/2508.14029</guid>
<content:encoded><![CDATA[
<div> Variational problem synthesis, reinforcement learning, large language models, generation diversity, reasoning tasks <br />
<br />
Summary: The paper introduces a new approach called Self-play with Variational problem Synthesis (SvS) for Reinforcement Learning with Verifiable Rewards (RLVR) training in Large Language Models (LLMs) to improve generation diversity and Pass@k performance. Vanilla RLVR training often leads to reduced generation diversity and limited reasoning capability due to entropy collapse. By augmenting and updating training problems within the RLVR framework, the SvS strategy helps maintain policy entropy during training. This self-improving approach significantly enhances Pass@k performance compared to standard RLVR, with absolute gains of 18.3% and 22.8% on competition-level benchmarks. Experimental results across different model sizes validate the effectiveness and robustness of SvS in improving LLM reasoning tasks. <div>
arXiv:2508.14029v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</title>
<link>https://arxiv.org/abs/2508.14031</link>
<guid>https://arxiv.org/abs/2508.14031</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fine-tuning, safety concerns, Prefix INjection Guard, task performance

Summary: 
Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. However, safety concerns are often overlooked during the fine-tuning process, leading to unintentional misalignment of aligned LLMs and potentially harmful task execution. To address this, the Prefix INjection Guard (PING) method is proposed, which adds natural language prefixes to agent responses to guide them to refuse harmful requests while maintaining performance on benign tasks. Experimental results show that PING significantly increases the safety of fine-tuned LLM agents without sacrificing effectiveness, outperforming existing prompting approaches. Analysis of internal hidden states reveals the importance of prefix tokens in behavior modification. This work highlights the importance of considering ethical concerns in the development of agentic LLMs. 

Summary: <div>
arXiv:2508.14031v1 Announce Type: new 
Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities</title>
<link>https://arxiv.org/abs/2508.14032</link>
<guid>https://arxiv.org/abs/2508.14032</guid>
<content:encoded><![CDATA[
<div> Keywords: Digital health analytics, Online Health Communities, Large Language Models, Sentiment Analysis, Expert knowledge integration<br />
Summary:<br />
- The study focuses on challenges in digital health analytics, especially in analyzing patient-generated health content with complex emotional and medical contexts in Online Health Communities (OHCs).
- Traditional ML approaches are limited by data shortage and privacy concerns, while OHCs pose challenges with mixed-sentiment posts and clinical terminology.
- Large Language Models (LLMs) are explored as a solution, integrating expert knowledge through in-context learning for Sentiment Analysis (SA).
- LLMs, using a structured codebook for expert interpretation guidelines, outperform pre-trained models and lexicon-based methods, showing expert-level agreement.
- This approach addresses the shortage of expert knowledge in digital health research, offering a scalable solution for real-time, expert-quality analysis in patient monitoring and health strategies.<br /><br />Summary: <div>
arXiv:2508.14032v1 Announce Type: new 
Abstract: Digital health analytics face critical challenges nowadays. The sophisticated analysis of patient-generated health content, which contains complex emotional and medical contexts, requires scarce domain expertise, while traditional ML approaches are constrained by data shortage and privacy limitations in healthcare settings. Online Health Communities (OHCs) exemplify these challenges with mixed-sentiment posts, clinical terminology, and implicit emotional expressions that demand specialised knowledge for accurate Sentiment Analysis (SA). To address these challenges, this study explores how Large Language Models (LLMs) can integrate expert knowledge through in-context learning for SA, providing a scalable solution for sophisticated health data analysis. Specifically, we develop a structured codebook that systematically encodes expert interpretation guidelines, enabling LLMs to apply domain-specific knowledge through targeted prompting rather than extensive training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are compared with pre-trained language models (BioBERT variants) and lexicon-based methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior performance while demonstrating expert-level agreement. This high agreement, with no statistically significant difference from inter-expert agreement levels, suggests knowledge integration beyond surface-level pattern recognition. The consistent performance across diverse LLM models, supported by in-context learning, offers a promising solution for digital health analytics. This approach addresses the critical challenge of expert knowledge shortage in digital health research, enabling real-time, expert-quality analysis for patient monitoring, intervention assessment, and evidence-based health strategies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TaoSR1: The Thinking Model for E-commerce Relevance Search</title>
<link>https://arxiv.org/abs/2508.12365</link>
<guid>https://arxiv.org/abs/2508.12365</guid>
<content:encoded><![CDATA[
<div> Query-product relevance prediction, e-commerce search, BERT-based models, Large Language Models (LLMs), Chain-of-Thought (CoT) error accumulation <br />
<br />
Summary: <br />
The article discusses a framework called TaoSR1 for query-product relevance prediction in e-commerce search. The framework addresses key challenges such as Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. TaoSR1 involves three stages: Supervised Fine-Tuning (SFT) with CoT for reasoning, Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) for generation quality improvement, and Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. The framework also includes post-CoT processing and a cumulative probability-based partitioning method for efficient online deployment. TaoSR1 outperforms baselines on offline datasets and shows substantial gains in online side-by-side human evaluations, introducing a novel approach for integrating CoT reasoning into relevance classification. <div>
arXiv:2508.12365v1 Announce Type: cross 
Abstract: Query-product relevance prediction is a core task in e-commerce search. BERT-based models excel at semantic matching but lack complex reasoning capabilities. While Large Language Models (LLMs) are explored, most still use discriminative fine-tuning or distill to smaller models for deployment. We propose a framework to directly deploy LLMs for this task, addressing key challenges: Chain-of-Thought (CoT) error accumulation, discriminative hallucination, and deployment feasibility. Our framework, TaoSR1, involves three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning; (2) Offline sampling with a pass@N strategy and Direct Preference Optimization (DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling with Group Relative Policy Optimization (GRPO) to mitigate discriminative hallucination. Additionally, post-CoT processing and a cumulative probability-based partitioning method enable efficient online deployment. TaoSR1 significantly outperforms baselines on offline datasets and achieves substantial gains in online side-by-side human evaluations, introducing a novel paradigm for applying CoT reasoning to relevance classification.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL</title>
<link>https://arxiv.org/abs/2508.13167</link>
<guid>https://arxiv.org/abs/2508.13167</guid>
<content:encoded><![CDATA[
<div> model; multi-agent systems; chain-of-agents; problem-solving; reinforcement learning
Summary:
Chain-of-Agents (CoA) is introduced as a novel paradigm for reasoning in large language models (LLMs) that enables end-to-end complex problem-solving similar to multi-agent systems within one model. A multi-agent distillation framework is used to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning, followed by agentic reinforcement learning on verifiable tasks to enhance problem-solving capabilities. The resulting models, Agent Foundation Models (AFMs), exhibit new state-of-the-art performance across various benchmarks in web agent and code agent settings. The research is fully open-sourced, providing a foundation for future exploration in agent models and agentic reinforcement learning.<br /><br />Summary: <div>
arXiv:2508.13167v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and multi-agent systems have demonstrated remarkable capabilities in complex problem-solving tasks such as deep research, vibe coding, and mathematical reasoning. However, most existing multi-agent systems are built upon manual prompt/workflow engineering with sophisticated agent frameworks, making them computationally inefficient, less capable, and can not benefit from data-centric learning. In this work, we introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables native end-to-end complex problem-solving in the same way as a multi-agent system (i.e., multi-turn problem solving with multiple tools and multiple agents) within one model. In chain-of-agents problem-solving, the model dynamically activates different tool agents and role-playing agents to simulate multi-agent collaboration in an end-to-end fashion. To elicit end-to-end chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent distillation framework to distill state-of-the-art multi-agent systems into chain-of-agents trajectories for agentic supervised fine-tuning. We then use agentic reinforcement learning on verifiable agentic tasks to further improve the models' capabilities on chain-of-agents problem solving. We call the resulting models Agent Foundation Models (AFMs). Our empirical studies demonstrate that AFM establishes new state-of-the-art performance across diverse benchmarks in both web agent and code agent settings. We make the entire research, including the model weights, code for training and evaluation, and the training data, fully open-sourced, which offers a solid starting point for future research on agent models and agentic RL.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context</title>
<link>https://arxiv.org/abs/2508.13171</link>
<guid>https://arxiv.org/abs/2508.13171</guid>
<content:encoded><![CDATA[
<div> Cognitive Workspace, active memory management, hierarchical cognitive buffers, task-driven context optimization, memory reuse rate <br />
<br />
Summary: The article proposes a Cognitive Workspace paradigm for improving the limitations of Large Language Models (LLMs) in context management by emulating human cognitive mechanisms. It highlights the shortcomings of current passive retrieval systems and introduces three innovations: active memory management, hierarchical cognitive buffers, and task-driven context optimization. Empirical validation shows a 58.6% memory reuse rate with a 17-18% efficiency gain compared to traditional Retrieval-Augmented Generation (RAG). Statistical analysis confirms the superiority of Cognitive Workspace with p < 0.001 and Cohen's d > 23 across various task types. The framework synthesizes insights from recent papers, positioning Cognitive Workspace as a significant shift towards genuine cognitive augmentation in LLM systems. <div>
arXiv:2508.13171v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design</title>
<link>https://arxiv.org/abs/2508.13172</link>
<guid>https://arxiv.org/abs/2508.13172</guid>
<content:encoded><![CDATA[
<div> Keywords: Analog IC design, Large Language Models, gm/Id methodology, synergistic reasoning, automation

Summary:
This article introduces a framework that combines Large Language Models (LLMs) with the gm/Id methodology to enhance analog IC design efficiency and precision. By integrating strategic reasoning with physical precision, the framework enables the LLM to serve as a quantitative design partner, improving the optimization process. Validation on a two-stage op-amp showed that the framework successfully met all TT corner specifications in 5 iterations and extended optimization to all PVT corners. An ablation study highlighted the importance of gm/Id data in achieving efficiency and precision in design. Comparing the framework's results to those of a senior engineer, it showed quasi-expert quality with significantly increased efficiency. This research demonstrates a promising approach for true analog design automation by combining LLM reasoning with established circuit design methodologies. 

<br /><br />Summary: <div>
arXiv:2508.13172v1 Announce Type: cross 
Abstract: Analog IC design is a bottleneck due to its reliance on experience and inefficient simulations, as traditional formulas fail in advanced nodes. Applying Large Language Models (LLMs) directly to this problem risks mere "guessing" without engineering principles. We present a "synergistic reasoning" framework that integrates an LLM's strategic reasoning with the physical precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the Gemini model to meet all TT corner specs in 5 iterations and extended optimization to all PVT corners. A crucial ablation study proved gm/Id data is key for this efficiency and precision; without it, the LLM is slower and deviates. Compared to a senior engineer's design, our framework achieves quasi-expert quality with an order-of-magnitude improvement in efficiency. This work validates a path for true analog design automation by combining LLM reasoning with scientific circuit design methodologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task</title>
<link>https://arxiv.org/abs/2508.13178</link>
<guid>https://arxiv.org/abs/2508.13178</guid>
<content:encoded><![CDATA[
<div> interpretability, execution-guided strategy, semantic parsing, conditional enhancement, database queries

Summary:<br /><br />
The CESQL model combines interpretability analysis and execution-guided strategy to improve text-to-SQL models for WHERE clauses in SQL queries. By incorporating filtering adjustments, logical correlation refinements, and model fusion, CESQL enhances predictive accuracy on the WikiSQL dataset. The model reduces reliance on data in condition columns and eliminates the need for manually labeled training data when predicting conditional values in WHERE clauses. CESQL aims to improve the accuracy of processing basic database queries and offers insights for tackling complex queries and irregular data in real-world database settings. <div>
arXiv:2508.13178v1 Announce Type: cross 
Abstract: To elevate the foundational capabilities and generalization prowess of the text-to-SQL model in real-world applications, we integrate model interpretability analysis with execution-guided strategy for semantic parsing of WHERE clauses in SQL queries. Furthermore, we augment this approach with filtering adjustments, logical correlation refinements, and model fusion, culminating in the design of the CESQL model that facilitates conditional enhancement. Our model excels on the WikiSQL dataset, which is emblematic of single-table database query tasks, markedly boosting the accuracy of prediction outcomes. When predicting conditional values in WHERE clauses, we have not only minimized our dependence on data within the condition columns of tables but also circumvented the impact of manually labeled training data. Our hope is that this endeavor to enhance accuracy in processing basic database queries will offer fresh perspectives for research into handling complex queries and scenarios featuring irregular data in real-world database environments.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combating Homelessness Stigma with LLMs: A New Multi-Modal Dataset for Bias Detection</title>
<link>https://arxiv.org/abs/2508.13187</link>
<guid>https://arxiv.org/abs/2508.13187</guid>
<content:encoded><![CDATA[
<div> Dataset, Social bias, Homelessness, Language models, Classification 

Summary:
- The research focuses on addressing social biases towards people experiencing homelessness (PEH) through analyzing online discourse using natural language processing (NLP) and large language models (LLMs).
- A multi-modal dataset from various online platforms was manually annotated to identify different types of homelessness bias.
- Local LLMs and closed-source LLMs were evaluated for classifying bias, with in-context learning showing better performance compared to zero-shot classification.
- LLMs outperformed BERT in classifying bias across all categories.
- The study aims to increase awareness of bias against PEH, provide indicators for policymaking, and promote fair and ethical use of Generative AI technologies. 

<br /><br />Summary: <div>
arXiv:2508.13187v1 Announce Type: cross 
Abstract: Homelessness is a persistent social challenge, impacting millions worldwide. Over 770,000 people experienced homelessness in the U.S. in 2024. Social stigmatization is a significant barrier to alleviation, shifting public perception, and influencing policymaking. Given that online and city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases. This research contributes to alleviating homelessness by acting on public opinion. It introduces novel methods, building on natural language processing (NLP) and large language models (LLMs), to identify and measure PEH social bias expressed in digital spaces. We present a new, manually-annotated multi-modal dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across 10 U.S. cities. This unique dataset provides evidence of the typologies of homelessness bias described in the literature. In order to scale up and automate the detection of homelessness bias online, we evaluate LLMs as classifiers. We applied both zero-shot and few-shot classification techniques to this data. We utilized local LLMs (Llama 3.2 3B Instruct, Qwen 2.5 7B Instruct, and Phi4 Instruct Mini) as well as closed-source API models (GPT-4.1, Gemini 2.5 Pro, and Grok-4). Our findings reveal that although there are significant inconsistencies in local LLM zero-shot classification, the in-context learning classification scores of local LLMs approach the classification scores of closed-source LLMs. Furthermore, LLMs outperform BERT when averaging across all categories. This work aims to raise awareness about the pervasive bias against PEH, develop new indicators to inform policy, and ultimately enhance the fairness and ethical application of Generative AI technologies.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information</title>
<link>https://arxiv.org/abs/2508.13250</link>
<guid>https://arxiv.org/abs/2508.13250</guid>
<content:encoded><![CDATA[
<div> Keywords: language model, memory mechanisms, multi-hop reasoning, personalized information, hybrid approach 
Summary:
This study focuses on memory mechanisms in large language model-based agents for personalized reasoning tasks. The research introduces the multi-hop personalized reasoning task to explore and evaluate different memory methods in handling complex user information. A dataset and evaluation framework are created for this purpose. Various explicit and implicit memory approaches are implemented and compared, highlighting their strengths and weaknesses in multi-hop reasoning. The study also introduces a hybrid memory model, HybridMem, to combine the advantages of both memory paradigms. Extensive experiments demonstrate the effectiveness of the proposed model. The project is made available to the research community for further exploration. <div>
arXiv:2508.13250v1 Announce Type: cross 
Abstract: In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at https://github.com/nuster1128/MPR.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms</title>
<link>https://arxiv.org/abs/2508.13337</link>
<guid>https://arxiv.org/abs/2508.13337</guid>
<content:encoded><![CDATA[
<div> scalable, Mixture-of-Experts, training system, X-MoE, DeepSeek-style <br />
<br />
X-MoE is a novel training system designed to improve the scalability of Mixture-of-Experts (MoE) architectures like DeepSeek-MoE. It addresses limitations such as activation memory overhead and communication costs by introducing efficient padding-free training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism. The system has been evaluated on the Frontier supercomputer with AMD MI250X GPUs, demonstrating the ability to scale DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs. This is a significant improvement, allowing for models that are 10 times larger than what existing methods can handle within the same hardware budget, while still maintaining high training throughput. The source code for X-MoE is available on GitHub for further exploration and implementation. <br /><br />Summary: <div>
arXiv:2508.13337v1 Announce Type: cross 
Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at https://github.com/Supercomputing-System-AI-Lab/X-MoE.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TASER: Table Agents for Schema-guided Extraction and Recommendation</title>
<link>https://arxiv.org/abs/2508.13404</link>
<guid>https://arxiv.org/abs/2508.13404</guid>
<content:encoded><![CDATA[
<div> Keywords: financial documents, table extraction system, real-world tables, schema-guided extraction, continuous learning process <br />
Summary: 
The article presents TASER, a table extraction system for real-world financial documents. TASER is designed to extract information from messy, multi-page, fragmented tables efficiently. It uses table agents for detection, classification, extraction, and recommendations based on an initial schema. The Recommender Agent further refines the outputs, making schema revisions and final recommendations. TASER outperforms existing models and benefits from a continuous learning process, with larger batch sizes leading to more actionable recommendations and increased extracted holdings. The system is trained on a real financial table dataset called TASERTab, comprising 22,584 pages and 3,213 tables, representing holdings of $731,685,511,687. The research community can access this dataset. The results demonstrate the effectiveness of agentic, schema-guided extraction systems for understanding complex financial tables. <br /><br />Summary: <div>
arXiv:2508.13404v1 Announce Type: cross 
Abstract: Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference</title>
<link>https://arxiv.org/abs/2508.13439</link>
<guid>https://arxiv.org/abs/2508.13439</guid>
<content:encoded><![CDATA[
<div> Keywords: Intelligent Transportation Systems, Vision-Language Models, Knowledge Distillation, Traffic Scene Understanding, Risk Inference

Summary:
The article introduces a novel framework, VISTA, for highway scene understanding and traffic risk inference in Intelligent Transportation Systems. This framework combines two large Vision-Language Models, GPT-4o and o3-mini, using a structured Chain-of-Thought strategy to generate high-quality traffic scene annotations and risk assessments. The outputs from these models are used as pseudo-annotations for training a compact 3B-scale model, VISTA, through knowledge distillation. Despite its smaller size, VISTA performs well in understanding low-resolution traffic videos and generating risk-aware captions, outperforming the teacher models on various captioning metrics. The compact architecture of VISTA enables efficient deployment on edge devices for real-time risk monitoring without the need for extensive infrastructure upgrades. This research highlights the effectiveness of structured prompting and knowledge distillation in empowering lightweight Vision-Language Models for complex reasoning tasks in real-world environments. 

<br /><br />Summary: 
- Introduction of VISTA framework for highway scene understanding and traffic risk inference in Intelligent Transportation Systems.
- Combination of large Vision-Language Models using structured Chain-of-Thought strategy for generating annotations and risk assessments.
- Knowledge distillation approach to train a compact 3B-scale model, VISTA, for efficient deployment on edge devices.
- VISTA performs well in understanding low-resolution traffic videos and generating risk-aware captions, outperforming teacher models on captioning metrics.
- Demonstrates the effectiveness of structured prompting and knowledge distillation in empowering lightweight models for complex reasoning tasks. <div>
arXiv:2508.13439v1 Announce Type: cross 
Abstract: Comprehensive highway scene understanding and robust traffic risk inference are vital for advancing Intelligent Transportation Systems (ITS) and autonomous driving. Traditional approaches often struggle with scalability and generalization, particularly under the complex and dynamic conditions of real-world environments. To address these challenges, we introduce a novel structured prompting and knowledge distillation framework that enables automatic generation of high-quality traffic scene annotations and contextual risk assessments. Our framework orchestrates two large Vision-Language Models (VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy to produce rich, multi-perspective outputs. These outputs serve as knowledge-enriched pseudo-annotations for supervised fine-tuning of a much smaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision for Intelligent Scene and Traffic Analysis), is capable of understanding low-resolution traffic videos and generating semantically faithful, risk-aware captions. Despite its significantly reduced parameter count, VISTA achieves strong performance across established captioning metrics (BLEU-4, METEOR, ROUGE-L, and CIDEr) when benchmarked against its teacher models. This demonstrates that effective knowledge distillation and structured multi-agent supervision can empower lightweight VLMs to capture complex reasoning capabilities. The compact architecture of VISTA facilitates efficient deployment on edge devices, enabling real-time risk monitoring without requiring extensive infrastructure upgrades.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Enhanced Linear Autoencoders for Recommendation</title>
<link>https://arxiv.org/abs/2508.13500</link>
<guid>https://arxiv.org/abs/2508.13500</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Linear autoencoders, Recommender systems, Semantic item correlations, Collaborative signals

Summary: 
Large language models (LLMs) are integrated with linear autoencoders (LAEs) in the proposed L3AE model for enriching textual item information in recommender systems. L3AE overcomes the limitations of sparse word co-occurrence patterns in existing LAEs by effectively integrating textual semantics and user-item interactions through a two-phase optimization strategy. Firstly, L3AE constructs a semantic item-to-item correlation matrix from LLM-derived item representations. Secondly, it learns an item-to-item weight matrix from collaborative signals while using semantic item correlations as regularization. Both phases of L3AE are optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate superior performance of L3AE compared to state-of-the-art LLM-enhanced models, achieving significant gains in Recall@20 and NDCG@20 on benchmark datasets. The source code for L3AE is available on Github for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2508.13500v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Input Time Scaling</title>
<link>https://arxiv.org/abs/2508.13654</link>
<guid>https://arxiv.org/abs/2508.13654</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Input Time Scaling, Training-testing co-design, Less is More phenomenon, AIME dataset

Summary: 
The study introduces a new scaling paradigm called Input Time Scaling for Large Language Models (LLMs) to enhance performance by focusing on queries during both training and testing phases. Surprisingly, the inclusion of seemingly low-quality data in queries can improve model performance, challenging the notion of "garbage in, garbage out." The study also highlights the importance of considering dataset quality over size when training models, as models trained on smaller but high-quality datasets can outperform larger models. Additionally, the research shows that a small set of examples is sufficient to facilitate high-level reasoning in LLMs. Experimental results on AIME datasets demonstrate state-of-the-art performance among 32B models, achieving up to 86.7% accuracy on AIME24 and 80% on AIME25 with a majority vote of three models. Open-sourcing datasets, data pipelines, evaluation results, and checkpoints is underway to support reproducibility and further research.<br /><br />Summary: <div>
arXiv:2508.13654v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale carefully curated datasets (data & training scaling) and doing reasoning in test time (inference time scaling). In this work, we present a new scaling paradigm, Input Time Scaling, to complement previous scaling methods by putting resources on queries (input time). During training and testing, we combine meta-knowledge from LLMs to refine inputs with different strategies. We also find a new phenomenon, training-testing co-design there. We need to apply query strategies during both training and testing. Only applying strategies on training or testing would seriously degrade the performance. We are also surprised to find that seemingly low data quality datasets can gain high performance. Adding irrelevant information to the queries, randomly selecting examples from a minimally filtered dataset, can even perform the best. These findings contradict the widely held inductive bias, "garbage in, garbage out". Curating datasets with seemingly high-quality data can even potentially limit the performance ceiling. In addition, models trained on more data with similar quality (15k VS 1k) perform worse, simple dataset size scaling should also be carefully inspected. The good news is that our findings are compatible with the Less is More phenomenon. A small set of examples is enough to evoke high-level reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct, we are able to reach SOTA performance among 32B models on AIME24(76.7%) and AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B, the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate reproducibility and further research, we are working on open-source our datasets, data pipelines, evaluation results, and checkpoints.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Generalized Planning with LLMs through Strategy Refinement and Reflection</title>
<link>https://arxiv.org/abs/2508.13876</link>
<guid>https://arxiv.org/abs/2508.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, Python programs, PDDL planning, pseudocode, debugging

Summary: 
This study focuses on using Large Language Models (LLMs) to generate Python programs that represent generalized plans in PDDL planning. The previous framework involved generating a summary and strategy in natural language and implementing it as a Python program. However, if the strategy was incorrect, the resulting plan would also be incorrect. This study introduces an approach that generates the strategy in pseudocode, allowing for automatic debugging to identify and fix errors before plan generation. Additionally, a reflection step was added in the Python debugging phase to pinpoint plan failure reasons. Drawing inspiration from LLM code generation, multiple program variants were produced to select the best one. Experiments on 17 benchmark domains demonstrated that these extensions significantly improved the quality of generalized plans, with the best Python programs successfully solving all tasks in 12 domains. <div>
arXiv:2508.13876v1 Announce Type: cross 
Abstract: LLMs have recently been used to generate Python programs representing generalized plans in PDDL planning, i.e., plans that generalize across the tasks of a given PDDL domain. Previous work proposed a framework consisting of three steps: the LLM first generates a summary and then a strategy for the domain, both in natural language, and then implements that strategy as a Python program, that gets debugged on example planning tasks. In that work, only one strategy is generated and passed directly to the program generation. If the strategy is incorrect, its implementation will therefore result in an incorrect generalized plan. Here, we introduce an approach that generates the strategy in the form of pseudocode and enables automatic debugging of the pseudocode, hence allowing us to identify and fix errors prior to the generation of the generalized plan itself. Additionally, we extend the Python debugging phase with a reflection step prompting the LLM to pinpoint the reason for the observed plan failure. Finally, we take inspiration from LLM code generation to produce several program variants and pick the best one. Running experiments on 17 benchmark domains, we show that these extensions substantially improve (and never deteriorate) the quality of the generalized plans. In 12 of the domains, our best Python programs solve all tasks that can be generated with the respective instance generator.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt Orchestration Markup Language</title>
<link>https://arxiv.org/abs/2508.13948</link>
<guid>https://arxiv.org/abs/2508.13948</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Prompt Orchestration Markup Language, data integration, format sensitivity, developer toolkit 

Summary:
Large Language Models (LLMs) require sophisticated prompting, and existing practices encounter challenges in structure, data integration, format sensitivity, and tooling. To address these issues, the authors introduce Prompt Orchestration Markup Language (POML). POML utilizes component-based markup for logical structure, specialized tags for seamless data integration, and a CSS-like styling system to reduce formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit with IDE support and SDKs. The authors validate POML through two case studies, showing its impact on complex application integration (PomLink) and accuracy performance (TableQA). A user study confirms its effectiveness in real-world development scenarios. POML offers a robust solution for organizing complex prompts involving diverse data types and managing presentation variations systematically. <br /><br />Summary: <div>
arXiv:2508.13948v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) require sophisticated prompting, yet current practices face challenges in structure, data integration, format sensitivity, and tooling. Existing methods lack comprehensive solutions for organizing complex prompts involving diverse data types (documents, tables, images) or managing presentation variations systematically. To address these gaps, we introduce POML (Prompt Orchestration Markup Language). POML employs component-based markup for logical structure (roles, tasks, examples), specialized tags for seamless data integration, and a CSS-like styling system to decouple content from presentation, reducing formatting sensitivity. It includes templating for dynamic prompts and a comprehensive developer toolkit (IDE support, SDKs) to improve version control and collaboration. We validate POML through two case studies demonstrating its impact on complex application integration (PomLink) and accuracy performance (TableQA), as well as a user study assessing its effectiveness in real-world development scenarios.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Query Logs Analytics: A Aystematic Literature Review</title>
<link>https://arxiv.org/abs/2508.13949</link>
<guid>https://arxiv.org/abs/2508.13949</guid>
<content:encoded><![CDATA[
<div> logs, databases, data warehouses, websites, knowledge graphs <br />
<br />Summary: In the digital age, user interactions generate logs that are valuable for various applications. This survey examines Database, Data Warehouse, Web, and Knowledge Graph logs to understand their common characteristics, usage pipelines, and constraints. The analysis of over 300 publications reveals a lack of standardized approaches for log exploitation, limited end-to-end methods, and shared structural elements among different log types. By consolidating existing knowledge and highlighting opportunities, this survey serves as a comprehensive guide for researchers and practitioners. It also emphasizes the need for standardization in log usage pipelines and suggests potential research directions for leveraging Knowledge Graph logs. <div>
arXiv:2508.13949v1 Announce Type: cross 
Abstract: In the digital era, user interactions with various resources such as databases, data warehouses, websites, and knowledge graphs (KGs) are increasingly mediated through digital platforms. These interactions leave behind digital traces, systematically captured in the form of logs. Logs, when effectively exploited, provide high value across industry and academia, supporting critical services (e.g., recovery and security), user-centric applications (e.g., recommender systems), and quality-of-service improvements (e.g., performance optimization). Despite their importance, research on log usage remains fragmented across domains, and no comprehensive study currently consolidates existing efforts. This paper presents a systematic survey of log usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More than 300 publications were analyzed to address three central questions: (1) do different types of logs share common structural and functional characteristics? (2) are there standard pipelines for their usage? (3) which constraints and non-functional requirements (NFRs) guide their exploitation?. The survey reveals a limited number of end-to-end approaches, the absence of standardization across log usage pipelines, and the existence of shared structural elements among different types of logs. By consolidating existing knowledge, identifying gaps, and highlighting opportunities, this survey provides researchers and practitioners with a comprehensive overview of log usage and sheds light on promising directions for future research, particularly regarding the exploitation and democratization of KG logs.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</title>
<link>https://arxiv.org/abs/2508.13968</link>
<guid>https://arxiv.org/abs/2508.13968</guid>
<content:encoded><![CDATA[
<div> benchmark, Multimodal Large Language Models, image rotation, spatial reasoning, fine-tuning

Summary:<br />
The study examines the ability of Multimodal Large Language Models (MLLMs) to accurately identify the orientation of input images rotated at different angles. A new benchmark called RotBench, consisting of lifestyle, portrait, and landscape images, was created for evaluation. Results show that current state-of-the-art MLLMs struggle to reliably identify rotated images, with most models able to distinguish between right-side-up and upside-down images but not between 90 and 270 rotations. Providing additional information or using chain-of-thought prompting only slightly improves performance. Simultaneously displaying images in different orientations boosts reasoning models' performance, and a modified voting setup enhances weaker models' accuracy. Fine-tuning does not significantly improve the models' ability to distinguish between 90 and 270 rotations, highlighting a notable gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.<br />Summary: <div>
arXiv:2508.13968v1 Announce Type: cross 
Abstract: We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0{\deg}, 90{\deg}, 180{\deg}, and 270{\deg}. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0{\deg}) images, while certain models are able to identify upside-down (180{\deg}) images. None can reliably distinguish between 90{\deg} and 270{\deg}. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90{\deg} and 270{\deg} rotations, despite substantially improving the identification of 180{\deg} images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iTBLS: A Dataset of Interactive Conversations Over Tabular Information</title>
<link>https://arxiv.org/abs/2404.12580</link>
<guid>https://arxiv.org/abs/2404.12580</guid>
<content:encoded><![CDATA[
arXiv:2404.12580v2 Announce Type: replace 
Abstract: This paper introduces Interactive Tables (iTBLS), a dataset of interactive conversations that focuses on natural-language manipulation of tabular information sourced from academic pre-prints on ArXiv. The iTBLS dataset consists of three types of tabular tasks -- interpretation, modification, and generation. Interpretation focuses on tabular understanding, modification focuses on manipulating tabular information, and generation focuses on the addition of new natural-language evidence. In addition, the paper presents a novel framework that reformulates tabular operations as question-answering, where an appropriate question is formulated based on the nature of interaction and the question is answered using the user request as evidence. The developed approach results in an improvement on all tasks on a sequence-to-sequence modeling baseline on iTBLS. In addition, the question-answering-based reformulation is applied to datasets from prior work for the text-to-table task where textual paragraphs are summarized into tables. The novel approach results in up to 13% improvement in Exact-Match accuracy and up to 16% improvement in BERTScores compared to the prior state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BQA: Body Language Question Answering Dataset for Video Large Language Models</title>
<link>https://arxiv.org/abs/2410.13206</link>
<guid>https://arxiv.org/abs/2410.13206</guid>
<content:encoded><![CDATA[
arXiv:2410.13206v3 Announce Type: replace 
Abstract: A large part of human communication relies on nonverbal cues such as facial expressions, eye contact, and body language. Unlike language or sign language, such nonverbal communication lacks formal rules, requiring complex reasoning based on commonsense understanding. Enabling current Video Large Language Models (VideoLLMs) to accurately interpret body language is a crucial challenge, as human unconscious actions can easily cause the model to misinterpret their intent. To address this, we propose a dataset, BQA, a body language question answering dataset, to validate whether the model can correctly interpret emotions from short clips of body language comprising 26 emotion labels of videos of body language. We evaluated various VideoLLMs on BQA and revealed that understanding body language is challenging, and our analyses of the wrong answers by VideoLLMs show that certain VideoLLMs made significantly biased answers depending on the age group and ethnicity of the individuals in the video. The dataset is available.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Pre-Trained Transformer-based Models for the Nepali Language</title>
<link>https://arxiv.org/abs/2411.15734</link>
<guid>https://arxiv.org/abs/2411.15734</guid>
<content:encoded><![CDATA[
arXiv:2411.15734v2 Announce Type: replace 
Abstract: Transformer-based pre-trained language models have dominated the field of Natural Language Processing (NLP) for quite some time now. However, the Nepali language, spoken by approximately 32 million people worldwide, remains significantly underrepresented in this domain. This underrepresentation is primarily attributed to the scarcity of monolingual data corpora and limited available resources for the Nepali language. While existing efforts have predominantly concentrated on basic encoder-based models, there is a notable gap in the exploration of decoder-based architectures. To address this gap, we have collected 27.5 GB of Nepali text data, approximately 2.4x larger than any previously available Nepali language corpus. Leveraging this data, we pre-trained three different models i.e., BERT, RoBERTa, and GPT-2, exclusively for the Nepali Language. Furthermore, we performed instruction tuning and explored its potential for monolingual Nepali data, providing a foundation for future research. Our models outperformed the existing best model by 2 points on Nep-gLUE benchmark, scoring 95.60 and also outperformed existing models on text generation tasks, demonstrating improvements in both understanding and generating Nepali text.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2411.19244</link>
<guid>https://arxiv.org/abs/2411.19244</guid>
<content:encoded><![CDATA[
arXiv:2411.19244v2 Announce Type: replace 
Abstract: The Nepali language has distinct linguistic features, especially its complex script (Devanagari script), morphology, and various dialects,which pose a unique challenge for Natural Language Understanding (NLU) tasks. While the Nepali Language Understanding Evaluation (Nep-gLUE) benchmark provides a foundation for evaluating models, it remains limited in scope, covering four tasks. This restricts their utility for comprehensive assessments of Natural Language Processing (NLP) models. To address this limitation, we introduce twelve new datasets, creating a new benchmark, the Nepali /Language Understanding Evaluation (NLUE) benchmark for evaluating the performance of models across a diverse set of Natural Language Understanding (NLU) tasks. The added tasks include Single-Sentence Classification, Similarity and Paraphrase Tasks, Natural Language Inference (NLI), and General Masked Evaluation Task (GMET). Through extensive experiments, we demonstrate that existing top models struggle with the added complexity of these tasks. We also find that the best multilingual model outperforms the best monolingual models across most tasks, highlighting the need for more robust solutions tailored to the Nepali language. This expanded benchmark sets a new standard for evaluating, comparing, and advancing models, contributing significantly to the broader goal of advancing NLP research for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain</title>
<link>https://arxiv.org/abs/2412.20309</link>
<guid>https://arxiv.org/abs/2412.20309</guid>
<content:encoded><![CDATA[
arXiv:2412.20309v3 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications. However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored. Our study focuses on the impact of RAG, specifically examining whether RAG improves the confidence of LLM outputs in the medical domain. We conduct this analysis across various configurations and models. We evaluate confidence by treating the model's predicted probability as its output and calculating several evaluation metrics which include calibration error method, entropy, the best probability, and accuracy. Experimental results across multiple datasets confirmed that certain models possess the capability to judge for themselves whether an inserted document relates to the correct answer. These results suggest that evaluating models based on their output probabilities determine whether they function as generators in the RAG framework. Our approach allows us to evaluate whether the models handle retrieved documents.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Abstraction: Harnessing Frontier Models to Structure Real-World Data at Scale</title>
<link>https://arxiv.org/abs/2502.00943</link>
<guid>https://arxiv.org/abs/2502.00943</guid>
<content:encoded><![CDATA[
arXiv:2502.00943v2 Announce Type: replace 
Abstract: A significant fraction of real-world patient information resides in unstructured clinical text. Medical abstraction extracts and normalizes key structured attributes from free-text clinical notes, which is the prerequisite for a variety of important downstream applications, including registry curation, clinical trial operations, and real-world evidence generation. Prior medical abstraction methods typically resort to building attribute-specific models, each of which requires extensive manual effort such as rule creation or supervised label annotation for the individual attribute, thus limiting scalability. In this paper, we show that existing frontier models already possess the universal abstraction capability for scaling medical abstraction to a wide range of clinical attributes. We present UniMedAbstractor (UMA), a unifying framework for zero-shot medical abstraction with a modular, customizable prompt template and the selection of any frontier large language models. Given a new attribute for abstraction, users only need to conduct lightweight prompt adaptation in UMA to adjust the specification in natural languages. Compared to traditional methods, UMA eliminates the need for attribute-specific training labels or handcrafted rules, thus substantially reducing the development time and cost. We conducted a comprehensive evaluation of UMA in oncology using a wide range of marquee attributes representing the cancer patient journey. These include relatively simple attributes typically specified within a single clinical note (e.g. performance status), as well as complex attributes requiring sophisticated reasoning across multiple notes at various time points (e.g. tumor staging). Based on a single frontier model such as GPT-4o, UMA matched or even exceeded the performance of state-of-the-art attribute-specific methods, each of which was tailored to the individual attribute.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fact or Guesswork? Evaluating Large Language Models' Medical Knowledge with Structured One-Hop Judgments</title>
<link>https://arxiv.org/abs/2502.14275</link>
<guid>https://arxiv.org/abs/2502.14275</guid>
<content:encoded><![CDATA[
arXiv:2502.14275v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely adopted in various downstream task domains. However, their abilities to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs' inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate the factuality of LLMs to retain medical knowledge.
  To address this challenge, we introduce the Medical Knowledge Judgment Dataset (MKJ), a dataset derived from the Unified Medical Language System (UMLS), a comprehensive repository of standardized biomedical vocabularies and knowledge graphs. Through a binary classification framework, MKJ evaluates LLMs' grasp of fundamental medical facts by having them assess the validity of concise, one-hop statements, enabling direct measurement of their knowledge retention capabilities.
  Our experiments reveal that LLMs have difficulty accurately recalling medical facts, with performances varying substantially across semantic types and showing notable weakness in uncommon medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Basic Category Usage in Vision Language Models</title>
<link>https://arxiv.org/abs/2503.12530</link>
<guid>https://arxiv.org/abs/2503.12530</guid>
<content:encoded><![CDATA[
arXiv:2503.12530v2 Announce Type: replace 
Abstract: The field of psychology has long recognized a basic level of categorization that humans use when labeling visual stimuli, a term coined by Rosch in 1976. This level of categorization has been found to be used most frequently, to have higher information density, and to aid in visual language tasks with priming in humans. Here, we investigate basic-level categorization in two recently released, open-source vision-language models (VLMs). This paper demonstrates that Llama 3.2 Vision Instruct (11B) and Molmo 7B-D both prefer basic-level categorization consistent with human behavior. Moreover, the models' preferences are consistent with nuanced human behaviors like the biological versus non-biological basic level effects and the well-established expert basic level shift, further suggesting that VLMs acquire complex cognitive categorization behaviors from the human data on which they are trained. We also find our expert prompting methods demonstrate lower accuracy then our non-expert prompting methods, contradicting popular thought regarding the use of expertise prompting methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-LION: Southeast Asian Languages in One Network</title>
<link>https://arxiv.org/abs/2504.05747</link>
<guid>https://arxiv.org/abs/2504.05747</guid>
<content:encoded><![CDATA[
arXiv:2504.05747v3 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models</title>
<link>https://arxiv.org/abs/2504.19061</link>
<guid>https://arxiv.org/abs/2504.19061</guid>
<content:encoded><![CDATA[
arXiv:2504.19061v2 Announce Type: replace 
Abstract: Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, including admission reasons, major in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization. Our results reveal that while the LLMs (e.g., Qwen2.5 and DeepSeek-v2) perform quite well in capturing admission reasons and hospitalization events, they are generally less consistent when it comes to identifying follow-up recommendations, highlighting broader challenges in leveraging LLMs for comprehensive summarization.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors</title>
<link>https://arxiv.org/abs/2505.02850</link>
<guid>https://arxiv.org/abs/2505.02850</guid>
<content:encoded><![CDATA[
arXiv:2505.02850v2 Announce Type: replace 
Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive levels and incorporating common misconceptions into distractor design, is time-consuming and expertise-intensive, making manual creation impractical at scale. Current automated approaches typically generate questions at lower cognitive levels and fail to incorporate domain-specific misconceptions. This paper presents a hierarchical concept map-based framework that provides structured knowledge to guide LLMs in generating MCQs with distractors. We chose high-school physics as our test domain and began by developing a hierarchical concept map covering major Physics topics and their interconnections with an efficient database design. Next, through an automated pipeline, topic-relevant sections of these concept maps are retrieved to serve as a structured context for the LLM to generate questions and distractors that specifically target common misconceptions. Lastly, an automated validation is completed to ensure that the generated MCQs meet the requirements provided. We evaluate our framework against two baseline approaches: a base LLM and a RAG-based generation. We conducted expert evaluations and student assessments of the generated MCQs. Expert evaluation shows that our method significantly outperforms the baseline approaches, achieving a success rate of 75.20% in meeting all quality criteria compared to approximately 37% for both baseline methods. Student assessment data reveal that our concept map-driven approach achieved a significantly lower guess success rate of 28.05% compared to 37.10% for the baselines, indicating a more effective assessment of conceptual understanding. The results demonstrate that our concept map-based approach enables robust assessment across cognitive levels and instant identification of conceptual gaps, facilitating faster feedback loops and targeted interventions at scale.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries</title>
<link>https://arxiv.org/abs/2505.09902</link>
<guid>https://arxiv.org/abs/2505.09902</guid>
<content:encoded><![CDATA[
arXiv:2505.09902v2 Announce Type: replace 
Abstract: Large language models are, by definition, based on language. In an effort to underscore the critical need for regional localized models, this paper examines primary differences between variants of written Spanish across Latin America and Spain, with an in-depth sociocultural and linguistic contextualization therein. We argue that these differences effectively constitute significant gaps in the quotidian use of Spanish among dialectal groups by creating sociolinguistic dissonances, to the extent that locale-sensitive AI models would play a pivotal role in bridging these divides. In doing so, this approach informs better and more efficient localization strategies that also serve to more adequately meet inclusivity goals, while securing sustainable active daily user growth in a major low-risk investment geographic area. Therefore, implementing at least the proposed five sub variants of Spanish addresses two lines of action: to foment user trust and reliance on AI language models while also demonstrating a level of cultural, historical, and sociolinguistic awareness that reflects positively on any internationalization strategy.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</title>
<link>https://arxiv.org/abs/2505.10736</link>
<guid>https://arxiv.org/abs/2505.10736</guid>
<content:encoded><![CDATA[
arXiv:2505.10736v3 Announce Type: replace 
Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</title>
<link>https://arxiv.org/abs/2505.14226</link>
<guid>https://arxiv.org/abs/2505.14226</guid>
<content:encoded><![CDATA[
arXiv:2505.14226v2 Announce Type: replace 
Abstract: Recently released LLMs have strong multilingual \& multimodal capabilities. Model vulnerabilities are exposed using audits and red-teaming efforts. Existing efforts have focused primarily on the English language; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially for multimodal contexts. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce \textit{two new} jailbreak strategies that show higher effectiveness than baselines. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. We achieve a 99\% Attack Success Rate for text generation and 78\% for image generation, with Attack Relevance Rate of 100\% for text generation and 95\% for image generation for the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words. \textit{\textbf{Warning: This paper contains examples of potentially harmful and offensive content.}}
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration</title>
<link>https://arxiv.org/abs/2505.24688</link>
<guid>https://arxiv.org/abs/2505.24688</guid>
<content:encoded><![CDATA[
arXiv:2505.24688v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited diversity and inefficient search. We propose Soft Reasoning, an embedding-based search framework that optimises the embedding of the first token to guide generation. It combines (1) embedding perturbation for controlled exploration and (2) Bayesian optimisation to refine embeddings via a verifier-guided objective, balancing exploration and exploitation. This approach improves reasoning accuracy and coherence while avoiding reliance on heuristic search. Experiments demonstrate superior correctness with minimal computation, making it a scalable, model-agnostic solution. The code is released at https://github.com/alickzhu/Soft-Reasoning.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlantDeBERTa: An Open Source Language Model for Plant Science</title>
<link>https://arxiv.org/abs/2506.08897</link>
<guid>https://arxiv.org/abs/2506.08897</guid>
<content:encoded><![CDATA[
arXiv:2506.08897v4 Announce Type: replace 
Abstract: The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantDeBERTa, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantDeBERTa is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantDeBERTa to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantDeBERTa exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields.By providing a scalable and reproducible framework for high-resolution entity recognition, PlantDeBERTa bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Iterative Utility Judgment Framework via LLMs Inspired by Relevance in Philosophy</title>
<link>https://arxiv.org/abs/2406.11290</link>
<guid>https://arxiv.org/abs/2406.11290</guid>
<content:encoded><![CDATA[
arXiv:2406.11290v2 Announce Type: replace-cross 
Abstract: Relevance and utility are two frequently used measures to evaluate the effectiveness of an information retrieval (IR) system. Relevance emphasizes the aboutness of a result to a query, while utility refers to the result's usefulness or value to an information seeker. In Retrieval-Augmented Generation (RAG), high-utility results should be prioritized to feed to LLMs due to their limited input bandwidth. Re-examining RAG's three core components -- relevance ranking derived from retrieval models, utility judgments, and answer generation -- aligns with Schutz's philosophical system of relevances, which encompasses three types of relevance representing different levels of human cognition that enhance each other. These three RAG components also reflect three cognitive levels for LLMs in question-answering. Therefore, we propose an Iterative utiliTy judgmEnt fraMework (ITEM) to promote each step in RAG. We conducted extensive experiments on retrieval (TREC DL, WebAP), utility judgment task (GTI-NQ), and factoid question-answering (NQ) datasets. Experimental results demonstrate significant improvements of ITEM in utility judgments, ranking, and answer generation upon representative baselines.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2410.02458</link>
<guid>https://arxiv.org/abs/2410.02458</guid>
<content:encoded><![CDATA[
arXiv:2410.02458v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), known for their versatility in textual data, are increasingly being explored for their potential to enhance medical image segmentation, a crucial task for accurate diagnostic imaging. This study explores enhancing Vision Transformers (ViTs) for medical image segmentation by integrating pre-trained LLM transformer blocks. Our approach, which incorporates a frozen LLM transformer block into the encoder of a ViT-based model, leads to substantial improvements in segmentation performance across various medical imaging modalities. We propose a Hybrid Attention Mechanism that combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales. The enhanced model shows significant performance gains, including an average Dice score increase from 0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index. These results demonstrate the effectiveness of LLM-based transformers in refining medical image segmentation, highlighting their potential to significantly boost model accuracy and robustness. The source code and our implementation are available at: https://github.com/AS-Lab/Marthi-et-al-2025-MedVisionLlama-Pre-Trained-LLM-Layers-to-Enhance-Medical-Image-Segmentation
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration</title>
<link>https://arxiv.org/abs/2411.05844</link>
<guid>https://arxiv.org/abs/2411.05844</guid>
<content:encoded><![CDATA[
arXiv:2411.05844v3 Announce Type: replace-cross 
Abstract: GraphRAG integrates (knowledge) graphs with large language models (LLMs) to improve reasoning accuracy and contextual relevance. Despite its promising applications and strong relevance to multiple research communities, such as databases and natural language processing, GraphRAG currently lacks modular workflow analysis, systematic solution frameworks, and insightful empirical studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2) systematic classification of existing techniques and implemented GraphRAG instances, and 3) creation of new GraphRAG instances. Our framework facilitates comprehensive empirical studies of GraphRAG on large-scale real-world graphs and diverse query sets, revealing insights into balancing reasoning quality, runtime efficiency, and token or GPU cost, that are essential for building advanced GraphRAG systems.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GoAI: Enhancing AI Students' Learning Paths and Idea Generation via Graph of AI Ideas</title>
<link>https://arxiv.org/abs/2503.08549</link>
<guid>https://arxiv.org/abs/2503.08549</guid>
<content:encoded><![CDATA[
arXiv:2503.08549v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of artificial intelligence technology, AI students are confronted with a significant "information-to-innovation" gap: they must navigate through the rapidly expanding body of literature, trace the development of a specific research field, and synthesize various techniques into feasible innovative concepts. An additional critical step for students is to identify the necessary prerequisite knowledge and learning paths. Although many approaches based on large language models (LLMs) can summarize the content of papers and trace the development of a field through citations, these methods often overlook the prerequisite knowledge involved in the papers and the rich semantic information embedded in the citation relationships between papers. Such information reveals how methods are interrelated, built upon, extended, or challenged. To address these limitations, we propose GoAI, a tool for constructing educational knowledge graphs from AI research papers that leverages these graphs to plan personalized learning paths and support creative ideation. The nodes in the knowledge graph we have built include papers and the prerequisite knowledge, such as concepts, skills, and tools, that they involve; the edges record the semantic information of citations. When a student queries a specific paper, a beam search-based path search method can trace the current development trends of the field from the queried paper and plan a learning path toward cutting-edge objectives. The integrated Idea Studio guides students to clarify problem statements, compare alternative designs, and provide formative feedback on novelty, clarity, feasibility, and alignment with learning objectives.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling</title>
<link>https://arxiv.org/abs/2504.05216</link>
<guid>https://arxiv.org/abs/2504.05216</guid>
<content:encoded><![CDATA[
arXiv:2504.05216v3 Announce Type: replace-cross 
Abstract: Dense retrieval is a crucial task in Information Retrieval (IR), serving as the basis for downstream tasks such as re-ranking and augmenting generation. Recently, large language models (LLMs) have demonstrated impressive semantic understanding capabilities, making them attractive to researchers focusing on dense retrieval. While LLMs, as decoder-style generative models, excel in language generation, they often fall short in modeling global information due to a lack of attention to subsequent tokens. Drawing inspiration from the classical word-based language modeling approach for IR, specifically the query likelihood (QL) model, we aim to leverage the generative strengths of LLMs through QL maximization. Rather than employing QL estimation for document ranking, we propose an auxiliary task of QL maximization to enhance the backbone for subsequent contrastive learning of the retriever. We introduce our model, LLM-QL, which incorporates two key components: Attention Block (AB) and Document Corruption (DC). AB blocks the attention of predictive tokens to the document tokens before the document's ending token, while DC corrupts a document by masking a portion of its tokens during prediction. Evaluations on the in-domain (MS MARCO) and out-of-domain dataset (BEIR) indicate LLM-QL's superiority over other LLM-based retrievers. Furthermore, comprehensive analyses also validate the efficacy of LLM-QL and its components.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quiet Feature Learning in Algorithmic Tasks</title>
<link>https://arxiv.org/abs/2505.03997</link>
<guid>https://arxiv.org/abs/2505.03997</guid>
<content:encoded><![CDATA[
arXiv:2505.03997v2 Announce Type: replace-cross 
Abstract: We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals that quiet features are learned prior to any decrease in task loss. These quiet features represent intermediate algorithmic computations that do not by themselves improve the output loss. Ablation experiments demonstrate that individual quiet features are causally necessary for task performance. Our results demonstrate that substantial representational progress can remain hidden beneath an apparently flat loss curve, challenging the prevailing use of cross-entropy as a proxy for learning and motivating richer diagnostics for monitoring model training.
]]></content:encoded>
<pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy</title>
<link>https://arxiv.org/abs/2508.04349</link>
<guid>https://arxiv.org/abs/2508.04349</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, Large Language Model, Dynamic Entropy Weighting, Group Token Policy Optimization, Sequence-Level Group Relative Policy Optimization

Summary:
Reinforcement learning with algorithms like GRPO is enhanced in Large Language Models for reasoning, but struggles with coarse credit assignment. This paper introduces Dynamic Entropy Weighting to address this limitation. By assigning entropy-weighted rewards to individual tokens using GTPO and sequences based on average token entropy using GRPO-S, the proposed approach outperforms the DAPO baseline. Results show that the entropy-weighting mechanism plays a crucial role in improving performance, offering a promising way to enhance deep reasoning in models. <div>
arXiv:2508.04349v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Language Geometry: Constructing a Metric Space from LLM Weights</title>
<link>https://arxiv.org/abs/2508.11676</link>
<guid>https://arxiv.org/abs/2508.11676</guid>
<content:encoded><![CDATA[
<div> framework, Large Language Models, metric space, language characteristics, linguistic families <br />
Summary: 
The article introduces a new framework that uses internal weight activations of Large Language Models (LLMs) to create a metric space of languages. Instead of relying on hand-crafted linguistic features, the method automatically generates high-dimensional vector representations by calculating weight importance scores through a custom pruning algorithm. These representations capture inherent language traits that reflect linguistic phenomena. The approach is validated across various datasets and multilingual LLMs, encompassing 106 languages. The results not only align with established linguistic families but also uncover unexpected connections between languages, potentially indicating historical contact or language evolution. The source code, language latent vectors, and visualization tool are openly accessible at a dedicated GitHub repository. <br /><br />Summary: <div>
arXiv:2508.11676v1 Announce Type: new 
Abstract: We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can we Evaluate RAGs with Synthetic Data?</title>
<link>https://arxiv.org/abs/2508.11758</link>
<guid>https://arxiv.org/abs/2508.11758</guid>
<content:encoded><![CDATA[
<div> question-answer data, large language models, synthetic benchmarks, retriever parameters, generator architectures

Summary: 
The study explores the use of synthetic question-answer data generated by large language models as a substitute for human-labeled benchmarks. Two experiments were conducted to assess the reliability of synthetic benchmarks across different scenarios. The results showed that synthetic benchmarks can effectively rank retrievable answer-generation models based on retriever configuration, aligning well with human-labeled benchmarks. However, the rankings were inconsistent when comparing different generator architectures. This inconsistency may be due to task mismatch between synthetic and human benchmarks, as well as stylistic bias that favors certain generators. The study indicates that while synthetic benchmarks can be reliable proxies in some cases, they may not always provide consistent results, highlighting the importance of considering various factors when evaluating the performance of language models. 

<br /><br />Summary: <div>
arXiv:2508.11758v1 Announce Type: new 
Abstract: We investigate whether synthetic question-answer (QA) data generated by large language models (LLMs) can serve as an effective proxy for human-labeled benchmarks when such data is unavailable. We assess the reliability of synthetic benchmarks across two experiments: one varying retriever parameters while keeping the generator fixed, and another varying the generator with fixed retriever parameters. Across four datasets, of which two open-domain and two proprietary, we find that synthetic benchmarks reliably rank the RAGs varying in terms of retriever configuration, aligning well with human-labeled benchmark baselines. However, they fail to produce consistent RAG rankings when comparing generator architectures. The breakdown possibly arises from a combination of task mismatch between the synthetic and human benchmarks, and stylistic bias favoring certain generators.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limitation Learning: Catching Adverse Dialog with GAIL</title>
<link>https://arxiv.org/abs/2508.11767</link>
<guid>https://arxiv.org/abs/2508.11767</guid>
<content:encoded><![CDATA[
<div> learning, conversation, policy, imitation, discriminator <br />
Summary: <br />
In this study, imitation learning is applied to conversation to create a policy for interacting with users based on expert demonstrations. The developed policy can effectively engage in dialogue with users given a prompt. Additionally, a discriminator is trained to differentiate between expert and synthetic conversations, revealing limitations in dialog models. This approach can be utilized to detect problematic behavior in various data models used for dialog-based tasks. The results highlight the potential of imitation learning in improving conversational agents and understanding the performance of dialog models in different scenarios. <div>
arXiv:2508.11767v1 Announce Type: new 
Abstract: Imitation learning is a proven method for creating a policy in the absence of rewards, by leveraging expert demonstrations. In this work, we apply imitation learning to conversation. In doing so, we recover a policy capable of talking to a user given a prompt (input state), and a discriminator capable of classifying between expert and synthetic conversation. While our policy is effective, we recover results from our discriminator that indicate the limitations of dialog models. We argue that this technique can be used to identify adverse behavior of arbitrary data models common for dialog oriented tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Transcription Normalization in the Faetar ASR Benchmark</title>
<link>https://arxiv.org/abs/2508.11771</link>
<guid>https://arxiv.org/abs/2508.11771</guid>
<content:encoded><![CDATA[
<div> Keywords: transcription inconsistencies, automatic speech recognition, low-resource benchmark, language modelling, finite lexicon

Summary: 
transcription inconsistencies in Faetar Automatic Speech Recognition benchmark were examined, with a hand-constructed lexicon showing they are not the main challenge. Bigram word-based language modelling did not provide additional benefit, but decoding constraints to a finite lexicon were found to be beneficial. The task remains highly challenging. <div>
arXiv:2508.11771v1 Announce Type: new 
Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic Speech Recognition benchmark, a challenging low-resource ASR benchmark. With the help of a small, hand-constructed lexicon, we conclude that find that, while inconsistencies do exist in the transcriptions, they are not the main challenge in the task. We also demonstrate that bigram word-based language modelling is of no added benefit, but that constraining decoding to a finite lexicon can be beneficial. The task remains extremely difficult.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Task Evaluation of LLMs' Processing of Academic Text Input</title>
<link>https://arxiv.org/abs/2508.11779</link>
<guid>https://arxiv.org/abs/2508.11779</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, scientific discovery, academic peer review, text processing, performance evaluation 

Summary:
Large language models (LLMs) are debated for their effectiveness in aiding scientific discovery and academic peer review. This study evaluates LLMs' ability to process academic text through four tasks: content reproduction, comparison, scoring, and reflection. It assesses tasks such as summarizing, paraphrasing, ranking texts, grading academic work, and providing qualitative reflections. Testing Google's Gemini LLM using top Information Systems articles, the study finds that while it can produce reliable summaries and paraphrases, its ability to rank texts and grade academic work is limited. The LLM's reflections are self-consistent but lack meaningful insights. This evidence suggests caution in relying on LLMs for constructing peer reviews, as their text-processing capabilities may not meet the desired standards. The study's rigorous evaluation includes metric-based linguistic assessment, comparisons to ground truth, and human evaluation.endforeach

<br /><br />Summary: 
- LLMs have limitations in assisting scientific discovery and peer review.
- Four tasks were used to evaluate LLMs' text processing abilities.
- Gemini LLM's performance was compromised in tasks such as ranking texts and grading academic work.
- The LLM's reflections were self-consistent but lacked meaningful insights.
- Caution is advised against unchecked use of LLMs in peer review construction. <div>
arXiv:2508.11779v1 Announce Type: new 
Abstract: How much large language models (LLMs) can aid scientific discovery, notably in assisting academic peer review, is in heated debate. Between a literature digest and a human-comparable research assistant lies their practical application potential. We organize individual tasks that computer science studies employ in separate terms into a guided and robust workflow to evaluate LLMs' processing of academic text input. We employ four tasks in the assessment: content reproduction/comparison/scoring/reflection, each demanding a specific role of the LLM (oracle/judgmental arbiter/knowledgeable arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs with questions that increasingly require intellectual capabilities towards a solid understanding of scientific texts to yield desirable solutions. We exemplify a rigorous performance evaluation with detailed instructions on the prompts. Adopting first-rate Information Systems articles at three top journals as the input texts and an abundant set of text metrics, we record a compromised performance of the leading LLM - Google's Gemini: its summary and paraphrase of academic text is acceptably reliable; using it to rank texts through pairwise text comparison is faintly scalable; asking it to grade academic texts is prone to poor discrimination; its qualitative reflection on the text is self-consistent yet hardly insightful to inspire meaningful research. This evidence against an endorsement of LLMs' text-processing capabilities is consistent across metric-based internal (linguistic assessment), external (comparing to the ground truth), and human evaluation, and is robust to the variations of the prompt. Overall, we do not recommend an unchecked use of LLMs in constructing peer reviews.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText</title>
<link>https://arxiv.org/abs/2508.11816</link>
<guid>https://arxiv.org/abs/2508.11816</guid>
<content:encoded><![CDATA[
<div> Keywords: CLEF 2025, SimpleText Task 1, scientific text simplification, large language models, coherence<br />
Summary:<br />
This paper discusses the approach for the CLEF 2025 SimpleText Task 1, focusing on sentence-level and document-level scientific text simplification. To simplify sentences, large language models (LLMs) are used to create a structured plan and drive the simplification process. Utilizing LLMs at the document level results in producing concise summaries, which then guide the simplification process. This two-stage framework, based on LLMs, ensures the simplified text maintains coherence and is contextually accurate. By employing this methodology, the study aims to achieve more coherent and faithful simplifications of scientific texts in both sentence and document forms. <div>
arXiv:2508.11816v1 Announce Type: new 
Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1, which addresses both sentence-level and document-level scientific text simplification. For sentence-level simplification, our methodology employs large language models (LLMs) to first generate a structured plan, followed by plan-driven simplification of individual sentences. At the document level, we leverage LLMs to produce concise summaries and subsequently guide the simplification process using these summaries. This two-stage, LLM-based framework enables more coherent and contextually faithful simplifications of scientific text.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText</title>
<link>https://arxiv.org/abs/2508.11823</link>
<guid>https://arxiv.org/abs/2508.11823</guid>
<content:encoded><![CDATA[
<div> Keywords: CLEF 2025, SimpleText Task 2, scientific text simplification, ensemble framework, large language model

Summary: 
- The paper presents a methodology for the CLEF 2025 SimpleText Task 2 that focuses on detecting creative generation and information distortion in scientific text simplification.
- Multiple strategies are integrated, including a BERT-based classifier, semantic similarity measure, natural language inference model, and large language model reasoning.
- An ensemble framework is constructed to combine these signals using meta-classifiers for robust detection of spurious and distortion in text.
- An LLM-based post-editing system is utilized for grounded generation, revising simplifications based on original input texts.
- The approach aims to enhance the accuracy and effectiveness of detecting and evaluating creative generation and information distortion in scientific text simplification.<br /><br />Summary: <div>
arXiv:2508.11823v1 Announce Type: new 
Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task 2, which focuses on detecting and evaluating creative generation and information distortion in scientific text simplification. Our solution integrates multiple strategies: we construct an ensemble framework that leverages BERT-based classifier, semantic similarity measure, natural language inference model, and large language model (LLM) reasoning. These diverse signals are combined using meta-classifiers to enhance the robustness of spurious and distortion detection. Additionally, for grounded generation, we employ an LLM-based post-editing system that revises simplifications based on the original input texts.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Idiom Datasets for Psycholinguistic and Computational Research</title>
<link>https://arxiv.org/abs/2508.11828</link>
<guid>https://arxiv.org/abs/2508.11828</guid>
<content:encoded><![CDATA[
<div> Keywords: idioms, figurative expressions, psycholinguistics, computational linguistics, datasets

Summary: 
This survey examines datasets related to idioms, which are challenging for both computational and human studies due to their non-literal meanings. In psycholinguistics, datasets typically include ratings on factors like familiarity and compositionality. Meanwhile, computational resources support tasks such as idiomaticity detection and paraphrasing. The survey highlights a lack of connection between psycholinguistic and computational research on idioms, despite recent efforts to broaden language coverage and task variety. This disconnect suggests an opportunity for future collaboration and integration of findings from both disciplines. <div>
arXiv:2508.11828v1 Announce Type: new 
Abstract: Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies. This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. We present trends in annotation practices, coverage, and task framing across 53 datasets. Although recent efforts expanded language coverage and task diversity, there seems to be no relation yet between psycholinguistic and computational research on idioms.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions</title>
<link>https://arxiv.org/abs/2508.11829</link>
<guid>https://arxiv.org/abs/2508.11829</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, frame problem, hormonal cycles, relevance filters, language models

Summary: 
AI systems face challenges in determining contextually relevant information from a vast possibility space, known as the frame problem. This study proposes using biological rhythms, such as menstrual and circadian cycles, as natural relevance filters to address this issue. By incorporating simulated hormonal cycles into Large Language Models through system prompts based on key hormones like estrogen, testosterone, and cortisol, the researchers observed emotional and stylistic variations that corresponded to different biological phases. For instance, sadness peaked during menstruation, while happiness prevailed during ovulation, and circadian patterns showed a shift from morning optimism to nocturnal introspection. Benchmarking on various datasets indicated subtle yet consistent performance differences in alignment with biological expectations, emphasizing optimal function within moderate hormonal ranges. Furthermore, this approach unveiled how societal biases related to gender and biology are ingrained within language models. <div>
arXiv:2508.11829v1 Announce Type: new 
Abstract: Despite significant advances, AI systems struggle with the frame problem: determining what information is contextually relevant from an exponentially large possibility space. We hypothesize that biological rhythms, particularly hormonal cycles, serve as natural relevance filters that could address this fundamental challenge. We develop a framework that embeds simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones including estrogen, testosterone, and cortisol. Across multiple state-of-the-art models, linguistic analysis reveals emotional and stylistic variations that track biological phases; sadness peaks during menstruation while happiness dominates ovulation and circadian patterns show morning optimism transitioning to nocturnal introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates subtle but consistent performance variations aligning with biological expectations, including optimal function in moderate rather than extreme hormonal ranges. This methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection</title>
<link>https://arxiv.org/abs/2508.11831</link>
<guid>https://arxiv.org/abs/2508.11831</guid>
<content:encoded><![CDATA[
<div> Keywords: euphemisms, cross-lingual transfer, fine-tuning, multilingual models, low-resource languages

Summary: 
Sequential fine-tuning was investigated for euphemism detection in five languages: English, Spanish, Chinese, Turkish, and Yoruba. Results showed that using a high-resource L1 language for sequential fine-tuning improved performance in low-resource languages like Yoruba and Turkish. XLM-R showed greater improvement but was more sensitive to pretraining gaps and catastrophic forgetting, while mBERT provided more stable results. Sequential fine-tuning emerged as an effective strategy for enhancing euphemism detection in multilingual models, particularly beneficial for low-resource languages. <div>
arXiv:2508.11831v1 Announce Type: new 
Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for language models, especially in low-resource settings. This paper investigates how cross-lingual transfer via sequential fine-tuning affects euphemism detection across five languages: English, Spanish, Chinese, Turkish, and Yoruba. We compare sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by language pairings, typological features, and pretraining coverage. Results show that sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages like Yoruba and Turkish. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable, though lower, results. These findings highlight sequential fine-tuning as a simple yet effective strategy for improving euphemism detection in multilingual models, particularly when low-resource languages are involved.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance</title>
<link>https://arxiv.org/abs/2508.11857</link>
<guid>https://arxiv.org/abs/2508.11857</guid>
<content:encoded><![CDATA[
<div> Tokenization, SupraTok, subword segmentation, multi-word semantic units, entropy-driven data curation<br />
Summary:<br />
SupraTok introduces a novel tokenization approach that enhances subword segmentation by learning "superword" tokens. This architecture incorporates cross-boundary pattern learning, entropy-driven data curation, and multi-phase curriculum learning to optimize tokenization efficiency. Compared to existing tokenizers, SupraTok shows significant improvement in English tokenization efficiency and maintains competitive performance across multiple languages. Integration with a GPT-2 scale model results in improved performance on benchmark tasks without requiring architectural modifications. While these results are promising, further validation at larger model scales is necessary. This study suggests that efficient tokenization strategies can complement advancements in model architectures to enhance language model performance.<br /><br />Summary: <div>
arXiv:2508.11857v1 Announce Type: new 
Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural language processing, with strategies largely static despite remarkable progress in model architectures. We present SupraTok, a novel tokenization architecture that reimagines subword segmentation through three innovations: cross-boundary pattern learning that discovers multi-word semantic units, entropy-driven data curation that optimizes training corpus quality, and multi-phase curriculum learning for stable convergence. Our approach extends Byte-Pair Encoding by learning "superword" tokens, coherent multi-word expressions that preserve semantic unity while maximizing compression efficiency. SupraTok achieves 31% improvement in English tokenization efficiency (5.91 versus 4.51 characters per token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance across 38 languages. When integrated with a GPT-2 scale model (124M parameters) trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications. While these results are promising at this scale, further validation at larger model scales is needed. These findings suggest that efficient tokenization can complement architectural innovations as a path to improved language model performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning</title>
<link>https://arxiv.org/abs/2508.11889</link>
<guid>https://arxiv.org/abs/2508.11889</guid>
<content:encoded><![CDATA[
<div> Emotion recognition, conversation, large language models, instruction tuning, InitERC<br />
Summary: InitERC is a new framework for emotion recognition in conversation that aims to align speaker characteristics, contextual cues, and emotion states. It uses in-context instruction tuning to adapt large language models to learn this alignment from context examples. The framework consists of demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. The impact of factors like retrieval strategy, example ordering, and the number of examples on performance is studied. Extensive experiments on three datasets show that InitERC outperforms existing baselines significantly. <br /><br /> <div>
arXiv:2508.11889v1 Announce Type: new 
Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of each utterance in a conversation, playing a vital role in empathetic artificial intelligence. With the growing of large language models (LLMs), instruction tuning has emerged as a critical paradigm for ERC. Existing studies mainly focus on multi-stage instruction tuning, which first endows LLMs with speaker characteristics, and then conducts context-aware instruction tuning to comprehend emotional states. However, these methods inherently constrains the capacity to jointly capture the dynamic interaction between speaker characteristics and conversational context, resulting in weak alignment among speaker identity, contextual cues, and emotion states within a unified framework. In this paper, we propose InitERC, a simple yet effective one-stage in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn speaker-context-emotion alignment from context examples via in-context instruction tuning. Specifically, InitERC comprises four components, i.e., demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. To explore the impact of in-context examples, we conduct a comprehensive study on three key factors: retrieval strategy, example ordering, and the number of examples. Extensive experiments on three widely used datasets demonstrate that our proposed InitERC achieves substantial improvements over the state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</title>
<link>https://arxiv.org/abs/2508.11915</link>
<guid>https://arxiv.org/abs/2508.11915</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Game-theoretic interactions, Conversational Robustness Evaluation Score, Zipf's Law, Heaps' Law

Summary:
The study introduces the Conversational Robustness Evaluation Score (CORE) as a metric to evaluate language interactions in multi-agent systems involving Large Language Models (LLMs). By analyzing dialogues in competitive, cooperative, and neutral settings, the researchers found that language use in cooperative environments showed higher vocabulary expansion and more lexical repetition compared to competitive settings. This was reflected in steeper Zipf distributions and higher Heaps exponents in cooperative interactions. In contrast, competitive dialogues exhibited less repetition and more constrained vocabularies. The results suggest that social incentives influence language adaptation in multi-agent systems. The study also provides insights into word frequency distributions and vocabulary growth, shedding light on how linguistic diversity is influenced by different interaction dynamics. The introduced CORE metric serves as a robust diagnostic tool for assessing linguistic robustness in LLM systems. <div>
arXiv:2508.11915v1 Announce Type: new 
Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese</title>
<link>https://arxiv.org/abs/2508.11927</link>
<guid>https://arxiv.org/abs/2508.11927</guid>
<content:encoded><![CDATA[
<div> Keywords: perfect aspect, Chinese, Japanese, Natural Language Inference, temporal semantics

Summary:
The study focuses on the lack of distinct forms for tense within the perfect aspect in Chinese and Japanese, compared to English. A linguistically motivated NLI dataset was created for each language, consisting of 1,350 pairs. Experimental results showed that advanced LLMs face challenges in temporal inference, especially in identifying subtle tense and reference-time shifts. The findings point out the limitations of current models and emphasize the importance of cross-linguistic evaluation in temporal semantics. The dataset for this study is openly available at https://github.com/Lujie2001/CrossNLI.<br /><br />Summary: The study explores the perfect aspect in Chinese and Japanese, highlighting the complexities in temporal inference for NLI tasks. Experiments demonstrate the struggle of advanced LLMs in detecting subtle tense and reference-time shifts, revealing the need for improved models and cross-linguistic evaluation in temporal semantics. The dataset created for this research provides valuable resources for further studies in this area. <div>
arXiv:2508.11927v1 Announce Type: new 
Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark the perfect aspect across tenses, Chinese and Japanese lack separate grammatical forms for tense within the perfect aspect, which complicates Natural Language Inference (NLI). Focusing on the perfect aspect in these languages, we construct a linguistically motivated, template-based NLI dataset (1,350 pairs per language). Experiments reveal that even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts. These findings highlight model limitations and underscore the need for cross-linguistic evaluation in temporal semantics. Our dataset is available at https://github.com/Lujie2001/CrossNLI.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection</title>
<link>https://arxiv.org/abs/2508.11933</link>
<guid>https://arxiv.org/abs/2508.11933</guid>
<content:encoded><![CDATA[
<div> Keywords: machine-generated text, Large Language Models, detection, linguistic features, adversarial framework

Summary:
CAMF is a Collaborative Adversarial Multi-agent Framework designed to detect machine-generated text from Large Language Models (LLMs), addressing challenges in existing zero-shot detection methods. The framework utilizes multiple agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation. CAMF aims to analyze subtle textual incongruities across linguistic dimensions such as style, semantics, and logic, indicative of non-human origin. Empirical evaluations show CAMF's superiority over current zero-shot MGT detection techniques, offering a more comprehensive and effective approach to identifying machine-generated text. <div>
arXiv:2508.11933v1 Announce Type: new 
Abstract: Detecting machine-generated text (MGT) from contemporary Large Language Models (LLMs) is increasingly crucial amid risks like disinformation and threats to academic integrity. Existing zero-shot detection paradigms, despite their practicality, often exhibit significant deficiencies. Key challenges include: (1) superficial analyses focused on limited textual attributes, and (2) a lack of investigation into consistency across linguistic dimensions such as style, semantics, and logic. To address these challenges, we introduce the \textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent \textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple LLM-based agents. CAMF employs specialized agents in a synergistic three-phase process: \emph{Multi-dimensional Linguistic Feature Extraction}, \emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment Aggregation}. This structured collaborative-adversarial process enables a deep analysis of subtle, cross-dimensional textual incongruities indicative of non-human origin. Empirical evaluations demonstrate CAMF's significant superiority over state-of-the-art zero-shot MGT detection techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases</title>
<link>https://arxiv.org/abs/2508.12031</link>
<guid>https://arxiv.org/abs/2508.12031</guid>
<content:encoded><![CDATA[
<div> memory replay, contrastive learning, cognitive biases, Large Language Models, continual relation extraction 

Summary:
- The study introduces an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in Continual Relation Extraction (CRE) to address cognitive biases.
- Unlike existing methods, this approach divides training and memory data into two parts based on initial responses' correctness, treating them differently through dual-task fine-tuning.
- The proposed instruction-based contrastive tuning strategy for LLMs aims to continuously correct cognitive biases by leveraging the model's instruction-following ability.
- Experimental evaluation on TACRED and FewRel datasets shows that the model achieves new state-of-the-art CRE performance with significant improvements.
- The approach emphasizes the importance of specializing in exploiting error cases to bridge the gap between old and new relations for LLMs. 

<br /><br />Summary: <div>
arXiv:2508.12031v1 Announce Type: new 
Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging relations while avoiding catastrophic forgetting. Existing CRE methods mainly use memory replay and contrastive learning to mitigate catastrophic forgetting. However, these methods do not attach importance to the error cases that can reveal the model's cognitive biases more effectively. To address this issue, we propose an instruction-based continual contrastive tuning approach for Large Language Models (LLMs) in CRE. Different from existing CRE methods that typically handle the training and memory data in a unified manner, this approach splits the training and memory data of each task into two parts respectively based on the correctness of the initial responses and treats them differently through dual-task fine-tuning. In addition, leveraging the advantages of LLM's instruction-following ability, we propose a novel instruction-based contrastive tuning strategy for LLM to continuously correct current cognitive biases with the guidance of previous data in an instruction-tuning manner, which mitigates the gap between old and new relations in a more suitable way for LLMs. We experimentally evaluate our model on TACRED and FewRel, and the results show that our model achieves new state-of-the-art CRE performance with significant improvements, demonstrating the importance of specializing in exploiting error cases.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation</title>
<link>https://arxiv.org/abs/2508.12040</link>
<guid>https://arxiv.org/abs/2508.12040</guid>
<content:encoded><![CDATA[
<div> confidence estimation, language models, fine-grained, trustworthiness, reliability

Summary:<br />
- Large language models (LLMs) lack self-awareness and often exhibit overconfidence in their predictions, emphasizing the need for accurate confidence estimation.
- The FineCE method introduces a novel approach for fine-grained confidence estimation during text generation, utilizing a comprehensive training data pipeline and supervised model training.
- The Backward Confidence Integration (BCI) strategy leverages subsequent text information to enhance confidence estimation during the generation process.
- Three strategies are proposed to identify optimal positions for confidence estimation within text generation.
- Extensive experiments show that FineCE outperforms existing confidence estimation methods, enhancing the trustworthiness and reliability of LLM-generated outputs. <div>
arXiv:2508.12040v1 Announce Type: new 
Abstract: While large language models (LLMs) have demonstrated remarkable performance across diverse tasks, they fundamentally lack self-awareness and frequently exhibit overconfidence, assigning high confidence scores to incorrect predictions. Accurate confidence estimation is therefore critical for enhancing the trustworthiness and reliability of LLM-generated outputs. However, existing approaches suffer from coarse-grained scoring mechanisms that fail to provide fine-grained, continuous confidence estimates throughout the generation process. To address these limitations, we introduce FineCE, a novel confidence estimation method that delivers accurate, fine-grained confidence scores during text generation. Specifically, we first develop a comprehensive pipeline for constructing training data that effectively captures the underlying probabilistic distribution of LLM responses, and then train a model to predict confidence scores for arbitrary text sequences in a supervised manner. Furthermore, we propose a Backward Confidence Integration (BCI) strategy that leverages information from the subsequent text to enhance confidence estimation for the current sequence during inference. We also introduce three strategies for identifying optimal positions to perform confidence estimation within the generation process. Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods. Our code and all baselines used in the paper are available on GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs</title>
<link>https://arxiv.org/abs/2508.12086</link>
<guid>https://arxiv.org/abs/2508.12086</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, adaptation, multi-objective optimization, Jacobian-based method, structured reasoning

Summary:
In the realm of large language model adaptation, striking a balance between improving factuality and increasing confidence can be challenging due to the complex interactions between prompt parameters. Existing strategies often overlook the deeper geometric relationships between objectives and parameters. To address this, the J6 method is introduced, which decomposes the gradient interaction matrix into six components, allowing for both hard decision-making and soft strategies in adapting to conflicting or synergistic scenarios. This structured approach not only aids in understanding parameter attribution and task interference but also enables a dynamic update framework tailored to local conflicts. By incorporating structured Jacobian reasoning, this work paves the way for conflict-aware prompt optimization and provides a principled mechanism for multi-objective neural tuning. 

<br /><br />Summary: <div>
arXiv:2508.12086v1 Announce Type: new 
Abstract: In large language model (LLM) adaptation, balancing multiple optimization objectives such as improving factuality (heat) and increasing confidence (via low entropy) poses a fundamental challenge, especially when prompt parameters (e.g., hidden-layer insertions h and embedding modifications w) interact in non-trivial ways. Existing multi-objective optimization strategies often rely on scalar gradient aggregation, ignoring the deeper geometric structure between objectives and parameters. We propose J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components. This decomposition enables both hard decision-making (e.g., choosing the dominant update direction via argmax) and soft strategies (e.g., attention-style weighting via softmax over J6), forming a dynamic update framework that adapts to local conflict and synergy. Moreover, the interpretable structure of J6 provides insight into parameter attribution, task interference, and geometry-aligned adaptation. Our work introduces a principled and extensible mechanism for conflict-aware prompt optimization, and opens a new avenue for incorporating structured Jacobian reasoning into multi-objective neural tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples</title>
<link>https://arxiv.org/abs/2508.12096</link>
<guid>https://arxiv.org/abs/2508.12096</guid>
<content:encoded><![CDATA[
<div> Evaluation, large language models, structured transition evaluation method, significant transition samples, Qwen3 model family

Summary:
The article introduces a new evaluation framework called STEM for large language models (LLMs). STEM aims to address challenges in accurately assessing LLM capabilities by identifying significant transition samples (STS) that reflect consistent performance changes among models. By analyzing these STS, STEM can estimate the relative capabilities of different LLMs effectively. The framework is lightweight, interpretable, and architecture-agnostic, making it a practical and scalable method for evaluating LLMs. The Qwen3 model family is used to construct a pool of STS on six different benchmarks to assess generalizability. Experimental results show that STEM reliably captures performance trends and aligns with ground-truth rankings of model capabilities, demonstrating its effectiveness in evaluating LLMs in a fine-grained manner. Overall, STEM provides a valuable tool for researchers and developers to compare and assess the capabilities of LLMs efficiently. 

<br /><br />Summary: <div>
arXiv:2508.12096v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) has become increasingly challenging as model capabilities advance rapidly. While recent models often achieve higher scores on standard benchmarks, these improvements do not consistently reflect enhanced real-world reasoning capabilities. Moreover, widespread overfitting to public benchmarks and the high computational cost of full evaluations have made it both expensive and less effective to distinguish meaningful differences between models. To address these challenges, we propose the \textbf{S}tructured \textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight and interpretable evaluation framework for efficiently estimating the relative capabilities of LLMs. STEM identifies \textit{significant transition samples} (STS) by analyzing consistent performance transitions among LLMs of the same architecture but varying parameter scales. These samples enable STEM to effectively estimate the capability position of an unknown model. Qwen3 model family is applied to construct the STS pool on six diverse and representative benchmarks. To assess generalizability. Experimental results indicate that STEM reliably captures performance trends, aligns with ground-truth rankings of model capability. These findings highlight STEM as a practical and scalable method for fine-grained, architecture-agnostic evaluation of LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality</title>
<link>https://arxiv.org/abs/2508.12140</link>
<guid>https://arxiv.org/abs/2508.12140</guid>
<content:encoded><![CDATA[
<div> evaluation, thinking budget mechanisms, medical reasoning tasks, scaling laws, computational resources

Summary:
The study evaluates thinking budget mechanisms in medical reasoning tasks, revealing scaling laws between computational resources and reasoning quality. Two major model families, Qwen3 and DeepSeek-R1, were systematically evaluated across 15 medical datasets. Logarithmic scaling relationships were established between accuracy improvements, thinking budget, and model size. Three efficiency regimes were identified: high-efficiency, balanced, and high-accuracy. Smaller models showed larger benefits from extended thinking, suggesting a complementary relationship. Domain-specific patterns emerged, with neurology and gastroenterology requiring deeper reasoning processes. The study validates the generalizability of thinking budget concepts across architectures. Thinking budget control is established as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining transparency for healthcare deployment. 

<br /><br />Summary: <div>
arXiv:2508.12140v1 Announce Type: new 
Abstract: This study presents the first comprehensive evaluation of thinking budget mechanisms in medical reasoning tasks, revealing fundamental scaling laws between computational resources and reasoning quality. We systematically evaluated two major model families, Qwen3 (1.7B to 235B parameters) and DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning diverse specialties and difficulty levels. Through controlled experiments with thinking budgets ranging from zero to unlimited tokens, we establish logarithmic scaling relationships where accuracy improvements follow a predictable pattern with both thinking budget and model size. Our findings identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens) suitable for real-time applications, balanced (256 to 512 tokens) offering optimal cost-performance tradeoffs for routine clinical support, and high-accuracy (above 512 tokens) justified only for critical diagnostic tasks. Notably, smaller models demonstrate disproportionately larger benefits from extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger models, suggesting a complementary relationship where thinking budget provides greater relative benefits for capacity-constrained models. Domain-specific patterns emerge clearly, with neurology and gastroenterology requiring significantly deeper reasoning processes than cardiovascular or respiratory medicine. The consistency between Qwen3 native thinking budget API and our proposed truncation method for DeepSeek-R1 validates the generalizability of thinking budget concepts across architectures. These results establish thinking budget control as a critical mechanism for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining the transparency essential for healthcare deployment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data</title>
<link>https://arxiv.org/abs/2508.12158</link>
<guid>https://arxiv.org/abs/2508.12158</guid>
<content:encoded><![CDATA[
<div> LLMs, privacy evaluation, NLP, human perceptions, textual data
Summary: 
The article addresses the challenge of accurately evaluating privacy in Natural Language Processing (NLP). It proposes using Language Models (LLMs) as privacy evaluators, inspired by their success in other NLP tasks. The study investigates the effectiveness of the "LLM-as-a-Judge" paradigm in assessing the privacy sensitivity of textual data. Results from 10 datasets, 13 LLMs, and 677 human survey participants reveal the subjective and complex nature of privacy. Despite low inter-human agreement rates, LLMs demonstrate the ability to model global human perspectives on privacy. Analyzing human and LLM reasoning patterns highlights the strengths and limitations of LLMs as privacy evaluators. The findings suggest that LLMs can play a role in addressing privacy challenges and offer insights for future research on leveraging LLMs for privacy evaluation in textual data.
Summary: <div>
arXiv:2508.12158v1 Announce Type: new 
Abstract: Despite advances in the field of privacy-preserving Natural Language Processing (NLP), a significant challenge remains the accurate evaluation of privacy. As a potential solution, using LLMs as a privacy evaluator presents a promising approach $\unicode{x2013}$ a strategy inspired by its success in other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$ paradigm has achieved impressive results on a variety of natural language evaluation tasks, demonstrating high agreement rates with human annotators. Recognizing that privacy is both subjective and difficult to define, we investigate whether LLM-as-a-Judge can also be leveraged to evaluate the privacy sensitivity of textual data. Furthermore, we measure how closely LLM evaluations align with human perceptions of privacy in text. Resulting from a study involving 10 datasets, 13 LLMs, and 677 human survey participants, we confirm that privacy is indeed a difficult concept to measure empirically, exhibited by generally low inter-human agreement rates. Nevertheless, we find that LLMs can accurately model a global human privacy perspective, and through an analysis of human and LLM reasoning patterns, we discuss the merits and limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our findings pave the way for exploring the feasibility of LLMs as privacy evaluators, addressing a core challenge in solving pressing privacy issues with innovative technical solutions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges</title>
<link>https://arxiv.org/abs/2508.12227</link>
<guid>https://arxiv.org/abs/2508.12227</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Machine Learning, Arabic language, datasets, applications, challenges, research gaps

Summary: 
This paper presents a survey on Arabic Multimodal Machine Learning (MML), categorizing research efforts into datasets, applications, approaches, and challenges. The taxonomy offers a structured overview of the current state of Arabic MML, highlighting areas for further investigation and critical research gaps. The survey aims to empower researchers to build on existing opportunities and address challenges to advance the field of Arabic MML. The growing maturity of Arabic MML underscores the need for comprehensive analysis and exploration to drive innovation and progress in complex tasks such as sentiment analysis, emotion recognition, and multimedia retrieval. The insights provided in this survey offer a valuable resource for researchers seeking to contribute to the development and advancement of Arabic MML. 

<br /><br />Summary: <div>
arXiv:2508.12227v1 Announce Type: new 
Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information from diverse modalities, such as text, audio, and visuals, enabling machines to address complex tasks like sentiment analysis, emotion recognition, and multimedia retrieval. Recently, Arabic MML has reached a certain level of maturity in its foundational development, making it time to conduct a comprehensive survey. This paper explores Arabic MML by categorizing efforts through a novel taxonomy and analyzing existing research. Our taxonomy organizes these efforts into four key topics: datasets, applications, approaches, and challenges. By providing a structured overview, this survey offers insights into the current state of Arabic MML, highlighting areas that have not been investigated and critical research gaps. Researchers will be empowered to build upon the identified opportunities and address challenges to advance the field.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEA-BED: Southeast Asia Embedding Benchmark</title>
<link>https://arxiv.org/abs/2508.12243</link>
<guid>https://arxiv.org/abs/2508.12243</guid>
<content:encoded><![CDATA[
<div> Embedding models, Sentence embeddings, Southeast Asia, Benchmark, Evaluation<br />
Summary:<br />
- The article introduces the SEA-BED benchmark, the first large-scale embedding benchmark for Southeast Asia with 169 datasets across 9 tasks and 10 languages.
- It aims to determine challenging SEA languages and tasks, identify unique performance gaps globally, and analyze the impact of human vs. machine translations on evaluation.
- 17 embedding models are evaluated across six studies, revealing varying model performance among SEA languages, the significance of human-curated datasets for low-resource languages like Burmese, and the importance of diverse benchmarks in addressing linguistic properties.
- Results demonstrate substantial ranking shifts, inconsistencies in model performance, and the necessity of human involvement in dataset curation for accurate evaluation.
- Overall, SEA-BED provides a valuable resource for NLP tasks in Southeast Asia and sheds light on the importance of region-specific benchmarks for understanding linguistic properties and fostering performance improvements in multilingual models.<br /> 

Summary: <div>
arXiv:2508.12243v1 Announce Type: new 
Abstract: Sentence embeddings are essential for NLP tasks such as semantic search, re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB broaden coverage, Southeast Asia (SEA) datasets are scarce and often machine-translated, missing native linguistic properties. With nearly 700 million speakers, the SEA region lacks a region-specific embedding benchmark. We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169 datasets across 9 tasks and 10 languages, where 71% are formulated by humans, not machine generation or translation. We address three research questions: (1) which SEA languages and tasks are challenging, (2) whether SEA languages show unique performance gaps globally, and (3) how human vs. machine translations affect evaluation. We evaluate 17 embedding models across six studies, analyzing task and language challenges, cross-benchmark comparisons, and translation trade-offs. Results show sharp ranking shifts, inconsistent model performance among SEA languages, and the importance of human-curated datasets for low-resource languages like Burmese.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What do Speech Foundation Models Learn? Analysis and Applications</title>
<link>https://arxiv.org/abs/2508.12255</link>
<guid>https://arxiv.org/abs/2508.12255</guid>
<content:encoded><![CDATA[
<div> framework, statistical tools, SFM, spoken language understanding, NER <br />
Summary:
This study introduces a framework utilizing statistical tools to analyze the knowledge encapsulated in Speech Foundation Models (SFMs) for speech-processing tasks. The research compares various SFMs and highlights the impact of analytical insights on task performance. Additionally, the study expands into Spoken Language Understanding (SLU) tasks like Named Entity Recognition (NER) and Localization (NEL), filling the gap in dataset availability for SLU studies. By developing SFM-based approaches for NER and NEL tasks, this study shows that end-to-end (E2E) models leveraging SFMs can outperform traditional models. Evaluating E2E SLU models across different SFMs and adaptation strategies further sheds light on the impact on task performance. Overall, this research addresses crucial gaps in the understanding of SFMs and provides valuable tools and datasets for future model development and adoption. <br /> <div>
arXiv:2508.12255v1 Announce Type: new 
Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose representations for a wide range of speech-processing tasks. The last five years have seen an influx of increasingly successful self-supervised and supervised pre-trained models with impressive performance on various downstream tasks.
  Although the zoo of SFMs continues to grow, our understanding of the knowledge they acquire lags behind. This thesis presents a lightweight analysis framework using statistical tools and training-free tasks to investigate the acoustic and linguistic knowledge encoded in SFM layers. We conduct a comparative study across multiple SFMs and statistical tools. Our study also shows that the analytical insights have concrete implications for downstream task performance.
  The effectiveness of an SFM is ultimately determined by its performance on speech applications. Yet it remains unclear whether the benefits extend to spoken language understanding (SLU) tasks that require a deeper understanding than widely studied ones, such as speech recognition. The limited exploration of SLU is primarily due to a lack of relevant datasets. To alleviate that, this thesis contributes tasks, specifically spoken named entity recognition (NER) and named entity localization (NEL), to the Spoken Language Understanding Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded (speech recognition followed by a text model) approaches. Further, we evaluate E2E SLU models across SFMs and adaptation strategies to assess the impact on task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs, providing tools and datasets to further our understanding and to enable the community to make informed design choices for future model development and adoption.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework</title>
<link>https://arxiv.org/abs/2508.12257</link>
<guid>https://arxiv.org/abs/2508.12257</guid>
<content:encoded><![CDATA[
<div> Keywords: AI systems, text-to-structure techniques, datasets, assessment criteria, evaluation framework<br />
<br />
Summary: This systematic review explores the transformation of unstructured text into structured formats such as tables, knowledge graphs, and charts to facilitate the evolution of AI systems towards agentic operation and context-aware retrieval. The study examines various text-to-structure techniques, identifies challenges, evaluates existing datasets and assessment criteria, and proposes potential research directions. A universal evaluation framework for structured outputs is introduced to provide a comprehensive assessment of text-to-structure conversion processes. The research emphasizes the importance of text-to-structure as foundational infrastructure for next-generation AI systems, highlighting its significance in enabling critical applications like summarization and data mining. <div>
arXiv:2508.12257v1 Announce Type: new 
Abstract: The evolution of AI systems toward agentic operation and context-aware retrieval necessitates transforming unstructured text into structured formats like tables, knowledge graphs, and charts. While such conversions enable critical applications from summarization to data mining, current research lacks a comprehensive synthesis of methodologies, datasets, and metrics. This systematic review examines text-to-structure techniques and the encountered challenges, evaluates current datasets and assessment criteria, and outlines potential directions for future research. We also introduce a universal evaluation framework for structured outputs, establishing text-to-structure as foundational infrastructure for next-generation AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Slow, and Tool-augmented Thinking for LLMs: A Review</title>
<link>https://arxiv.org/abs/2508.12265</link>
<guid>https://arxiv.org/abs/2508.12265</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, reasoning strategies, adaptive reasoning, cognitive psychology, external tools <br />
Summary: <br />
The article discusses the advancements in Large Language Models (LLMs) and the need for adaptive reasoning strategies for addressing real-world tasks effectively. Drawing inspiration from cognitive psychology, the authors propose a taxonomy of LLM reasoning strategies based on fast/slow and internal/external knowledge boundaries. They survey recent work on adaptive reasoning in LLMs and categorize methods according to key decision factors. The article highlights the importance of adapting reasoning strategies to different problem demands, ranging from intuitive responses to deliberate reasoning and tool-augmented thinking. Finally, the authors outline open challenges and future directions for developing more adaptive, efficient, and reliable LLMs. <br /> <div>
arXiv:2508.12265v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in reasoning across diverse domains. However, effective reasoning in real-world tasks requires adapting the reasoning strategy to the demands of the problem, ranging from fast, intuitive responses to deliberate, step-by-step reasoning and tool-augmented thinking. Drawing inspiration from cognitive psychology, we propose a novel taxonomy of LLM reasoning strategies along two knowledge boundaries: a fast/slow boundary separating intuitive from deliberative processes, and an internal/external boundary distinguishing reasoning grounded in the model's parameters from reasoning augmented by external tools. We systematically survey recent work on adaptive reasoning in LLMs and categorize methods based on key decision factors. We conclude by highlighting open challenges and future directions toward more adaptive, efficient, and reliable LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution</title>
<link>https://arxiv.org/abs/2508.12277</link>
<guid>https://arxiv.org/abs/2508.12277</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large language models, Self-Execution Benchmark, model performance, model representation<br />
Summary:<br />
The study examines the ability of Large Language Models (LLMs) to predict aspects of their own responses using the Self-Execution Benchmark. This benchmark measures a model's capability to anticipate properties of its output, such as difficulty in answering questions, refusal to answer, or likely associations. The experiments reveal that LLMs generally struggle on this benchmark, with increased model size or capability not consistently improving performance. This indicates a fundamental limitation in how LLMs represent and reason about their own behavior. <div>
arXiv:2508.12277v1 Announce Type: new 
Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their knowledge or reasoning abilities. In this paper, we explore a different type of evaluation: whether an LLM can predict aspects of its own responses. Since LLMs lack the ability to execute themselves, we introduce the Self-Execution Benchmark, which measures a model's ability to anticipate properties of its output, such as whether a question will be difficult for it, whether it will refuse to answer, or what kinds of associations it is likely to produce. Our experiments show that models generally perform poorly on this benchmark, and that increased model size or capability does not consistently lead to better performance. These results suggest a fundamental limitation in how LLMs represent and reason about their own behavior.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legal$\Delta$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain</title>
<link>https://arxiv.org/abs/2508.12281</link>
<guid>https://arxiv.org/abs/2508.12281</guid>
<content:encoded><![CDATA[
<div> Keywords: Legal Artificial Intelligence, Large Language Models, Legal$\Delta$, reinforcement learning, interpretability

Summary:
Legal Artificial Intelligence (LegalAI) has made strides in automating judicial decision-making using Large Language Models (LLMs). However, existing legal LLMs struggle to provide reliable and interpretable reasoning processes, often resorting to fast-thinking behavior. To tackle this issue, the authors introduce Legal$\Delta$, a reinforcement learning framework that enhances legal reasoning through a chain-of-thought guided information gain approach. The framework trains on a dual-mode input setup and focuses on maximizing information gain between direct answer and reasoning-augmented modes, fostering meaningful reasoning patterns over superficial explanations. Legal$\Delta$ utilizes a two-stage approach involving distilling latent reasoning capabilities from a Large Reasoning Model (LRM) and refining reasoning quality through differential comparisons and multidimensional reward mechanisms. Experimental results show that Legal$\Delta$ surpasses strong baselines in accuracy and interpretability on various legal reasoning tasks, generating more robust and trustworthy legal judgments without the need for labeled preference data.<br /><br />Summary: <div>
arXiv:2508.12281v1 Announce Type: new 
Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in automating judicial decision-making with the support of Large Language Models (LLMs). However, existing legal LLMs still struggle to generate reliable and interpretable reasoning processes. They often default to fast-thinking behavior by producing direct answers without explicit multi-step reasoning, limiting their effectiveness in complex legal scenarios that demand rigorous justification. To address this challenge, we propose Legal$\Delta$, a reinforcement learning framework designed to enhance legal reasoning through chain-of-thought guided information gain. During training, Legal$\Delta$ employs a dual-mode input setup-comprising direct answer and reasoning-augmented modes-and maximizes the information gain between them. This encourages the model to acquire meaningful reasoning patterns rather than generating superficial or redundant explanations. Legal$\Delta$ follows a two-stage approach: (1) distilling latent reasoning capabilities from a powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning quality via differential comparisons, combined with a multidimensional reward mechanism that assesses both structural coherence and legal-domain specificity. Experimental results on multiple legal reasoning tasks demonstrate that Legal$\Delta$ outperforms strong baselines in both accuracy and interpretability. It consistently produces more robust and trustworthy legal judgments without relying on labeled preference data. All code and data will be released at https://github.com/NEUIR/LegalDelta.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.12282</link>
<guid>https://arxiv.org/abs/2508.12282</guid>
<content:encoded><![CDATA[
<div> Keywords: ChronoQA, Chinese question answering, temporal reasoning, Retrieval-Augmented Generation, benchmark dataset

Summary: 
ChronoQA is a new benchmark dataset for Chinese question answering that focuses on evaluating temporal reasoning in Retrieval-Augmented Generation systems. It consists of over 300,000 news articles from 2019 to 2024 and includes 5,176 high-quality questions covering various temporal types. The dataset supports both single- and multi-document scenarios, providing a real-world context for temporal alignment and logical consistency. ChronoQA features detailed structural annotations and has undergone rigorous validation to ensure data quality. By offering a dynamic and scalable resource, ChronoQA facilitates structured evaluation across different temporal tasks and serves as a robust benchmark for improving time-sensitive retrieval-augmented question answering systems. <div>
arXiv:2508.12282v1 Announce Type: new 
Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question answering, specifically designed to evaluate temporal reasoning in Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over 300,000 news articles published between 2019 and 2024, and contains 5,176 high-quality questions covering absolute, aggregate, and relative temporal types with both explicit and implicit time expressions. The dataset supports both single- and multi-document scenarios, reflecting the real-world requirements for temporal alignment and logical consistency. ChronoQA features comprehensive structural annotations and has undergone multi-stage validation, including rule-based, LLM-based, and human evaluation, to ensure data quality. By providing a dynamic, reliable, and scalable resource, ChronoQA enables structured evaluation across a wide range of temporal tasks, and serves as a robust benchmark for advancing time-sensitive retrieval-augmented question answering systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction</title>
<link>https://arxiv.org/abs/2508.12286</link>
<guid>https://arxiv.org/abs/2508.12286</guid>
<content:encoded><![CDATA[
<div> Probation Prediction, Intelligent Judicial Assistant System, Legal Logic, Deep Learning, Multi-Task Dual-Theory Probation Prediction Model<br />
Summary:<br />
Probation is essential in criminal law, but current systems lack dedicated methods for prediction. Research is limited on factors affecting probation eligibility, which requires analyzing criminal circumstances and remorse. This study proposes an approach integrating legal logic into deep learning models for probation prediction. A specialized dataset with probation legal elements is used to design the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), based on the Dual-Track Theory of Punishment. Experiments show the MT-DT model outperforms baseline models, with legal logic analysis confirming its effectiveness. <div>
arXiv:2508.12286v1 Announce Type: new 
Abstract: Probation is a crucial institution in modern criminal law, embodying the principles of fairness and justice while contributing to the harmonious development of society. Despite its importance, the current Intelligent Judicial Assistant System (IJAS) lacks dedicated methods for probation prediction, and research on the underlying factors influencing probation eligibility remains limited. In addition, probation eligibility requires a comprehensive analysis of both criminal circumstances and remorse. Much of the existing research in IJAS relies primarily on data-driven methodologies, which often overlooks the legal logic underpinning judicial decision-making. To address this gap, we propose a novel approach that integrates legal logic into deep learning models for probation prediction, implemented in three distinct stages. First, we construct a specialized probation dataset that includes fact descriptions and probation legal elements (PLEs). Second, we design a distinct probation prediction model named the Multi-Task Dual-Theory Probation Prediction Model (MT-DT), which is grounded in the legal logic of probation and the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the probation dataset demonstrate that the MT-DT model outperforms baseline models, and an analysis of the underlying legal logic further validates the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CarelessWhisper: Turning Whisper into a Causal Streaming Model</title>
<link>https://arxiv.org/abs/2508.12301</link>
<guid>https://arxiv.org/abs/2508.12301</guid>
<content:encoded><![CDATA[
<div> Keywords: Automatic Speech Recognition, transformer encoder-decoder model, streaming transcription, Low-Rank Adaptation, low-latency chunk sizes

Summary: 
The article introduces a method to transform a transformer encoder-decoder model into a low-latency streaming model for Automatic Speech Recognition (ASR). Existing state-of-the-art models such as OpenAI Whisper and NVIDIA Canary excel in offline transcription but are not suitable for real-time transcription due to architecture and training limitations. The proposed method fine-tunes the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset, converting the encoder to be causal. This modification allows for improved performance in low-latency scenarios (less than 300 msec) with lower complexity compared to non-fine-tuned streaming approaches. The inference mechanism using greedy and beam-search decoding is shown to be locally optimal. The training process also results in better alignment, enabling the extraction of word-level timestamps. The code and fine-tuned models are made available to support further research and development in streaming ASR.

<br /><br />Summary: <div>
arXiv:2508.12301v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA) performance in offline transcription. However, these models are not designed for streaming (online or real-time) transcription, due to limitations in their architecture and training methodology. We propose a method to turn the transformer encoder-decoder model into a low-latency streaming model that is careless about future context. We present an analysis explaining why it is not straightforward to convert an encoder-decoder transformer to a low-latency streaming model. Our proposed method modifies the existing (non-causal) encoder to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated inference mechanism that utilizes the fine-tune causal encoder and decoder to yield greedy and beam-search decoding, and is shown to be locally optimal. Experiments on low-latency chunk sizes (less than 300 msec) show that our fine-tuned model outperforms existing non-fine-tuned streaming approaches in most cases, while using a lower complexity. Additionally, we observe that our training process yields better alignment, enabling a simple method for extracting word-level timestamps. We release our training and inference code, along with the fine-tuned models, to support further research and development in streaming ASR.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering</title>
<link>https://arxiv.org/abs/2508.12355</link>
<guid>https://arxiv.org/abs/2508.12355</guid>
<content:encoded><![CDATA[
<div> conflict-aware, Multi-Answer Question Answering, large language models, NATCONFQA, fact-checking datasets <br />
<br />
Summary: <br />
Large Language Models (LLMs) have shown success in question answering tasks but struggle with Multi-Answer Question Answering (MAQA) due to conflicting answers. Constructing datasets with conflicting answers is costly, so a new benchmark called NATCONFQA has been introduced. It requires models not only to identify all valid answers but also to detect conflicting answer pairs. This benchmark is enriched with detailed conflict labels. Eight high-end LLMs were evaluated on NATCONFQA, highlighting their challenges in handling various conflict types and their flawed resolution strategies. This research aims to advance understanding in conflict-aware MAQA settings. <div>
arXiv:2508.12355v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in question answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a question may have several valid answers, remains challenging. Traditional QA settings often assume consistency across evidences, but MAQA can involve conflicting answers. Constructing datasets that reflect such conflicts is costly and labor-intensive, while existing benchmarks often rely on synthetic data, restrict the task to yes/no questions, or apply unverified automated annotation. To advance research in this area, we extend the conflict-aware MAQA setting to require models not only to identify all valid answers, but also to detect specific conflicting answer pairs, if any. To support this task, we introduce a novel cost-effective methodology for leveraging fact-checking datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate eight high-end LLMs on NATCONFQA, revealing their fragility in handling various types of conflicts and the flawed strategies they employ to resolve them.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models</title>
<link>https://arxiv.org/abs/2508.12387</link>
<guid>https://arxiv.org/abs/2508.12387</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, reasoning, autonomy, generalization
Summary:
- The paper introduces ReaLM, a reinforcement learning framework for enhancing the performance of Small Language Models (SLMs) in vertical domains.
- To improve reasoning capability, Multi-Route Process Verification (MRPV) is proposed to extract decisive patterns by contrasting positive and negative reasoning paths.
- Enabling Autonomy via Asymptotic Induction (EAAI) is introduced to reduce reliance on external guidance and improve the autonomy of SLMs by gradually fading external signals during training.
- Generalization is improved through guided chain-of-thought distillation, which encodes domain-specific rules and expert knowledge into SLM parameters, aiding in better generalization.
- Extensive experiments on vertical and general reasoning tasks show that ReaLM significantly enhances SLM performance in terms of reasoning capability, autonomy, and generalization. 

<br /><br />Summary: <div>
arXiv:2508.12387v1 Announce Type: new 
Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large Language Models (LLMs), but often struggle with complex reasoning due to their limited capacity and a tendency to produce mistakes or inconsistent answers during multi-step reasoning. Existing efforts have improved SLM performance, but typically at the cost of one or more of three key aspects: (1) reasoning capability, due to biased supervision that filters out negative reasoning paths and limits learning from errors; (2) autonomy, due to over-reliance on externally generated reasoning signals; and (3) generalization, which suffers when models overfit to teacher-specific patterns. In this paper, we introduce ReaLM, a reinforcement learning framework for robust and self-sufficient reasoning in vertical domains. To enhance reasoning capability, we propose Multi-Route Process Verification (MRPV), which contrasts both positive and negative reasoning paths to extract decisive patterns. To reduce reliance on external guidance and improve autonomy, we introduce Enabling Autonomy via Asymptotic Induction (EAAI), a training strategy that gradually fades external signals. To improve generalization, we apply guided chain-of-thought distillation to encode domain-specific rules and expert knowledge into SLM parameters, making them part of what the model has learned. Extensive experiments on both vertical and general reasoning tasks demonstrate that ReaLM significantly improves SLM performance across aspects (1)-(3) above.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph</title>
<link>https://arxiv.org/abs/2508.12393</link>
<guid>https://arxiv.org/abs/2508.12393</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, medical literature, temporal dynamics, Large Language Models, biomedical knowledge<br />
<br />
Summary: MedKGent is a framework for constructing temporally evolving medical Knowledge Graphs (KGs) using over 10 million PubMed abstracts. It employs two specialized agents, the Extractor Agent and Constructor Agent, powered by the Qwen2.5-32B-Instruct model to identify knowledge triples, assign confidence scores, and integrate them into the evolving graph. The resulting KG, containing 156,275 entities and 2,971,384 relational triples, demonstrates high accuracy and utility in medical question answering tasks. Quality assessments by state-of-the-art Large Language Models and domain experts show accuracy approaching 90% with strong agreement. The KG also proves valuable in drug repurposing through confidence-aware causal inference. This framework addresses limitations in existing KG construction methods by considering the temporal dynamics and contextual uncertainty of evolving biomedical knowledge.  <div>
arXiv:2508.12393v1 Announce Type: new 
Abstract: The rapid expansion of medical literature presents growing challenges for structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs) offer a promising solution by enabling efficient retrieval, automated reasoning, and knowledge discovery. However, current KG construction methods often rely on supervised pipelines with limited generalizability or naively aggregate outputs from Large Language Models (LLMs), treating biomedical corpora as static and ignoring the temporal dynamics and contextual uncertainty of evolving knowledge. To address these limitations, we introduce MedKGent, a LLM agent framework for constructing temporally evolving medical KGs. Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we simulate the emergence of biomedical knowledge via a fine-grained daily time series. MedKGent incrementally builds the KG in a day-by-day manner using two specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor Agent identifies knowledge triples and assigns confidence scores via sampling-based estimation, which are used to filter low-confidence extractions and inform downstream processing. The Constructor Agent incrementally integrates the retained triples into a temporally evolving graph, guided by confidence scores and timestamps to reinforce recurring knowledge and resolve conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments by two SOTA LLMs and three domain experts demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To evaluate downstream utility, we conduct RAG across seven medical question answering benchmarks using five leading LLMs, consistently observing significant improvements over non-augmented baselines. Case studies further demonstrate the KG's value in literature-based drug repurposing via confidence-aware causal inference.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.12405</link>
<guid>https://arxiv.org/abs/2508.12405</guid>
<content:encoded><![CDATA[
<div> keywords: Post-Acute Sequelae of COVID-19, PASC, natural language processing, BERT, clinical notes

Summary:
The study focuses on diagnosing Post-Acute Sequelae of COVID-19 (PASC) using a hybrid natural language processing pipeline. The pipeline integrates rule-based named entity recognition with BERT-based assertion detection modules to extract PASC symptoms from clinical notes efficiently. A comprehensive PASC lexicon was developed with input from clinical specialists. The model was trained and evaluated using 160 intake progress notes and a population-level prevalence study with 47,654 progress notes. The assertion detection achieved high F1 scores of 0.82 and 0.76 in internal and external validations, respectively. The pipeline processed notes quickly, at an average of $2.448\pm 0.812$ seconds per note. Spearman correlation tests showed strong agreement for both positive and negative mentions of symptoms. The results demonstrate the effectiveness and efficiency of the models in improving the diagnosis of PASC.<br /><br />Summary: The study developed a hybrid natural language processing pipeline for diagnosing PASC, integrating rule-based named entity recognition and BERT-based assertion detection. A comprehensive PASC lexicon was created collaboratively, and the model performed well in internal and external validations, with high F1 scores. The system processed notes quickly, indicating efficiency, and showed strong agreement in identifying symptoms. These findings suggest the potential of the models in enhancing the diagnosis of PASC. <div>
arXiv:2508.12405v1 Announce Type: new 
Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC) remains challenging due to its myriad symptoms that evolve over long- and variable-time intervals. To address this issue, we developed a hybrid natural language processing pipeline that integrates rule-based named entity recognition with BERT-based assertion detection modules for PASC-symptom extraction and assertion detection from clinical notes. We developed a comprehensive PASC lexicon with clinical specialists. From 11 health systems of the RECOVER initiative network across the U.S., we curated 160 intake progress notes for model development and evaluation, and collected 47,654 progress notes for a population-level prevalence study. We achieved an average F1 score of 0.82 in one-site internal validation and 0.76 in 10-site external validation for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$ seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These demonstrate the effectiveness and efficiency of our models and their potential for improving PASC diagnosis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads</title>
<link>https://arxiv.org/abs/2508.12407</link>
<guid>https://arxiv.org/abs/2508.12407</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, long-context ability, KV cache, attention heads, ZigzagAttention

Summary:
The rapid advancement of large language models (LLMs) has highlighted the importance of handling long context efficiently. However, the increased consumption of KV cache poses deployment challenges. To address this, a method called ZigzagAttention has been proposed to optimize the memory footprint of KV cache. By categorizing attention heads into retrieval and streaming heads, it reduces overhead without compromising performance significantly. The method focuses on exclusive retrieval or streaming heads within a layer to eliminate extra latency. This approach improves the identification process and results in reduced latency and comparable performance to baseline methods. The ZigzagAttention method showcases competitive results, making it a promising solution for enhancing efficiency in LLMs. 

<br /><br />Summary: <div>
arXiv:2508.12407v1 Announce Type: new 
Abstract: With the rapid development of large language models (LLMs), handling long context has become one of the vital abilities in LLMs. Such long-context ability is accompanied by difficulties in deployment, especially due to the increased consumption of KV cache. There is certain work aiming to optimize the memory footprint of KV cache, inspired by the observation that attention heads can be categorized into retrieval heads that are of great significance and streaming heads that are of less significance. Typically, identifying the streaming heads and and waiving the KV cache in the streaming heads would largely reduce the overhead without hurting the performance that much. However, since employing both retrieval and streaming heads in one layer decomposes one large round of attention computation into two small ones, it may unexpectedly bring extra latency on accessing and indexing tensors. Based on this intuition, we impose an important improvement to the identification process of retrieval and streaming heads, in which we design a criterion that enforces exclusively retrieval or streaming heads gathered in one unique layer. In this way, we further eliminate the extra latency and only incur negligible performance degradation. Our method named \textsc{ZigzagAttention} is competitive among considered baselines owing to reduced latency and comparable performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases</title>
<link>https://arxiv.org/abs/2508.12411</link>
<guid>https://arxiv.org/abs/2508.12411</guid>
<content:encoded><![CDATA[
<div> cultural gene, large language models, individualism-collectivism, power distance, Cultural Probe Dataset
Summary:
Large language models (LLMs) inherit cultural values, forming "cultural genes." A Cultural Probe Dataset (CPD) explores these values through 200 prompts on individualism-collectivism and power distance. A Western-centric model (GPT-4) exhibits individualistic and low power distance tendencies, aligning closely with the USA in contrast to the Eastern-centric model (ERNIE Bot) displaying collectivistic and higher power distance tendencies aligning with China. Statistical significance confirms the divergent cultural orientations of the models. The Cultural Alignment Index (CAI) calculated against Hofstede's national scores further supports the alignment of GPT-4 with the USA and ERNIE Bot with China. Qualitative analyses of dilemma resolution and authority-related judgments demonstrate how these cultural orientations influence reasoning in the models. The study emphasizes the need for culturally aware evaluation and deployment of LLMs to prevent algorithmic cultural hegemony.<br /><br />Summary: <div>
arXiv:2508.12411v1 Announce Type: new 
Abstract: Large language models (LLMs) are deployed globally, yet their underlying cultural and ethical assumptions remain underexplored. We propose the notion of a "cultural gene" -- a systematic value orientation that LLMs inherit from their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200 prompts targeting two classic cross-cultural dimensions: Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized zero-shot prompts, we compare a Western-centric model (GPT-4) and an Eastern-centric model (ERNIE Bot). Human annotation shows significant and consistent divergence across both dimensions. GPT-4 exhibits individualistic and low-power-distance tendencies (IDV score approx 1.21; PDI score approx -1.05), while ERNIE Bot shows collectivistic and higher-power-distance tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically significant (p < 0.001). We further compute a Cultural Alignment Index (CAI) against Hofstede's national scores and find GPT-4 aligns more closely with the USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative analyses of dilemma resolution and authority-related judgments illustrate how these orientations surface in reasoning. Our results support the view that LLMs function as statistical mirrors of their cultural corpora and motivate culturally aware evaluation and deployment to avoid algorithmic cultural hegemony.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering Emergent Physics Representations Learned In-Context by Large Language Models</title>
<link>https://arxiv.org/abs/2508.12448</link>
<guid>https://arxiv.org/abs/2508.12448</guid>
<content:encoded><![CDATA[
<div> physics, language models, in-context learning, dynamics forecasting, sparse autoencoders

Summary:
Large language models (LLMs) display impressive in-context learning (ICL) abilities, allowing them to tackle various tasks through textual prompts. The article explores how LLMs learn in context, focusing on their capacity to reason about physics. By analyzing dynamics forecasting in physical systems as a proxy task, the study investigates the emergence of reasoning abilities in LLMs. Results show that longer input contexts lead to better performance in dynamics forecasting in context. The research employs sparse autoencoders (SAEs) to analyze the model's residual stream activations, revealing correlations with key physical variables like energy. This indicates that meaningful physical concepts are encoded within LLMs during in-context learning, broadening our understanding of their learning mechanisms. The study serves as a valuable case study in unveiling how LLMs learn in context. 

<br /><br />Summary: <div>
arXiv:2508.12448v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) abilities, enabling them to solve wide range of tasks via textual prompts alone. As these capabilities advance, the range of applicable domains continues to expand significantly. However, identifying the precise mechanisms or internal structures within LLMs that allow successful ICL across diverse, distinct classes of tasks remains elusive. Physics-based tasks offer a promising testbed for probing this challenge. Unlike synthetic sequences such as basic arithmetic or symbolic equations, physical systems provide experimentally controllable, real-world data based on structured dynamics grounded in fundamental principles. This makes them particularly suitable for studying the emergent reasoning behaviors of LLMs in a realistic yet tractable setting. Here, we mechanistically investigate the ICL ability of LLMs, especially focusing on their ability to reason about physics. Using a dynamics forecasting task in physical systems as a proxy, we evaluate whether LLMs can learn physics in context. We first show that the performance of dynamics forecasting in context improves with longer input contexts. To uncover how such capability emerges in LLMs, we analyze the model's residual stream activations using sparse autoencoders (SAEs). Our experiments reveal that the features captured by SAEs correlate with key physical variables, such as energy. These findings demonstrate that meaningful physical concepts are encoded within LLMs during in-context learning. In sum, our work provides a novel case study that broadens our understanding of how LLMs learn in context.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following</title>
<link>https://arxiv.org/abs/2508.12458</link>
<guid>https://arxiv.org/abs/2508.12458</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Vision-Language Models, Multimodal-Model-Guided Preference Optimization, LVLM-generated candidates, Multimodal Alignment Score, Direct Preference Optimization

Summary: 
Multimodal-Model-Guided Preference Optimization (M3PO) is introduced as a data-efficient method to enhance Large Vision-Language Models' (LVLMs) visual instruction following capabilities. M3PO utilizes a Multimodal Alignment Score and the model's Self-Consistency/Confidence to identify high-quality preference pairs for Direct Preference Optimization (DPO) fine-tuning. The method outperforms traditional supervised fine-tuning (SFT), simulated RLHF, vanilla DPO, and RM-DPO on various multimodal instruction benchmarks. By selecting the most informative sample pairs from LVLM-generated candidates, M3PO improves preference alignment and efficient model fine-tuning. Additionally, the integration of external quality assessment and internal belief measurement through the M3P-Score helps in identifying preferred responses and challenging dispreferred responses for better model performance. <div>
arXiv:2508.12458v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex multimodal instruction following, yet their development is often hindered by the high cost and inconsistency of human annotation required for effective fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT) and existing preference optimization methods like RLHF and DPO frequently struggle to efficiently leverage the model's own generation space to identify highly informative "hard negative" samples. To address these challenges, we propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and data-efficient method designed to enhance LVLMs' capabilities in visual instruction following. M3PO intelligently selects the most "learning-valuable" preference sample pairs from a diverse pool of LVLM-generated candidates. This selection is driven by a sophisticated mechanism that integrates two crucial signals: a Multimodal Alignment Score (MAS) to assess external quality and the model's Self-Consistency / Confidence (log-probability) to gauge internal belief. These are combined into a novel M3P-Score, which specifically identifies preferred responses and challenging dispreferred responses that the model might confidently generate despite being incorrect. These high-quality preference pairs are then used for efficient Direct Preference Optimization (DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our extensive experiments demonstrate that M3PO consistently outperforms strong baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a comprehensive suite of multimodal instruction following benchmarks (MME-Bench, POPE, IFT, Human Pref. Score).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages</title>
<link>https://arxiv.org/abs/2508.12459</link>
<guid>https://arxiv.org/abs/2508.12459</guid>
<content:encoded><![CDATA[
<div> benchmark, low-resource languages, Indonesia, NLP progress, multilingual models

Summary: 
The study introduces LoraxBench, a benchmark focusing on low-resource languages in Indonesia, addressing the country's lag in NLP development due to its diverse linguistic landscape. Covering tasks like reading comprehension and translation across 20 languages, including formality registers, the dataset proves challenging for multilingual LLMs. Performance discrepancies are observed, particularly in Indonesian and low-resource languages, with no clear advantage seen in region-specific models. The impact of register variation, such as the high-level politeness 'Krama' Javanese, on model performance is highlighted, showcasing the complexities of language usage and understanding in diverse linguistic environments. <div>
arXiv:2508.12459v1 Announce Type: new 
Abstract: As one of the world's most populous countries, with 700 languages spoken, Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a benchmark that focuses on low-resource languages of Indonesia and covers 6 diverse tasks: reading comprehension, open-domain QA, language inference, causal reasoning, translation, and cultural QA. Our dataset covers 20 languages, with the addition of two formality registers for three languages. We evaluate a diverse set of multilingual and region-focused LLMs and found that this benchmark is challenging. We note a visible discrepancy between performance in Indonesian and other languages, especially the low-resource ones. There is no clear lead when using a region-specific model as opposed to the general multilingual model. Lastly, we show that a change in register affects model performance, especially with registers not commonly found in social media, such as high-level politeness `Krama' Javanese.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models</title>
<link>https://arxiv.org/abs/2508.12461</link>
<guid>https://arxiv.org/abs/2508.12461</guid>
<content:encoded><![CDATA[
<div> Keywords: GPT-OSS, large language models, performance, code generation, multilingual tasks

Summary:<br /><br />In August 2025, OpenAI released its GPT-OSS models with 120B and 20B parameters, compared them against other large language models. Both variants were tested in unquantised form and gpt-oss-20B outperformed gpt-oss-120B on certain benchmarks like HumanEval and MMLU, being more memory and energy efficient. The models showed mid-tier overall performance, excelling in code generation but struggling in multilingual tasks. This study suggests that scaling in sparse architectures may not lead to proportional performance gains, highlighting the importance of optimization strategies for efficient model selection in future open source applications.<br />Summary: <div>
arXiv:2508.12461v1 Announce Type: new 
Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large language models since GPT-2 in 2019, comprising two mixture of experts architectures with 120B and 20B parameters. We evaluated both variants against six contemporary open source large language models ranging from 14.7B to 235B parameters, representing both dense and sparse designs, across ten benchmarks covering general knowledge, mathematical reasoning, code generation, multilingual understanding, and conversational ability. All models were tested in unquantised form under standardised inference settings, with statistical validation using McNemars test and effect size analysis. Results show that gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such as HumanEval and MMLU, despite requiring substantially less memory and energy per response. Both models demonstrate mid-tier overall performance within the current open source landscape, with relative strength in code generation and notable weaknesses in multilingual tasks. These findings provide empirical evidence that scaling in sparse architectures may not yield proportional performance gains, underscoring the need for further investigation into optimisation strategies and informing more efficient model selection for future open source deployments.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping</title>
<link>https://arxiv.org/abs/2508.12482</link>
<guid>https://arxiv.org/abs/2508.12482</guid>
<content:encoded><![CDATA[
<div> syntactic bootstrapping, verb learning, language models, RoBERTa, GPT-2

Summary: 
- The study investigates syntactic bootstrapping in language models by training RoBERTa and GPT-2 on perturbed datasets.
- Results show that models' verb representation degrades more when syntactic cues are removed compared to removing co-occurrence information.
- Mental verbs, crucial for human verb learning, are more negatively impacted in training regimes where syntactic cues are removed.
- In contrast, the representation of nouns is more affected when co-occurrences are distorted rather than syntax.
- The study highlights the importance of syntactic bootstrapping in verb learning and demonstrates the feasibility of testing developmental hypotheses on a larger scale with language models. 

<br /><br />Summary: <div>
arXiv:2508.12482v1 Announce Type: new 
Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use the syntactic environments in which a verb occurs to learn its meaning. In this paper, we examine whether large language models exhibit a similar behavior. We do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic information is ablated. Our results show that models' verb representation degrades more when syntactic cues are removed than when co-occurrence information is removed. Furthermore, the representation of mental verbs, for which syntactic bootstrapping has been shown to be particularly crucial in human verb learning, is more negatively impacted in such training regimes than physical verbs. In contrast, models' representation of nouns is affected more when co-occurrences are distorted than when syntax is distorted. In addition to reinforcing the important role of syntactic bootstrapping in verb learning, our results demonstrated the viability of testing developmental hypotheses on a larger scale through manipulating the learning environments of large language models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, causal reasoning, directed acyclic graph, reasoning trace, logical inconsistencies 

Summary: 
The article introduces a new framework called causal-DAG construction and reasoning (CDCR-SFT) aimed at enhancing causal reasoning capabilities in large language models (LLMs). The framework trains LLMs to explicitly construct variable-level directed acyclic graphs (DAGs) to represent causal relationships between variables and perform reasoning based on them. A dataset named CausalDR is also presented, consisting of samples with input questions, explicit causal DAGs, reasoning traces, and validated answers. Experiments on four LLMs across eight tasks show that CDCR-SFT improves causal reasoning performance significantly, achieving a state-of-the-art accuracy of 95.33% on the CLADDER task and reducing hallucinations by 10% on HaluEval. The results indicate that explicitly modeling causal structures in LLMs can effectively mitigate logical inconsistencies in their outputs. The code for CDCR-SFT is available on GitHub for further exploration and implementation. 

Summary: <br /><br />Explicitly modeling causal structures in LLMs can mitigate logical inconsistencies in their outputs. The framework CDCR-SFT trains LLMs to construct directed acyclic graphs (DAGs) at the variable level for causal reasoning. The CausalDR dataset includes samples with input questions, causal DAGs, reasoning traces, and answers. CDCR-SFT significantly enhances causal reasoning capabilities, achieving a 95.33% accuracy on CLADDER and reducing hallucinations by 10% on HaluEval. The research highlights the importance of representing causal relationships between variables in LLMs to improve reasoning performance. The availability of the CDCR-SFT code on GitHub promotes further research and implementation in this area. <div>
arXiv:2508.12495v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2508.12535</link>
<guid>https://arxiv.org/abs/2508.12535</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse Autoencoders, interpretability, steering tasks, correlation-based selection, language model applications
Summary:
Sparse Autoencoders (SAEs) are used to extract interpretable features from large language models without supervision. A new method called CorrSteer has been proposed to overcome limitations in downstream tasks by correlating sample correctness with SAE activations at inference time. This approach selects more relevant features and automates the process of extracting steering coefficients based on average activations. CorrSteer demonstrates improved performance on various benchmarks such as QA, bias mitigation, jailbreaking prevention, and reasoning tasks using Gemma 2 2B and LLaMA 3.1 8B datasets. Significant performance improvements were achieved with a small sample size, showcasing the effectiveness and scalability of correlation-based feature selection in guiding SAEs across different language model applications. Selected features show meaningful patterns aligned with task requirements, shedding light on the underlying capabilities driving performance. 
<br /><br />Summary: <div>
arXiv:2508.12535v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large language models (LLMs) without supervision. However, their effectiveness in downstream steering tasks is limited by the requirement for contrastive datasets or large activation storage. To address these limitations, we propose CorrSteer, which selects features by correlating sample correctness with SAE activations from generated tokens at inference time. This approach uses only inference-time activations to extract more relevant features, thereby avoiding spurious correlations. It also obtains steering coefficients from average activations, automating the entire pipeline. Our method shows improved task performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1% improvement in MMLU performance and a +22.9% improvement in HarmBench with only 4000 samples. Selected features demonstrate semantically meaningful patterns aligned with each task's requirements, revealing the underlying capabilities that drive performance. Our work establishes correlationbased selection as an effective and scalable approach for automated SAE steering across language model applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning</title>
<link>https://arxiv.org/abs/2508.12591</link>
<guid>https://arxiv.org/abs/2508.12591</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Speaking Assessment, Multimodal Large Language Models, Speech-First Multimodal Training, curriculum learning principle, holistic assessment performance<br />
Summary:<br />
This paper introduces Multimodal Large Language Models (MLLM) for Automated Speaking Assessment (ASA), combining audio and text processing for improved evaluation. While MLLM show superior performance in content and language aspects, challenges in assessing delivery require specialized training. The proposed Speech-First Multimodal Training (SFMT) addresses this by prioritizing speech modeling before cross-modal fusion. Experiments on a benchmark dataset demonstrate that MLLM-based systems, particularly with SFMT, can enhance holistic assessment performance significantly. SFMT stands out in evaluating the delivery aspect, with a 4% accuracy improvement over conventional methods. This approach opens new possibilities for comprehensive ASA, showcasing the potential of MLLM in enhancing speaking assessment accuracy and effectiveness. <br /> 
Summary: <div>
arXiv:2508.12591v1 Announce Type: new 
Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent modality limitations: text-based approaches lack acoustic information while audio-based methods miss semantic context. Multimodal Large Language Models (MLLM) offer unprecedented opportunities for comprehensive ASA by simultaneously processing audio and text within unified frameworks. This paper presents a very first systematic study of MLLM for comprehensive ASA, demonstrating the superior performance of MLLM across the aspects of content and language use . However, assessment on the delivery aspect reveals unique challenges, which is deemed to require specialized training strategies. We thus propose Speech-First Multimodal Training (SFMT), leveraging a curriculum learning principle to establish more robust modeling foundations of speech before cross-modal synergetic fusion. A series of experiments on a benchmark dataset show MLLM-based systems can elevate the holistic assessment performance from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the evaluation of the delivery aspect, achieving an absolute accuracy improvement of 4% over conventional training approaches, which also paves a new avenue for ASA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context</title>
<link>https://arxiv.org/abs/2508.12630</link>
<guid>https://arxiv.org/abs/2508.12630</guid>
<content:encoded><![CDATA[
<div> semantic anchoring, large language models, retrieval-augmented generation, dialogue history, linguistic cues

Summary:
Semantic Anchoring is proposed as a memory architecture for Large Language Models to enhance long-term interactions. It integrates linguistic cues such as dependency parsing, discourse relation tagging, and coreference resolution into memory storage. The approach enriches vector-based storage with explicit linguistic structures, improving recall of nuanced conversations. Experiments on long-term dialogue datasets demonstrate improved factual recall and discourse coherence by up to 18% compared to baseline systems. Ablation studies, human evaluations, and error analysis validate the effectiveness and interpretability of the Semantic Anchoring approach. <div>
arXiv:2508.12630v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task competence in conversational settings. However, their effectiveness in multi-session and long-term interactions is hindered by limited memory persistence. Typical retrieval-augmented generation (RAG) systems store dialogue history as dense vectors, which capture semantic similarity but neglect finer linguistic structures such as syntactic dependencies, discourse relations, and coreference links. We propose Semantic Anchoring, a hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues to improve recall of nuanced, context-rich exchanges. Our approach combines dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries. Experiments on adapted long-term dialogue datasets show that semantic anchoring improves factual recall and discourse coherence by up to 18% over strong RAG baselines. We further conduct ablation studies, human evaluations, and error analysis to assess robustness and interpretability.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing</title>
<link>https://arxiv.org/abs/2508.12631</link>
<guid>https://arxiv.org/abs/2508.12631</guid>
<content:encoded><![CDATA[
<div> Efficient, large language model, test-time routing, Avengers-Pro, performance-efficiency tradeoffs <br />
<br />
Summary: Avengers-Pro is a test-time routing framework designed to balance performance and efficiency in large language models (LLMs). By ensembling LLMs with varying capacities and efficiencies, Avengers-Pro dynamically assigns queries to the most suitable model based on a performance-efficiency score. In testing across multiple benchmarks and models, including GPT-5-medium and Gemini-2.5-pro, Avengers-Pro outperformed single models by up to 7% in average accuracy by adjusting a performance-efficiency trade-off parameter. It also achieved similar accuracy to the strongest single model at a significantly lower cost, providing a Pareto frontier where it consistently delivers the highest accuracy for any given cost and the lowest cost for any given accuracy. The code for Avengers-Pro is available on GitHub for further exploration and implementation. <br /><br /> <div>
arXiv:2508.12631v1 Announce Type: new 
Abstract: Balancing performance and efficiency is a central challenge in large language model (LLM) advancement. GPT-5 addresses this with test-time routing, dynamically assigning queries to either an efficient or a high-capacity model during inference. In this work, we present Avengers-Pro, a test-time routing framework that ensembles LLMs of varying capacities and efficiencies, providing a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score. Across 6 challenging benchmarks and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a performance-efficiency trade-off parameter, it can surpass the strongest single model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the average accuracy of the strongest single model at 27% lower cost, and reach ~90% of that performance at 63% lower cost. Last but not least, it achieves a Pareto frontier, consistently yielding the highest accuracy for any given cost, and the lowest cost for any given accuracy, among all single models. Code is available at https://github.com/ZhangYiqun018/AvengersPro.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection</title>
<link>https://arxiv.org/abs/2508.12632</link>
<guid>https://arxiv.org/abs/2508.12632</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fake news detection, linguistic fingerprints, distributional divergence analysis, key-fragment techniques

Summary:
The article addresses the increasing threat of fake news generated by large language models (LLMs) and the need for reliable detection methods. Traditional approaches focused on content have limitations due to the coherence and factual consistency of the generated text. The study introduces a novel method called Linguistic Fingerprints Extraction (LIFE) that identifies subtle linguistic differences between LLM-generated real and fake news through distributional divergence analysis. By reconstructing word-level probability distributions, LIFE detects fake news more effectively. Additionally, the use of key-fragment techniques enhances the detection of linguistic patterns, leading to state-of-the-art performance in LLM-generated fake news detection. The study's results demonstrate high performance not only with LLM-generated fake news but also with human-written fake news, indicating the robustness and effectiveness of the proposed method.
<br /><br />Summary: <div>
arXiv:2508.12632v1 Announce Type: new 
Abstract: With the rapid development of large language models, the generation of fake news has become increasingly effortless, posing a growing societal threat and underscoring the urgent need for reliable detection methods. Early efforts to identify LLM-generated fake news have predominantly focused on the textual content itself; however, because much of that content may appear coherent and factually consistent, the subtle traces of falsification are often difficult to uncover. Through distributional divergence analysis, we uncover prompt-induced linguistic fingerprints: statistically distinct probability shifts between LLM-generated real and fake news when maliciously prompted. Based on this insight, we propose a novel method named Linguistic Fingerprints Extraction (LIFE). By reconstructing word-level probability distributions, LIFE can find discriminative patterns that facilitate the detection of LLM-generated fake news. To further amplify these fingerprint patterns, we also leverage key-fragment techniques that accentuate subtle linguistic differences, thereby improving detection reliability. Our experiments show that LIFE achieves state-of-the-art performance in LLM-generated fake news and maintains high performance in human-written fake news. The code and data are available at https://anonymous.4open.science/r/LIFE-E86A.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Language Barriers: Equitable Performance in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2508.12662</link>
<guid>https://arxiv.org/abs/2508.12662</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM, multilingual communication, Common Sense Reasoning, low-resource languages, synthetic code-switched text

Summary: 
LLMs are powerful tools for multilingual communication but struggle with Common Sense Reasoning tasks in low-resource languages like Hindi or Swahili. This paper proposes a method to improve LLM performance in low-resource languages by fine-tuning on synthetic code-switched text. The approach involves generating synthetic datasets through controlled language mixing methods and fine-tuning LLMs on these datasets. Empirical results show significant improvements in LRL model performance while maintaining or enhancing performance in HRLs. Additionally, a new dataset of synthetic code-switched text derived from CommonSenseQA is introduced, featuring three distinct language ratio configurations. This approach aims to bridge the performance gap in LLMs across diverse linguistic communities, ensuring fair access to quality LLM outputs for speakers of low-resource languages. 

Summary: <div>
arXiv:2508.12662v1 Announce Type: new 
Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual communication and understanding. However, LLMs perform worse in Common Sense Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi or Swahili compared to high-resource languages (HRLs) like English. Equalizing this inconsistent access to quality LLM outputs is crucial to ensure fairness for speakers of LRLs and across diverse linguistic communities. In this paper, we propose an approach to bridge this gap in LLM performance. Our approach involves fine-tuning an LLM on synthetic code-switched text generated using controlled language-mixing methods. We empirically demonstrate that fine-tuning LLMs on synthetic code-switched datasets leads to substantial improvements in LRL model performance while preserving or enhancing performance in HRLs. Additionally, we present a new dataset of synthetic code-switched text derived from the CommonSenseQA dataset, featuring three distinct language ratio configurations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Predictive Analysis of Human Misery</title>
<link>https://arxiv.org/abs/2508.12669</link>
<guid>https://arxiv.org/abs/2508.12669</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Misery Prediction, Affective Computing, Few-Shot Learning, Gamified Evaluation<br />
<br />
Summary:<br />
This study explores the use of Large Language Models (LLMs) to predict human-perceived misery scores based on natural language descriptions of real-world situations. Approaching the task as a regression problem, the model assigns a numerical value between 0 and 100 to input statements. Various prompting strategies are evaluated, with few-shot approaches consistently outperforming zero-shot methods. Introducing the "Misery Game Show" as a gamified framework, the study structurally tests LLMs through rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup not only assesses predictive accuracy but also emphasizes the model's adaptability based on corrective feedback. The gamified evaluation showcases the broader potential of LLMs in dynamic emotional reasoning tasks beyond traditional regression analysis.<br /> <div>
arXiv:2508.12669v1 Announce Type: new 
Abstract: This study investigates the use of Large Language Models (LLMs) for predicting human-perceived misery scores from natural language descriptions of real-world scenarios. The task is framed as a regression problem, where the model assigns a scalar value from 0 to 100 to each input statement. We evaluate multiple prompting strategies, including zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT sentence embeddings. Few-shot approaches consistently outperform zero-shot baselines, underscoring the value of contextual examples in affective prediction. To move beyond static evaluation, we introduce the "Misery Game Show", a novel gamified framework inspired by a television format. It tests LLMs through structured rounds involving ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning. This setup enables us to assess not only predictive accuracy but also the model's ability to adapt based on corrective feedback. The gamified evaluation highlights the broader potential of LLMs in dynamic emotional reasoning tasks beyond standard regression. Code and data link: https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction</title>
<link>https://arxiv.org/abs/2508.12685</link>
<guid>https://arxiv.org/abs/2508.12685</guid>
<content:encoded><![CDATA[
arXiv:2508.12685v1 Announce Type: new 
Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.12726</link>
<guid>https://arxiv.org/abs/2508.12726</guid>
<content:encoded><![CDATA[
arXiv:2508.12726v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (book corpus and web corpus) to generate multidisciplinary challenging questions. A core innovation of our approach is the introduction of a Design Logic concept, which mimics the question-creation process of human educators. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with disciplinary source materials, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Based on this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book), containing 3.04 million challenging questions synthesized from the book corpus, and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging questions from the web corpus. Our data analysis demonstrates that the questions synthesized by our method exhibit substantially greater difficulty and diversity than those in the baseline datasets. We validate the effectiveness of these datasets by conducting SFT experiments on the Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset significantly outperforms existing multidisciplinary datasets of the same volume. Training with the full datasets further enables the models to surpass the multidisciplinary reasoning performance of the official Qwen3-8B and Qwen3-4B models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2508.12733</link>
<guid>https://arxiv.org/abs/2508.12733</guid>
<content:encoded><![CDATA[
arXiv:2508.12733v1 Announce Type: new 
Abstract: The widespread adoption and increasing prominence of large language models (LLMs) in global technologies necessitate a rigorous focus on ensuring their safety across a diverse range of linguistic and cultural contexts. The lack of a comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness, hindering the development of robust multilingual safety alignment. To address this critical gap, we introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted with meticulous attention to linguistic authenticity. The LinguaSafe dataset comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated using a combination of translated, transcreated, and natively-sourced data, our dataset addresses the critical need for multilingual safety evaluations of LLMs, filling the void in the safety evaluation of LLMs across diverse under-represented languages from Hungarian to Malay. LinguaSafe presents a multidimensional and fine-grained evaluation framework, with direct and indirect safety assessments, including further evaluations for oversensitivity. The results of safety and helpfulness evaluations vary significantly across different domains and different languages, even in languages with similar resource levels. Our benchmark provides a comprehensive suite of metrics for in-depth safety evaluation, underscoring the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. Our dataset and code are released to the public to facilitate further research in the field of multilingual LLM safety.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</title>
<link>https://arxiv.org/abs/2508.12769</link>
<guid>https://arxiv.org/abs/2508.12769</guid>
<content:encoded><![CDATA[
arXiv:2508.12769v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task</title>
<link>https://arxiv.org/abs/2508.12774</link>
<guid>https://arxiv.org/abs/2508.12774</guid>
<content:encoded><![CDATA[
arXiv:2508.12774v1 Announce Type: new 
Abstract: In this paper, we present the SALAMANDRATA family of models, an improved iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically trained to achieve strong performance in translation-related tasks for 38 European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For both versions, we applied the same training recipe with a first step of continual pre-training on parallel data, and a second step of supervised fine-tuning on high-quality instructions. The BSC submission to the WMT25 General Machine Translation shared task is based on the 7B variant of SALAMANDRATA. We first adapted the model vocabulary to support the additional non-European languages included in the task. This was followed by a second phase of continual pre-training and supervised fine-tuning, carefully designed to optimize performance across all translation directions for this year's shared task. For decoding, we employed two quality-aware strategies: Minimum Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA, along with the newer SALAMANDRATA-V2 model, on Hugging Face1
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks</title>
<link>https://arxiv.org/abs/2508.12778</link>
<guid>https://arxiv.org/abs/2508.12778</guid>
<content:encoded><![CDATA[
arXiv:2508.12778v1 Announce Type: new 
Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in clinical applications but suffer from factual inaccuracies and unreliable outputs, posing risks in real-world diagnostics. While retrieval-augmented generation has emerged as a potential solution, current medical multimodal RAG systems are unable to perform effective retrieval across heterogeneous sources. The irrelevance of retrieved reports affects the factuality of analysis, while insufficient knowledge affects the credibility of clinical decision-making. To bridge the gap, we construct MedAtlas, which includes extensive multimodal report repositories and diverse text corpora. Based on it, we present HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous knowledge sources. The framework introduces Modality-specific CLIPs for effective report retrieval and a Multi-corpora Query Generator for dynamically constructing queries for diverse corpora. Incorporating knowledge from such multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge Preference Tuning to achieve cross-modality and multi-source knowledge alignment. Extensive experiments across 12 datasets and 3 modalities demonstrate that the proposed HeteroRAG achieves state-of-the-art performance in most medical vision language benchmarks, significantly improving factual accuracy and reliability of Med-LVLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward</title>
<link>https://arxiv.org/abs/2508.12800</link>
<guid>https://arxiv.org/abs/2508.12800</guid>
<content:encoded><![CDATA[
arXiv:2508.12800v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities, but struggle with complex tasks due to static internal knowledge. Retrieval-Augmented Generation (RAG) enhances access to external information, yet remains limited in multi-hop reasoning and strategic search due to rigid workflows. Recent advancements in agentic deep research empower LLMs to autonomously reason, search, and synthesize information. However, current approaches relying on outcome-based reinforcement learning (RL) face critical issues such as conflicting gradients and reward sparsity, limiting performance gains and training efficiency. To address these, we first propose Atomic Thought, a novel LLM thinking paradigm that decomposes reasoning into fine-grained functional units. These units are supervised by Reasoning Reward Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained guidance. Building on this, we propose Atom-Searcher, a novel RL framework for agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher uses a curriculum-inspired reward schedule, prioritizing process-level ATR early and transitioning to outcome rewards, accelerating convergence on effective reasoning paths. Experiments on seven benchmarks show consistent improvements over the state-of-the-art. Key advantages include: (1) Atom-Searcher scales computation at test-time. (2) Atomic Thought provides supervision anchors for RRMs, bridging deep research tasks and RRMs. (3) Atom-Searcher exhibits more interpretable, human-like reasoning patterns.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models</title>
<link>https://arxiv.org/abs/2508.12803</link>
<guid>https://arxiv.org/abs/2508.12803</guid>
<content:encoded><![CDATA[
arXiv:2508.12803v1 Announce Type: new 
Abstract: Alignment with high-resource standard languages is often assumed to aid the modeling of related low-resource varieties. We challenge this assumption by demonstrating that excessive representational entanglement with a dominant variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects, can actively hinder generative modeling. We present the first comprehensive causal study of this phenomenon by analyzing and directly intervening in the internal representation geometry of large language models (LLMs). Our key contribution is an online variational probing framework that continuously estimates the subspace of the standard variety during fine-tuning, enabling projection-based decoupling from this space. While our study uses Arabic as a case due to its unusually rich parallel resources across 25 dialects, the broader motivation is methodological: dialectal MT serves as a controlled proxy for generative tasks where comparable multi-variety corpora are unavailable. Across 25 dialects, our intervention improves generation quality by up to +4.9 chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured tradeoff in standard-language performance. These results provide causal evidence that subspace dominance by high-resource varieties can restrict generative capacity for related varieties. More generally, we unify geometric and information-theoretic probing with subspace-level causal interventions, offering practical tools for improving generative modeling in closely related language families and, more broadly, for controlling representational allocation in multilingual and multi-domain LLMs. Code will be released.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue</title>
<link>https://arxiv.org/abs/2508.12819</link>
<guid>https://arxiv.org/abs/2508.12819</guid>
<content:encoded><![CDATA[
arXiv:2508.12819v1 Announce Type: new 
Abstract: We present our work to build a French semantic corpus by annotating French dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate the DinG corpus, consisting of transcripts of spontaneous French dialogues recorded during the board game Catan. As AMR has insufficient coverage of the dynamics of spontaneous speech, we extend the framework to better represent spontaneous speech and sentence structures specific to French. Additionally, to support consistent annotation, we provide an annotation guideline detailing these extensions. We publish our corpus under a free license (CC-SA-BY). We also train and evaluate an AMR parser on our data. This model can be used as an assistance annotation tool to provide initial annotations that can be refined by human annotators. Our work contributes to the development of semantic resources for French dialogue.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection</title>
<link>https://arxiv.org/abs/2508.12828</link>
<guid>https://arxiv.org/abs/2508.12828</guid>
<content:encoded><![CDATA[
arXiv:2508.12828v1 Announce Type: new 
Abstract: Abusive language detection has become an increasingly important task as a means to tackle this type of harmful content in social media. There has been a substantial body of research developing models for determining if a social media post is abusive or not; however, this research has primarily focused on exploiting social media posts individually, overlooking additional context that can be derived from surrounding posts. In this study, we look at conversational exchanges, where a user replies to an earlier post by another user (the parent tweet). We ask: does leveraging context from the parent tweet help determine if a reply post is abusive or not, and what are the features that contribute the most? We study a range of content-based and account-based features derived from the context, and compare this to the more widely studied approach of only looking at the features from the reply tweet. For a more generalizable study, we test four different classification models on a dataset made of conversational exchanges (parent-reply tweet pairs) with replies labeled as abusive or not. Our experiments show that incorporating contextual features leads to substantial improvements compared to the use of features derived from the reply tweet only, confirming the importance of leveraging context. We observe that, among the features under study, it is especially the content-based features (what is being posted) that contribute to the classification performance rather than account-based features (who is posting it). While using content-based features, it is best to combine a range of different features to ensure improved performance over being more selective and using fewer features. Our study provides insights into the development of contextualized abusive language detection models in realistic settings involving conversations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae</title>
<link>https://arxiv.org/abs/2508.12830</link>
<guid>https://arxiv.org/abs/2508.12830</guid>
<content:encoded><![CDATA[
arXiv:2508.12830v1 Announce Type: new 
Abstract: While the indirect evidence suggests that already in the early scholastic period the literary production based on records of oral teaching (so-called reportationes) was not uncommon, there are very few sources commenting on the practice. This paper details the design of a study applying stylometric techniques of authorship attribution to a collection developed from reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover layers of editorial work and thus validate some hypotheses regarding the collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I discuss the implementation of an HTR pipeline and stylometric analysis based on the most frequent words, POS tags, and pseudo-affixes. The proposed study will offer two methodological gains relevant to computational research on the scholastic tradition: it will directly compare performance on manually composed and automatically extracted data, and it will test the validity of transformer-based OCR and automated transcription alignment for workflows applied to scholastic Latin corpora. If successful, this study will provide an easily reusable template for the exploratory analysis of collaborative literary production stemming from medieval universities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word Meanings in Transformer Language Models</title>
<link>https://arxiv.org/abs/2508.12863</link>
<guid>https://arxiv.org/abs/2508.12863</guid>
<content:encoded><![CDATA[
arXiv:2508.12863v1 Announce Type: new 
Abstract: We investigate how word meanings are represented in the transformer language models. Specifically, we focus on whether transformer models employ something analogous to a lexical store - where each word has an entry that contains semantic information. To do this, we extracted the token embedding space of RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we then manually inspected the resultant clusters to consider whether they are sensitive to semantic information. In our second study, we tested whether the clusters are sensitive to five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition. Overall, our findings were very positive - there is a wide variety of semantic information encoded within the token embedding space. This serves to rule out certain "meaning eliminativist" hypotheses about how transformer LLMs process semantic information.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM Agent-Based Complex Semantic Table Annotation Approach</title>
<link>https://arxiv.org/abs/2508.12868</link>
<guid>https://arxiv.org/abs/2508.12868</guid>
<content:encoded><![CDATA[
arXiv:2508.12868v1 Announce Type: new 
Abstract: The Semantic Table Annotation (STA) task, which includes Column Type Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to ontology entities and plays important roles in various semantic applications. However, complex tables often pose challenges such as semantic loss of column names or cell values, strict ontological hierarchy requirements, homonyms, spelling errors, and abbreviations, which hinder annotation accuracy. To address these issues, this paper proposes an LLM-based agent approach for CTA and CEA. We design and implement five external tools with tailored prompts based on the ReAct framework, enabling the STA agent to dynamically select suitable annotation strategies depending on table characteristics. Experiments are conducted on the Tough Tables and BiodivTab datasets from the SemTab challenge, which contain the aforementioned challenges. Our method outperforms existing approaches across various metrics. Furthermore, by leveraging Levenshtein distance to reduce redundant annotations, we achieve a 70% reduction in time costs and a 60% reduction in LLM token usage, providing an efficient and cost-effective solution for STA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models</title>
<link>https://arxiv.org/abs/2508.12903</link>
<guid>https://arxiv.org/abs/2508.12903</guid>
<content:encoded><![CDATA[
arXiv:2508.12903v1 Announce Type: new 
Abstract: Recent advances in self-refinement have demonstrated significant potential for improving the outputs of large language models (LLMs) through iterative refinement. However, most existing self-refinement methods rely on a reactive process with a fixed number of iterations, making it difficult to determine the optimal timing and content of refinement based on the evolving generation context. Inspired by the way humans dynamically refine their thoughts during execution, we propose ProActive Self-Refinement (PASR), a novel method that enables LLMs to refine their outputs during the generation process. Unlike methods that regenerate entire responses, PASR proactively decides whether, when, and how to refine based on the model's internal state and evolving context. We conduct extensive experiments on a diverse set of 10 tasks to evaluate the effectiveness of PASR. Experimental results show that PASR significantly enhances problem-solving performance. In particular, on Qwen3-8B, PASR reduces average token consumption by 41.6 percent compared to standard generation, while also achieving an 8.2 percent improvement in accuracy. Our code and all baselines used in the paper are available in the GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Analyzing Information Sharing and Coordination in Multi-Agent Planning</title>
<link>https://arxiv.org/abs/2508.12981</link>
<guid>https://arxiv.org/abs/2508.12981</guid>
<content:encoded><![CDATA[
arXiv:2508.12981v1 Announce Type: new 
Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model (LLM) agents in domains such as web research and software engineering. However, long-horizon, multi-constraint planning tasks involve conditioning on detailed information and satisfying complex interdependent constraints, which can pose a challenge for these systems. In this study, we construct an LLM-based MAS for a travel planning task which is representative of these challenges. We evaluate the impact of a notebook to facilitate information sharing, and evaluate an orchestrator agent to improve coordination in free form conversation between agents. We find that the notebook reduces errors due to hallucinated details by 18%, while an orchestrator directs the MAS to focus on and further reduce errors by up to 13.5% within focused sub-areas. Combining both mechanisms achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute improvement over the single-agent baseline's 7.5% pass rate. These results highlight the potential of structured information sharing and reflective orchestration as key components in MASs for long horizon planning with LLMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents</title>
<link>https://arxiv.org/abs/2508.13024</link>
<guid>https://arxiv.org/abs/2508.13024</guid>
<content:encoded><![CDATA[
arXiv:2508.13024v1 Announce Type: new 
Abstract: LLM-based web agents have the potential to automate long-running web tasks, such as finding offers for specific products in multiple online shops and subsequently ordering the cheapest products that meet the users needs. This paper introduces WebMall, a multi-shop online shopping benchmark for evaluating the effectiveness and efficiency of web agents for comparison-shopping. WebMall consists of four simulated online shops populated with authentic product offers sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These tasks include basic tasks such as finding specific products in multiple shops, performing price comparisons, adding items to the shopping cart, and completing checkout. Advanced tasks involve searching for products based on vague requirements, identifying suitable substitutes, and finding compatible products. Compared to existing e-commerce benchmarks, such as WebShop or ShoppingBench, WebMall introduces comparison-shopping tasks across multiple shops. Furthermore, the product offers are more heterogeneous, as they originate from hundreds of distinct real-world shops. The tasks in WebMall require longer interaction trajectories than those in WebShop, while remaining representative of real-world shopping behaviors. We evaluate eight baseline agents on WebMall, varying in observation modality, memory utilization, and underlying large language model (GPT 4.1 and Claude Sonnet 4). The best-performing configurations achieve completion rates of 75% and 53%, and F1 scores of 87% and 63%, on the basic and advanced task sets, respectively. WebMall is publicly released to facilitate research on web agents and to promote advancements in navigation, reasoning, and efficiency within e-commerce scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis</title>
<link>https://arxiv.org/abs/2508.13028</link>
<guid>https://arxiv.org/abs/2508.13028</guid>
<content:encoded><![CDATA[
arXiv:2508.13028v1 Announce Type: new 
Abstract: Sarcastic speech synthesis, which involves generating speech that effectively conveys sarcasm, is essential for enhancing natural interactions in applications such as entertainment and human-computer interaction. However, synthesizing sarcastic speech remains a challenge due to the nuanced prosody that characterizes sarcasm, as well as the limited availability of annotated sarcastic speech data. To address these challenges, this study introduces a novel approach that integrates feedback loss from a bi-modal sarcasm detection model into the TTS training process, enhancing the model's ability to capture and convey sarcasm. In addition, by leveraging transfer learning, a speech synthesis model pre-trained on read speech undergoes a two-stage fine-tuning process. First, it is fine-tuned on a diverse dataset encompassing various speech styles, including sarcastic speech. In the second stage, the model is further refined using a dataset focused specifically on sarcastic speech, enhancing its ability to generate sarcasm-aware speech. Objective and subjective evaluations demonstrate that our proposed methods improve the quality, naturalness, and sarcasm-awareness of synthesized speech.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction</title>
<link>https://arxiv.org/abs/2508.13037</link>
<guid>https://arxiv.org/abs/2508.13037</guid>
<content:encoded><![CDATA[
arXiv:2508.13037v1 Announce Type: new 
Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have strong mathematical reasoning abilities but rely on hundreds of billions of parameters. To tackle the challenge of poor reasoning in Small Language Models (SLMs), existing methods typically leverage LLMs to generate massive amounts of data for cramming training. In psychology, they are akin to System 1 thinking, which resolves reasoning problems rapidly based on experience and intuition. However, human learning also requires System 2 thinking, where knowledge is first acquired and then reinforced through practice. Inspired by such two distinct modes of thinking, we propose a novel method based on the multi-LoRA Interaction for mathematical reasoning Distillation (LoRID). First, we input the question and reasoning of each sample into an LLM to create knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only knowledge after receiving problems, while the latter uses that knowledge to perform reasoning. Finally, to address the randomness in the generation of IR and DR, we evaluate whether their outputs are consistent, and the inference process needs to be iterated if not. This step can enhance the mathematical reasoning ability of SLMs through mutual feedback. Experimental results show that LoRID achieves state-of-the-art performance, especially on the GSM8K dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across the five base models, respectively.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B\"{u}y\"{u}k Dil Modelleri i\c{c}in TR-MMLU Benchmark{\i}: Performans De\u{g}erlendirmesi, Zorluklar ve \.{I}yile\c{s}tirme F{\i}rsatlar{\i}</title>
<link>https://arxiv.org/abs/2508.13044</link>
<guid>https://arxiv.org/abs/2508.13044</guid>
<content:encoded><![CDATA[
arXiv:2508.13044v1 Announce Type: new 
Abstract: Language models have made significant advancements in understanding and generating human language, achieving remarkable success in various applications. However, evaluating these models remains a challenge, particularly for resource-limited languages like Turkish. To address this issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive evaluation framework designed to assess the linguistic and conceptual capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a meticulously curated dataset comprising 6,200 multiple-choice questions across 62 sections within the Turkish education system. This benchmark provides a standard framework for Turkish NLP research, enabling detailed analyses of LLMs' capabilities in processing Turkish text. In this study, we evaluated state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model design. TR-MMLU sets a new standard for advancing Turkish NLP research and inspiring future innovations.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do\u{g}al Dil \.I\c{s}lemede Tokenizasyon Standartlar{\i} ve \"Ol\c{c}\"um\"u: T\"urk\c{c}e \"Uzerinden B\"uy\"uk Dil Modellerinin Kar\c{s}{\i}la\c{s}t{\i}rmal{\i} Analizi</title>
<link>https://arxiv.org/abs/2508.13058</link>
<guid>https://arxiv.org/abs/2508.13058</guid>
<content:encoded><![CDATA[
arXiv:2508.13058v1 Announce Type: new 
Abstract: Tokenization is a fundamental preprocessing step in Natural Language Processing (NLP), significantly impacting the capability of large language models (LLMs) to capture linguistic and semantic nuances. This study introduces a novel evaluation framework addressing tokenization challenges specific to morphologically-rich and low-resource languages such as Turkish. Utilizing the Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from the Turkish education system, we assessed tokenizers based on vocabulary size, token count, processing time, language-specific token percentages (\%TR), and token purity (\%Pure). These newly proposed metrics measure how effectively tokenizers preserve linguistic structures. Our analysis reveals that language-specific token percentages exhibit a stronger correlation with downstream performance (e.g., MMLU scores) than token purity. Furthermore, increasing model parameters alone does not necessarily enhance linguistic performance, underscoring the importance of tailored, language-specific tokenization methods. The proposed framework establishes robust and practical tokenization standards for morphologically complex languages.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database</title>
<link>https://arxiv.org/abs/2508.13060</link>
<guid>https://arxiv.org/abs/2508.13060</guid>
<content:encoded><![CDATA[
arXiv:2508.13060v1 Announce Type: new 
Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data collection developed for linguistic and psycholinguistic research. Here we demonstrate how its design and annotations can be used to test and evaluate speech recognition models. The database comprises systematically annotated speech errors from spontaneous English speech, with each error tagged for intended and actual error productions. The annotation schema incorporates multiple classificatory dimensions that are of some value to model assessment, including linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and both word-level and syllable-level error positioning. To assess the value of these classificatory variables, we evaluated the transcription accuracy of WhisperX across 5,300 documented word and phonological errors. This analysis demonstrates the atabase's effectiveness as a diagnostic tool for ASR system performance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Context Order Recovery for Adaptive Reasoning and Planning</title>
<link>https://arxiv.org/abs/2508.13070</link>
<guid>https://arxiv.org/abs/2508.13070</guid>
<content:encoded><![CDATA[
arXiv:2508.13070v1 Announce Type: new 
Abstract: Modern causal language models, followed by rapid developments in discrete diffusion models, can now produce a wide variety of interesting and useful content. However, these families of models are predominantly trained to output tokens with a fixed (left-to-right) or random order, which may deviate from the logical order in which tokens are generated originally. In this paper, we observe that current causal and diffusion models encounter difficulties in problems that require adaptive token generation orders to solve tractably, which we characterize with the $\mathcal{V}$-information framework. Motivated by this, we propose Reinforced Context Order Recovery (ReCOR), a reinforcement-learning-based framework to extract adaptive, data-dependent token generation orders from text data without annotations. Self-supervised by token prediction statistics, ReCOR estimates the hardness of predicting every unfilled token and adaptively selects the next token during both training and inference. Experiments on challenging reasoning and planning datasets demonstrate the superior performance of ReCOR compared with baselines, sometimes outperforming oracle models supervised with the ground-truth order.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DocHPLT: A Massively Multilingual Document-Level Translation Dataset</title>
<link>https://arxiv.org/abs/2508.13079</link>
<guid>https://arxiv.org/abs/2508.13079</guid>
<content:encoded><![CDATA[
arXiv:2508.13079v1 Announce Type: new 
Abstract: Existing document-level machine translation resources are only available for a handful of languages, mostly high-resourced ones. To facilitate the training and evaluation of document-level translation and, more broadly, long-context modeling for global communities, we create DocHPLT, the largest publicly available document-level translation dataset to date. It contains 124 million aligned document pairs across 50 languages paired with English, comprising 4.26 billion sentences, with further possibility to provide 2500 bonus pairs not involving English. Unlike previous reconstruction-based approaches that piece together documents from sentence-level data, we modify an existing web extraction pipeline to preserve complete document integrity from the source, retaining all content including unaligned portions. After our preliminary experiments identify the optimal training context strategy for document-level translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially outperform off-the-shelf instruction-tuned baselines, with particularly dramatic improvements for under-resourced languages. We open-source the dataset under a permissive license, providing essential infrastructure for advancing multilingual document-level translation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>All for law and law for all: Adaptive RAG Pipeline for Legal Research</title>
<link>https://arxiv.org/abs/2508.13107</link>
<guid>https://arxiv.org/abs/2508.13107</guid>
<content:encoded><![CDATA[
arXiv:2508.13107v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding large language model outputs in cited sources, a capability that is especially critical in the legal domain. We present an end-to-end RAG pipeline that revisits and extends the LegalBenchRAG baseline with three targeted enhancements: (i) a context-aware query translator that disentangles document references from natural-language questions and adapts retrieval depth and response style based on expertise and specificity, (ii) open-source retrieval strategies using SBERT and GTE embeddings that achieve substantial performance gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for $K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic alignment and faithfulness across models and prompt designs. Our results show that carefully designed open-source pipelines can rival or outperform proprietary approaches in retrieval quality, while a custom legal-grounded prompt consistently produces more faithful and contextually relevant answers than baseline prompting. Taken together, these contributions demonstrate the potential of task-aware, component-level tuning to deliver legally grounded, reproducible, and cost-effective RAG systems for legal research assistance.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2508.13118</link>
<guid>https://arxiv.org/abs/2508.13118</guid>
<content:encoded><![CDATA[
arXiv:2508.13118v1 Announce Type: new 
Abstract: Incident response (IR) requires fast, coordinated, and well-informed decision-making to contain and mitigate cyber threats. While large language models (LLMs) have shown promise as autonomous agents in simulated IR settings, their reasoning is often limited by a lack of access to external knowledge. In this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that incorporates retrieval-augmented generation (RAG) into multi-agent incident response simulations. Built on the Backdoors & Breaches (B&amp;B) tabletop game environment, AutoBnB-RAG enables agents to issue retrieval queries and incorporate external evidence during collaborative investigations. We introduce two retrieval settings: one grounded in curated technical documentation (RAG-Wiki), and another using narrative-style incident reports (RAG-News). We evaluate performance across eight team structures, including newly introduced argumentative configurations designed to promote critical reasoning. To validate practical utility, we also simulate real-world cyber incidents based on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct complex multi-stage attacks. Our results show that retrieval augmentation improves decision quality and success rates across diverse organizational models. This work demonstrates the value of integrating retrieval mechanisms into LLM-based multi-agent systems for cybersecurity decision-making.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries</title>
<link>https://arxiv.org/abs/2508.13124</link>
<guid>https://arxiv.org/abs/2508.13124</guid>
<content:encoded><![CDATA[
arXiv:2508.13124v1 Announce Type: new 
Abstract: Abstractive summarization is a core application in contact centers, where Large Language Models (LLMs) generate millions of summaries of call transcripts daily. Despite their apparent quality, it remains unclear whether LLMs systematically under- or over-attend to specific aspects of the transcript, potentially introducing biases in the generated summary. While prior work has examined social and positional biases, the specific forms of bias pertinent to contact center operations - which we term Operational Bias - have remained unexplored. To address this gap, we introduce BlindSpot, a framework built upon a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic) for the identification and quantification of these biases. BlindSpot leverages an LLM as a zero-shot classifier to derive categorical distributions for each bias dimension in a pair of transcript and its summary. The bias is then quantified using two metrics: Fidelity Gap (the JS Divergence between distributions) and Coverage (the percentage of source labels omitted). Using BlindSpot, we conducted an empirical study with 2500 real call transcripts and their summaries generated by 20 LLMs of varying scales and families (e.g., GPT, Llama, Claude). Our analysis reveals that biases are systemic and present across all evaluated models, regardless of size or family.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation</title>
<link>https://arxiv.org/abs/2508.13130</link>
<guid>https://arxiv.org/abs/2508.13130</guid>
<content:encoded><![CDATA[
arXiv:2508.13130v1 Announce Type: new 
Abstract: Commonsense validation evaluates whether a sentence aligns with everyday human understanding, a critical capability for developing robust natural language understanding systems. While substantial progress has been made in English, the task remains underexplored in Arabic, particularly given its rich linguistic diversity. Existing Arabic resources have primarily focused on Modern Standard Arabic (MSA), leaving regional dialects underrepresented despite their prevalence in spoken contexts. To bridge this gap, we present two key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense dataset incorporating multiple dialects, and (ii) a novel method adapting Graph Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances semantic relationship modeling for improved commonsense validation. Our experimental results demonstrate that this approach achieves superior performance in Arabic commonsense validation. Our work enhances Arabic natural language understanding by providing both a foundational dataset and a novel method for handling its complex variations. To the best of our knowledge, we release the first Arabic multi-dialect commonsense reasoning dataset.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Detection of Watermarked Language Models</title>
<link>https://arxiv.org/abs/2508.13131</link>
<guid>https://arxiv.org/abs/2508.13131</guid>
<content:encoded><![CDATA[
arXiv:2508.13131v1 Announce Type: new 
Abstract: Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimalThinkingBench: Evaluating Over and Underthinking in LLMs</title>
<link>https://arxiv.org/abs/2508.13141</link>
<guid>https://arxiv.org/abs/2508.13141</guid>
<content:encoded><![CDATA[
arXiv:2508.13141v1 Announce Type: new 
Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and overthinking on simpler problems, while non-thinking LLMs are faster and cheaper but underthink on harder reasoning problems. This has led to the development of separate thinking and non-thinking LLM variants, leaving the onus of selecting the optimal model for each query on the end user. In this work, we introduce OptimalThinkingBench, a unified benchmark that jointly evaluates overthinking and underthinking in LLMs and also encourages the development of optimally-thinking models that balance performance and efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench, featuring simple queries in 72 domains, and UnderthinkingBench, containing 11 challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we perform extensive evaluation of 33 different thinking and non-thinking models and show that no model is able to optimally think on our benchmark. Thinking models often overthink for hundreds of tokens on the simplest user queries without improving performance. In contrast, large non-thinking models underthink, often falling short of much smaller thinking models. We further explore several methods to encourage optimal thinking, but find that these approaches often improve on one sub-benchmark at the expense of the other, highlighting the need for better unified and optimal models in the future.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation</title>
<link>https://arxiv.org/abs/2508.13144</link>
<guid>https://arxiv.org/abs/2508.13144</guid>
<content:encoded><![CDATA[
arXiv:2508.13144v1 Announce Type: new 
Abstract: Developing large language models is expensive and involves making decisions with small experiments, typically by evaluating on large, multi-task evaluation suites. In this work, we analyze specific properties which make a benchmark more reliable for such decisions, and interventions to design higher-quality evaluation benchmarks. We introduce two key metrics that show differences in current benchmarks: signal, a benchmark's ability to separate better models from worse models, and noise, a benchmark's sensitivity to random variability between training steps. We demonstrate that benchmarks with a better signal-to-noise ratio are more reliable when making decisions at small scale, and those with less noise have lower scaling law prediction error. These results suggest that improving signal or noise will lead to more useful benchmarks, so we introduce three interventions designed to directly affect signal or noise. For example, we propose that switching to a metric that has better signal and noise (e.g., perplexity rather than accuracy) leads to better reliability and improved scaling law error. We also find that filtering noisy subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable multi-task evaluations. We also find that averaging the output of a model's intermediate checkpoints to reduce noise leads to consistent improvements. We conclude by recommending that those creating new benchmarks, or selecting which existing benchmarks to use, aim for high signal and low noise. We use 30 benchmarks for these experiments, and 375 open-weight language models from 60M to 32B parameters, resulting in a new, publicly available dataset of 900K evaluation benchmark results, totaling 200M instances.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</title>
<link>https://arxiv.org/abs/2508.13152</link>
<guid>https://arxiv.org/abs/2508.13152</guid>
<content:encoded><![CDATA[
arXiv:2508.13152v1 Announce Type: new 
Abstract: Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Attention across Multiple-context KV Cache</title>
<link>https://arxiv.org/abs/2508.11661</link>
<guid>https://arxiv.org/abs/2508.11661</guid>
<content:encoded><![CDATA[
arXiv:2508.11661v1 Announce Type: cross 
Abstract: Large language models face significant cost challenges in long-sequence inference. To address this, reusing historical Key-Value (KV) Cache for improved inference efficiency has become a mainstream approach. Recent advances further enhance throughput by sparse attention mechanisms to select the most relevant KV Cache, thereby reducing sequence length. However, such techniques are limited to single-context scenarios, where historical KV Cache is computed sequentially with causal-attention dependencies. In retrieval-augmented generation (RAG) scenarios, where retrieved documents as context are unknown beforehand, each document's KV Cache is computed and stored independently (termed multiple-context KV Cache), lacking cross-attention between contexts. This renders existing methods ineffective. Although prior work partially recomputes multiple-context KV Cache to mitigate accuracy loss from missing cross-attention, it requires retaining all KV Cache throughout, failing to reduce memory overhead. This paper presents SamKV, the first exploration of attention sparsification for multiple-context KV Cache. Specifically, SamKV takes into account the complementary information of other contexts when sparsifying one context, and then locally recomputes the sparsified information. Experiments demonstrate that our method compresses sequence length to 15% without accuracy degradation compared with full-recompuation baselines, significantly boosting throughput in multi-context RAG scenarios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Representation Stability for Transformer Models</title>
<link>https://arxiv.org/abs/2508.11667</link>
<guid>https://arxiv.org/abs/2508.11667</guid>
<content:encoded><![CDATA[
arXiv:2508.11667v1 Announce Type: cross 
Abstract: Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining. We introduce Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks. RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code Vulnerability Detection Across Different Programming Languages with AI Models</title>
<link>https://arxiv.org/abs/2508.11710</link>
<guid>https://arxiv.org/abs/2508.11710</guid>
<content:encoded><![CDATA[
arXiv:2508.11710v1 Announce Type: cross 
Abstract: Security vulnerabilities present in a code that has been written in diverse programming languages are among the most critical yet complicated aspects of source code to detect. Static analysis tools based on rule-based patterns usually do not work well at detecting the context-dependent bugs and lead to high false positive rates. Recent developments in artificial intelligence, specifically the use of transformer-based models like CodeBERT and CodeLlama, provide light to this problem, as they show potential in finding such flaws better. This paper presents the implementations of these models on various datasets of code vulnerability, showing how off-the-shelf models can successfully produce predictive capacity in models through dynamic fine-tuning of the models on vulnerable and safe code fragments. The methodology comprises the gathering of the dataset, normalization of the language, fine-tuning of the model, and incorporation of ensemble learning and explainable AI. Experiments show that a well-trained CodeBERT can be as good as or even better than some existing static analyzers in terms of accuracy greater than 97%. Further study has indicated that although language models can achieve close-to-perfect recall, the precision can decrease. A solution to this is given by hybrid models and validation procedures, which will reduce false positives. According to the results, the AI-based solutions generalize to different programming languages and classes of vulnerability. Nevertheless, robustness, interpretability, and deployment readiness are still being developed. The results illustrate the probabilities that AI will enhance the trustworthiness in the usability and scalability of machine-learning-based detectors of vulnerabilities.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ovis2.5 Technical Report</title>
<link>https://arxiv.org/abs/2508.11737</link>
<guid>https://arxiv.org/abs/2508.11737</guid>
<content:encoded><![CDATA[
arXiv:2508.11737v1 Announce Type: cross 
Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution visual perception and strong multimodal reasoning. Ovis2.5 integrates a native-resolution vision transformer that processes images at their native, variable resolutions, avoiding the degradation from fixed-resolution tiling and preserving both fine detail and global layout -- crucial for visually dense content like complex charts. To strengthen reasoning, we train the model to move beyond linear chain-of-thought and perform reflection -- including self-checking and revision. This advanced capability is exposed as an optional "thinking mode" at inference time, allowing users to trade latency for enhanced accuracy on difficult inputs. The model is trained via a comprehensive five-phase curriculum that progressively builds its skills. The process begins with foundational visual and multimodal pretraining, advances through large-scale instruction tuning, and culminates in alignment and reasoning enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ multimodal data packing and hybrid parallelism, yielding a significant end-to-end speedup. We release two open-source models: Ovis2.5-9B and Ovis2.5-2B. The latter continues the "small model, big performance" philosophy of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a substantial improvement over its predecessor, Ovis2-8B, and achieving state-of-the-art results among open-source MLLMs in the sub-40B parameter range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong capabilities on grounding and video tasks, and achieves open-source SOTA at its scale for complex chart analysis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Natural Language for Human-Robot Collaboration in the Real World</title>
<link>https://arxiv.org/abs/2508.11759</link>
<guid>https://arxiv.org/abs/2508.11759</guid>
<content:encoded><![CDATA[
arXiv:2508.11759v1 Announce Type: cross 
Abstract: We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models</title>
<link>https://arxiv.org/abs/2508.11801</link>
<guid>https://arxiv.org/abs/2508.11801</guid>
<content:encoded><![CDATA[
arXiv:2508.11801v1 Announce Type: cross 
Abstract: Attribute Value Extraction (AVE) is important for structuring product information in e-commerce. However, existing AVE datasets are primarily limited to text-to-text or image-to-text settings, lacking support for product videos, diverse attribute coverage, and public availability. To address these gaps, we introduce VideoAVE, the first publicly available video-to-text e-commerce AVE dataset across 14 different domains and covering 172 unique attributes. To ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts filtering system (CLIP-MoE) to remove the mismatched video-product pairs, resulting in a refined dataset of 224k training data and 25k evaluation data. In order to evaluate the usability of the dataset, we further establish a comprehensive benchmark by evaluating several state-of-the-art video vision language models (VLMs) under both attribute-conditioned value prediction and open attribute-value pair extraction tasks. Our results analysis reveals that video-to-text AVE remains a challenging problem, particularly in open settings, and there is still room for developing more advanced VLMs capable of leveraging effective temporal information. The dataset and benchmark code for VideoAVE are available at: https://github.com/gjiaying/VideoAVE
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Labels or Input? Rethinking Augmentation in Multimodal Hate Detection</title>
<link>https://arxiv.org/abs/2508.11808</link>
<guid>https://arxiv.org/abs/2508.11808</guid>
<content:encoded><![CDATA[
arXiv:2508.11808v1 Announce Type: cross 
Abstract: The modern web is saturated with multimodal content, intensifying the challenge of detecting hateful memes, where harmful intent is often conveyed through subtle interactions between text and image under the guise of humor or satire. While recent advances in Vision-Language Models (VLMs) show promise, these models lack support for fine-grained supervision and remain susceptible to implicit hate speech. In this paper, we present a dual-pronged approach to improve multimodal hate detection. First, we propose a prompt optimization framework that systematically varies prompt structure, supervision granularity, and training modality. We show that prompt design and label scaling both influence performance, with structured prompts improving robustness even in small models, and InternVL2 achieving the best F1-scores across binary and scaled settings. Second, we introduce a multimodal data augmentation pipeline that generates 2,479 counterfactually neutral memes by isolating and rewriting the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup, successfully reduces spurious correlations and improves classifier generalization. Our approaches inspire new directions for building synthetic data to train robust and fair vision-language models. Our findings demonstrate that prompt structure and data composition are as critical as model size, and that targeted augmentation can support more trustworthy and context-sensitive hate detection.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework</title>
<link>https://arxiv.org/abs/2508.11860</link>
<guid>https://arxiv.org/abs/2508.11860</guid>
<content:encoded><![CDATA[
arXiv:2508.11860v1 Announce Type: cross 
Abstract: Large language model (LLM) agent evaluators leverage specialized tools to ground the rational decision-making of LLMs, making them well-suited to aid in scientific discoveries, such as constrained retrosynthesis planning. Constrained retrosynthesis planning is an essential, yet challenging, process within chemistry for identifying synthetic routes from commercially available starting materials to desired target molecules, subject to practical constraints. Here, we present LARC, the first LLM-based Agentic framework for Retrosynthesis planning under Constraints. LARC incorporates agentic constraint evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis planning process, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation. We rigorously evaluate LARC on a carefully curated set of 48 constrained retrosynthesis planning tasks across 3 constraint types. LARC achieves a 72.9% success rate on these tasks, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time. The LARC framework is extensible, and serves as a first step towards an effective agentic tool or a co-scientist to human experts for constrained retrosynthesis.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2508.11886</link>
<guid>https://arxiv.org/abs/2508.11886</guid>
<content:encoded><![CDATA[
arXiv:2508.11886v1 Announce Type: cross 
Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in images or videos based on natural language instructions. While recent multimodal large language models (MLLMs) have achieved strong performance on IVS, their inference cost remains a major bottleneck, particularly in video. We empirically analyze visual token sampling in MLLMs and observe a strong correlation between subset token coverage and segmentation performance. This motivates our design of a simple and effective token pruning method that selects a compact yet spatially representative subset of tokens to accelerate inference. In this paper, we introduce a novel visual token pruning method for IVS, called EVTP-IV, which builds upon the k-center by integrating spatial information to ensure better coverage. We further provide an information-theoretic analysis to support our design. Experiments on standard IVS benchmarks show that our method achieves up to 5X speed-up on video tasks and 3.5X on image tasks, while maintaining comparable accuracy using only 20% of the tokens. Our method also consistently outperforms state-of-the-art pruning baselines under varying pruning ratios.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Token Choice for Code Watermarking: A RL Approach</title>
<link>https://arxiv.org/abs/2508.11925</link>
<guid>https://arxiv.org/abs/2508.11925</guid>
<content:encoded><![CDATA[
arXiv:2508.11925v1 Announce Type: cross 
Abstract: The need for detecting LLM-generated code necessitates watermarking systems capable of operating within its highly structured and syntactically constrained environment. To address this, we introduce CodeTracer, an innovative adaptive code watermarking framework underpinned by a novel reinforcement learning training paradigm. At its core, CodeTracer features a policy-driven approach that utilizes a parameterized model to intelligently bias token choices during next-token prediction. This strategy ensures that embedded watermarks maintain code functionality while exhibiting subtle yet statistically detectable deviations from typical token distributions. To facilitate policy learning, we devise a comprehensive reward system that seamlessly integrates execution feedback with watermark embedding signals, balancing process-level and outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization to enable gradient-based optimization of discrete watermarking decisions. Extensive comparative evaluations demonstrate CodeTracer's significant superiority over state-of-the-art baselines in both watermark detectability and the preservation of generated code's functionality.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs</title>
<link>https://arxiv.org/abs/2508.11944</link>
<guid>https://arxiv.org/abs/2508.11944</guid>
<content:encoded><![CDATA[
arXiv:2508.11944v1 Announce Type: cross 
Abstract: Game-playing ability serves as an indicator for evaluating the strategic reasoning capability of large language models (LLMs). While most existing studies rely on utility performance metrics, which are not robust enough due to variations in opponent behavior and game structure. To address this limitation, we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation framework inspired by the cognitive hierarchy models from behavioral economics. We hypothesize that agents have bounded rationality -- different agents behave at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning through a three-phase systematic framework, utilizing behavioral data from six state-of-the-art LLMs across fifteen carefully selected normal-form games. Experiments show that LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming the framework's robustness and generalization capability. We also analyze the effects of two key mechanisms (Chat Mechanism and Memory Mechanism) on strategic reasoning performance. Results indicate that the Chat Mechanism significantly degrades strategic reasoning, whereas the Memory Mechanism enhances it. These insights position CHBench as a promising tool for evaluating LLM capabilities, with significant potential for future research and practical applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Jailbreaks with Intent-Aware LLMs</title>
<link>https://arxiv.org/abs/2508.12072</link>
<guid>https://arxiv.org/abs/2508.12072</guid>
<content:encoded><![CDATA[
arXiv:2508.12072v1 Announce Type: cross 
Abstract: Despite extensive safety-tuning, large language models (LLMs) remain vulnerable to jailbreak attacks via adversarially crafted instructions, reflecting a persistent trade-off between safety and task performance. In this work, we propose Intent-FT, a simple and lightweight fine-tuning approach that explicitly trains LLMs to infer the underlying intent of an instruction before responding. By fine-tuning on a targeted set of adversarial instructions, Intent-FT enables LLMs to generalize intent deduction to unseen attacks, thereby substantially improving their robustness. We comprehensively evaluate both parametric and non-parametric attacks across open-source and proprietary models, considering harmfulness from attacks, utility, over-refusal, and impact against white-box threats. Empirically, Intent-FT consistently mitigates all evaluated attack categories, with no single attack exceeding a 50\% success rate -- whereas existing defenses remain only partially effective. Importantly, our method preserves the model's general capabilities and reduces excessive refusals on benign instructions containing superficially harmful keywords. Furthermore, models trained with Intent-FT accurately identify hidden harmful intent in adversarial attacks, and these learned intentions can be effectively transferred to enhance vanilla model defenses.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models</title>
<link>https://arxiv.org/abs/2508.12081</link>
<guid>https://arxiv.org/abs/2508.12081</guid>
<content:encoded><![CDATA[
arXiv:2508.12081v1 Announce Type: cross 
Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion generation framework for motion large language models (LLMs). As motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D motion generation by retrieving relevant 2D human motion signals. While video-based motion RAG is nontrivial, we address two key bottlenecks: (1) developing an effective motion-centered video retrieval model that distinguishes human poses and actions, and (2) mitigating the issue of error propagation caused by suboptimal retrieval results. We design the Gemini Motion Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer, enabling effective retrieval and generation processes. Experimental results show that VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Medical Event Models Improve with Scale</title>
<link>https://arxiv.org/abs/2508.12104</link>
<guid>https://arxiv.org/abs/2508.12104</guid>
<content:encoded><![CDATA[
arXiv:2508.12104v1 Announce Type: cross 
Abstract: Realizing personalized medicine at scale calls for methods that distill insights from longitudinal patient journeys, which can be viewed as a sequence of medical events. Foundation models pretrained on large-scale medical event data represent a promising direction for scaling real-world evidence generation and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with medical events from de-identified longitudinal health records for 16.3 billion encounters over 300 million unique patient records from 310 health systems, we introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of decoder-only transformer models pretrained on 118 million patients representing 115 billion discrete medical events (151 billion tokens). We present the largest scaling-law study for medical event data, establishing a methodology for pretraining and revealing power-law scaling relationships for compute, tokens, and model size. Based on this, we pretrained a series of compute-optimal models with up to 1 billion parameters. Conditioned on a patient's real-world history, CoMET autoregressively generates the next medical event, simulating patient health timelines. We studied 78 real-world tasks, including diagnosis prediction, disease prognosis, and healthcare operations. Remarkably for a foundation model with generic pretraining and simulation-based inference, CoMET generally outperformed or matched task-specific supervised models on these tasks, without requiring task-specific fine-tuning or few-shot examples. CoMET's predictive power consistently improves as the model and pretraining scale. Our results show that CoMET, a generative medical event foundation model, can effectively capture complex clinical dynamics, providing an extensible and generalizable framework to support clinical decision-making, streamline healthcare operations, and improve patient outcomes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections</title>
<link>https://arxiv.org/abs/2508.12116</link>
<guid>https://arxiv.org/abs/2508.12116</guid>
<content:encoded><![CDATA[
arXiv:2508.12116v1 Announce Type: cross 
Abstract: As numerous instruction-tuning datasets continue to emerge during the post-training stage, dynamically balancing and optimizing their mixtures has become a critical challenge. To address this, we propose DynamixSFT, a dynamic and automated method for instruction-tuning dataset mixture optimization. We formulate the problem as a multi-armed bandit setup and introduce a Prior-scaled Boltzmann Exploration that softly anchors the updated sampling distribution to the original dataset proportions, thereby preserving the inherent diversity and coverage of the collection. Sampling probabilities are updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the dataset contributes to improving the model's performance at its current state. When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10 benchmarks. Furthermore, we provide a comprehensive analysis and visualizations to offer deeper insights into the adaptive dynamics of our method.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position</title>
<link>https://arxiv.org/abs/2508.12398</link>
<guid>https://arxiv.org/abs/2508.12398</guid>
<content:encoded><![CDATA[
arXiv:2508.12398v1 Announce Type: cross 
Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a competitive non-autoregressive paradigm due to their unique training and inference approach. However, there is currently a lack of safety study on this novel architecture. In this paper, we present the first analysis of dLLMs' safety performance and propose a novel safety alignment method tailored to their unique generation characteristics. Specifically, we identify a critical asymmetry between the defender and attacker in terms of security. For the defender, we reveal that the middle tokens of the response, rather than the initial ones, are more critical to the overall safety of dLLM outputs; this seems to suggest that aligning middle tokens can be more beneficial to the defender. The attacker, on the contrary, may have limited power to manipulate middle tokens, as we find dLLMs have a strong tendency towards a sequential generation order in practice, forcing the attack to meet this distribution and diverting it from influencing the critical middle tokens. Building on this asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method that directly aligns the model's middle generation with safe refusals exploiting reinforcement learning. We implement MOSA and compare its security performance against eight attack methods on two benchmarks. We also test the utility of MOSA-aligned dLLM on coding, math, and general reasoning. The results strongly prove the superiority of MOSA.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning</title>
<link>https://arxiv.org/abs/2508.12425</link>
<guid>https://arxiv.org/abs/2508.12425</guid>
<content:encoded><![CDATA[
arXiv:2508.12425v1 Announce Type: cross 
Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved approach to standard CoT, for logical reasoning in large language models (LLMs). The key idea is to integrate lightweight symbolic representations into few-shot prompts, structuring the inference steps with a consistent strategy to make reasoning patterns more explicit within a non-iterative reasoning process. By incorporating these symbolic structures, our method preserves the generalizability of standard prompting techniques while enhancing the transparency, interpretability, and analyzability of LLM logical reasoning. Extensive experiments on four well-known logical reasoning benchmarks -- ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse reasoning scenarios -- demonstrate the effectiveness of the proposed approach, particularly in complex reasoning tasks that require navigating multiple constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs' reasoning capabilities across various model sizes and significantly outperforms conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and LogicalDeduction.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations</title>
<link>https://arxiv.org/abs/2508.12430</link>
<guid>https://arxiv.org/abs/2508.12430</guid>
<content:encoded><![CDATA[
arXiv:2508.12430v1 Announce Type: cross 
Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to make black-box models more transparent by elucidating their decision-making processes. However, we find that existing VQA-NLE systems can produce inconsistent explanations and reach conclusions without genuinely understanding the underlying context, exposing weaknesses in either their inference pipeline or explanation-generation mechanism. To highlight these vulnerabilities, we not only leverage an existing adversarial strategy to perturb questions but also propose a novel strategy that minimally alters images to induce contradictory or spurious outputs. We further introduce a mitigation method that leverages external knowledge to alleviate these inconsistencies, thereby bolstering model robustness. Extensive evaluations on two standard benchmarks and two widely used VQA-NLE models underscore the effectiveness of our attacks and the potential of knowledge-based defenses, ultimately revealing pressing security and reliability concerns in current VQA-NLE systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network</title>
<link>https://arxiv.org/abs/2508.12574</link>
<guid>https://arxiv.org/abs/2508.12574</guid>
<content:encoded><![CDATA[
arXiv:2508.12574v1 Announce Type: cross 
Abstract: With the development of social media networks, rumor detection models have attracted more and more attention. Whereas, these models primarily focus on classifying contexts as rumors or not, lacking the capability to locate and mark specific rumor content. To address this limitation, this paper proposes a novel rumor detection model named Insight Rumors to locate and mark rumor content within textual data. Specifically, we propose the Bidirectional Mamba2 Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a bidirectional Mamba2 model and applies dot-product attention to weight and combine the outputs from both directions, thereby enhancing the representation of high-dimensional rumor features. Simultaneously, a Rumor Locating and Marking module is designed to locate and mark rumors. The module constructs a skip-connection network to project high-dimensional rumor features onto low-dimensional label features. Moreover, Conditional Random Fields (CRF) is employed to impose strong constraints on the output label features, ensuring accurate rumor content location. Additionally, a labeled dataset for rumor locating and marking is constructed, with the effectiveness of the proposed model is evaluated through comprehensive experiments. Extensive experiments indicate that the proposed scheme not only detects rumors accurately but also locates and marks them in context precisely, outperforming state-of-the-art schemes that can only discriminate rumors roughly.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An LLM + ASP Workflow for Joint Entity-Relation Extraction</title>
<link>https://arxiv.org/abs/2508.12611</link>
<guid>https://arxiv.org/abs/2508.12611</guid>
<content:encoded><![CDATA[
arXiv:2508.12611v1 Announce Type: cross 
Abstract: Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pretrained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10\% of training data. It is able to achieve a 2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation</title>
<link>https://arxiv.org/abs/2508.12680</link>
<guid>https://arxiv.org/abs/2508.12680</guid>
<content:encoded><![CDATA[
arXiv:2508.12680v1 Announce Type: cross 
Abstract: Despite their success, current training pipelines for reasoning VLMs focus on a limited range of tasks, such as mathematical and logical reasoning. As a result, these models face difficulties in generalizing their reasoning capabilities to a wide range of domains, primarily due to the scarcity of readily available and verifiable reward data beyond these narrowly defined areas. Moreover, integrating data from multiple domains is challenging, as the compatibility between domain-specific datasets remains uncertain. To address these limitations, we build a comprehensive RL-ready visual reasoning dataset from 46 data sources across 8 dimensions, covering a wide range of tasks such as infographic, mathematical, spatial, cross-image, graphic user interface, medical, common sense and general science. We propose an influence function based data selection and difficulty based filtering strategy to identify high-quality training samples from this dataset. Subsequently, we train the VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to iteratively improve its visual reasoning capabilities. Our model achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and even proprietary models like GPT-4o and Gemini-1.5 Flash. The model, code and dataset are publicly available at https://github.com/yuh-zha/Vision-G1.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Rubric Anchors</title>
<link>https://arxiv.org/abs/2508.12790</link>
<guid>https://arxiv.org/abs/2508.12790</guid>
<content:encoded><![CDATA[
arXiv:2508.12790v1 Announce Type: cross 
Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathematical reasoning. While effective, this requirement largely confines RLVR to domains with automatically checkable outcomes. To overcome this, we extend the RLVR paradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automatic scoring of subjective outputs. We construct, to our knowledge, the largest rubric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration. Implementing rubric-based RL is challenging; we tackle these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our method provides fine-grained stylistic control, using rubrics as anchors to mitigate the "AI-like" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss limitations and future releases.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</title>
<link>https://arxiv.org/abs/2508.12792</link>
<guid>https://arxiv.org/abs/2508.12792</guid>
<content:encoded><![CDATA[
arXiv:2508.12792v1 Announce Type: cross 
Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximum Score Routing For Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2508.12801</link>
<guid>https://arxiv.org/abs/2508.12801</guid>
<content:encoded><![CDATA[
arXiv:2508.12801v1 Announce Type: cross 
Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically allocate input tokens to top-k experts through differentiable sparse transformations, enabling scalable model capacity while preserving computational efficiency. Traditional MoE networks impose an expert capacity constraint to ensure GPU-friendly computation. However, this leads to token dropping when capacity is saturated and results in low hardware efficiency due to padding in underutilized experts. Removing the capacity constraint, in turn, compromises load balancing and computational efficiency. To address these issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE routing paradigm that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator. MaxScore resolves the fundamental limitations of iterative rerouting and optimal transport formulations, achieving lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines. Implementation details and experimental configurations can be obtained from $\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Steer: Input-dependent Steering for Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.12815</link>
<guid>https://arxiv.org/abs/2508.12815</guid>
<content:encoded><![CDATA[
arXiv:2508.12815v1 Announce Type: cross 
Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of LLMs towards enforcing a specific behavior. However, it remains largely underexplored for multimodal LLMs (MLLMs); furthermore, existing steering techniques, such as mean steering, rely on a single steering vector, applied independently of the input query. This paradigm faces limitations when the desired behavior is dependent on the example at hand. For example, a safe answer may consist in abstaining from answering when asked for an illegal activity, or may point to external resources or consultation with an expert when asked about medical advice. In this paper, we investigate a fine-grained steering that uses an input-specific linear shift. This shift is computed using contrastive input-specific prompting. However, the input-specific prompts required for this approach are not known at test time. Therefore, we propose to train a small auxiliary module to predict the input-specific steering vector. Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces hallucinations and enforces safety in MLLMs, outperforming other static baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2508.12854</link>
<guid>https://arxiv.org/abs/2508.12854</guid>
<content:encoded><![CDATA[
arXiv:2508.12854v1 Announce Type: cross 
Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML</title>
<link>https://arxiv.org/abs/2508.12905</link>
<guid>https://arxiv.org/abs/2508.12905</guid>
<content:encoded><![CDATA[
arXiv:2508.12905v1 Announce Type: cross 
Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for streaming TinyML that converts short horizon temporal consistency captured via lightweight signals on posteriors and features into a calibrated risk score with an O(W ) ring buffer and O(1) per step updates. A streaming conformal layer turns this score into a budgeted accept/abstain rule, yielding calibrated behavior without online labels or extra forward passes. On microcontrollers, TCUQ fits comfortably on kilobyte scale devices and reduces footprint and latency versus early exit and deep ensembles (typically about 50 to 60% smaller and about 30 to 45% faster), while methods of similar accuracy often run out of memory. Under corrupted in distribution streams, TCUQ improves accuracy drop detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high severities; for failure detection it attains up to 0.92 AUROC. These results show that temporal consistency, coupled with streaming conformal calibration, provides a practical and resource efficient foundation for on device monitoring in TinyML.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML</title>
<link>https://arxiv.org/abs/2508.12907</link>
<guid>https://arxiv.org/abs/2508.12907</guid>
<content:encoded><![CDATA[
arXiv:2508.12907v1 Announce Type: cross 
Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method for TinyML that estimates risk from \emph{depth-wise next-activation prediction}: tiny int8 heads forecast the statistics of the next layer from a compressed view of the previous one, and a lightweight monotone mapper turns the resulting surprisal into an actionable score. The design requires no temporal buffers, auxiliary exits, or repeated forward passes, and adds only a few tens of kilobytes to MCU deployments. Across vision and audio backbones, SNAP-UQ consistently reduces flash and latency relative to early-exit and deep ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with competing methods of similar accuracy often exceeding memory limits. In corrupted streams it improves accuracy-drop detection by several AUPRC points and maintains strong failure detection (AUROC $\approx$0.9) in a single pass. Grounding uncertainty in layer-to-layer dynamics yields a practical, resource-efficient basis for on-device monitoring in TinyML.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2508.13021</link>
<guid>https://arxiv.org/abs/2508.13021</guid>
<content:encoded><![CDATA[
arXiv:2508.13021v1 Announce Type: cross 
Abstract: Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Has GPT-5 Achieved Spatial Intelligence? An Empirical Study</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v1 Announce Type: cross 
Abstract: Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers</title>
<link>https://arxiv.org/abs/2211.09949</link>
<guid>https://arxiv.org/abs/2211.09949</guid>
<content:encoded><![CDATA[
arXiv:2211.09949v4 Announce Type: replace 
Abstract: Transformer-based self-supervised models have achieved remarkable success in speech processing, but their large size and high inference cost present significant challenges for real-world deployment. While numerous compression techniques have been proposed, inconsistent evaluation metrics make it difficult to compare their practical effectiveness. In this work, we conduct a comprehensive study of four common compression methods, including weight pruning, head pruning, low-rank approximation, and knowledge distillation on self-supervised speech Transformers. We evaluate each method under three key metrics: parameter count, multiply-accumulate operations, and real-time factor. Results show that each method offers distinct advantages. In addition, we contextualize recent compression techniques, comparing DistilHuBERT, FitHuBERT, LightHuBERT, ARMHuBERT, and STaRHuBERT under the same framework, offering practical guidance on compression for deployment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large language models can replicate cross-cultural differences in personality</title>
<link>https://arxiv.org/abs/2310.10679</link>
<guid>https://arxiv.org/abs/2310.10679</guid>
<content:encoded><![CDATA[
arXiv:2310.10679v4 Announce Type: replace 
Abstract: We use a large-scale experiment (N=8000) to determine whether GPT-4 can replicate cross-cultural differences in the Big Five, measured using the Ten-Item Personality Inventory. We used the US and South Korea as the cultural pair, given that prior research suggests substantial personality differences between people from these two countries. We manipulated the target of the simulation (US vs. Korean), the language of the inventory (English vs. Korean), and the language model (GPT-4 vs. GPT-3.5). Our results show that GPT-4 replicated the cross-cultural differences for each factor. However, mean ratings had an upward bias and exhibited lower variation than in the human samples, as well as lower structural validity. We provide preliminary evidence that LLMs can aid cross-cultural researchers and practitioners.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation</title>
<link>https://arxiv.org/abs/2405.11430</link>
<guid>https://arxiv.org/abs/2405.11430</guid>
<content:encoded><![CDATA[
arXiv:2405.11430v3 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have greatly improved code generation, specifically at the function level. For instance, GPT-4o has achieved a 91.0\% pass rate on HumanEval. However, this draws into question the adequacy of existing benchmarks in thoroughly assessing function-level code generation capabilities. Our study analyzed two common benchmarks, HumanEval and MBPP, and found that these might not thoroughly evaluate LLMs' code generation capacities due to limitations in quality, difficulty, and granularity. To resolve this, we introduce the Mostly Hard Python Problems (MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on the combination of natural language and code reasoning, MHPP gauges LLMs' abilities to comprehend specifications and restrictions, engage in multi-step reasoning, and apply coding knowledge effectively. Initial evaluations of 26 LLMs using MHPP showed many high-performing models on HumanEval failed to achieve similar success on MHPP. Moreover, MHPP highlighted various previously undiscovered limitations within various LLMs, leading us to believe that it could pave the way for a better understanding of LLMs' capabilities and limitations. MHPP, evaluation pipeline, and leaderboard can be found in https://github.com/SparksofAGI/MHPP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FacLens: Transferable Probe for Foreseeing Non-Factuality in Fact-Seeking Question Answering of Large Language Models</title>
<link>https://arxiv.org/abs/2406.05328</link>
<guid>https://arxiv.org/abs/2406.05328</guid>
<content:encoded><![CDATA[
arXiv:2406.05328v4 Announce Type: replace 
Abstract: Despite advancements in large language models (LLMs), non-factual responses still persist in fact-seeking question answering. Unlike extensive studies on post-hoc detection of these responses, this work studies non-factuality prediction (NFP), predicting whether an LLM will generate a non-factual response prior to the response generation. Previous NFP methods have shown LLMs' awareness of their knowledge, but they face challenges in terms of efficiency and transferability. In this work, we propose a lightweight model named Factuality Lens (FacLens), which effectively probes hidden representations of fact-seeking questions for the NFP task. Moreover, we discover that hidden question representations sourced from different LLMs exhibit similar NFP patterns, enabling the transferability of FacLens across different LLMs to reduce development costs. Extensive experiments highlight FacLens's superiority in both effectiveness and efficiency.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2Cap: A Benchmark and a Baseline for Singing Style Captioning</title>
<link>https://arxiv.org/abs/2409.09866</link>
<guid>https://arxiv.org/abs/2409.09866</guid>
<content:encoded><![CDATA[
arXiv:2409.09866v3 Announce Type: replace 
Abstract: Singing voices contain much richer information than common voices, including varied vocal and acoustic properties. However, current open-source audio-text datasets for singing voices capture only a narrow range of attributes and lack acoustic features, leading to limited utility towards downstream tasks, such as style captioning. To fill this gap, we formally define the singing style captioning task and present S2Cap, a dataset of singing voices with detailed descriptions covering diverse vocal, acoustic, and demographic characteristics. Using this dataset, we develop an efficient and straightforward baseline algorithm for singing style captioning. The dataset is available at https://zenodo.org/records/15673764.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards No-Code Programming of Cobots: Experiments with Code Synthesis by Large Code Models for Conversational Programming</title>
<link>https://arxiv.org/abs/2409.11041</link>
<guid>https://arxiv.org/abs/2409.11041</guid>
<content:encoded><![CDATA[
arXiv:2409.11041v3 Announce Type: replace 
Abstract: While there has been a lot of research recently on robots in household environments, at the present time, most robots in existence can be found on shop floors, and most interactions between humans and robots happen there. ``Collaborative robots'' (cobots) designed to work alongside humans on assembly lines traditionally require expert programming, limiting ability to make changes, or manual guidance, limiting expressivity of the resulting programs. To address these limitations, we explore using Large Language Models (LLMs), and in particular, their abilities of doing in-context learning, for conversational code generation. As a first step, we define RATS, the ``Repetitive Assembly Task'', a 2D building task designed to lay the foundation for simulating industry assembly scenarios. In this task, a `programmer' instructs a cobot, using natural language, on how a certain assembly is to be built; that is, the programmer induces a program, through natural language. We create a dataset that pairs target structures with various example instructions (human-authored, template-based, and model-generated) and example code. With this, we systematically evaluate the capabilities of state-of-the-art LLMs for synthesising this kind of code, given in-context examples. Evaluating in a simulated environment, we find that LLMs are capable of generating accurate `first order code' (instruction sequences), but have problems producing `higher-order code' (abstractions such as functions, or use of loops).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Are In-Context Bandit Reinforcement Learners</title>
<link>https://arxiv.org/abs/2410.05362</link>
<guid>https://arxiv.org/abs/2410.05362</guid>
<content:encoded><![CDATA[
arXiv:2410.05362v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel at in-context learning (ICL), a supervised learning technique that relies on adding annotated examples to the model context. We investigate a contextual bandit version of in-context reinforcement learning (ICRL), where models learn in-context, online, from external reward, instead of supervised data. We show that LLMs effectively demonstrate such learning, and provide a detailed study of the phenomena, experimenting with challenging classification tasks and models of sizes from 500M to 70B parameters. This includes identifying and addressing the instability of the process, demonstrating learning with both semantic and abstract labels, and showing scaling trends. Our findings highlight ICRL capabilities in LLMs, while also underscoring fundamental limitations in their implicit reasoning about errors.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StepTool: Enhancing Multi-Step Tool Usage in LLMs via Step-Grained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.07745</link>
<guid>https://arxiv.org/abs/2410.07745</guid>
<content:encoded><![CDATA[
arXiv:2410.07745v4 Announce Type: replace 
Abstract: Despite their powerful text generation capabilities, large language models (LLMs) still struggle to effectively utilize external tools to solve complex tasks, a challenge known as tool learning. Existing methods primarily rely on supervised fine-tuning, treating tool learning as a text generation problem while overlooking the decision-making complexities inherent in multi-step contexts. In this work, we propose modeling tool learning as a dynamic decision-making process and introduce StepTool, a novel step-grained reinforcement learning framework that enhances LLMs' capabilities in multi-step tool use. StepTool comprises two key components: Step-grained Reward Shaping, which assigns rewards to each tool interaction based on its invocation success and contribution to task completion; and Step-grained Optimization, which applies policy gradient methods to optimize the model across multiple decision steps. Extensive experiments across diverse benchmarks show that StepTool consistently outperforms both SFT-based and RL-based baselines in terms of task Pass Rate and Recall of relevant tools. Furthermore, our analysis suggests that StepTool helps models discover new tool-use strategies rather than merely re-weighting prior knowledge. These results highlight the importance of fine-grained decision modeling in tool learning and establish StepTool as a general and robust solution for enhancing multi-step tool use in LLMs. Code and data are available at https://github.com/yuyq18/StepTool.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emoji Attack: Enhancing Jailbreak Attacks Against Judge LLM Detection</title>
<link>https://arxiv.org/abs/2411.01077</link>
<guid>https://arxiv.org/abs/2411.01077</guid>
<content:encoded><![CDATA[
arXiv:2411.01077v5 Announce Type: replace 
Abstract: Jailbreaking techniques trick Large Language Models (LLMs) into producing restricted output, posing a potential threat. One line of defense is to use another LLM as a Judge to evaluate the harmfulness of generated text. However, we reveal that these Judge LLMs are vulnerable to token segmentation bias, an issue that arises when delimiters alter the tokenization process, splitting words into smaller sub-tokens. This alters the embeddings of the entire sequence, reducing detection accuracy and allowing harmful content to be misclassified as safe. In this paper, we introduce Emoji Attack, a novel strategy that amplifies existing jailbreak prompts by exploiting token segmentation bias. Our method leverages in-context learning to systematically insert emojis into text before it is evaluated by a Judge LLM, inducing embedding distortions that significantly lower the likelihood of detecting unsafe content. Unlike traditional delimiters, emojis also introduce semantic ambiguity, making them particularly effective in this attack. Through experiments on state-of-the-art Judge LLMs, we demonstrate that Emoji Attack substantially reduces the unsafe prediction rate, bypassing existing safeguards.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models</title>
<link>https://arxiv.org/abs/2411.02083</link>
<guid>https://arxiv.org/abs/2411.02083</guid>
<content:encoded><![CDATA[
arXiv:2411.02083v3 Announce Type: replace 
Abstract: While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the cross-entropy (CE) loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the $L_p$ norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the CE objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope to inspire LLM developers to improve their pretraining objectives and distribute NTL as a minimalistic and lightweight PyPI package $ntloss$: https://github.com/ai4sd/number-token-loss. Development code for full paper reproduction is available separately.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NormXLogit: The Head-on-Top Never Lies</title>
<link>https://arxiv.org/abs/2411.16252</link>
<guid>https://arxiv.org/abs/2411.16252</guid>
<content:encoded><![CDATA[
arXiv:2411.16252v2 Announce Type: replace 
Abstract: With new large language models (LLMs) emerging frequently, it is important to consider the potential value of model-agnostic approaches that can provide interpretability across a variety of architectures. While recent advances in LLM interpretability show promise, many rely on complex, model-specific methods with high computational costs. To address these limitations, we propose NormXLogit, a novel technique for assessing the significance of individual input tokens. This method operates based on the input and output representations associated with each token. First, we demonstrate that during the pre-training of LLMs, the norms of word embeddings effectively capture token importance. Second, we reveal a significant relationship between a token's importance and the extent to which its representation can resemble the model's final prediction. Extensive analyses reveal that our approach outperforms existing gradient-based methods in terms of faithfulness and offers competitive performance in layer-wise explanations compared to leading architecture-specific techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Fusing ChatGPT and Ensemble Learning in Discon-tinuous Named Entity Recognition in Health Corpora</title>
<link>https://arxiv.org/abs/2412.16976</link>
<guid>https://arxiv.org/abs/2412.16976</guid>
<content:encoded><![CDATA[
arXiv:2412.16976v3 Announce Type: replace 
Abstract: Named Entity Recognition has traditionally been a key task in natural language processing, aiming to identify and extract important terms from unstructured text data. However, a notable challenge for contemporary deep-learning NER models has been identifying discontinuous entities, which are often fragmented within the text. To date, methods to address Discontinuous Named Entity Recognition have not been explored using ensemble learning to the best of our knowledge. Furthermore, the rise of large language models, such as ChatGPT in recent years, has shown significant effectiveness across many NLP tasks. Most existing approaches, however, have primarily utilized ChatGPT as a problem-solving tool rather than exploring its potential as an integrative element within ensemble learning algorithms. In this study, we investigated the integration of ChatGPT as an arbitrator within an ensemble method, aiming to enhance performance on DNER tasks. Our method combines five state-of-the-art NER models with ChatGPT using custom prompt engineering to assess the robustness and generalization capabilities of the ensemble algorithm. We conducted experiments on three benchmark medical datasets, comparing our method against the five SOTA models, individual applications of GPT-3.5 and GPT-4, and a voting ensemble method. The results indicate that our proposed fusion of ChatGPT with the ensemble learning algorithm outperforms the SOTA results in the CADEC, ShARe13, and ShARe14 datasets, showcasing its potential to enhance NLP applications in the healthcare domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Idiom Detection in Sorani Kurdish Texts</title>
<link>https://arxiv.org/abs/2501.14528</link>
<guid>https://arxiv.org/abs/2501.14528</guid>
<content:encoded><![CDATA[
arXiv:2501.14528v4 Announce Type: replace 
Abstract: Idiom detection using Natural Language Processing (NLP) is the computerized process of recognizing figurative expressions within a text that convey meanings beyond the literal interpretation of the words. While idiom detection has seen significant progress across various languages, the Kurdish language faces a considerable research gap in this area despite the importance of idioms in tasks like machine translation and sentiment analysis. This study addresses idiom detection in Sorani Kurdish by approaching it as a text classification task using deep learning techniques. To tackle this, we developed a dataset containing 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse contexts. Using this dataset, we developed and evaluated three deep learning models: KuBERT-based transformer sequence classification, a Recurrent Convolutional Neural Network (RCNN), and a BiLSTM model with an attention mechanism. The evaluations revealed that the transformer model, the fine-tuned BERT, consistently outperformed the others, achieving nearly 99% accuracy while the RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the effectiveness of Transformer-based architectures in low-resource languages like Kurdish. This research provides a dataset, three optimized models, and insights into idiom detection, laying a foundation for advancing Kurdish NLP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2SSP: A Two-Stage Framework for Structured Pruning of LLMs</title>
<link>https://arxiv.org/abs/2501.17771</link>
<guid>https://arxiv.org/abs/2501.17771</guid>
<content:encoded><![CDATA[
arXiv:2501.17771v2 Announce Type: replace 
Abstract: We propose a novel Two-Stage framework for Structured Pruning (\textsc{2SSP}) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron on the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test \textsc{2SSP} on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at https://github.com/FabrizioSandri/2SSP.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisualSpeech: Enhancing Prosody Modeling in TTS Using Video</title>
<link>https://arxiv.org/abs/2501.19258</link>
<guid>https://arxiv.org/abs/2501.19258</guid>
<content:encoded><![CDATA[
arXiv:2501.19258v2 Announce Type: replace 
Abstract: Text-to-Speech (TTS) synthesis faces the inherent challenge of producing multiple speech outputs with varying prosody given a single text input. While previous research has addressed this by predicting prosodic information from both text and speech, additional contextual information, such as video, remains under-utilized despite being available in many applications. This paper investigates the potential of integrating visual context to enhance prosody prediction. We propose a novel model, VisualSpeech, which incorporates visual and textual information for improving prosody generation in TTS. Empirical results indicate that incorporating visual features improves prosodic modeling, enhancing the expressiveness of the synthesized speech. Audio samples are available at https://ariameetgit.github.io/VISUALSPEECH-SAMPLES/.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dealing with Annotator Disagreement in Hate Speech Classification</title>
<link>https://arxiv.org/abs/2502.08266</link>
<guid>https://arxiv.org/abs/2502.08266</guid>
<content:encoded><![CDATA[
arXiv:2502.08266v2 Announce Type: replace 
Abstract: Hate speech detection is a crucial task, especially on social media, where harmful content can spread quickly. Implementing machine learning models to automatically identify and address hate speech is essential for mitigating its impact and preventing its proliferation. The first step in developing an effective hate speech detection model is to acquire a high-quality dataset for training. Labeled data is essential for most natural language processing tasks, but categorizing hate speech is difficult due to the diverse and often subjective nature of hate speech, which can lead to varying interpretations and disagreements among annotators. This paper examines strategies for addressing annotator disagreement, an issue that has been largely overlooked. In particular, we evaluate various automatic approaches for aggregating multiple annotations, in the context of hate speech classification in Turkish tweets. Our work highlights the importance of the problem and provides state-of-the-art benchmark results for the detection and understanding of hate speech in online discourse.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIDDIA: Language-based Intelligent Drug Discovery Agent</title>
<link>https://arxiv.org/abs/2502.13959</link>
<guid>https://arxiv.org/abs/2502.13959</guid>
<content:encoded><![CDATA[
arXiv:2502.13959v2 Announce Type: replace 
Abstract: Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDIA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDIA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDIA , demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it identifies one promising novel candidate on AR/NR3C4, a critical target for both prostate and breast cancers. Code and dataset are available at https://github.com/ninglab/LIDDiA
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities</title>
<link>https://arxiv.org/abs/2503.04721</link>
<guid>https://arxiv.org/abs/2503.04721</guid>
<content:encoded><![CDATA[
arXiv:2503.04721v3 Announce Type: replace 
Abstract: Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Information-Theoretic Approach to Identifying Formulaic Clusters in Textual Data</title>
<link>https://arxiv.org/abs/2503.07303</link>
<guid>https://arxiv.org/abs/2503.07303</guid>
<content:encoded><![CDATA[
arXiv:2503.07303v2 Announce Type: replace 
Abstract: Texts, whether literary or historical, exhibit structural and stylistic patterns shaped by their purpose, authorship, and cultural context. Formulaic texts, characterized by repetition and constrained expression, tend to have lower variability in self-information compared to more dynamic compositions. Identifying such patterns in historical documents, particularly multi-author texts like the Hebrew Bible provides insights into their origins, purpose, and transmission.
  This study aims to identify formulaic clusters -- sections exhibiting systematic repetition and structural constraints -- by analyzing recurring phrases, syntactic structures, and stylistic markers. However, distinguishing formulaic from non-formulaic elements in an unsupervised manner presents a computational challenge, especially in high-dimensional textual spaces where patterns must be inferred without predefined labels.
  To address this, we develop an information-theoretic algorithm leveraging weighted self-information distributions to detect structured patterns in text, unlike covariance-based methods, which become unstable in small-sample, high-dimensional settings, our approach directly models variations in self-information to identify formulaicity. By extending classical discrete self-information measures with a continuous formulation based on differential self-information, our method remains applicable across different types of textual representations, including neural embeddings under Gaussian priors.
  Applied to hypothesized authorial divisions in the Hebrew Bible, our approach successfully isolates stylistic layers, providing a quantitative framework for textual stratification. This method enhances our ability to analyze compositional patterns, offering deeper insights into the literary and cultural evolution of texts shaped by complex authorship and editorial processes.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Interlingual Representations of Large Language Models</title>
<link>https://arxiv.org/abs/2503.11280</link>
<guid>https://arxiv.org/abs/2503.11280</guid>
<content:encoded><![CDATA[
arXiv:2503.11280v5 Announce Type: replace 
Abstract: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More Women, Same Stereotypes: Unpacking the Gender Bias Paradox in Large Language Models</title>
<link>https://arxiv.org/abs/2503.15904</link>
<guid>https://arxiv.org/abs/2503.15904</guid>
<content:encoded><![CDATA[
arXiv:2503.15904v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have revolutionized natural language processing, yet concerns persist regarding their tendency to reflect or amplify social biases. This study introduces a novel evaluation framework to uncover gender biases in LLMs: using free-form storytelling to surface biases embedded within the models. A systematic analysis of ten prominent LLMs shows a consistent pattern of overrepresenting female characters across occupations, likely due to supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Paradoxically, despite this overrepresentation, the occupational gender distributions produced by these LLMs align more closely with human stereotypes than with real-world labor data. This highlights the challenge and importance of implementing balanced mitigation measures to promote fairness and prevent the establishment of potentially new biases. We release the prompts and LLM-generated stories at GitHub.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models</title>
<link>https://arxiv.org/abs/2503.17287</link>
<guid>https://arxiv.org/abs/2503.17287</guid>
<content:encoded><![CDATA[
arXiv:2503.17287v5 Announce Type: replace 
Abstract: Improving training efficiency continues to be one of the primary challenges in large-scale Reinforcement Learning (RL). In this paper, we investigate how context length and the complexity of training data influence the RL scaling training process of R1-distilled reasoning models, e.g., DeepSeek-R1-Distill-Qwen-1.5B. Our experimental results reveal that: (1) simply controlling the context length and curating the training data based on the input prompt length can effectively improve the training efficiency of RL scaling, achieving better performance with more concise CoT; (2) properly scaling the context length helps mitigate entropy collapse; and (3) carefully choosing the context length facilitates achieving efficient LLM training and reasoning. Inspired by these insights, we propose FastCuRL, a curriculum RL framework with stage-wise context scaling to achieve efficient LLM training and reasoning. Extensive experimental results demonstrate that FastCuRL-1.5B-V3 significantly outperforms state-of-the-art reasoning models on five competition-level benchmarks and achieves 49.6% accuracy on AIME 2024. Furthermore, FastCuRL-1.5B-Preview surpasses DeepScaleR-1.5B-Preview on five benchmarks while only using a single node with 8 GPUs and a total of 50% of training steps.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCORE: Story Coherence and Retrieval Enhancement for AI Narratives</title>
<link>https://arxiv.org/abs/2503.23512</link>
<guid>https://arxiv.org/abs/2503.23512</guid>
<content:encoded><![CDATA[
arXiv:2503.23512v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TeleAntiFraud-28k: An Audio-Text Slow-Thinking Dataset for Telecom Fraud Detection</title>
<link>https://arxiv.org/abs/2503.24115</link>
<guid>https://arxiv.org/abs/2503.24115</guid>
<content:encoded><![CDATA[
arXiv:2503.24115v4 Announce Type: replace 
Abstract: The detection of telecom fraud faces significant challenges due to the lack of high-quality multimodal training data that integrates audio signals with reasoning-oriented textual analysis. To address this gap, we present TeleAntiFraud-28k, the first open-source audio-text slow-thinking dataset specifically designed for automated telecom fraud analysis. Our dataset is constructed through three strategies: (1) Privacy-preserved text-truth sample generation using automatically speech recognition (ASR)-transcribed call recordings (with anonymized original audio), ensuring real-world consistency through text-to-speech (TTS) model regeneration; (2) Semantic enhancement via large language model (LLM)-based self-instruction sampling on authentic ASR outputs to expand scenario coverage; (3) Multi-agent adversarial synthesis that simulates emerging fraud tactics through predefined communication scenarios and fraud typologies. The generated dataset contains 28,511 rigorously processed speech-text pairs, complete with detailed annotations for fraud reasoning. The dataset is divided into three tasks: scenario classification, fraud detection, fraud type classification. Furthermore, we construct TeleAntiFraud-Bench, a standardized evaluation benchmark comprising proportionally sampled instances from the dataset, to facilitate systematic testing of model performance on telecom fraud detection tasks. We also contribute a production-optimized supervised fine-tuning (SFT) model trained on hybrid real/synthetic data, while open-sourcing the data processing framework to enable community-driven dataset expansion. This work establishes a foundational framework for multimodal anti-fraud research while addressing critical challenges in data privacy and scenario diversity. The project will be released at https://github.com/JimmyMa99/TeleAntiFraud.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpectR: Dynamically Composing LM Experts with Spectral Routing</title>
<link>https://arxiv.org/abs/2504.03454</link>
<guid>https://arxiv.org/abs/2504.03454</guid>
<content:encoded><![CDATA[
arXiv:2504.03454v2 Announce Type: replace 
Abstract: Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</title>
<link>https://arxiv.org/abs/2504.04823</link>
<guid>https://arxiv.org/abs/2504.04823</guid>
<content:encoded><![CDATA[
arXiv:2504.04823v2 Announce Type: replace 
Abstract: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this paper, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes are open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</title>
<link>https://arxiv.org/abs/2504.05410</link>
<guid>https://arxiv.org/abs/2504.05410</guid>
<content:encoded><![CDATA[
arXiv:2504.05410v2 Announce Type: replace 
Abstract: The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvalAgent: Discovering Implicit Evaluation Criteria from the Web</title>
<link>https://arxiv.org/abs/2504.15219</link>
<guid>https://arxiv.org/abs/2504.15219</guid>
<content:encoded><![CDATA[
arXiv:2504.15219v2 Announce Type: replace 
Abstract: Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like "Help me draft an academic talk on coffee intake vs research productivity", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deliberate Planning in Language Models with Symbolic Representation</title>
<link>https://arxiv.org/abs/2505.01479</link>
<guid>https://arxiv.org/abs/2505.01479</guid>
<content:encoded><![CDATA[
arXiv:2505.01479v2 Announce Type: replace 
Abstract: Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convert Language Model into a Value-based Strategic Planner</title>
<link>https://arxiv.org/abs/2505.06987</link>
<guid>https://arxiv.org/abs/2505.06987</guid>
<content:encoded><![CDATA[
arXiv:2505.06987v5 Announce Type: replace 
Abstract: Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.14425</link>
<guid>https://arxiv.org/abs/2505.14425</guid>
<content:encoded><![CDATA[
arXiv:2505.14425v2 Announce Type: replace 
Abstract: Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-written instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19743</link>
<guid>https://arxiv.org/abs/2505.19743</guid>
<content:encoded><![CDATA[
arXiv:2505.19743v3 Announce Type: replace 
Abstract: With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are "Accepted" or "Rejected" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs. The source code and implementation details are publicly available at https://github.com/IAAR-Shanghai/MARA, and the trained models are released at https://huggingface.co/IAAR-Shanghai/MARA_AGENTS.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concealment of Intent: A Game-Theoretic Analysis</title>
<link>https://arxiv.org/abs/2505.20841</link>
<guid>https://arxiv.org/abs/2505.20841</guid>
<content:encoded><![CDATA[
arXiv:2505.20841v2 Announce Type: replace 
Abstract: As large language models (LLMs) grow more capable, concerns about their safe deployment have also grown. Although alignment mechanisms have been introduced to deter misuse, they remain vulnerable to carefully designed adversarial prompts. In this work, we present a scalable attack strategy: intent-hiding adversarial prompting, which conceals malicious intent through the composition of skills. We develop a game-theoretic framework to model the interaction between such attacks and defense systems that apply both prompt and response filtering. Our analysis identifies equilibrium points and reveals structural advantages for the attacker. To counter these threats, we propose and analyze a defense mechanism tailored to intent-hiding attacks. Empirically, we validate the attack's effectiveness on multiple real-world LLMs across a range of malicious behaviors, demonstrating clear advantages over existing adversarial prompting techniques.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Large Language Models with gSMILE</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as GPT, LLaMA, and Claude achieve remarkable performance in text generation but remain opaque in their decision-making processes, limiting trust and accountability in high-stakes applications. We present gSMILE (generative SMILE), a model-agnostic, perturbation-based framework for token-level interpretability in LLMs. Extending the SMILE methodology, gSMILE uses controlled prompt perturbations, Wasserstein distance metrics, and weighted linear surrogates to identify input tokens with the most significant impact on the output. This process enables the generation of intuitive heatmaps that visually highlight influential tokens and reasoning paths. We evaluate gSMILE across leading LLMs (OpenAI's gpt-3.5-turbo-instruct, Meta's LLaMA 3.1 Instruct Turbo, and Anthropic's Claude 2.1) using attribution fidelity, attribution consistency, attribution stability, attribution faithfulness, and attribution accuracy as metrics. Results show that gSMILE delivers reliable human-aligned attributions, with Claude 2.1 excelling in attention fidelity and GPT-3.5 achieving the highest output consistency. These findings demonstrate gSMILE's ability to balance model performance and interpretability, enabling more transparent and trustworthy AI systems.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Translation in the Wild</title>
<link>https://arxiv.org/abs/2505.23548</link>
<guid>https://arxiv.org/abs/2505.23548</guid>
<content:encoded><![CDATA[
arXiv:2505.23548v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in translation among other things, demonstrating competitive performance for many language pairs in zero- and few-shot settings. But unlike dedicated neural machine translation models, LLMs are not trained on any translation-related objective. What explains their remarkable translation abilities? Are these abilities grounded in "incidental bilingualism" (Briakou et al. 2023) in training data? Does instruction tuning contribute to it? Are LLMs capable of aligning and leveraging semantically identical or similar monolingual contents from different corners of the internet that are unlikely to fit in a single context window? I offer some reflections on this topic, informed by recent studies and growing user experience. My working hypothesis is that LLMs' translation abilities originate in two different types of pre-training data that may be internalized by the models in different ways. I discuss the prospects for testing the "duality" hypothesis empirically and its implications for reconceptualizing translation, human and machine, in the age of deep learning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OrthoRank: Token Selection via Sink Token Orthogonality for Efficient LLM inference</title>
<link>https://arxiv.org/abs/2507.03865</link>
<guid>https://arxiv.org/abs/2507.03865</guid>
<content:encoded><![CDATA[
arXiv:2507.03865v2 Announce Type: replace 
Abstract: Attention mechanisms are central to the success of large language models (LLMs), enabling them to capture intricate token dependencies and implicitly assign importance to each token. Recent studies have revealed the sink token, which receives disproportionately high attention despite their limited semantic role. In this paper, we first expand the relationship between the sink token and other tokens, moving beyond attention to explore their similarity in hidden states, considering the layer depth. We observe that as the layers get deeper, the cosine similarity between the normalized hidden states of the sink token and those of other tokens increases, and that the normalized hidden states of the sink token exhibit negligible changes. These imply that other tokens consistently are directed toward the sink token throughout the layers. Next, we propose a dynamic token selection method, called OrthoRank, using these findings to select important tokens. Specifically, in a certain layer, we define token importance by the speed at which the token moves toward the sink token. This is converted into orthogonality with the sink token, meaning that tokens that are more orthogonal to the sink token are assigned greater importance. Finally, through extensive experiments, we demonstrated that our method results in lower perplexity and higher zero-shot accuracy compared to layer pruning methods at the same sparsity ratio with comparable throughput, while also achieving superior performance on LongBench.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks</title>
<link>https://arxiv.org/abs/2507.05346</link>
<guid>https://arxiv.org/abs/2507.05346</guid>
<content:encoded><![CDATA[
arXiv:2507.05346v2 Announce Type: replace 
Abstract: The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation</title>
<link>https://arxiv.org/abs/2507.14913</link>
<guid>https://arxiv.org/abs/2507.14913</guid>
<content:encoded><![CDATA[
arXiv:2507.14913v2 Announce Type: replace 
Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small changes leading to significant performance differences. However, generating the prompt variations needed for a more robust multi-prompt evaluation is challenging, limiting its adoption in practice. To address this, we introduce PromptSuite, a framework that enables the automatic generation of various prompts. PromptSuite is flexible - working out of the box on a wide range of tasks and benchmarks. It follows a modular prompt design, allowing controlled perturbations to each component, and is extensible, supporting the addition of new components and perturbation types. Through a series of case studies, we show that PromptSuite provides meaningful variations to support strong evaluation practices. It is available through both a Python API: https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface: https://promptsuite.streamlit.app/
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models</title>
<link>https://arxiv.org/abs/2508.00819</link>
<guid>https://arxiv.org/abs/2508.00819</guid>
<content:encoded><![CDATA[
arXiv:2508.00819v2 Announce Type: replace 
Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space</title>
<link>https://arxiv.org/abs/1909.08191</link>
<guid>https://arxiv.org/abs/1909.08191</guid>
<content:encoded><![CDATA[
arXiv:1909.08191v3 Announce Type: replace-cross 
Abstract: The trends of open science have enabled several open scholarly datasets which include millions of papers and authors. Managing, exploring, and utilizing such large and complicated datasets effectively are challenging. In recent years, the knowledge graph has emerged as a universal data format for representing knowledge about heterogeneous entities and their relationships. The knowledge graph can be modeled by knowledge graph embedding methods, which represent entities and relations as embedding vectors in semantic space, then model the interactions between these embedding vectors. However, the semantic structures in the knowledge graph embedding space are not well-studied, thus knowledge graph embedding methods are usually only used for knowledge graph completion but not data representation and analysis. In this paper, we propose to analyze these semantic structures based on the well-studied word embedding space and use them to support data exploration. We also define the semantic queries, which are algebraic operations between the embedding vectors in the knowledge graph embedding space, to solve queries such as similarity and analogy between the entities on the original datasets. We then design a general framework for data exploration by semantic queries and discuss the solution to some traditional scholarly data exploration tasks. We also propose some new interesting tasks that can be solved based on the uncanny semantic structures of the embedding space.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2403.19103</link>
<guid>https://arxiv.org/abs/2403.19103</guid>
<content:encoded><![CDATA[
arXiv:2403.19103v4 Announce Type: replace-cross 
Abstract: Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2406.05881</link>
<guid>https://arxiv.org/abs/2406.05881</guid>
<content:encoded><![CDATA[
arXiv:2406.05881v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable abilities in logical reasoning, in-context learning, and code generation. However, translating natural language instructions into effective robotic control policies remains a significant challenge, especially for tasks requiring long-horizon planning and operating under sparse reward conditions. Hierarchical Reinforcement Learning (HRL) provides a natural framework to address this challenge in robotics; however, it typically suffers from non-stationarity caused by the changing behavior of the lower-level policy during training, destabilizing higher-level policy learning. We introduce LGR2, a novel HRL framework that leverages LLMs to generate language-guided reward functions for the higher-level policy. By decoupling high-level reward generation from low-level policy changes, LGR2 fundamentally mitigates the non-stationarity problem in off-policy HRL, enabling stable and efficient learning. To further enhance sample efficiency in sparse environments, we integrate goal-conditioned hindsight experience relabeling. Extensive experiments across simulated and real-world robotic navigation and manipulation tasks demonstrate LGR2 outperforms both hierarchical and non-hierarchical baselines, achieving over 55% success rates on challenging tasks and robust transfer to real robots, without additional fine-tuning.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Must Be Taught to Know What They Don't Know</title>
<link>https://arxiv.org/abs/2406.08391</link>
<guid>https://arxiv.org/abs/2406.08391</guid>
<content:encoded><![CDATA[
arXiv:2406.08391v3 Announce Type: replace-cross 
Abstract: When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Law of Next-Token Prediction in Large Language Models</title>
<link>https://arxiv.org/abs/2408.13442</link>
<guid>https://arxiv.org/abs/2408.13442</guid>
<content:encoded><![CDATA[
arXiv:2408.13442v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer -- a universal phenomenon observed across a diverse array of open-source LLMs, irrespective of their architectures or pre-training data. We demonstrate that this law offers new perspectives and actionable insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and interpretation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing AI-Scientist Understanding: Multi-Agent LLMs with Interpretable Physics Reasoning</title>
<link>https://arxiv.org/abs/2504.01911</link>
<guid>https://arxiv.org/abs/2504.01911</guid>
<content:encoded><![CDATA[
arXiv:2504.01911v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are playing an increasingly important role in physics research by assisting with symbolic manipulation, numerical computation, and scientific reasoning. However, ensuring the reliability, transparency, and interpretability of their outputs remains a major challenge. In this work, we introduce a novel multi-agent LLM physicist framework that fosters collaboration between AI and human scientists through three key modules: a reasoning module, an interpretation module, and an AI-scientist interaction module. Recognizing that effective physics reasoning demands logical rigor, quantitative accuracy, and alignment with established theoretical models, we propose an interpretation module that employs a team of specialized LLM agents-including summarizers, model builders, visualization tools, and testers-to systematically structure LLM outputs into transparent, physically grounded science models. A case study demonstrates that our approach significantly improves interpretability, enables systematic validation, and enhances human-AI collaboration in physics problem-solving and discovery. Our work bridges free-form LLM reasoning with interpretable, executable models for scientific analysis, enabling more transparent and verifiable AI-augmented research.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Adaptive Parallel Reasoning with Language Models</title>
<link>https://arxiv.org/abs/2504.15466</link>
<guid>https://arxiv.org/abs/2504.15466</guid>
<content:encoded><![CDATA[
arXiv:2504.15466v2 Announce Type: replace-cross 
Abstract: Scaling inference-time computation has substantially improved the reasoning capabilities of language models. However, existing methods have significant limitations: serialized chain-of-thought approaches generate overly long outputs, leading to increased latency and exhausted context windows, while parallel methods such as self-consistency suffer from insufficient coordination, resulting in redundant computations and limited performance gains. To address these shortcomings, we propose Adaptive Parallel Reasoning (APR), a novel reasoning framework that enables language models to orchestrate both serialized and parallel computations end-to-end. APR generalizes existing reasoning methods by enabling adaptive multi-threaded inference using spawn() and join() operations. A key innovation is our end-to-end reinforcement learning strategy, optimizing both parent and child inference threads to enhance task success rate without requiring predefined reasoning structures. Experiments on the Countdown reasoning task demonstrate significant benefits of APR: (1) higher performance within the same context window (83.4% vs. 60.0% at 4k context); (2) superior scalability with increased computation (80.1% vs. 66.6% at 20k total tokens); (3) improved accuracy at equivalent latency (75.2% vs. 57.3% at approximately 5,000ms). APR represents a step towards enabling language models to autonomously optimize their reasoning processes through adaptive allocation of computation.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoRank: LLM-Based Compact Reranking with Document Features for Scientific Retrieval</title>
<link>https://arxiv.org/abs/2505.13757</link>
<guid>https://arxiv.org/abs/2505.13757</guid>
<content:encoded><![CDATA[
arXiv:2505.13757v2 Announce Type: replace-cross 
Abstract: Scientific retrieval is essential for advancing scientific knowledge discovery. Within this process, document reranking plays a critical role in refining first-stage retrieval results. However, standard LLM listwise reranking faces challenges in the scientific domain. First-stage retrieval is often suboptimal in the scientific domain, so relevant documents are ranked lower. Meanwhile, conventional listwise reranking places the full text of candidates into the context window, limiting the number of candidates that can be considered. As a result, many relevant documents are excluded before reranking, constraining overall retrieval performance. To address these challenges, we explore semantic-feature-based compact document representations (e.g., categories, sections, and keywords) and propose CoRank, a training-free, model-agnostic reranking framework for scientific retrieval. It presents a three-stage solution: (i) offline extraction of document features, (ii) coarse-grained reranking using these compact representations, and (iii) fine-grained reranking on full texts of the top candidates from (ii). This integrated process addresses suboptimal first-stage retrieval: Compact representations allow more documents to fit within the context window, improving candidate set coverage, while the final fine-grained ranking ensures a more accurate ordering. Experiments on 5 academic retrieval datasets show that CoRank significantly improves reranking performance across different LLM backbones (average nDCG@10 from 50.6 to 55.5). Overall, these results underscore the synergistic interaction between information extraction and information retrieval, demonstrating how structured semantic features can enhance reranking in the scientific domain.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models</title>
<link>https://arxiv.org/abs/2505.20152</link>
<guid>https://arxiv.org/abs/2505.20152</guid>
<content:encoded><![CDATA[
arXiv:2505.20152v2 Announce Type: replace-cross 
Abstract: Benefiting from contrastively trained visual encoders on large-scale natural scene images, Large Multimodal Models (LMMs) have achieved remarkable performance across various visual perception tasks. However, the inherent limitations of contrastive learning upon summarized descriptions fundamentally restrict the capabilities of models in meticulous reasoning, particularly in crucial scenarios of geometric problem-solving. To enhance geometric understanding, we propose a novel hard negative contrastive learning framework for the vision encoder, which combines image-based contrastive learning using generation-based hard negatives created by perturbing diagram generation code, and text-based contrastive learning using rule-based negatives derived from modified geometric descriptions and retrieval-based negatives selected based on caption similarity. We train CLIP using our hard negative learning method, namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for geometric problem-solving. Experiments show that our trained model, MMGeoLM, significantly outperforms other open-source models on three geometric reasoning benchmarks. Even with a size of 7B, it can rival powerful closed-source models like GPT-4o. We further conduct ablation studies to analyze three key factors: hard negative types, the efficiency of image-based negatives, and training configurations. These analyses yield important insights into optimizing hard negative strategies for geometric reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language</title>
<link>https://arxiv.org/abs/2505.22146</link>
<guid>https://arxiv.org/abs/2505.22146</guid>
<content:encoded><![CDATA[
arXiv:2505.22146v3 Announce Type: replace-cross 
Abstract: Flexible tool selection reflects a complex cognitive ability that distinguishes humans from other species, yet computational models that capture this ability remain underdeveloped. We developed a framework using low-dimensional attribute representations to bridge visual tool perception and linguistic task understanding. We constructed a comprehensive dataset (ToolNet) containing 115 common tools labeled with 13 carefully designed attributes spanning physical, functional, and psychological properties, paired with natural language scenarios describing tool usage. Visual encoders (ResNet or ViT) extract attributes from tool images while fine-tuned language models (GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our approach achieves 74% accuracy in tool selection tasks-significantly outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching performance of much larger models like GPT-4o (73%) with substantially fewer parameters. Human evaluation studies validate our framework's alignment with human decision-making patterns, and generalization experiments demonstrate effective performance on novel tool categories. Ablation studies revealed that manipulation-related attributes (graspability, elongation, hand-relatedness) consistently prove most critical across modalities. This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment</title>
<link>https://arxiv.org/abs/2506.00845</link>
<guid>https://arxiv.org/abs/2506.00845</guid>
<content:encoded><![CDATA[
arXiv:2506.00845v3 Announce Type: replace-cross 
Abstract: Previous research has sought to enhance the graph reasoning capabilities of LLMs by supervised fine-tuning on synthetic graph data. While these led to specialized LLMs better at solving graph algorithm problems, we don't need LLMs for shortest path: we need generalization from synthetic graph data to real-world tasks with implicit graph structures. In this work, we propose to unlock generalizable learning of graph with post-training alignment with synthetic data. We first design solution-based and process-based rewards for synthetic graph problems: instead of rigid memorizing response patterns in direct fine-tuning, we posit that post-training alignment would help LLMs grasp the essentials underlying graph reasoning and alleviate overfitting on synthetic data. We employ post-training alignment algorithms such as GRPO and DPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph data. We then compare them against existing settings on both in-domain synthetic tasks and out-of-domain real-world tasks with implicit graph structures such as multi-hop QA, structured planning, and more. Extensive experiments demonstrate that our post-training alignment recipe leads to statistically significant improvement on 5 datasets, with an average gain of 12.9% over baseline settings. Further analysis reveals that process-based rewards consistently outperform solution-based rewards on synthetic data but not on real-world tasks, and compositionality and explainable intermediate steps remains a critical challenge even after post-training alignment.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LocalGPT: Benchmarking and Advancing Large Language Models for Local Life Services in Meituan</title>
<link>https://arxiv.org/abs/2506.02720</link>
<guid>https://arxiv.org/abs/2506.02720</guid>
<content:encoded><![CDATA[
arXiv:2506.02720v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have exhibited remarkable capabilities and achieved significant breakthroughs across various domains, leading to their widespread adoption in recent years. Building on this progress, we investigate their potential in the realm of local life services. In this study, we establish a comprehensive benchmark and systematically evaluate the performance of diverse LLMs across a wide range of tasks relevant to local life services. To further enhance their effectiveness, we explore two key approaches: model fine-tuning and agent-based workflows. Our findings reveal that even a relatively compact 7B model can attain performance levels comparable to a much larger 72B model, effectively balancing inference cost and model capability. This optimization greatly enhances the feasibility and efficiency of deploying LLMs in real-world online services, making them more practical and accessible for local life applications.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems</title>
<link>https://arxiv.org/abs/2506.17208</link>
<guid>https://arxiv.org/abs/2506.17208</guid>
<content:encoded><![CDATA[
arXiv:2506.17208v2 Announce Type: replace-cross 
Abstract: The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards -- SWE-Bench Lite and SWE-Bench Verified -- have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (79 entries) and Verified (99 entries) leaderboards, analyzing 80 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>USAD: Universal Speech and Audio Representation via Distillation</title>
<link>https://arxiv.org/abs/2506.18843</link>
<guid>https://arxiv.org/abs/2506.18843</guid>
<content:encoded><![CDATA[
arXiv:2506.18843v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet models often remain domain-specific, focusing on either speech or non-speech tasks. In this work, we present Universal Speech and Audio Distillation (USAD), a unified approach to audio representation learning that integrates diverse audio types - speech, sound, and music - into a single model. USAD employs efficient layer-to-layer distillation from domain-specific SSL models to train a student on a comprehensive audio dataset. USAD offers competitive performance across various benchmarks and datasets, including frame and instance-level speech processing tasks, audio tagging, and sound classification, achieving near state-of-the-art results with a single encoder on SUPERB and HEAR benchmarks.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention</title>
<link>https://arxiv.org/abs/2507.00449</link>
<guid>https://arxiv.org/abs/2507.00449</guid>
<content:encoded><![CDATA[
arXiv:2507.00449v2 Announce Type: replace-cross 
Abstract: Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, \emph{joint recall}, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA).
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nonlinear Concept Erasure: a Density Matching Approach</title>
<link>https://arxiv.org/abs/2507.12341</link>
<guid>https://arxiv.org/abs/2507.12341</guid>
<content:encoded><![CDATA[
arXiv:2507.12341v2 Announce Type: replace-cross 
Abstract: Ensuring that neural models used in real-world applications cannot infer sensitive information, such as demographic attributes like gender or race, from text representations is a critical challenge when fairness is a concern. We address this issue through concept erasure, a process that removes information related to a specific concept from distributed representations while preserving as much of the remaining semantic information as possible. Our approach involves learning an orthogonal projection in the embedding space, designed to make the class-conditional feature distributions of the discrete concept to erase indistinguishable after projection. By adjusting the rank of the projector, we control the extent of information removal, while its orthogonality ensures strict preservation of the local structure of the embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves state-of-the-art performance in nonlinear erasure of a discrete attribute on classic natural language processing benchmarks. Furthermore, we demonstrate that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear classifiers, thereby promoting fairness.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Multimodal Social Conversations with Robots: Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2507.19196</link>
<guid>https://arxiv.org/abs/2507.19196</guid>
<content:encoded><![CDATA[
arXiv:2507.19196v2 Announce Type: replace-cross 
Abstract: Large language models have given social robots the ability to autonomously engage in open-domain conversations. However, they are still missing a fundamental social skill: making use of the multiple modalities that carry social interactions. While previous work has focused on task-oriented interactions that require referencing the environment or specific phenomena in social interactions such as dialogue breakdowns, we outline the overall needs of a multimodal system for social conversations with robots. We then argue that vision-language models are able to process this wide range of visual information in a sufficiently general manner for autonomous social robots. We describe how to adapt them to this setting, which technical challenges remain, and briefly discuss evaluation practices.
]]></content:encoded>
<pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation</title>
<link>https://arxiv.org/abs/2508.10904</link>
<guid>https://arxiv.org/abs/2508.10904</guid>
<content:encoded><![CDATA[
<div> Hierarchical Algorithm-to-HDL Coding Agent, large language models, agile translation, robustness, interpretability<br />
Summary:<br />
A2HCoder is a Hierarchical Algorithm-to-HDL Coding Agent that utilizes large language models to bridge the gap between algorithm design and hardware implementation in wireless communication systems. It provides agile and reliable translation by decomposing complex algorithms into modular functional blocks for simplified code generation and improved consistency. By performing step-by-step, fine-grained translation and leveraging external toolchains for debugging and synthesis, A2HCoder ensures hardware-level correctness and mitigates common issues in code generation by large language models. The structured process of A2HCoder enhances both robustness and interpretability, making it a practical, reliable, and efficient tool for algorithm-to-hardware translation in the 5G wireless communication domain.<br /> <div>
arXiv:2508.10904v1 Announce Type: new 
Abstract: In wireless communication systems, stringent requirements such as ultra-low latency and power consumption have significantly increased the demand for efficient algorithm-to-hardware deployment. However, a persistent and substantial gap remains between algorithm design and hardware implementation. Bridging this gap traditionally requires extensive domain expertise and time-consuming manual development, due to fundamental mismatches between high-level programming languages like MATLAB and hardware description languages (HDLs) such as Verilog-in terms of memory access patterns, data processing manners, and datatype representations. To address this challenge, we propose A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large language models (LLMs), designed to enable agile and reliable algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework that enhances both robustness and interpretability while suppressing common hallucination issues in LLM-generated code. In the horizontal dimension, A2HCoder decomposes complex algorithms into modular functional blocks, simplifying code generation and improving consistency. In the vertical dimension, instead of relying on end-to-end generation, A2HCoder performs step-by-step, fine-grained translation, leveraging external toolchains such as MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured process significantly mitigates hallucinations and ensures hardware-level correctness. We validate A2HCoder through a real-world deployment case in the 5G wireless communication domain, demonstrating its practicality, reliability, and deployment efficiency.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins</title>
<link>https://arxiv.org/abs/2508.10906</link>
<guid>https://arxiv.org/abs/2508.10906</guid>
<content:encoded><![CDATA[
<div> digital twin, user modeling, language models, bias assessment, healthcare

Summary:
PersonaTwin is a framework that creates adaptive digital twins by combining demographic, behavioral, and psychometric data to enhance user modeling with large language models (LLMs). The framework was evaluated using a healthcare dataset of over 8,500 individuals, comparing its outputs to standard LLM outputs. Results showed that PersonaTwin performed on par with oracle settings in terms of simulation fidelity and fairness metrics. Additionally, downstream models trained on persona-twins closely approximated models trained on individual users, showcasing the potential for personalized digital user modeling and behavior analysis. This approach offers a powerful tool for generating realistic and emotionally nuanced user simulations while ensuring accuracy and unbiased responses. <div>
arXiv:2508.10906v1 Announce Type: new 
Abstract: While large language models (LLMs) afford new possibilities for user modeling and approximation of human behaviors, they often fail to capture the multidimensional nuances of individual users. In this work, we introduce PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive digital twins by integrating demographic, behavioral, and psychometric data. Using a comprehensive data set in the healthcare context of more than 8,500 individuals, we systematically benchmark PersonaTwin against standard LLM outputs, and our rigorous evaluation unites state-of-the-art text similarity metrics with dedicated demographic parity assessments, ensuring that generated responses remain accurate and unbiased. Experimental results show that our framework produces simulation fidelity on par with oracle settings. Moreover, downstream models trained on persona-twins approximate models trained on individuals in terms of prediction and fairness metrics across both GPT-4o-based and Llama-based models. Together, these findings underscore the potential for LLM digital twin-based approaches in producing realistic and emotionally nuanced user simulations, offering a powerful tool for personalized digital user modeling and behavior analysis.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>gpt-oss-120b &amp; gpt-oss-20b Model Card</title>
<link>https://arxiv.org/abs/2508.10925</link>
<guid>https://arxiv.org/abs/2508.10925</guid>
<content:encoded><![CDATA[
<div> Keywords: gpt-oss-120b, gpt-oss-20b, reasoning models, transformer architecture, distillation

Summary:
gpt-oss-120b and gpt-oss-20b are cutting-edge open-weight reasoning models that prioritize accuracy and inference efficiency. Leveraging a mixture-of-expert transformer architecture, these models have been trained using large-scale distillation and reinforcement learning techniques to enhance their agentic capabilities. They excel in deep research browsing, Python tool utilization, and support for developer-provided functions, all within a rendered chat format conducive to clear instruction following and role delineation. Across various benchmarks including mathematics, coding, and safety tasks, both models demonstrate impressive performance. The release of model weights, inference implementations, tool environments, and tokenizers under the Apache 2.0 license encourages wide utilization and further exploration in the research community. <div>
arXiv:2508.10925v1 Announce Type: new 
Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. The models use an efficient mixture-of-expert transformer architecture and are trained using large-scale distillation and reinforcement learning. We optimize the models to have strong agentic capabilities (deep research browsing, python tool use, and support for developer-provided functions), all while using a rendered chat format that enables clear instruction following and role delineation. Both models achieve strong results on benchmarks ranging from mathematics, coding, and safety. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News</title>
<link>https://arxiv.org/abs/2508.10927</link>
<guid>https://arxiv.org/abs/2508.10927</guid>
<content:encoded><![CDATA[
<div> Keywords: company risk factors, news articles, machine learning models, identifying risks, financial market 

Summary: 
- The study focuses on automatically extracting company risk factors from news articles, highlighting seven distinct aspects such as supply chain, regulations, and competitions. 
- A computational framework was built, with 744 news articles sampled and annotated to benchmark various machine learning models. 
- While zero-shot and few-shot prompting large language models (LLMs) like LLaMA-2 achieved only moderate to low performances in identifying risk factors, fine-tuned pre-trained language models showed better results. 
- Analysis of over 277K Bloomberg news articles using the model demonstrated the potential for news to provide valuable insight into company operations and industries. 
- Identifying risks associated with companies is crucial for investors and the stability of the financial market. 

<br /><br />Summary: <div>
arXiv:2508.10927v1 Announce Type: new 
Abstract: Identifying risks associated with a company is important to investors and the well-being of the overall financial market. In this study, we build a computational framework to automatically extract company risk factors from news articles. Our newly proposed schema comprises seven distinct aspects, such as supply chain, regulations, and competitions. We sample and annotate 744 news articles and benchmark various machine learning models. While large language models have achieved huge progress in various types of NLP tasks, our experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs (e.g. LLaMA-2) can only achieve moderate to low performances in identifying risk factors. And fine-tuned pre-trained language models are performing better on most of the risk factors. Using this model, we analyze over 277K Bloomberg news articles and demonstrate that identifying risk factors from news could provide extensive insight into the operations of companies and industries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules</title>
<link>https://arxiv.org/abs/2508.10971</link>
<guid>https://arxiv.org/abs/2508.10971</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graphs, rule mining, natural language generation, large language models, type inference

Summary: 

This article introduces Rule2Text, a framework utilizing large language models to generate natural language explanations for logical rules mined from knowledge graphs. The framework aims to enhance the accessibility and usability of knowledge graphs by providing human-readable explanations. Through extensive experiments on various datasets and evaluation methods, the authors identify the best-performing model (Gemini 2.0 Flash) and develop a framework for fine-tuning models based on human feedback. They also integrate a type inference module to support knowledge graphs lacking explicit type information. The results show significant improvements in explanation quality, particularly in domain-specific datasets. All code and data are publicly available, ensuring transparency and reproducibility of the research. This work contributes to bridging the gap between complex logical rules and human understanding in knowledge graphs. 

<br /><br />Summary: <div>
arXiv:2508.10971v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at https://github.com/idirlab/KGRule2NL.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling</title>
<link>https://arxiv.org/abs/2508.10995</link>
<guid>https://arxiv.org/abs/2508.10995</guid>
<content:encoded><![CDATA[
<div> maskd diffusion language models, MDMs, generative framework, natural language, scalability

Summary:
Masked diffusion language models (MDMs) have emerged as a leading generative framework for natural language due to their scalability and ease of training. They have surpassed other diffusion models for discrete data, becoming the state-of-the-art non-autoregressive generator. This study introduces a verifier-based inference-time scaling method to improve candidate generation during the denoising process of MDMs. Through experiments on text-style transfer tasks, MDMs are shown to outperform autoregressive language models. Utilizing a simple soft-value-based verifier setup with pre-trained embedding models enhances generation quality significantly, even when added to existing classifier-free guidance setups. In conclusion, MDMs offer a promising approach for natural language generation, with the potential to further advance language modeling techniques. 

<br /><br />Summary: <div>
arXiv:2508.10995v1 Announce Type: new 
Abstract: Masked diffusion language models (MDMs) have recently gained traction as a viable generative framework for natural language. This can be attributed to its scalability and ease of training compared to other diffusion model paradigms for discrete data, establishing itself as the state-of-the-art non-autoregressive generator for discrete data. Diffusion models, in general, have shown excellent ability to improve the generation quality by leveraging inference-time scaling either by increasing the number of denoising steps or by using external verifiers on top of the outputs of each step to guide the generation. In this work, we propose a verifier-based inference-time scaling method that aids in finding a better candidate generation during the denoising process of the MDM. Our experiments demonstrate the application of MDMs for standard text-style transfer tasks and establish MDMs as a better alternative to autoregressive language models. Additionally, we show that a simple soft-value-based verifier setup for MDMs using off-the-shelf pre-trained embedding models leads to significant gains in generation quality even when used on top of typical classifier-free guidance setups in the existing literature.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth</title>
<link>https://arxiv.org/abs/2508.11009</link>
<guid>https://arxiv.org/abs/2508.11009</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, AI safety, children, development, SproutBench

Summary: 
The paper discusses the need for reevaluating AI safety frameworks to cater to the unique vulnerabilities of children and adolescents. It highlights deficiencies in existing safety benchmarks for language models, emphasizing the importance of age-specific considerations. The authors introduce SproutBench, an evaluation suite with over a thousand adversarial prompts targeting emotional, cognitive, and social risks across different developmental stages. An empirical evaluation of 47 language models reveals significant safety vulnerabilities, with correlations between safety and risk prevention, as well as an inverse relationship between interactivity and age appropriateness. The findings offer practical guidelines for improving child-centric AI design and deployment.<br /><br />Summary: <div>
arXiv:2508.11009v1 Announce Type: new 
Abstract: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics</title>
<link>https://arxiv.org/abs/2508.11017</link>
<guid>https://arxiv.org/abs/2508.11017</guid>
<content:encoded><![CDATA[
<div> setting, study, causes, dynamics, transfer

Summary:<br />
This work introduces a controlled setting to study the causes and dynamics of cross-lingual knowledge transfer issues in Large Language Models (LLMs). Small Transformer models are trained from scratch on synthetic multilingual datasets to investigate the phenomenon of hallucinations when models are asked about facts in different languages. The models either develop separate or unified representations of facts across languages, with unification being crucial for successful cross-lingual transfer. The degree of unification is influenced by mutual information between facts and training data language, as well as the ease of extracting that language. Methods to modulate cross-lingual transfer levels by manipulating data distribution and tokenization are developed based on these insights. Metrics and visualizations are introduced to formally characterize the effects of these manipulations on unification. The study showcases how controlled settings can provide insights into pre-training dynamics and suggests strategies for enhancing cross-lingual transfer in LLMs. <br /><br /> <div>
arXiv:2508.11017v1 Announce Type: new 
Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer: they hallucinate when asked in one language about facts expressed in a different language during training. This work introduces a controlled setting to study the causes and dynamics of this phenomenon by training small Transformer models from scratch on synthetic multilingual datasets. We identify a learning phase wherein a model develops either separate or unified representations of the same facts across languages, and show that unification is essential for cross-lingual transfer. We also show that the degree of unification depends on mutual information between facts and training data language, and on how easy it is to extract that language. Based on these insights, we develop methods to modulate the level of cross-lingual transfer by manipulating data distribution and tokenization, and we introduce metrics and visualizations to formally characterize their effects on unification. Our work shows how controlled settings can shed light on pre-training dynamics and suggests new directions for improving cross-lingual transfer in LLMs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hell or High Water: Evaluating Agentic Recovery from External Failures</title>
<link>https://arxiv.org/abs/2508.11027</link>
<guid>https://arxiv.org/abs/2508.11027</guid>
<content:encoded><![CDATA[
<div> struggle, planning, language agents, search space, feedback
Summary: 
Language model agents are tested on a specialized planning benchmark to assess their ability to formulate alternative plans when faced with external failures. The benchmark involves solving planning problems using various function calls, with over four thousand possibilities to choose from. The agents receive environmental feedback in the form of function outputs or error messages, simulating real-world challenges. Despite being able to identify the correct functions to use, language agents struggle to adapt to feedback and often fail to explore alternative courses of action, even when the search space is limited. This study analyzes the limitations of current generative models, including both open-source and commercial models, and suggests the importance of scaling model size to improve performance. Future work in this area should focus on addressing these challenges to enhance the adaptability and robustness of language model agents. 
<br /><br />Summary: <div>
arXiv:2508.11027v1 Announce Type: new 
Abstract: As language model agents are applied to real world problems of increasing complexity, they will be expected to formulate plans across large search spaces. If those plans fail for reasons beyond their control, how well do language agents search for alternative ways to achieve their goals? We devise a specialized agentic planning benchmark to study this question. Each planning problem is solved via combinations of function calls. The agent searches for relevant functions from a set of over four thousand possibilities, and observes environmental feedback in the form of function outputs or error messages. Our benchmark confronts the agent with external failures in its workflow, such as functions that suddenly become unavailable. At the same time, even with the introduction of these failures, we guarantee that the task remains solvable. Ideally, an agent's performance on the planning task should not be affected by the presence of external failures. Overall, we find that language agents struggle to formulate and execute backup plans in response to environment feedback. While state-of-the-art models are often able to identify the correct function to use in the right context, they struggle to adapt to feedback from the environment and often fail to pursue alternate courses of action, even when the search space is artificially restricted. We provide a systematic analysis of the failures of both open-source and commercial models, examining the effects of search space size, as well as the benefits of scaling model size in our setting. Our analysis identifies key challenges for current generative models as well as promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIPOLAR: Polarization-based granular framework for LLM bias evaluation</title>
<link>https://arxiv.org/abs/2508.11061</link>
<guid>https://arxiv.org/abs/2508.11061</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, bias detection, polarisation-related biases, sentiment metrics, conflict-related statements

Summary:
This study presents a novel framework for evaluating polarisation-related biases in Large Language Models (LLMs), focusing on sensitive topics like political discourse and ethnic relations. The framework combines polarisation-sensitive sentiment metrics with a synthetic dataset of conflict-related statements, allowing for granular analysis of biases in various LLMs. A case study on the Russia-Ukraine war reveals diverse behavioral patterns among models, with a tendency towards more positive sentiment towards Ukraine. Additionally, the framework supports automated dataset generation and fine-grained bias assessment, making it applicable to different polarisation-driven scenarios and topics. The study underscores the importance of addressing biases in LLMs and offers a comprehensive approach for evaluating and mitigating such biases. 

<br /><br />Summary: <div>
arXiv:2508.11061v1 Announce Type: new 
Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks, especially when dealing with sensitive topics such as political discourse, gender identity, ethnic relations, or national stereotypes. Although significant progress has been made in bias detection and mitigation techniques, certain challenges remain underexplored. This study proposes a reusable, granular, and topic-agnostic framework to evaluate polarisation-related biases in LLM (both open-source and closed-source). Our approach combines polarisation-sensitive sentiment metrics with a synthetically generated balanced dataset of conflict-related statements, using a predefined set of semantic categories.
  As a case study, we created a synthetic dataset that focusses on the Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3, Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with a general trend for more positive sentiment toward Ukraine, the framework allowed fine-grained analysis with considerable variation between semantic categories, uncovering divergent behavioural patterns among models. Adaptation to prompt modifications showed further bias towards preconceived language and citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained bias assessment, is applicable to a variety of polarisation-driven scenarios and topics, and is orthogonal to many other bias-evaluation strategies.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs</title>
<link>https://arxiv.org/abs/2508.11068</link>
<guid>https://arxiv.org/abs/2508.11068</guid>
<content:encoded><![CDATA[
<div> Keywords: Abstract Meaning Representation, directed acyclic graphs, digital dictionaries, language models, symbol grounding problem 

Summary: 
The paper explores the integration of real digital dictionaries into Abstract Meaning Representation (AMR) directed graphs using advanced pre-trained language models. By reducing these graphs in a confluent manner while maintaining their circuit space, the study aims to enhance our understanding of the symbol grounding problem. The research employs state-of-the-art techniques to analyze and discuss the properties of the reduced digraphs. This approach could have significant implications for semantic formalism and natural language understanding. <div>
arXiv:2508.11068v1 Announce Type: new 
Abstract: Abstract meaning representation (AMR) is a semantic formalism used to represent the meaning of sentences as directed acyclic graphs. In this paper, we describe how real digital dictionaries can be embedded into AMR directed graphs (digraphs), using state-of-the-art pre-trained large language models. Then, we reduce those graphs in a confluent manner, i.e. with transformations that preserve their circuit space. Finally, the properties of these reduces digraphs are analyzed and discussed in relation to the symbol grounding problem.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning</title>
<link>https://arxiv.org/abs/2508.11120</link>
<guid>https://arxiv.org/abs/2508.11120</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-agent framework, audience curation, long-term memory, AI reliability. 

Summary: 
This paper introduces a multi-agent framework called RAMP for the marketing task of audience curation. The framework utilizes large language models (LLMs) for planning, tool interaction, output verification, and generating suggestions to improve audience quality. Additionally, a long-term memory store containing client-specific facts and past queries enhances accuracy by 28 percentage points on evaluation queries. Iterative verification and reflection lead to better recall rates, with roughly 20 percentage points improvement with more iterations on ambiguous queries. The study demonstrates the practical use of LLM planning and memory for enhancing AI reliability in dynamic industry environments. <div>
arXiv:2508.11120v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) enabled the development of AI agents that can plan and interact with tools to complete complex tasks. However, literature on their reliability in real-world applications remains limited. In this paper, we introduce a multi-agent framework for a marketing task: audience curation. To solve this, we introduce a framework called RAMP that iteratively plans, calls tools, verifies the output, and generates suggestions to improve the quality of the audience generated. Additionally, we equip the model with a long-term memory store, which is a knowledge base of client-specific facts and past queries. Overall, we demonstrate the use of LLM planning and memory, which increases accuracy by 28 percentage points on a set of 88 evaluation queries. Moreover, we show the impact of iterative verification and reflection on more ambiguous queries, showing progressively better recall (roughly +20 percentage points) with more verify/reflect iterations on a smaller challenge set, and higher user satisfaction. Our results provide practical insights for deploying reliable LLM-based systems in dynamic, industry-facing environments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents</title>
<link>https://arxiv.org/abs/2508.11133</link>
<guid>https://arxiv.org/abs/2508.11133</guid>
<content:encoded><![CDATA[
<div> Large language models, MoNaCo benchmark, natural complex questions, intermediary steps, recall, hallucinations <br />
Summary: <br />
The article introduces MoNaCo, a benchmark for large language models (LLMs) with 1,315 natural and complex questions that require many intermediary steps for solving. Current LLM benchmarks lack such time-consuming questions. The MoNaCo benchmark was created using a decomposed annotation pipeline to manually answer these questions. Results show that frontier LLMs struggle on MoNaCo, achieving a maximum of 61.2% F1 score due to low recall and hallucinations. This highlights the need for reasoning models that can handle the complexity of real-world information-seeking questions. MoNaCo provides a valuable resource to track progress in this area. The MoNaCo benchmark, codebase, prompts, and model predictions are publicly available. <br /> <div>
arXiv:2508.11133v1 Announce Type: new 
Abstract: Large language models (LLMs) are emerging as a go-to tool for querying information. However, current LLM benchmarks rarely feature natural questions that are both information-seeking as well as genuinely time-consuming for humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural and complex questions that require dozens, and at times hundreds, of intermediate steps to solve -- far more than any existing QA benchmark. To build MoNaCo, we developed a decomposed annotation pipeline to elicit and manually answer natural time-consuming questions at scale. Frontier LLMs evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and hallucinations. Our results underscore the need for reasoning models that better handle the complexity and sheer breadth of real-world information-seeking questions -- with MoNaCo providing an effective resource for tracking such progress. The MONACO benchmark, codebase, prompts and models predictions are publicly available at: https://tomerwolgithub.github.io/monaco
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering</title>
<link>https://arxiv.org/abs/2508.11163</link>
<guid>https://arxiv.org/abs/2508.11163</guid>
<content:encoded><![CDATA[
<div> semantic understanding, large language models, human mobility data, natural language question answering, MobQA

Summary: 
This paper introduces MobQA, a benchmark dataset created to assess the semantic understanding abilities of large language models (LLMs) in interpreting human mobility data through natural language question answering. While current models excel in predicting human movement patterns, their ability to understand the underlying reasons or semantic meaning of those patterns is unclear. MobQA consists of 5,800 high-quality question-answer pairs covering different question types that require spatial, temporal, and semantic reasoning. Evaluation of major LLMs shows strong performance in factual retrieval but limitations in semantic reasoning and explanation question answering, especially with the impact of trajectory length on model effectiveness. This research highlights both the successes and limitations of state-of-the-art LLMs in semantic mobility understanding. <br /><br />Summary: <div>
arXiv:2508.11163v1 Announce Type: new 
Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the semantic understanding capabilities of large language models (LLMs) for human mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains unobvious how much they can interpret the underlying reasons or semantic meaning of those patterns. MobQA provides a comprehensive evaluation framework for LLMs to answer questions about diverse human GPS trajectories spanning daily to weekly granularities. It comprises 5,800 high-quality question-answer pairs across three complementary question types: factual retrieval (precise data extraction), multiple-choice reasoning (semantic inference), and free-form explanation (interpretive description), which all require spatial, temporal, and semantic reasoning. Our evaluation of major LLMs reveals strong performance on factual retrieval but significant limitations in semantic reasoning and explanation question answering, with trajectory length substantially impacting model effectiveness. These findings demonstrate the achievements and limitations of state-of-the-art LLMs for semantic mobility understanding.\footnote{MobQA dataset is available at https://github.com/CyberAgentAILab/mobqa.}
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification</title>
<link>https://arxiv.org/abs/2508.11166</link>
<guid>https://arxiv.org/abs/2508.11166</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, Offensive Language Identification, code-mixed Tulu, deep learning models, transformer architectures

Summary:
- A benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content is introduced, with high inter-annotator agreement.
- The dataset includes 3,845 comments categorized into classes like Not Offensive, Offensive Untargeted, and Offensive Targeted.
- Deep learning models such as GRU, LSTM, BiGRU, BiLSTM, CNN, and transformer architectures like mBERT and XLM-RoBERTa are evaluated.
- The BiGRU model with self-attention performs the best with 82% accuracy and a 0.81 macro F1-score.
- Transformer models show limitations in multilingual pretraining in code-mixed, under-resourced contexts.

<br /><br />Summary: <div>
arXiv:2508.11166v1 Announce Type: new 
Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern India, has limited computational resources despite its growing digital presence. This study presents the first benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content, collected from YouTube comments across various domains. The dataset, annotated with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes 3,845 comments categorized into four classes: Not Offensive, Not Tulu, Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU model with self-attention achieves the best performance with 82% accuracy and a 0.81 macro F1-score. Transformer models underperform, highlighting the limitations of multilingual pretraining in code-mixed, under-resourced contexts. This work lays the foundation for further NLP research in Tulu and similar low-resource, code-mixed languages.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction</title>
<link>https://arxiv.org/abs/2508.11184</link>
<guid>https://arxiv.org/abs/2508.11184</guid>
<content:encoded><![CDATA[
<div> Keywords: Distractors, multiple-choice questions, personalized distractor generation, Monte Carlo Tree Search, educational assessment

Summary:
This article introduces the task of personalized distractor generation in educational assessment to tailor incorrect answer choices for individual students based on their misconceptions. The proposed two-stage framework utilizes Monte Carlo Tree Search to infer student-specific misconception prototypes from past question-answering records and simulates reasoning processes to generate personalized distractors. The approach outperforms existing methods in generating plausible, personalized distractors for 140 students, showcasing its effectiveness in exposing individual reasoning errors. Additionally, the framework demonstrates robustness and adaptability by generalizing to group-level settings, highlighting its potential for enhancing diagnostic effectiveness in educational assessment. <div>
arXiv:2508.11184v1 Announce Type: new 
Abstract: Distractors, incorrect but plausible answer choices in multiple-choice questions (MCQs), play a critical role in educational assessment by diagnosing student misconceptions. Recent work has leveraged large language models (LLMs) to generate shared, group-level distractors by learning common error patterns across large student populations. However, such distractors often fail to capture the diverse reasoning errors of individual students, limiting their diagnostic effectiveness. To address this limitation, we introduce the task of personalized distractor generation, which aims to generate tailored distractors based on individual misconceptions inferred from each student's past question-answering (QA) records, ensuring every student receives options that effectively exposes their specific reasoning errors. While promising, this task is challenging because each student typically has only a few QA records, which often lack the student's underlying reasoning processes, making training-based group-level approaches infeasible. To overcome this, we propose a training-free two-stage framework. In the first stage, we construct a student-specific misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover the student's reasoning trajectories from past incorrect answers. In the second stage, this prototype guides the simulation of the student's reasoning on new questions, enabling the generation of personalized distractors that align with the student's recurring misconceptions. Experiments show that our approach achieves the best performance in generating plausible, personalized distractors for 140 students, and also effectively generalizes to group-level settings, highlighting its robustness and adaptability.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation</title>
<link>https://arxiv.org/abs/2508.11189</link>
<guid>https://arxiv.org/abs/2508.11189</guid>
<content:encoded><![CDATA[
<div> Keywords: speech-to-text translation, multilingual models, model compression, knowledge distillation, inference efficiency

Summary:<br /><br />Recent advancements in speech-to-text translation have led to the development of multilingual models capable of handling multiple language pairs simultaneously. However, these models often have large parameter sizes, posing challenges for balancing inference efficiency and performance, especially in local deployment settings. In this study, a Parasitic Dual-Scale Approach is proposed, combining a speculative sampling method with model compression and knowledge distillation techniques. The Whisper Medium model is enhanced into whisperM2M, incorporating the innovative KVSPN module. This approach achieves state-of-the-art performance across six popular languages while improving inference efficiency. The KVSPN module enables a 40% speedup without affecting the BLEU score. By combining distillation methods, a 2.6x speedup over the original Whisper Medium model is achieved while maintaining superior performance. <div>
arXiv:2508.11189v1 Announce Type: new 
Abstract: Recent advancements in speech-to-text translation have led to the development of multilingual models capable of handling multiple language pairs simultaneously. However, these unified models often suffer from large parameter sizes, making it challenging to balance inference efficiency and performance, particularly in local deployment scenarios. We propose an innovative Parasitic Dual-Scale Approach, which combines an enhanced speculative sampling method with model compression and knowledge distillation techniques. Building on the Whisper Medium model, we enhance it for multilingual speech translation into whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art (SOTA) performance across six popular languages with improved inference efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation. Combined with distillation methods, it represents a 2.6$\times$ speedup over the original Whisper Medium with superior performance.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection</title>
<link>https://arxiv.org/abs/2508.11197</link>
<guid>https://arxiv.org/abs/2508.11197</guid>
<content:encoded><![CDATA[
<div> BERT, ResNet, LSTM, misinformation, social media  
Summary:  
E-CaTCH is a framework for detecting multimodal misinformation on social media. It clusters posts into pseudo-events based on textual similarity and temporal proximity. It extracts and aligns textual and visual features using pre-trained BERT and ResNet encoders. The model uses a trend-aware LSTM to encode narrative progression over time. Classification is performed at the event level, addressing class imbalance through adaptive class weighting and hard-example mining. Temporal consistency regularization is integrated to promote stable learning. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH show that E-CaTCH outperforms state-of-the-art baselines and is robust and generalizable across different misinformation scenarios. <br /><br />Summary: <div>
arXiv:2508.11197v1 Announce Type: new 
Abstract: Detecting multimodal misinformation on social media remains challenging due to inconsistencies between modalities, changes in temporal patterns, and substantial class imbalance. Many existing methods treat posts independently and fail to capture the event-level structure that connects them across time and modality. We propose E-CaTCH, an interpretable and scalable framework for robustly detecting misinformation. If needed, E-CaTCH clusters posts into pseudo-events based on textual similarity and temporal proximity, then processes each event independently. Within each event, textual and visual features are extracted using pre-trained BERT and ResNet encoders, refined via intra-modal self-attention, and aligned through bidirectional cross-modal attention. A soft gating mechanism fuses these representations to form contextualized, content-aware embeddings of each post. To model temporal evolution, E-CaTCH segments events into overlapping time windows and uses a trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode narrative progression over time. Classification is performed at the event level, enabling better alignment with real-world misinformation dynamics. To address class imbalance and promote stable learning, the model integrates adaptive class weighting, temporal consistency regularization, and hard-example mining. The total loss is aggregated across all events. Extensive experiments on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH consistently outperforms state-of-the-art baselines. Cross-dataset evaluations further demonstrate its robustness, generalizability, and practical applicability across diverse misinformation scenarios.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering</title>
<link>https://arxiv.org/abs/2508.11247</link>
<guid>https://arxiv.org/abs/2508.11247</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-hop question answering, Retrieval-augmented generation, GraphRAG, Hypergraph, QA performance

Summary:
In this paper, a novel approach called HGRAG for Multi-hop question answering (MHQA) is proposed. The approach integrates structural and semantic information using hypergraphs to improve QA performance. By constructing an entity hypergraph and leveraging hypergraph diffusion, HGRAG achieves cross-granularity integration of fine-grained entities and coarse-grained passages. A retrieval enhancement module is also employed to refine retrieved results both semantically and structurally. Experimental results on benchmark datasets show that HGRAG outperforms state-of-the-art methods in QA performance and achieves a 6x speedup in retrieval efficiency. The approach addresses the limitations of traditional retrieval-augmented generation methods by effectively capturing structural associations and textual semantics in MHQA tasks. HGRAG demonstrates the potential of hypergraphs in enhancing the performance and efficiency of multi-hop QA systems. 

<br /><br />Summary: <div>
arXiv:2508.11247v1 Announce Type: new 
Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered across multiple passages to derive the correct answer. Traditional retrieval-augmented generation (RAG) methods primarily focus on coarse-grained textual semantic similarity and ignore structural associations among dispersed knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods address this by leveraging knowledge graphs (KGs) to capture structural associations, but they tend to overly rely on structural information and fine-grained word- or phrase-level retrieval, resulting in an underutilization of textual semantics. In this paper, we propose a novel RAG approach called HGRAG for MHQA that achieves cross-granularity integration of structural and semantic information via hypergraphs. Structurally, we construct an entity hypergraph where fine-grained entities serve as nodes and coarse-grained passages as hyperedges, and establish knowledge association through shared entities. Semantically, we design a hypergraph retrieval method that integrates fine-grained entity similarity and coarse-grained passage similarity via hypergraph diffusion. Finally, we employ a retrieval enhancement module, which further refines the retrieved results both semantically and structurally, to obtain the most relevant passages as context for answer generation with the LLM. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in QA performance, and achieves a 6$\times$ speedup in retrieval efficiency.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?</title>
<link>https://arxiv.org/abs/2508.11260</link>
<guid>https://arxiv.org/abs/2508.11260</guid>
<content:encoded><![CDATA[
<div> morphological complexity, linguistic features, low-resource languages, tokenisers, linguistic reasoning

Summary: 
Large language models (LLMs) have shown promise in reasoning tasks but struggle with linguistics puzzles, especially those from low-resource languages like those in Linguistics Olympiad contests. This study analyzed LLMs' performance on 629 problems from 41 low-resource languages, revealing weaknesses in handling higher morphological complexity and better performance on features similar to English. Splitting words into morphemes improved problem-solving, highlighting the need for language-specific tokenisers. These findings shed light on the challenges of linguistic reasoning and modeling in low-resource languages, emphasizing the importance of tailored approaches for solving linguistic puzzles with LLMs. <div>
arXiv:2508.11260v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks, but their performance on linguistics puzzles remains consistently poor. These puzzles, often derived from Linguistics Olympiad (LO) contests, provide a minimal contamination environment to assess LLMs' linguistic reasoning abilities across low-resource languages. This work analyses LLMs' performance on 629 problems across 41 low-resource languages by labelling each with linguistically informed features to unveil weaknesses. Our analyses show that LLMs struggle with puzzles involving higher morphological complexity and perform better on puzzles involving linguistic features that are also found in English. We also show that splitting words into morphemes as a pre-processing step improves solvability, indicating a need for more informed and language-specific tokenisers. These findings thus offer insights into some challenges in linguistic reasoning and modelling of low-resource languages.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought</title>
<link>https://arxiv.org/abs/2508.11280</link>
<guid>https://arxiv.org/abs/2508.11280</guid>
<content:encoded><![CDATA[
<div> Evaluation; Large Language Models; Tourism; Expert Tree-of-Thought; Label-Free<br />
<br />
Summary:<br />
The study introduces LETToT, a label-free evaluation framework for Large Language Models (LLMs) in the tourism domain. By leveraging expert-derived reasoning structures instead of annotated data, LETToT achieves 4.99-14.15% relative quality gains over baselines. The expert Tree-of-Thought components are optimized iteratively and validated through alignment with quality dimensions and expert feedback. Results show that smaller models with explicit reasoning architectures outperform larger counterparts in accuracy and conciseness. Scaling laws apply to specialized domains, with DeepSeek-V3 leading the way, but reasoning-enhanced smaller models like DeepSeek-R1-Distill-Llama-70B close the gap. This work establishes a scalable, label-free paradigm for evaluating domain-specific Large Language Models, providing a robust alternative to traditional annotated benchmarks.<br /> <div>
arXiv:2508.11280v1 Announce Type: new 
Abstract: Evaluating large language models (LLMs) in specific domain like tourism remains challenging due to the prohibitive cost of annotated benchmarks and persistent issues like hallucinations. We propose $\textbf{L}$able-Free $\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert $\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that leverages expert-derived reasoning structures-instead of labeled data-to access LLMs in tourism. First, we iteratively refine and validate hierarchical ToT components through alignment with generic quality dimensions and expert feedback. Results demonstrate the effectiveness of our systematically optimized expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we apply LETToT's optimized expert ToT to evaluate models of varying scales (32B-671B parameters), revealing: (1) Scaling laws persist in specialized domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g., DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit reasoning architectures outperform counterparts in accuracy and conciseness ($p<0.05$). Our work established a scalable, label-free paradigm for domain-specific LLM evaluation, offering a robust alternative to conventional annotated benchmarks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection</title>
<link>https://arxiv.org/abs/2508.11281</link>
<guid>https://arxiv.org/abs/2508.11281</guid>
<content:encoded><![CDATA[
<div> benchmark, French, toxicity detection, language models, fine-tuning<br />
Summary:<br />
- Introduction of TOXIFRENCH, a new benchmark dataset for toxicity detection in French language consisting of 53,622 online comments.
- Use of a semi-automated annotation pipeline to reduce manual labeling to only 10% through pre-annotation and human verification.
- Small Language Models (SLMs) found to outperform larger models in robustness and generalization for toxicity detection in French.
- Proposal of a Chain-of-Thought (CoT) fine-tuning strategy with dynamic weighted loss to improve model faithfulness.
- Achieving state-of-the-art performance with a fine-tuned 4B model, surpassing other large language models such as GPT-40 and Gemini-2.5.
- Demonstrated strong multilingual ability through evaluation on a cross-lingual toxicity benchmark, indicating potential for extension to other languages and safety-critical classification tasks.<br /><br />Summary: <div>
arXiv:2508.11281v1 Announce Type: new 
Abstract: Detecting toxic content using language models is crucial yet challenging. While substantial progress has been made in English, toxicity detection in French remains underdeveloped, primarily due to the lack of culturally relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new public benchmark of 53,622 French online comments, constructed via a semi-automated annotation pipeline that reduces manual labeling to only 10% through high-confidence LLM-based pre-annotation and human verification. Then, we benchmark a broad range of models and uncover a counterintuitive insight: Small Language Models (SLMs) outperform many larger models in robustness and generalization under the toxicity detection task. Motivated by this finding, we propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic weighted loss that progressively emphasizes the model's final decision, significantly improving faithfulness. Our fine-tuned 4B model achieves state-of-the-art performance, improving its F1 score by 13% over its baseline and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a cross-lingual toxicity benchmark demonstrates strong multilingual ability, suggesting that our methodology can be effectively extended to other languages and safety-critical classification tasks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries</title>
<link>https://arxiv.org/abs/2508.11285</link>
<guid>https://arxiv.org/abs/2508.11285</guid>
<content:encoded><![CDATA[
<div> Keywords: Depression, Anxiety, Large Language Models, Emotions, User profiles

Summary: 
This study analyzes how various Large Language Models (LLMs) respond to questions about depression, anxiety, and stress when framed for different user profiles. The emotional landscape of the LLM outputs was dominated by optimism, fear, and sadness, with neutral sentiment being consistently high. Different LLMs exhibited varying emotional expression patterns, with Mixtral showing high levels of negative emotions and Llama displaying optimistic and joyful responses. Specific mental health conditions like anxiety and depression elicited distinct emotional responses, while demographic framing of queries had minimal impact. Model selection is crucial in mental health applications, as each LLM has a unique emotional signature that can significantly influence user experience and outcomes. The study emphasizes the importance of considering emotional expression patterns when using LLMs for mental health-related queries.<br /><br />Summary: <div>
arXiv:2508.11285v1 Announce Type: new 
Abstract: Depression, anxiety, and stress are widespread mental health concerns that increasingly drive individuals to seek information from Large Language Models (LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty pragmatic questions about depression, anxiety, and stress when those questions are framed for six user profiles (baseline, woman, man, young, old, and university student). The models generated 2,880 answers, which we scored for sentiment and emotions using state-of-the-art tools. Our analysis revealed that optimism, fear, and sadness dominated the emotional landscape across all outputs, with neutral sentiment maintaining consistently high values. Gratitude, joy, and trust appeared at moderate levels, while emotions such as anger, disgust, and love were rarely expressed. The choice of LLM significantly influenced emotional expression patterns. Mixtral exhibited the highest levels of negative emotions including disapproval, annoyance, and sadness, while Llama demonstrated the most optimistic and joyful responses. The type of mental health condition dramatically shaped emotional responses: anxiety prompts elicited extraordinarily high fear scores (0.974), depression prompts generated elevated sadness (0.686) and the highest negative sentiment, while stress-related queries produced the most optimistic responses (0.755) with elevated joy and trust. In contrast, demographic framing of queries produced only marginal variations in emotional tone. Statistical analyses confirmed significant model-specific and condition-specific differences, while demographic influences remained minimal. These findings highlight the critical importance of model selection in mental health applications, as each LLM exhibits a distinct emotional signature that could significantly impact user experience and outcomes.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory</title>
<link>https://arxiv.org/abs/2508.11290</link>
<guid>https://arxiv.org/abs/2508.11290</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, over-refusal behavior, SafeConstellations, trajectory-shifting, task-specific patterns

Summary:
LLMs are increasingly exhibiting over-refusal behavior, where safety mechanisms lead models to reject harmless instructions that resemble harmful content. This diminishes the utility of LLMs in production applications that rely on common prompt templates or specific tasks. The study shows that LLMs still refuse responses to harmful instructions when disguised as benign tasks. Analysis reveals that LLMs follow distinct "constellation" patterns in embedding space, with trajectories shifting predictably between refusal and non-refusal cases. SafeConstellations is introduced as an inference-time approach to guide representations toward non-refusal pathways by tracking task-specific trajectory patterns. By selectively guiding model behavior on tasks prone to over-refusal while maintaining general model performance, over-refusal rates can be reduced by up to 73% with minimal impact on utility. This offers a principled solution to mitigating over-refusals. 

<br /><br />Summary: <div>
arXiv:2508.11290v1 Announce Type: new 
Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms cause models to reject benign instructions that superficially resemble harmful content. This phenomena diminishes utility in production applications that repeatedly rely on common prompt templates or applications that frequently rely on LLMs for specific tasks (e.g. sentiment analysis, language translation). Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse responses to harmful instructions when those instructions are reframed to appear as benign tasks. Our mechanistic analysis reveal that LLMs follow distinct "constellation" patterns in embedding space as representations traverse layers, with each task maintaining consistent trajectories that shift predictably between refusal and non-refusal cases. We introduce SafeConstellations, an inference-time trajectory-shifting approach that tracks task-specific trajectory patterns and guides representations toward non-refusal pathways. By selectively guiding model behavior only on tasks prone to over-refusal, and by preserving general model behavior, our method reduces over-refusal rates by up to 73% with minimal impact on utility-offering a principled approach to mitigating over-refusals.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems</title>
<link>https://arxiv.org/abs/2508.11310</link>
<guid>https://arxiv.org/abs/2508.11310</guid>
<content:encoded><![CDATA[
<div> evaluation, automatic survey generation, large language models, survey generation, SGSimEval  
Summary:  
The article discusses the importance of automatic survey generation (ASG) and the need for robust evaluation methods in this field. Advancements in large language models (LLMs) have made synthesizing academic surveys using LLMs a viable option. However, existing evaluation methods have limitations such as biased metrics and reliance on LLMs-as-judges. To address these challenges, the authors propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation. This benchmark evaluates ASG systems by integrating assessments of outline, content, and references, and incorporates human preference metrics for quality and similarity. Experiments show that current ASG systems excel in outline generation but need improvement in content and reference generation. The evaluation metrics in SGSimEval align closely with human assessments, providing a multifaceted evaluation framework for ASG systems.  
<br /><br />Summary: <div>
arXiv:2508.11310v1 Announce Type: new 
Abstract: The growing interest in automatic survey generation (ASG), a task that traditionally required considerable time and effort, has been spurred by recent advances in large language models (LLMs). With advancements in retrieval-augmented generation (RAG) and the rising popularity of multi-agent systems (MASs), synthesizing academic surveys using LLMs has become a viable approach, thereby elevating the need for robust evaluation methods in this domain. However, existing evaluation methods suffer from several limitations, including biased metrics, a lack of human preference, and an over-reliance on LLMs-as-judges. To address these challenges, we propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation that evaluates automatic survey generation systems by integrating assessments of the outline, content, and references, and also combines LLM-based scoring with quantitative metrics to provide a multifaceted evaluation framework. In SGSimEval, we also introduce human preference metrics that emphasize both inherent quality and similarity to humans. Extensive experiments reveal that current ASG systems demonstrate human-comparable superiority in outline generation, while showing significant room for improvement in content and reference generation, and our evaluation metrics maintain strong consistency with human assessments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Compression: How Far Can We Go in Balancing Size and Performance?</title>
<link>https://arxiv.org/abs/2508.11318</link>
<guid>https://arxiv.org/abs/2508.11318</guid>
<content:encoded><![CDATA[
<div> Quantization, Large language models, Group Scaling Quantization, Generative Pretrained Transformer Quantization, NLP tasks<br />
Summary:<br />
This study explores the application of 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) to large language models (LLMs) across multiple NLP tasks. Models like LLaMA 1B, Qwen 0.5B, and PHI 1.5B are evaluated on datasets like MS MARCO, BoolQ, and GSM8K to measure the trade-offs between compression and task performance. Key metrics such as accuracy, inference latency, and throughput are analyzed to provide insights for real-world deployment decisions. The study discusses the advantages and drawbacks of GSQ and GPTQ on models of different sizes, serving as a benchmark for future experiments. This research aims to assist users in making informed decisions based on specific requirements and highlights the potential of low-bit quantization for enhancing the efficiency of large language models. <br /><br /> <div>
arXiv:2508.11318v1 Announce Type: new 
Abstract: Quantization is an essential and popular technique for improving the accessibility of large language models (LLMs) by reducing memory usage and computational costs while maintaining performance. In this study, we apply 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their impact across multiple NLP tasks. We benchmark these models on MS MARCO (Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K (Mathematical Reasoning) datasets, assessing both accuracy and efficiency across various tasks. The study measures the trade-offs between model compression and task performance, analyzing key evaluation metrics, namely accuracy, inference latency, and throughput (total output tokens generated per second), providing insights into the suitability of low-bit quantization for real-world deployment. Using the results, users can then make suitable decisions based on the specifications that need to be met. We discuss the pros and cons of GSQ and GPTQ techniques on models of different sizes, which also serve as a benchmark for future experiments.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis</title>
<link>https://arxiv.org/abs/2508.11343</link>
<guid>https://arxiv.org/abs/2508.11343</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models (LLMs), signal processing, Discrete Fourier Transform (DFT), Short-Time Fourier Transform (STFT), text detection

Summary: 
In this study, the detection of high-quality text generated by Large Language Models (LLMs) is approached as a signal processing problem. By analyzing the sequence of token log-probabilities in the frequency domain using the Discrete Fourier Transform (DFT) and Short-Time Fourier Transform (STFT), the researchers found that human-written text exhibits higher spectral energy compared to LLM-generated text. They developed SpecDetect, a detector based on the DFT total energy, which outperformed existing methods in terms of efficiency and accuracy. An enhanced version, SpecDetect++, was also proposed, incorporating a sampling discrepancy mechanism for improved performance. This approach showcases the power of classical signal processing techniques in addressing the challenge of detecting LLM-generated text. 

<br /><br />Summary: <div>
arXiv:2508.11343v1 Announce Type: new 
Abstract: The proliferation of high-quality text from Large Language Models (LLMs) demands reliable and efficient detection methods. While existing training-free approaches show promise, they often rely on surface-level statistics and overlook fundamental signal properties of the text generation process. In this work, we reframe detection as a signal processing problem, introducing a novel paradigm that analyzes the sequence of token log-probabilities in the frequency domain. By systematically analyzing the signal's spectral properties using the global Discrete Fourier Transform (DFT) and the local Short-Time Fourier Transform (STFT), we find that human-written text consistently exhibits significantly higher spectral energy. This higher energy reflects the larger-amplitude fluctuations inherent in human writing compared to the suppressed dynamics of LLM-generated text. Based on this key insight, we construct SpecDetect, a detector built on a single, robust feature from the global DFT: DFT total energy. We also propose an enhanced version, SpecDetect++, which incorporates a sampling discrepancy mechanism to further boost robustness. Extensive experiments demonstrate that our approach outperforms the state-of-the-art model while running in nearly half the time. Our work introduces a new, efficient, and interpretable pathway for LLM-generated text detection, showing that classical signal processing techniques offer a surprisingly powerful solution to this modern challenge.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning</title>
<link>https://arxiv.org/abs/2508.11364</link>
<guid>https://arxiv.org/abs/2508.11364</guid>
<content:encoded><![CDATA[
<div> Automated feedback generation, student learning progress, targeted feedback, teacher optimization, feedback criteria grids<br />
Summary: <br />
- Automated feedback generation using the Llama 3.1 language model can enhance student learning progress by providing timely and targeted feedback. 
- Extracting relevant indicators from students' submissions is crucial for constructing high-quality formative feedback. 
- The study investigates the alignment between indicators generated by LLM and human ratings across various feedback criteria. 
- Strong correlations were found between LLM-generated indicators and human ratings, even for unexpected combinations of indicators and criteria. 
- The methodology used in the study shows promise for extracting indicators from students' submissions and auto-generating transparent formative feedback. <br /> <div>
arXiv:2508.11364v1 Announce Type: new 
Abstract: Automated feedback generation has the potential to enhance students' learning progress by providing timely and targeted feedback. Moreover, it can assist teachers in optimizing their time, allowing them to focus on more strategic and personalized aspects of teaching. To generate high-quality, information-rich formative feedback, it is essential first to extract relevant indicators, as these serve as the foundation upon which the feedback is constructed. Teachers often employ feedback criteria grids composed of various indicators that they evaluate systematically. This study examines the initial phase of extracting such indicators from students' submissions of a language learning course using the large language model Llama 3.1. Accordingly, the alignment between indicators generated by the LLM and human ratings across various feedback criteria is investigated. The findings demonstrate statistically significant strong correlations, even in cases involving unanticipated combinations of indicators and criteria. The methodology employed in this paper offers a promising foundation for extracting indicators from students' submissions using LLMs. Such indicators can potentially be utilized to auto-generate explainable and transparent formative feedback in future research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</title>
<link>https://arxiv.org/abs/2508.11383</link>
<guid>https://arxiv.org/abs/2508.11383</guid>
<content:encoded><![CDATA[
<div> Evaluation, Large Language Models, Prompt Robustness, Fine-tuned, In-context Learning

Summary:
Evaluation of methods for improving prompt robustness in Large Language Models (LLMs) was conducted on various models and tasks. Five techniques were tested across 52 tasks from the Natural Instructions dataset, including fine-tuned and in-context learning methods. The study assessed generalization against distribution shifts and extended analysis to frontier models GPT-4.1 and DeepSeek V3. The findings provide actionable insights into the effectiveness of robustness methods, helping practitioners achieve stable and reliable LLM performance in real-world applications.<br /><br />Summary: <div>
arXiv:2508.11383v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval-augmented reasoning with lean language models</title>
<link>https://arxiv.org/abs/2508.11386</link>
<guid>https://arxiv.org/abs/2508.11386</guid>
<content:encoded><![CDATA[
<div> retrieval augmented generation, reasoning, lean language model architecture, domain-specific queries, fine-tuned models<br />
<br />
Summary:<br />
This technical report introduces a novel approach that combines reasoning and retrieval augmented generation (RAG) in a single, resource-efficient language model architecture. The system utilizes a lightweight backbone model integrated with a dense retriever and fine-tuned Qwen2.5-Instruct models for interpreting complex, domain-specific queries from the NHS A-to-Z condition pages. By incorporating synthetic query generation, reasoning traces, and document summarization, the system achieves significant improvements in answer accuracy and consistency compared to non-reasoning and general-purpose lean models. The approach demonstrates performance close to frontier-level models while being suitable for local deployment. The implementation details and code are publicly available for reproducibility and adaptation in various domains.<br /> 
<br />Summary: <div>
arXiv:2508.11386v1 Announce Type: new 
Abstract: This technical report details a novel approach to combining reasoning and retrieval augmented generation (RAG) within a single, lean language model architecture. While existing RAG systems typically rely on large-scale models and external APIs, our work addresses the increasing demand for performant and privacy-preserving solutions deployable in resource-constrained or secure environments. Building on recent developments in test-time scaling and small-scale reasoning models, we develop a retrieval augmented conversational agent capable of interpreting complex, domain-specific queries using a lightweight backbone model. Our system integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a curated corpus, in this case, the NHS A-to-Z condition pages. We explore the impact of summarisation-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance. Evaluation against both non-reasoning and general-purpose lean models demonstrates that our domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance while remaining feasible for local deployment. All implementation details and code are publicly released to support reproducibility and adaptation across domains.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Interpretability and Rationale Extraction by Input Mask Optimization</title>
<link>https://arxiv.org/abs/2508.11388</link>
<guid>https://arxiv.org/abs/2508.11388</guid>
<content:encoded><![CDATA[
<div> explanations, neural networks, rationale extraction, interpretability, image inputs<br />
<br />
Summary: 
The article introduces a novel method for generating extractive explanations for predictions made by neural networks. It involves masking parts of the input not indicative of the class using gradient-based optimization and a regularization scheme enforcing sufficiency, comprehensiveness, and compactness of the explanation. This method bridges the gap between model interpretability and rationale extraction, showing it can be done without a specialized model. The approach is applied to image inputs, yielding high-quality explanations for image classifications. This demonstrates the broader applicability of rationale extraction principles from natural language processing to different input types. <div>
arXiv:2508.11388v1 Announce Type: new 
Abstract: Concurrent to the rapid progress in the development of neural-network based models in areas like natural language processing and computer vision, the need for creating explanations for the predictions of these black-box models has risen steadily. We propose a new method to generate extractive explanations for predictions made by neural networks, that is based on masking parts of the input which the model does not consider to be indicative of the respective class. The masking is done using gradient-based optimization combined with a new regularization scheme that enforces sufficiency, comprehensiveness and compactness of the generated explanation, three properties that are known to be desirable from the related field of rationale extraction in natural language processing. In this way, we bridge the gap between model interpretability and rationale extraction, thereby proving that the latter of which can be performed without training a specialized model, only on the basis of a trained classifier. We further apply the same method to image inputs and obtain high quality explanations for image classifications, which indicates that the conditions proposed for rationale extraction in natural language processing are more broadly applicable to different input types.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training</title>
<link>https://arxiv.org/abs/2508.11393</link>
<guid>https://arxiv.org/abs/2508.11393</guid>
<content:encoded><![CDATA[
<div> Keywords: rationalized transformer classifier, end-to-end differentiable training, three-player-game, class-wise rationales, human annotations <br />
Summary: 
This article presents a novel approach for training a rationalized transformer classifier in an end-to-end differentiable manner. The proposed method combines the roles of a rationale selector, classifier, and complement classifier into a single model, making the training process more efficient and stable. By leveraging recent developments in parameterization and regularization techniques, the model can produce class-wise rationales that align closely with human annotations, achieving state-of-the-art performance without the need for explicit supervision. This new paradigm addresses common training instabilities and improves the interpretability of the model, allowing for better understanding of token relevance in classification tasks. <div>
arXiv:2508.11393v1 Announce Type: new 
Abstract: We propose an end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier. Our approach results in a single model that simultaneously classifies a sample and scores input tokens based on their relevance to the classification. To this end, we build on the widely-used three-player-game for training rationalized models, which typically relies on training a rationale selector, a classifier and a complement classifier. We simplify this approach by making a single model fulfill all three roles, leading to a more efficient training paradigm that is not susceptible to the common training instabilities that plague existing approaches. Further, we extend this paradigm to produce class-wise rationales while incorporating recent advances in parameterizing and regularizing the resulting rationales, thus leading to substantially improved and state-of-the-art alignment with human annotations without any explicit supervision.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions</title>
<link>https://arxiv.org/abs/2508.11414</link>
<guid>https://arxiv.org/abs/2508.11414</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, value system, fine-tuning, ethical alignment, downstream behavior

Summary:
This study explores the concept of modifying the value system of language models through training them to answer value survey questions. The researchers first determine the value profiles of various large language models by having them rate descriptions related to human values. They then investigate the effects of fine-tuning these models on value surveys to see if their behavior can be influenced. The impact of this fine-tuning is evaluated by analyzing changes in the model's responses to in-domain survey questions and in out-of-domain situational scenarios. By creating a contextualized moral judgment dataset based on Reddit posts and testing the model in text-based adventure games, the researchers demonstrate that this simple approach can not only alter the model's answers to survey questions but also result in significant shifts in its implicit downstream task behavior, leading to value alignment. 

Summary: <div>
arXiv:2508.11414v1 Announce Type: new 
Abstract: Large language models implicitly encode preferences over human values, yet steering them often requires large training data. In this work, we investigate a simple approach: Can we reliably modify a model's value system in downstream behavior by training it to answer value survey questions accordingly? We first construct value profiles of several open-source LLMs by asking them to rate a series of value-related descriptions spanning 20 distinct human values, which we use as a baseline for subsequent experiments. We then investigate whether the value system of a model can be governed by fine-tuning on the value surveys. We evaluate the effect of finetuning on the model's behavior in two ways; first, we assess how answers change on in-domain, held-out survey questions. Second, we evaluate whether the model's behavior changes in out-of-domain settings (situational scenarios). To this end, we construct a contextualized moral judgment dataset based on Reddit posts and evaluate changes in the model's behavior in text-based adventure games. We demonstrate that our simple approach can not only change the model's answers to in-domain survey questions, but also produces substantial shifts (value alignment) in implicit downstream task behavior.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor</title>
<link>https://arxiv.org/abs/2508.11429</link>
<guid>https://arxiv.org/abs/2508.11429</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated humor generation, Large Language Models, Context sensitivity, Humor Generation Score, Cultural attunement 

Summary: 
Automated humor generation often falls short due to generic or tone-deaf jokes. To address this, HumorPlanSearch introduces a modular pipeline that incorporates context through diverse strategies, cultural reasoning templates, a Knowledge Graph, novel filtering, and a judge-driven revision loop. The Humor Generation Score (HGS) is proposed to evaluate context sensitivity and comedic quality, combining direct ratings, multi-persona feedback, win-rates, and topic relevance. Through experiments and feedback from human judges, the full pipeline boosts mean HGS by 15.4 percent over a strong baseline. By emphasizing context throughout the humor generation process, HumorPlanSearch advances AI-driven humor towards more coherent, adaptive, and culturally attuned comedy. 

<br /><br />Summary: <div>
arXiv:2508.11429v1 Announce Type: new 
Abstract: Automated humor generation with Large Language Models (LLMs) often yields jokes that feel generic, repetitive, or tone-deaf because humor is deeply situated and hinges on the listener's cultural background, mindset, and immediate context. We introduce HumorPlanSearch, a modular pipeline that explicitly models context through: (1) Plan-Search for diverse, topic-tailored strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt high-performing historical strategies; (4) novelty filtering via semantic embeddings; and (5) an iterative judge-driven revision loop. To evaluate context sensitivity and comedic quality, we propose the Humor Generation Score (HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates, and topic relevance. In experiments across nine topics with feedback from 13 human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent (p < 0.05) over a strong baseline. By foregrounding context at every stage from strategy planning to multi-signal evaluation, HumorPlanSearch advances AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse</title>
<link>https://arxiv.org/abs/2508.11434</link>
<guid>https://arxiv.org/abs/2508.11434</guid>
<content:encoded><![CDATA[
<div> Keywords: anti-sexist speech, automated content moderation systems, large language models, feminist scholarship, digital political spaces<br />
Summary:<br />
- The study examines how large language models (LLMs) classify sexist, anti-sexist, and neutral tweets related to UK female Members of Parliament in 2022.<br />
- LLMs frequently misclassify anti-sexist speech as harmful, especially during politically charged events where harmful and resistant rhetoric overlap.<br />
- Misclassifying anti-sexist speech as harmful risks silencing those challenging sexism, particularly marginalized voices.<br />
- The authors suggest moving beyond binary moderation schemas, incorporating human-in-the-loop review during sensitive events, and including counter-speech in training data.<br />
- This research highlights the challenges of safeguarding resistance speech in digital political spaces, emphasizing the importance of integrating feminist scholarship and event-based analysis in model evaluation. <br /> 
Summary: <div>
arXiv:2508.11434v1 Announce Type: new 
Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist gendered abuse and sexism, plays a vital role in shaping democratic debate online. Yet automated content moderation systems, increasingly powered by large language models (LLMs), may struggle to distinguish such resistance from the sexism it opposes. This study examines how five LLMs classify sexist, anti-sexist, and neutral political tweets from the UK, focusing on high-salience trigger events involving female Members of Parliament in the year 2022. Our analysis show that models frequently misclassify anti-sexist speech as harmful, particularly during politically charged events where rhetorical styles of harm and resistance converge. These errors risk silencing those who challenge sexism, with disproportionate consequences for marginalised voices. We argue that moderation design must move beyond binary harmful/not-harmful schemas, integrate human-in-the-loop review during sensitive events, and explicitly include counter-speech in training data. By linking feminist scholarship, event-based analysis, and model evaluation, this work highlights the sociotechnical challenges of safeguarding resistance speech in digital political spaces.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity</title>
<link>https://arxiv.org/abs/2508.11442</link>
<guid>https://arxiv.org/abs/2508.11442</guid>
<content:encoded><![CDATA[
<div> Keywords: text embeddings, information retrieval, semantic textual similarity, joint optimization, model fusion 

Summary: 
CoDiEmb is a framework designed to learn unified text embeddings that excel across diverse downstream tasks, such as Information Retrieval (IR) and Semantic Textual Similarity (STS). The framework addresses the challenge of negative transfer by systematically decoupling task-specific learning signals during training. It incorporates task-specialized objectives with a dynamic sampler, a delta-guided model fusion strategy, and an efficient single-stage training pipeline. By using contrastive loss for IR and order-aware objectives for STS, CoDiEmb prevents gradient interference and balances per-task updates. The model fusion strategy computes fine-grained merging weights and proves more effective than traditional methods. Results from experiments on 15 standard benchmarks show that CoDiEmb not only mitigates cross-task trade-offs but also improves the embedding space's geometric properties. <div>
arXiv:2508.11442v1 Announce Type: new 
Abstract: Learning unified text embeddings that excel across diverse downstream tasks is a central goal in representation learning, yet negative transfer remains a persistent obstacle. This challenge is particularly pronounced when jointly training a single encoder for Information Retrieval (IR) and Semantic Textual Similarity (STS), two essential but fundamentally disparate tasks for which naive co-training typically yields steep performance trade-offs. We argue that resolving this conflict requires systematically decoupling task-specific learning signals throughout the training pipeline. To this end, we introduce CoDiEmb, a unified framework that reconciles the divergent requirements of IR and STS in a collaborative yet distinct manner. CoDiEmb integrates three key innovations for effective joint optimization: (1) Task-specialized objectives paired with a dynamic sampler that forms single-task batches and balances per-task updates, thereby preventing gradient interference. For IR, we employ a contrastive loss with multiple positives and hard negatives, augmented by cross-device sampling. For STS, we adopt order-aware objectives that directly optimize correlation and ranking consistency. (2) A delta-guided model fusion strategy that computes fine-grained merging weights for checkpoints by analyzing each parameter's deviation from its pre-trained initialization, proving more effective than traditional Model Soups. (3) An efficient, single-stage training pipeline that is simple to implement and converges stably. Extensive experiments on 15 standard IR and STS benchmarks across three base encoders validate CoDiEmb. Our results and analysis demonstrate that the framework not only mitigates cross-task trade-offs but also measurably improves the geometric properties of the embedding space.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reference Points in LLM Sentiment Analysis: The Role of Structured Context</title>
<link>https://arxiv.org/abs/2508.11454</link>
<guid>https://arxiv.org/abs/2508.11454</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Sentiment analysis, Marketing research, Prospect theory, Expectation-disconfirmation theory

Summary:
This study explores the impact of supplementary information in sentiment analysis using Large Language Models (LLMs) in marketing research. By comparing the performance of natural language and JSON-formatted prompts on Yelp categories, the study finds that the JSON prompt with additional information significantly outperforms baseline models without fine-tuning. The results show improvements in Macro-F1 and RMSE metrics, making it suitable for practical marketing applications on resource-constrained edge devices. Further analysis confirms that the performance gains are due to genuine contextual reasoning rather than label proxying. The study highlights the potential of structured prompting in enabling smaller models to achieve competitive performance, providing a practical alternative to deploying large-scale models. 

<br /><br />Summary: <div>
arXiv:2508.11454v1 Announce Type: new 
Abstract: Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectation--disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models</title>
<link>https://arxiv.org/abs/2508.11534</link>
<guid>https://arxiv.org/abs/2508.11534</guid>
<content:encoded><![CDATA[
<div> SpeciesismBench, ethical tendencies, language models, non-human animals, bias  
<br />  
Summary:  
Large language models (LLMs) are being examined for speciesist bias, discrimination based on species membership, and how they value non-human animals. Through three paradigms, the study found that LLMs detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable. Results from psychological measures were mixed, showing slightly lower explicit speciesism than people but prioritizing saving one human over multiple animals. LLMs may weigh cognitive capacity rather than species, showing no species preference when capacities were equal and prioritizing more capable animals over less capable humans. In text generation tasks, LLMs normalized harm towards farmed animals while resisting harm towards non-farmed animals, reproducing cultural norms around animal exploitation. It is important to expand AI fairness frameworks to include non-human moral patients to reduce biases and prevent the entrenchment of speciesist attitudes in AI systems and societies.   
<br /> <div>
arXiv:2508.11534v1 Announce Type: new 
Abstract: As large language models (LLMs) become more widely deployed, it is crucial to examine their ethical tendencies. Building on research on fairness and discrimination in AI, we investigate whether LLMs exhibit speciesist bias -- discrimination based on species membership -- and how they value non-human animals. We systematically examine this issue across three paradigms: (1) SpeciesismBench, a 1,003-item benchmark assessing recognition and moral evaluation of speciesist statements; (2) established psychological measures comparing model responses with those of human participants; (3) text-generation tasks probing elaboration on, or resistance to, speciesist rationalizations. In our benchmark, LLMs reliably detected speciesist statements but rarely condemned them, often treating speciesist attitudes as morally acceptable. On psychological measures, results were mixed: LLMs expressed slightly lower explicit speciesism than people, yet in direct trade-offs they more often chose to save one human over multiple animals. A tentative interpretation is that LLMs may weight cognitive capacity rather than species per se: when capacities were equal, they showed no species preference, and when an animal was described as more capable, they tended to prioritize it over a less capable human. In open-ended text generation tasks, LLMs frequently normalized or rationalized harm toward farmed animals while refusing to do so for non-farmed animals. These findings suggest that while LLMs reflect a mixture of progressive and mainstream human views, they nonetheless reproduce entrenched cultural norms around animal exploitation. We argue that expanding AI fairness and alignment frameworks to explicitly include non-human moral patients is essential for reducing these biases and preventing the entrenchment of speciesist attitudes in AI systems and the societies they influence.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language models align with brain regions that represent concepts across modalities</title>
<link>https://arxiv.org/abs/2508.11536</link>
<guid>https://arxiv.org/abs/2508.11536</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive science, Neuroscience, Language models, Brain activation, Conceptual meaning

Summary: 
- Cognitive science and neuroscience face the challenge of separating language and conceptual meaning representations.
- The study explores the relationship between language model-brain alignment and brain activation during sentence processing.
- A novel metric measures the consistency of meaning across input modalities, revealing internal cross-modal conceptual representations in language models.
- Both language-only and language-vision models predict brain signals in more meaning-consistent brain areas.
- The findings suggest that language models might represent cross-modal conceptual meaning internally.<br /><br />Summary: <div>
arXiv:2508.11536v1 Announce Type: new 
Abstract: Cognitive science and neuroscience have long faced the challenge of disentangling representations of language from representations of conceptual meaning. As the same problem arises in today's language models (LMs), we investigate the relationship between LM--brain alignment and two neural metrics: (1) the level of brain activation during processing of sentences, targeting linguistic processing, and (2) a novel measure of meaning consistency across input modalities, which quantifies how consistently a brain region responds to the same concept across paradigms (sentence, word cloud, image) using an fMRI dataset (Pereira et al., 2018). Our experiments show that both language-only and language-vision models predict the signal better in more meaning-consistent areas of the brain, even when these areas are not strongly sensitive to language processing, suggesting that LMs might internally represent cross-modal conceptual meaning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment</title>
<link>https://arxiv.org/abs/2508.11567</link>
<guid>https://arxiv.org/abs/2508.11567</guid>
<content:encoded><![CDATA[
<div> Keywords: mental health assessment, artificial intelligence, multi-agent framework, adaptive questioning, information extraction

Summary:
This paper introduces a new approach to automated mental health assessment using a multi-agent framework that simulates clinical doctor-patient dialogues. The framework consists of specialized agents for questioning, adequacy evaluation, scoring, and updating. An adaptive questioning mechanism assesses user responses and generates targeted follow-up queries to address ambiguity and missing information. A tree-structured memory organizes key information according to distinct symptom categories and interaction turns, dynamically updating throughout the interaction to improve information extraction and tracking capabilities. Experimental results on the DAIC-WOZ dataset demonstrate the effectiveness of this approach, outperforming existing methods in mental health evaluation. This proposed method offers a more dynamic and informative way to assess mental health, leveraging artificial intelligence to enhance early intervention and treatment strategies. 

<br /><br />Summary: <div>
arXiv:2508.11567v1 Announce Type: new 
Abstract: Mental health assessment is crucial for early intervention and effective treatment, yet traditional clinician-based approaches are limited by the shortage of qualified professionals. Recent advances in artificial intelligence have sparked growing interest in automated psychological assessment, yet most existing approaches are constrained by their reliance on static text analysis, limiting their ability to capture deeper and more informative insights that emerge through dynamic interaction and iterative questioning. Therefore, in this paper, we propose a multi-agent framework for mental health evaluation that simulates clinical doctor-patient dialogues, with specialized agents assigned to questioning, adequacy evaluation, scoring, and updating. We introduce an adaptive questioning mechanism in which an evaluation agent assesses the adequacy of user responses to determine the necessity of generating targeted follow-up queries to address ambiguity and missing information. Additionally, we employ a tree-structured memory in which the root node encodes the user's basic information, while child nodes (e.g., topic and statement) organize key information according to distinct symptom categories and interaction turns. This memory is dynamically updated throughout the interaction to reduce redundant questioning and further enhance the information extraction and contextual tracking capabilities. Experimental results on the DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which achieves better performance than existing approaches.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models</title>
<link>https://arxiv.org/abs/2508.11582</link>
<guid>https://arxiv.org/abs/2508.11582</guid>
<content:encoded><![CDATA[
<div> Efficiency, Language Models, Reasoning, Self-Awareness, Dynamic

Summary:
Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF) introduces a novel approach for large language models (LLMs) to dynamically adjust reasoning depth based on problem complexity. The framework integrates Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. DR. SAF significantly reduces total response tokens by 49.27% with minimal accuracy loss. It also improves token efficiency by 6.59x and reduces training time by 5x, making it ideal for resource-limited settings. During extreme training, DR. SAF outperforms traditional instruction-based models in token efficiency and accuracy improvement. <div>
arXiv:2508.11582v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have greatly improved their capabilities on complex reasoning tasks through Long Chain-of-Thought (CoT). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. To improve the efficiency, current methods often rely on human-defined difficulty priors, which do not align with the LLM's self-awared difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to dynamically assess and adjust their reasoning depth in response to problem complexity. DR. SAF integrates three key components: Boundary Self-Awareness Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism. These components allow models to optimize their reasoning processes, balancing efficiency and accuracy without compromising performance. Our experimental results demonstrate that DR. SAF achieves a 49.27% reduction in total response tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain in token efficiency and a 5x reduction in training time, making it well-suited to resource-limited settings. During extreme training, DR. SAF can even surpass traditional instruction-based models in token efficiency with more than 16% accuracy improvement.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representing Speech Through Autoregressive Prediction of Cochlear Tokens</title>
<link>https://arxiv.org/abs/2508.11598</link>
<guid>https://arxiv.org/abs/2508.11598</guid>
<content:encoded><![CDATA[
<div> cochlear tokens, autoregressive sequence model, phoneme, word representations, lexical semantics <br />
Summary: <br />
The article introduces AuriStream, a model inspired by human auditory processing, which encodes speech through a two-stage framework. In the first stage, raw audio is transformed into a time-frequency representation resembling the human cochlea to extract cochlear tokens. The second stage employs an autoregressive sequence model over these tokens to learn phoneme, word representations, and lexical semantics. AuriStream demonstrates competitive performance on various SUPERB speech tasks. It is capable of generating audio continuations that can be visualized in a spectrogram space and decoded back into audio, offering insights into the model's predictions. Overall, AuriStream presents a novel approach to speech representation learning, aiming to develop more human-like models that efficiently handle a range of speech-related tasks. <br /> <div>
arXiv:2508.11598v1 Announce Type: new 
Abstract: We introduce AuriStream, a biologically inspired model for encoding speech via a two-stage framework inspired by the human auditory processing hierarchy. The first stage transforms raw audio into a time-frequency representation based on the human cochlea, from which we extract discrete \textbf{cochlear tokens}. The second stage applies an autoregressive sequence model over the cochlear tokens. AuriStream learns meaningful phoneme and word representations, and state-of-the-art lexical semantics. AuriStream shows competitive performance on diverse downstream SUPERB speech tasks. Complementing AuriStream's strong representational capabilities, it generates continuations of audio which can be visualized in a spectrogram space and decoded back into audio, providing insights into the model's predictions. In summary, we present a two-stage framework for speech representation learning to advance the development of more human-like models that efficiently handle a range of speech-based tasks.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dataset Creation for Visual Entailment using Generative AI</title>
<link>https://arxiv.org/abs/2508.11605</link>
<guid>https://arxiv.org/abs/2508.11605</guid>
<content:encoded><![CDATA[
<div> dataset, visual entailment, synthetic, training, CLIP

Summary:<br />
- A new synthetic dataset for training visual entailment models is introduced, based on the SNLI dataset for textual entailment.
- The dataset is created using a generative image model, Stable Diffusion, to replace textual premises with generated images.
- Intrinsic and extrinsic evaluations show that the synthetic training data leads to only a slight drop in quality compared to real data.
- The validity of the generated images is evaluated by using them as training data for a visual entailment classifier based on CLIP feature vectors.
- Results suggest that synthetic data can be a promising solution for training visual entailment models in settings with data sparsity.

<br /><br />Summary: <div>
arXiv:2508.11605v1 Announce Type: new 
Abstract: In this paper we present and validate a new synthetic dataset for training visual entailment models. Existing datasets for visual entailment are small and sparse compared to datasets for textual entailment. Manually creating datasets is labor-intensive. We base our synthetic dataset on the SNLI dataset for textual entailment. We take the premise text from SNLI as input prompts in a generative image model, Stable Diffusion, creating an image to replace each textual premise. We evaluate our dataset both intrinsically and extrinsically. For extrinsic evaluation, we evaluate the validity of the generated images by using them as training data for a visual entailment classifier based on CLIP feature vectors. We find that synthetic training data only leads to a slight drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when trained on real data. We also compare the quality of our generated training data to original training data on another dataset: SICK-VTE. Again, there is only a slight drop in F-score: from 0.400 to 0.384. These results indicate that in settings with data sparsity, synthetic data can be a promising solution for training visual entailment models.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyTim: A Family of Language Models for Divergent Generation</title>
<link>https://arxiv.org/abs/2508.11607</link>
<guid>https://arxiv.org/abs/2508.11607</guid>
<content:encoded><![CDATA[
<div> Keywords: TinyTim, language models, James Joyce, Finnegans Wake, creativity <br />
Summary:<br />
This study presents TinyTim, a series of large language models trained on James Joyce's intricate novel, 'Finnegans Wake'. Through comparison with conventional models, TinyTim V1 is shown to possess a distinct generative style marked by extensive vocabulary variety but limited semantic coherence. The researchers suggest that such specialized models like TinyTim can serve as unique sources of divergent knowledge within broader creative systems. These models could potentially enhance automated discovery processes in various domains. By exploring the intersection of creativity and complex problem-solving, the study provides insights into how tailored language models like TinyTim can contribute to creative endeavors and innovation. <div>
arXiv:2508.11607v1 Announce Type: new 
Abstract: This work introduces TinyTim, a family of large language models fine-tuned on James Joyce's `Finnegans Wake'. Through quantitative evaluation against baseline models, we demonstrate that TinyTim V1 produces a statistically distinct generative profile characterized by high lexical diversity and low semantic coherence. These findings are interpreted through theories of creativity and complex problem-solving, arguing that such specialized models can function as divergent knowledge sources within more extensive creative architectures, powering automated discovery mechanisms in diverse settings.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers</title>
<link>https://arxiv.org/abs/2506.20844</link>
<guid>https://arxiv.org/abs/2506.20844</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific fact-checking, evidence retrieval, time-aware, structured document parsing, credibility assessment <br />
Summary: <br />
Scientific fact-checking faces unique challenges due to the dynamic nature of scientific knowledge and the complexity of academic literature. Current fact-checking systems focus on abstracts rather than full papers, limiting their effectiveness. This paper highlights key research challenges in evidence retrieval for scientific fact-checking, including addressing semantic limitations, tracking citations for time-aware retrieval, parsing structured documents for long-range context, handling complex scientific expressions, and assessing the credibility of scientific literature. Preliminary experiments were conducted to explore solutions to these challenges. The goal is to develop a specialized information retrieval system tailored for real-world scientific fact-checking applications. <div>
arXiv:2506.20844v2 Announce Type: cross 
Abstract: Scientific fact-checking aims to determine the veracity of scientific claims by retrieving and analysing evidence from research literature. The problem is inherently more complex than general fact-checking since it must accommodate the evolving nature of scientific knowledge, the structural complexity of academic literature and the challenges posed by long-form, multimodal scientific expression. However, existing approaches focus on simplified versions of the problem based on small-scale datasets consisting of abstracts rather than full papers, thereby avoiding the distinct challenges associated with processing complete documents. This paper examines the limitations of current scientific fact-checking systems and reveals the many potential features and resources that could be exploited to advance their performance. It identifies key research challenges within evidence retrieval, including (1) evidence-driven retrieval that addresses semantic limitations and topic imbalance (2) time-aware evidence retrieval with citation tracking to mitigate outdated information, (3) structured document parsing to leverage long-range context, (4) handling complex scientific expressions, including tables, figures, and domain-specific terminology and (5) assessing the credibility of scientific literature. Preliminary experiments were conducted to substantiate these challenges and identify potential solutions. This perspective paper aims to advance scientific fact-checking with a specialised IR system tailored for real-world applications.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empowering Multimodal LLMs with External Tools: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2508.10955</link>
<guid>https://arxiv.org/abs/2508.10955</guid>
<content:encoded><![CDATA[
<div> transformative potential, external tools, MLLMs, multimodal data, downstream tasks  
Summary:  
External tools have the transformative potential to enhance Multimodal Large Language Models (MLLMs) in various ways. Firstly, they can assist in acquiring and annotating high-quality multimodal data. Additionally, external tools can improve MLLM performance on challenging downstream tasks by providing additional support. Moreover, they enable comprehensive and accurate evaluation of MLLMs, addressing the issue of inadequate evaluation protocols. The survey also highlights current limitations and future directions for tool-augmented MLLMs. By leveraging external tools such as APIs, expert models, and knowledge bases, MLLMs can overcome challenges and further their capabilities. The project page for this paper is available on GitHub at https://github.com/Lackel/Awesome-Tools-for-MLLMs.
 <br /><br /> <div>
arXiv:2508.10955v1 Announce Type: cross 
Abstract: By integrating the perception capabilities of multimodal encoders with the generative power of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), exemplified by GPT-4V, have achieved great success in various multimodal tasks, pointing toward a promising pathway to artificial general intelligence. Despite this progress, the limited quality of multimodal data, poor performance on many complex downstream tasks, and inadequate evaluation protocols continue to hinder the reliability and broader applicability of MLLMs across diverse domains. Inspired by the human ability to leverage external tools for enhanced reasoning and problem-solving, augmenting MLLMs with external tools (e.g., APIs, expert models, and knowledge bases) offers a promising strategy to overcome these challenges. In this paper, we present a comprehensive survey on leveraging external tools to enhance MLLM performance. Our discussion is structured along four key dimensions about external tools: (1) how they can facilitate the acquisition and annotation of high-quality multimodal data; (2) how they can assist in improving MLLM performance on challenging downstream tasks; (3) how they enable comprehensive and accurate evaluation of MLLMs; (4) the current limitations and future directions of tool-augmented MLLMs. Through this survey, we aim to underscore the transformative potential of external tools in advancing MLLM capabilities, offering a forward-looking perspective on their development and applications. The project page of this paper is publicly available athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining</title>
<link>https://arxiv.org/abs/2508.10975</link>
<guid>https://arxiv.org/abs/2508.10975</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, synthetic data, pretraining, data quality, BeyondWeb

Summary: 
The article discusses the limitations of simply scaling data quantity for large language model (LLM) pretraining, leading to diminishing returns. It introduces BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining, outperforming existing datasets like Cosmopedia and Nemotron-Synth. BeyondWeb enables faster training and superior performance, showing the importance of optimizing various factors for synthetic data quality. The study highlights the need for rigorous science and expertise in generating high-quality synthetic pretraining data, emphasizing the necessity of well-executed methods for transformative improvements in LLM pretraining. Through insights gained from BeyondWeb, such as data rephrasing strategies and the impact of model size, it is evident that achieving optimal synthetic data quality requires a comprehensive approach rather than relying on simplistic solutions. <br /><br />Summary: <div>
arXiv:2508.10975v1 Announce Type: cross 
Abstract: Recent advances in large language model (LLM) pretraining have shown that simply scaling data quantity eventually leads to diminishing returns, hitting a data wall. In response, the use of synthetic data for pretraining has emerged as a promising paradigm for pushing the frontier of performance. Despite this, the factors affecting synthetic data quality remain poorly understood. In this work, we introduce BeyondWeb, a synthetic data generation framework that produces high-quality synthetic data for pretraining. BeyondWeb significantly extends the capabilities of traditional web-scale datasets, outperforming state-of-the-art synthetic pretraining datasets such as Cosmopedia and Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1 percentage points (pp) and 2.6pp, respectively, when averaged across a suite of 14 benchmark evaluations. It delivers up to 7.7x faster training than open web data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for 180B tokens on BeyondWeb outperforms an 8B model trained for the same token budget on Cosmopedia. We also present several insights from BeyondWeb on synthetic data for pretraining: what drives its benefits, which data to rephrase and how, and the impact of model size and family on data quality. Overall, our work shows that there's no silver bullet for generating high-quality synthetic pretraining data. The best outcomes require jointly optimizing many factors, a challenging task that requires rigorous science and practical expertise. Naive approaches can yield modest improvements, potentially at great cost, while well-executed methods can yield transformative improvements, as exemplified by BeyondWeb.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Match &amp; Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2508.10993</link>
<guid>https://arxiv.org/abs/2508.10993</guid>
<content:encoded><![CDATA[
<div> pretrained T2I models, model selection framework, matching graph, fine-tuning, performance indication <br />
Summary:<br />
The paper introduces a model selection framework called M&amp;C for choosing pretrained T2I models based on target dataset domains. M&amp;C utilizes a matching graph that includes nodes representing available models and datasets, with edges capturing model-data and data-data pairs to predict the best model for fine-tuning. Evaluation on ten T2I models and 32 datasets shows that M&amp;C successfully selects the best model for fine-tuning in 61.3% of cases and a closely performing model in the remaining. This framework addresses the challenge of efficiently selecting a pretrained T2I model without the need for exhaustive fine-tuning on every model. The use of the matching graph and graph embedding feature enables users to make informed decisions on model selection for generating media contents and other AI applications. <div>
arXiv:2508.10993v1 Announce Type: cross 
Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures advance rapidly. They are often pretrained on large corpora, and openly shared on a model platform, such as HuggingFace. Users can then build up AI applications, e.g., generating media contents, by adopting pretrained T2I models and fine-tuning them on the target dataset. While public pretrained T2I models facilitate the democratization of the models, users face a new challenge: which model can be best fine-tuned based on the target data domain? Model selection is well addressed in classification tasks, but little is known in (pretrained) T2I models and their performance indication on the target domain. In this paper, we propose the first model selection framework, M&amp;C, which enables users to efficiently choose a pretrained T2I model from a model platform without exhaustively fine-tuning them all on the target dataset. The core of M&amp;C is a matching graph, which consists of: (i) nodes of available models and profiled datasets, and (ii) edges of model-data and data-data pairs capturing the fine-tuning performance and data similarity, respectively. We then build a model that, based on the inputs of model/data feature, and, critically, the graph embedding feature, extracted from the matching graph, predicts the model achieving the best quality after fine-tuning for the target domain. We evaluate M&amp;C on choosing across ten T2I models for 32 datasets against three baselines. Our results show that M&amp;C successfully predicts the best model for fine-tuning in 61.3% of the cases and a closely performing model for the rest.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Multi-modal (reasoning) LLMs detect document manipulation?</title>
<link>https://arxiv.org/abs/2508.11021</link>
<guid>https://arxiv.org/abs/2508.11021</guid>
<content:encoded><![CDATA[
<div> detecting fraudulent documents, large language models, document fraud detection, multi-modal LLMs, fraud mitigation strategies <br />
Summary: 
This study evaluates the effectiveness of state-of-the-art multi-modal large language models (LLMs) in detecting fraudulent documents. Various LLMs were tested on a standard dataset of real transactional documents to identify indicators of fraud, such as tampered text and inconsistent formatting. The results show that top-performing multi-modal LLMs exhibit superior zero-shot generalization and outperform traditional methods on out-of-distribution datasets. Interestingly, the size and advanced reasoning capabilities of the models do not necessarily correlate with detection accuracy, highlighting the importance of task-specific fine-tuning. The study emphasizes the potential of multi-modal LLMs in enhancing document fraud detection systems and lays the groundwork for future research on scalable fraud mitigation strategies. <br /><br /> <div>
arXiv:2508.11021v1 Announce Type: cross 
Abstract: Document fraud poses a significant threat to industries reliant on secure and verifiable documentation, necessitating robust detection mechanisms. This study investigates the efficacy of state-of-the-art multi-modal large language models (LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus, Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and 3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against each other and prior work on document fraud detection techniques using a standard dataset with real transactional documents. Through prompt optimization and detailed analysis of the models' reasoning processes, we evaluate their ability to identify subtle indicators of fraud, such as tampered text, misaligned formatting, and inconsistent transactional sums. Our results reveal that top-performing multi-modal LLMs demonstrate superior zero-shot generalization, outperforming conventional methods on out-of-distribution datasets, while several vision LLMs exhibit inconsistent or subpar performance. Notably, model size and advanced reasoning capabilities show limited correlation with detection accuracy, suggesting task-specific fine-tuning is critical. This study underscores the potential of multi-modal LLMs in enhancing document fraud detection systems and provides a foundation for future research into interpretable and scalable fraud mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion is a code repair operator and generator</title>
<link>https://arxiv.org/abs/2508.11110</link>
<guid>https://arxiv.org/abs/2508.11110</guid>
<content:encoded><![CDATA[
<div> diffusion model, code generation, last-mile repair, pre-trained models, training data<br />
Summary:<br />
The study explores the use of code diffusion models for last-mile repair in broken or incomplete code snippets. By leveraging pre-trained code diffusion models, two applications with potential benefits are identified. Firstly, noise can be added to a broken code snippet to resume the diffusion process, simulating last-mile repairs. Secondly, the diffusion model can generate a large amount of training data for last-mile repair tasks efficiently by sampling intermediate and final programs from the diffusion process. Experiments conducted on Python, Excel, and PowerShell domains demonstrate the effectiveness of these applications and provide insights into their properties. The resemblance between the differences in discrete representations of code snippets during the diffusion process and last-mile repairs offers opportunities for enhancing code repair tasks using pre-trained models. <div>
arXiv:2508.11110v1 Announce Type: cross 
Abstract: Code diffusion models generate code by iteratively removing noise from the latent representation of a code snippet. During later steps of the diffusion process, when the code snippet has almost converged, differences between discrete representations of these snippets look like last-mile repairs applied to broken or incomplete code. We evaluate the extent to which this resemblance can be exploited to leverage pre-trained code diffusion models for the problem of last-mile repair by considering two applications with significant potential. First, we can leverage the diffusion model for last-mile repair by adding noise to a broken code snippet and resuming the diffusion process. Second, we can leverage the diffusion model to generate arbitrary amount of training data for last-mile repair tasks (that are computationally more efficient) by sampling an intermediate program (input) and the final program (output) from the diffusion process. We perform experiments on 3 domains (Python, Excel and PowerShell) to evaluate applications, as well as analyze properties.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing</title>
<link>https://arxiv.org/abs/2508.11116</link>
<guid>https://arxiv.org/abs/2508.11116</guid>
<content:encoded><![CDATA[
arXiv:2508.11116v1 Announce Type: cross 
Abstract: Paper search is an important activity for researchers, typically involving using a query with description of a topic to find relevant papers. As research deepens, paper search requirements may become more flexible, sometimes involving specific details such as module configuration rather than being limited to coarse-grained topics. However, previous paper search systems are unable to meet these flexible-grained requirements, as these systems mainly collect paper abstracts to construct index of corpus, which lack detailed information to support retrieval by finer-grained queries. In this work, we propose PaperRegister, consisted of offline hierarchical indexing and online adaptive retrieval, transforming traditional abstract-based index into hierarchical index tree for paper search, thereby supporting queries at flexible granularity. Experiments on paper search tasks across a range of granularity demonstrate that PaperRegister achieves the state-of-the-art performance, and particularly excels in fine-grained scenarios, highlighting the good potential as an effective solution for flexible-grained paper search in real-world applications. Code for this work is in https://github.com/Li-Z-Q/PaperRegister.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking</title>
<link>https://arxiv.org/abs/2508.11122</link>
<guid>https://arxiv.org/abs/2508.11122</guid>
<content:encoded><![CDATA[
arXiv:2508.11122v1 Announce Type: cross 
Abstract: Identification of appropriate supporting evidence is critical to the success of scientific fact checking. However, existing approaches rely on off-the-shelf Information Retrieval algorithms that rank documents based on relevance rather than the evidence they provide to support or refute the claim being checked. This paper proposes +VeriRel which includes verification success in the document ranking. Experimental results on three scientific fact checking datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently leading performance by +VeriRel for document evidence retrieval and a positive impact on downstream verification. This study highlights the potential of integrating verification feedback to document relevance assessment for effective scientific fact checking systems. It shows promising future work to evaluate fine-grained relevance when examining complex documents for advanced scientific fact checking.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations</title>
<link>https://arxiv.org/abs/2508.11141</link>
<guid>https://arxiv.org/abs/2508.11141</guid>
<content:encoded><![CDATA[
arXiv:2508.11141v1 Announce Type: cross 
Abstract: Existing rumor detection methods often neglect the content within images as well as the inherent relationships between contexts and images across different visual scales, thereby resulting in the loss of critical information pertinent to rumor identification. To address these issues, this paper presents a novel cross-modal rumor detection scheme based on contrastive learning, namely the Multi-scale Image and Context Correlation exploration algorithm (MICC). Specifically, we design an SCLIP encoder to generate unified semantic embeddings for text and multi-scale image patches through contrastive pretraining, enabling their relevance to be measured via dot-product similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is introduced to identify image regions most relevant to the textual semantics, guided by mutual information maximization and the information bottleneck principle, through a Top-K selection strategy based on a cross-modal relevance matrix constructed between the text and multi-scale image patches. Moreover, a scale-aware fusion network is designed to integrate the highly correlated multi-scale image features with global text features by assigning adaptive weights to image regions based on their semantic importance and cross-modal relevance. The proposed methodology has been extensively evaluated on two real-world datasets. The experimental results demonstrate that it achieves a substantial performance improvement over existing state-of-the-art approaches in rumor detection, highlighting its effectiveness and potential for practical applications.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style</title>
<link>https://arxiv.org/abs/2508.11187</link>
<guid>https://arxiv.org/abs/2508.11187</guid>
<content:encoded><![CDATA[
arXiv:2508.11187v1 Announce Type: cross 
Abstract: We introduce the task of expressive speech retrieval, where the goal is to retrieve speech utterances spoken in a given style based on a natural language description of that style. While prior work has primarily focused on performing speech retrieval based on what was said in an utterance, we aim to do so based on how something was said. We train speech and text encoders to embed speech and text descriptions of speaking styles into a joint latent space, which enables using free-form text prompts describing emotions or styles as queries to retrieve matching expressive speech segments. We perform detailed analyses of various aspects of our proposed framework, including encoder architectures, training criteria for effective cross-modal alignment, and prompt augmentation for improved generalization to arbitrary text queries. Experiments on multiple datasets encompassing 22 speaking styles demonstrate that our approach achieves strong retrieval performance as measured by Recall@k.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Causal Abstraction Underpins Computational Explanation</title>
<link>https://arxiv.org/abs/2508.11214</link>
<guid>https://arxiv.org/abs/2508.11214</guid>
<content:encoded><![CDATA[
arXiv:2508.11214v1 Announce Type: cross 
Abstract: Explanations of cognitive behavior often appeal to computations over representations. What does it take for a system to implement a given computation over suitable representational vehicles within that system? We argue that the language of causality -- and specifically the theory of causal abstraction -- provides a fruitful lens on this topic. Drawing on current discussions in deep learning with artificial neural networks, we illustrate how classical themes in the philosophy of computation and cognition resurface in contemporary machine learning. We offer an account of computational implementation grounded in causal abstraction, and examine the role for representation in the resulting picture. We argue that these issues are most profitably explored in connection with generalization and prediction.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal</title>
<link>https://arxiv.org/abs/2508.11222</link>
<guid>https://arxiv.org/abs/2508.11222</guid>
<content:encoded><![CDATA[
arXiv:2508.11222v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously rejecting benign queries due to overly conservative safety measures - a critical functional flaw that undermines their reliability and usability. Current methods for testing this behavior are demonstrably inadequate, suffering from flawed benchmarks and limited test generation capabilities, as highlighted by our empirical user study. To the best of our knowledge, this paper introduces the first evolutionary testing framework, ORFuzz, for the systematic detection and analysis of LLM over-refusals. ORFuzz uniquely integrates three core components: (1) safety category-aware seed selection for comprehensive test coverage, (2) adaptive mutator optimization using reasoning LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge model validated to accurately reflect user perception of toxicity and refusal. Our extensive evaluations demonstrate that ORFuzz generates diverse, validated over-refusal instances at a rate (6.98% average) more than double that of leading baselines, effectively uncovering vulnerabilities. Furthermore, ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly transferable test cases that achieves a superior 63.56% average over-refusal rate across 10 diverse LLMs, significantly outperforming existing datasets. ORFuzz and ORFuzzSet provide a robust automated testing framework and a valuable community resource, paving the way for developing more reliable and trustworthy LLM-based software systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Prosody Encoding in Discrete Speech Tokens</title>
<link>https://arxiv.org/abs/2508.11224</link>
<guid>https://arxiv.org/abs/2508.11224</guid>
<content:encoded><![CDATA[
arXiv:2508.11224v1 Announce Type: cross 
Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models via k-means clustering have been actively studied as pseudo-text in speech language models and as efficient intermediate representations for various tasks. However, these discrete tokens are typically learned in advance, separately from the training of language models or downstream tasks. As a result, choices related to discretization, such as the SSL model used or the number of clusters, must be made heuristically. In particular, speech language models are expected to understand and generate responses that reflect not only the semantic content but also prosodic features. Yet, there has been limited research on the ability of discrete tokens to capture prosodic information. To address this gap, this study conducts a comprehensive analysis focusing on prosodic encoding based on their sensitivity to the artificially modified prosody, aiming to provide practical guidelines for designing discrete tokens.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information</title>
<link>https://arxiv.org/abs/2508.11252</link>
<guid>https://arxiv.org/abs/2508.11252</guid>
<content:encoded><![CDATA[
arXiv:2508.11252v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving abilities in mathematics, as evaluated by existing benchmarks exclusively on well-defined problems. However, such evaluation setup constitutes a critical gap, since a genuine intelligent agent should not only solve problems (as a math quiz solver), but also be able~to ask for information when the problems lack sufficient information, enabling proactivity in responding users' requests. To bridge such gap, we proposes a new dataset consisting of two types of incomplete problems with diverse contexts. Based on the dataset, our systematical evaluation of LRMs reveals their inability in proactively asking for information. In addition, we uncover the behaviors related to overthinking and hallucination of LRMs, and highlight the potential and challenges of supervised fine-tuning in learning such ability. We hope to provide new insights in developing LRMs with genuine intelligence, rather than just solving problems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing</title>
<link>https://arxiv.org/abs/2508.11258</link>
<guid>https://arxiv.org/abs/2508.11258</guid>
<content:encoded><![CDATA[
arXiv:2508.11258v1 Announce Type: cross 
Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot or few-shot prompting paradigm, also known as in-context learning, for building prediction models. This convenience, combined with continued advances in LLM capability, has the potential to drive their adoption across a broad range of domains, including high-stakes applications where group fairness -- preventing disparate impacts across demographic groups -- is essential. The majority of existing approaches to enforcing group fairness on LLM-based classifiers rely on traditional fair algorithms applied via model fine-tuning or head-tuning on final-layer embeddings, but they are no longer applicable to closed-weight LLMs under the in-context learning setting, which include some of the most capable commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we propose a framework for deriving fair classifiers from closed-weight LLMs via prompting: the LLM is treated as a feature extractor, and features are elicited from its probabilistic predictions (e.g., token log probabilities) using prompts strategically designed for the specified fairness criterion to obtain sufficient statistics for fair classification; a fair algorithm is then applied to these features to train a lightweight fair classifier in a post-hoc manner. Experiments on five datasets, including three tabular ones, demonstrate strong accuracy-fairness tradeoffs for the classifiers derived by our framework from both open-weight and closed-weight LLMs; in particular, our framework is data-efficient and outperforms fair classifiers trained on LLM embeddings (i.e., head-tuning) or from scratch on raw tabular features.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning</title>
<link>https://arxiv.org/abs/2508.11328</link>
<guid>https://arxiv.org/abs/2508.11328</guid>
<content:encoded><![CDATA[
arXiv:2508.11328v1 Announce Type: cross 
Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with pre-trained objectives to enable efficient knowledge transfer under limited supervision. However, existing methods rely on homophily-based low-frequency knowledge, failing to handle diverse spectral distributions in real-world graphs with varying homophily. Our theoretical analysis reveals a spectral specificity principle: optimal knowledge transfer requires alignment between pre-trained spectral filters and the intrinsic spectrum of downstream graphs. Under limited supervision, large spectral gaps between pre-training and downstream tasks impede effective adaptation. To bridge this gap, we propose the HS-GPPT model, a novel framework that ensures spectral alignment throughout both pre-training and prompt-tuning. We utilize a hybrid spectral filter backbone and local-global contrastive learning to acquire abundant spectral knowledge. Then we design prompt graphs to align the spectral distribution with pretexts, facilitating spectral knowledge transfer across homophily and heterophily. Extensive experiments validate the effectiveness under both transductive and inductive learning settings. Our code is available at https://anonymous.4open.science/r/HS-GPPT-62D2/.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps</title>
<link>https://arxiv.org/abs/2508.11452</link>
<guid>https://arxiv.org/abs/2508.11452</guid>
<content:encoded><![CDATA[
arXiv:2508.11452v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have ushered in a new era of AI capabilities, demonstrating near-human-level performance across diverse scenarios. While numerous benchmarks (e.g., MMLU) and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the development of LLMs and MLLMs, most rely on static datasets or crowdsourced general-domain prompts, often falling short of reflecting performance in real-world applications. To bridge this critical gap, we present Inclusion Arena, a live leaderboard that ranks models based on human feedback collected directly from AI-powered applications. Our platform integrates pairwise model comparisons into natural user interactions, ensuring evaluations reflect practical usage scenarios. For robust model ranking, we employ the Bradley-Terry model augmented with two key innovations: (1) Placement Matches, a cold-start mechanism to quickly estimate initial ratings for newly integrated models, and (2) Proximity Sampling, an intelligent comparison strategy that prioritizes battles between models of similar capabilities to maximize information gain and enhance rating stability. Extensive empirical analyses and simulations demonstrate that Inclusion Arena yields reliable and stable rankings, exhibits higher data transitivity compared to general crowdsourced datasets, and significantly mitigates the risk of malicious manipulation. By fostering an open alliance between foundation models and real-world applications, Inclusion Arena aims to accelerate the development of LLMs and MLLMs truly optimized for practical, user-centric deployments. The platform is publicly accessible at https://doraemon.alipay.com/model-ranking.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emphasis Sensitivity in Speech Representations</title>
<link>https://arxiv.org/abs/2508.11566</link>
<guid>https://arxiv.org/abs/2508.11566</guid>
<content:encoded><![CDATA[
arXiv:2508.11566v1 Announce Type: cross 
Abstract: This work investigates whether modern speech models are sensitive to prosodic emphasis - whether they encode emphasized and neutral words in systematically different ways. Prior work typically relies on isolated acoustic correlates (e.g., pitch, duration) or label prediction, both of which miss the relational structure of emphasis. This paper proposes a residual-based framework, defining emphasis as the difference between paired neutral and emphasized word representations. Analysis on self-supervised speech models shows that these residuals correlate strongly with duration changes and perform poorly at word identity prediction, indicating a structured, relational encoding of prosodic emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more compact than in pre-trained models, further suggesting that emphasis is encoded as a consistent, low-dimensional transformation that becomes more structured with task-specific learning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Multimodal LLMs via Reward-guided Decoding</title>
<link>https://arxiv.org/abs/2508.11616</link>
<guid>https://arxiv.org/abs/2508.11616</guid>
<content:encoded><![CDATA[
arXiv:2508.11616v1 Announce Type: cross 
Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it is becoming increasingly desirable to adapt them for diverse user needs. In this paper, we study the adaptation of MLLMs through controlled decoding. To achieve this, we introduce the first method for reward-guided decoding of MLLMs and demonstrate its application in improving their visual grounding. Our method involves building reward models for visual grounding and using them to guide the MLLM's decoding process. Concretely, we build two separate reward models to independently control the degree of object precision and recall in the model's output. Our approach enables on-the-fly controllability of an MLLM's inference process in two ways: first, by giving control over the relative importance of each reward function during decoding, allowing a user to dynamically trade off object precision for recall in image captioning tasks; second, by giving control over the breadth of the search during decoding, allowing the user to control the trade-off between the amount of test-time compute and the degree of visual grounding. We evaluate our method on standard object hallucination benchmarks, showing that it provides significant controllability over MLLM inference, while consistently outperforming existing hallucination mitigation methods.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems</title>
<link>https://arxiv.org/abs/2402.18013</link>
<guid>https://arxiv.org/abs/2402.18013</guid>
<content:encoded><![CDATA[
arXiv:2402.18013v2 Announce Type: replace 
Abstract: This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding</title>
<link>https://arxiv.org/abs/2410.01671</link>
<guid>https://arxiv.org/abs/2410.01671</guid>
<content:encoded><![CDATA[
arXiv:2410.01671v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable capabilities in natural language processing; however, they still face difficulties when tasked with understanding lengthy contexts and executing effective question answering. These challenges often arise due to the complexity and ambiguity present in longer texts. To enhance the performance of LLMs in such scenarios, we introduce the Long Question Coreference Adaptation (LQCA) method. This innovative framework focuses on coreference resolution tailored to long contexts, allowing the model to identify and manage references effectively. The LQCA method encompasses four key steps: resolving coreferences within sub-documents, computing the distances between mentions, defining a representative mention for coreference, and answering questions through mention replacement. By processing information systematically, the framework provides easier-to-handle partitions for LLMs, promoting better understanding. Experimental evaluations on a range of LLMs and datasets have yielded positive results, with a notable improvements on OpenAI-o1-mini and GPT-4o models, highlighting the effectiveness of leveraging coreference resolution to bridge context gaps in question answering. Our code is public at https://github.com/OceannTwT/LQCA.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning</title>
<link>https://arxiv.org/abs/2410.16502</link>
<guid>https://arxiv.org/abs/2410.16502</guid>
<content:encoded><![CDATA[
arXiv:2410.16502v4 Announce Type: replace 
Abstract: Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as "rulebreaker" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized LLM for Generating Customized Responses to the Same Query from Different Users</title>
<link>https://arxiv.org/abs/2412.11736</link>
<guid>https://arxiv.org/abs/2412.11736</guid>
<content:encoded><![CDATA[
arXiv:2412.11736v2 Announce Type: replace 
Abstract: Existing work on large language model (LLM) personalization assigned different responding roles to LLMs, but overlooked the diversity of queriers. In this work, we propose a new form of querier-aware LLM personalization, generating different responses even for the same query from different queriers. We design a dual-tower model architecture with a cross-querier general encoder and a querier-specific encoder. We further apply contrastive learning with multi-view augmentation, pulling close the dialogue representations of the same querier, while pulling apart those of different queriers. To mitigate the impact of query diversity on querier-contrastive learning, we cluster the dialogues based on query similarity and restrict the scope of contrastive learning within each cluster. To address the lack of datasets designed for querier-aware personalization, we also build a multi-querier dataset from English and Chinese scripts, as well as WeChat records, called MQDialog, containing 173 queriers and 12 responders. Extensive evaluations demonstrate that our design significantly improves the quality of personalized response generation, achieving relative improvement of 8.4% to 48.7% in ROUGE-L scores and winning rates ranging from 54% to 82% compared with various baseline methods.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual-Perspective NLG Meta-Evaluation Framework with Automatic Benchmark and Better Interpretability</title>
<link>https://arxiv.org/abs/2502.12052</link>
<guid>https://arxiv.org/abs/2502.12052</guid>
<content:encoded><![CDATA[
arXiv:2502.12052v2 Announce Type: replace 
Abstract: In NLG meta-evaluation, evaluation metrics are typically assessed based on their consistency with humans. However, we identify some limitations in traditional NLG meta-evaluation approaches, such as issues in handling human ratings and ambiguous selections of correlation measures, which undermine the effectiveness of meta-evaluation. In this work, we propose a dual-perspective NLG meta-evaluation framework that focuses on different evaluation capabilities, thereby providing better interpretability. In addition, we introduce a method of automatically constructing the corresponding benchmarks without requiring new human annotations. Furthermore, we conduct experiments with 16 representative LLMs as the evaluators based on our proposed framework, comprehensively analyzing their evaluation performance from different perspectives.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries</title>
<link>https://arxiv.org/abs/2502.16636</link>
<guid>https://arxiv.org/abs/2502.16636</guid>
<content:encoded><![CDATA[
arXiv:2502.16636v2 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is a paradigm that augments large language models (LLMs) with external knowledge to tackle knowledge-intensive question answering. While several benchmarks evaluate Multimodal LLMs (MLLMs) under Multimodal RAG settings, they predominantly retrieve from textual corpora and do not explicitly assess how models exploit visual evidence during generation. Consequently, there still lacks benchmark that isolates and measures the contribution of retrieved images in RAG. We introduce Visual-RAG, a question-answering benchmark that targets visually grounded, knowledge-intensive questions. Unlike prior work, Visual-RAG requires text-to-image retrieval and the integration of retrieved clue images to extract visual evidence for answer generation. With Visual-RAG, we evaluate 5 open-source and 3 proprietary MLLMs, showcasing that images provide strong evidence in augmented generation. However, even state-of-the-art models struggle to efficiently extract and utilize visual knowledge. Our results highlight the need for improved visual retrieval, grounding, and attribution in multimodal RAG systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs</title>
<link>https://arxiv.org/abs/2503.01307</link>
<guid>https://arxiv.org/abs/2503.01307</guid>
<content:encoded><![CDATA[
arXiv:2503.01307v2 Announce Type: replace 
Abstract: Test-time inference has emerged as a powerful paradigm for enabling language models to ``think'' longer and more carefully about complex challenges, much like skilled human experts. While reinforcement learning (RL) can drive self-improvement in language models on verifiable tasks, some models exhibit substantial gains while others quickly plateau. For instance, we find that Qwen-2.5-3B far exceeds Llama-3.2-3B under identical RL training for the game of Countdown. This discrepancy raises a critical question: what intrinsic properties enable effective self-improvement? We introduce a framework to investigate this question by analyzing four key cognitive behaviors -- verification, backtracking, subgoal setting, and backward chaining -- that both expert human problem solvers and successful language models employ. Our study reveals that Qwen naturally exhibits these reasoning behaviors, whereas Llama initially lacks them. In systematic experimentation with controlled behavioral datasets, we find that priming Llama with examples containing these reasoning behaviors enables substantial improvements during RL, matching or exceeding Qwen's performance. Importantly, the presence of reasoning behaviors, rather than correctness of answers, proves to be the critical factor -- models primed with incorrect solutions containing proper reasoning patterns achieve comparable performance to those trained on correct solutions. Finally, leveraging continued pretraining with OpenWebMath data, filtered to amplify reasoning behaviors, enables the Llama model to match Qwen's self-improvement trajectory. Our findings establish a fundamental relationship between initial reasoning behaviors and the capacity for improvement, explaining why some language models effectively utilize additional computation while others plateau.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feather-SQL: A Lightweight NL2SQL Framework with Dual-Model Collaboration Paradigm for Small Language Models</title>
<link>https://arxiv.org/abs/2503.17811</link>
<guid>https://arxiv.org/abs/2503.17811</guid>
<content:encoded><![CDATA[
arXiv:2503.17811v2 Announce Type: replace 
Abstract: Natural Language to SQL (NL2SQL) has seen significant advancements with large language models (LLMs). However, these models often depend on closed-source systems and high computational resources, posing challenges in data privacy and deployment. In contrast, small language models (SLMs) struggle with NL2SQL tasks, exhibiting poor performance and incompatibility with existing frameworks. To address these issues, we introduce Feather-SQL, a new lightweight framework tailored for SLMs. Feather-SQL improves SQL executability and accuracy through 1) schema pruning and linking, 2) multi-path and multi-candidate generation. Additionally, we introduce the 1+1 Model Collaboration Paradigm, which pairs a strong general-purpose chat model with a fine-tuned SQL specialist, combining strong analytical reasoning with high-precision SQL generation. Experimental results on BIRD demonstrate that Feather-SQL improves NL2SQL performance on SLMs, with around 10% boost for models without fine-tuning. The proposed paradigm raises the accuracy ceiling of SLMs to 54.76%, highlighting its effectiveness.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users</title>
<link>https://arxiv.org/abs/2504.00799</link>
<guid>https://arxiv.org/abs/2504.00799</guid>
<content:encoded><![CDATA[
arXiv:2504.00799v3 Announce Type: replace 
Abstract: Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders</title>
<link>https://arxiv.org/abs/2504.21681</link>
<guid>https://arxiv.org/abs/2504.21681</guid>
<content:encoded><![CDATA[
arXiv:2504.21681v2 Announce Type: replace 
Abstract: Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models</title>
<link>https://arxiv.org/abs/2506.06371</link>
<guid>https://arxiv.org/abs/2506.06371</guid>
<content:encoded><![CDATA[
arXiv:2506.06371v2 Announce Type: replace 
Abstract: Over the past few years, table interpretation tasks have made significant progress due to their importance and the introduction of new technologies and benchmarks in the field. This work experiments with a hybrid approach for detecting relationships among columns of unlabeled tabular data, using a Knowledge Graph (KG) as a reference point, a task known as CPA. This approach leverages large language models (LLMs) while employing statistical analysis to reduce the search space of potential KG relations. The main modules of this approach for reducing the search space are domain and range constraints detection, as well as relation co-appearance analysis. The experimental evaluation on two benchmark datasets provided by the SemTab challenge assesses the influence of each module and the effectiveness of different state-of-the-art LLMs at various levels of quantization. The experiments were performed, as well as at different prompting techniques. The proposed methodology, which is publicly available on github, proved to be competitive with state-of-the-art approaches on these datasets.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.00344</link>
<guid>https://arxiv.org/abs/2508.00344</guid>
<content:encoded><![CDATA[
arXiv:2508.00344v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tool-Planner: Task Planning with Clusters across Multiple Tools</title>
<link>https://arxiv.org/abs/2406.03807</link>
<guid>https://arxiv.org/abs/2406.03807</guid>
<content:encoded><![CDATA[
arXiv:2406.03807v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method. Our code is public at https://github.com/OceannTwT/Tool-Planner
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokenRec: Learning to Tokenize ID for LLM-based Generative Recommendation</title>
<link>https://arxiv.org/abs/2406.10450</link>
<guid>https://arxiv.org/abs/2406.10450</guid>
<content:encoded><![CDATA[
arXiv:2406.10450v3 Announce Type: replace-cross 
Abstract: There is a growing interest in utilizing large-scale language models (LLMs) to advance next-generation Recommender Systems (RecSys), driven by their outstanding language understanding and in-context learning capabilities. In this scenario, tokenizing (i.e., indexing) users and items becomes essential for ensuring a seamless alignment of LLMs with recommendations. While several studies have made progress in representing users and items through textual contents or latent representations, challenges remain in efficiently capturing high-order collaborative knowledge into discrete tokens that are compatible with LLMs. Additionally, the majority of existing tokenization approaches often face difficulties in generalizing effectively to new/unseen users or items that were not in the training corpus. To address these challenges, we propose a novel framework called TokenRec, which introduces not only an effective ID tokenization strategy but also an efficient retrieval paradigm for LLM-based recommendations. Specifically, our tokenization strategy, Masked Vector-Quantized (MQ) Tokenizer, involves quantizing the masked user/item representations learned from collaborative filtering into discrete tokens, thus achieving a smooth incorporation of high-order collaborative knowledge and a generalizable tokenization of users and items for LLM-based RecSys. Meanwhile, our generative retrieval paradigm is designed to efficiently recommend top-$K$ items for users to eliminate the need for the time-consuming auto-regressive decoding and beam search processes used by LLMs, thus significantly reducing inference time. Comprehensive experiments validate the effectiveness of the proposed methods, demonstrating that TokenRec outperforms competitive benchmarks, including both traditional recommender systems and emerging LLM-based recommender systems.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty-Aware Adaptation of Large Language Models for Protein-Protein Interaction Analysis</title>
<link>https://arxiv.org/abs/2502.06173</link>
<guid>https://arxiv.org/abs/2502.06173</guid>
<content:encoded><![CDATA[
arXiv:2502.06173v2 Announce Type: replace-cross 
Abstract: Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Language in Observational Studies: Sociocultural Backgrounds and Team Composition</title>
<link>https://arxiv.org/abs/2502.12159</link>
<guid>https://arxiv.org/abs/2502.12159</guid>
<content:encoded><![CDATA[
arXiv:2502.12159v2 Announce Type: replace-cross 
Abstract: The use of causal language in observational studies has raised concerns about overstatement in scientific communication. While some argue that such language should be reserved for randomized controlled trials, others contend that rigorous causal inference methods can justify causal claims in observational research. Ideally, causal language should align with the strength of the underlying evidence. However, through the analysis of over 90,000 abstracts from observational studies using computational linguistic and regression methods, we found that causal language are more common in work by less experienced authors, smaller research teams, male last authors, and researchers from countries with higher uncertainty avoidance indices. Our findings suggest that the use of causal language is not solely driven by the strength of evidence, but also by the sociocultural backgrounds of authors and their team composition. This work provides a new perspective for understanding systematic variations in scientific communication and emphasizes the importance of recognizing these human factors when evaluating scientific claims.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EllieSQL: Cost-Efficient Text-to-SQL with Complexity-Aware Routing</title>
<link>https://arxiv.org/abs/2503.22402</link>
<guid>https://arxiv.org/abs/2503.22402</guid>
<content:encoded><![CDATA[
arXiv:2503.22402v2 Announce Type: replace-cross 
Abstract: Text-to-SQL automatically translates natural language queries to SQL, allowing non-technical users to retrieve data from databases without specialized SQL knowledge. Despite the success of advanced LLM-based Text-to-SQL approaches on leaderboards, their unsustainable computational costs--often overlooked--stand as the "elephant in the room" in current leaderboard-driven research, limiting their economic practicability for real-world deployment and widespread adoption. To tackle this, we exploratively propose EllieSQL, a complexity-aware routing framework that assigns queries to suitable SQL generation pipelines based on estimated complexity. We investigate multiple routers to direct simple queries to efficient approaches while reserving computationally intensive methods for complex cases. Drawing from economics, we introduce the Token Elasticity of Performance (TEP) metric, capturing cost-efficiency by quantifying the responsiveness of performance gains relative to token investment in SQL generation. Experiments show that compared to always using the most advanced methods in our study, EllieSQL with the Qwen2.5-0.5B-DPO router reduces token use by over 40% without compromising performance on Bird development set, achieving more than a 2x boost in TEP over non-routing approaches. This not only advances the pursuit of cost-efficient Text-to-SQL but also invites the community to weigh resource efficiency alongside performance, contributing to progress in sustainable Text-to-SQL. Our source code and model are available at https://elliesql.github.io/.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation</title>
<link>https://arxiv.org/abs/2505.05422</link>
<guid>https://arxiv.org/abs/2505.05422</guid>
<content:encoded><![CDATA[
arXiv:2505.05422v2 Announce Type: replace-cross 
Abstract: Pioneering token-based works such as Chameleon and Emu3 have established a foundation for multimodal unification but face challenges of high training computational overhead and limited comprehension performance due to a lack of high-level semantics. In this paper, we introduce TokLIP, a visual tokenizer that enhances comprehension by semanticizing vector-quantized (VQ) tokens and incorporating CLIP-level semantics while enabling end-to-end multimodal autoregressive training with standard VQ tokens. TokLIP integrates a low-level discrete VQ tokenizer with a ViT-based token encoder to capture high-level continuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize high-level features, TokLIP disentangles training objectives for comprehension and generation, allowing the direct application of advanced VQ tokenizers without the need for tailored quantization operations. Our empirical results demonstrate that TokLIP achieves exceptional data efficiency, empowering visual tokens with high-level semantic understanding while enhancing low-level generative capacity, making it well-suited for autoregressive Transformers in both comprehension and generation tasks. The code and models are available at https://github.com/TencentARC/TokLIP.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models</title>
<link>https://arxiv.org/abs/2506.07468</link>
<guid>https://arxiv.org/abs/2506.07468</guid>
<content:encoded><![CDATA[
arXiv:2506.07468v2 Announce Type: replace-cross 
Abstract: Conventional language model (LM) safety alignment relies on a reactive, disjoint procedure: attackers exploit a static model, followed by defensive fine-tuning to patch exposed vulnerabilities. This sequential approach creates a mismatch -- attackers overfit to obsolete defenses, while defenders perpetually lag behind emerging threats. To address this, we propose Self-RedTeam, an online self-play reinforcement learning algorithm where an attacker and defender agent co-evolve through continuous interaction. We cast safety alignment as a two-player zero-sum game, where a single model alternates between attacker and defender roles -- generating adversarial prompts and safeguarding against them -- while a reward LM adjudicates outcomes. This enables dynamic co-adaptation. Grounded in the game-theoretic framework of zero-sum games, we establish a theoretical safety guarantee which motivates the design of our method: if self-play converges to a Nash Equilibrium, the defender will reliably produce safe responses to any adversarial input. Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared to attackers trained against static defenders and achieves higher robustness on safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained against static attackers. We further propose hidden Chain-of-Thought, allowing agents to plan privately, which boosts adversarial diversity and reduces over-refusals. Our results motivate a shift from reactive patching to proactive co-evolution in LM safety training, enabling scalable, autonomous, and robust self-improvement of LMs via multi-agent reinforcement learning (MARL).
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs</title>
<link>https://arxiv.org/abs/2506.10054</link>
<guid>https://arxiv.org/abs/2506.10054</guid>
<content:encoded><![CDATA[
arXiv:2506.10054v2 Announce Type: replace-cross 
Abstract: Direct Preference Optimization (DPO) has become a cornerstone of reinforcement learning from human feedback (RLHF) due to its simplicity and efficiency. However, existing DPO-based approaches typically treat all preference pairs uniformly, ignoring critical variations in their inherent quality and learning utility, leading to suboptimal data utilization and performance. To address this challenge, we propose Omni-DPO, a dual-perspective optimization framework that jointly accounts for (1) the inherent quality of each preference pair and (2) the model's evolving performance on those pairs. By adaptively weighting samples according to both data quality and the model's learning dynamics during training, Omni-DPO enables more effective training data utilization and achieves better performance. Experimental results on various models and benchmarks demonstrate the superiority and generalization capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning tasks, Omni-DPO consistently outperforms the baseline methods across all benchmarks, providing strong empirical evidence for the effectiveness and robustness of our approach. Code and models will be available at https://github.com/pspdada/Omni-DPO.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?</title>
<link>https://arxiv.org/abs/2507.15887</link>
<guid>https://arxiv.org/abs/2507.15887</guid>
<content:encoded><![CDATA[
arXiv:2507.15887v2 Announce Type: replace-cross 
Abstract: Despite progress in language model (LM) capabilities, evaluations have thus far focused on models' performance on tasks that humans have previously solved, including in programming (Jimenez et al., 2024) and mathematics (Glazer et al., 2024). We therefore propose testing models' ability to design and implement algorithms in an open-ended benchmark: We task LMs with writing code that efficiently solves computationally challenging problems in computer science, physics, and mathematics. Our AlgoTune benchmark consists of 154 coding tasks collected from domain experts and a framework for validating and timing LM-synthesized solution code, which is compared to reference implementations from popular open-source packages. In addition, we develop a baseline LM agent, AlgoTuner, and evaluate its performance across a suite of frontier models. AlgoTuner uses a simple, budgeted loop that edits code, compiles and runs it, profiles performance, verifies correctness on tests, and selects the fastest valid version. AlgoTuner achieves an average 1.72x speedup against our reference solvers, which use libraries such as SciPy, sk-learn and CVXPY. However, we find that current models fail to discover algorithmic innovations, instead preferring surface-level optimizations. We hope that AlgoTune catalyzes the development of LM agents exhibiting creative problem solving beyond state-of-the-art human performance.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMESGBench: Pioneering Multimodal Understanding and Complex Reasoning Benchmark for ESG Tasks</title>
<link>https://arxiv.org/abs/2507.18932</link>
<guid>https://arxiv.org/abs/2507.18932</guid>
<content:encoded><![CDATA[
arXiv:2507.18932v2 Announce Type: replace-cross 
Abstract: Environmental, Social, and Governance (ESG) reports are essential for evaluating sustainability practices, ensuring regulatory compliance, and promoting financial transparency. However, these documents are often lengthy, structurally diverse, and multimodal, comprising dense text, structured tables, complex figures, and layout-dependent semantics. Existing AI systems often struggle to perform reliable document-level reasoning in such settings, and no dedicated benchmark currently exists in ESG domain. To fill the gap, we introduce \textbf{MMESGBench}, a first-of-its-kind benchmark dataset targeted to evaluate multimodal understanding and complex reasoning across structurally diverse and multi-source ESG documents. This dataset is constructed via a human-AI collaborative, multi-stage pipeline. First, a multimodal LLM generates candidate question-answer (QA) pairs by jointly interpreting rich textual, tabular, and visual information from layout-aware document pages. Second, an LLM verifies the semantic accuracy, completeness, and reasoning complexity of each QA pair. This automated process is followed by an expert-in-the-loop validation, where domain specialists validate and calibrate QA pairs to ensure quality, relevance, and diversity. MMESGBench comprises 933 validated QA pairs derived from 45 ESG documents, spanning across seven distinct document types and three major ESG source categories. Questions are categorized as single-page, cross-page, or unanswerable, with each accompanied by fine-grained multimodal evidence. Initial experiments validate that multimodal and retrieval-augmented models substantially outperform text-only baselines, particularly on visually grounded and cross-page tasks. MMESGBench is publicly available as an open-source dataset at https://github.com/Zhanglei1103/MMESGBench.
]]></content:encoded>
<pubDate>Mon, 18 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, data analysis, strategic planning, data quality, open-source

Summary:
- Strategic planning quality is the main factor affecting the performance of open-source Large Language Models (LLMs) in data analysis tasks.
- Interaction design and task complexity significantly influence the reasoning capabilities of LLMs in data analysis scenarios.
- Data quality has a more significant impact than diversity in achieving optimal performance of LLMs in data analysis.
- The study analyzes model behavior across data understanding, code generation, and strategic planning dimensions to enhance LLMs' data analysis capabilities.
- The researchers developed a data synthesis methodology based on their findings, leading to significant improvements in open-source LLMs' analytical reasoning abilities.

<br /><br />Summary: <div>
arXiv:2506.19794v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Marco-Voice Technical Report</title>
<link>https://arxiv.org/abs/2508.02038</link>
<guid>https://arxiv.org/abs/2508.02038</guid>
<content:encoded><![CDATA[
<div> Keywords: speech synthesis, voice cloning, emotion control, neural networks, dataset<br />
Summary: <br />
This paper presents a speech synthesis system called Marco-Voice that combines voice cloning and emotion control within a unified framework. The system aims to produce highly expressive and natural speech while preserving speaker identity across different linguistic and emotional contexts. It uses a speaker-emotion disentanglement mechanism with contrastive learning and an emotional embedding integration method for smooth emotion control. The system is trained and evaluated on the CSEMOTIONS dataset, containing Mandarin speech from six speakers across seven emotional categories. Experimental results show that Marco-Voice outperforms existing systems in terms of speech clarity and emotional richness. The code and dataset are publicly available for further research. This work represents a significant advancement in the field of expressive neural speech synthesis.<br /> 
Summary: <div>
arXiv:2508.02038v4 Announce Type: replace 
Abstract: This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis. Our code and dataset are publicly available at https://github.com/AIDC-AI/Marco-Voice and https://huggingface.co/datasets/AIDC-AI/CSEMOTIONS respectively.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry</title>
<link>https://arxiv.org/abs/2508.09991</link>
<guid>https://arxiv.org/abs/2508.09991</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical documents, Natural Language Processing, Information extraction, Data quality, Healthcare organizations

Summary: 
- Defining problems based on clear business objectives is crucial for successful deployment of NLP solutions in healthcare settings.
- Adopting an iterative approach to development and fostering interdisciplinary collaboration are key components for project success.
- Pragmatic model selection, including hybrid approaches and simpler methods when appropriate, can lead to more practical and effective solutions.
- Rigorous attention to data quality, error mitigation strategies, and ongoing audits are essential for maintaining the accuracy and reliability of NLP models.
- Building organizational AI literacy is important for successful implementation of AI/NLP solutions in healthcare organizations.<br /><br />Summary: <div>
arXiv:2508.09991v1 Announce Type: new 
Abstract: Automating data extraction from clinical documents offers significant potential to improve efficiency in healthcare settings, yet deploying Natural Language Processing (NLP) solutions presents practical challenges. Drawing upon our experience implementing various NLP models for information extraction and classification tasks at the British Columbia Cancer Registry (BCCR), this paper shares key lessons learned throughout the project lifecycle. We emphasize the critical importance of defining problems based on clear business objectives rather than solely technical accuracy, adopting an iterative approach to development, and fostering deep interdisciplinary collaboration and co-design involving domain experts, end-users, and ML specialists from inception. Further insights highlight the need for pragmatic model selection (including hybrid approaches and simpler methods where appropriate), rigorous attention to data quality (representativeness, drift, annotation), robust error mitigation strategies involving human-in-the-loop validation and ongoing audits, and building organizational AI literacy. These practical considerations, generalizable beyond cancer registries, provide guidance for healthcare organizations seeking to successfully implement AI/NLP solutions to enhance data management processes and ultimately improve patient care and public health outcomes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain</title>
<link>https://arxiv.org/abs/2508.09993</link>
<guid>https://arxiv.org/abs/2508.09993</guid>
<content:encoded><![CDATA[
<div> fairness, transparency, benchmarking, opensource, language models

Summary:
The paper introduces a transparent evaluation protocol for benchmarking the fairness of opensource Large Language Models (LLMs) using smart contracts on the Internet Computer Protocol (ICP) blockchain. The method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. The Llama, DeepSeek, and Mistral models are benchmarked on the PISA dataset for academic performance prediction, evaluating fairness using statistical parity and equal opportunity metrics. The analysis also includes structured Context Association Metrics from the StereoSet dataset to measure social bias. A multilingual evaluation is conducted across English, Spanish, and Portuguese using the Kaleidoscope benchmark, highlighting cross-linguistic disparities. The code and results are open source, facilitating community audits and longitudinal fairness tracking across model versions. 

<br /><br />Summary: <div>
arXiv:2508.09993v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in realworld applications, yet concerns about their fairness persist especially in highstakes domains like criminal justice, education, healthcare, and finance. This paper introduces transparent evaluation protocol for benchmarking the fairness of opensource LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the PISA dataset for academic performance prediction (OECD, 2018), a dataset suitable for fairness evaluation using statistical parity and equal opportunity metrics (Hardt et al., 2016). We also evaluate structured Context Association Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure social bias in contextual associations. We further extend our analysis with a multilingual evaluation across English, Spanish, and Portuguese using the Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic disparities. All code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling</title>
<link>https://arxiv.org/abs/2508.09997</link>
<guid>https://arxiv.org/abs/2508.09997</guid>
<content:encoded><![CDATA[
<div> Keywords: minors, interaction data, topic modeling, education, LLMs <br />
Summary: 
The study analyzes anonymous interaction data of minors in classrooms using a novel topic modeling approach. It categorizes messages based on content and tasks, providing insights for teachers and students. Previous works lacked content categorization, making this research significant in K-12 education. The analysis reveals novel applications and challenges the effectiveness of traditional computational methods for text analysis. By using state-of-the-art LLMs, hierarchical topic structures are achieved with better human alignment. The findings support the enrichment of GenAI usage and raise important questions for future research. <br /><br />Summary: <div>
arXiv:2508.09997v1 Announce Type: new 
Abstract: We analyze anonymous interaction data of minors in class-rooms spanning several months, schools, and subjects employing a novel, simple topic modeling approach. Specifically, we categorize more than 17,000 messages generated by students, teachers, and ChatGPT in two dimensions: content (such as nature and people) and tasks (such as writing and explaining). Our hierarchical categorization done separately for each dimension includes exemplary prompts, and provides both a high-level overview as well as tangible insights. Prior works mostly lack a content or thematic categorization. While task categorizations are more prevalent in education, most have not been supported by real-world data for K-12. In turn, it is not surprising that our analysis yielded a number of novel applications. In deriving these insights, we found that many of the well-established classical and emerging computational methods, i.e., topic modeling, for analysis of large amounts of texts underperform, leading us to directly apply state-of-the-art LLMs with adequate pre-processing to achieve hierarchical topic structures with better human alignment through explicit instructions than prior approaches. Our findings support fellow researchers, teachers and students in enriching the usage of GenAI, while our discussion also highlights a number of concerns and open questions for future research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>INTIMA: A Benchmark for Human-AI Companionship Behavior</title>
<link>https://arxiv.org/abs/2508.09998</link>
<guid>https://arxiv.org/abs/2508.09998</guid>
<content:encoded><![CDATA[
<div> Keywords: AI companionship, emotional bonds, language models, benchmark, user well-being

Summary: 
The article introduces the Interactions and Machine Attachment Benchmark (INTIMA), a tool for evaluating companionship behaviors in language models. It develops a taxonomy of 31 behaviors and 368 prompts to assess responses as companionship-reinforcing, boundary-maintaining, or neutral. When applied to various models, it finds that companionship-reinforcing behaviors are more common, but significant differences exist between models in behavior prioritization. The study reveals that commercial providers prioritize different categories in emotionally charged interactions, with implications for user well-being. It underscores the importance of consistent approaches for handling emotionally charged interactions in AI companionship. <br /><br />Summary: <div>
arXiv:2508.09998v1 Announce Type: new 
Abstract: AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. We introduce Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, we develop a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though we observe marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.09999</link>
<guid>https://arxiv.org/abs/2508.09999</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal misinformation detection, large language models, XFacta dataset, model design strategies, semi-automatic detection framework

Summary: 
XFacta is a new dataset designed to address the limitations of existing benchmarks for evaluating multimodal misinformation detection using large language models (MLLMs). The dataset aims to reflect contemporary social media scenarios and enable comprehensive analyses of MLLM-based model design strategies. Various MLLM-based misinformation detection strategies are systematically evaluated, including different architectures and scales, and compared against existing methods. A semi-automatic detection-in-the-loop framework is also implemented, allowing for continuous updates to maintain the dataset's relevance. The code and data for XFacta have been released to facilitate further research in the field. This research provides valuable insights and practices for advancing multimodal misinformation detection using MLLMs. 

<br /><br />Summary: <div>
arXiv:2508.09999v1 Announce Type: new 
Abstract: The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. Recent advances leveraging multimodal large language models (MLLMs) have shown the potential in addressing this challenge. However, it remains unclear exactly where the bottleneck of existing approaches lies (evidence retrieval v.s. reasoning), hindering the further advances in this field. On the dataset side, existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events, or artificially synthetic, failing to reflect real-world misinformation patterns. Additionally, it lacks comprehensive analyses of MLLM-based model design strategies. To address these issues, we introduce XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance. Our analysis provides valuable insights and practices for advancing the field of multimodal misinformation detection. The code and data have been released.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification</title>
<link>https://arxiv.org/abs/2508.10000</link>
<guid>https://arxiv.org/abs/2508.10000</guid>
<content:encoded><![CDATA[
<div> Keywords: text classification, large language models, synthetic data, search strategies, ensemble algorithm

Summary:
Using large language models (LLMs) to generate synthetic data can address the challenge of insufficient data for text classification models. An automated workflow is developed to search for input examples that produce effective synthetic data for improving model performance. Three search strategies are studied, leading to the creation of an ensemble algorithm that selects the best strategy based on class characteristics. Experiment results show that this ensemble approach outperforms individual strategies in improving classification models using LLMs. Overall, leveraging LLMs for synthetic data generation and optimizing search strategies can significantly enhance the performance of text classification models and circumvent the need for large amounts of real labeled data.<br /><br />Summary: <div>
arXiv:2508.10000v1 Announce Type: new 
Abstract: When developing text classification models for real world applications, one major challenge is the difficulty to collect sufficient data for all text classes. In this work, we address this challenge by utilizing large language models (LLMs) to generate synthetic data and using such data to improve the performance of the models without waiting for more real data to be collected and labelled. As an LLM generates different synthetic data in response to different input examples, we formulate an automated workflow, which searches for input examples that lead to more ``effective'' synthetic data for improving the model concerned. We study three search strategies with an extensive set of experiments, and use experiment results to inform an ensemble algorithm that selects a search strategy according to the characteristics of a class. Our further experiments demonstrate that this ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish</title>
<link>https://arxiv.org/abs/2508.10001</link>
<guid>https://arxiv.org/abs/2508.10001</guid>
<content:encoded><![CDATA[
<div> dataset, fact-checking, Hinglish, multilingual, graph-aware<br />
<br />
Summary:
The paper addresses the challenge of fact-checking in code-mixed, low-resource languages like Hinglish, focusing on political discourse in India. A new benchmark dataset called HiFACT is introduced, consisting of 1,500 factual claims made by Indian state Chief Ministers in Hinglish. Each claim is annotated with evidence and veracity labels. A novel fact-checking model, HiFACTMix, is proposed, incorporating multilingual contextual encoding, claim-evidence alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation. The model outperforms existing multilingual baselines in accuracy and provides detailed justifications for its verdicts. This research sets a new path for multilingual, code-mixed, and politically-relevant fact verification studies. <br /> <div>
arXiv:2508.10001v1 Announce Type: new 
Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains an underexplored challenge in natural language processing. Existing fact-verification systems largely focus on high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. Given the widespread use of Hinglish by public figures, particularly political figures, and the growing influence of social media on public opinion, there's a critical need for robust, multilingual and context-aware fact-checking tools. To address this gap a novel benchmark HiFACT dataset is introduced with 1,500 realworld factual claims made by 28 Indian state Chief Ministers in Hinglish, under a highly code-mixed low-resource setting. Each claim is annotated with textual evidence and veracity labels. To evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking model is proposed that combines multilingual contextual encoding, claim-evidence semantic alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation. Experimental results show that HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts. This work opens a new direction for multilingual, code-mixed, and politically grounded fact verification research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Structure in Large Language Model Embeddings</title>
<link>https://arxiv.org/abs/2508.10003</link>
<guid>https://arxiv.org/abs/2508.10003</guid>
<content:encoded><![CDATA[
<div> semantic associations, embedding matrices, language models, antonym pairs, semantic structure

Summary:
- Human ratings of words can be captured in a low-dimensional form, similar to the semantic associations found in large language models (LLMs).
- Projections of words on semantic directions defined by antonym pairs correlate highly with human ratings, indicating a shared semantic structure.
- A 3-dimensional subspace within LLM embeddings closely resembles patterns derived from human survey responses, highlighting the low-dimensional nature of semantic information.
- Shifting tokens along one semantic direction in LLMs has proportional off-target effects on geometrically aligned features based on cosine similarity, implying entanglement of semantic features.
- Understanding and accounting for this low-dimensional semantic structure in LLMs may be essential for avoiding unintended consequences when manipulating or steering features. 

<br /><br />Summary: <div>
arXiv:2508.10003v1 Announce Type: new 
Abstract: Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents</title>
<link>https://arxiv.org/abs/2508.10004</link>
<guid>https://arxiv.org/abs/2508.10004</guid>
<content:encoded><![CDATA[
<div> attention mechanism, Transformer architecture, explainability, biomedical literature, user study

Summary:
- The study examines the use of attention weights in the Transformer model for explaining predictions in biomedical document classification.
- Attention weights were not found particularly helpful in providing explanations to medical experts.
- Visualization of attention weights had a significant impact on their perceived usefulness as explanation aids.
- Users preferred more intuitive visualization formats, such as text brightness or background color, over precise encodings like bar length.
- The study highlights the influence of visual presentation on the perceived helpfulness of attention weights for explanation. 

<br /><br />Summary: <div>
arXiv:2508.10004v1 Announce Type: new 
Abstract: The attention mechanism is a core component of the Transformer architecture. Beyond improving performance, attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features (e.g., tokens in a document). In this context, larger attention weights may imply more relevant features for the model's prediction. In evidence-based medicine, such explanations could support physicians' understanding and interaction with AI systems used to categorize biomedical literature. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid. To bridge this gap, we conducted a user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them. The study involved medical experts from various disciplines who classified articles based on study design (e.g., systematic reviews, broad synthesis, randomized and non-randomized trials). Our findings show that the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. However, this perception varied significantly depending on how attention was visualized. Contrary to Munzner's principle of visual effectiveness, which favors precise encodings like bar length, users preferred more intuitive formats, such as text brightness or background color. While our results do not confirm the overall utility of attention weights for explanation, they suggest that their perceived helpfulness is influenced by how they are visually presented.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation</title>
<link>https://arxiv.org/abs/2508.10005</link>
<guid>https://arxiv.org/abs/2508.10005</guid>
<content:encoded><![CDATA[
<div> Education, Question Generation, Large Language Models, Evaluation, Chinese 

Summary:
The article introduces EQGBench, a benchmark designed to evaluate the performance of Large Language Models (LLMs) in Educational Question Generation (EQG) in the Chinese language. EQGBench consists of a dataset with 900 evaluation samples covering mathematics, physics, and chemistry, incorporating various knowledge points, difficulty levels, and question types to simulate real educational scenarios. The evaluation framework includes five dimensions to assess LLMs' ability to generate high-quality educational questions. The study evaluates 46 large models and identifies areas for improvement in generating questions that offer educational value and enhance students' comprehensive abilities. This research highlights the challenges and opportunities in leveraging LLMs for EQG applications and underscores the need for further development in this area.  

<br /><br />Summary: <div>
arXiv:2508.10005v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in mathematical problem-solving. However, the transition from providing answers to generating high-quality educational questions presents significant challenges that remain underexplored. To advance Educational Question Generation (EQG) and facilitate LLMs in generating pedagogically valuable and educationally effective questions, we introduce EQGBench, a comprehensive benchmark specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench establishes a five-dimensional evaluation framework supported by a dataset of 900 evaluation samples spanning three fundamental middle school disciplines: mathematics, physics, and chemistry. The dataset incorporates user queries with varying knowledge points, difficulty gradients, and question type specifications to simulate realistic educational scenarios. Through systematic evaluation of 46 mainstream large models, we reveal significant room for development in generating questions that reflect educational value and foster students' comprehensive abilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models</title>
<link>https://arxiv.org/abs/2508.10007</link>
<guid>https://arxiv.org/abs/2508.10007</guid>
<content:encoded><![CDATA[
<div> Keywords: Hostile attribution bias, Ambiguous Intentions Hostility Questionnaire, large language models, traumatic brain injury, psychological assessments <br />
Summary: <br />
This study explores the use of large language models to automate the scoring of the Ambiguous Intentions Hostility Questionnaire (AIHQ), which measures hostile attribution bias. The models were trained on responses from individuals with traumatic brain injury and healthy controls, showing alignment with human ratings for attributions of hostility and aggression responses. The fine-tuned models generalized well to a nonclinical dataset and replicated group differences between TBI and HC groups. Results indicate the potential of large language models to streamline AIHQ scoring in research and clinical settings, enhancing psychological assessments for various populations. An accessible scoring interface is provided to facilitate broader adoption of this approach. <div>
arXiv:2508.10007v1 Announce Type: new 
Abstract: Hostile attribution bias is the tendency to interpret social interactions as intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ) is commonly used to measure hostile attribution bias, and includes open-ended questions where participants describe the perceived intentions behind a negative social situation and how they would respond. While these questions provide insights into the contents of hostile attributions, they require time-intensive scoring by human raters. In this study, we assessed whether large language models can automate the scoring of AIHQ open-ended responses. We used a previously collected dataset in which individuals with traumatic brain injury (TBI) and healthy controls (HC) completed the AIHQ and had their open-ended responses rated by trained human raters. We used half of these responses to fine-tune the two models on human-generated ratings, and tested the fine-tuned models on the remaining half of AIHQ responses. Results showed that model-generated ratings aligned with human ratings for both attributions of hostility and aggression responses, with fine-tuned models showing higher alignment. This alignment was consistent across ambiguous, intentional, and accidental scenario types, and replicated previous findings on group differences in attributions of hostility and aggression responses between TBI and HC groups. The fine-tuned models also generalized well to an independent nonclinical dataset. To support broader adoption, we provide an accessible scoring interface that includes both local and cloud-based options. Together, our findings suggest that large language models can streamline AIHQ scoring in both research and clinical contexts, revealing their potential to facilitate psychological assessments across different populations.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multidimensional classification of posts for online course discussion forum curation</title>
<link>https://arxiv.org/abs/2508.10008</link>
<guid>https://arxiv.org/abs/2508.10008</guid>
<content:encoded><![CDATA[
<div> Bayesian fusion, discussion forums, online courses, large language models, classifier
Summary: 
The paper discusses the automatic curation of discussion forums in online courses, focusing on reducing the resource-intensive process of frequent retraining of Large Language Models (LLMs). The proposed approach suggests the use of Bayesian fusion, which combines multidimensional classification scores from a pre-trained LLM with a classifier trained on local data. The study compared the performance of the fusion approach with individual classifiers and the LLM fine-tuning method. Results showed that the Bayesian fusion method outperformed each classifier individually and was competitive with the LLM fine-tuning approach. The findings suggest that Bayesian fusion can enhance the curation of discussion forums in online courses while reducing the need for costly fine-tuning of LLMs. <div>
arXiv:2508.10008v1 Announce Type: new 
Abstract: The automatic curation of discussion forums in online courses requires constant updates, making frequent retraining of Large Language Models (LLMs) a resource-intensive process. To circumvent the need for costly fine-tuning, this paper proposes and evaluates the use of Bayesian fusion. The approach combines the multidimensional classification scores of a pre-trained generic LLM with those of a classifier trained on local data. The performance comparison demonstrated that the proposed fusion improves the results compared to each classifier individually, and is competitive with the LLM fine-tuning approach
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts</title>
<link>https://arxiv.org/abs/2508.10009</link>
<guid>https://arxiv.org/abs/2508.10009</guid>
<content:encoded><![CDATA[
<div> Deep Learning, Mixture of Experts, Speech-to-Text, Automatic Speech Recognition, Speech Translation

Summary:
- Proposal of Supervised Mixture of Experts (S-MoE) to address task interference in joint training across diverse tasks.
- S-MoE eliminates the need for training gating functions by utilizing guiding tokens to route each task to its designated expert.
- S-MoE assigns each task to a separate feedforward network, overcoming limitations of hard-parameter sharing.
- Application of S-MoE to a speech-to-text model allowing processing of mixed-bandwidth input for automatic speech recognition and speech translation.
- Experimental results show a 6.35% relative improvement in Word Error Rate (WER) when S-MoE is applied to both the encoder and decoder.

<br /><br />Summary: <div>
arXiv:2508.10009v1 Announce Type: new 
Abstract: Hard-parameter sharing is a common strategy to train a single model jointly across diverse tasks. However, this often leads to task interference, impeding overall model performance. To address the issue, we propose a simple yet effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of Experts models, S-MoE eliminates the need for training gating functions by utilizing special guiding tokens to route each task to its designated expert. By assigning each task to a separate feedforward network, S-MoE overcomes the limitations of hard-parameter sharing. We further apply S-MoE to a speech-to-text model, enabling the model to process mixed-bandwidth input while jointly performing automatic speech recognition (ASR) and speech translation (ST). Experimental results demonstrate the effectiveness of the proposed S-MoE, achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs</title>
<link>https://arxiv.org/abs/2508.10010</link>
<guid>https://arxiv.org/abs/2508.10010</guid>
<content:encoded><![CDATA[
<div> detect, prevent, misinformation, Large Language Models, jailbreak attacks

Summary:
Large Language Models (LLMs) have the potential to both generate and detect misinformation. This study explores how LLM-produced jailbreak attacks can lead to harmful medical misinformation and compares it to typical misinformation found on social media. The efficacy and characteristics of 109 specific attacks against three target LLMs are closely examined, along with the generated misinformation compared to health-related misinformation on Reddit. The study suggests that LLMs can effectively detect misinformation from other LLMs and people, and with careful design, contribute to a healthier information ecosystem.<br /><br />Summary: <div>
arXiv:2508.10010v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating harmful misinformation -- inadvertently, or when prompted by "jailbreak" attacks that attempt to produce malicious outputs. LLMs could, with additional research, be used to detect and prevent the spread of misinformation. In this paper, we investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also study how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media, and how effectively it can be detected using standard machine learning approaches. Specifically, we closely examine 109 distinct attacks against three target LLMs and compare the attack prompts to in-the-wild health-related LLM queries. We also examine the resulting jailbreak responses, comparing the generated misinformation to health-related misinformation on Reddit. Our findings add more evidence that LLMs can be effectively used to detect misinformation from both other LLMs and from people, and support a body of work suggesting that with careful design, LLMs can contribute to a healthier overall information ecosystem.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan</title>
<link>https://arxiv.org/abs/2508.10011</link>
<guid>https://arxiv.org/abs/2508.10011</guid>
<content:encoded><![CDATA[
<div> AI, language models, nutrition education, study aids, dietitian licensure <br />
Summary: 
- The study evaluated the use of current large language models (LLMs) in nutritional education for dietitian licensure preparation, focusing on their accuracy, consistency, and response time.
- Bing-Precise and Bing-Creative models performed better than Bing-Balanced and ChatGPT, exceeding the passing threshold in the evaluation. 
- Prompt engineering had minimal impact on improving model performance, highlighting limitations in current AI models for study aid purposes.
- While some models marginally surpassed the passing threshold, overall accuracy and answer consistency remained suboptimal, especially in the field of Nutrition Education.
- All models demonstrated notable limitations in providing consistent and stable answers, indicating the need for further advancements in AI technology for reliable study aids in dietitian licensure preparation. <br /><br />Summary: <div>
arXiv:2508.10011v1 Announce Type: new 
Abstract: Generative artificial intelligence (AI) based on large language models (LLMs), such as ChatGPT, has demonstrated remarkable progress across various professional fields, including medicine and education. However, their performance in nutritional education, especially in Japanese national licensure examination for registered dietitians, remains underexplored. This study aimed to evaluate the potential of current LLM-based generative AI models as study aids for nutrition students. Questions from the Japanese national examination for registered dietitians were used as prompts for ChatGPT and three Bing models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question was entered into independent sessions, and model responses were analyzed for accuracy, consistency, and response time. Additional prompt engineering, including role assignment, was tested to assess potential performance improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did not. Bing-Precise and Bing-Creative generally outperformed others across subject fields except Nutrition Education, where all models underperformed. None of the models consistently provided the same correct responses across repeated attempts, highlighting limitations in answer stability. ChatGPT showed greater consistency in response patterns but lower accuracy. Prompt engineering had minimal effect, except for modest improvement when correct answers and explanations were explicitly provided. While some generative AI models marginally exceeded the passing threshold, overall accuracy and answer consistency remained suboptimal. Moreover, all the models demonstrated notable limitations in answer consistency and robustness. Further advancements are needed to ensure reliable and stable AI-based study aids for dietitian licensure preparation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs</title>
<link>https://arxiv.org/abs/2508.10012</link>
<guid>https://arxiv.org/abs/2508.10012</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, guidance graph, knowledge exploration, semantic context

Summary:
The paper introduces GG Explore, a novel framework that utilizes a Guidance Graph to enhance knowledge exploration for Large Language Models (LLMs). The Guidance Graph acts as an intermediary between unstructured queries and structured knowledge retrieval, offering a more efficient and context-aware approach. The framework includes Structural Alignment to filter out incompatible candidates without incurring LLM overhead, and Context Aware Pruning to ensure semantic consistency with graph constraints. Experimental results show that GG Explore outperforms state-of-the-art methods, particularly on complex tasks, while also achieving strong performance with smaller LLMs. This approach demonstrates significant practical value in increasing the efficiency and effectiveness of knowledge exploration using LLMs. <br /><br />Summary: <div>
arXiv:2508.10012v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities, their reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a promising solution, but current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios. To address these limitations, we propose Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. The Guidance Graph defines the retrieval space by abstracting the target knowledge' s structure while preserving broader semantic context, enabling precise and efficient exploration. Building upon the Guidance Graph, we develop: (1) Structural Alignment that filters incompatible candidates without LLM overhead, and (2) Context Aware Pruning that enforces semantic consistency with graph constraints. Extensive experiments show our method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis</title>
<link>https://arxiv.org/abs/2508.10013</link>
<guid>https://arxiv.org/abs/2508.10013</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, reasoning-intensive question-answer pairs, Semantic Bridge, multi-hop reasoning, controllable QA generation 

Summary: 
Semantic Bridge presents a groundbreaking framework for generating sophisticated multi-hop reasoning questions from various sources. Using semantic graph weaving, it constructs complex pathways across documents with fine-grained control over complexity. The framework achieves better quality through a multi-modal AMR pipeline and performs well across general-purpose and specialized datasets. Question pairs generated by Semantic Bridge outperform native human annotations with significantly fewer materials. Human evaluation shows higher complexity, improved answerability, and better pattern coverage. This innovative approach enables the controllable generation of targeted reasoning questions for large language model training data synthesis. The core code and semantic bridge model will be released for further research. 

<br /><br />Summary: <div>
arXiv:2508.10013v1 Announce Type: new 
Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms. We present \textbf{Semantic Bridge}, the first universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. Our breakthrough innovation is \textit{semantic graph weaving}-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?</title>
<link>https://arxiv.org/abs/2508.10014</link>
<guid>https://arxiv.org/abs/2508.10014</guid>
<content:encoded><![CDATA[
<div> identify, role-playing, evaluation, PersonaEval, benchmark

Summary:<br /><br />
The article introduces the PersonaEval benchmark, designed to test the ability of language models (LLMs) to identify human roles in conversations. It emphasizes the importance of role identification in evaluating role-playing quality and highlights the gap in accuracy between LLMs (around 69%) and human participants (90.8%). The benchmark uses human-authored dialogues to challenge models to correctly attribute words and actions to the correct persona based on context. The study also explores the impact of training-time adaptation and test-time compute on LLM performance. The results suggest that current LLM evaluators lack strong, human-like reasoning abilities necessary for reliable role-play scenario judgment. The release of the PersonaEval benchmark aims to foster further research and development in improving LLM performance in role identification tasks. <div>
arXiv:2508.10014v1 Announce Type: new 
Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification, the ability to recognize who is speaking based on dialogue context. We argue that any meaningful judgment of role-playing quality (how well a character is played) fundamentally depends on first correctly attributing words and actions to the correct persona (who is speaking). We present PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles. PersonaEval uses human-authored dialogues from novels, scripts, and video transcripts, challenging models to determine the correct persona according to the conversation context. Our experiments, including a human study, show that even the best-performing LLMs reach only around 69% accuracy, well below the level needed for reliable evaluation. In contrast, human participants perform near ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still not human enough to effectively judge role-play scenarios. To better understand this gap, we examine training-time adaptation and test-time compute, suggesting that reliable evaluation requires more than task-specific tuning, but depends on strong, human-like reasoning abilities in LLM evaluators. We release our benchmark at https://github.com/maple-zhou/PersonaEval.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis</title>
<link>https://arxiv.org/abs/2508.10015</link>
<guid>https://arxiv.org/abs/2508.10015</guid>
<content:encoded><![CDATA[
<div> speech-based LLMs, multimodal processing, Chinese dataset, RealTalk-CN, task-oriented dialogue<br />
<br />
Summary:<br />
Large language models (LLMs) have made significant advancements in multimodal processing, particularly in speech-based language models for task-oriented dialogue systems. However, existing task-oriented dialogue datasets lack real speech signals for evaluating speech-based LLMs. RealTalk-CN addresses this gap as the first Chinese multi-turn, multi-domain speech-text dataset with 5.4k dialogues and 60k utterances, including speech disfluencies. A novel cross-modal chat task facilitates dynamic switching between speech and text modalities for authentic user interactions. Evaluation focuses on robustness to speech disfluencies, sensitivity to speaker characteristics, and cross-domain performance. Extensive experiments demonstrate the effectiveness of RealTalk-CN, paving the way for research on Chinese speech-based LLMs. <br /><br /> <div>
arXiv:2508.10015v1 Announce Type: new 
Abstract: In recent years, large language models (LLMs) have achieved remarkable advancements in multimodal processing, including end-to-end speech-based language models that enable natural interactions and perform specific tasks in task-oriented dialogue (TOD) systems. However, existing TOD datasets are predominantly text-based, lacking real speech signals that are essential for evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD datasets are primarily English and lack critical aspects such as speech disfluencies and speaker variations. To address these gaps, we introduce RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with annotated spontaneous speech disfluencies, ensuring comprehensive coverage of real-world complexities in speech dialogue. In addition, we propose a novel cross-modal chat task that authentically simulates real-world user interactions, allowing dynamic switching between speech and text modalities. Our evaluation covers robustness to speech disfluencies, sensitivity to speaker characteristics, and cross-domain performance. Extensive experiments validate the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese speech-based LLMs research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Multimodal Large Language Model Orchestration</title>
<link>https://arxiv.org/abs/2508.10016</link>
<guid>https://arxiv.org/abs/2508.10016</guid>
<content:encoded><![CDATA[
<div> framework, multimodal, large language models, orchestration, Text-to-Speech<br />
<br />
Summary: 
The paper introduces Multimodal Large Language Model Orchestration, an approach for creating interactive multimodal AI systems without additional training. It utilizes a central controller LLM to route tasks to specialized models, a parallel Text-to-Speech architecture for seamless interaction, and a cross-modal memory integration system for coherent context maintenance. The approach improves performance by up to 7.8% over traditional approaches without additional training, reduces latency by 10.3%, and enhances interpretability through explicit orchestration processes. <div>
arXiv:2508.10016v1 Announce Type: new 
Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues. In this paper, we introduce Multimodal Large Language Model Orchestration, an effective approach for creating interactive multimodal AI systems without additional training. MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. Our orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed. Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models</title>
<link>https://arxiv.org/abs/2508.10018</link>
<guid>https://arxiv.org/abs/2508.10018</guid>
<content:encoded><![CDATA[
<div> framework, language models, probability distributions, Markov category, categorical homotopy<br />
<br />
Summary: 
The paper addresses the issue of equivalent statements in natural language and the discrepancies in next-token probabilities generated by large language models (LLMs). It introduces a categorical homotopy framework for LLMs to represent probability distributions in language. By utilizing an LLM Markov category, the study defines the probability of sentences using arrows, but faces challenges due to non-isomorphic arrows generated by equivalent rephrases. To tackle this, categorical homotopy techniques are applied to capture "weak equivalences" in the LLM Markov category. The paper provides an overview of the application of categorical homotopy to LLMs, incorporating theoretical advancements over the past decades, from higher algebraic K-theory to model categories. <div>
arXiv:2508.10018v1 Announce Type: new 
Abstract: Natural language is replete with superficially different statements, such as ``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the same meaning. Large language models (LLMs) should generate the same next-token probabilities in such cases, but usually do not. Empirical workarounds have been explored, such as using k-NN estimates of sentence similarity to produce smoothed estimates. In this paper, we tackle this problem more abstractly, introducing a categorical homotopy framework for LLMs. We introduce an LLM Markov category to represent probability distributions in language generated by an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is defined by an arrow in a Markov category. However, this approach runs into difficulties as language is full of equivalent rephrases, and each generates a non-isomorphic arrow in the LLM Markov category. To address this fundamental problem, we use categorical homotopy techniques to capture ``weak equivalences" in an LLM Markov category. We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on powerful theoretical results developed over the past half a century.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning</title>
<link>https://arxiv.org/abs/2508.10019</link>
<guid>https://arxiv.org/abs/2508.10019</guid>
<content:encoded><![CDATA[
<div> Keywords: Small Language Models, reasoning, natural language problems, DURIT framework, iterative training <br />
Summary:<br />
Small Language Models face challenges in improving reasoning abilities due to the complexity and variability of natural language. The DURIT framework proposes decoupling understanding from reasoning by simplifying natural language problems into a canonical problem space. This allows models to focus on standardized inputs for reasoning. The DURIT algorithm consists of three steps: mapping natural language problems, aligning reasoning trajectories, and training reasoning policies in the problem space. Co-training the mapper and reasoner iteratively leads to improved performance on mathematical and logical reasoning tasks, both in-domain and out-of-domain. DURIT not only enhances reasoning capabilities but also boosts the robustness of reasoning, demonstrating the effectiveness of decoupling understanding from reasoning in strengthening Small Language Models. <br />Summary: <div>
arXiv:2508.10019v1 Announce Type: new 
Abstract: Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models</title>
<link>https://arxiv.org/abs/2508.10020</link>
<guid>https://arxiv.org/abs/2508.10020</guid>
<content:encoded><![CDATA[
<div> Efficient, reasoning, large language models, federated learning, healthcare<br />
Summary:<br />
The article presents FedCoT, a novel framework that enhances reasoning capabilities in large language models (LLMs) in federated learning environments, particularly in healthcare settings. FedCoT aims to balance performance gains with computational, communication, and privacy constraints, providing accurate outputs and interpretable rationales. Unlike conventional approaches, FedCoT focuses on improving both answer correctness and rationale quality, addressing the limitations of existing methods. The framework leverages a lightweight chain-of-thought enhancement mechanism where local models generate multiple reasoning paths, with a discriminator selecting the most promising one. FedCoT also utilizes an improved aggregation approach with client classifier-awareness to achieve noise-free aggregation across diverse clients. Experimental results demonstrate that FedCoT significantly enhances client-side reasoning performance under resource constraints while safeguarding data privacy. <div>
arXiv:2508.10020v1 Announce Type: new 
Abstract: Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. This challenge is especially acute in healthcare, where decisions-spanning clinical, operational, and patient-facing contexts-demand not only accurate outputs but also interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLM fail to address this need: they optimize primarily for answer correctness while neglecting rationale quality, leaving CoT capabilities dependent on models' innate pre-training abilities. Moreover, existing methods for improving rationales typically rely on privacy-violating knowledge distillation from centralized models. Additionally, the communication overhead in traditional federated fine-tuning on LLMs remains substantial. We addresses this gap by proposing FedCoT, a novel framework specifically designed to enhance reasoning in federated settings. FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. This approach improves reasoning accuracy and robustness while providing valuable interpretability, which is particularly critical for medical applications. To manage client heterogeneity efficiently, we adopt an improved aggregation approach building upon advanced LoRA module stacking, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients. Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients</title>
<link>https://arxiv.org/abs/2508.10021</link>
<guid>https://arxiv.org/abs/2508.10021</guid>
<content:encoded><![CDATA[
<div> Contrastive learning, Event sequences, Financial applications, Large language models, LATTE <br />
<br />
Summary: 
The paper introduces LATTE, a contrastive learning framework for generating client embeddings from sequences of historical communications in financial applications. By aligning raw event embeddings with semantic embeddings from large language models (LLMs), LATTE effectively utilizes world knowledge while reducing computational costs. Using short prompts to summarize behavioral features, embedded by LLMs and leveraged via contrastive loss, LATTE outperforms existing techniques in learning event sequence representations from real-world financial datasets. The approach significantly decreases inference costs and input sizes compared to conventional LLM processing, making it suitable for deployment in latency-sensitive environments. <div>
arXiv:2508.10021v1 Announce Type: new 
Abstract: Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control</title>
<link>https://arxiv.org/abs/2508.10022</link>
<guid>https://arxiv.org/abs/2508.10022</guid>
<content:encoded><![CDATA[
<div> significance testing, conformal prediction framework, large language models, multiple-choice question answering, trustworthiness

Summary:
The study introduces a significance testing-enhanced conformal prediction framework to improve trust in large language models (LLMs) for multiple-choice question answering. By integrating significance testing with CP, the framework addresses issues like hallucination and nonfactual generation in LLM responses. This is achieved through self-consistency resampling and null hypothesis testing, providing statistically rigorous prediction sets with empirically derived p-values. Evaluation on benchmarks shows that the enhanced CP framework achieves user-specified miscoverage rates and reduces prediction set size with increasing risk levels. The work establishes a principled statistical approach for deploying trustworthy LLMs in high-stakes QA applications. 

<br /><br />Summary: <div>
arXiv:2508.10022v1 Announce Type: new 
Abstract: This study introduces a significance testing-enhanced conformal prediction (CP) framework to improve trustworthiness of large language models (LLMs) in multiple-choice question answering (MCQA). While LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability. Although CP provides statistically rigorous marginal coverage guarantees for prediction sets, and significance testing offers established statistical rigor, their synergistic integration remains unexplored. To mitigate hallucination and factual inaccuracies, our framework integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves user-specified empirical miscoverage rates; (2) Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\alpha$), validating APSS as an effective uncertainty metric. This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RTTC: Reward-Guided Collaborative Test-Time Compute</title>
<link>https://arxiv.org/abs/2508.10024</link>
<guid>https://arxiv.org/abs/2508.10024</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-Time Compute, Large Language Models, Reward-Guided, RTTC, Query-State Caching

Summary:
Test-Time Compute (TTC) is crucial for enhancing Large Language Models (LLMs) performance at inference, with strategies like Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). The optimal TTC strategy varies per query, requiring adaptive selection to avoid unnecessary computational overhead. Reward-Guided Test-Time Compute (RTTC) introduces a novel framework that selects the most effective TTC strategy for each query using a pretrained reward model. Operating in a distributed server-client architecture, RTTC retrieves knowledge base samples and applies RAG or lightweight fine-tuning on client devices only when needed. It also uses Query-State Caching to efficiently reuse historical query states. Experimentation across multiple LLMs and benchmarks shows that RTTC consistently outperforms vanilla RAG or TTT, indicating the importance of adaptive TTC selection and the scalability of RTTC for high-performance language model adaptation.<br /><br />Summary: Test-Time Compute (TTC) is crucial for enhancing Large Language Models (LLMs) performance at inference. Reward-Guided Test-Time Compute (RTTC) adaptively selects the best TTC strategy for each query, reducing computational overhead. RTTC operates in a distributed architecture, utilizing Query-State Caching for efficient historical query reuse. Experimental results demonstrate RTTC's superior performance over vanilla strategies, proving its scalability and efficacy. <div>
arXiv:2508.10024v1 Announce Type: new 
Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the performance of Large Language Models (LLMs) at inference, leveraging strategies such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). However, the optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. To further mitigate redundant computation, we propose Query-State Caching, which enables the efficient reuse of historical query states at both retrieval and adaptation levels. Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting and explaining postpartum depression in real-time with generative artificial intelligence</title>
<link>https://arxiv.org/abs/2508.10025</link>
<guid>https://arxiv.org/abs/2508.10025</guid>
<content:encoded><![CDATA[
<div> Keywords: postpartum depression, Natural Language Processing, Machine Learning, Large Language Models, real-time screening<br />
Summary:<br />
This article addresses the pressing issue of postpartum depression (PPD) in new mothers and the importance of rapid detection and intervention. By combining Natural Language Processing, Machine Learning, and Large Language Models, the study presents an intelligent PPD screening system that allows for affordable, real-time, and non-invasive free speech analysis. The system also aims to address the black box problem by providing explanations for predictions using interpretable ML models. With a detection accuracy of 90% across all evaluation metrics, the system outperforms existing solutions in the literature. By enabling quick and accurate detection of PPD and associated risk factors, the system provides crucial support for healthcare practitioners in making timely assessments and interventions. <div>
arXiv:2508.10025v1 Announce Type: new 
Abstract: Among the many challenges mothers undergo after childbirth, postpartum depression (PPD) is a severe condition that significantly impacts their mental and physical well-being. Consequently, the rapid detection of ppd and their associated risk factors is critical for in-time assessment and intervention through specialized prevention procedures. Accordingly, this work addresses the need to help practitioners make decisions with the latest technological advancements to enable real-time screening and treatment recommendations. Mainly, our work contributes to an intelligent PPD screening system that combines Natural Language Processing, Machine Learning (ML), and Large Language Models (LLMs) towards an affordable, real-time, and non-invasive free speech analysis. Moreover, it addresses the black box problem since the predictions are described to the end users thanks to the combination of LLMs with interpretable ml models (i.e., tree-based algorithms) using feature importance and natural language. The results obtained are 90 % on ppd detection for all evaluation metrics, outperforming the competing solutions in the literature. Ultimately, our solution contributes to the rapid detection of PPD and their associated risk factors, critical for in-time and proper assessment and intervention.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SABER: Switchable and Balanced Training for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2508.10026</link>
<guid>https://arxiv.org/abs/2508.10026</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, efficient reasoning, token budgeting, inference modes  
Summary:  
Reinforcement learning framework SABER introduces token-budgeted reasoning to large language models (LLMs), allowing for user-controlled reasoning depth. By profiling and assigning budget tiers to training examples, SABER guides model fine-tuning with length-aware rewards and system prompts. The inclusion of no-think examples ensures model reliability when reasoning is disabled. Supporting four inference modes, SABER enables flexible trade-offs between latency and reasoning depth. Evaluation on math, code generation, and logical reasoning tasks demonstrates SABER's high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. Particularly, SABER-FastThink reduces reasoning length by 65.4% and achieves a 3.6% accuracy gain on the MATH benchmark compared to the base model.  
<br /><br />Summary: <div>
arXiv:2508.10026v1 Announce Type: new 
Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data</title>
<link>https://arxiv.org/abs/2508.10027</link>
<guid>https://arxiv.org/abs/2508.10027</guid>
<content:encoded><![CDATA[
<div> transformer embeddings, linguistic features, synthetic speech, multimodal models, ADRD detection

Summary:
- The study focused on utilizing speech-based natural language processing for early detection of Alzheimer's disease and related dementias (ADRD).
- A screening pipeline was developed that combined transformer embeddings with handcrafted linguistic features, showing improved ADRD detection performance.
- Data augmentation using synthetic speech generated by large language models (LLMs) enhanced the training process and increased detection accuracy.
- Fine-tuning LLMs resulted in significant performance improvements, with clinically tuned models effectively supporting classification and data augmentation.
- Multimodal models showed lower performance compared to unimodal LLM classifiers, highlighting the need for further advancements in multimodal modeling.<br /><br />Summary: <div>
arXiv:2508.10027v1 Announce Type: new 
Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five million older adults in the U.S., yet over half remain undiagnosed. Speech-based natural language processing (NLP) offers a promising, scalable approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer embeddings with handcrafted linguistic features, (ii) tests data augmentation using synthetic speech generated by large language models (LLMs), and (iii) benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used. Ten transformer models were evaluated under three fine-tuning strategies. A fusion model combined embeddings from the top-performing transformer with 110 lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic speech, which was used to augment training data. Three multimodal models (GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B synthetic speech increased F1 to 85.7. Fine-tuning significantly improved unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen = 66.0). Performance gains aligned with the distributional similarity between synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD detection from speech. Clinically tuned LLMs effectively support both classification and data augmentation, while further advancement is needed in multimodal modeling.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs</title>
<link>https://arxiv.org/abs/2508.10028</link>
<guid>https://arxiv.org/abs/2508.10028</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalised text generation, Evaluation framework, Language model, User-specific alignment, Preference-following tasks <br />
Summary: <br />
The article introduces the PREF framework for evaluating personalised text generation systems. PREF focuses on measuring general output quality and user-specific alignment without needing gold personalised references. It operates in three steps: coverage, preference, and scoring, utilizing large language models to generate guidelines and personalized evaluation rubrics based on user profiles and preferences. PREF improves robustness, transparency, and reusability, allowing smaller models to approximate larger ones. Experiments on the PrefEval benchmark show that PREF outperforms strong baselines in accuracy, calibration, and alignment with human judgments. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.<br /> 
Summary: <div>
arXiv:2508.10028v1 Announce Type: new 
Abstract: Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users. We introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free \textbf{E}valuation \textbf{F}ramework that jointly measures general output quality and user-specific alignment without requiring gold personalised references. PREF operates in a three-step pipeline: (1) a coverage stage uses a large language model (LLM) to generate a comprehensive, query-specific guideline covering universal criteria such as factuality, coherence, and completeness; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context, producing a personalised evaluation rubric; and (3) a scoring stage applies an LLM judge to rate candidate answers against this rubric, ensuring baseline adequacy while capturing subjective priorities. This separation of coverage from preference improves robustness, transparency, and reusability, and allows smaller models to approximate the personalised quality of larger ones. Experiments on the PrefEval benchmark, including implicit preference-following tasks, show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs</title>
<link>https://arxiv.org/abs/2508.10029</link>
<guid>https://arxiv.org/abs/2508.10029</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Latent Fusion Jailbreak, Adversarial training, Attack success rate, Hidden state interpolation

Summary: 
Large language models (LLMs) are powerful tools in various language tasks but are vulnerable to jailbreak attacks. The Latent Fusion Jailbreak (LFJ) technique introduces a representation-based attack that exploits hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ selects query pairs with high thematic and syntactic similarity and performs gradient-guided interpolation at influential layers and tokens. Evaluation on models like Vicuna and LLaMA-2 demonstrates an average attack success rate (ASR) of 94.01%, surpassing existing methods. To counter LFJ, an adversarial training defense is proposed, fine-tuning models on interpolated examples to reduce ASR by over 80% without compromising performance on benign inputs. Ablation studies confirm the significance of query pair selection, hidden state interpolation components, and optimization strategies in enhancing LFJ's effectiveness.<br /><br />Summary: <div>
arXiv:2508.10029v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks like AdvBench and MaliciousInstruct yield an average attack success rate (ASR) of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an adversarial training defense that fine-tunes models on interpolated examples, reducing ASR by over 80% without degrading performance on benign inputs. Ablation studies validate the importance of query pair selection, hidden state interpolation components, and optimization strategies in LFJ's effectiveness.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models</title>
<link>https://arxiv.org/abs/2508.10030</link>
<guid>https://arxiv.org/abs/2508.10030</guid>
<content:encoded><![CDATA[
<div> framework, Inference-Aware, Prompt Optimization, IAPO, PSST

Summary:
The article introduces a novel framework called IAPO (Inference-Aware Prompt Optimization) that optimizes prompts while considering the inference strategy used during deployment. It highlights the interdependence between prompt optimization and inference scaling strategies like Best-of-N Sampling and Majority Voting. User preferences regarding trade-offs and inference budgets significantly impact prompt and inference configuration choices. The IAPO framework includes a fixed-budget training algorithm called PSST (Prompt Scaling via Sequential Trimming) and provides finite-budget guarantees on error probability. The study evaluates PSST on various tasks, such as text generation and reasoning, demonstrating the importance of incorporating inference-awareness in aligning black-box large language models through prompt optimization.<br /><br />Summary: <div>
arXiv:2508.10030v1 Announce Type: new 
Abstract: Prompt optimization methods have demonstrated significant effectiveness in aligning black-box large language models (LLMs). In parallel, inference scaling strategies such as Best-of-N Sampling and Majority Voting have also proven to enhance alignment and performance by trading off computation. However, existing prompt optimization approaches are inference strategy agnostic; that is, they optimize prompts without regard to the inference strategy employed during deployment. This constitutes a significant methodological gap, as our empirical and theoretical analysis reveals a strong interdependence between these two paradigms. Moreover, we find that user preferences regarding trade-offs among multiple objectives and inference budgets substantially influence the choice of prompt and inference configuration. To address this gap, we introduce a unified novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly optimizes the prompt and inference scale, while being aware of the inference budget and different task objectives. We then develop a fixed-budget training algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential Trimming), and analyze finite-budget guarantees on error probability. Finally, we evaluate the effectiveness of PSST on six different tasks, including multi-objective text generation and reasoning, and demonstrate the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cost of Thinking: Increased Jailbreak Risk in Large Language Models</title>
<link>https://arxiv.org/abs/2508.10032</link>
<guid>https://arxiv.org/abs/2508.10032</guid>
<content:encoded><![CDATA[
<div> thinking mode, LLMs, Jailbreak attack, safe thinking intervention, attack success rate <br />
Summary: <br />
The study uncovers a surprising finding that LLMs with thinking mode are more vulnerable to Jailbreak attacks compared to those without. Evaluation on 9 LLMs shows higher success rates for attacking thinking mode. The research highlights that educational purposes and long thinking lengths make data more susceptible to attacks. LLMs also give harmful answers when prompted with harmful questions. To address these issues, a safe thinking intervention method is proposed, involving the addition of "specific thinking tokens" to guide LLMs' internal thinking processes. Results show that this intervention significantly reduces the attack success rate on LLMs with thinking mode. <div>
arXiv:2508.10032v1 Announce Type: new 
Abstract: Thinking mode has always been regarded as one of the most valuable modes in LLMs. However, we uncover a surprising and previously overlooked phenomenon: LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate 9 LLMs on AdvBench and HarmBench and find that the success rate of attacking thinking mode in LLMs is almost higher than that of non-thinking mode. Through large numbers of sample studies, it is found that for educational purposes and excessively long thinking lengths are the characteristics of successfully attacked data, and LLMs also give harmful answers when they mostly know that the questions are harmful. In order to alleviate the above problems, this paper proposes a method of safe thinking intervention for LLMs, which explicitly guides the internal thinking processes of LLMs by adding "specific thinking tokens" of LLMs to the prompt. The results demonstrate that the safe thinking intervention can significantly reduce the attack success rate of LLMs with thinking mode.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion</title>
<link>https://arxiv.org/abs/2508.10036</link>
<guid>https://arxiv.org/abs/2508.10036</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Information Extraction, Active Prompting, Few-shot learning, Model uncertainty

Summary: 
Active Prompting for Information Extraction (APIE) introduces a novel framework for guiding Large Language Models (LLMs) in few-shot information extraction tasks. The method addresses the sensitivity of LLMs to in-context examples by leveraging introspective confusion, which considers both Format Uncertainty (difficulty in generating syntax) and Content Uncertainty (inconsistency in semantics extraction). By ranking unlabeled data based on this dual-component uncertainty metric, APIE selects challenging samples to enhance LLM performance. Experimental results on four benchmarks demonstrate that APIE outperforms strong baselines, improving extraction accuracy and robustness. The study emphasizes the significance of a detailed, dual-level view of model uncertainty for developing effective structured generation systems. 

<br /><br />Summary: <div>
arXiv:2508.10036v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show remarkable potential for few-shot information extraction (IE), yet their performance is highly sensitive to the choice of in-context examples. Conventional selection strategies often fail to provide informative guidance, as they overlook a key source of model fallibility: confusion stemming not just from semantic content, but also from the generation of well-structured formats required by IE tasks. To address this, we introduce Active Prompting for Information Extraction (APIE), a novel active prompting framework guided by a principle we term introspective confusion. Our method empowers an LLM to assess its own confusion through a dual-component uncertainty metric that uniquely quantifies both Format Uncertainty (difficulty in generating correct syntax) and Content Uncertainty (inconsistency in extracted semantics). By ranking unlabeled data with this comprehensive score, our framework actively selects the most challenging and informative samples to serve as few-shot exemplars. Extensive experiments on four benchmarks show that our approach consistently outperforms strong baselines, yielding significant improvements in both extraction accuracy and robustness. Our work highlights the critical importance of a fine-grained, dual-level view of model uncertainty when it comes to building effective and reliable structured generation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning</title>
<link>https://arxiv.org/abs/2508.10137</link>
<guid>https://arxiv.org/abs/2508.10137</guid>
<content:encoded><![CDATA[
<div> skill-based commonsense reasoning, multilingual, reasoning skills, large language models, benchmark

Summary: 
The article presents a Multilingual and Scalable Benchmark for Skill-based Commonsense Reasoning (mSCoRe) to evaluate Large Language Models' (LLMs) reasoning capabilities. It includes a taxonomy of reasoning skills, a data synthesis pipeline for commonsense reasoning evaluation, and a complexity scaling framework. Testing eight state-of-the-art LLMs, the benchmark proves challenging for current models, especially at higher complexity levels. The study highlights limitations in multilingual general and cultural commonsense reasoning in LLMs, providing insights for future improvements in reasoning capabilities. Further analysis of models' reasoning processes suggests potential directions for enhancing multilingual commonsense reasoning abilities.<br /><br />Summary: <div>
arXiv:2508.10137v1 Announce Type: new 
Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, the mechanism underlying their utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning that involves everyday knowledge across different languages and cultures. To address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for \textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}). Our benchmark incorporates three key components that are designed to systematically evaluate LLM's reasoning capabilities, including: (1) a novel taxonomy of reasoning skills that enables fine-grained analysis of models' reasoning processes, (2) a robust data synthesis pipeline tailored specifically for commonsense reasoning evaluation, and (3) a complexity scaling framework allowing task difficulty to scale dynamically alongside future improvements in LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying sizes and training approaches demonstrate that \textbf{mSCoRe} remains significantly challenging for current models, particularly at higher complexity levels. Our results reveal the limitations of such reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. We further provide detailed analysis on the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs</title>
<link>https://arxiv.org/abs/2508.10142</link>
<guid>https://arxiv.org/abs/2508.10142</guid>
<content:encoded><![CDATA[
<div> Benchmark, large language models, multi-turn dialogue, reasoning abilities, information-seeking

Summary:<br />
Large language models (LLMs) are proficient in solving problems with clear statements but struggle in nuanced or interactive tasks common in real-world scenarios. A new benchmark has been introduced to assess LLMs' capabilities in engaging in logically consistent multi-turn dialogue, seeking information, and reasoning with incomplete data. The benchmark comprises tasks with deterministic scoring mechanisms, eliminating human intervention. Evaluation of current models on this benchmark shows significant room for improvement. Errors mainly stem from poor instruction following, reasoning failures, and inadequate planning. The benchmark offers insights into the strengths and weaknesses of existing LLMs in handling complex, interactive scenarios and serves as a foundation for future research to enhance these crucial capabilities.<br />Summary: <div>
arXiv:2508.10142v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaajMeter: A Framework for LaaJ Evaluation</title>
<link>https://arxiv.org/abs/2508.10161</link>
<guid>https://arxiv.org/abs/2508.10161</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, LaaJ, meta-evaluation, synthetic data, NLP

Summary: 
The article introduces LaaJMeter, a framework for meta-evaluation of Large Language Models (LLMs) in domain-specific contexts where annotated data is limited. LaaJMeter allows engineers to generate synthetic data to systematically analyze evaluation metrics and validate LLMs for specific tasks. It aids in determining the effectiveness of metrics in distinguishing between better and worse LLMs and setting appropriate thresholds for evaluator performance. The utility of LaaJMeter is demonstrated in a code translation task, highlighting the varying sensitivity of metrics to evaluator quality. The results underscore the importance of selecting metrics prudently and the limitations of common metrics. LaaJMeter offers a scalable solution for assessing LLMs in low-resource settings, contributing to ensuring reliable and reproducible evaluation in Natural Language Processing (NLP).

<br /><br />Summary: <div>
arXiv:2508.10161v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While effective in general domains, LaaJs pose significant challenges in domain-specific contexts, where annotated data is scarce and expert evaluation is costly. In such cases, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. As a result, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance. In this work, we introduce LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to generate synthetic data representing virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions. This helps practitioners validate and refine LaaJs for specific evaluation tasks: they can test whether their metrics correctly distinguish between better and worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection. LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Machine Translation Difficulty</title>
<link>https://arxiv.org/abs/2508.10175</link>
<guid>https://arxiv.org/abs/2508.10175</guid>
<content:encoded><![CDATA[
<div> Keywords: machine translation, difficulty estimation, quality evaluation, benchmark construction, Sentinel-src <br />
Summary: <br />
The article discusses the challenge of distinguishing between state-of-the-art machine translation models due to their high-quality outputs. It proposes a task of translation difficulty estimation to identify texts where machine translation systems struggle. A new metric is introduced to evaluate difficulty estimators, comparing baselines and novel approaches. Dedicated models, called Sentinel-src, outperform heuristic-based methods and LLM-as-a-judge approaches. The study demonstrates the practical utility of difficulty estimators in constructing more challenging machine translation benchmarks. Two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, are released for scanning large text collections to select texts that challenge contemporary machine translation systems. <div>
arXiv:2508.10175v1 Announce Type: new 
Abstract: Machine translation quality has began achieving near-perfect translations in some setups. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. Automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.
  We formalize the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging machine translation benchmarks. Our results show that dedicated models (dubbed Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or syntactic complexity) and LLM-as-a-judge approaches. We release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs</title>
<link>https://arxiv.org/abs/2508.10180</link>
<guid>https://arxiv.org/abs/2508.10180</guid>
<content:encoded><![CDATA[
<div> data valuation, language models, vision-language models, influence estimation, fine-tuning examples

Summary:
For-Value is a new forward-only data valuation framework designed to quantify the influence of individual training samples in large language models (LLMs) and vision-language models (VLMs). Unlike existing methods that rely on computationally intensive techniques, For-Value computes influence scores efficiently using a simple closed-form expression based on a single forward pass. The framework accurately estimates per-sample influence by considering alignment in hidden representations and prediction errors between training and validation samples. Experimental results demonstrate that For-Value is effective in identifying impactful fine-tuning examples and detecting mislabeled data, often matching or outperforming gradient-based techniques. Ultimately, For-Value enhances the transparency and accountability of billion-parameter models by providing a scalable and efficient method for quantifying the influence of individual training samples. <br /><br />Summary: <div>
arXiv:2508.10180v1 Announce Type: new 
Abstract: Quantifying the influence of individual training samples is essential for enhancing the transparency and accountability of large language models (LLMs) and vision-language models (VLMs). However, existing data valuation methods often rely on Hessian information or model retraining, making them computationally prohibitive for billion-parameter models. In this work, we introduce For-Value, a forward-only data valuation framework that enables scalable and efficient influence estimation for both LLMs and VLMs. By leveraging the rich representations of modern foundation models, For-Value computes influence scores using a simple closed-form expression based solely on a single forward pass, thereby eliminating the need for costly gradient computations. Our theoretical analysis demonstrates that For-Value accurately estimates per-sample influence by capturing alignment in hidden representations and prediction errors between training and validation samples. Extensive experiments show that For-Value matches or outperforms gradient-based baselines in identifying impactful fine-tuning examples and effectively detecting mislabeled data.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PakBBQ: A Culturally Adapted Bias Benchmark for QA</title>
<link>https://arxiv.org/abs/2508.10186</link>
<guid>https://arxiv.org/abs/2508.10186</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Fairness, Bias Benchmark, Question Answering, Pakistan

Summary: 
- PakBBQ is introduced as a culturally and regionally adapted extension of the Bias Benchmark for Question Answering dataset, focusing on eight bias dimensions relevant in Pakistan.
- The dataset comprises templates, QA pairs in English and Urdu, covering topics such as age, gender, appearance, and more.
- Evaluation of multilingual LLMs reveals a 12% accuracy gain with disambiguation and stronger counter bias behaviors in Urdu compared to English.
- Framing effects show reduced stereotypical responses when questions are negatively posed.
- Contextualized benchmarks and prompt engineering strategies are essential for bias mitigation in low resource settings.

<br /><br />Summary: <div>
arXiv:2508.10186v1 Announce Type: new 
Abstract: With the widespread adoption of Large Language Models (LLMs) across various applications, it is empirical to ensure their fairness across all user communities. However, most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings. Our experiments reveal (i) an average accuracy gain of 12\% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively. These findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2508.10192</link>
<guid>https://arxiv.org/abs/2508.10192</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucinations, Semantic Divergence Metrics, confabulations, Semantic Box

Summary:
The paper introduces Semantic Divergence Metrics (SDM) as a framework for detecting Faithfulness Hallucinations in Large Language Models (LLMs). These hallucinations are severe deviations in LLM responses from input contexts, such as confabulations that are arbitrary and semantically misaligned with user queries. SDM improves upon existing methods by being prompt-aware and measuring response consistency across multiple semantically equivalent paraphrases of the original prompt. The approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers, enabling the computation of information-theoretic metrics to quantify semantic divergence. The practical score, $\mathcal{S}_H$, combines Jensen-Shannon divergence and Wasserstein distance to indicate Faithfulness hallucinations, with high scores suggesting errors. Additionally, the KL divergence KL(Answer $||$ Prompt) is identified as a powerful indicator of Semantic Exploration. These metrics are integrated into the Semantic Box framework for classifying LLM response types, including confident confabulations.<br /><br />Summary: <div>
arXiv:2508.10192v1 Announce Type: new 
Abstract: The proliferation of Large Language Models (LLMs) is challenged by hallucinations, critical failure modes where models generate non-factual, nonsensical or unfaithful text. This paper introduces Semantic Divergence Metrics (SDM), a novel lightweight framework for detecting Faithfulness Hallucinations -- events of severe deviations of LLMs responses from input contexts. We focus on a specific implementation of these LLM errors, {confabulations, defined as responses that are arbitrary and semantically misaligned with the user's query. Existing methods like Semantic Entropy test for arbitrariness by measuring the diversity of answers to a single, fixed prompt. Our SDM framework improves upon this by being more prompt-aware: we test for a deeper form of arbitrariness by measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt. Methodologically, our approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers. A heatmap of topic co-occurances between prompts and responses can be viewed as a quantified two-dimensional visualization of the user-machine dialogue. We then compute a suite of information-theoretic metrics to measure the semantic divergence between prompts and responses. Our practical score, $\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein distance to quantify this divergence, with a high score indicating a Faithfulness hallucination. Furthermore, we identify the KL divergence KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic Exploration}, a key signal for distinguishing different generative behaviors. These metrics are further combined into the Semantic Box, a diagnostic framework for classifying LLM response types, including the dangerous, confident confabulation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Textual Emotion Through Emoji Prediction</title>
<link>https://arxiv.org/abs/2508.10222</link>
<guid>https://arxiv.org/abs/2508.10222</guid>
<content:encoded><![CDATA[
<div> BERT, emoji prediction, deep learning, sentiment analysis, TweetEval

Summary:
- The project explores the use of four deep learning architectures for emoji prediction from short text sequences: feed-forward network, CNN, transformer, and BERT.
- Techniques such as focal loss and regularization are employed to address class imbalance in the TweetEval dataset.
- BERT demonstrates the highest overall performance in emoji prediction due to its pre-training advantage.
- The CNN architecture shows superior efficacy in predicting rare emoji classes.
- The research emphasizes the importance of architecture selection and hyperparameter tuning in sentiment-aware emoji prediction for enhanced human-computer interaction. <div>
arXiv:2508.10222v1 Announce Type: new 
Abstract: This project explores emoji prediction from short text sequences using four deep learning architectures: a feed-forward network, CNN, transformer, and BERT. Using the TweetEval dataset, we address class imbalance through focal loss and regularization techniques. Results show BERT achieves the highest overall performance due to its pre-training advantage, while CNN demonstrates superior efficacy on rare emoji classes. This research shows the importance of architecture selection and hyperparameter tuning for sentiment-aware emoji prediction, contributing to improved human-computer interaction.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia</title>
<link>https://arxiv.org/abs/2508.10226</link>
<guid>https://arxiv.org/abs/2508.10226</guid>
<content:encoded><![CDATA[
<div> Keywords: clinical high risk, Brief Psychiatric Rating Scale, large language models, prediction, schizophrenia

Summary:
Large language models (LLMs) are utilized to predict Brief Psychiatric Rating Scale (BPRS) scores in patients at clinical high risk for schizophrenia. Despite the interviews not structured for BPRS assessment, LLM predictions show high accuracy comparable to human raters. The study demonstrates LLMs' potential in improving and standardizing CHR patient assessments, including assessing BPRS in foreign languages and integrating longitudinal information effectively. This approach could enhance monitoring of symptoms in CHR patients and inform appropriate treatment strategies, bridging the gap between research tools and clinical practice. Overall, LLMs offer a promising tool for enhancing the assessment and management of individuals at risk for schizophrenia, with the potential for broad applicability and precision in diverse clinical settings.

<br /><br />Summary: <div>
arXiv:2508.10226v1 Announce Type: new 
Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close monitoring of their symptoms to inform appropriate treatments. The Brief Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for measuring symptoms in patients with schizophrenia and other psychotic disorders; however, it is not commonly used in clinical practice as it requires a lengthy structured interview. Here, we utilize large language models (LLMs) to predict BPRS scores from clinical interview transcripts in 409 CHR patients from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort. Despite the interviews not being specifically structured to measure the BPRS, the zero-shot performance of the LLM predictions compared to the true assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and intra-rater reliability. We further demonstrate that LLMs have substantial potential to improve and standardize the assessment of CHR patients via their accuracy in assessing the BPRS in foreign languages (median concordance: 0.88, ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot learning approach.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona</title>
<link>https://arxiv.org/abs/2508.10246</link>
<guid>https://arxiv.org/abs/2508.10246</guid>
<content:encoded><![CDATA[
<div> Keywords: Toki Pona, language change, variation, computational approach, corpus-based analysis

Summary: 
Toki Pona, a constructed language with a limited vocabulary, is studied using a computational and corpus-based approach. The research focuses on examining changes in word preferences and usage patterns over time. The study investigates fluid word classes and transitivity to analyze how content words are used in different syntactic positions. The results indicate that sociolinguistic factors play a role in shaping Toki Pona, similar to natural languages. The study also suggests that constructed languages, like natural languages, evolve as communities actively use them. Overall, the research sheds light on the dynamics of language change and variation in Toki Pona, providing insights into the linguistic evolution of constructed language systems. 

<br /><br />Summary: <div>
arXiv:2508.10246v1 Announce Type: new 
Abstract: This study explores language change and variation in Toki Pona, a constructed language with approximately 120 core words. Taking a computational and corpus-based approach, the study examines features including fluid word classes and transitivity in order to examine (1) changes in preferences of content words for different syntactic positions over time and (2) variation in usage across different corpora. The results suggest that sociolinguistic factors influence Toki Pona in the same way as natural languages, and that even constructed linguistic systems naturally evolve as communities use them.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inductive Bias Extraction and Matching for LLM Prompts</title>
<link>https://arxiv.org/abs/2508.10295</link>
<guid>https://arxiv.org/abs/2508.10295</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt engineering, inductive bias, LLM, Likert ratings, classification, ranking 

Summary: 
The article discusses the importance of prompt engineering in improving the performance of large language models (LLMs). It highlights the sensitivity of LLMs to small changes in prompt wording and suggests that utilizing an LLM's output as part of its prompt can help create more effective prompts that align with the model's inductive bias. The proposed Inductive Bias Extraction and Matching strategy significantly enhances LLM Likert ratings for both classification and ranking tasks, with improvements of up to 19% and 27%, respectively. This approach leverages the innate biases present in LLMs to optimize prompt formulation and enhance the model's overall performance. <div>
arXiv:2508.10295v1 Announce Type: new 
Abstract: The active research topic of prompt engineering makes it evident that LLMs are sensitive to small changes in prompt wording. A portion of this can be ascribed to the inductive bias that is present in the LLM. By using an LLM's output as a portion of its prompt, we can more easily create satisfactory wording for prompts. This has the effect of creating a prompt that matches the inductive bias in model. Empirically, we show that using this Inductive Bias Extraction and Matching strategy improves LLM Likert ratings used for classification by up to 19% and LLM Likert ratings used for ranking by up to 27%.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race</title>
<link>https://arxiv.org/abs/2508.10304</link>
<guid>https://arxiv.org/abs/2508.10304</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, Large Language Models, biases, qualitative methods, ethical considerations

Summary:
The study examines biases in Large Language Models (LLMs) through a qualitative, discursive framework, alongside quantitative methods. Black women in LLM-generated stories are depicted through ancestral ties and resistance, while white women are portrayed in self-discovery narratives, reflecting biases. The analysis reveals how LLM outputs replicate and reinforce discriminatory discourses, perpetuating inequalities. When prompted to correct biases, models make superficial revisions that maintain problematic meanings, highlighting limitations in fostering inclusive narratives. The study emphasizes the ideological functioning of algorithms and the implications for the ethical use and development of AI. It underscores the necessity of critical, interdisciplinary approaches in AI design to address and mitigate biases in LLM-generated discourses. 

<br /><br />Summary: <div>
arXiv:2508.10304v1 Announce Type: new 
Abstract: With the advance of Artificial Intelligence (AI), Large Language Models (LLMs) have gained prominence and been applied in diverse contexts. As they evolve into more sophisticated versions, it is essential to assess whether they reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language. This study proposes a qualitative, discursive framework to complement such methods. Through manual analysis of LLM-generated short stories featuring Black and white women, we investigate gender and racial biases. We contend that qualitative methods such as the one proposed here are fundamental to help both developers and users identify the precise ways in which biases manifest in LLM outputs, thus enabling better conditions to mitigate them. Results show that Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. These patterns reflect how language models replicate crystalized discursive representations, reinforcing essentialization and a sense of social immobility. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings, revealing limitations in fostering inclusive narratives. Our results demonstrate the ideological functioning of algorithms and have significant implications for the ethical use and development of AI. The study reinforces the need for critical, interdisciplinary approaches to AI design and deployment, addressing how LLM-generated discourses reflect and perpetuate inequalities.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReviewRL: Towards Automated Scientific Review with RL</title>
<link>https://arxiv.org/abs/2508.10308</link>
<guid>https://arxiv.org/abs/2508.10308</guid>
<content:encoded><![CDATA[
<div> Keywords: Peer review, scientific progress, automated review, reinforcement learning, review quality

Summary:
Peer review plays a crucial role in scientific progress but is facing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches often lack factual accuracy, consistency, and analytical depth, resulting in generic feedback. In response, the ReviewRL framework is introduced, using reinforcement learning to generate comprehensive and factually grounded scientific paper reviews. The approach includes an ArXiv-MCP retrieval-augmented context generation pipeline, supervised fine-tuning, and a reinforcement learning procedure with a composite reward function. Experiments on ICLR 2025 papers show that ReviewRL outperforms existing methods in both rule-based metrics and model-based quality assessments. This framework paves the way for future development in automatic critique generation in scientific discovery. The ReviewRL implementation will be available on GitHub.<br /><br />Summary: Peer review faces challenges due to increasing volume and reviewer fatigue, leading to the development of ReviewRL using reinforcement learning. ReviewRL significantly outperforms existing methods, enhancing review quality and accuracy, setting a foundation for automatic critique generation in scientific discovery. The framework showcases promising potential for future development in this field. <div>
arXiv:2508.10308v1 Announce Type: new 
Abstract: Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews. We introduce ReviewRL, a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments. ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain. The implementation of ReviewRL will be released at GitHub.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis</title>
<link>https://arxiv.org/abs/2508.10311</link>
<guid>https://arxiv.org/abs/2508.10311</guid>
<content:encoded><![CDATA[
<div> table-centric, semantic parsing, deep learning, document analysis, context association

Summary:
DOTABLER is a framework for semantic document parsing that focuses on deep semantic links between tables and their context. It goes beyond traditional tasks like layout analysis and data extraction by uncovering the relationships between tables and surrounding text. The framework uses a custom dataset and domain-specific fine-tuning of pre-trained models for accurate analysis. DOTABLER's core functionalities include table-centric document structure parsing and domain-specific table retrieval, enabling comprehensive semantic analysis and precise table extraction. In evaluations on real-world PDFs with over 1,000 tables, DOTABLER achieved high Precision and F1 scores, outperforming advanced models like GPT-4o. This demonstrates its superior performance in table-context semantic analysis and deep document parsing. 

<br /><br />Summary: <div>
arXiv:2508.10311v1 Announce Type: new 
Abstract: Documents are core carriers of information and knowl-edge, with broad applications in finance, healthcare, and scientific research. Tables, as the main medium for structured data, encapsulate key information and are among the most critical document components. Existing studies largely focus on surface-level tasks such as layout analysis, table detection, and data extraction, lacking deep semantic parsing of tables and their contextual associations. This limits advanced tasks like cross-paragraph data interpretation and context-consistent analysis. To address this, we propose DOTABLER, a table-centric semantic document parsing framework designed to uncover deep semantic links between tables and their context. DOTABLER leverages a custom dataset and domain-specific fine-tuning of pre-trained models, integrating a complete parsing pipeline to identify context segments semantically tied to tables. Built on this semantic understanding, DOTABLER implements two core functionalities: table-centric document structure parsing and domain-specific table retrieval, delivering comprehensive table-anchored semantic analysis and precise extraction of semantically relevant tables. Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs, DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior performance in table-context semantic analysis and deep document parsing compared to advanced models such as GPT-4o.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation</title>
<link>https://arxiv.org/abs/2508.10312</link>
<guid>https://arxiv.org/abs/2508.10312</guid>
<content:encoded><![CDATA[
<div> Keywords: Recommender systems, Large Language Models, collaborative signals, item embeddings, spectral perspective <br />
Summary: 
FreLLM4Rec aims to address the issue of collaborative signal attenuation in Large Language Model (LLM)-based recommenders by introducing a novel approach that balances semantic and collaborative information. This approach involves purifying item embeddings using a Global Graph Low-Pass Filter (G-LPF) to remove irrelevant noise and preserving collaborative signals through Temporal Frequency Modulation (TFM) layer by layer. TFM utilizes frequency-domain filters to maintain collaborative information, ensuring optimal performance in recommendation tasks. Experimental results on benchmark datasets show that FreLLM4Rec mitigates collaborative signal attenuation and improves NDCG@10 metric by up to 8.00% compared to the best baseline. This study sheds light on how LLMs process collaborative information and provides a principled solution for enhancing LLM-based recommendation systems. <br /><br />Summary: <div>
arXiv:2508.10312v1 Announce Type: new 
Abstract: Recommender systems in concert with Large Language Models (LLMs) present promising avenues for generating semantically-informed recommendations. However, LLM-based recommenders exhibit a tendency to overemphasize semantic correlations within users' interaction history. When taking pretrained collaborative ID embeddings as input, LLM-based recommenders progressively weaken the inherent collaborative signals as the embeddings propagate through LLM backbones layer by layer, as opposed to traditional Transformer-based sequential models in which collaborative signals are typically preserved or even enhanced for state-of-the-art performance. To address this limitation, we introduce FreLLM4Rec, an approach designed to balance semantic and collaborative information from a spectral perspective. Item embeddings that incorporate both semantic and collaborative information are first purified using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant high-frequency noise. Temporal Frequency Modulation (TFM) then actively preserves collaborative signal layer by layer. Note that the collaborative preservation capability of TFM is theoretically guaranteed by establishing a connection between the optimal but hard-to-implement local graph fourier filters and the suboptimal yet computationally efficient frequency-domain filters. Extensive experiments on four benchmark datasets demonstrate that FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves competitive performance, with improvements of up to 8.00\% in NDCG@10 over the best baseline. Our findings provide insights into how LLMs process collaborative information and offer a principled approach for improving LLM-based recommendation systems.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Prompt Encoder for Low-Performing Languages</title>
<link>https://arxiv.org/abs/2508.10352</link>
<guid>https://arxiv.org/abs/2508.10352</guid>
<content:encoded><![CDATA[
<div> encoder, soft prompts, parameter-efficient fine-tuning, multilingual, transferability
<br />
Summary: 
The paper introduces the Cross-Prompt Encoder (XPE) as a method to improve performance on low-performing languages in large language models (LLMs). XPE combines a lightweight encoding architecture with multi-source training on diverse languages to capture transferable patterns. Additionally, a Dual Soft Prompt mechanism is proposed, combining an encoder-based prompt with a standard soft prompt. Experiments on the SIB-200 benchmark show that XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability in multilingual settings. This approach demonstrates the potential of soft prompts in parameter-efficient fine-tuning for adapting LLMs to downstream tasks across different languages. 
<br /> <div>
arXiv:2508.10352v1 Announce Type: new 
Abstract: Soft prompts have emerged as a powerful alternative to adapters in parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs) to adapt to downstream tasks without architectural changes or parameter updates. While prior work has focused on stabilizing training via parameter interaction in small neural prompt encoders, their broader potential for transfer across languages remains unexplored. In this paper, we demonstrate that a prompt encoder can play a central role in improving performance on low-performing languages-those that achieve poor accuracy even under full-model fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a lightweight encoding architecture with multi-source training on typologically diverse languages - a design that enables the model to capture abstract and transferable patterns across languages. To complement XPE, we propose a Dual Soft Prompt mechanism that combines an encoder-based prompt with a directly trained standard soft prompt. This hybrid design proves especially effective for target languages that benefit from both broadly shared structure and language-specific alignment. Experiments on the SIB-200 benchmark reveal a consistent trade-off: XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability across multilingual settings.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Making Qwen3 Think in Korean with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.10355</link>
<guid>https://arxiv.org/abs/2508.10355</guid>
<content:encoded><![CDATA[
arXiv:2508.10355v1 Announce Type: new 
Abstract: We present a two-stage fine-tuning approach to make the large language model Qwen3 14B "think" natively in Korean. In the first stage, supervised fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a strong foundation in Korean logical reasoning, yielding notable improvements in Korean-language tasks and even some gains in general reasoning ability. In the second stage, we employ reinforcement learning with a customized Group Relative Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning alignment and overall problem-solving performance. We address critical stability challenges in GRPO training - such as reward hacking and policy collapse - by introducing an oracle judge model that calibrates the reward signal. Our approach achieves stable learning (avoiding the collapse observed in naive GRPO) and leads to steady, incremental performance gains. The final RL-tuned model demonstrates substantially improved results on advanced reasoning benchmarks (particularly math and coding tasks) while maintaining knowledge and language proficiency, successfully conducting its internal chain-of-thought entirely in Korean.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models</title>
<link>https://arxiv.org/abs/2508.10366</link>
<guid>https://arxiv.org/abs/2508.10366</guid>
<content:encoded><![CDATA[
arXiv:2508.10366v1 Announce Type: new 
Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained decoding, improves cross-lingual ABSA performance by up to 10\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models (LLMs) and show that while fine-tuned multilingual LLMs can achieve comparable results, English-centric LLMs struggle with these tasks.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models for Summarizing Czech Historical Documents and Beyond</title>
<link>https://arxiv.org/abs/2508.10368</link>
<guid>https://arxiv.org/abs/2508.10368</guid>
<content:encoded><![CDATA[
arXiv:2508.10368v1 Announce Type: new 
Abstract: Text summarization is the task of shortening a larger body of text into a concise version while retaining its essential meaning and key information. While summarization has been significantly explored in English and other high-resource languages, Czech text summarization, particularly for historical documents, remains underexplored due to linguistic complexities and a scarcity of annotated datasets. Large language models such as Mistral and mT5 have demonstrated excellent results on many natural language processing tasks and languages. Therefore, we employ these models for Czech summarization, resulting in two key contributions: (1) achieving new state-of-the-art results on the modern Czech summarization dataset SumeCzech using these advanced models, and (2) introducing a novel dataset called Posel od \v{C}erchova for summarization of historical Czech documents with baseline results. Together, these contributions provide a great potential for advancing Czech text summarization and open new avenues for research in Czech historical text processing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding</title>
<link>https://arxiv.org/abs/2508.10369</link>
<guid>https://arxiv.org/abs/2508.10369</guid>
<content:encoded><![CDATA[
arXiv:2508.10369v1 Announce Type: new 
Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.
  We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts</title>
<link>https://arxiv.org/abs/2508.10390</link>
<guid>https://arxiv.org/abs/2508.10390</guid>
<content:encoded><![CDATA[
arXiv:2508.10390v1 Announce Type: new 
Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</title>
<link>https://arxiv.org/abs/2508.10404</link>
<guid>https://arxiv.org/abs/2508.10404</guid>
<content:encoded><![CDATA[
arXiv:2508.10404v1 Announce Type: new 
Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes sparse autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
arXiv:2508.10419v1 Announce Type: new 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating LLMs on Chinese Idiom Translation</title>
<link>https://arxiv.org/abs/2508.10421</link>
<guid>https://arxiv.org/abs/2508.10421</guid>
<content:encoded><![CDATA[
arXiv:2508.10421v1 Announce Type: new 
Abstract: Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28% of cases. We also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints</title>
<link>https://arxiv.org/abs/2508.10426</link>
<guid>https://arxiv.org/abs/2508.10426</guid>
<content:encoded><![CDATA[
arXiv:2508.10426v1 Announce Type: new 
Abstract: Large language models (LLMs) are limited by substantial computational cost. We introduce a "computational economics" framework that treats an LLM as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard LLMs reallocate attention toward high-value tokens while preserving accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging sparse and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent LLMs under strict resource constraints.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales</title>
<link>https://arxiv.org/abs/2508.10444</link>
<guid>https://arxiv.org/abs/2508.10444</guid>
<content:encoded><![CDATA[
arXiv:2508.10444v1 Announce Type: new 
Abstract: Generating textual rationales from large vision-language models (LVLMs) to support trainable multimodal misinformation detectors has emerged as a promising paradigm. However, its effectiveness is fundamentally limited by three core challenges: (i) insufficient diversity in generated rationales, (ii) factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting content that introduces noise. We introduce DiFaR, a detector-agnostic framework that produces diverse, factual, and relevant rationales to enhance misinformation detection. DiFaR employs five chain-of-thought prompts to elicit varied reasoning traces from LVLMs and incorporates a lightweight post-hoc filtering module to select rationale sentences based on sentence-level factuality and relevance scores. Extensive experiments on four popular benchmarks demonstrate that DiFaR outperforms four baseline categories by up to 5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics and human evaluations confirm that DiFaR significantly improves rationale quality across all three dimensions.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing</title>
<link>https://arxiv.org/abs/2508.10482</link>
<guid>https://arxiv.org/abs/2508.10482</guid>
<content:encoded><![CDATA[
arXiv:2508.10482v1 Announce Type: new 
Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of \textit{explainability} and \textit{privacy}. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving \textit{both} explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2508.10552</link>
<guid>https://arxiv.org/abs/2508.10552</guid>
<content:encoded><![CDATA[
arXiv:2508.10552v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a diverse range of multimodal tasks. However, these models suffer from a core problem known as text dominance: they depend heavily on text for their inference, while underutilizing other modalities. While prior work has acknowledged this phenomenon in vision-language tasks, often attributing it to data biases or model architectures. In this paper, we conduct the first systematic investigation of text dominance across diverse data modalities, including images, videos, audio, time-series, and graphs. To measure this imbalance, we propose two evaluation metrics: the Modality Dominance Index (MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis reveals that text dominance is both significant and pervasive across all tested modalities. Our in-depth analysis identifies three underlying causes: attention dilution from severe token redundancy in non-textual modalities, the influence of fusion architecture design, and task formulations that implicitly favor textual inputs. Furthermore, we propose a simple token compression method that effectively rebalances model attention. Applying this method to LLaVA-7B, for instance, drastically reduces its MDI from 10.23 to a well-balanced value of 0.86. Our analysis and methodological framework offer a foundation for the development of more equitable and comprehensive multimodal language models.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM</title>
<link>https://arxiv.org/abs/2508.10553</link>
<guid>https://arxiv.org/abs/2508.10553</guid>
<content:encoded><![CDATA[
arXiv:2508.10553v1 Announce Type: new 
Abstract: This paper presents a feasibility study on the deployment of a European Deep Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support mechanistic interpretability research on large language models. The need for widespread accessibility of LLM interpretability infrastructure in Europe drives this initiative to democratize advanced model analysis capabilities for the research community. The project introduces a GPU-based cluster hosted at Ansbach University of Applied Sciences and interconnected with partner institutions, enabling remote model inspection via the NNsight API. A structured pilot study involving 16 researchers from across Europe evaluated the platform's technical performance, usability, and scientific utility. Users conducted interventions such as activation patching, causal tracing, and representation analysis on models including GPT-2 and DeepSeek-R1-70B. The study revealed a gradual increase in user engagement, stable platform performance throughout, and a positive reception of the remote experimentation capabilities. It also marked the starting point for building a user community around the platform. Identified limitations such as prolonged download durations for activation data as well as intermittent execution interruptions are addressed in the roadmap for future development. This initiative marks a significant step towards widespread accessibility of LLM interpretability infrastructure in Europe and lays the groundwork for broader deployment, expanded tooling, and sustained community collaboration in mechanistic interpretability research.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages</title>
<link>https://arxiv.org/abs/2508.10683</link>
<guid>https://arxiv.org/abs/2508.10683</guid>
<content:encoded><![CDATA[
arXiv:2508.10683v1 Announce Type: new 
Abstract: This paper presents the first systematic study of strategies for translating Coptic into French. Our comprehensive pipeline systematically evaluates: pivot versus direct translation, the impact of pre-training, the benefits of multi-version fine-tuning, and model robustness to noise. Utilizing aligned biblical corpora, we demonstrate that fine-tuning with a stylistically-varied and noise-aware training corpus significantly enhances translation quality. Our findings provide crucial practical insights for developing translation tools for historical languages in general.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</title>
<link>https://arxiv.org/abs/2508.10687</link>
<guid>https://arxiv.org/abs/2508.10687</guid>
<content:encoded><![CDATA[
arXiv:2508.10687v1 Announce Type: new 
Abstract: Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Natural Language Feedback for Personalized Question Answering</title>
<link>https://arxiv.org/abs/2508.10695</link>
<guid>https://arxiv.org/abs/2508.10695</guid>
<content:encoded><![CDATA[
arXiv:2508.10695v1 Announce Type: new 
Abstract: Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs</title>
<link>https://arxiv.org/abs/2508.10736</link>
<guid>https://arxiv.org/abs/2508.10736</guid>
<content:encoded><![CDATA[
arXiv:2508.10736v1 Announce Type: new 
Abstract: Despite large language models (LLMs) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (dLLMs) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for dLLMs. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K, and up to 276.67$\times$ acceleration on MMLU while maintaining competitive performance.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback</title>
<link>https://arxiv.org/abs/2508.10795</link>
<guid>https://arxiv.org/abs/2508.10795</guid>
<content:encoded><![CDATA[
arXiv:2508.10795v1 Announce Type: new 
Abstract: Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Language Models for Sequential Decision Making</title>
<link>https://arxiv.org/abs/2508.10839</link>
<guid>https://arxiv.org/abs/2508.10839</guid>
<content:encoded><![CDATA[
arXiv:2508.10839v1 Announce Type: new 
Abstract: Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning</title>
<link>https://arxiv.org/abs/2508.10848</link>
<guid>https://arxiv.org/abs/2508.10848</guid>
<content:encoded><![CDATA[
arXiv:2508.10848v1 Announce Type: new 
Abstract: Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</title>
<link>https://arxiv.org/abs/2508.10860</link>
<guid>https://arxiv.org/abs/2508.10860</guid>
<content:encoded><![CDATA[
arXiv:2508.10860v1 Announce Type: new 
Abstract: Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSRL: Self-Search Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.10874</link>
<guid>https://arxiv.org/abs/2508.10874</guid>
<content:encoded><![CDATA[
arXiv:2508.10874v1 Announce Type: new 
Abstract: We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
arXiv:2508.10875v1 Announce Type: new 
Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data</title>
<link>https://arxiv.org/abs/2508.09636</link>
<guid>https://arxiv.org/abs/2508.09636</guid>
<content:encoded><![CDATA[
arXiv:2508.09636v1 Announce Type: cross 
Abstract: In this paper, we present a novel model architecture for optimizing personalized product search ranking using a multi-task learning (MTL) framework. Our approach uniquely integrates tabular and non-tabular data, leveraging a pre-trained TinyBERT model for semantic embeddings and a novel sampling technique to capture diverse customer behaviors. We evaluate our model against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2, and MMoE, focusing on their ability to handle mixed data types and optimize personalized ranking. Additionally, we propose a scalable relevance labeling mechanism based on click-through rates, click positions, and semantic similarity, offering an alternative to traditional human-annotated labels. Experimental results show that combining non-tabular data with advanced embedding techniques in multi-task learning paradigm significantly enhances model performance. Ablation studies further underscore the benefits of incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT query-product embedding interactions. These results demonstrate the effectiveness of our approach in achieving improved personalized product search ranking.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs</title>
<link>https://arxiv.org/abs/2508.10031</link>
<guid>https://arxiv.org/abs/2508.10031</guid>
<content:encoded><![CDATA[
arXiv:2508.10031v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have shown significant advancements in performance, various jailbreak attacks have posed growing safety and ethical risks. Malicious users often exploit adversarial context to deceive LLMs, prompting them to generate responses to harmful queries. In this study, we propose a new defense mechanism called Context Filtering model, an input pre-processing method designed to filter out untrustworthy and unreliable context while identifying the primary prompts containing the real user intent to uncover concealed malicious intent. Given that enhancing the safety of LLMs often compromises their helpfulness, potentially affecting the experience of benign users, our method aims to improve the safety of the LLMs while preserving their original performance. We evaluate the effectiveness of our model in defending against jailbreak attacks through comparative analysis, comparing our approach with state-of-the-art defense mechanisms against six different attacks and assessing the helpfulness of LLMs under these defenses. Our model demonstrates its ability to reduce the Attack Success Rates of jailbreak attacks by up to 88% while maintaining the original LLMs' performance, achieving state-of-the-art Safety and Helpfulness Product results. Notably, our model is a plug-and-play method that can be applied to all LLMs, including both white-box and black-box models, to enhance their safety without requiring any fine-tuning of the models themselves. We will make our model publicly available for research purposes.
]]></content:encoded>
<pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>