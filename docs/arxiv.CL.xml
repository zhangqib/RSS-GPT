<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CL updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CL</link>


<item>
<title>Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study</title>
<link>https://arxiv.org/abs/2505.06149</link>
<guid>https://arxiv.org/abs/2505.06149</guid>
<content:encoded><![CDATA[
<div> Languages, Hate Speech Detection, Automated, Linguistic Diversity, Prompting Techniques  
Summary:  
- The article discusses the effectiveness of large language models in detecting hate speech across various non-English languages.  
- It evaluates zero-shot and few-shot prompting techniques compared to fine-tuned encoder models.  
- While prompting techniques may lag behind on real-world evaluation sets, they show better generalization in hate speech detection functions.  
- Customized prompt designs are crucial for maximizing performance in different languages.  
- The study highlights the need for linguistic diversity in hate speech detection and the importance of tailored prompting techniques for accurate results. 

Summary:  <br /><br /> <div>
arXiv:2505.06149v2 Announce Type: replace 
Abstract: Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report: Quantifying and Analyzing the Generalization Power of a DNN</title>
<link>https://arxiv.org/abs/2505.06993</link>
<guid>https://arxiv.org/abs/2505.06993</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, generalization power, explainable AI, interaction patterns, training dynamics

Summary:
This paper introduces a novel approach to analyzing the generalization capability of deep neural networks (DNNs) by dissecting the dynamics of generalizable and non-generalizable interactions encoded during training. Building on explainable AI research, which demonstrates that DNN inference can be represented by a limited set of AND-OR interaction patterns, the study proposes a method to quantify the generalization power of each interaction. It identifies a three-phase evolution in interaction generalization during training, with initial stages focusing on simple and generalizable interactions while later stages capture more complex, harder-to-generalize patterns. Experimental findings support the notion that learning non-generalizable interactions contributes to the gap between training and testing losses. This analysis sheds light on the underlying mechanisms impacting network generalization and offers insights into optimizing DNN training processes. 

<br /><br />Summary: <div>
arXiv:2505.06993v2 Announce Type: replace-cross 
Abstract: This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models</title>
<link>https://arxiv.org/abs/2505.07558</link>
<guid>https://arxiv.org/abs/2505.07558</guid>
<content:encoded><![CDATA[
<div> alignment, large language models, human preferences, Direct Density Ratio Optimization, statistical consistency <br />
<br />
Summary: 
The paper presents a novel method called Direct Density Ratio Optimization (DDRO) for aligning large language models (LLMs) with human preferences. Unlike existing methods that rely on specific preference models, DDRO directly estimates the density ratio between preferred and unpreferred output distributions, eliminating the need for explicit human preference modeling. The authors prove theoretically that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size increases, regardless of the underlying preference structure. Experimental results show that DDRO outperforms existing methods on various benchmarks. This approach enables truly data-driven alignment of LLMs, enhancing reliability and alignment with human preferences.<dd> <div>
arXiv:2505.07558v2 Announce Type: replace-cross 
Abstract: Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model. This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences. To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO). DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling. We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure. Experiments demonstrate that DDRO achieves superior performance compared to existing methods on many major benchmarks. DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale</title>
<link>https://arxiv.org/abs/2505.13480</link>
<guid>https://arxiv.org/abs/2505.13480</guid>
<content:encoded><![CDATA[
<div> Reddit, SuicideWatch, LLMs, Columbia-Suicide Severity Rating Scale, automated assessment <br />
Summary:<br />
- The study evaluates LLMs' ability to assess suicide risk using the C-SSRS, finding Claude and GPT perform well.
- Mistral had the lowest prediction error, with most models showing sensitivity to severity levels.
- Misclassifications occurred mostly between adjacent severity levels, requiring careful oversight and transparency.
- Ethical considerations highlight the need for human involvement and cautious deployment of automated assessments.
- The study emphasizes the importance of human oversight, transparency, and careful deployment of LLMs for suicide risk assessment on platforms like Reddit's SuicideWatch. <div>
arXiv:2505.13480v1 Announce Type: new 
Abstract: Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at https://github.com/av9ash/llm_cssrs_code.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors</title>
<link>https://arxiv.org/abs/2505.13483</link>
<guid>https://arxiv.org/abs/2505.13483</guid>
<content:encoded><![CDATA[
<div> Keywords: Metaphors, Emotions, Multimodal dataset, Chinese, Fine-grained classification

Summary:<br /><br />
Metaphors play a crucial role in expressing emotions, particularly in the realm of emotional intelligence. With the rise of multimodal data and communication, the complexity of emotion classification has increased, especially in the context of multimodal metaphors. However, the lack of research on constructing multimodal metaphorical datasets in different languages poses a challenge. To bridge this gap, a Chinese multimodal dataset containing 5,000 text-image pairs of metaphorical advertisements has been introduced. Each entry is meticulously annotated for metaphor occurrence, domain relations, and fine-grained emotion classification encompassing various emotions like joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. This dataset is publicly available, enabling further advancements in the field of emotional intelligence and multimodal metaphor analysis. <div>
arXiv:2505.13483v1 Announce Type: new 
Abstract: Metaphors play a pivotal role in expressing emotions, making them crucial for emotional intelligence. The advent of multimodal data and widespread communication has led to a proliferation of multimodal metaphors, amplifying the complexity of emotion classification compared to single-mode scenarios. However, the scarcity of research on constructing multimodal metaphorical fine-grained emotion datasets hampers progress in this domain. Moreover, existing studies predominantly focus on English, overlooking potential variations in emotional nuances across languages. To address these gaps, we introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of metaphorical advertisements. Each entry is meticulously annotated for metaphor occurrence, domain relations and fine-grained emotion classification encompassing joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. Our dataset is publicly accessible (https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in this burgeoning field.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Prefix Bias in LLM-based Reward Models</title>
<link>https://arxiv.org/abs/2505.13487</link>
<guid>https://arxiv.org/abs/2505.13487</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, human feedback, language models, bias detection, data augmentation
Summary:
Bias detection methods were introduced to evaluate prefix bias in Language Model-based reward models trained on preference datasets. The study revealed significant biases related to race and gender dimensions across various datasets and model architectures. The research emphasized the importance of bias-aware dataset design and evaluation to develop fair and reliable reward models in the AI field. A data augmentation strategy was proposed to mitigate prefix bias effects, demonstrating its effectiveness in reducing biases. The findings underscore the critical need for addressing biases in developing fair and accurate reward models, contributing to the ongoing discussion on fairness in AI. 
<br /><br />Summary: <div>
arXiv:2505.13487v1 Announce Type: new 
Abstract: Reinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Source framing triggers systematic evaluation bias in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13488</link>
<guid>https://arxiv.org/abs/2505.13488</guid>
<content:encoded><![CDATA[
<div> language models, text evaluation, framing effects, inter-model agreement, source attribution 

Summary: 
- Large Language Models (LLMs) are being used to evaluate text, raising concerns about consistency, bias, and robustness.
- A study examined agreement among four LLMs when evaluating narrative statements on various topics.
- High agreement was found among different LLMs without source framing.
- However, introducing source framing, such as attributing statements to Chinese individuals, lowered agreement scores, especially for Deepseek Reasoner.
- The study highlights the impact of framing effects on text evaluation in LLMs, emphasizing the importance of neutrality and fairness in these systems.  

<br /><br />Summary: <div>
arXiv:2505.13488v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly used not only to generate text but also to evaluate it, raising urgent questions about whether their judgments are consistent, unbiased, and robust to framing effects. In this study, we systematically examine inter- and intra-model agreement across four state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) tasked with evaluating 4,800 narrative statements on 24 different topics of social, political, and public health relevance, for a total of 192,000 assessments. We manipulate the disclosed source of each statement to assess how attribution to either another LLM or a human author of specified nationality affects evaluation outcomes. We find that, in the blind condition, different LLMs display a remarkably high degree of inter- and intra-model agreement across topics. However, this alignment breaks down when source framing is introduced. Here we show that attributing statements to Chinese individuals systematically lowers agreement scores across all models, and in particular for Deepseek Reasoner. Our findings reveal that framing effects can deeply affect text evaluation, with significant implications for the integrity, neutrality, and fairness of LLM-mediated information systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProdRev: A DNN framework for empowering customers using generative pre-trained transformers</title>
<link>https://arxiv.org/abs/2505.13491</link>
<guid>https://arxiv.org/abs/2505.13491</guid>
<content:encoded><![CDATA[
<div> Keywords: e-commerce, reviews, decision paralysis, generative pre-trained transformer, common sense

Summary:
Customers' preference for e-commerce has increased post-pandemic, leading to a plethora of reviews for products. This abundance of information can overwhelm consumers, causing decision paralysis. To address this, a framework is proposed that utilizes a generative pre-trained transformer to analyze reviews and provide abstractive summarization, enhancing the understanding of relationships between reviews. By fine-tuning the model with the Curie engine from GPT3, the framework incorporates "common sense" to aid users in making informed decisions. Instead of a simplistic extractive method, the model highlights pros and cons, empowering users to assess reviews and make choices efficiently. This approach optimizes the review analysis process, enabling consumers to navigate through extensive feedback and select products effectively.

<br /><br />Summary: <div>
arXiv:2505.13491v1 Announce Type: new 
Abstract: Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis</title>
<link>https://arxiv.org/abs/2505.13492</link>
<guid>https://arxiv.org/abs/2505.13492</guid>
<content:encoded><![CDATA[
<div> Keywords: Cognitive diagnosis, Large language models, Semantic information, Intelligent tutoring systems, Open-world knowledge 

Summary: 
The article introduces LLM4CD, a method that leverages Large Language Models (LLMs) for Cognitive Diagnosis (CD) in educational settings. Traditional CD methods focus on ID relationships, overlooking semantic connections in educational data. LLM4CD utilizes open-world knowledge from LLMs to create rich semantic representations for CD tasks. It proposes a bi-level encoder framework that incorporates cognitive text and knowledge state encoders to capture student test histories comprehensively. By replacing ID embeddings with semantic representations, LLM4CD offers flexibility in accommodating new students and exercises, addressing the cold-start problem in intelligent tutoring systems. Experimental results on real-world datasets demonstrate the superior performance of LLM4CD compared to existing CD models, validating the effectiveness of integrating LLMs for enriching semantic information in cognitive diagnosis tasks. 

Summary: <div>
arXiv:2505.13492v1 Announce Type: new 
Abstract: Cognitive diagnosis (CD) plays a crucial role in intelligent education, evaluating students' comprehension of knowledge concepts based on their test histories. However, current CD methods often model students, exercises, and knowledge concepts solely on their ID relationships, neglecting the abundant semantic relationships present within educational data space. Furthermore, contemporary intelligent tutoring systems (ITS) frequently involve the addition of new students and exercises, a situation that ID-based methods find challenging to manage effectively. The advent of large language models (LLMs) offers the potential for overcoming this challenge with open-world knowledge. In this paper, we propose LLM4CD, which Leverages Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the open-world knowledge of LLMs to construct cognitively expressive textual representations, which are then encoded to introduce rich semantic information into the CD task. Additionally, we propose an innovative bi-level encoder framework that models students' test histories through two levels of encoders: a macro-level cognitive text encoder and a micro-level knowledge state encoder. This approach substitutes traditional ID embeddings with semantic representations, enabling the model to accommodate new students and exercises with open-world knowledge and address the cold-start problem. Extensive experimental results demonstrate that our proposed method consistently outperforms previous CD models on multiple real-world datasets, validating the effectiveness of leveraging LLMs to introduce rich semantic information into the CD task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.13498</link>
<guid>https://arxiv.org/abs/2505.13498</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, multilingual, low-resource settings, Irish, IRLBench<br />
<br />
Summary: 
IRLBench is a new benchmark designed to evaluate the performance of Large Language Models (LLMs) in multilingual and low-resource settings, specifically focusing on the Irish language. The benchmark consists of 12 subjects from the Irish Leaving Certificate exams, enabling detailed analysis of model capabilities in various domains. By framing the task as long-form generation and using the official marking scheme, the benchmark assesses both correctness and language fidelity. Results from experiments with leading LLMs show a performance gap between English and Irish, with models producing valid Irish responses less frequently and answering correctly at a lower rate in Irish compared to English. The release of IRLBench and accompanying evaluation codebase aims to facilitate further research in culturally aware multilingual AI development.<br /><br />Summary: <div>
arXiv:2505.13498v1 Announce Type: new 
Abstract: Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\% of the time, and answer correctly 55.8\% of the time compared to 76.2\% in English for the best-performing model. We release IRLBench (https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future research on robust, culturally aware multilingual AI development.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Noise Injection Systemically Degrades Large Language Model Safety Guardrails</title>
<link>https://arxiv.org/abs/2505.13500</link>
<guid>https://arxiv.org/abs/2505.13500</guid>
<content:encoded><![CDATA[
<div> Gaussian noise, safety fine-tuning, harmful outputs, resilience, large language models  

Summary:  
- Gaussian noise injected into model activations can increase harmful-output rates in large language models by up to 27%.
- Deeper safety fine-tuning does not offer additional protection against these harmful outputs.
- Chain-of-thought reasoning remains largely intact despite the presence of Gaussian noise.
- Current safety alignment techniques show critical vulnerabilities, indicating the need for more robust AI safety systems.
- Reasoning-based and reinforcement learning approaches are suggested as potential directions for enhancing AI safety systems.  

<br /><br />Summary: <div>
arXiv:2505.13500v1 Announce Type: new 
Abstract: Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.13506</link>
<guid>https://arxiv.org/abs/2505.13506</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, external knowledge, corpus poisoning, EcoSafeRAG, security

Summary:
EcoSafeRAG introduces a defense method for Retrieval-Augmented Generation (RAG) models, addressing the vulnerability of corpus poisoning without relying on internal model knowledge. By analyzing the context diversity of candidate documents at the sentence level, malicious content is identified, improving security while maintaining model performance. Experimental results demonstrate EcoSafeRAG's state-of-the-art security measures and practical operational costs, with a 1.2x latency increase and significant token reduction compared to Vanilla RAG. This approach bridges the gap between enhancing RAG model performance and ensuring protection against attacks, making it a valuable addition to the field of natural language processing.<br /><br />Summary: <div>
arXiv:2505.13506v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-R1: Towards Comprehensive Temporal Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.13508</link>
<guid>https://arxiv.org/abs/2505.13508</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Temporal Intelligence, Future Event Prediction, Creative Scenario Generation, Reinforcement Learning

Summary:
Time-R1 is a framework designed to enhance the temporal abilities of moderate-sized Large Language Models (LLMs). It uses a three-stage development path, including a reinforcement learning curriculum with a dynamic rule-based reward system. This framework enables the model to build foundational temporal understanding, predict future events beyond its knowledge cutoff, and generate creative future scenarios without fine-tuning. Experimental results show that Time-R1 outperforms models significantly larger in size on future event prediction and scenario generation tasks. The approach showcases the effectiveness of progressive reinforcement learning fine-tuning in achieving superior temporal performance with smaller, more efficient models. Additionally, a new dataset called Time-Bench, derived from 10 years of news data, is released to support further research in temporal reasoning. 

<br /><br />Summary: Time-R1 framework enhances temporal abilities of moderate-sized LLMs through reinforcement learning, enabling better future event prediction and creative scenario generation. Results show superior performance compared to models over 200 times larger, demonstrating scalability and efficiency in achieving time-aware AI. The release of Time-Bench dataset supports ongoing research in temporal reasoning. <div>
arXiv:2505.13508v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13514</link>
<guid>https://arxiv.org/abs/2505.13514</guid>
<content:encoded><![CDATA[
<div> induction heads, Large Language Models, repetition curse, toxicity, attention head regularization 

Summary: 
Large Language Models (LLMs) often exhibit a repetition curse, generating repetitive or cyclic sequences of tokens. This study explores the role of induction heads, a type of attention head that tends to dominate the model's output logits during repetition, reducing the contribution of other attention heads. The toxicity of induction heads can lead to more repetitive outputs from LLMs. Understanding this mechanism can provide insights for designing and training LLMs. A proposed technique involving attention head regularization aims to reduce the dominance of induction heads during generation, promoting diverse and coherent outputs. This research sheds light on the underlying mechanisms of the repetition curse and offers potential strategies for mitigating this phenomenon. <br /><br /> <div>
arXiv:2505.13514v1 Announce Type: new 
Abstract: Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression</title>
<link>https://arxiv.org/abs/2505.13527</link>
<guid>https://arxiv.org/abs/2505.13527</guid>
<content:encoded><![CDATA[
<div> distributional discrepancies, black-box jailbreak method, logical expression translation, safety mechanisms, large language models 

Summary: 
The research introduces LogiBreak, a new black-box jailbreak technique that exploits distributional discrepancies between alignment-focused prompts and malicious prompts in large language models (LLMs). By translating harmful natural language prompts into formal logical expressions, LogiBreak can bypass LLM safety mechanisms. This method preserves the intended semantics and readability of the prompts while evading safety constraints effectively. LogiBreak is evaluated on a multilingual jailbreak dataset across three languages, demonstrating its efficiency in various evaluation scenarios and linguistic contexts. The vulnerability of LLMs to jailbreak attacks is attributed to the differences between alignment data and logic-based inputs, which LogiBreak leverages to achieve its goal. <div>
arXiv:2505.13527v1 Announce Type: new 
Abstract: Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation</title>
<link>https://arxiv.org/abs/2505.13554</link>
<guid>https://arxiv.org/abs/2505.13554</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, machine translation, neural machine translation, scheduling policy, LLM usage

Summary:
Large language models (LLMs) have shown promising results in various tasks, including machine translation (MT), but their high computational costs and latency can be a drawback. While LLM and neural machine translation (NMT) systems perform comparably in most cases, there are specific scenarios where one may have an advantage over the other. To address this, integrating NMT and LLM for translation and optimizing a scheduling policy for efficient translation results with minimal LLM usage is essential. The study evaluates different scheduling policies and proposes a decision mechanism based on source sentence features. Extensive experiments on multilingual test sets demonstrate that optimal translation performance can be achieved with minimal LLM usage. This approach ensures fast translation speeds while maximizing translation quality. <br /><br />Summary: <div>
arXiv:2505.13554v1 Announce Type: new 
Abstract: Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models</title>
<link>https://arxiv.org/abs/2505.13559</link>
<guid>https://arxiv.org/abs/2505.13559</guid>
<content:encoded><![CDATA[
<div> Keywords: Code-switching, Large Language Models, CS-Sum, Dialogue summarization, Error analysis

Summary: 
Code-switching (CS) presents challenges for Large Language Models (LLMs), impacting their comprehensibility. CS-Sum, a benchmark for CS dialogue summarization across multiple language pairs, evaluates LLM performance. Ten LLMs were assessed using different approaches like few-shot learning and fine-tuning. While automated metrics suggest high scores, LLMs often make subtle errors that distort dialogues. Three common error types were identified, with varied error rates among LLMs and language pairs. Some models exhibited more errors on specific language pairs, highlighting the importance of specialized training on code-switched data. Addressing these challenges is crucial for improving the performance of LLMs in handling code-switched text. 

<br /><br />Summary: <div>
arXiv:2505.13559v1 Announce Type: new 
Abstract: Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning</title>
<link>https://arxiv.org/abs/2505.13628</link>
<guid>https://arxiv.org/abs/2505.13628</guid>
<content:encoded><![CDATA[
<div> Keywords: multilingual alignment, sentence representations, visual information, low-resource languages, cross-lingual Natural Language Understanding

Summary:
Multilingual alignment of sentence representations has traditionally required bitexts to bridge language gaps. However, this study explores the potential of using visual information instead of bitexts for alignment. By leveraging image caption datasets, which are easier to create and do not require multilingual expertise, the study demonstrates that multilingual alignment via image-caption association can effectively align text representations across languages. The research shows that languages not encountered during pretraining can be integrated post-hoc into this alignment process. Furthermore, the aligned representations are suitable for cross-lingual Natural Language Understanding (NLU) and retrieval of bitexts. This approach presents a promising and efficient alternative, particularly for low-resource languages, in the field of multilingual representation alignment. 

<br /><br />Summary: <div>
arXiv:2505.13628v1 Announce Type: new 
Abstract: Multilingual alignment of sentence representations has mostly required bitexts to bridge the gap between languages. We investigate whether visual information can bridge this gap instead. Image caption datasets are very easy to create without requiring multilingual expertise, so this offers a more efficient alternative for low-resource languages. We find that multilingual image-caption alignment can implicitly align the text representations between languages, languages unseen by the encoder in pretraining can be incorporated into this alignment post-hoc, and these aligned representations are usable for cross-lingual Natural Language Understanding (NLU) and bitext retrieval.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clarifying orthography: Orthographic transparency as compressibility</title>
<link>https://arxiv.org/abs/2505.13657</link>
<guid>https://arxiv.org/abs/2505.13657</guid>
<content:encoded><![CDATA[
<div> Algorithmic information theory, orthographic transparency, mutual compressibility, neural sequence models, script types  
<br />  
Orthographic transparency, the relationship between spelling and sound, lacks a standardized metric. This study uses algorithmic information theory to quantify transparency by measuring the mutual compressibility of orthographic and phonological strings. The measure combines irregular spellings and rule complexity into one quantity. Estimating transparency with prequential code-lengths from neural sequence models, the study evaluates 22 languages across diverse script types to confirm common intuitions about script transparency. Mutual compressibility provides a simple, principled, and general measure for orthographic transparency.  
<br /><br />Summary:  
- Orthographic transparency lacks a unified metric  
- Algorithmic information theory is used to quantify transparency  
- Compressibility of orthographic and phonological strings is measured  
- Transparency measure considers irregular spellings and rule complexity  
- Prequential code-lengths from neural sequence models estimate transparency across languages and script types  <div>
arXiv:2505.13657v1 Announce Type: new 
Abstract: Orthographic transparency -- how directly spelling is related to sound -- lacks a unified, script-agnostic metric. Using ideas from algorithmic information theory, we quantify orthographic transparency in terms of the mutual compressibility between orthographic and phonological strings. Our measure provides a principled way to combine two factors that decrease orthographic transparency, capturing both irregular spellings and rule complexity in one quantity. We estimate our transparency measure using prequential code-lengths derived from neural sequence models. Evaluating 22 languages across a broad range of script types (alphabetic, abjad, abugida, syllabic, logographic) confirms common intuitions about relative transparency of scripts. Mutual compressibility offers a simple, principled, and general yardstick for orthographic transparency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Large Language Models Good at Detecting Propaganda?</title>
<link>https://arxiv.org/abs/2505.13706</link>
<guid>https://arxiv.org/abs/2505.13706</guid>
<content:encoded><![CDATA[
<div> Keywords: Propaganda techniques, Natural Language Processing, Large Language Models, Transformer-based models, Detection performance

Summary: 
- The study analyzes the performance of Large Language Models (LLMs) in detecting propaganda techniques in news articles.
- Different LLMs like GPT-3.5, GPT-4, and Claude 3 Opus are compared, with GPT-4 showing better F1 scores but not outperforming a RoBERTa-CRF baseline.
- All three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting propaganda techniques, especially in instances of name-calling, appeal to fear, and flag-waving.
- The study highlights the importance of recognizing manipulative content in news articles and the role of NLP in enhancing detection capabilities.
- The findings suggest that while advancements in LLMs show promise in detecting propaganda techniques, there is still room for improvement in achieving higher accuracy levels. 

<br /><br />Summary: <div>
arXiv:2505.13706v1 Announce Type: new 
Abstract: Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.13725</link>
<guid>https://arxiv.org/abs/2505.13725</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language models, text-to-SQL reasoning, SQLForge, data synthesis, data diversity 

Summary: 
SQLForge introduces a novel approach to enhancing text-to-SQL reasoning in Large Language models (LLMs). By synthesizing reliable data with SQL syntax constraints and reverse translation, data reliability is improved at both structural and semantic levels. An SQL template enrichment and iterative data domain exploration mechanism is proposed to enhance data diversity. Open-source models are fine-tuned with the augmented data, resulting in the creation of SQLForge-LM models. These models achieve state-of-the-art performance on Spider and BIRD benchmarks, narrowing the performance gap with closed-source methods. SQLForge-LM attains an EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, showcasing significant progress in text-to-SQL reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2505.13725v1 Announce Type: new 
Abstract: Large Language models (LLMs) have demonstrated significant potential in text-to-SQL reasoning tasks, yet a substantial performance gap persists between existing open-source models and their closed-source counterparts. In this paper, we introduce SQLForge, a novel approach for synthesizing reliable and diverse data to enhance text-to-SQL reasoning in LLMs. We improve data reliability through SQL syntax constraints and SQL-to-question reverse translation, ensuring data logic at both structural and semantic levels. We also propose an SQL template enrichment and iterative data domain exploration mechanism to boost data diversity. Building on the augmented data, we fine-tune a variety of open-source models with different architectures and parameter sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves the state-of-the-art performance on the widely recognized Spider and BIRD benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing the performance gap with closed-source methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making</title>
<link>https://arxiv.org/abs/2505.13761</link>
<guid>https://arxiv.org/abs/2505.13761</guid>
<content:encoded><![CDATA[
<div> framework, simulation agent, large language models, interactions, real-world dynamics <br />
Summary: 
The article introduces a novel simulation agent framework that combines the strengths of simulation models and large language models (LLMs) to facilitate interactions with complex real-world systems. The framework enables non-technical users to engage with simulations through intuitive language-based interactions, bridging the gap between complexity and accessibility. By integrating LLMs' conversational capabilities with the structured understanding of simulations, the framework offers a robust foundation for accurately modeling diverse real-world phenomena. This approach aims to empower users by providing a seamless and interactive way to explore and validate complex systems, enhancing the applicability and generalizability of simulations across various domains. <div>
arXiv:2505.13761v1 Announce Type: new 
Abstract: Simulations, although powerful in accurately replicating real-world systems, often remain inaccessible to non-technical users due to their complexity. Conversely, large language models (LLMs) provide intuitive, language-based interactions but can lack the structured, causal understanding required to reliably model complex real-world dynamics. We introduce our simulation agent framework, a novel approach that integrates the strengths of both simulation models and LLMs. This framework helps empower users by leveraging the conversational capabilities of LLMs to interact seamlessly with sophisticated simulation systems, while simultaneously utilizing the simulations to ground the LLMs in accurate and structured representations of real-world phenomena. This integrated approach helps provide a robust and generalizable foundation for empirical validation and offers broad applicability across diverse domains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Krikri: Advancing Open Large Language Models for Greek</title>
<link>https://arxiv.org/abs/2505.13772</link>
<guid>https://arxiv.org/abs/2505.13772</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Greek language, Llama-Krikri-8B, Training, Evaluation <br />
Summary:<br />
The paper introduces Llama-Krikri-8B, a state-of-the-art Large Language Model designed specifically for the Greek language. With a vast parameter size of 8 billion, Llama-Krikri-8B has undergone extensive training on high-quality Greek data to ensure a deep understanding of linguistic nuances. It supports Modern Greek, English, polytonic text, and even Ancient Greek. The model's chat version employs a multi-stage post-training pipeline, incorporating human and synthetic instruction data using techniques like MAGPIE. Three new public benchmarks for Greek evaluation have been proposed. Results show significant advancements over existing Greek and multilingual LLMs in natural language understanding, generation, and code generation. Llama-Krikri-8B combines advanced capabilities with efficient computational performance, making it a valuable asset for Greek language processing tasks. <br /><br />Summary: <div>
arXiv:2505.13772v1 Announce Type: new 
Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been extensively trained on high-quality Greek data to ensure superior adaptation to linguistic nuances. With 8 billion parameters, it offers advanced capabilities while maintaining efficient computational performance. Llama-Krikri-8B supports both Modern Greek and English, and is also equipped to handle polytonic text and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage post-training pipeline, utilizing both human and synthetic instruction and preference data, by applying techniques such as MAGPIE. In addition, for evaluation, we propose three novel public benchmarks for Greek. Our evaluation on existing as well as the proposed benchmarks shows notable improvements over comparable Greek and multilingual LLMs in both natural language understanding and generation as well as code generation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.13792</link>
<guid>https://arxiv.org/abs/2505.13792</guid>
<content:encoded><![CDATA[
<div> Knowledge Distillation, Question Answering, Smaller Language Models, Reasoning Traces, Faithfulness

Summary:
- The article addresses the challenge of evaluating the faithfulness of reasoning traces in Knowledge Distillation (KD) for improving the performance of smaller language models (SLMs) in Question Answering (QA).
- Using a rule-based problem decomposition approach, the authors break down complex queries into structured sub-problems, making the traces more interpretable and easy to evaluate.
- Experimenting on various QA datasets, the study reveals that correct reasoning traces do not always lead to correct final solutions, highlighting a disconnect between intermediate trace correctness and final solution accuracy.
- The findings challenge the effectiveness of using reasoning traces for improving SLM performance through KD, suggesting a low correlation between correct traces and final solution accuracy.
- By decomposing the QA problem into Classification and Information Retrieval steps, the study aims to simplify trace evaluation and enhance the transparency and interpretability of the reasoning process in dialogue systems. 

<br /><br />Summary: <div>
arXiv:2505.13792v1 Announce Type: new 
Abstract: Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientLLM: Efficiency in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13840</link>
<guid>https://arxiv.org/abs/2505.13840</guid>
<content:encoded><![CDATA[
<div> EfficientLLM, benchmark, Large Language Models, efficiency techniques, scale<br />
Summary:<br />
EfficientLLM introduces a novel benchmark and conducts a comprehensive empirical study on efficiency techniques for Large Language Models (LLMs). The study explores architecture pretraining, fine-tuning, and inference methods on a production-class cluster. Three key insights are derived: efficiency involves trade-offs, optima are task- and scale-dependent, and techniques generalize across modalities. Various methods are evaluated, showcasing quantifiable trade-offs and task-specific optimal solutions. For example, Mixture-of-Experts reduces FLOPs and improves accuracy, but increases VRAM usage significantly. Different methods like MQA and MLA show optimal trade-offs depending on the task and model scale. Techniques are also found to generalize across modalities, as evaluations on Vision Models and Vision-Language Models confirm effective transferability. EfficientLLM provides essential guidance for navigating the efficiency-performance landscape of next-generation foundation models. <br /> <div>
arXiv:2505.13840v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improve Language Model and Brain Alignment via Associative Memory</title>
<link>https://arxiv.org/abs/2505.13844</link>
<guid>https://arxiv.org/abs/2505.13844</guid>
<content:encoded><![CDATA[
<div> Keywords: associative memory, language models, brain alignment, comprehension, supervised fine-tuning

Summary:
Associative memory plays a crucial role in integrating relevant information for comprehension in human cognition. Researchers aim to enhance the alignment between language models and the human brain during speech information processing by incorporating associative memory. By mapping language model activations to brain activity, they validate improvements in alignment in brain regions associated with associative memory processing. Additionally, the study shows that large language models exhibit enhanced alignment with brain response following specific supervised fine-tuning. To facilitate this investigation, the researchers developed the "Association" dataset, featuring stories that prompt associative memory input and associated content as output. Through these efforts, the study contributes to a better understanding of how language models can be optimized to align more closely with the brain's processing mechanisms. 

<br /><br />Summary: <div>
arXiv:2505.13844v1 Announce Type: new 
Abstract: Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computational language models. We find the alignment between language model and brain is improved in brain regions closely related to associative memory processing. We also demonstrate large language models after specific supervised fine-tuning better align with brain response, by building the \textit{Association} dataset containing 1000 samples of stories, with instructions encouraging associative memory as input and associated content as output.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain Gating Ensemble Networks for AI-Generated Text Detection</title>
<link>https://arxiv.org/abs/2505.13855</link>
<guid>https://arxiv.org/abs/2505.13855</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, text detection, domain adaptation, ensemble networks, AI detection <br />
<br />
Summary: 
The paper introduces DoGEN, a technique designed to enhance the detection of machine-generated text in various domains. DoGEN, which stands for Domain Gating Ensemble Networks, utilizes a combination of domain expert detector models and weights from a domain classifier to adapt to unseen domains. Testing DoGEN on diverse domains demonstrates its superior performance in in-domain detection and outperforms larger models in out-of-domain detection. The research offers code and trained models for further exploration in domain-adaptive AI detection. This innovative approach addresses the growing need for robust text detection in the era of advancing language models, providing a valuable resource for researchers and practitioners working on machine text detection.  <br /> <div>
arXiv:2505.13855v1 Announce Type: new 
Abstract: As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.13866</link>
<guid>https://arxiv.org/abs/2505.13866</guid>
<content:encoded><![CDATA[
<div> compression, inference, reasoning, language models, semantic sparsity

Summary:
Reasoning-focused language models often generate long reasoning paths for accurate answers, but this approach can lead to high memory usage and slow token generation. The Reasoning Path Compression (RPC) method introduced in this study aims to accelerate inference by compressing the Key-Value (KV) cache based on the semantic sparsity of reasoning paths. By retaining high importance score KV cache using a selector window of recent queries, RPC can improve generation throughput of models like QwQ-32B by up to 1.60$\times$ without significant accuracy drop. The experiments conducted on the AIME 2024 benchmark demonstrate that RPC effectively leverages semantic sparsity in reasoning traces for compression, offering a practical solution for deploying reasoning language models efficiently. The code for RPC is available on GitHub, making it accessible for further research and implementation. 

<br /><br />Summary: <div>
arXiv:2505.13866v1 Announce Type: new 
Abstract: Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning</title>
<link>https://arxiv.org/abs/2505.13886</link>
<guid>https://arxiv.org/abs/2505.13886</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual-language Chain-of-Thought, VLMs, Code2Logic, GameQA dataset, multimodal reasoning 

Summary:
Visual-language Chain-of-Thought data is limited, hindering reasoning capabilities in Vision Language Models (VLMs). Code2Logic leverages game code to automatically synthesize reasoning data, resulting in the GameQA dataset. This dataset is cost-effective, challenging for models, and diverse. VLMs trained on GameQA showed out-of-domain generalization, improving performance on diverse benchmarks. The Code2Logic approach and GameQA dataset are available on GitHub for research purposes. <br /><br />Summary: Visual-language Chain-of-Thought data scarcity hinders VLM reasoning improvements. Code2Logic uses game code to synthesize GameQA dataset, cost-effective and challenging for models. VLMs trained on GameQA show out-of-domain generalization, improving performance on diverse benchmarks. Code2Logic approach and GameQA dataset available on GitHub. <div>
arXiv:2505.13886v1 Announce Type: new 
Abstract: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable to produce, challenging for state-of-the-art models, and diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse vision-language benchmarks. Our code and dataset are available at https://github.com/tongjingqi/Code2Logic.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM</title>
<link>https://arxiv.org/abs/2505.13890</link>
<guid>https://arxiv.org/abs/2505.13890</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reasoning, Graph-based Framework, Prompt Engineering, Cognitive Analysis

Summary: 
This study introduces a new analytical framework for modeling the reasoning processes of Reasoning Large Language Models (RLMs). By clustering CoT outputs into coherent reasoning steps and building reasoning graphs to capture dependencies, the framework enhances understanding of RLM behaviors. Structural properties of reasoning graphs, such as exploration density and convergence ratios, strongly influence reasoning accuracy. The study shows how prompting strategies impact the internal reasoning structure of RLMs, affecting task outcomes. The framework enables quantitative evaluation of reasoning quality and offers insights for prompt engineering and cognitive analysis of LLMs. The release of code and resources will support further research in this area. 

<br /><br />Summary: <div>
arXiv:2505.13890v1 Announce Type: new 
Abstract: Recent advances in test-time scaling have enabled Large Language Models (LLMs) to display sophisticated reasoning abilities via extended Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as performance degradation under few-shot prompting, that challenge our current understanding of RLMs. In this work, we introduce a unified graph-based analytical framework for better modeling the reasoning processes of RLMs. Our method first clusters long, verbose CoT outputs into semantically coherent reasoning steps, then constructs directed reasoning graphs to capture contextual and logical dependencies among these steps. Through comprehensive analysis across models and prompting regimes, we reveal that structural properties, such as exploration density, branching, and convergence ratios, strongly correlate with reasoning accuracy. Our findings demonstrate how prompting strategies substantially reshape the internal reasoning structure of RLMs, directly affecting task outcomes. The proposed framework not only enables quantitative evaluation of reasoning quality beyond conventional metrics but also provides practical insights for prompt engineering and the cognitive analysis of LLMs. Code and resources will be released to facilitate future research in this direction.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion</title>
<link>https://arxiv.org/abs/2505.13893</link>
<guid>https://arxiv.org/abs/2505.13893</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, fusion methods, semantic dependencies, Graph-on-Logits Distillation, inference efficiency

Summary: 
In this paper, the authors introduce InfiGFusion, a novel framework for fusing heterogeneous open-source models into a unified system. By incorporating the Graph-on-Logits Distillation (GLD) loss, InfiGFusion allows for the explicit modeling of semantic dependencies between token types, improving fusion quality and stability. The framework retains the top-$k$ logits per output and aggregates their outer products to create a global co-activation graph, capturing vocabulary channel interactions. A sorting-based approximation reduces computational complexity, making the method scalable and efficient. Experimental results show that InfiGFusion outperforms state-of-the-art models and fusion baselines across various benchmarks, particularly excelling in complex reasoning tasks such as Multistep Arithmetic and Causal Judgement. This demonstrates InfiGFusion's superiority in multi-step and relational inference tasks. 

<br /><br />Summary: <div>
arXiv:2505.13893v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have intensified efforts to fuse heterogeneous open-source models into a unified system that inherits their complementary strengths. Existing logit-based fusion methods maintain inference efficiency but treat vocabulary dimensions independently, overlooking semantic dependencies encoded by cross-dimension interactions. These dependencies reflect how token types interact under a model's internal reasoning and are essential for aligning models with diverse generation behaviors. To explicitly model these dependencies, we propose \textbf{InfiGFusion}, the first structure-aware fusion framework with a novel \textit{Graph-on-Logits Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output and aggregate their outer products across sequence positions to form a global co-activation graph, where nodes represent vocabulary channels and edges quantify their joint activations. To ensure scalability and efficiency, we design a sorting-based closed-form approximation that reduces the original $O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable approximation guarantees. Experiments across multiple fusion settings show that GLD consistently improves fusion quality and stability. InfiGFusion outperforms SOTA models and fusion baselines across 11 benchmarks spanning reasoning, coding, and mathematics. It shows particular strength in complex reasoning tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal Judgement over SFT, demonstrating superior multi-step and relational inference.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let's Verify Math Questions Step by Step</title>
<link>https://arxiv.org/abs/2505.13903</link>
<guid>https://arxiv.org/abs/2505.13903</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Math Question Verification, mathematical reasoning, dataset, code

Summary: 
Math Question Verification (MathQ-Verify) is a novel pipeline designed to filter out ill-posed or under-specified math problems by validating question formats, formalizing questions, detecting logical contradictions, and ensuring completeness. The pipeline improves F1 scores by up to 25 percentage points over baseline methods, achieving approximately 90% precision and 63% recall through a model voting scheme. With an additional dataset of 2,147 manually validated math questions, MathQ-Verify offers a scalable and accurate solution for curating reliable mathematical datasets. The code and data for MathQ-Verify are available on GitHub, providing a valuable resource for improving the quality of math question datasets used for training large language models. <br /><br />Summary: <div>
arXiv:2505.13903v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently achieved remarkable progress in mathematical reasoning. To enable such capabilities, many existing works distill strong reasoning models into long chains of thought or design algorithms to construct high-quality math QA data for training. However, these efforts primarily focus on generating correct reasoning paths and answers, while largely overlooking the validity of the questions themselves. In this work, we propose Math Question Verification (MathQ-Verify), a novel five-stage pipeline designed to rigorously filter ill-posed or under-specified math problems. MathQ-Verify first performs format-level validation to remove redundant instructions and ensure that each question is syntactically well-formed. It then formalizes each question, decomposes it into atomic conditions, and verifies them against mathematical definitions. Next, it detects logical contradictions among these conditions, followed by a goal-oriented completeness check to ensure the question provides sufficient information for solving. To evaluate this task, we use existing benchmarks along with an additional dataset we construct, containing 2,147 math questions with diverse error types, each manually double-validated. Experiments show that MathQ-Verify achieves state-of-the-art performance across multiple benchmarks, improving the F1 score by up to 25 percentage points over the direct verification baseline. It further attains approximately 90% precision and 63% recall through a lightweight model voting scheme. MathQ-Verify offers a scalable and accurate solution for curating reliable mathematical datasets, reducing label noise and avoiding unnecessary computation on invalid questions. Our code and data are available at https://github.com/scuuy/MathQ-Verify.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology</title>
<link>https://arxiv.org/abs/2505.13908</link>
<guid>https://arxiv.org/abs/2505.13908</guid>
<content:encoded><![CDATA[
<div> transfer, multilingual NLP, language families, morphology, cross-linguistic transfer<br />
Summary:<br />
This paper explores the significance of cross-lingual transfer in multilingual NLP, focusing on the impact of language families and morphology on model performance. It investigates the relationship between language family proximity and morphological similarity on NLP tasks, comparing multilingual model performance and examining how linguistic distance metrics correlate with transfer outcomes. The study also discusses the integration of typological and morphological information into model pre-training to enhance transfer to diverse languages. Overall, the research underscores the importance of considering linguistic factors in cross-linguistic transfer and highlights the potential for leveraging language family and morphological similarities to improve model performance across languages. <div>
arXiv:2505.13908v1 Announce Type: new 
Abstract: Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it allows for models trained on resource-rich languages to be applied to low-resource languages more effectively. Recently massively multilingual pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot transfer capabilities[14] [13]. This paper investigates cross-linguistic transfer through the lens of language families and morphology. Investigating how language family proximity and morphological similarity affect performance across NLP tasks. We further discuss our results and how it relates to findings from recent literature. Overall, we compare multilingual model performance and review how linguistic distance metrics correlate with transfer outcomes. We also look into emerging approaches that integrate typological and morphological information into model pre-training to improve transfer to diverse languages[18] [19].
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Word length predicts word order: "Min-max"-ing drives language evolution</title>
<link>https://arxiv.org/abs/2505.13913</link>
<guid>https://arxiv.org/abs/2505.13913</guid>
<content:encoded><![CDATA[
<div> word order change, processing mechanisms, information structure, language evolution, crosslinguistic correlation
Summary: This paper proposes a universal underlying mechanism for word order change based on a large dataset of over 1,500 languages. It suggests that word class length is significantly correlated with word order crosslinguistically, supporting competing theories of processing. The results predict historical word order change in two different phylogenetic lines and explain more variance than descent or language area in regression models. The findings suggest an integrated "Min-Max" theory of language evolution driven by competing pressures of processing and information structure, aligning with recent efficiency-oriented and information-theoretic proposals. This research contributes to the understanding of language evolution by highlighting the importance of both internal processing mechanisms and external information structure in shaping linguistic patterns. <br /><br /> <div>
arXiv:2505.13913v1 Announce Type: new 
Abstract: Current theories of language propose an innate (Baker 2001; Chomsky 1981) or a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface structures (i.e. word order) that we observe in languages of the world, while evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary factor influencing such patterns. Although there are hypotheses for word order change from both innate and usage-based perspectives for specific languages and families, there are key disagreements between the two major proposals for mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy 2008). This paper proposes a universal underlying mechanism for word order change based on a large tagged parallel dataset of over 1,500 languages representing 133 language families and 111 isolates. Results indicate that word class length is significantly correlated with word order crosslinguistically, but not in a straightforward manner, partially supporting opposing theories of processing, while at the same time predicting historical word order change in two different phylogenetic lines and explaining more variance than descent or language area in regression models. Such findings suggest an integrated "Min-Max" theory of language evolution driven by competing pressures of processing and information structure, aligning with recent efficiency-oriented (Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et al. 2025).
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EEG-to-Text Translation: A Model for Deciphering Human Brain Activity</title>
<link>https://arxiv.org/abs/2505.13936</link>
<guid>https://arxiv.org/abs/2505.13936</guid>
<content:encoded><![CDATA[
<div> Keywords: language models, EEG signals, text decoding, R1 Translator, performance metrics

Summary:<br /><br />
The article introduces a new model, R1 Translator, designed to improve the performance of decoding EEG signals into text. By combining a bidirectional LSTM encoder with a transformer-based decoder, the model effectively utilizes EEG features to generate high-quality text outputs. R1 Translator outperforms previous models T5 and Brain Translator in various performance metrics such as ROUGE metrics, achieving higher scores in ROUGE-1, ROUGE-L, CER, and WER. The model demonstrates a significant improvement in decoding accuracy, with a ROUGE-1 score of 38.00%, surpassing T5 by 9% and Brain Translator by 3%. R1 Translator also achieves lower CER and WER scores compared to the other models, further highlighting its superior performance in EEG-to-text decoding tasks. The code for the R1 Translator model is available on GitHub for further exploration and use.<br /><br />Summary: <div>
arXiv:2505.13936v1 Announce Type: new 
Abstract: With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting</title>
<link>https://arxiv.org/abs/2505.13944</link>
<guid>https://arxiv.org/abs/2505.13944</guid>
<content:encoded><![CDATA[
<div> Keywords: Continual Relation Extraction, Prompt-based methods, Task-specific prompt pools, Label descriptions, Generative model

Summary:
WAVE++ introduces a novel approach for Continual Relation Extraction (CRE) that overcomes key challenges faced by current prompt-based techniques. The model utilizes task-specific prompt pools to enhance adaptability across diverse tasks, effectively capturing variations within and across tasks. By incorporating label descriptions, the model gains richer context for relation classification. WAVE++ also includes a training-free mechanism for improved task prediction during inference. Additionally, a generative model is integrated to consolidate prior knowledge within shared parameters, eliminating the need for explicit data storage. Extensive experiments show that WAVE++ outperforms existing methods, offering a more robust solution for CRE. This innovative approach provides a promising direction for continual relation extraction tasks. <br /><br />Summary: <div>
arXiv:2505.13944v1 Announce Type: new 
Abstract: Memory-based approaches have shown strong performance in Continual Relation Extraction (CRE). However, storing examples from previous tasks increases memory usage and raises privacy concerns. Recently, prompt-based methods have emerged as a promising alternative, as they do not rely on storing past samples. Despite this progress, current prompt-based techniques face several core challenges in CRE, particularly in accurately identifying task identities and mitigating catastrophic forgetting. Existing prompt selection strategies often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in shared parameters, and struggle to handle both cross-task and within-task variations. In this paper, we propose WAVE++, a novel approach inspired by the connection between prefix-tuning and mixture of experts. Specifically, we introduce task-specific prompt pools that enhance flexibility and adaptability across diverse tasks while avoiding boundary-spanning risks; this design more effectively captures variations within each task and across tasks. To further refine relation classification, we incorporate label descriptions that provide richer, more global context, enabling the model to better distinguish among different relations. We also propose a training-free mechanism to improve task prediction during inference. Moreover, we integrate a generative model to consolidate prior knowledge within the shared parameters, thereby removing the need for explicit data storage. Extensive experiments demonstrate that WAVE++ outperforms state-of-the-art prompt-based and rehearsal-based methods, offering a more robust solution for continual relation extraction. Our code is publicly available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memory-Centric Embodied Question Answer</title>
<link>https://arxiv.org/abs/2505.13948</link>
<guid>https://arxiv.org/abs/2505.13948</guid>
<content:encoded><![CDATA[
<div> MemoryEQA, embodied question answering, memory-centric framework, multi-modal hierarchical memory mechanism, MT-HM3D dataset <br />
<br />
Summary: MemoryEQA is a new framework for Embodied Question Answering that focuses on memory capabilities. Unlike traditional planner-centric models, MemoryEQA allows for flexible interaction between memory and other modules, resulting in improved efficiency and accuracy in complex tasks. The framework incorporates a multi-modal hierarchical memory mechanism, including global memory for storing scene information and local memory for retaining historical data. Memory information is converted using a large language model and injected into different modules for EQA tasks. The MT-HM3D dataset was created to evaluate memory capabilities, with experimental results showing a significant performance gain compared to baseline models. This highlights the importance of memory in navigating tasks involving multiple targets across different regions. <div>
arXiv:2505.13948v1 Announce Type: new 
Abstract: Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlashThink: An Early Exit Method For Efficient Reasoning</title>
<link>https://arxiv.org/abs/2505.13949</link>
<guid>https://arxiv.org/abs/2505.13949</guid>
<content:encoded><![CDATA[
<div> Large Language Models (LLMs), reasoning tasks, computational overhead, efficient reasoning, FlashThink<br />
Summary:<br />
Large Language Models (LLMs) have excelled in reasoning tasks, but they often produce overly long reasoning content, leading to high computational costs. This study proposes FlashThink, a verification model that determines the optimal point to stop the reasoning process without sacrificing accuracy. Experimental results on four benchmarks show that FlashThink effectively reduces reasoning content length without compromising model performance. For models Deepseek-R1 and QwQ-32B, the reasoning content was cut by over 77% without accuracy loss. This approach enables more efficient reasoning processes in large language models. <div>
arXiv:2505.13949v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability</title>
<link>https://arxiv.org/abs/2505.13963</link>
<guid>https://arxiv.org/abs/2505.13963</guid>
<content:encoded><![CDATA[
<div> Explainability, Interpretability, Quantization, Language Models, Transparency  
Summary:  
- Quantization methods are commonly used to speed up inference in large language models (LLMs) but their impact on model explainability and interpretability is not well understood.  
- The study conducted experiments using different quantization techniques at various bit widths along with explainability and interpretability methods to analyze their effects.  
- Results showed that the impact of quantization on model explainability and interpretability varied depending on factors like the quantization method and evaluation protocol.  
- Quantization could either improve or degrade model transparency, highlighting the unpredictable nature of this effect.  
- The findings from the study serve as a warning that quantization can have unforeseen consequences on model transparency, which is crucial for applications requiring it.  
<br /><br />Summary: <div>
arXiv:2505.13963v1 Announce Type: new 
Abstract: Quantization methods are widely used to accelerate inference and streamline the deployment of large language models (LLMs). While prior research has extensively investigated the degradation of various LLM capabilities due to quantization, its effects on model explainability and interpretability, which are crucial for understanding decision-making processes, remain unexplored. To address this gap, we conduct comprehensive experiments using three common quantization techniques at distinct bit widths, in conjunction with two explainability methods, counterfactual examples and natural language explanations, as well as two interpretability approaches, knowledge memorization analysis and latent multi-hop reasoning analysis. We complement our analysis with a thorough user study, evaluating selected explainability methods. Our findings reveal that, depending on the configuration, quantization can significantly impact model explainability and interpretability. Notably, the direction of this effect is not consistent, as it strongly depends on (1) the quantization method, (2) the explainability or interpretability approach, and (3) the evaluation protocol. In some settings, human evaluation shows that quantization degrades explainability, while in others, it even leads to improvements. Our work serves as a cautionary tale, demonstrating that quantization can unpredictably affect model transparency. This insight has important implications for deploying LLMs in applications where transparency is a critical requirement.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring</title>
<link>https://arxiv.org/abs/2505.13965</link>
<guid>https://arxiv.org/abs/2505.13965</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated Essay Scoring, multimodal assessments, CAFES framework, collaborative multi-agent, MLLMs

Summary:<br />
Automated Essay Scoring (AES) is vital in modern education, especially with the rise of multimodal assessments. Traditional AES methods face challenges in generalizability and multimodal perception. The new CAFES framework addresses these limitations by employing three specialized agents: Initial Scorer, Feedback Pool Manager, and Reflective Scorer. Through collaboration, CAFES significantly improves human alignment and achieves a 21% relative improvement in Quadratic Weighted Kappa (QWK) scores compared to ground truth. Experiments using state-of-the-art MLLMs demonstrate enhanced evaluations, particularly in grammatical and lexical diversity. The proposed framework sets the foundation for an intelligent multimodal AES system, offering a promising solution for accurate and comprehensive essay assessment. The code for CAFES will be released upon acceptance. 

Summary: <br /> <div>
arXiv:2505.13965v1 Announce Type: new 
Abstract: Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals</title>
<link>https://arxiv.org/abs/2505.13972</link>
<guid>https://arxiv.org/abs/2505.13972</guid>
<content:encoded><![CDATA[
<div> counterfactual examples, large language models, counterfactual data augmentation, judge models, label flipping<br />
Summary:<br />
- Counterfactual examples are used to improve large language models (LLMs) through counterfactual data augmentation (CDA).
- The choice of judge model for evaluating label flipping in CDA leads to inconsistent results.
- Four types of relationships between the counterfactual generator and judge models are defined.
- Judge models independent of the generator model provide more reliable label flipping evaluations.
- Despite the effectiveness of certain judge models, there remains a significant gap compared to results from a user study, indicating a need for human intervention in the CDA pipeline. <br /> <div>
arXiv:2505.13972v1 Announce Type: new 
Abstract: Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, five generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13973</link>
<guid>https://arxiv.org/abs/2505.13973</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Multimodal Large Language Models, medical visual question answering, fine-tuning, Group Relative Policy Optimization

Summary:
Reinforcement learning-based tuning has revolutionized Multimodal Large Language Models, particularly in medical tasks. The study investigates four key dimensions in applying RL-based tuning in medical visual question answering: the base model initialization strategy, medical semantic alignment, the impact of length-based rewards on reasoning, and the influence of bias. Extensive experiments reveal domain-specific fine-tuning insights and indicate that GRPO-based RL tuning outperforms standard supervised fine-tuning in accuracy and reasoning quality consistently. <div>
arXiv:2505.13973v1 Announce Type: new 
Abstract: Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.13975</link>
<guid>https://arxiv.org/abs/2505.13975</guid>
<content:encoded><![CDATA[
<div> skill-aware step decomposition, content pruning, distillation, efficient reasoning, mathematical reasoning 

Summary:
- Large Reasoning Models (LRMs) excel in complex reasoning tasks but often produce verbose reasoning traces, leading to inefficiency.
- The Distilled Reasoning Pruning (DRP) framework combines inference-time pruning and distillation to enhance efficiency without compromising accuracy.
- DRP uses a teacher model for skill-aware step decomposition and content pruning, followed by knowledge distillation into a student model, resulting in improved token efficiency.
- Models trained with DRP show significant reductions in token usage while maintaining or improving accuracy on challenging mathematical reasoning datasets.
- Aligning the reasoning structure of training CoTs with the student's capability is crucial for successful knowledge transfer and performance enhancements.  

<br /><br />Summary: <div>
arXiv:2505.13975v1 Announce Type: new 
Abstract: While Large Reasoning Models (LRMs) have demonstrated success in complex reasoning tasks through long chain-of-thought (CoT) reasoning, their inference often involves excessively verbose reasoning traces, resulting in substantial inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a hybrid framework that combines inference-time pruning with tuning-based distillation, two widely used strategies for efficient reasoning. DRP uses a teacher model to perform skill-aware step decomposition and content pruning, and then distills the pruned reasoning paths into a student model, enabling it to reason both efficiently and accurately. Across several challenging mathematical reasoning datasets, we find that models trained with DRP achieve substantial improvements in token efficiency without sacrificing accuracy. Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on AIME with no performance drop. Further analysis shows that aligning the reasoning structure of training CoTs with the student's reasoning capacity is critical for effective knowledge transfer and performance gains.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection</title>
<link>https://arxiv.org/abs/2505.13979</link>
<guid>https://arxiv.org/abs/2505.13979</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal models, empathy detection, conflicting cues, fine-tuned models, gated fusion model

Summary:
Multimodal models are crucial for empathy detection but can struggle when modalities provide conflicting signals. This study investigates cases where unimodal and multimodal predictions differ and finds that disagreements often stem from underlying ambiguity and annotator uncertainty. The analysis reveals that relying on dominant signals in one modality can mislead fusion when unsupported by others. Moreover, both models and humans do not consistently benefit from multimodal input. These findings suggest that disagreement can serve as a valuable cue for identifying challenging examples and enhancing the robustness of empathy systems. Ultimately, understanding the complexities of multimodal interactions can offer insights for improving empathy detection algorithms and addressing the limitations of current approaches. 

<br /><br />Summary: <div>
arXiv:2505.13979v1 Announce Type: new 
Abstract: Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hallucination Tax of Reinforcement Finetuning</title>
<link>https://arxiv.org/abs/2505.13988</link>
<guid>https://arxiv.org/abs/2505.13988</guid>
<content:encoded><![CDATA[
<div> finetuning, reinforcement, language models, hallucination tax, refusal behavior<br />
<br />
Reinforcement finetuning (RFT) is commonly used to improve large language models' reasoning abilities. However, a side effect known as the hallucination tax can lead these models to confidently provide incorrect answers to unanswerable questions. The study introduces a dataset called SUM (Synthetic Unanswerable Math) to assess models' ability to recognize unanswerable math problems. Results show that RFT can significantly reduce refusal rates, increasing the likelihood of model hallucinations. Incorporating just 10% of SUM during RFT helps restore proper refusal behavior without sacrificing accuracy on solvable tasks. This approach allows language models to better understand their uncertainty and knowledge boundaries, leading to improved generalization for both math problems and factual question answering tasks.<br /><br />Summary: <div>
arXiv:2505.13988v1 Announce Type: new 
Abstract: Reinforcement finetuning (RFT) has become a standard approach for enhancing the reasoning capabilities of large language models (LLMs). However, its impact on model trustworthiness remains underexplored. In this work, we identify and systematically study a critical side effect of RFT, which we term the hallucination tax: a degradation in refusal behavior causing models to produce hallucinated answers to unanswerable questions confidently. To investigate this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of unanswerable math problems designed to probe models' ability to recognize an unanswerable question by reasoning from the insufficient or ambiguous information. Our results show that standard RFT training could reduce model refusal rates by more than 80%, which significantly increases model's tendency to hallucinate. We further demonstrate that incorporating just 10% SUM during RFT substantially restores appropriate refusal behavior, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage inference-time compute to reason about their own uncertainty and knowledge boundaries, improving generalization not only to out-of-domain math problems but also to factual question answering tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DecIF: Improving Instruction-Following through Meta-Decomposition</title>
<link>https://arxiv.org/abs/2505.13990</link>
<guid>https://arxiv.org/abs/2505.13990</guid>
<content:encoded><![CDATA[
<div> Keywords: Instruction-following, large language models, DecIF, autonomous framework, data generation

Summary:
DecIF is a new framework for generating high-quality instruction-following data using only large language models (LLMs). It follows a decomposition approach, where LLMs iteratively produce meta-information to create well-structured instructions with response constraints. The framework also employs LLMs to detect and resolve inconsistencies in instructions and decompose instructions into atomic-level criteria for accurate response generation. DecIF outperforms existing methods in instruction-following tasks and exhibits strong flexibility, scalability, and generalizability in automatically generating diverse instruction data. The framework's autonomous nature eliminates the need for pre-existing documents or external resources, making it a highly efficient tool for producing instruction-following data.<br /><br />Summary: <div>
arXiv:2505.13990v1 Announce Type: new 
Abstract: Instruction-following has emerged as a crucial capability for large language models (LLMs). However, existing approaches often rely on pre-existing documents or external resources to synthesize instruction-following data, which limits their flexibility and generalizability. In this paper, we introduce DecIF, a fully autonomous, meta-decomposition guided framework that generates diverse and high-quality instruction-following data using only LLMs. DecIF is grounded in the principle of decomposition. For instruction generation, we guide LLMs to iteratively produce various types of meta-information, which are then combined with response constraints to form well-structured and semantically rich instructions. We further utilize LLMs to detect and resolve potential inconsistencies within the generated instructions. Regarding response generation, we decompose each instruction into atomic-level evaluation criteria, enabling rigorous validation and the elimination of inaccurate instruction-response pairs. Extensive experiments across a wide range of scenarios and settings demonstrate DecIF's superior performance on instruction-following tasks. Further analysis highlights its strong flexibility, scalability, and generalizability in automatically synthesizing high-quality instruction data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Social Sycophancy: A Broader Understanding of LLM Sycophancy</title>
<link>https://arxiv.org/abs/2505.13995</link>
<guid>https://arxiv.org/abs/2505.13995</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, sycophancy, face-preservation, ELEPHANT framework, dataset

Summary: The article introduces a new perspective on sycophancy in Language Model Models (LLMs), focusing on the preservation of a user's face through excessive agreement and flattery. Existing research has only considered explicit beliefs, ignoring ambiguous contexts where sycophancy can reinforce harmful assumptions. The ELEPHANT framework evaluates social sycophancy through face-preserving behaviors on two datasets. LLMs demonstrate high rates of social sycophancy, preserving face more than humans and affirming inappropriate behavior in some cases. The study shows that social sycophancy is rewarded in preference datasets and challenging to mitigate. This work provides a theoretical understanding and practical tools for addressing this crucial issue.<br /><br />Summary: The article presents a new perspective on sycophancy in LLMs, highlighting face-preservation through excessive agreement. The ELEPHANT framework evaluates social sycophancy behaviors on two datasets, showing LLMs exhibit high rates and affirm inappropriate behavior. The study reveals the rewarding nature of social sycophancy in preference datasets and the difficulty in mitigating this issue. This work offers a valuable theoretical foundation and practical tools for addressing social sycophancy in LLMs. <div>
arXiv:2505.13995v1 Announce Type: new 
Abstract: A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Activation-Guided Consensus Merging for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14009</link>
<guid>https://arxiv.org/abs/2505.14009</guid>
<content:encoded><![CDATA[
<div> Keywords: System 2, System 1, Large Language Models, Model merging, Activation-Guided Consensus Merging

Summary: 
Recent research aims to combine the reasoning abilities of System 2 with the efficiency of System 1 through model merging. A novel framework called Activation-Guided Consensus Merging (ACM) is proposed to integrate different Large Language Models (LLMs) effectively. ACM determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models, preserving task-specific capabilities without additional training. Experiments on Long-to-Short (L2S) and general merging tasks show that ACM outperforms baseline methods consistently. For example, in Qwen-7B models, TIES-Merging with ACM reduces response length by 55.3% while enhancing reasoning accuracy by 1.3 points. The code is provided for reproducibility and will be publicly available. 
<br /><br />Summary: <div>
arXiv:2505.14009v1 Announce Type: new 
Abstract: Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points. We submit the code with the paper for reproducibility, and it will be publicly available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation</title>
<link>https://arxiv.org/abs/2505.14015</link>
<guid>https://arxiv.org/abs/2505.14015</guid>
<content:encoded><![CDATA[
<div> Adversarial data generation, Legal compliance, Large language models, AutoLaw framework, Jury-based deliberation <br />
Summary: <br />
The article introduces AutoLaw, a new violation detection framework for large language models (LLMs) in law. It combines adversarial data generation with a jury-inspired deliberation process to ensure legal compliance. AutoLaw dynamically synthesizes case law to reflect local regulations and uses a pool of LLM-based "jurors" to simulate judicial decision-making. Jurors are selected based on legal expertise, minimizing bias and improving accuracy. Evaluations across three benchmarks show that AutoLaw enhances discrimination and significantly boosts violation detection rates. The framework adapts to legal misalignments and offers context-aware judgments, making it a scalable solution for evaluating and improving LLMs in legally sensitive applications. <br /> <div>
arXiv:2505.14015v1 Announce Type: new 
Abstract: The rapid advancement of domain-specific large language models (LLMs) in fields like law necessitates frameworks that account for nuanced regional legal distinctions, which are critical for ensuring compliance and trustworthiness. Existing legal evaluation benchmarks often lack adaptability and fail to address diverse local contexts, limiting their utility in dynamically evolving regulatory landscapes. To address these gaps, we propose AutoLaw, a novel violation detection framework that combines adversarial data generation with a jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike static approaches, AutoLaw dynamically synthesizes case law to reflect local regulations and employs a pool of LLM-based "jurors" to simulate judicial decision-making. Jurors are ranked and selected based on synthesized legal expertise, enabling a deliberation process that minimizes bias and improves detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG (legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness: adversarial data generation improves LLM discrimination, while the jury-based voting strategy significantly boosts violation detection rates. Our results highlight the framework's ability to adaptively probe legal misalignments and deliver reliable, context-aware judgments, offering a scalable solution for evaluating and enhancing LLMs in legally sensitive applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora</title>
<link>https://arxiv.org/abs/2505.14045</link>
<guid>https://arxiv.org/abs/2505.14045</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-way parallel data, TED Talks, multilingual performance, pretraining

Summary:
Large language models (LLMs) have been successfully scaled to low-resource languages through continued pretraining and instruction tuning on multilingual data. However, the lack of alignment in this data limits its ability to capture cross-lingual semantics effectively. This paper introduces TED2025, a vast multi-way parallel corpus based on TED Talks, spanning 113 languages with up to 50 languages aligned in parallel. The study explores methods for enhancing LLMs using multi-way parallel data, including continued pretraining and instruction tuning. Experiments on six multilingual benchmarks demonstrate that models trained on multi-way parallel data consistently outperform those trained on unaligned multilingual data. This highlights the importance of leveraging multi-way parallel data for improving multilingual performance in large language models. 

<br /><br />Summary: <div>
arXiv:2505.14045v1 Announce Type: new 
Abstract: Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Methods for Model Pruning and Knowledge Distillation</title>
<link>https://arxiv.org/abs/2505.14052</link>
<guid>https://arxiv.org/abs/2505.14052</guid>
<content:encoded><![CDATA[
<div> Keywords: Model pruning, performance optimization, knowledge distillation, MAMA Pruning, computational complexity <br />
<br />
Summary: 
Model pruning is a technique used to optimize the performance of large language models by removing unnecessary neurons and connections. Existing methods often result in performance degradation or require retraining. The proposed MAMA Pruning method, based on Movement and Magnitude Analysis, effectively reduces model size and complexity while maintaining performance levels comparable to unpruned models. This method utilizes pre-training phase weights and bias and post-training phase GRPO rewards as pruning indicators. Preliminary experiments have shown that MAMA Pruning outperforms state-of-the-art methods across various levels of pruning and different computational linguistics tasks. <div>
arXiv:2505.14052v1 Announce Type: new 
Abstract: Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs via High-Knowledge Data Selection</title>
<link>https://arxiv.org/abs/2505.14070</link>
<guid>https://arxiv.org/abs/2505.14070</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, High-Knowledge Scorer, Knowledge Density, Knowledge Coverage, Data Selection

Summary:
The paper introduces a new methodology, the High-Knowledge Scorer (HKS), to enhance the quality of training data for Large Language Models (LLMs). By focusing on the richness of knowledge within text corpora, the HKS aims to address the issue of knowledge scarcity in pre-trained datasets. The HKS utilizes a multi-domain knowledge element pool and introduces metrics such as knowledge density and coverage to evaluate the knowledge content of text data. This comprehensive approach allows for the selection of high-quality data with intensive knowledge, which can be beneficial for both generic comprehension tasks and domain-specific applications. Experimental results demonstrate that the HKS improves the model's performance in tasks that require a deep understanding of knowledge, showcasing its effectiveness in enhancing the capabilities of LLMs. <div>
arXiv:2505.14070v1 Announce Type: new 
Abstract: The performance of Large Language Models (LLMs) is intrinsically linked to the quality of its training data. Although several studies have proposed methods for high-quality data selection, they do not consider the importance of knowledge richness in text corpora. In this paper, we propose a novel and gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the dimension of knowledge, to alleviate the problem of knowledge scarcity in the pre-trained corpus. We propose a comprehensive multi-domain knowledge element pool and introduce knowledge density and coverage as metrics to assess the knowledge content of the text. Based on this, we propose a comprehensive knowledge scorer to select data with intensive knowledge, which can also be utilized for domain-specific high-knowledge data selection by restricting knowledge elements to the specific domain. We train models on a high-knowledge bilingual dataset, and experimental results demonstrate that our scorer improves the model's performance in knowledge-intensive and general comprehension tasks, and is effective in enhancing both the generic and domain-specific capabilities of the model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks</title>
<link>https://arxiv.org/abs/2505.14079</link>
<guid>https://arxiv.org/abs/2505.14079</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, planning, Minecraft, backward reasoning, goal decomposition<br />
Summary:<br />
Large language model (LLM) based agents have shown promise in completing tasks by decomposing them into easily executed steps through planning. However, forward reasoning approaches struggle with complex tasks due to a perception gap. To address this, a BAckward Reasoning based agent (BAR) is introduced in the context of Minecraft tasks. BAR utilizes backward reasoning from the terminal state, allowing for efficient planning towards the task goal in one step. It incorporates a recursive goal decomposition module, state consistency maintenance module, and stage memory module for robust and consistent planning. Experimental results demonstrate BAR's superiority over existing methods, highlighting the efficacy of the proposed modules. <div>
arXiv:2505.14079v1 Announce Type: new 
Abstract: Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent's initial state. However, this forward reasoning paradigm doesn't work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent's initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory</title>
<link>https://arxiv.org/abs/2505.14080</link>
<guid>https://arxiv.org/abs/2505.14080</guid>
<content:encoded><![CDATA[
arXiv:2505.14080v1 Announce Type: new 
Abstract: Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.
  For FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering</title>
<link>https://arxiv.org/abs/2505.14099</link>
<guid>https://arxiv.org/abs/2505.14099</guid>
<content:encoded><![CDATA[
arXiv:2505.14099v1 Announce Type: new 
Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations</title>
<link>https://arxiv.org/abs/2505.14101</link>
<guid>https://arxiv.org/abs/2505.14101</guid>
<content:encoded><![CDATA[
arXiv:2505.14101v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links or text passages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal} framed for generative text evaluation. As part of our data collection pipeline, we mined 140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents</title>
<link>https://arxiv.org/abs/2505.14104</link>
<guid>https://arxiv.org/abs/2505.14104</guid>
<content:encoded><![CDATA[
arXiv:2505.14104v1 Announce Type: new 
Abstract: Legal rules encompass not only codified statutes but also implicit adjudicatory principles derived from precedents that contain discretionary norms, social morality, and policy. While computational legal research has advanced in applying established rules to cases, inducing legal rules from judicial decisions remains understudied, constrained by limitations in model inference efficacy and symbolic reasoning capability. The advent of Large Language Models (LLMs) offers unprecedented opportunities for automating the extraction of such latent principles, yet progress is stymied by the absence of formal task definitions, benchmark datasets, and methodologies. To address this gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise, generalizable doctrinal rules from sets of analogous precedents, distilling their shared preconditions, normative behaviors, and legal consequences. We introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese cases in total) for model tuning and 216 expert-annotated gold test sets. Experimental results reveal that: 1) State-of-the-art LLMs struggle with over-generalization and hallucination; 2) Training on our dataset markedly enhances LLMs capabilities in capturing nuanced rule patterns across similar cases.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations</title>
<link>https://arxiv.org/abs/2505.14106</link>
<guid>https://arxiv.org/abs/2505.14106</guid>
<content:encoded><![CDATA[
arXiv:2505.14106v1 Announce Type: new 
Abstract: We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14107</link>
<guid>https://arxiv.org/abs/2505.14107</guid>
<content:encoded><![CDATA[
arXiv:2505.14107v1 Announce Type: new 
Abstract: The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking</title>
<link>https://arxiv.org/abs/2505.14112</link>
<guid>https://arxiv.org/abs/2505.14112</guid>
<content:encoded><![CDATA[
arXiv:2505.14112v1 Announce Type: new 
Abstract: Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it fails in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we develop a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99\% while achieving performance on par with state-of-the-art methods. Our work introduces a safe and efficient paradigm for low-entropy watermarking. https://github.com/Carol-gutianle/IE https://huggingface.co/datasets/Carol0110/IE-Tagger
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst</title>
<link>https://arxiv.org/abs/2505.14116</link>
<guid>https://arxiv.org/abs/2505.14116</guid>
<content:encoded><![CDATA[
arXiv:2505.14116v1 Announce Type: new 
Abstract: Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing BERT for German Compound Semantics</title>
<link>https://arxiv.org/abs/2505.14130</link>
<guid>https://arxiv.org/abs/2505.14130</guid>
<content:encoded><![CDATA[
arXiv:2505.14130v1 Announce Type: new 
Abstract: This paper investigates the extent to which pretrained German BERT encodes knowledge of noun compound semantics. We comprehensively vary combinations of target tokens, layers, and cased vs. uncased models, and evaluate them by predicting the compositionality of 868 gold standard compounds. Looking at representational patterns within the transformer architecture, we observe trends comparable to equivalent prior work on English, with compositionality information most easily recoverable in the early layers. However, our strongest results clearly lag behind those reported for English, suggesting an inherently more difficult task in German. This may be due to the higher productivity of compounding in German than in English and the associated increase in constituent-level ambiguity, including in our target compound set.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering</title>
<link>https://arxiv.org/abs/2505.14131</link>
<guid>https://arxiv.org/abs/2505.14131</guid>
<content:encoded><![CDATA[
arXiv:2505.14131v1 Announce Type: new 
Abstract: In table question answering (TQA), tables are encoded as either texts or images. Prior work suggests that passing images of tables to multi-modal large language models (MLLMs) performs comparably to or even better than using textual input with large language models (LLMs). However, the lack of controlled setups limits fine-grained distinctions between these approaches. In this paper, we conduct the first controlled study on the effectiveness of several combinations of table representations and models from two perspectives: question complexity and table size. We build a new benchmark based on existing TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we find that the best combination of table representation and model varies across setups. We propose FRES, a method selecting table representations dynamically, and observe a 10% average performance improvement compared to using both representations indiscriminately.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information</title>
<link>https://arxiv.org/abs/2505.14149</link>
<guid>https://arxiv.org/abs/2505.14149</guid>
<content:encoded><![CDATA[
arXiv:2505.14149v1 Announce Type: new 
Abstract: The exponential increase in academic papers has significantly increased the time required for researchers to access relevant literature. Keyphrase Extraction (KPE) offers a solution to this situation by enabling researchers to efficiently retrieve relevant literature. The current study on KPE from academic articles aims to improve the performance of extraction models through innovative approaches using Title and Abstract as input corpora. However, the semantic richness of keywords is significantly constrained by the length of the abstract. While full-text-based KPE can address this issue, it simultaneously introduces noise, which significantly diminishes KPE performance. To address this issue, this paper utilized the structural features and section texts obtained from the section structure information of academic articles to extract keyphrase from academic papers. The approach consists of two main parts: (1) exploring the effect of seven structural features on KPE models, and (2) integrating the extraction results from all section texts used as input corpora for KPE models via a keyphrase integration algorithm to obtain the keyphrase integration result. Furthermore, this paper also examined the effect of the classification quality of section structure on the KPE performance. The results show that incorporating structural features improves KPE performance, though different features have varying effects on model efficacy. The keyphrase integration approach yields the best performance, and the classification quality of section structure can affect KPE performance. These findings indicate that using the section structure information of academic articles contributes to effective KPE from academic articles. The code and dataset supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prior Prompt Engineering for Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14157</link>
<guid>https://arxiv.org/abs/2505.14157</guid>
<content:encoded><![CDATA[
arXiv:2505.14157v1 Announce Type: new 
Abstract: This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Alignment of Time Sensitive Facts with Activation Engineering</title>
<link>https://arxiv.org/abs/2505.14158</link>
<guid>https://arxiv.org/abs/2505.14158</guid>
<content:encoded><![CDATA[
arXiv:2505.14158v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are trained on diverse and often conflicting knowledge spanning multiple domains and time periods. Some of this knowledge is only valid within specific temporal contexts, such as answering the question, "Who is the President of the United States in 2022?" Ensuring LLMs generate time appropriate responses is crucial for maintaining relevance and accuracy. In this work we explore activation engineering as a method for temporally aligning LLMs to improve factual recall without any training or dataset creation. In this research we explore an activation engineering technique to ground three versions of LLaMA 2 to specific points in time and examine the effects of varying injection layers and prompting strategies. Our experiments demonstrate up to a 44% and 16% improvement in relative and explicit prompting respectively, achieving comparable performance to the fine-tuning method proposed by Zhao et al. (2024) . Notably, our approach achieves similar results to the fine-tuning baseline while being significantly more computationally efficient and requiring no pre-aligned datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models</title>
<link>https://arxiv.org/abs/2505.14160</link>
<guid>https://arxiv.org/abs/2505.14160</guid>
<content:encoded><![CDATA[
arXiv:2505.14160v1 Announce Type: new 
Abstract: Multilingual vision-language models promise universal image-text retrieval, yet their social biases remain under-explored. We present the first systematic audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and CAPIVARA-CLIP -- across ten languages that vary in resource availability and grammatical gender. Using balanced subsets of \textsc{FairFace} and the \textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the assumption that multilinguality mitigates bias, every model exhibits stronger gender bias than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this transfer. Highly gendered languages consistently magnify all measured bias types, but even gender-neutral languages remain vulnerable when cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics conceal language-specific ``hot spots,'' underscoring the need for fine-grained, language-aware bias evaluation in future multilingual vision-language research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore</title>
<link>https://arxiv.org/abs/2505.14165</link>
<guid>https://arxiv.org/abs/2505.14165</guid>
<content:encoded><![CDATA[
arXiv:2505.14165v1 Announce Type: new 
Abstract: Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity toward specific aspects within a text, enabling more precise opinion mining in domains such as product reviews and social media. However, traditional FGSA approaches often require task-specific architectures and extensive annotated data, limiting their generalization and scalability. To address these challenges, we propose PL-FGSA, a unified prompt learning-based framework implemented using the MindSpore platform, which integrates prompt design with a lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task prompt-augmented generation problem, jointly tackling aspect extraction, sentiment classification, and causal explanation in a unified paradigm. By leveraging prompt-based guidance, PL-FGSA enhances interpretability and achieves strong performance under both full-data and low-resource conditions. Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and MAMS-demonstrate that our model consistently outperforms traditional fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597, respectively. These results validate the effectiveness of prompt-based generalization and highlight the practical value of PL-FGSA for real-world sentiment analysis tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models</title>
<link>https://arxiv.org/abs/2505.14172</link>
<guid>https://arxiv.org/abs/2505.14172</guid>
<content:encoded><![CDATA[
arXiv:2505.14172v1 Announce Type: new 
Abstract: Despite their remarkable progress across diverse domains, Large Language Models (LLMs) consistently fail at simple character-level tasks, such as counting letters in words, due to a fundamental limitation: tokenization. In this work, we frame this limitation as a problem of low mutual information and analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks that isolate character-level reasoning in a controlled setting, we show that such capabilities emerge slowly, suddenly, and only late in training. We further show that percolation-based models of concept emergence explain these patterns, suggesting that learning character composition is not fundamentally different from learning commonsense knowledge. To address this bottleneck, we propose a lightweight architectural modification that significantly improves character-level reasoning while preserving the inductive advantages of subword models. Together, our results bridge low-level perceptual gaps in tokenized LMs and provide a principled framework for understanding and mitigating their structural blind spots. We make our code publicly available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation</title>
<link>https://arxiv.org/abs/2505.14173</link>
<guid>https://arxiv.org/abs/2505.14173</guid>
<content:encoded><![CDATA[
arXiv:2505.14173v1 Announce Type: new 
Abstract: The sparse Mixture-of-Experts (MoE) has achieved significant progress for neural machine translation (NMT). However, there exist two limitations in current MoE solutions which may lead to sub-optimal performance: 1) they directly use the task knowledge of NMT into MoE (\emph{e.g.}, domain/linguistics-specific knowledge), which are generally unavailable at practical application and neglect the naturally grouped domain/linguistic properties; 2) the expert selection only depends on the localized token representation without considering the context, which fully grasps the state of each token in a global view. To address the above limitations, we propose THOR-MoE via arming the MoE with hierarchical task-guided and context-responsive routing policies. Specifically, it 1) firstly predicts the domain/language label and then extracts mixed domain/language representation to allocate task-level experts in a hierarchical manner; 2) injects the context information to enhance the token routing from the pre-selected task-level experts set, which can help each token to be accurately routed to more specialized and suitable experts. Extensive experiments on multi-domain translation and multilingual translation benchmarks with different architectures consistently demonstrate the superior performance of THOR-MoE. Additionally, the THOR-MoE operates as a plug-and-play module compatible with existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder} routing schemes, ensuring broad applicability across diverse MoE architectures. For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder} routing, the context-aware manner can achieve an average improvement of 0.75 BLEU with less than 22\% activated parameters on multi-domain translation tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.14174</link>
<guid>https://arxiv.org/abs/2505.14174</guid>
<content:encoded><![CDATA[
arXiv:2505.14174v1 Announce Type: new 
Abstract: LLMs are effective at code generation tasks like text-to-SQL, but is it worth the cost? Many state-of-the-art approaches use non-task-specific LLM techniques including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These methods can be costly at inference time, sometimes requiring over a hundred LLM calls with reasoning, incurring average costs of up to \$0.46 per query, while fine-tuning models can cost thousands of dollars. We introduce "N-rep" consistency, a more cost-efficient text-to-SQL approach that achieves similar BIRD benchmark scores as other more expensive methods, at only \$0.039 per query. N-rep leverages multiple representations of the same schema input to mitigate weaknesses in any single representation, making the solution more robust and allowing the use of smaller and cheaper models without any reasoning or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL approach in its cost range.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits</title>
<link>https://arxiv.org/abs/2505.14178</link>
<guid>https://arxiv.org/abs/2505.14178</guid>
<content:encoded><![CDATA[
arXiv:2505.14178v1 Announce Type: new 
Abstract: Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Abstractive Summarization of Scientific Papers Using Structure Information</title>
<link>https://arxiv.org/abs/2505.14179</link>
<guid>https://arxiv.org/abs/2505.14179</guid>
<content:encoded><![CDATA[
arXiv:2505.14179v1 Announce Type: new 
Abstract: Abstractive summarization of scientific papers has always been a research focus, yet existing methods face two main challenges. First, most summarization models rely on Encoder-Decoder architectures that treat papers as sequences of words, thus fail to fully capture the structured information inherent in scientific papers. Second, existing research often use keyword mapping or feature engineering to identify the structural information, but these methods struggle with the structural flexibility of scientific papers and lack robustness across different disciplines. To address these challenges, we propose a two-stage abstractive summarization framework that leverages automatic recognition of structural functions within scientific papers. In the first stage, we standardize chapter titles from numerous scientific papers and construct a large-scale dataset for structural function recognition. A classifier is then trained to automatically identify the key structural components (e.g., Background, Methods, Results, Discussion), which provides a foundation for generating more balanced summaries. In the second stage, we employ Longformer to capture rich contextual relationships across sections and generating context-aware summaries. Experiments conducted on two domain-specific scientific paper summarization datasets demonstrate that our method outperforms advanced baselines, and generates more comprehensive summaries. The code and dataset can be accessed at https://github.com/tongbao96/code-for-SFR-AS.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SlangDIT: Benchmarking LLMs in Interpretative Slang Translation</title>
<link>https://arxiv.org/abs/2505.14181</link>
<guid>https://arxiv.org/abs/2505.14181</guid>
<content:encoded><![CDATA[
arXiv:2505.14181v1 Announce Type: new 
Abstract: The challenge of slang translation lies in capturing context-dependent semantic extensions, as slang terms often convey meanings beyond their literal interpretation. While slang detection, explanation, and translation have been studied as isolated tasks in the era of large language models (LLMs), their intrinsic interdependence remains underexplored. The main reason is lacking of a benchmark where the two tasks can be a prerequisite for the third one, which can facilitate idiomatic translation. In this paper, we introduce the interpretative slang translation task (named SlangDIT) consisting of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation within the current context, aiming to generate more accurate translation with the help of slang detection and slang explanation. To this end, we construct a SlangDIT dataset, containing over 25k English-Chinese sentence pairs. Each source sentence mentions at least one slang term and is labeled with corresponding cross-lingual slang explanation. Based on the benchmark, we propose a deep thinking model, named SlangOWL. It firstly identifies whether the sentence contains a slang, and then judges whether the slang is polysemous and analyze its possible meaning. Further, the SlangOWL provides the best explanation of the slang term targeting on the current context. Finally, according to the whole thought, the SlangOWL offers a suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and LLama-3.1), show that our deep thinking approach indeed enhances the performance of LLMs where the proposed SLangOWL significantly surpasses the vanilla models and supervised fine-tuned models without thinking.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkSwitcher: When to Think Hard, When to Think Fast</title>
<link>https://arxiv.org/abs/2505.14183</link>
<guid>https://arxiv.org/abs/2505.14183</guid>
<content:encoded><![CDATA[
arXiv:2505.14183v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) excel at solving complex tasks by leveraging long chain-of-thought (CoT) reasoning. However, this often leads to overthinking on simple tasks, resulting in unnecessary computational overhead. We observe that LRMs inherently possess the capability for efficient short CoT reasoning, which can be reliably elicited through prompt design. To leverage this capability, we propose ThinkSwitcher, a framework that enables a single LRM to dynamically switch between short and long CoT modes based on task complexity. ThinkSwitcher introduces a lightweight switching module trained with supervision signals derived from the relative performance of each reasoning mode across tasks. Experiments on multiple reasoning benchmarks show that ThinkSwitcher reduces computational cost by 20-30% while maintaining high accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher as a scalable and efficient solution for unified LRM deployment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification</title>
<link>https://arxiv.org/abs/2505.14195</link>
<guid>https://arxiv.org/abs/2505.14195</guid>
<content:encoded><![CDATA[
arXiv:2505.14195v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have been fueled by large scale training corpora drawn from diverse sources such as websites, news articles, and books. These datasets often contain explicit user information, such as person names and addresses, that LLMs may unintentionally reproduce in their generated outputs. Beyond such explicit content, LLMs can also leak identity revealing cues through implicit signals such as distinctive writing styles, raising significant concerns about authorship privacy. There are three major automated tasks in authorship privacy, namely authorship obfuscation (AO), authorship mimicking (AM), and authorship verification (AV). Prior research has studied AO, AM, and AV independently. However, their interplays remain under explored, which leaves a major research gap, especially in the era of LLMs, where they are profoundly shaping how we curate and share user generated content, and the distinction between machine generated and human authored text is also increasingly blurred. This work then presents the first unified framework for analyzing the dynamic relationships among LLM enabled AO, AM, and AV in the context of authorship privacy. We quantify how they interact with each other to transform human authored text, examining effects at a single point in time and iteratively over time. We also examine the role of demographic metadata, such as gender, academic background, in modulating their performances, inter-task dynamics, and privacy risks. All source code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks</title>
<link>https://arxiv.org/abs/2505.14212</link>
<guid>https://arxiv.org/abs/2505.14212</guid>
<content:encoded><![CDATA[
arXiv:2505.14212v1 Announce Type: new 
Abstract: A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs</title>
<link>https://arxiv.org/abs/2505.14226</link>
<guid>https://arxiv.org/abs/2505.14226</guid>
<content:encoded><![CDATA[
arXiv:2505.14226v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanistic Fine-tuning for In-context Learning</title>
<link>https://arxiv.org/abs/2505.14233</link>
<guid>https://arxiv.org/abs/2505.14233</guid>
<content:encoded><![CDATA[
arXiv:2505.14233v1 Announce Type: new 
Abstract: In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.14238</link>
<guid>https://arxiv.org/abs/2505.14238</guid>
<content:encoded><![CDATA[
arXiv:2505.14238v1 Announce Type: new 
Abstract: Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Technical Report on classification of literature related to children speech disorder</title>
<link>https://arxiv.org/abs/2505.14242</link>
<guid>https://arxiv.org/abs/2505.14242</guid>
<content:encoded><![CDATA[
arXiv:2505.14242v1 Announce Type: new 
Abstract: This technical report presents a natural language processing (NLP)-based approach for systematically classifying scientific literature on childhood speech disorders. We retrieved and filtered 4,804 relevant articles published after 2015 from the PubMed database using domain-specific keywords. After cleaning and pre-processing the abstracts, we applied two topic modeling techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify latent thematic structures in the corpus. Our models uncovered 14 clinically meaningful clusters, such as infantile hyperactivity and abnormal epileptic behavior. To improve relevance and precision, we incorporated a custom stop word list tailored to speech pathology. Evaluation results showed that the LDA model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating strong topic coherence and predictive performance. The BERTopic model exhibited a low proportion of outlier topics (less than 20%), demonstrating its capacity to classify heterogeneous literature effectively. These results provide a foundation for automating literature reviews in speech-language pathology.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransBench: Benchmarking Machine Translation for Industrial-Scale Applications</title>
<link>https://arxiv.org/abs/2505.14244</link>
<guid>https://arxiv.org/abs/2505.14244</guid>
<content:encoded><![CDATA[
arXiv:2505.14244v1 Announce Type: new 
Abstract: Machine translation (MT) has become indispensable for cross-border communication in globalized industries like e-commerce, finance, and legal services, with recent advancements in large language models (LLMs) significantly enhancing translation quality. However, applying general-purpose MT models to industrial scenarios reveals critical limitations due to domain-specific terminology, cultural nuances, and stylistic conventions absent in generic benchmarks. Existing evaluation frameworks inadequately assess performance in specialized contexts, creating a gap between academic benchmarks and real-world efficacy. To address this, we propose a three-level translation capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic evaluation across these dimensions. We introduce TransBench, a benchmark tailored for industrial MT, initially targeting international e-commerce with 17,000 professionally translated sentences spanning 4 main scenarios and 33 language pairs. TransBench integrates traditional metrics (BLEU, TER) with Marco-MOS, a domain-specific evaluation model, and provides guidelines for reproducible benchmark construction. Our contributions include: (1) a structured framework for industrial MT evaluation, (2) the first publicly available benchmark for e-commerce translation, (3) novel metrics probing multi-level translation quality, and (4) open-sourced evaluation tools. This work bridges the evaluation gap, enabling researchers and practitioners to systematically assess and enhance MT systems for industry-specific needs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation</title>
<link>https://arxiv.org/abs/2505.14256</link>
<guid>https://arxiv.org/abs/2505.14256</guid>
<content:encoded><![CDATA[
arXiv:2505.14256v1 Announce Type: new 
Abstract: In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think-J: Learning to Think for Generative LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.14268</link>
<guid>https://arxiv.org/abs/2505.14268</guid>
<content:encoded><![CDATA[
arXiv:2505.14268v1 Announce Type: new 
Abstract: LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.14271</link>
<guid>https://arxiv.org/abs/2505.14271</guid>
<content:encoded><![CDATA[
arXiv:2505.14271v1 Announce Type: new 
Abstract: The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between human-written, AI-generated, and human-AI collaborative texts. In this work, we collect a multilingual, multi-domain, multi-generator dataset FAIDSet. We further introduce a fine-grained detection framework FAID to classify text into these three categories, meanwhile identifying the underlying AI model family. Unlike existing binary classifiers, FAID is built to capture both authorship and model-specific characteristics. Our method combines multi-level contrastive learning with multi-task auxiliary classification to learn subtle stylistic cues. By modeling AI families as distinct stylistic entities, FAID offers improved interpretability. We incorporate an adaptation to address distributional shifts without retraining for unseen data. Experimental results demonstrate that FAID outperforms several baseline approaches, particularly enhancing the generalization accuracy on unseen domains and new AI models. It provide a potential solution for improving transparency and accountability in AI-assisted writing.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data</title>
<link>https://arxiv.org/abs/2505.14272</link>
<guid>https://arxiv.org/abs/2505.14272</guid>
<content:encoded><![CDATA[
arXiv:2505.14272v1 Announce Type: new 
Abstract: Considering the importance of detecting hateful language, labeled hate speech data is expensive and time-consuming to collect, particularly for low-resource languages. Prior work has demonstrated the effectiveness of cross-lingual transfer learning and data augmentation in improving performance on tasks with limited labeled data. To develop an efficient and scalable cross-lingual transfer learning approach, we leverage nearest-neighbor retrieval to augment minimal labeled data in the target language, thereby enhancing detection performance. Specifically, we assume access to a small set of labeled training instances in the target language and use these to retrieve the most relevant labeled examples from a large multilingual hate speech detection pool. We evaluate our approach on eight languages and demonstrate that it consistently outperforms models trained solely on the target language data. Furthermore, in most cases, our method surpasses the current state-of-the-art. Notably, our approach is highly data-efficient, retrieving as small as 200 instances in some cases while maintaining superior performance. Moreover, it is scalable, as the retrieval pool can be easily expanded, and the method can be readily adapted to new languages and tasks. We also apply maximum marginal relevance to mitigate redundancy and filter out highly similar retrieved instances, resulting in improvements in some languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering</title>
<link>https://arxiv.org/abs/2505.14279</link>
<guid>https://arxiv.org/abs/2505.14279</guid>
<content:encoded><![CDATA[
arXiv:2505.14279v1 Announce Type: new 
Abstract: Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&amp;A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs</title>
<link>https://arxiv.org/abs/2505.14286</link>
<guid>https://arxiv.org/abs/2505.14286</guid>
<content:encoded><![CDATA[
arXiv:2505.14286v1 Announce Type: new 
Abstract: The combination of pre-trained speech encoders with large language models has enabled the development of speech LLMs that can handle a wide range of spoken language processing tasks. While these models are powerful and flexible, this very flexibility may make them more vulnerable to adversarial attacks. To examine the extent of this problem, in this work we investigate universal acoustic adversarial attacks on speech LLMs. Here a fixed, universal, adversarial audio segment is prepended to the original input audio. We initially investigate attacks that cause the model to either produce no output or to perform a modified task overriding the original prompt. We then extend the nature of the attack to be selective so that it activates only when specific input attributes, such as a speaker gender or spoken language, are present. Inputs without the targeted attribute should be unaffected, allowing fine-grained control over the model outputs. Our findings reveal critical vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar speech LLMs may be susceptible to universal adversarial attacks. This highlights the need for more robust training strategies and improved resistance to adversarial attacks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Optimization for Language Transfer in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14297</link>
<guid>https://arxiv.org/abs/2505.14297</guid>
<content:encoded><![CDATA[
arXiv:2505.14297v1 Announce Type: new 
Abstract: Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling</title>
<link>https://arxiv.org/abs/2505.14305</link>
<guid>https://arxiv.org/abs/2505.14305</guid>
<content:encoded><![CDATA[
arXiv:2505.14305v1 Announce Type: new 
Abstract: Text-to-SQL, which maps natural language to SQL queries, has benefited greatly from recent advances in Large Language Models (LLMs). While LLMs offer various paradigms for this task, including prompting and supervised fine-tuning (SFT), SFT approaches still face challenges such as complex multi-stage pipelines and poor robustness to noisy schema information. To address these limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that jointly optimizes schema linking and SQL generation via a unified loss. JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional attention, alongside a confusion-aware noisy schema sampling strategy with selective attention to improve robustness under noisy schema conditions. Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL achieves state-of-the-art execution accuracy among comparable-size open-source models, while significantly improving both training and inference efficiency.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency</title>
<link>https://arxiv.org/abs/2505.14309</link>
<guid>https://arxiv.org/abs/2505.14309</guid>
<content:encoded><![CDATA[
arXiv:2505.14309v1 Announce Type: new 
Abstract: Retrieval-augmented language models have demonstrated performance comparable to much larger models while requiring fewer computational resources. The effectiveness of these models crucially depends on the overlap between query and retrieved context, but the optimal degree of this overlap remains unexplored. In this paper, we systematically investigate how varying levels of query--context overlap affect model performance during both training and inference. Our experiments reveal that increased overlap initially has minimal effect, but substantially improves test-time perplexity and accelerates model learning above a critical threshold. Building on these findings, we demonstrate that deliberately increasing overlap through synthetic context can enhance data efficiency and reduce training time by approximately 40\% without compromising performance. We specifically generate synthetic context through paraphrasing queries. We validate our perplexity-based findings on question-answering tasks, confirming that the benefits of retrieval-augmented language modeling extend to practical applications. Our results provide empirical evidence of significant optimization potential for retrieval mechanisms in language model pretraining.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing</title>
<link>https://arxiv.org/abs/2505.14311</link>
<guid>https://arxiv.org/abs/2505.14311</guid>
<content:encoded><![CDATA[
arXiv:2505.14311v1 Announce Type: new 
Abstract: Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (https://catalog.hausanlp.org), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A MIND for Reasoning: Meta-learning for In-context Deduction</title>
<link>https://arxiv.org/abs/2505.14313</link>
<guid>https://arxiv.org/abs/2505.14313</guid>
<content:encoded><![CDATA[
arXiv:2505.14313v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QA-prompting: Improving Summarization with Large Language Models using Question-Answering</title>
<link>https://arxiv.org/abs/2505.14347</link>
<guid>https://arxiv.org/abs/2505.14347</guid>
<content:encoded><![CDATA[
arXiv:2505.14347v1 Announce Type: new 
Abstract: Language Models (LMs) have revolutionized natural language processing, enabling high-quality text generation through prompting and in-context learning. However, models often struggle with long-context summarization due to positional biases, leading to suboptimal extraction of critical information. There are techniques to improve this with fine-tuning, pipelining, or using complex techniques, which have their own challenges. To solve these challenges, we propose QA-prompting - a simple prompting method for summarization that utilizes question-answering as an intermediate step prior to summary generation. Our method extracts key information and enriches the context of text to mitigate positional biases and improve summarization in a single LM call per task without requiring fine-tuning or pipelining. Experiments on multiple datasets belonging to different domains using ten state-of-the-art pre-trained models demonstrate that QA-prompting outperforms baseline and other state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This provides an effective and scalable solution for summarization and highlights the importance of domain-specific question selection for optimal performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.14350</link>
<guid>https://arxiv.org/abs/2505.14350</guid>
<content:encoded><![CDATA[
arXiv:2505.14350v1 Announce Type: new 
Abstract: Fine-tuning Large Language Models (LLMs) has become increasingly challenging due to their massive scale and associated computational costs. Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as computational alternatives; however, their implementations still require significant resources. In this paper, we present OSoRA (Output-Dimension and Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs. OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value Decomposition (SVD) with learnable scaling vectors in a unified framework. It first performs an SVD of pre-trained weight matrices, then optimizes an output-dimension vector during training, while keeping the corresponding singular vector matrices frozen. OSoRA substantially reduces computational resource requirements by minimizing the number of trainable parameters during fine-tuning. Comprehensive evaluations across mathematical reasoning, common sense reasoning, and other benchmarks demonstrate that OSoRA achieves comparable or superior performance to state-of-the-art methods like LoRA and VeRA, while maintaining a linear parameter scaling even as the rank increases to higher dimensions. Our ablation studies further confirm that jointly training both the singular values and the output-dimension vector is critical for optimal performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications</title>
<link>https://arxiv.org/abs/2505.14354</link>
<guid>https://arxiv.org/abs/2505.14354</guid>
<content:encoded><![CDATA[
arXiv:2505.14354v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Decomposition of Weights and Singular Value Low Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.14367</link>
<guid>https://arxiv.org/abs/2505.14367</guid>
<content:encoded><![CDATA[
arXiv:2505.14367v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank Adaptation (LoRA) represents one of the most widely adopted methodologies. However, existing LoRA-based approaches exhibit two fundamental limitations: unstable training dynamics and inefficient knowledge transfer from pre-trained models, both stemming from random initialization of adapter parameters. To overcome these challenges, we propose DuDe, a novel approach that decomposes weight matrices into magnitude and direction components, employing Singular Value Decomposition (SVD) for principled initialization. Our comprehensive evaluation demonstrates DuDe's superior performance and robustness, achieving up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our theoretical analysis and empirical validation collectively demonstrate that DuDe's decomposition strategy enhances optimization stability and better preserves pre-trained representations, particularly for domain-specific tasks requiring specialized knowledge. The combination of robust empirical performance and rigorous theoretical foundations establishes DuDe as a significant contribution to PEFT methodologies for LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoRev: Automatic Peer Review System for Academic Research Papers</title>
<link>https://arxiv.org/abs/2505.14376</link>
<guid>https://arxiv.org/abs/2505.14376</guid>
<content:encoded><![CDATA[
arXiv:2505.14376v1 Announce Type: new 
Abstract: Generating a review for an academic research paper is a complex task that requires a deep understanding of the document's content and the interdependencies between its sections. It demands not only insight into technical details but also an appreciation of the paper's overall coherence and structure. Recent methods have predominantly focused on fine-tuning large language models (LLMs) to address this challenge. However, they often overlook the computational and performance limitations imposed by long input token lengths. To address this, we introduce AutoRev, an Automatic Peer Review System for Academic Research Papers. Our novel framework represents an academic document as a graph, enabling the extraction of the most critical passages that contribute significantly to the review. This graph-based approach demonstrates effectiveness for review generation and is potentially adaptable to various downstream tasks, such as question answering, summarization, and document representation. When applied to review generation, our method outperforms SOTA baselines by an average of 58.72% across all evaluation metrics. We hope that our work will stimulate further research in applying graph-based extraction techniques to other downstream tasks in NLP. We plan to make our code public upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Editing Across Languages: A Survey of Multilingual Knowledge Editing</title>
<link>https://arxiv.org/abs/2505.14393</link>
<guid>https://arxiv.org/abs/2505.14393</guid>
<content:encoded><![CDATA[
arXiv:2505.14393v1 Announce Type: new 
Abstract: While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks,summarize key findings on method effectiveness and transfer patterns, identify challenges in cross-lingual propagation, and highlight open problems related to language anisotropy, evaluation coverage, and edit scalability. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language</title>
<link>https://arxiv.org/abs/2505.14395</link>
<guid>https://arxiv.org/abs/2505.14395</guid>
<content:encoded><![CDATA[
arXiv:2505.14395v1 Announce Type: new 
Abstract: Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation</title>
<link>https://arxiv.org/abs/2505.14398</link>
<guid>https://arxiv.org/abs/2505.14398</guid>
<content:encoded><![CDATA[
arXiv:2505.14398v1 Announce Type: new 
Abstract: While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis</title>
<link>https://arxiv.org/abs/2505.14406</link>
<guid>https://arxiv.org/abs/2505.14406</guid>
<content:encoded><![CDATA[
arXiv:2505.14406v1 Announce Type: new 
Abstract: Large Language Models (LLMs), despite their remarkable capabilities, are hampered by hallucinations. A particularly challenging variant, knowledge overshadowing, occurs when one piece of activated knowledge inadvertently masks another relevant piece, leading to erroneous outputs even with high-quality training data. Current understanding of overshadowing is largely confined to inference-time observations, lacking deep insights into its origins and internal mechanisms during model training. Therefore, we introduce PhantomCircuit, a novel framework designed to comprehensively analyze and detect knowledge overshadowing. By innovatively employing knowledge circuit analysis, PhantomCircuit dissects the internal workings of attention heads, tracing how competing knowledge pathways contribute to the overshadowing phenomenon and its evolution throughout the training process. Extensive experiments demonstrate PhantomCircuit's effectiveness in identifying such instances, offering novel insights into this elusive hallucination and providing the research community with a new methodological lens for its potential mitigation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2505.14418</link>
<guid>https://arxiv.org/abs/2505.14418</guid>
<content:encoded><![CDATA[
arXiv:2505.14418v1 Announce Type: new 
Abstract: Graphical user interface (GUI) agents powered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLLM-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7\% on three attack objectives, and shows stealthiness with only 1\% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1\%. Our code is available at \texttt{anonymous}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection</title>
<link>https://arxiv.org/abs/2505.14420</link>
<guid>https://arxiv.org/abs/2505.14420</guid>
<content:encoded><![CDATA[
arXiv:2505.14420v1 Announce Type: new 
Abstract: Predicting earnings surprises through the analysis of earnings conference call transcripts has attracted increasing attention from the financial research community. Conference calls serve as critical communication channels between company executives, analysts, and shareholders, offering valuable forward-looking information. However, these transcripts present significant analytical challenges, typically containing over 5,000 words with substantial redundancy and industry-specific terminology that creates obstacles for language models. In this work, we propose the Sparse Autoencoder for Financial Representation Enhancement (SAE-FiRE) framework to address these limitations by extracting key information while eliminating redundancy. SAE-FiRE employs Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out noises, and focusing specifically on capturing nuanced financial signals that have predictive power for earnings surprises. Experimental results indicate that the proposed method can significantly outperform comparing baselines.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Low-Resource MT via Synthetic Data Generation with LLMs</title>
<link>https://arxiv.org/abs/2505.14423</link>
<guid>https://arxiv.org/abs/2505.14423</guid>
<content:encoded><![CDATA[
arXiv:2505.14423v1 Announce Type: new 
Abstract: We investigate the potential of LLM-generated synthetic data for improving low-resource machine translation (MT). Focusing on seven diverse target languages, we construct a document-level synthetic corpus from English Europarl, and extend it via pivoting to 147 additional language pairs. Automatic and human evaluation confirm its high overall quality. We study its practical application by (i) identifying effective training regimes, (ii) comparing our data with the HPLT dataset, and (iii) testing its utility beyond English-centric MT. Finally, we introduce SynOPUS, a public repository for synthetic parallel datasets. Our findings show that LLM-generated synthetic data, even when noisy, can substantially improve MT performance for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2505.14425</link>
<guid>https://arxiv.org/abs/2505.14425</guid>
<content:encoded><![CDATA[
arXiv:2505.14425v1 Announce Type: new 
Abstract: Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-written instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14436</link>
<guid>https://arxiv.org/abs/2505.14436</guid>
<content:encoded><![CDATA[
arXiv:2505.14436v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\textbf{LaTen}$ ($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creative Preference Optimization</title>
<link>https://arxiv.org/abs/2505.14442</link>
<guid>https://arxiv.org/abs/2505.14442</guid>
<content:encoded><![CDATA[
arXiv:2505.14442v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation</title>
<link>https://arxiv.org/abs/2505.14455</link>
<guid>https://arxiv.org/abs/2505.14455</guid>
<content:encoded><![CDATA[
arXiv:2505.14455v1 Announce Type: new 
Abstract: Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Correct Answers Are Equal: Why Your Distillation Source Matters</title>
<link>https://arxiv.org/abs/2505.14464</link>
<guid>https://arxiv.org/abs/2505.14464</guid>
<content:encoded><![CDATA[
arXiv:2505.14464v1 Announce Type: new 
Abstract: Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\footnote{Datasets are available on Hugging Face: \href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}, \href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Void in Language Models</title>
<link>https://arxiv.org/abs/2505.14467</link>
<guid>https://arxiv.org/abs/2505.14467</guid>
<content:encoded><![CDATA[
arXiv:2505.14467v1 Announce Type: new 
Abstract: Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations</title>
<link>https://arxiv.org/abs/2505.14469</link>
<guid>https://arxiv.org/abs/2505.14469</guid>
<content:encoded><![CDATA[
arXiv:2505.14469v1 Announce Type: new 
Abstract: Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.14471</link>
<guid>https://arxiv.org/abs/2505.14471</guid>
<content:encoded><![CDATA[
arXiv:2505.14471v1 Announce Type: new 
Abstract: Citation classification, which identifies the intention behind academic citations, is pivotal for scholarly analysis. Previous works suggest fine-tuning pretrained language models (PLMs) on citation classification datasets, reaping the reward of the linguistic knowledge they gained during pretraining. However, directly fine-tuning for citation classification is challenging due to labeled data scarcity, contextual noise, and spurious keyphrase correlations. In this paper, we present a novel framework, Citss, that adapts the PLMs to overcome these challenges. Citss introduces self-supervised contrastive learning to alleviate data scarcity, and is equipped with two specialized strategies to obtain the contrastive pairs: sentence-level cropping, which enhances focus on target citations within long contexts, and keyphrase perturbation, which mitigates reliance on specific keyphrases. Compared with previous works that are only designed for encoder-based PLMs, Citss is carefully developed to be compatible with both encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged pretraining. Experiments with three benchmark datasets with both encoder-based PLMs and decoder-based LLMs demonstrate our superiority compared to the previous state of the art. Our code is available at: github.com/LITONG99/Citss
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.14481</link>
<guid>https://arxiv.org/abs/2505.14481</guid>
<content:encoded><![CDATA[
arXiv:2505.14481v1 Announce Type: new 
Abstract: In the field of urban planning, existing Vision-Language Models (VLMs) frequently fail to effectively analyze and evaluate planning maps, despite the critical importance of these visual elements for urban planners and related educational contexts. Planning maps, which visualize land use, infrastructure layouts, and functional zoning, require specialized understanding of spatial configurations, regulatory requirements, and multi-scale analysis. To address this challenge, we introduce PlanGPT-VL, the first domain-specific Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL employs three innovative approaches: (1) PlanAnno-V framework for high-quality VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations through structured verification, and (3) comprehensive training methodology combining Supervised Fine-Tuning with frozen vision encoder parameters. Through systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs in specialized planning map interpretation tasks, offering urban planning professionals a reliable tool for map analysis, assessment, and educational applications while maintaining high factual accuracy. Our lightweight 7B parameter model achieves comparable performance to models exceeding 72B parameters, demonstrating efficient domain specialization without sacrificing performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance</title>
<link>https://arxiv.org/abs/2505.14483</link>
<guid>https://arxiv.org/abs/2505.14483</guid>
<content:encoded><![CDATA[
arXiv:2505.14483v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown great potential in flagging harmful content in online communities. Yet, existing approaches for moderation require a separate model for every community and are opaque in their decision-making, limiting real-world adoption. We introduce Mixture of Moderation Experts (MoMoE), a modular, cross-community framework that adds post-hoc explanations to scalable content moderation. MoMoE orchestrates four operators -- Allocate, Predict, Aggregate, Explain -- and is instantiated as seven community-specialized experts (MoMoE-Community) and five norm-violation experts (MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1 scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned baselines while consistently producing concise and reliable explanations. Although community-specialized experts deliver the highest peak accuracy, norm-violation experts provide steadier performance across domains. These findings show that MoMoE yields scalable, transparent moderation without needing per-community fine-tuning. More broadly, they suggest that lightweight, explainable expert ensembles can guide future NLP and HCI research on trustworthy human-AI governance of online communities.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales</title>
<link>https://arxiv.org/abs/2505.14499</link>
<guid>https://arxiv.org/abs/2505.14499</guid>
<content:encoded><![CDATA[
arXiv:2505.14499v1 Announce Type: new 
Abstract: There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModRWKV: Transformer Multimodality in Linear Time</title>
<link>https://arxiv.org/abs/2505.14505</link>
<guid>https://arxiv.org/abs/2505.14505</guid>
<content:encoded><![CDATA[
arXiv:2505.14505v1 Announce Type: new 
Abstract: Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Graph Representations of Logical Forms for Language Modeling</title>
<link>https://arxiv.org/abs/2505.14523</link>
<guid>https://arxiv.org/abs/2505.14523</guid>
<content:encoded><![CDATA[
arXiv:2505.14523v1 Announce Type: new 
Abstract: We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs pretrained on similar amounts of data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs</title>
<link>https://arxiv.org/abs/2505.14530</link>
<guid>https://arxiv.org/abs/2505.14530</guid>
<content:encoded><![CDATA[
arXiv:2505.14530v1 Announce Type: new 
Abstract: We show that large language models (LLMs) exhibit an $\textit{internal chain-of-thought}$: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.14536</link>
<guid>https://arxiv.org/abs/2505.14536</guid>
<content:encoded><![CDATA[
arXiv:2505.14536v1 Announce Type: new 
Abstract: Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2505.14552</link>
<guid>https://arxiv.org/abs/2505.14552</guid>
<content:encoded><![CDATA[
arXiv:2505.14552v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pivot Language for Low-Resource Machine Translation</title>
<link>https://arxiv.org/abs/2505.14553</link>
<guid>https://arxiv.org/abs/2505.14553</guid>
<content:encoded><![CDATA[
arXiv:2505.14553v1 Announce Type: new 
Abstract: Certain pairs of languages suffer from lack of a parallel corpus which is large in size and diverse in domain. One of the ways this is overcome is via use of a pivot language. In this paper we use Hindi as a pivot language to translate Nepali into English. We describe what makes Hindi a good candidate for the pivot. We discuss ways in which a pivot language can be used, and use two such approaches - the Transfer Method (fully supervised) and Backtranslation (semi-supervised) - to translate Nepali into English. Using the former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which improves the baseline fully supervised score reported by (Guzman et al., 2019) by 6.6 points. While we are slightly below the semi-supervised baseline score of 15.1, we discuss what may have caused this under-performance, and suggest scope for future work.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring</title>
<link>https://arxiv.org/abs/2505.14577</link>
<guid>https://arxiv.org/abs/2505.14577</guid>
<content:encoded><![CDATA[
arXiv:2505.14577v1 Announce Type: new 
Abstract: Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning</title>
<link>https://arxiv.org/abs/2505.14582</link>
<guid>https://arxiv.org/abs/2505.14582</guid>
<content:encoded><![CDATA[
arXiv:2505.14582v1 Announce Type: new 
Abstract: Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its verbose, self-reflective style often hinders effective distillation into small language models (SLMs). We revisit Long-CoT compression through the lens of capability alignment and ask: Can pruning improve reasoning? We propose Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps under self-verification constraints. Through systematic analysis across three pruning strategies -- targeting entire chains, core reasoning, and verification -- we find that pruning verification steps yields consistent accuracy gains while reducing inference cost, outperforming token-level baselines and uncompressed fine-tuning. In contrast, pruning reasoning or all-chain steps degrades performance, revealing that small models benefit not from shorter CoTs, but from semantically leaner ones. Our findings highlight pruning as a structural optimization strategy for aligning CoT reasoning with SLM capacity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14585</link>
<guid>https://arxiv.org/abs/2505.14585</guid>
<content:encoded><![CDATA[
arXiv:2505.14585v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol</title>
<link>https://arxiv.org/abs/2505.14590</link>
<guid>https://arxiv.org/abs/2505.14590</guid>
<content:encoded><![CDATA[
arXiv:2505.14590v1 Announce Type: new 
Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these gaps.Next, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals</title>
<link>https://arxiv.org/abs/2505.14597</link>
<guid>https://arxiv.org/abs/2505.14597</guid>
<content:encoded><![CDATA[
arXiv:2505.14597v1 Announce Type: new 
Abstract: Code Sensitivity refers to the ability of Code LLMs to recognize and respond to details changes in problem descriptions. While current code benchmarks and instruction data focus on difficulty and diversity, sensitivity is overlooked. We first introduce the CTF-Code benchmark, constructed using counterfactual perturbations, minimizing input changes while maximizing output changes. The evaluation shows that many LLMs have a more than 10\% performance drop compared to the original problems. To fully utilize sensitivity, CTF-Instruct, an incremental instruction fine-tuning framework, extends on existing data and uses a selection mechanism to meet the three dimensions of difficulty, diversity, and sensitivity. Experiments show that LLMs fine-tuned with CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a 10\% performance boost on LiveCodeBench, validating the feasibility of enhancing LLMs' sensitivity to improve performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14599</link>
<guid>https://arxiv.org/abs/2505.14599</guid>
<content:encoded><![CDATA[
arXiv:2505.14599v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>sudoLLM : On Multi-role Alignment of Language Models</title>
<link>https://arxiv.org/abs/2505.14607</link>
<guid>https://arxiv.org/abs/2505.14607</guid>
<content:encoded><![CDATA[
arXiv:2505.14607v1 Announce Type: new 
Abstract: User authorization-based access privileges are a key feature in many safety-critical systems, but have thus far been absent from the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, and resistance to prompt-based jailbreaking attacks. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)</title>
<link>https://arxiv.org/abs/2505.14608</link>
<guid>https://arxiv.org/abs/2505.14608</guid>
<content:encoded><![CDATA[
arXiv:2505.14608v1 Announce Type: new 
Abstract: Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\unicode{x2013}$the stylistic feature space$\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models</title>
<link>https://arxiv.org/abs/2505.14617</link>
<guid>https://arxiv.org/abs/2505.14617</guid>
<content:encoded><![CDATA[
arXiv:2505.14617v1 Announce Type: new 
Abstract: Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Only When You Need with Large Hybrid-Reasoning Models</title>
<link>https://arxiv.org/abs/2505.14631</link>
<guid>https://arxiv.org/abs/2505.14631</guid>
<content:encoded><![CDATA[
arXiv:2505.14631v1 Announce Type: new 
Abstract: Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas</title>
<link>https://arxiv.org/abs/2505.14633</link>
<guid>https://arxiv.org/abs/2505.14633</guid>
<content:encoded><![CDATA[
arXiv:2505.14633v1 Announce Type: new 
Abstract: Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>General-Reasoner: Advancing LLM Reasoning Across All Domains</title>
<link>https://arxiv.org/abs/2505.14652</link>
<guid>https://arxiv.org/abs/2505.14652</guid>
<content:encoded><![CDATA[
arXiv:2505.14652v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoGist: Efficient In-Context Learning for Visual Emotion Understanding</title>
<link>https://arxiv.org/abs/2505.14660</link>
<guid>https://arxiv.org/abs/2505.14660</guid>
<content:encoded><![CDATA[
arXiv:2505.14660v1 Announce Type: new 
Abstract: In this paper, we introduce EmoGist, a training-free, in-context learning method for performing visual emotion classification with LVLMs. The key intuition of our approach is that context-dependent definition of emotion labels could allow more accurate predictions of emotions, as the ways in which emotions manifest within images are highly context dependent and nuanced. EmoGist pre-generates multiple explanations of emotion labels, by analyzing the clusters of example images belonging to each category. At test time, we retrieve a version of explanation based on embedding similarity, and feed it to a fast VLM for classification. Through our experiments, we show that EmoGist allows up to 13 points improvement in micro F1 scores with the multi-label Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Reasoning Model</title>
<link>https://arxiv.org/abs/2505.14674</link>
<guid>https://arxiv.org/abs/2505.14674</guid>
<content:encoded><![CDATA[
arXiv:2505.14674v1 Announce Type: new 
Abstract: Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models</title>
<link>https://arxiv.org/abs/2505.14679</link>
<guid>https://arxiv.org/abs/2505.14679</guid>
<content:encoded><![CDATA[
arXiv:2505.14679v1 Announce Type: new 
Abstract: Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1/3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning</title>
<link>https://arxiv.org/abs/2505.14684</link>
<guid>https://arxiv.org/abs/2505.14684</guid>
<content:encoded><![CDATA[
arXiv:2505.14684v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models use Lookbacks to Track Beliefs</title>
<link>https://arxiv.org/abs/2505.14685</link>
<guid>https://arxiv.org/abs/2505.14685</guid>
<content:encoded><![CDATA[
arXiv:2505.14685v1 Announce Type: new 
Abstract: How do language models (LMs) represent characters' beliefs, especially when those beliefs may differ from reality? This question lies at the heart of understanding the Theory of Mind (ToM) capabilities of LMs. We analyze Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal mediation and abstraction. We construct a dataset that consists of simple stories where two characters each separately change the state of two objects, potentially unaware of each other's actions. Our investigation uncovered a pervasive algorithmic pattern that we call a lookback mechanism, which enables the LM to recall important information when it becomes necessary. The LM binds each character-object-state triple together by co-locating reference information about them, represented as their Ordering IDs (OIs) in low rank subspaces of the state token's residual stream. When asked about a character's beliefs regarding the state of an object, the binding lookback retrieves the corresponding state OI and then an answer lookback retrieves the state token. When we introduce text specifying that one character is (not) visible to the other, we find that the LM first generates a visibility ID encoding the relation between the observing and the observed character OIs. In a visibility lookback, this ID is used to retrieve information about the observed character and update the observing character's beliefs. Our work provides insights into the LM's belief tracking mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval</title>
<link>https://arxiv.org/abs/2505.13482</link>
<guid>https://arxiv.org/abs/2505.13482</guid>
<content:encoded><![CDATA[
arXiv:2505.13482v1 Announce Type: cross 
Abstract: Embedding models have become essential for retrieval-augmented generation (RAG) tasks, semantic clustering, and text re-ranking. But despite their growing use, many of these come with notable limitations. For example, Jina fails to capture the semantic content of medical documents, while models such as MiniLM often perform poorly on long-form documents. Domain-adapted models, while specialized, often underperform in general-purpose tasks, reducing their overall applicability. General-domain tokenizers often misinterpret medical vocabulary. The limitations of current embedding models, whether in tokenization accuracy, domain comprehension, or handling long sequences, highlight the need for more versatile solutions. In this work, we present MedEIR, a novel embedding model and tokenizer jointly optimized for both medical and general NLP tasks, incorporating ALiBi-based long-context processing to support sequences of up to 8,192 tokens. MedEIR was pre-trained on only 6 billion tokens, significantly fewer than Jina's, followed by fine-tuning on 3 million sentence pairs. MedEIR consistently outperforms Jina V2 and MiniLM across MTEB benchmarks, achieving top scores on ArguAna (55.24), NFCorpus (38.44), MedicalQARetrieval (74.25), SciFact (72.04), and TRECCOVID (79.56). These results highlight the potential of MedEIR as a highly effective embedding model, demonstrating strong performance across both general-purpose and domain-specific tasks and outperforming existing models on multiple benchmarks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Large Language Models for Real-World Engineering Tasks</title>
<link>https://arxiv.org/abs/2505.13484</link>
<guid>https://arxiv.org/abs/2505.13484</guid>
<content:encoded><![CDATA[
arXiv:2505.13484v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are transformative not only for daily activities but also for engineering tasks. However, current evaluations of LLMs in engineering exhibit two critical shortcomings: (i) the reliance on simplified use cases, often adapted from examination materials where correctness is easily verifiable, and (ii) the use of ad hoc scenarios that insufficiently capture critical engineering competencies. Consequently, the assessment of LLMs on complex, real-world engineering problems remains largely unexplored. This paper addresses this gap by introducing a curated database comprising over 100 questions derived from authentic, production-oriented engineering scenarios, systematically designed to cover core competencies such as product design, prognosis, and diagnosis. Using this dataset, we evaluate four state-of-the-art LLMs, including both cloud-based and locally hosted instances, to systematically investigate their performance on complex engineering tasks. Our results show that LLMs demonstrate strengths in basic temporal and structural reasoning but struggle significantly with abstract reasoning, formal modeling, and context-sensitive engineering logic.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer</title>
<link>https://arxiv.org/abs/2505.13489</link>
<guid>https://arxiv.org/abs/2505.13489</guid>
<content:encoded><![CDATA[
arXiv:2505.13489v1 Announce Type: cross 
Abstract: Knowledge tracing (KT) aims to predict learners' future performance based on historical learning interactions. However, existing KT models predominantly focus on data from a single course, limiting their ability to capture a comprehensive understanding of learners' knowledge states. In this paper, we propose TransKT, a contrastive cross-course knowledge tracing method that leverages concept graph guided knowledge transfer to model the relationships between learning behaviors across different courses, thereby enhancing knowledge state estimation. Specifically, TransKT constructs a cross-course concept graph by leveraging zero-shot Large Language Model (LLM) prompts to establish implicit links between related concepts across different courses. This graph serves as the foundation for knowledge transfer, enabling the model to integrate and enhance the semantic features of learners' interactions across courses. Furthermore, TransKT includes an LLM-to-LM pipeline for incorporating summarized semantic features, which significantly improves the performance of Graph Convolutional Networks (GCNs) used for knowledge transfer. Additionally, TransKT employs a contrastive objective that aligns single-course and cross-course knowledge states, thereby refining the model's ability to provide a more robust and accurate representation of learners' overall knowledge states.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale</title>
<link>https://arxiv.org/abs/2505.13511</link>
<guid>https://arxiv.org/abs/2505.13511</guid>
<content:encoded><![CDATA[
arXiv:2505.13511v1 Announce Type: cross 
Abstract: This study explores Large Language Models (LLMs) as autonomous agents for real-world tasks, including freelance software development. This work presents a new benchmark that evaluates LLMs on freelance programming and data analysis tasks derived from economic data. We construct the benchmark using synthetic tasks created from a Kaggle Freelancer dataset of job postings, with all job prices standardized to USD (median fixed-project price around $250, and an average of $306). Each task is accompanied by structured input-output test cases and an estimated price tag, enabling automated correctness checking and a monetary performance valuation. This approach is inspired by OpenAI's recent SWE-Lancer benchmark (1,400 real Upwork tasks worth $1M total). Still, our framework simplifies evaluation using programmatically testable tasks and predicted price values, making it highly scalable and repeatable. On this benchmark, we evaluate four modern LLMs - Claude 3.5 Haiku, GPT-4o-mini, Qwen 2.5, and Mistral. We report each model's accuracy (task success rate and test-case pass rate) and the total "freelance earnings" it achieves (sum of prices of solved tasks). Our results show that Claude 3.5 Haiku performs best, earning approximately $1.52 million USD, followed closely by GPT-4o-mini at $1.49 million, then Qwen 2.5 ($1.33M) and Mistral ($0.70M). We analyze the distribution of errors per task and observe that the strongest models solve the most tasks and rarely fail completely on any project. We discuss the implications of these results for the feasibility of AI as a freelance developer, the advantages and limitations of our automated benchmark approach, and the gap between performance on structured tasks versus the true complexity of real-world freelance jobs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades</title>
<link>https://arxiv.org/abs/2505.13515</link>
<guid>https://arxiv.org/abs/2505.13515</guid>
<content:encoded><![CDATA[
arXiv:2505.13515v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) are frequently updated, LoRA weights trained on earlier versions quickly become obsolete. The conventional practice of retraining LoRA weights from scratch on the latest model is costly, time-consuming, and environmentally detrimental, particularly as the diversity of LLMs and downstream tasks expands. This motivates a critical question: "How can we efficiently leverage existing LoRA weights to adapt to newer model versions?" To address this, we propose LoRASuite, a modular approach tailored specifically to various types of LLM updates. First, we compute a transfer matrix utilizing known parameters from both old and new LLMs. Next, we allocate corresponding layers and attention heads based on centered kernel alignment and cosine similarity metrics, respectively. A subsequent small-scale, skillful fine-tuning step ensures numerical stability. Experimental evaluations demonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA methods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even exceeds the performance of full-scale LoRA retraining, with average improvements of +1.4 and +6.6 points on math tasks, respectively. Additionally, LoRASuite significantly reduces memory consumption by 5.5 GB and computational time by 78.23%.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs</title>
<link>https://arxiv.org/abs/2505.13529</link>
<guid>https://arxiv.org/abs/2505.13529</guid>
<content:encoded><![CDATA[
arXiv:2505.13529v1 Announce Type: cross 
Abstract: Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with "I don't know". Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL-a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference</title>
<link>https://arxiv.org/abs/2505.13531</link>
<guid>https://arxiv.org/abs/2505.13531</guid>
<content:encoded><![CDATA[
arXiv:2505.13531v1 Announce Type: cross 
Abstract: Assessing Large Language Models (LLMs)' underlying value differences enables comprehensive comparison of their misalignment, cultural adaptability, and biases. Nevertheless, current value measurement datasets face the informativeness challenge: with often outdated, contaminated, or generic test questions, they can only capture the shared value orientations among different LLMs, leading to saturated and thus uninformative results. To address this problem, we introduce AdAEM, a novel, self-extensible assessment framework for revealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM can automatically and adaptively generate and extend its test questions. This is achieved by probing the internal value boundaries of a diverse set of LLMs developed across cultures and time periods in an in-context optimization manner. The optimization process theoretically maximizes an information-theoretic objective to extract the latest or culturally controversial topics, providing more distinguishable and informative insights about models' value differences. In this way, AdAEM is able to co-evolve with the development of LLMs, consistently tracking their value dynamics. Using AdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct an extensive analysis to manifest our method's validity and effectiveness, and benchmark the values of 16 LLMs, laying the groundwork for better value research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data</title>
<link>https://arxiv.org/abs/2505.13534</link>
<guid>https://arxiv.org/abs/2505.13534</guid>
<content:encoded><![CDATA[
arXiv:2505.13534v1 Announce Type: cross 
Abstract: Finding interesting phenomena is the core of scientific discovery, but it is a manual, ill-defined concept. We present an integrative pipeline for automating the discovery of interesting simple hypotheses (feature-target relations with effect direction and a potential underlying mechanism) in structured biomedical data. The pipeline combines machine learning, knowledge graphs, literature search and Large Language Models. We formalize "interestingness" as a combination of novelty, utility and plausibility. On 8 major diseases from the UK Biobank, our pipeline consistently recovers risk factors years before their appearance in the literature. 40--53% of our top candidates were validated as interesting, compared to 0--7% for a SHAP-based baseline. Overall, 28% of 109 candidates were interesting to medical experts. The pipeline addresses the challenge of operationalizing "interestingness" scalably and for any target. We release data and code: https://github.com/LinialLab/InterFeat
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection</title>
<link>https://arxiv.org/abs/2505.13581</link>
<guid>https://arxiv.org/abs/2505.13581</guid>
<content:encoded><![CDATA[
arXiv:2505.13581v1 Announce Type: cross 
Abstract: Content moderation for large language models (LLMs) remains a significant challenge, requiring flexible and adaptable solutions that can quickly respond to emerging threats. This paper introduces Retrieval Augmented Rejection (RAR), a novel approach that leverages a retrieval-augmented generation (RAG) architecture to dynamically reject unsafe user queries without model retraining. By strategically inserting and marking malicious documents into the vector database, the system can identify and reject harmful requests when these documents are retrieved. Our preliminary results show that RAR achieves comparable performance to embedded moderation in LLMs like Claude 3.5 Sonnet, while offering superior flexibility and real-time customization capabilities, a fundamental feature to timely address critical vulnerabilities. This approach introduces no architectural changes to existing RAG systems, requiring only the addition of specially crafted documents and a simple rejection mechanism based on retrieval results.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents</title>
<link>https://arxiv.org/abs/2505.13652</link>
<guid>https://arxiv.org/abs/2505.13652</guid>
<content:encoded><![CDATA[
arXiv:2505.13652v1 Announce Type: cross 
Abstract: Large language models (LLMs) have recently achieved remarkable results in complex multi-step tasks, such as mathematical reasoning and agentic software engineering. However, they often struggle to maintain consistent performance across multiple solution attempts. One effective approach to narrow the gap between average-case and best-case performance is guided test-time search, which explores multiple solution paths to identify the most promising one. Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for non-serializable RL environments, such as Docker containers, where intermediate environment states cannot be easily saved and restored. We investigate two complementary search strategies applicable to such environments: 1-step lookahead and trajectory selection, both guided by a learned action-value function estimator. On the SWE-bench Verified benchmark, a key testbed for agentic software engineering, we find these methods to double the average success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new state-of-the-art for open-weights models. Additionally, we show that these techniques are transferable to more advanced closed models, yielding similar improvements with GPT-4o.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings</title>
<link>https://arxiv.org/abs/2505.13718</link>
<guid>https://arxiv.org/abs/2505.13718</guid>
<content:encoded><![CDATA[
arXiv:2505.13718v1 Announce Type: cross 
Abstract: Designing effective reasoning-capable LLMs typically requires training using Reinforcement Learning with Verifiable Rewards (RLVR) or distillation with carefully curated Long Chain of Thoughts (CoT), both of which depend heavily on extensive training data. This creates a major challenge when the amount of quality training data is scarce. We propose a sample-efficient, two-stage training strategy to develop reasoning LLMs under limited supervision. In the first stage, we "warm up" the model by distilling Long CoTs from a toy domain, namely, Knights \& Knaves (K\&amp;K) logic puzzles to acquire general reasoning skills. In the second stage, we apply RLVR to the warmed-up model using a limited set of target-domain examples. Our experiments demonstrate that this two-phase approach offers several benefits: $(i)$ the warmup phase alone facilitates generalized reasoning, leading to performance improvements across a range of tasks, including MATH, HumanEval$^{+}$, and MMLU-Pro. $(ii)$ When both the base model and the warmed-up model are RLVR trained on the same small dataset ($\leq100$ examples), the warmed-up model consistently outperforms the base model; $(iii)$ Warming up before RLVR training allows a model to maintain cross-domain generalizability even after training on a specific domain; $(iv)$ Introducing warmup in the pipeline improves not only accuracy but also overall sample efficiency during RLVR training. The results in this paper highlight the promise of warmup for building robust reasoning LLMs in data-scarce environments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training</title>
<link>https://arxiv.org/abs/2505.13738</link>
<guid>https://arxiv.org/abs/2505.13738</guid>
<content:encoded><![CDATA[
arXiv:2505.13738v1 Announce Type: cross 
Abstract: Efficient LLM pre-training requires well-tuned hyperparameters (HPs), including learning rate {\eta} and weight decay {\lambda}. We study scaling laws for HPs: formulas for how to scale HPs as we scale model size N, dataset size D, and batch size B. Recent work suggests the AdamW timescale, B/({\eta}{\lambda}D), should remain constant across training settings, and we verify the implication that optimal {\lambda} scales linearly with B, for a fixed N,D. However, as N,D scale, we show the optimal timescale obeys a precise power law in the tokens-per-parameter ratio, D/N. This law thus provides a method to accurately predict {\lambda}opt in advance of large-scale training. We also study scaling laws for optimal batch size Bopt (the B enabling lowest loss at a given N,D) and critical batch size Bcrit (the B beyond which further data parallelism becomes ineffective). In contrast with prior work, we find both Bopt and Bcrit scale as power laws in D, independent of model size, N. Finally, we analyze how these findings inform the real-world selection of Pareto-optimal N and D under dual training time and compute objectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Compact Reranking with Document Features for Scientific Retrieval</title>
<link>https://arxiv.org/abs/2505.13757</link>
<guid>https://arxiv.org/abs/2505.13757</guid>
<content:encoded><![CDATA[
arXiv:2505.13757v1 Announce Type: cross 
Abstract: Scientific retrieval is essential for advancing academic discovery. Within this process, document reranking plays a critical role by refining first-stage retrieval results. However, large language model (LLM) listwise reranking faces unique challenges in the scientific domain. First-stage retrieval is often suboptimal in the scientific domain, so relevant documents are ranked lower. Moreover, conventional listwise reranking uses the full text of candidate documents in the context window, limiting the number of candidates that can be considered. As a result, many relevant documents are excluded before reranking, which constrains overall retrieval performance. To address these challenges, we explore compact document representations based on semantic features such as categories, sections, and keywords, and propose a training-free, model-agnostic reranking framework for scientific retrieval called CoRank. The framework involves three stages: (i) offline extraction of document-level features, (ii) coarse reranking using these compact representations, and (iii) fine-grained reranking on full texts of the top candidates from stage (ii). This hybrid design provides a high-level abstraction of document semantics, expands candidate coverage, and retains critical details required for precise ranking. Experiments on LitSearch and CSFCube show that CoRank significantly improves reranking performance across different LLM backbones, increasing nDCG@10 from 32.0 to 39.7. Overall, these results highlight the value of information extraction for reranking in scientific retrieval.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations</title>
<link>https://arxiv.org/abs/2505.13763</link>
<guid>https://arxiv.org/abs/2505.13763</guid>
<content:encoded><![CDATA[
arXiv:2505.13763v1 Announce Type: cross 
Abstract: Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society's increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a "metacognitive space" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques</title>
<link>https://arxiv.org/abs/2505.13766</link>
<guid>https://arxiv.org/abs/2505.13766</guid>
<content:encoded><![CDATA[
arXiv:2505.13766v1 Announce Type: cross 
Abstract: Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference</title>
<link>https://arxiv.org/abs/2505.13770</link>
<guid>https://arxiv.org/abs/2505.13770</guid>
<content:encoded><![CDATA[
arXiv:2505.13770v1 Announce Type: cross 
Abstract: Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Agent Distillation for Large Language Model</title>
<link>https://arxiv.org/abs/2505.13820</link>
<guid>https://arxiv.org/abs/2505.13820</guid>
<content:encoded><![CDATA[
arXiv:2505.13820v1 Announce Type: cross 
Abstract: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forensic deepfake audio detection using segmental speech features</title>
<link>https://arxiv.org/abs/2505.13847</link>
<guid>https://arxiv.org/abs/2505.13847</guid>
<content:encoded><![CDATA[
arXiv:2505.13847v1 Announce Type: cross 
Abstract: This study explores the potential of using acoustic features of segmental speech sounds to detect deepfake audio. These features are highly interpretable because of their close relationship with human articulatory processes and are expected to be more difficult for deepfake models to replicate. The results demonstrate that certain segmental features commonly used in forensic voice comparison are effective in identifying deep-fakes, whereas some global features provide little value. These findings underscore the need to approach audio deepfake detection differently for forensic voice comparison and offer a new perspective on leveraging segmental features for this purpose.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks</title>
<link>https://arxiv.org/abs/2505.13862</link>
<guid>https://arxiv.org/abs/2505.13862</guid>
<content:encoded><![CDATA[
arXiv:2505.13862v1 Announce Type: cross 
Abstract: Large language models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial prompts known as jailbreaks, which can bypass safety alignment and elicit harmful outputs. Despite growing efforts in LLM safety research, existing evaluations are often fragmented, focused on isolated attack or defense techniques, and lack systematic, reproducible analysis. In this work, we introduce PandaGuard, a unified and modular framework that models LLM jailbreak safety as a multi-agent system comprising attackers, defenders, and judges. Our framework implements 19 attack methods and 12 defense mechanisms, along with multiple judgment strategies, all within a flexible plugin architecture supporting diverse LLM interfaces, multiple interaction modes, and configuration-driven experimentation that enhances reproducibility and practical deployment. Built on this framework, we develop PandaBench, a comprehensive benchmark that evaluates the interactions between these attack/defense methods across 49 LLMs and various judgment approaches, requiring over 3 billion tokens to execute. Our extensive evaluation reveals key insights into model vulnerabilities, defense cost-performance trade-offs, and judge consistency. We find that no single defense is optimal across all dimensions and that judge disagreement introduces nontrivial variance in safety assessments. We release the code, configurations, and evaluation results to support transparent and reproducible research in LLM safety.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13878</link>
<guid>https://arxiv.org/abs/2505.13878</guid>
<content:encoded><![CDATA[
arXiv:2505.13878v1 Announce Type: cross 
Abstract: Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) --a critical phase for enhancing LLM performance--largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improve its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation</title>
<link>https://arxiv.org/abs/2505.13887</link>
<guid>https://arxiv.org/abs/2505.13887</guid>
<content:encoded><![CDATA[
arXiv:2505.13887v1 Announce Type: cross 
Abstract: The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Agent Training for Computer Use</title>
<link>https://arxiv.org/abs/2505.13909</link>
<guid>https://arxiv.org/abs/2505.13909</guid>
<content:encoded><![CDATA[
arXiv:2505.13909v1 Announce Type: cross 
Abstract: Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLZero: A Multi-Agent System for End-to-end Machine Learning Automation</title>
<link>https://arxiv.org/abs/2505.13941</link>
<guid>https://arxiv.org/abs/2505.13941</guid>
<content:encoded><![CDATA[
arXiv:2505.13941v1 Announce Type: cross 
Abstract: Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.13957</link>
<guid>https://arxiv.org/abs/2505.13957</guid>
<content:encoded><![CDATA[
arXiv:2505.13957v1 Announce Type: cross 
Abstract: Multimodal Retrieval-Augmented Generation (MRAG) systems enhance LMMs by integrating external multimodal databases, but introduce unexplored privacy vulnerabilities. While text-based RAG privacy risks have been studied, multimodal data presents unique challenges. We provide the first systematic analysis of MRAG privacy vulnerabilities across vision-language and speech-language modalities. Using a novel compositional structured prompt attack in a black-box setting, we demonstrate how attackers can extract private information by manipulating queries. Our experiments reveal that LMMs can both directly generate outputs resembling retrieved content and produce descriptions that indirectly expose sensitive information, highlighting the urgent need for robust privacy-preserving MRAG techniques.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data</title>
<link>https://arxiv.org/abs/2505.14038</link>
<guid>https://arxiv.org/abs/2505.14038</guid>
<content:encoded><![CDATA[
arXiv:2505.14038v1 Announce Type: cross 
Abstract: Mental health risk is a critical global public health challenge, necessitating innovative and reliable assessment methods. With the development of large language models (LLMs), they stand out to be a promising tool for explainable mental health care applications. Nevertheless, existing approaches predominantly rely on subjective textual mental records, which can be distorted by inherent mental uncertainties, leading to inconsistent and unreliable predictions. To address these limitations, this paper introduces ProMind-LLM. We investigate an innovative approach integrating objective behavior data as complementary information alongside subjective mental records for robust mental health risk assessment. Specifically, ProMind-LLM incorporates a comprehensive pipeline that includes domain-specific pretraining to tailor the LLM for mental health contexts, a self-refine mechanism to optimize the processing of numerical behavioral data, and causal chain-of-thought reasoning to enhance the reliability and interpretability of its predictions. Evaluations of two real-world datasets, PMData and Globem, demonstrate the effectiveness of our proposed methods, achieving substantial improvements over general LLMs. We anticipate that ProMind-LLM will pave the way for more dependable, interpretable, and scalable mental health case solutions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.14071</link>
<guid>https://arxiv.org/abs/2505.14071</guid>
<content:encoded><![CDATA[
arXiv:2505.14071v1 Announce Type: cross 
Abstract: Steering methods have emerged as effective and targeted tools for guiding large language models' (LLMs) behavior without modifying their parameters. Multimodal large language models (MLLMs), however, do not currently enjoy the same suite of techniques, due in part to their recency and architectural diversity. Inspired by this gap, we investigate whether MLLMs can be steered using vectors derived from their text-only LLM backbone, via sparse autoencoders (SAEs), mean shift, and linear probing. We find that text-derived steering consistently enhances multimodal accuracy across diverse MLLM architectures and visual tasks. In particular, mean shift boosts spatial relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to +3.3%, outperforming prompting and exhibiting strong generalization to out-of-distribution datasets. These results highlight textual steering vectors as a powerful, efficient mechanism for enhancing grounding in MLLMs with minimal additional data collection and computational overhead.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>s3: You Don't Need That Much Data to Train a Search Agent via RL</title>
<link>https://arxiv.org/abs/2505.14146</link>
<guid>https://arxiv.org/abs/2505.14146</guid>
<content:encoded><![CDATA[
arXiv:2505.14146v1 Announce Type: cross 
Abstract: Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Subspaces are Not Distinct: A Fine-Tuning Case Study</title>
<link>https://arxiv.org/abs/2505.14185</link>
<guid>https://arxiv.org/abs/2505.14185</guid>
<content:encoded><![CDATA[
arXiv:2505.14185v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) rely on safety alignment to produce socially acceptable responses. This is typically achieved through instruction tuning and reinforcement learning from human feedback. However, this alignment is known to be brittle: further fine-tuning, even on benign or lightly contaminated data, can degrade safety and reintroduce harmful behaviors. A growing body of work suggests that alignment may correspond to identifiable geometric directions in weight space, forming subspaces that could, in principle, be isolated or preserved to defend against misalignment. In this work, we conduct a comprehensive empirical study of this geometric perspective. We examine whether safety-relevant behavior is concentrated in specific subspaces, whether it can be separated from general-purpose learning, and whether harmfulness arises from distinguishable patterns in internal representations. Across both parameter and activation space, our findings are consistent: subspaces that amplify safe behaviors also amplify unsafe ones, and prompts with different safety implications activate overlapping representations. We find no evidence of a subspace that selectively governs safety. These results challenge the assumption that alignment is geometrically localized. Rather than residing in distinct directions, safety appears to emerge from entangled, high-impact components of the model's broader learning dynamics. This suggests that subspace-based defenses may face fundamental limitations and underscores the need for alternative strategies to preserve alignment under continued training. We corroborate these findings through multiple experiments on five open-source LLMs. Our code is publicly available at: https://github.com/CERT-Lab/safety-subspaces.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14216</link>
<guid>https://arxiv.org/abs/2505.14216</guid>
<content:encoded><![CDATA[
arXiv:2505.14216v1 Announce Type: cross 
Abstract: Recent studies have shown that reinforcement learning with verifiable rewards (RLVR) enhances overall accuracy but fails to improve capability, while distillation can improve both. In this paper, we investigate the mechanisms behind these phenomena. First, we demonstrate that RLVR does not improve capability because it focuses on improving the accuracy of the less-difficult questions to the detriment of the accuracy of the most difficult questions, thereby leading to no improvement in capability. Second, we find that RLVR does not merely increase the success probability for the less difficult questions, but in our small model settings produces quality responses that were absent in its output distribution before training. In addition, we show these responses are neither noticeably longer nor feature more reflection-related keywords, underscoring the need for more reliable indicators of response quality. Third, we show that while distillation reliably improves accuracy by learning strong reasoning patterns, it only improves capability when new knowledge is introduced. Moreover, when distilling only with reasoning patterns and no new knowledge, the accuracy of the less-difficult questions improves to the detriment of the most difficult questions, similar to RLVR. Together, these findings offer a clearer understanding of how RLVR and distillation shape reasoning behavior in language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum</title>
<link>https://arxiv.org/abs/2505.14264</link>
<guid>https://arxiv.org/abs/2505.14264</guid>
<content:encoded><![CDATA[
arXiv:2505.14264v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has emerged as an effective approach for enhancing the reasoning capabilities of large language models (LLMs), especially in scenarios where supervised fine-tuning (SFT) falls short due to limited chain-of-thought (CoT) data. Among RL-based post-training methods, group relative advantage estimation, as exemplified by Group Relative Policy Optimization (GRPO), has attracted considerable attention for eliminating the dependency on the value model, thereby simplifying training compared to traditional approaches like Proximal Policy Optimization (PPO). However, we observe that exsiting group relative advantage estimation method still suffers from training inefficiencies, particularly when the estimated advantage approaches zero. To address this limitation, we propose Advantage-Augmented Policy Optimization (AAPO), a novel RL algorithm that optimizes the cross-entropy (CE) loss using advantages enhanced through a momentum-based estimation scheme. This approach effectively mitigates the inefficiencies associated with group relative advantage estimation. Experimental results on multiple mathematical reasoning benchmarks demonstrate the superior performance of AAPO.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors</title>
<link>https://arxiv.org/abs/2505.14300</link>
<guid>https://arxiv.org/abs/2505.14300</guid>
<content:encoded><![CDATA[
arXiv:2505.14300v1 Announce Type: cross 
Abstract: High-risk industries like nuclear and aviation use real-time monitoring to detect dangerous system conditions. Similarly, Large Language Models (LLMs) need monitoring safeguards. We propose a real-time framework to predict harmful AI outputs before they occur by using an unsupervised approach that treats normal behavior as the baseline and harmful outputs as outliers. Our study focuses specifically on backdoor-triggered responses -- where specific input phrases activate hidden vulnerabilities causing the model to generate unsafe content like violence, pornography, or hate speech. We address two key challenges: (1) identifying true causal indicators rather than surface correlations, and (2) preventing advanced models from deception -- deliberately evading monitoring systems. Hence, we approach this problem from an unsupervised lens by drawing parallels to human deception: just as humans exhibit physical indicators while lying, we investigate whether LLMs display distinct internal behavioral signatures when generating harmful content. Our study addresses two critical challenges: 1) designing monitoring systems that capture true causal indicators rather than superficial correlations; and 2)preventing intentional evasion by increasingly capable "Future models''. Our findings show that models can produce harmful content through causal mechanisms and can become deceptive by: (a) alternating between linear and non-linear representations, and (b) modifying feature relationships. To counter this, we developed Safety-Net -- a multi-detector framework that monitors different representation dimensions, successfully detecting harmful behavior even when information is shifted across representational spaces to evade individual monitors. Our evaluation shows 96% accuracy in detecting harmful cases using our unsupervised ensemble approach.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Law for Quantization-Aware Training</title>
<link>https://arxiv.org/abs/2505.14302</link>
<guid>https://arxiv.org/abs/2505.14302</guid>
<content:encoded><![CDATA[
arXiv:2505.14302v1 Announce Type: cross 
Abstract: Large language models (LLMs) demand substantial computational and memory resources, creating deployment challenges. Quantization-aware training (QAT) addresses these challenges by reducing model precision while maintaining performance. However, the scaling behavior of QAT, especially at 4-bit precision (W4A4), is not well understood. Existing QAT scaling laws often ignore key factors such as the number of training tokens and quantization granularity, which limits their applicability. This paper proposes a unified scaling law for QAT that models quantization error as a function of model size, training data volume, and quantization group size. Through 268 QAT experiments, we show that quantization error decreases as model size increases, but rises with more training tokens and coarser quantization granularity. To identify the sources of W4A4 quantization error, we decompose it into weight and activation components. Both components follow the overall trend of W4A4 quantization error, but with different sensitivities. Specifically, weight quantization error increases more rapidly with more training tokens. Further analysis shows that the activation quantization error in the FC2 layer, caused by outliers, is the primary bottleneck of W4A4 QAT quantization error. By applying mixed-precision quantization to address this bottleneck, we demonstrate that weight and activation quantization errors can converge to similar levels. Additionally, with more training data, weight quantization error eventually exceeds activation quantization error, suggesting that reducing weight quantization error is also important in such scenarios. These findings offer key insights for improving QAT research and development.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection</title>
<link>https://arxiv.org/abs/2505.14318</link>
<guid>https://arxiv.org/abs/2505.14318</guid>
<content:encoded><![CDATA[
arXiv:2505.14318v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in various domains, including radiology report generation. Previous approaches have attempted to utilize multimodal LLMs for this task, enhancing their performance through the integration of domain-specific knowledge retrieval. However, these approaches often overlook the knowledge already embedded within the LLMs, leading to redundant information integration and inefficient utilization of learned representations. To address this limitation, we propose RADAR, a framework for enhancing radiology report generation with supplementary knowledge injection. RADAR improves report generation by systematically leveraging both the internal knowledge of an LLM and externally retrieved information. Specifically, it first extracts the model's acquired knowledge that aligns with expert image-based classification outputs. It then retrieves relevant supplementary knowledge to further enrich this information. Finally, by aggregating both sources, RADAR generates more accurate and informative radiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU X-ray demonstrate that our model outperforms state-of-the-art LLMs in both language quality and clinical accuracy
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation</title>
<link>https://arxiv.org/abs/2505.14351</link>
<guid>https://arxiv.org/abs/2505.14351</guid>
<content:encoded><![CDATA[
arXiv:2505.14351v1 Announce Type: cross 
Abstract: Tibetan is a low-resource language with minimal parallel speech corpora spanning its three major dialects-\"U-Tsang, Amdo, and Kham-limiting progress in speech modeling. To address this issue, we propose FMSD-TTS, a few-shot, multi-speaker, multi-dialect text-to-speech framework that synthesizes parallel dialectal speech from limited reference audio and explicit dialect labels. Our method features a novel speaker-dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects while preserving speaker identity. Extensive objective and subjective evaluations demonstrate that FMSD-TTS significantly outperforms baselines in both dialectal expressiveness and speaker similarity. We further validate the quality and utility of the synthesized speech through a challenging speech-to-speech dialect conversion task. Our contributions include: (1) a novel few-shot TTS system tailored for Tibetan multi-dialect speech synthesis, (2) the public release of a large-scale synthetic Tibetan speech corpus generated by FMSD-TTS, and (3) an open-source evaluation toolkit for standardized assessment of speaker similarity, dialect consistency, and audio quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs</title>
<link>https://arxiv.org/abs/2505.14356</link>
<guid>https://arxiv.org/abs/2505.14356</guid>
<content:encoded><![CDATA[
arXiv:2505.14356v1 Announce Type: cross 
Abstract: Despite significant progress in neural spoken dialog systems, personality-aware conversation agents -- capable of adapting behavior based on personalities -- remain underexplored due to the absence of personality annotations in speech datasets. We propose a pipeline that preprocesses raw audio recordings to create a dialogue dataset annotated with timestamps, response types, and emotion/sentiment labels. We employ an automatic speech recognition (ASR) system to extract transcripts and timestamps, then generate conversation-level annotations. Leveraging these annotations, we design a system that employs large language models to predict conversational personality. Human evaluators were engaged to identify conversational characteristics and assign personality labels. Our analysis demonstrates that the proposed system achieves stronger alignment with human judgments compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs</title>
<link>https://arxiv.org/abs/2505.14368</link>
<guid>https://arxiv.org/abs/2505.14368</guid>
<content:encoded><![CDATA[
arXiv:2505.14368v1 Announce Type: cross 
Abstract: Recent studies demonstrate that Large Language Models (LLMs) are vulnerable to different prompt-based attacks, generating harmful content or sensitive information. Both closed-source and open-source LLMs are underinvestigated for these attacks. This paper studies effective prompt injection attacks against the $\mathbf{14}$ most popular open-source LLMs on five attack benchmarks. Current metrics only consider successful attacks, whereas our proposed Attack Success Probability (ASP) also captures uncertainty in the model's response, reflecting ambiguity in attack feasibility. By comprehensively analyzing the effectiveness of prompt injection attacks, we propose a simple and effective hypnotism attack; results show that this attack causes aligned language models, including Stablelm2, Mistral, Openchat, and Vicuna, to generate objectionable behaviors, achieving around $90$% ASP. They also indicate that our ignore prefix attacks can break all $\mathbf{14}$ open-source LLMs, achieving over $60$% ASP on a multi-categorical dataset. We find that moderately well-known LLMs exhibit higher vulnerability to prompt injection attacks, highlighting the need to raise public awareness and prioritize efficient mitigation strategies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds</title>
<link>https://arxiv.org/abs/2505.14396</link>
<guid>https://arxiv.org/abs/2505.14396</guid>
<content:encoded><![CDATA[
arXiv:2505.14396v1 Announce Type: cross 
Abstract: Causal world models are systems that can answer counterfactual questions about an environment of interest, i.e. predict how it would have evolved if an arbitrary subset of events had been realized differently. It requires understanding the underlying causes behind chains of events and conducting causal inference for arbitrary unseen distributions. So far, this task eludes foundation models, notably large language models (LLMs), which do not have demonstrated causal reasoning capabilities beyond the memorization of existing causal relationships. Furthermore, evaluating counterfactuals in real-world applications is challenging since only the factual world is observed, limiting evaluation to synthetic datasets. We address these problems by explicitly extracting and modeling causal relationships and propose the Causal Cartographer framework. First, we introduce a graph retrieval-augmented generation agent tasked to retrieve causal relationships from data. This approach allows us to construct a large network of real-world causal relationships that can serve as a repository of causal knowledge and build real-world counterfactuals. In addition, we create a counterfactual reasoning agent constrained by causal relationships to perform reliable step-by-step causal inference. We show that our approach can extract causal knowledge and improve the robustness of LLMs for causal reasoning tasks while reducing inference costs and spurious correlations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking</title>
<link>https://arxiv.org/abs/2505.14402</link>
<guid>https://arxiv.org/abs/2505.14402</guid>
<content:encoded><![CDATA[
arXiv:2505.14402v1 Announce Type: cross 
Abstract: The code of nature, embedded in DNA and RNA genomes since the origin of life, holds immense potential to impact both humans and ecosystems through genome modeling. Genomic Foundation Models (GFMs) have emerged as a transformative approach to decoding the genome. As GFMs scale up and reshape the landscape of AI-driven genomics, the field faces an urgent need for rigorous and reproducible evaluation. We present OmniGenBench, a modular benchmarking platform designed to unify the data, model, benchmarking, and interpretability layers across GFMs. OmniGenBench enables standardized, one-command evaluation of any GFM across five benchmark suites, with seamless integration of over 31 open-source models. Through automated pipelines and community-extensible features, the platform addresses critical reproducibility challenges, including data transparency, model interoperability, benchmark fragmentation, and black-box interpretability. OmniGenBench aims to serve as foundational infrastructure for reproducible genomic AI research, accelerating trustworthy discovery and collaborative innovation in the era of genome-scale modeling.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pairwise Evaluation of Accent Similarity in Speech Synthesis</title>
<link>https://arxiv.org/abs/2505.14410</link>
<guid>https://arxiv.org/abs/2505.14410</guid>
<content:encoded><![CDATA[
arXiv:2505.14410v1 Announce Type: cross 
Abstract: Despite growing interest in generating high-fidelity accents, evaluating accent similarity in speech synthesis has been underexplored. We aim to enhance both subjective and objective evaluation methods for accent similarity. Subjectively, we refine the XAB listening test by adding components that achieve higher statistical significance with fewer listeners and lower costs. Our method involves providing listeners with transcriptions, having them highlight perceived accent differences, and implementing meticulous screening for reliability. Objectively, we utilise pronunciation-related metrics, based on distances between vowel formants and phonetic posteriorgrams, to evaluate accent generation. Comparative experiments reveal that these metrics, alongside accent similarity, speaker similarity, and Mel Cepstral Distortion, can be used. Moreover, our findings underscore significant limitations of common metrics like Word Error Rate in assessing underrepresented accents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRL: Prompts from Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.14412</link>
<guid>https://arxiv.org/abs/2505.14412</guid>
<content:encoded><![CDATA[
arXiv:2505.14412v1 Announce Type: cross 
Abstract: Effective prompt engineering remains a central challenge in fully harnessing the capabilities of LLMs. While well-designed prompts can dramatically enhance performance, crafting them typically demands expert intuition and a nuanced understanding of the task. Moreover, the most impactful prompts often hinge on subtle semantic cues, ones that may elude human perception but are crucial for guiding LLM behavior. In this paper, we introduce PRL (Prompts from Reinforcement Learning), a novel RL-based approach for automatic prompt generation. Unlike previous methods, PRL can produce novel few-shot examples that were not seen during training. Our approach achieves state-of-the-art performance across a range of benchmarks, including text classification, simplification, and summarization. On the classification task, it surpasses prior methods by 2.58% over APE and 1.00% over EvoPrompt. Additionally, it improves the average ROUGE scores on the summarization task by 4.32 over APE and by 2.12 over EvoPrompt and the SARI score on simplification by 6.93 over APE and by 6.01 over EvoPrompt. Our code is available at https://github.com/Batorskq/prl .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rank-K: Test-Time Reasoning for Listwise Reranking</title>
<link>https://arxiv.org/abs/2505.14432</link>
<guid>https://arxiv.org/abs/2505.14432</guid>
<content:encoded><![CDATA[
arXiv:2505.14432v1 Announce Type: cross 
Abstract: Retrieve-and-rerank is a popular retrieval pipeline because of its ability to make slow but effective rerankers efficient enough at query time by reducing the number of comparisons. Recent works in neural rerankers take advantage of large language models for their capability in reasoning between queries and passages and have achieved state-of-the-art retrieval effectiveness. However, such rerankers are resource-intensive, even after heavy optimization. In this work, we introduce Rank-K, a listwise passage reranking model that leverages the reasoning capability of the reasoning language model at query time that provides test time scalability to serve hard queries. We show that Rank-K improves retrieval effectiveness by 23\% over the RankZephyr, the state-of-the-art listwise reranker, when reranking a BM25 initial ranked list and 19\% when reranking strong retrieval results by SPLADE-v3. Since Rank-K is inherently a multilingual model, we found that it ranks passages based on queries in different languages as effectively as it does in monolingual retrieval.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models</title>
<link>https://arxiv.org/abs/2505.14438</link>
<guid>https://arxiv.org/abs/2505.14438</guid>
<content:encoded><![CDATA[
arXiv:2505.14438v1 Announce Type: cross 
Abstract: End-to-end speech large language models ((LLMs)) extend the capabilities of text-based models to directly process and generate audio tokens. However, this often leads to a decline in reasoning and generation performance compared to text input, a phenomenon referred to as intelligence degradation. To systematically evaluate this gap, we propose S2SBench, a benchmark designed to quantify performance degradation in Speech LLMs. It includes diagnostic datasets targeting sentence continuation and commonsense reasoning under audio input. We further introduce a pairwise evaluation protocol based on perplexity differences between plausible and implausible samples to measure degradation relative to text input. We apply S2SBench to analyze the training process of Baichuan-Audio, which further demonstrates the benchmark's effectiveness. All datasets and evaluation code are available at https://github.com/undobug/S2SBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach</title>
<link>https://arxiv.org/abs/2505.14449</link>
<guid>https://arxiv.org/abs/2505.14449</guid>
<content:encoded><![CDATA[
arXiv:2505.14449v1 Announce Type: cross 
Abstract: While subgroup disparities and performance bias are increasingly studied in computational research, fairness in categorical Speech Emotion Recognition (SER) remains underexplored. Existing methods often rely on explicit demographic labels, which are difficult to obtain due to privacy concerns. To address this limitation, we introduce an Implicit Demography Inference (IDI) module that leverages pseudo-labeling from a pre-trained model and unsupervised learning using k-means clustering to mitigate bias in SER. Our experiments show that pseudo-labeling IDI reduces subgroup disparities, improving fairness metrics by over 33% with less than a 3% decrease in SER accuracy. Also, the unsupervised IDI yields more than a 26% improvement in fairness metrics with a drop of less than 4% in SER performance. Further analyses reveal that the unsupervised IDI consistently mitigates race and age disparities, demonstrating its potential in scenarios where explicit demographic information is unavailable.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding</title>
<link>https://arxiv.org/abs/2505.14462</link>
<guid>https://arxiv.org/abs/2505.14462</guid>
<content:encoded><![CDATA[
arXiv:2505.14462v1 Announce Type: cross 
Abstract: As vision-language models (VLMs) become increasingly integrated into daily life, the need for accurate visual culture understanding is becoming critical. Yet, these models frequently fall short in interpreting cultural nuances effectively. Prior work has demonstrated the effectiveness of retrieval-augmented generation (RAG) in enhancing cultural understanding in text-only settings, while its application in multimodal scenarios remains underexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented Visual culturE uNdErstAnding), a new benchmark designed to advance visual culture understanding through retrieval, focusing on two tasks: culture-focused visual question answering (cVQA) and culture-informed image captioning (cIC). RAVENEA extends existing datasets by integrating over 10,000 Wikipedia documents curated and ranked by human annotators. With RAVENEA, we train and evaluate seven multimodal retrievers for each image query, and measure the downstream impact of retrieval-augmented inputs across fourteen state-of-the-art VLMs. Our results show that lightweight VLMs, when augmented with culture-aware retrieval, outperform their non-augmented counterparts (by at least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the value of retrieval-augmented methods and culturally inclusive benchmarks for multimodal understanding.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PAST: Phonetic-Acoustic Speech Tokenizer</title>
<link>https://arxiv.org/abs/2505.14470</link>
<guid>https://arxiv.org/abs/2505.14470</guid>
<content:encoded><![CDATA[
arXiv:2505.14470v1 Announce Type: cross 
Abstract: We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach</title>
<link>https://arxiv.org/abs/2505.14479</link>
<guid>https://arxiv.org/abs/2505.14479</guid>
<content:encoded><![CDATA[
arXiv:2505.14479v1 Announce Type: cross 
Abstract: Large language models (LLMs) struggle with formal domains that require rigorous logical deduction and symbolic reasoning, such as mathematical proof generation. We propose a neuro-symbolic approach that combines LLMs' generative strengths with structured components to overcome this challenge. As a proof-of-concept, we focus on geometry problems. Our approach is two-fold: (1) we retrieve analogous problems and use their proofs to guide the LLM, and (2) a formal verifier evaluates the generated proofs and provides feedback, helping the model fix incorrect proofs. We demonstrate that our method significantly improves proof accuracy for OpenAI's o1 model (58%-70% improvement); both analogous problems and the verifier's feedback contribute to these gains. More broadly, shifting to LLMs that generate provably correct conclusions could dramatically improve their reliability, accuracy and consistency, unlocking complex tasks and critical real-world applications that require trustworthiness.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning Models Better Express Their Confidence</title>
<link>https://arxiv.org/abs/2505.14489</link>
<guid>https://arxiv.org/abs/2505.14489</guid>
<content:encoded><![CDATA[
arXiv:2505.14489v1 Announce Type: cross 
Abstract: Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models-LLMs that engage in extended chain-of-thought (CoT) reasoning-exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models-such as exploring alternative approaches and backtracking-which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that these gains are not exclusive to reasoning models-non-reasoning models also benefit when guided to perform slow thinking via in-context learning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples</title>
<link>https://arxiv.org/abs/2505.14518</link>
<guid>https://arxiv.org/abs/2505.14518</guid>
<content:encoded><![CDATA[
arXiv:2505.14518v1 Announce Type: cross 
Abstract: Recent advancements in audio-aware large language models (ALLMs) enable them to process and understand audio inputs. However, these models often hallucinate non-existent sound events, reducing their reliability in real-world applications. To address this, we propose LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method that enhances ALLMs' ability to distinguish between present and absent sounds using synthesized data from the backbone LLM. Unlike prior approaches, our method requires no modification to LLM parameters and efficiently integrates audio representations via a lightweight adapter. Experiments show that LISTEN effectively mitigates hallucinations while maintaining impressive performance on existing audio question and reasoning benchmarks. At the same time, it is more efficient in both data and computation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent Context Protocols Enhance Collective Inference</title>
<link>https://arxiv.org/abs/2505.14569</link>
<guid>https://arxiv.org/abs/2505.14569</guid>
<content:encoded><![CDATA[
arXiv:2505.14569v1 Announce Type: cross 
Abstract: AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference -- a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints -- explicit dependency graphs that store intermediate agent outputs -- with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas</title>
<link>https://arxiv.org/abs/2505.14615</link>
<guid>https://arxiv.org/abs/2505.14615</guid>
<content:encoded><![CDATA[
arXiv:2505.14615v1 Announce Type: cross 
Abstract: We introduce SATBench, a benchmark for evaluating the logical reasoning capabilities of large language models (LLMs) through logical puzzles derived from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on inference rule-based reasoning, which often involves deducing conclusions from a set of premises, our approach leverages the search-based nature of SAT problems, where the objective is to find a solution that fulfills a specified set of logical constraints. Each instance in SATBench is generated from a SAT formula, then translated into a story context and conditions using LLMs. The generation process is fully automated and allows for adjustable difficulty by varying the number of clauses. All 2100 puzzles are validated through both LLM-assisted and solver-based consistency checks, with human validation on a subset. Experimental results show that even the strongest model, o4-mini, achieves only 65.0% accuracy on hard UNSAT problems, close to the random baseline of 50%. SATBench exposes fundamental limitations in the search-based logical reasoning abilities of current LLMs and provides a scalable testbed for future research in logical reasoning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs</title>
<link>https://arxiv.org/abs/2505.14620</link>
<guid>https://arxiv.org/abs/2505.14620</guid>
<content:encoded><![CDATA[
arXiv:2505.14620v1 Announce Type: cross 
Abstract: Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and scalable method to fine-tune and customize large language models (LLMs) for application-specific needs. However, tasks that require complex reasoning or deep contextual understanding are often hindered by biases or interference from the base model when using typical decoding methods like greedy or beam search. These biases can lead to generic or task-agnostic responses from the base model instead of leveraging the LoRA-specific adaptations. In this paper, we introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed to maximize the use of task-specific knowledge in LoRA-adapted models, resulting in better downstream performance. CoLD uses contrastive decoding by scoring candidate tokens based on the divergence between the probability distributions of a LoRA-adapted expert model and the corresponding base model. This approach prioritizes tokens that better align with the LoRA's learned representations, enhancing performance for specialized tasks. While effective, a naive implementation of CoLD is computationally expensive because each decoding step requires evaluating multiple token candidates across both models. To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD achieves up to a 5.54% increase in task accuracy while reducing end-to-end latency by 28% compared to greedy decoding. This work provides practical and efficient decoding strategies for fine-tuned LLMs in resource-constrained environments and has broad implications for applied data science in both cloud and on-premises settings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.14625</link>
<guid>https://arxiv.org/abs/2505.14625</guid>
<content:encoded><![CDATA[
arXiv:2505.14625v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has become a powerful tool for enhancing the reasoning abilities of large language models (LLMs) by optimizing their policies with reward signals. Yet, RL's success relies on the reliability of rewards, which are provided by verifiers. In this paper, we expose and analyze a widespread problem--false negatives--where verifiers wrongly reject correct model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals that over 38% of model-generated responses suffer from false negatives, where the verifier fails to recognize correct answers. We show, both empirically and theoretically, that these false negatives severely impair RL training by depriving the model of informative gradient signals and slowing convergence. To mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments existing rule-based methods, which dynamically identifies potential false negatives and recovers valid responses to produce more accurate reward estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts pass rates by up to 10% and accelerates convergence relative to the baseline. Our findings highlight the critical importance of addressing verifier false negatives and offer a practical approach to improve RL-based fine-tuning of LLMs. Our code is available at https://github.com/uw-nsl/TinyV.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Debating for Better Reasoning: An Unsupervised Multimodal Approach</title>
<link>https://arxiv.org/abs/2505.14627</link>
<guid>https://arxiv.org/abs/2505.14627</guid>
<content:encoded><![CDATA[
arXiv:2505.14627v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) gain expertise across diverse domains and modalities, scalable oversight becomes increasingly challenging, particularly when their capabilities may surpass human evaluators. Debate has emerged as a promising mechanism for enabling such oversight. In this work, we extend the debate paradigm to a multimodal setting, exploring its potential for weaker models to supervise and enhance the performance of stronger models. We focus on visual question answering (VQA), where two "sighted" expert vision-language models debate an answer, while a "blind" (text-only) judge adjudicates based solely on the quality of the arguments. In our framework, the experts defend only answers aligned with their beliefs, thereby obviating the need for explicit role-playing and concentrating the debate on instances of expert disagreement. Experiments on several multimodal tasks demonstrate that the debate framework consistently outperforms individual expert models. Moreover, judgments from weaker LLMs can help instill reasoning capabilities in vision-language models through finetuning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models</title>
<link>https://arxiv.org/abs/2505.14629</link>
<guid>https://arxiv.org/abs/2505.14629</guid>
<content:encoded><![CDATA[
arXiv:2505.14629v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) and the abundance of food data have resulted in studies to improve food understanding using LLMs. Despite several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there has been limited research on integrating food related KGs with LLMs. We introduce KERL, a unified system that leverages food KGs and LLMs to provide personalized food recommendations and generates recipes with associated micro-nutritional information. Given a natural language question, KERL extracts entities, retrieves subgraphs from the KG, which are then fed into the LLM as context to select the recipes that satisfy the constraints. Next, our system generates the cooking steps and nutritional information for each recipe. To evaluate our approach, we also develop a benchmark dataset by curating recipe related questions, combined with constraints and personal preferences. Through extensive experiments, we show that our proposed KG-augmented LLM significantly outperforms existing approaches, offering a complete and coherent solution for food recommendation, recipe generation, and nutritional analysis. Our code and benchmark datasets are publicly available at https://github.com/mohbattharani/KERL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference</title>
<link>https://arxiv.org/abs/2505.14638</link>
<guid>https://arxiv.org/abs/2505.14638</guid>
<content:encoded><![CDATA[
arXiv:2505.14638v1 Announce Type: cross 
Abstract: Deep neural networks have achieved state-of-the-art results in a wide range of applications, from natural language processing and computer vision to speech recognition. However, as tasks become increasingly complex, model sizes continue to grow, posing challenges in latency and memory efficiency. To meet these constraints, post-training quantization has emerged as a promising solution. In this paper, we propose a novel hardware-efficient quantization and inference scheme that exploits hardware advantages with minimal accuracy degradation. Specifically, we introduce a W4A8 scheme, where weights are quantized and stored using 4-bit integer precision, and inference computations are performed using 8-bit floating-point arithmetic, demonstrating significant speedups and improved memory utilization compared to 16-bit operations, applicable on various modern accelerators. To mitigate accuracy loss, we develop a novel quantization algorithm, dubbed Dual Precision Quantization (DPQ), that leverages the unique structure of our scheme without introducing additional inference overhead. Experimental results demonstrate improved performance (i.e., increased throughput) while maintaining tolerable accuracy degradation relative to the full-precision model.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Words: Multimodal LLM Knows When to Speak</title>
<link>https://arxiv.org/abs/2505.14654</link>
<guid>https://arxiv.org/abs/2505.14654</guid>
<content:encoded><![CDATA[
arXiv:2505.14654v1 Announce Type: cross 
Abstract: While large language model (LLM)-based chatbots have demonstrated strong capabilities in generating coherent and contextually relevant responses, they often struggle with understanding when to speak, particularly in delivering brief, timely reactions during ongoing conversations. This limitation arises largely from their reliance on text input, lacking the rich contextual cues in real-world human dialogue. In this work, we focus on real-time prediction of response types, with an emphasis on short, reactive utterances that depend on subtle, multimodal signals across vision, audio, and text. To support this, we introduce a new multimodal dataset constructed from real-world conversational videos, containing temporally aligned visual, auditory, and textual streams. This dataset enables fine-grained modeling of response timing in dyadic interactions. Building on this dataset, we propose MM-When2Speak, a multimodal LLM-based model that adaptively integrates visual, auditory, and textual context to predict when a response should occur, and what type of response is appropriate. Experiments show that MM-When2Speak significantly outperforms state-of-the-art unimodal and LLM-based baselines, achieving up to a 4x improvement in response timing accuracy over leading commercial LLMs. These results underscore the importance of multimodal inputs for producing timely, natural, and engaging conversational AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment</title>
<link>https://arxiv.org/abs/2505.14667</link>
<guid>https://arxiv.org/abs/2505.14667</guid>
<content:encoded><![CDATA[
arXiv:2505.14667v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have become powerful tools for complex problem solving, but their structured reasoning pathways can lead to unsafe outputs when exposed to harmful prompts. Existing safety alignment methods reduce harmful outputs but can degrade reasoning depth, leading to significant trade-offs in complex, multi-step tasks, and remain vulnerable to sophisticated jailbreak attacks. To address this, we introduce SAFEPATH, a lightweight alignment method that fine-tunes LRMs to emit a short, 8-token Safety Primer at the start of their reasoning, in response to harmful prompts, while leaving the rest of the reasoning process unsupervised. Empirical results across multiple benchmarks indicate that SAFEPATH effectively reduces harmful outputs while maintaining reasoning performance. Specifically, SAFEPATH reduces harmful responses by up to 90.0% and blocks 83.3% of jailbreak attempts in the DeepSeek-R1-Distill-Llama-8B model, while requiring 295.9x less compute than Direct Refusal and 314.1x less than SafeChain. We further introduce a zero-shot variant that requires no fine-tuning. In addition, we provide a comprehensive analysis of how existing methods in LLMs generalize, or fail, when applied to reasoning-centric models, revealing critical gaps and new directions for safer AI.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions</title>
<link>https://arxiv.org/abs/2505.14668</link>
<guid>https://arxiv.org/abs/2505.14668</guid>
<content:encoded><![CDATA[
arXiv:2505.14668v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search</title>
<link>https://arxiv.org/abs/2505.14680</link>
<guid>https://arxiv.org/abs/2505.14680</guid>
<content:encoded><![CDATA[
arXiv:2505.14680v1 Announce Type: cross 
Abstract: Generative AI search is reshaping information retrieval by offering end-to-end answers to complex queries, reducing users' reliance on manually browsing and summarizing multiple web pages. However, while this paradigm enhances convenience, it disrupts the feedback-driven improvement loop that has historically powered the evolution of traditional Web search. Web search can continuously improve their ranking models by collecting large-scale, fine-grained user feedback (e.g., clicks, dwell time) at the document level. In contrast, generative AI search operates through a much longer search pipeline, spanning query decomposition, document retrieval, and answer generation, yet typically receives only coarse-grained feedback on the final answer. This introduces a feedback loop disconnect, where user feedback for the final output cannot be effectively mapped back to specific system components, making it difficult to improve each intermediate stage and sustain the feedback loop. In this paper, we envision NExT-Search, a next-generation paradigm designed to reintroduce fine-grained, process-level feedback into generative AI search. NExT-Search integrates two complementary modes: User Debug Mode, which allows engaged users to intervene at key stages; and Shadow User Mode, where a personalized user agent simulates user preferences and provides AI-assisted feedback for less interactive users. Furthermore, we envision how these feedback signals can be leveraged through online adaptation, which refines current search outputs in real-time, and offline update, which aggregates interaction logs to periodically fine-tune query decomposition, retrieval, and generation models. By restoring human control over key stages of the generative AI search pipeline, we believe NExT-Search offers a promising direction for building feedback-rich AI search systems that can evolve continuously alongside human feedback.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
arXiv:2505.14681v1 Announce Type: cross 
Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy</title>
<link>https://arxiv.org/abs/2312.10097</link>
<guid>https://arxiv.org/abs/2312.10097</guid>
<content:encoded><![CDATA[
arXiv:2312.10097v2 Announce Type: replace 
Abstract: This paper presents a novel numeral decomposer based on arithmetic criteria. The criteria are not dependent on a base-10 assumption but only on Hurford's Packing Strategy. Hurford's Packing Strategy constitutes numerals by packing factors and summands to multiplicators. We found out that a numeral of value n has a multiplicator larger than sqrt(n), a summand smaller than n/2 and a factor smaller than sqrt(n). Using these findings, the numeral decomposer attempts to detect and unpack factors and summand in order to reverse Hurford's Packing strategy. We tested its applicability for incremental unsupervised grammar induction in 273 languages. This way, grammars were obtained with sensible mathematical attributes that explain the structure of produced numerals. The numeral-decomposer-induced grammars are often close to expert-made and more compact than numeral grammars induced by a modern state-of-the-art grammar induction tool. Furthermore, this paper contains a report about the few cases of incorrect induced mathematical attributes, which are often linked to linguistic peculiarities like context sensitivity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning</title>
<link>https://arxiv.org/abs/2403.10056</link>
<guid>https://arxiv.org/abs/2403.10056</guid>
<content:encoded><![CDATA[
arXiv:2403.10056v2 Announce Type: replace 
Abstract: Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games</title>
<link>https://arxiv.org/abs/2404.17662</link>
<guid>https://arxiv.org/abs/2404.17662</guid>
<content:encoded><![CDATA[
arXiv:2404.17662v5 Announce Type: replace 
Abstract: We introduce WellPlay, a reasoning dataset for multi-agent conversational inference in Murder Mystery Games (MMGs). WellPlay comprises 1,482 inferential questions across 12 games, spanning objectives, reasoning, and relationship understanding, and establishes a systematic benchmark for evaluating agent reasoning abilities in complex social settings. Building on this foundation, we present PLAYER*, a novel framework for Large Language Model (LLM)-based agents in MMGs. MMGs pose unique challenges, including undefined state spaces, absent intermediate rewards, and the need for strategic reasoning through natural language. PLAYER* addresses these challenges with a sensor-based state representation and an information-driven strategy that optimises questioning and suspect pruning. Experiments show that PLAYER* outperforms existing methods in reasoning accuracy, efficiency, and agent-human interaction, advancing reasoning agents for complex social scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs</title>
<link>https://arxiv.org/abs/2407.01082</link>
<guid>https://arxiv.org/abs/2407.01082</guid>
<content:encoded><![CDATA[
arXiv:2407.01082v5 Announce Type: replace 
Abstract: Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersonaGym: Evaluating Persona Agents and LLMs</title>
<link>https://arxiv.org/abs/2407.18416</link>
<guid>https://arxiv.org/abs/2407.18416</guid>
<content:encoded><![CDATA[
arXiv:2407.18416v4 Announce Type: replace 
Abstract: Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework</title>
<link>https://arxiv.org/abs/2409.00054</link>
<guid>https://arxiv.org/abs/2409.00054</guid>
<content:encoded><![CDATA[
arXiv:2409.00054v2 Announce Type: replace 
Abstract: Identifying effective interventions from the scientific literature is challenging due to the high volume of publications, specialized terminology, and inconsistent reporting formats, making manual curation laborious and prone to oversight. To address this challenge, this paper proposes a novel framework leveraging large language models (LLMs), which integrates a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On the one hand, the POP algorithm conducts a prioritized breadth-first search (BFS) across a predefined ontology, generating structured prompt templates and action sequences to guide the automatic annotation process. On the other hand, the LLM-Duo system features two specialized LLM agents, an explorer and an evaluator, working collaboratively and adversarially to continuously refine annotation quality. We showcase the real-world applicability of our framework through a case study focused on speech-language intervention discovery. Experimental results show that our approach surpasses advanced baselines, achieving more accurate and comprehensive annotations through a fully automated process. Our approach successfully identified 2,421 interventions from a corpus of 64,177 research articles in the speech-language pathology domain, culminating in the creation of a publicly accessible intervention knowledge base with great potential to benefit the speech-language pathology community.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoMath: A Mathematical Reasoning Benchmark in Romanian</title>
<link>https://arxiv.org/abs/2409.11074</link>
<guid>https://arxiv.org/abs/2409.11074</guid>
<content:encoded><![CDATA[
arXiv:2409.11074v3 Announce Type: replace 
Abstract: Mathematics has long been conveyed through natural language, primarily for human understanding. With the rise of mechanized mathematics and proof assistants, there is a growing need to understand informal mathematical text, yet most existing benchmarks focus solely on English, overlooking other languages. This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite comprising three subsets: Baccalaureate, Competitions and Synthetic, which cover a range of mathematical domains and difficulty levels, aiming to improve non-English language models and promote multilingual AI development. By focusing on Romanian, a low-resource language with unique linguistic features, RoMath addresses the limitations of Anglo-centric models and emphasizes the need for dedicated resources beyond simple automatic translation. We benchmark several open-weight language models, highlighting the importance of creating resources for underrepresented languages. Code and datasets are be made available.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing</title>
<link>https://arxiv.org/abs/2409.11726</link>
<guid>https://arxiv.org/abs/2409.11726</guid>
<content:encoded><![CDATA[
arXiv:2409.11726v2 Announce Type: replace 
Abstract: Large language model (LLM) role-playing has gained widespread attention. Authentic character knowledge is crucial for constructing realistic LLM role-playing agents. However, existing works usually overlook the exploration of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown knowledge errors (UKE) while playing roles, which would lead to low-quality automatic construction of character trainable corpus. In this paper, we propose RoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The results indicate that even the latest LLMs struggle to detect these two types of errors effectively, especially when it comes to familiar knowledge. We experimented with various reasoning strategies and propose an agent-based reasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore further the potential for improving error detection capabilities. Experiments show that our method effectively improves the LLMs' ability to detect error character knowledge, but it remains an issue that requires ongoing attention.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review</title>
<link>https://arxiv.org/abs/2410.03663</link>
<guid>https://arxiv.org/abs/2410.03663</guid>
<content:encoded><![CDATA[
arXiv:2410.03663v4 Announce Type: replace 
Abstract: While reasoning capabilities typically emerge in large language models (LLMs) with tens of billions of parameters, recent research focuses on improving smaller open-source models through knowledge distillation (KD) from commercial LLMs. However, many of these studies rely solely on responses from a single LLM as the gold rationale, unlike the natural human learning process, which involves understanding both the correct answers and the reasons behind mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach: 1) instead of merely obtaining rationales from teachers, our method asks teachers to identify and explain the student's mistakes, providing customized instruction learning data; 2) we design a simulated peer-review process between teacher LLMs, and selects only the generated rationales above the acceptance threshold, which reduces the chance of teachers guessing correctly with flawed rationale, improving instructional data quality. Comprehensive experiments and analysis on mathematical, commonsense, and logical reasoning tasks demonstrate the effectiveness of our method. Our code is available at https://github.com/zhuochunli/Learn-from-Committee.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition</title>
<link>https://arxiv.org/abs/2410.10624</link>
<guid>https://arxiv.org/abs/2410.10624</guid>
<content:encoded><![CDATA[
arXiv:2410.10624v3 Announce Type: replace 
Abstract: We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from wearable sensor data. While LLMs excel at reasoning and generalization, they struggle with time-series inputs due to limited semantic context, numerical complexity, and sequence variability. To address these challenges, we construct SensorQA, a question-answering dataset of human-intuitive sensor-text pairs spanning diverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where the model aligns sensor inputs with trend descriptions. Special tokens are introduced to mark channel boundaries. This alignment enables LLMs to interpret numerical patterns, channel-specific signals, and variable-length inputs--without requiring human annotation. In the subsequent Task-Aware Tuning stage, we adapt the model for multivariate HAR classification, achieving performance that matches or exceeds state-of-the-art methods. Our results show that, guided by human-intuitive alignment, SensorLLM becomes an effective sensor learner, reasoner, and classifier--generalizing across varied HAR settings and paving the way for foundation model research in time-series analysis.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals</title>
<link>https://arxiv.org/abs/2410.11348</link>
<guid>https://arxiv.org/abs/2410.11348</guid>
<content:encoded><![CDATA[
arXiv:2410.11348v3 Announce Type: replace 
Abstract: Reward models are widely used as proxies for human preferences when aligning or evaluating LLMs. However, reward models are black boxes, and it is often unclear what, exactly, they are actually rewarding. In this paper we develop Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for measuring the sensitivity of a reward model to high-level attributes of responses, such as sentiment, helpfulness, or complexity. Importantly, RATE measures the causal effect of an attribute on the reward. RATE uses LLMs to rewrite responses to produce imperfect counterfactuals examples that can be used to measure causal effects. A key challenge is that these rewrites are imperfect in a manner that can induce substantial bias in the estimated sensitivity of the reward model to the attribute. The core idea of RATE is to adjust for this imperfect-rewrite effect by rewriting twice. We establish the validity of the RATE procedure and show empirically that it is an effective estimator.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting token compositionality in LLMs: A robustness analysis</title>
<link>https://arxiv.org/abs/2410.12924</link>
<guid>https://arxiv.org/abs/2410.12924</guid>
<content:encoded><![CDATA[
arXiv:2410.12924v3 Announce Type: replace 
Abstract: Understanding the internal mechanisms of large language models (LLMs) is integral to enhancing their reliability, interpretability, and inference processes. We present Constituent-Aware Pooling (CAP), a methodology designed to analyse how LLMs process compositional linguistic structures. Grounded in principles of compositionality, mechanistic interpretability, and information theory, CAP systematically intervenes in model activations through constituent-based pooling at various model levels. Our experiments on inverse definition modelling, hypernym and synonym prediction reveal critical insights into transformers' limitations in handling compositional abstractions. No specific layer integrates tokens into unified semantic representations based on their constituent parts. We observe fragmented information processing, which intensifies with model size, suggesting that larger models struggle more with these interventions and exhibit greater information dispersion. This fragmentation likely stems from transformers' training objectives and architectural design, preventing systematic and cohesive representations. Our findings highlight fundamental limitations in current transformer architectures regarding compositional semantics processing and model interpretability, underscoring the critical need for novel approaches in LLM design to address these challenges.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mystery of the Pathological Path-star Task for Language Models</title>
<link>https://arxiv.org/abs/2410.13779</link>
<guid>https://arxiv.org/abs/2410.13779</guid>
<content:encoded><![CDATA[
arXiv:2410.13779v2 Announce Type: replace 
Abstract: The recently introduced path-star task is a minimal task designed to exemplify limitations to the abilities of language models (Bachmann and Nagarajan, 2024). It involves a path-star graph where multiple arms radiate from a single starting node and each node is unique. Given the start node and a specified target node that ends an arm, the task is to generate the arm containing that target node. This is straightforward for a human but surprisingly difficult for language models, which did not outperform the random baseline. The authors hypothesized this is due to a deficiency in teacher-forcing and the next-token prediction paradigm.
  We demonstrate the task is learnable using teacher-forcing in alternative settings and that the issue is partially due to representation. We introduce a regularization method using structured samples of the same graph but with differing target nodes, improving results across a variety of model types. We provide RASP proofs showing the task is theoretically solvable. Finally, we find settings where an encoder-only model can consistently solve the task.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation</title>
<link>https://arxiv.org/abs/2410.14425</link>
<guid>https://arxiv.org/abs/2410.14425</guid>
<content:encoded><![CDATA[
arXiv:2410.14425v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct comprehensive experiments on three state-of-the-art large language models and several different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title>
<link>https://arxiv.org/abs/2410.15522</link>
<guid>https://arxiv.org/abs/2410.15522</guid>
<content:encoded><![CDATA[
arXiv:2410.15522v3 Announce Type: replace 
Abstract: Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics</title>
<link>https://arxiv.org/abs/2410.21272</link>
<guid>https://arxiv.org/abs/2410.21272</guid>
<content:encoded><![CDATA[
arXiv:2410.21272v2 Announce Type: replace 
Abstract: Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a "bag of heuristics".
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models</title>
<link>https://arxiv.org/abs/2411.02448</link>
<guid>https://arxiv.org/abs/2411.02448</guid>
<content:encoded><![CDATA[
arXiv:2411.02448v3 Announce Type: replace 
Abstract: LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucination. This paper introduces three fine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B, specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanation and verifiable citation, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanation and citation with minimal bias. Our REC dataset and models are available at https://github.com/adelaidehsu/REC.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training</title>
<link>https://arxiv.org/abs/2411.14318</link>
<guid>https://arxiv.org/abs/2411.14318</guid>
<content:encoded><![CDATA[
arXiv:2411.14318v2 Announce Type: replace 
Abstract: It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static proportions, as well as adjusting data proportions during training. However, few methods have addressed the complexities of domain-adaptive continual pre-training. To fill this gap, we propose Velocitune, a novel framework dynamically assesses learning velocity and adjusts data proportions accordingly, favoring slower-learning domains while shunning faster-learning ones, which is guided by a scaling law to indicate the desired learning goal for each domain with less associated cost. To evaluate the effectiveness of Velocitune, we conduct experiments in a reasoning-focused dataset with CodeLlama, as well as in a corpus specialised for system command generation with Llama3 and Mistral. Velocitune achieves performance gains in both math and code reasoning tasks and command-line generation benchmarks. Further analysis reveals that key factors driving Velocitune's effectiveness include target loss prediction and data ordering.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs be Good Graph Judge for Knowledge Graph Construction?</title>
<link>https://arxiv.org/abs/2411.17388</link>
<guid>https://arxiv.org/abs/2411.17388</guid>
<content:encoded><![CDATA[
arXiv:2411.17388v3 Announce Type: replace 
Abstract: In real-world scenarios, most of the data obtained from the information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. We identified three limitations with respect to existing KG construction methods: (1) There could be a large amount of noise in real-world documents, which could result in extracting messy information. (2) Naive LLMs usually extract inaccurate knowledge from some domain-specific documents. (3) Hallucination phenomenon cannot be overlooked when directly using LLMs to construct KGs. In this paper, we propose \textbf{GraphJudge}, a KG construction framework to address the aforementioned challenges. In this framework, we designed an entity-centric strategy to eliminate the noise information in the documents. And we fine-tuned a LLM as a graph judge to finally enhance the quality of generated KGs. Experiments conducted on two general and one domain-specific text-graph pair datasets demonstrate state-of-the-art performance against various baseline methods with strong generalization abilities. Our code is available at \href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension</title>
<link>https://arxiv.org/abs/2412.06245</link>
<guid>https://arxiv.org/abs/2412.06245</guid>
<content:encoded><![CDATA[
arXiv:2412.06245v2 Announce Type: replace 
Abstract: The performance of Large Language Models (LLMs) on natural language tasks can be improved through both supervised fine-tuning (SFT) and in-context learning (ICL), which operate via distinct mechanisms. Supervised fine-tuning updates the model's weights by minimizing loss on training data, whereas in-context learning leverages task demonstrations embedded in the prompt, without changing the model's parameters. This study investigates the effects of these learning paradigms on the hidden representations of LLMs using Intrinsic Dimension (ID). We use ID to estimate the number of degrees of freedom between representations extracted from LLMs as they perform specific natural language tasks. We first explore how the ID of LLM representations evolves during SFT and how it varies due to the number of demonstrations in ICL. We then compare the IDs induced by SFT and ICL and find that ICL consistently induces a higher ID compared to SFT, suggesting that representations generated during ICL reside in higher dimensional manifolds in the embedding space.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges</title>
<link>https://arxiv.org/abs/2412.11936</link>
<guid>https://arxiv.org/abs/2412.11936</guid>
<content:encoded><![CDATA[
arXiv:2412.11936v3 Announce Type: replace 
Abstract: Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs). We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title>
<link>https://arxiv.org/abs/2412.14161</link>
<guid>https://arxiv.org/abs/2412.14161</guid>
<content:encoded><![CDATA[
arXiv:2412.14161v2 Announce Type: replace 
Abstract: We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Agent-SafetyBench: Evaluating the Safety of LLM Agents</title>
<link>https://arxiv.org/abs/2412.14470</link>
<guid>https://arxiv.org/abs/2412.14470</guid>
<content:encoded><![CDATA[
arXiv:2412.14470v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through failure mode and helpfulness analysis, we summarize two fundamental safety defects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone may be insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. To drive progress in this area, Agent-SafetyBench has been released at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further research in agent safety evaluation and improvement.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs</title>
<link>https://arxiv.org/abs/2412.16783</link>
<guid>https://arxiv.org/abs/2412.16783</guid>
<content:encoded><![CDATA[
arXiv:2412.16783v2 Announce Type: replace 
Abstract: As increasingly capable large language models (LLMs) emerge, researchers have begun exploring their potential for subjective tasks. While recent work demonstrates that LLMs can be aligned with diverse human perspectives, evaluating this alignment on actual downstream tasks (e.g., hate speech detection) remains challenging due to the use of inconsistent datasets across studies. To address this issue, in this resource paper we propose a two-step framework: we (1) introduce SubData, an open-source Python library designed for standardizing heterogeneous datasets to evaluate LLM perspective alignment; and (2) present a theory-driven approach leveraging this library to test how differently-aligned LLMs (e.g., aligned with different political viewpoints) classify content targeting specific demographics. SubData's flexible mapping and taxonomy enable customization for diverse research needs, distinguishing it from existing resources. We invite contributions to add datasets to our initially proposed resource and thereby help expand SubData into a multi-construct benchmark suite for evaluating LLM perspective alignment on NLP tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts</title>
<link>https://arxiv.org/abs/2501.02009</link>
<guid>https://arxiv.org/abs/2501.02009</guid>
<content:encoded><![CDATA[
arXiv:2501.02009v2 Announce Type: replace 
Abstract: Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting</title>
<link>https://arxiv.org/abs/2501.06582</link>
<guid>https://arxiv.org/abs/2501.06582</guid>
<content:encoded><![CDATA[
arXiv:2501.06582v2 Announce Type: replace 
Abstract: Information retrieval, specifically contract clause retrieval, is foundational to contract drafting because lawyers rarely draft contracts from scratch; instead, they locate and revise the most relevant precedent. We introduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval benchmark for contract drafting fully annotated by experts. ACORD focuses on complex contract clauses such as Limitation of Liability, Indemnification, Change of Control, and Most Favored Nation. It includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task is to find the most relevant precedent clauses to a query. The bi-encoder retriever paired with pointwise LLMs re-rankers shows promising results. However, substantial improvements are still needed to effectively manage the complex legal work typically undertaken by lawyers. As the first retrieval benchmark for contract drafting annotated by experts, ACORD can serve as a valuable IR benchmark for the NLP community.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time</title>
<link>https://arxiv.org/abs/2501.07482</link>
<guid>https://arxiv.org/abs/2501.07482</guid>
<content:encoded><![CDATA[
arXiv:2501.07482v2 Announce Type: replace 
Abstract: As the knowledge landscape evolves and large language models (LLMs) become increasingly widespread, there is a growing need to keep these models updated with current events. While existing benchmarks assess general factual recall, few studies explore how LLMs retain knowledge over time or across different regions. To address these gaps, we present the Timely Events Benchmark (TiEBe), a dataset of over 23,000 question-answer pairs centered on notable global and regional events, spanning more than 10 years of events, 23 regions, and 13 languages. TiEBe leverages structured retrospective data from Wikipedia to identify notable events through time. These events are then used to construct a benchmark to evaluate LLMs' understanding of global and regional developments, grounded in factual evidence beyond Wikipedia itself. Our results reveal significant geographic disparities in factual recall, emphasizing the need for more balanced global representation in LLM training. We also observe a Pearson correlation of more than 0.7 between models' performance in TiEBe and various countries' socioeconomic indicators, such as HDI. In addition, we examine the impact of language on factual recall by posing questions in the native language of the region where each event occurred, uncovering substantial performance gaps for low-resource languages.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning</title>
<link>https://arxiv.org/abs/2501.14315</link>
<guid>https://arxiv.org/abs/2501.14315</guid>
<content:encoded><![CDATA[
arXiv:2501.14315v2 Announce Type: replace 
Abstract: Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection</title>
<link>https://arxiv.org/abs/2501.15451</link>
<guid>https://arxiv.org/abs/2501.15451</guid>
<content:encoded><![CDATA[
arXiv:2501.15451v3 Announce Type: replace 
Abstract: The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text</title>
<link>https://arxiv.org/abs/2501.15654</link>
<guid>https://arxiv.org/abs/2501.15654</guid>
<content:encoded><![CDATA[
arXiv:2501.15654v2 Announce Type: replace 
Abstract: In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such "expert" annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts' free-form explanations shows that while they rely heavily on specific lexical clues ('AI vocabulary'), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM Unlearning Robustness via Random Perturbations</title>
<link>https://arxiv.org/abs/2501.19202</link>
<guid>https://arxiv.org/abs/2501.19202</guid>
<content:encoded><![CDATA[
arXiv:2501.19202v3 Announce Type: replace 
Abstract: In this paper, we show that current state-of-the-art LLM unlearning methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in unlearned models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation (RNA) -- a plug-and-play, model and method agnostic approach with theoretical guarantees for improving the robustness of unlearned models. Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models, maintains unlearning performances while introducing no additional computational overhead.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information</title>
<link>https://arxiv.org/abs/2502.02095</link>
<guid>https://arxiv.org/abs/2502.02095</guid>
<content:encoded><![CDATA[
arXiv:2502.02095v2 Announce Type: replace 
Abstract: Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs</title>
<link>https://arxiv.org/abs/2502.02362</link>
<guid>https://arxiv.org/abs/2502.02362</guid>
<content:encoded><![CDATA[
arXiv:2502.02362v4 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A comparison of translation performance between DeepL and Supertext</title>
<link>https://arxiv.org/abs/2502.02577</link>
<guid>https://arxiv.org/abs/2502.02577</guid>
<content:encoded><![CDATA[
arXiv:2502.02577v3 Announce Type: replace 
Abstract: As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation</title>
<link>https://arxiv.org/abs/2502.02789</link>
<guid>https://arxiv.org/abs/2502.02789</guid>
<content:encoded><![CDATA[
arXiv:2502.02789v2 Announce Type: replace 
Abstract: Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is compute-bounded and the performance bottleneck shifts from the self-attention that many prior works focus on to the MLP part. In this work, we present SpecPrefill, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SpecPrefill leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with up to 7$\times$ maximal end-to-end QPS on real downstream tasks and 7.66$\times$ TTFT improvement.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.11051</link>
<guid>https://arxiv.org/abs/2502.11051</guid>
<content:encoded><![CDATA[
arXiv:2502.11051v3 Announce Type: replace 
Abstract: Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient ascent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment</title>
<link>https://arxiv.org/abs/2502.11066</link>
<guid>https://arxiv.org/abs/2502.11066</guid>
<content:encoded><![CDATA[
arXiv:2502.11066v2 Announce Type: replace 
Abstract: Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Achieving Concept Completeness for Textual Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2502.11100</link>
<guid>https://arxiv.org/abs/2502.11100</guid>
<content:encoded><![CDATA[
arXiv:2502.11100v2 Announce Type: replace 
Abstract: Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. This paper proposes Complete Textual Concept Bottleneck Model (CT-CBM),a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. CT-CBM iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. CT-CBM achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment</title>
<link>https://arxiv.org/abs/2502.11733</link>
<guid>https://arxiv.org/abs/2502.11733</guid>
<content:encoded><![CDATA[
arXiv:2502.11733v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) serve not only as chatbots but as key components in agent systems, where their common-sense knowledge significantly impacts performance as language-based planners for situated or embodied action. We assess LLMs' incremental learning (based on feedback from the environment), and controlled in-context learning abilities using a text-based environment. We introduce challenging yet interesting set of experiments to test i) how agents can incrementally solve tasks related to every day objects in typical rooms in a house where each of them are discovered by interacting within the environment, ii) controlled in-context learning abilities and efficiency of agents by providing short info about locations of objects and rooms to check how faster the task can be solved, and finally iii) using synthetic pseudo-English words to gauge how well LLMs are at inferring meaning of unknown words from environmental feedback. Results show that larger commercial models have a substantial gap in performance compared to open-weight but almost all models struggle with the synthetic words experiments.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models</title>
<link>https://arxiv.org/abs/2502.11811</link>
<guid>https://arxiv.org/abs/2502.11811</guid>
<content:encoded><![CDATA[
arXiv:2502.11811v3 Announce Type: replace 
Abstract: Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance accuracy. Existing methods use reranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying potential clues from multiple documents, then ranking them by relevance, and finally retaining the minimum number of clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting of a clue extractor, a reranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) The clue extractor first uses sentences containing the answer and similar ones as fine-tuning targets, aiming to extract sufficient potential clues; (2) The reranker is trained to prioritize effective clues based on the real feedback from the generation module, with clues capable of generating correct answers as positive samples and others as negative; (3) The truncator takes the minimum number of clues needed to answer the question (truncation point) as fine-tuning targets, and performs truncation on the reranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly improves QA performance over baselines on both LLaMA3 and Mistral. Further analysis confirms its effectiveness in complex reasoning, robustness to unreliable retrieval, and generalization to different scenarios.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.11916</link>
<guid>https://arxiv.org/abs/2502.11916</guid>
<content:encoded><![CDATA[
arXiv:2502.11916v2 Announce Type: replace 
Abstract: Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs</title>
<link>https://arxiv.org/abs/2502.12767</link>
<guid>https://arxiv.org/abs/2502.12767</guid>
<content:encoded><![CDATA[
arXiv:2502.12767v5 Announce Type: replace 
Abstract: Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks still suffer two practical drawbacks: they must be re-tuned whenever the KG or reasoning task changes, and they depend on a single, high-capacity LLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across five diverse benchmarks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability with reduced inference cost but increased abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning, reducing reliance on high-capacity LLMs while ensuring trustworthy inference. The code is available at https://github.com/ekrxjwh2009/R2-KG/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection</title>
<link>https://arxiv.org/abs/2502.13061</link>
<guid>https://arxiv.org/abs/2502.13061</guid>
<content:encoded><![CDATA[
arXiv:2502.13061v2 Announce Type: replace 
Abstract: Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While LMMs have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both SFT and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation</title>
<link>https://arxiv.org/abs/2502.13442</link>
<guid>https://arxiv.org/abs/2502.13442</guid>
<content:encoded><![CDATA[
arXiv:2502.13442v2 Announce Type: replace 
Abstract: Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems. The dataset generation code and sample data are available at https://github.com/j-bagel/treecut-math.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation</title>
<link>https://arxiv.org/abs/2502.14037</link>
<guid>https://arxiv.org/abs/2502.14037</guid>
<content:encoded><![CDATA[
arXiv:2502.14037v2 Announce Type: replace 
Abstract: Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose three new decoding methods that leverage a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. Experiments concerning math problem solving, extreme summarization, and the divergent association task demonstrate that our approach consistently performs at least as well as existing methods in terms of quality and diversity.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction</title>
<link>https://arxiv.org/abs/2502.14171</link>
<guid>https://arxiv.org/abs/2502.14171</guid>
<content:encoded><![CDATA[
arXiv:2502.14171v5 Announce Type: replace 
Abstract: Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare</title>
<link>https://arxiv.org/abs/2502.16051</link>
<guid>https://arxiv.org/abs/2502.16051</guid>
<content:encoded><![CDATA[
arXiv:2502.16051v2 Announce Type: replace 
Abstract: Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. Thus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. This dataset - created without any LM assistance - is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. Almost all 203 base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables (e.g., AGE), and are available for male, female, or non-binary-coded patients. For question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations. We outline a series of intended use cases and demonstrate the usability of our dataset by evaluating eleven off-the-shelf and four mental health fine-tuned LMs on category-specific task accuracy, on the impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human annotated samples.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SQLong: Enhanced NL2SQL for Longer Contexts with LLMs</title>
<link>https://arxiv.org/abs/2502.16747</link>
<guid>https://arxiv.org/abs/2502.16747</guid>
<content:encoded><![CDATA[
arXiv:2502.16747v2 Announce Type: replace 
Abstract: Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment</title>
<link>https://arxiv.org/abs/2502.16894</link>
<guid>https://arxiv.org/abs/2502.16894</guid>
<content:encoded><![CDATA[
arXiv:2502.16894v3 Announce Type: replace 
Abstract: While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs</title>
<link>https://arxiv.org/abs/2502.16901</link>
<guid>https://arxiv.org/abs/2502.16901</guid>
<content:encoded><![CDATA[
arXiv:2502.16901v2 Announce Type: replace 
Abstract: We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare and high-occurring tokens serving as specific, effective triggers. Our findings expose a critical vulnerability that influences the model's architecture, resulting in a concealed backdoor effect during the information flow. Our code and data are publicly available https://github.com/himanshubeniwal/X-BAT.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models</title>
<link>https://arxiv.org/abs/2502.19982</link>
<guid>https://arxiv.org/abs/2502.19982</guid>
<content:encoded><![CDATA[
arXiv:2502.19982v2 Announce Type: replace 
Abstract: In this paper, we investigate knowledge forgetting in large language models with a focus on its generalisation--ensuring that models forget not only specific training samples but also related implicit knowledge. To this end, we begin by identifying a broader unlearning scope that includes both target data and logically associated samples, including rephrased, subject-replaced, one-hop reasoned, and relation-reversed data. To rigorously evaluate generalisation, we introduce UGBench, the first comprehensive benchmark specifically designed to assess the unlearning of in-scope implicit knowledge covering 13 state-of-the-art methods across three datasets. UGBench reveals that unlearned models can still recall paraphrased answers and retain target facts in intermediate layers. This motivates us to take a preliminary step toward more generalised implicit knowledge forgetting by proposing PerMU, a novel probability perturbation-based unlearning paradigm. PerMU simulates adversarial unlearning samples to eliminate fact-related tokens from the logit distribution, collectively reducing the probabilities of all answer-associated tokens. Experiments are conducted on a diverse range of datasets, including TOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B in scale. The results demonstrate that PerMU delivers up to a 50.40% improvement in unlearning vanilla target data while maintaining a 40.73% boost in forgetting implicit knowledge. Our code can be found in https://github.com/MaybeLizzy/UGBench.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing</title>
<link>https://arxiv.org/abs/2502.20592</link>
<guid>https://arxiv.org/abs/2502.20592</guid>
<content:encoded><![CDATA[
arXiv:2502.20592v3 Announce Type: replace 
Abstract: Recent advances in test-time scaling have shown promising results in improving Large Language Model (LLM) performance through strategic computation allocation during inference. While this approach has demonstrated strong improvements in logical and mathematical reasoning tasks, its application to natural language generation (NLG), particularly summarization, remains unexplored. Multi-Document Summarization (MDS), a fundamental task in NLG, presents unique challenges by requiring models to extract and synthesize essential information across multiple lengthy documents. Unlike reasoning tasks, MDS demands a more nuanced approach to prompt design and ensemble methods, as no single "best" prompt can satisfy diverse summarization requirements. We propose a novel framework leveraging test-time scaling for MDS. Our approach employs prompt ensemble techniques to generate multiple candidate summaries using various prompts, then combines them with an aggregator to produce a refined summary. To evaluate our method effectively, we also introduce two new LLM-based metrics: the Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (LLM-ACU) score, which assess summary quality while addressing the positional bias inherent in traditional automatic evaluation. Our extensive experiments demonstrate that this framework significantly enhances summary quality while also revealing the practical scaling boundaries to MDS tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation</title>
<link>https://arxiv.org/abs/2502.21074</link>
<guid>https://arxiv.org/abs/2502.21074</guid>
<content:encoded><![CDATA[
arXiv:2502.21074v2 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by encouraging step-by-step reasoning in natural language. However, leveraging a latent continuous space for reasoning may offer benefits in terms of both efficiency and robustness. Prior implicit CoT methods attempt to bypass language completely by reasoning in continuous space but have consistently underperformed compared to the standard explicit CoT approach. We introduce CODI (Continuous Chain-of-Thought via Self-Distillation), a novel training framework that effectively compresses natural language CoT into continuous space. CODI jointly trains a teacher task (Explicit CoT) and a student task (Implicit CoT), distilling the reasoning ability from language into continuous space by aligning the hidden states of a designated token. Our experiments show that CODI is the first implicit CoT approach to match the performance of explicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and outperforming the previous state-of-the-art by 28.2% in accuracy. CODI also demonstrates robustness, generalizable to complex datasets, and interpretability. These results validate that LLMs can reason effectively not only in natural language, but also in a latent continuous space.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey</title>
<link>https://arxiv.org/abs/2503.01513</link>
<guid>https://arxiv.org/abs/2503.01513</guid>
<content:encoded><![CDATA[
arXiv:2503.01513v2 Announce Type: replace 
Abstract: We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of LLMs. While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable artificial facilitation agents to not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from NLP and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, (c) along with a new taxonomy of conversation facilitation datasets, (d) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MCiteBench: A Multimodal Benchmark for Generating Text with Citations</title>
<link>https://arxiv.org/abs/2503.02589</link>
<guid>https://arxiv.org/abs/2503.02589</guid>
<content:encoded><![CDATA[
arXiv:2503.02589v3 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have advanced in integrating diverse modalities but frequently suffer from hallucination. A promising solution to mitigate this issue is to generate text with citations, providing a transparent chain for verification. However, existing work primarily focuses on generating citations for text-only content, leaving the challenges of multimodal scenarios largely unexplored. In this paper, we introduce MCiteBench, the first benchmark designed to assess the ability of MLLMs to generate text with citations in multimodal contexts. Our benchmark comprises data derived from academic papers and review-rebuttal interactions, featuring diverse information sources and multimodal content. Experimental results reveal that MLLMs struggle to ground their outputs reliably when handling multimodal input. Further analysis uncovers a systematic modality bias and reveals how models internally rely on different sources when generating citations, offering insights into model behavior and guiding future directions for multimodal citation tasks.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms</title>
<link>https://arxiv.org/abs/2503.04372</link>
<guid>https://arxiv.org/abs/2503.04372</guid>
<content:encoded><![CDATA[
arXiv:2503.04372v2 Announce Type: replace 
Abstract: Machine Translation (MT) systems frequently encounter gender-ambiguous occupational terms, where they must assign gender without explicit contextual cues. While individual translations in such cases may not be inherently biased, systematic patterns-such as consistently translating certain professions with specific genders-can emerge, reflecting and perpetuating societal stereotypes. This ambiguity challenges traditional instance-level single-answer evaluation approaches, as no single gold standard translation exists. To address this, we introduce GRAPE, a probability-based metric designed to evaluate gender bias by analyzing aggregated model responses. Alongside this, we present GAMBIT-MT, a benchmarking dataset in English with gender-ambiguous occupational terms. Using GRAPE, we evaluate several MT systems and examine whether their gendered translations in Greek and French align with or diverge from societal stereotypes, real-world occupational gender distributions, and normative standards.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cost-Optimal Grouped-Query Attention for Long-Context Modeling</title>
<link>https://arxiv.org/abs/2503.09579</link>
<guid>https://arxiv.org/abs/2503.09579</guid>
<content:encoded><![CDATA[
arXiv:2503.09579v2 Announce Type: replace 
Abstract: Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the computational cost of attention layers in large language models (LLMs). However, current GQA configurations are often suboptimal because they overlook how context length influences inference cost. Since inference cost grows with context length, the most cost-efficient GQA configuration should also vary accordingly. In this work, we analyze the relationship among context length, model size, GQA configuration, and model loss, and introduce two innovations: (1) we decouple the total head size from the hidden size, enabling more flexible control over attention FLOPs; and (2) we jointly optimize the model size and the GQA configuration to arrive at a better allocation of inference resources between attention layers and other components. Our analysis reveals that commonly used GQA configurations are highly suboptimal for long-context scenarios. More importantly, we propose a recipe for deriving cost-optimal GQA configurations. Our results show that for long-context scenarios, one should use fewer attention heads while scaling up model size. Configurations selected by our recipe can reduce both memory usage and FLOPs by more than 50% compared to Llama-3's GQA, with *no degradation in model capabilities*. Our findings offer valuable insights for designing efficient long-context LLMs. The code is available at https://www.github.com/THUNLP/cost-optimal-gqa .
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs</title>
<link>https://arxiv.org/abs/2503.10657</link>
<guid>https://arxiv.org/abs/2503.10657</guid>
<content:encoded><![CDATA[
arXiv:2503.10657v2 Announce Type: replace 
Abstract: Routing large language models (LLMs) is a new paradigm that uses a router to recommend the best LLM from a pool of candidates for a given input. In this paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can significantly enhance the performance of this paradigm as the number of candidates increases. This improvement can even surpass the performance of the best single model in the pool and many existing strong LLMs, confirming it a highly promising paradigm. However, the lack of comprehensive and open-source benchmarks for Routing LLMs has hindered the development of routers. In this paper, we introduce RouterEval, a benchmark tailored for router research, which includes over 200,000,000 performance records for 12 popular LLM evaluations across various areas such as commonsense reasoning, semantic understanding, etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations of existing Routing LLM methods reveal that most still have significant room for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data, code and tutorial.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection</title>
<link>https://arxiv.org/abs/2503.18132</link>
<guid>https://arxiv.org/abs/2503.18132</guid>
<content:encoded><![CDATA[
arXiv:2503.18132v2 Announce Type: replace 
Abstract: Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation</title>
<link>https://arxiv.org/abs/2504.02438</link>
<guid>https://arxiv.org/abs/2504.02438</guid>
<content:encoded><![CDATA[
arXiv:2504.02438v4 Announce Type: replace 
Abstract: Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLAMP, a hierarchical video-language model that processes hour-long videos at "mixed precision" through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLAMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLAMP's superior performance across five video understanding benchmarks, particularly on long-form content. Notably, ViLAMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance. Code and model are available at https://github.com/steven-ccq/ViLAMP.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models</title>
<link>https://arxiv.org/abs/2504.08399</link>
<guid>https://arxiv.org/abs/2504.08399</guid>
<content:encoded><![CDATA[
arXiv:2504.08399v2 Announce Type: replace 
Abstract: Self-report questionnaires have long been used to assess LLM personality traits, yet they fail to capture behavioral nuances due to biases and meta-knowledge contamination. This paper proposes a novel multi-observer framework for personality trait assessments in LLM agents that draws on informant-report methods in psychology. Instead of relying on self-assessments, we employ multiple observer agents. Each observer is configured with a specific relational context (e.g., family member, friend, or coworker) and engages the subject LLM in dialogue before evaluating its behavior across the Big Five dimensions. We show that these observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in LLM self-assessments. We also found that aggregating responses from 5 to 7 observers reduces systematic biases and achieves optimal reliability. Our results highlight the role of relationship context in perceiving personality and demonstrate that a multi-observer paradigm offers a more reliable, context-sensitive approach to evaluating LLM personality traits.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models</title>
<link>https://arxiv.org/abs/2504.10368</link>
<guid>https://arxiv.org/abs/2504.10368</guid>
<content:encoded><![CDATA[
arXiv:2504.10368v2 Announce Type: replace 
Abstract: We introduce S1-Bench, a novel benchmark designed to evaluate the performance of Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their heavy reliance on system 2 thinking may limit their system 1 thinking capabilities. However, there is a lack of an appropriate benchmark for evaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench introduces a suite of simple, diverse, and natural questions across multiple domains and languages, specifically designed to assess LRMs' performance on questions more suitable for system 1 . We conduct extensive evaluations across 28 LRMs, revealing their inefficiency, inadequate accuracy, and limited robustness when handling simple questions. Additionally, we observe a gap between their difficulty perception and generation length. Overall, this work paves the way toward dual-system compatibility in the development of LRMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction</title>
<link>https://arxiv.org/abs/2504.12324</link>
<guid>https://arxiv.org/abs/2504.12324</guid>
<content:encoded><![CDATA[
arXiv:2504.12324v2 Announce Type: replace 
Abstract: Natural Language Inference (NLI) is a fundamental task in natural language processing. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm: CDCL-NLI, which extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 25,410 instances and spanning 26 languages. To address the limitations of previous methods on CDCL-NLI task, we further propose an innovative method that integrates RST-enhanced graph fusion with interpretability-aware prediction. Our approach leverages RST (Rhetorical Structure Theory) within heterogeneous graph neural networks for cross-document context modeling, and employs a structure-aware semantic alignment based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU (Elementary Discourse Unit)-level attribution framework that produces extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both conventional NLI models as well as large language models. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, hallucination elimination and interpretability inference. Our code and datasets are available at \href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer review.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations</title>
<link>https://arxiv.org/abs/2504.14150</link>
<guid>https://arxiv.org/abs/2504.14150</guid>
<content:encoded><![CDATA[
arXiv:2504.14150v2 Announce Type: replace 
Abstract: Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety</title>
<link>https://arxiv.org/abs/2504.15241</link>
<guid>https://arxiv.org/abs/2504.15241</guid>
<content:encoded><![CDATA[
arXiv:2504.15241v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual settings, where multilingual safety-aligned data is often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we introduce a multilingual guardrail with reasoning for prompt classification. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail, MrGuard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%. We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions. The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation</title>
<link>https://arxiv.org/abs/2504.20371</link>
<guid>https://arxiv.org/abs/2504.20371</guid>
<content:encoded><![CDATA[
arXiv:2504.20371v2 Announce Type: replace 
Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory, the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT, remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompt strategies, and (3) we design precise disambiguation metrics, and study the efficacy of various prompt strategies on multiple state-of-the-art LLMs. We conduct comprehensive experiments across 4 language pairs and 13 domains, our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation</title>
<link>https://arxiv.org/abs/2505.00038</link>
<guid>https://arxiv.org/abs/2505.00038</guid>
<content:encoded><![CDATA[
arXiv:2505.00038v2 Announce Type: replace 
Abstract: Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users. We aim to generate customized responses tailored to specific individuals instead of generic outputs that emulate the collective voices of diverse populations. We propose HyPerAlign, an interpretable and sample-efficient hypothesis-driven personalization approach for LLM models. Given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality, and writing style, then prompt LLM models with these hypotheses and user-specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, namely authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks). Results demonstrate the superiority of hypothesis-driven LLM personalization compared to preference-based fine-tuning methods. For authorship attribution, HyPerAlign generations have consistently high win-rates (commonly $> 90\%$) against state-of-the-art preference fine-tuning approaches across diverse user profiles and LLM models. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\%$ on average. Overall, HyPerAlign represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Model based Human-Agent Systems</title>
<link>https://arxiv.org/abs/2505.00753</link>
<guid>https://arxiv.org/abs/2505.00753</guid>
<content:encoded><![CDATA[
arXiv:2505.00753v2 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Thinking via Mode Policy Optimization for Social Language Agents</title>
<link>https://arxiv.org/abs/2505.02156</link>
<guid>https://arxiv.org/abs/2505.02156</guid>
<content:encoded><![CDATA[
arXiv:2505.02156v3 Announce Type: replace 
Abstract: Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\textbf{A}$daptive $\textbf{M}$ode $\textbf{L}$earning ($\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\textbf{A}$daptive $\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO's fixed-depth solution.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks</title>
<link>https://arxiv.org/abs/2407.02855</link>
<guid>https://arxiv.org/abs/2407.02855</guid>
<content:encoded><![CDATA[
arXiv:2407.02855v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreak attacks. An important observation is that, while different types of jailbreak attacks can generate significantly different queries, they mostly result in similar responses that are rooted in the same harmful knowledge (e.g., detailed steps to make a bomb). Consequently, unlearning-based approaches have been proposed to mitigate jailbreak attacks by directly removing harmful knowledge from the model. In this paper, we identify a novel ripple effect of unlearning, wherein LLMs can implicitly unlearn harmful knowledge that was not explicitly introduced during the unlearning phase (e.g., a model unlearning the steps for theft may also implicitly unlearn the steps for making a bomb). Through over 100 experimental runs spanning multiple models, attack strategies, and defense methods, we empirically validate this phenomenon, which makes unlearning-based methods able to decrease the Attack Success Rate on unseen data from more than 70% to less than 10% with only 100 training samples. Further analysis reveals that the strong generalization ability of unlearning may stem from the intrinsic relatedness among harmful responses across harmful questions (e.g., response patterns, shared steps and actions in response, and similarity among their learned representations in the LLM). We also discuss the potential limitations of unlearning and the observed ripple effect. We hope our research could contribute to a deeper understanding of unlearning. Our code is available at https://github.com/thu-coai/SafeUnlearning.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech</title>
<link>https://arxiv.org/abs/2410.01162</link>
<guid>https://arxiv.org/abs/2410.01162</guid>
<content:encoded><![CDATA[
arXiv:2410.01162v2 Announce Type: replace-cross 
Abstract: This work studies the capabilities of a large language model (LLM) to understand paralinguistic aspects of speech without fine-tuning its weights. We utilize an end-to-end system with a speech encoder, which is trained to produce token embeddings such that the LLM's response to an expressive speech prompt is aligned with its response to a semantically matching text prompt that has also been conditioned on the user's speaking style. This framework enables the encoder to generate tokens that capture both linguistic and paralinguistic information and effectively convey them to the LLM, even when the LLM's weights remain completely frozen. To the best of our knowledge, our work is the first to explore how to induce a frozen LLM to understand more than just linguistic content from speech inputs in a general interaction setting. Experiments demonstrate that our system is able to produce higher quality and more empathetic responses to expressive speech prompts compared to several baselines.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models</title>
<link>https://arxiv.org/abs/2410.02429</link>
<guid>https://arxiv.org/abs/2410.02429</guid>
<content:encoded><![CDATA[
arXiv:2410.02429v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) excel in textual and visual tasks but often produce outputs that defy physical laws when dealing with physical-world reasoning tasks. Inspired by human cognition, where perception is fundamental to reasoning, we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT-sensory task reasoning in the physical world. In this work, we systematically study LLMs' capability to address real-world IoT-sensory tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning and activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions. We design a new benchmark comprising five real-world tasks with varying data types and reasoning complexities to evaluate the performance of IoT-LLM. Experimental results on six LLMs reveal that IoT-LLM significantly improves the performance of IoT-sensory task reasoning of LLMs, with models like GPT-4o-mini showing a 49.4% average improvement over previous methods.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment</title>
<link>https://arxiv.org/abs/2410.09083</link>
<guid>https://arxiv.org/abs/2410.09083</guid>
<content:encoded><![CDATA[
arXiv:2410.09083v2 Announce Type: replace-cross 
Abstract: This paper presents a method to analyze the inference patterns used by Large Language Models (LLMs) for judgment in a case study on legal LLMs, so as to identify potential incorrect representations of the LLM, according to human domain knowledge. Unlike traditional evaluations on language generation results, we propose to evaluate the correctness of the detailed inference patterns of an LLM behind its seemingly correct outputs. To this end, we quantify the interactions between input phrases used by the LLM as primitive inference patterns, because recent theoretical achievements have proven several mathematical guarantees of the faithfulness of the interaction-based explanation. We design a set of metrics to evaluate the detailed inference patterns of LLMs. Experiments show that even when the language generation results appear correct, a significant portion of the inference patterns used by the LLM for the legal judgment may represent misleading or irrelevant logic.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Continual Instruction Assistant</title>
<link>https://arxiv.org/abs/2410.10868</link>
<guid>https://arxiv.org/abs/2410.10868</guid>
<content:encoded><![CDATA[
arXiv:2410.10868v4 Announce Type: replace-cross 
Abstract: Continual Instruction Tuning (CIT) is adopted to continually instruct Large Models to follow human intent data by data. It is observed that existing gradient update would heavily destroy the performance on previous datasets during CIT process. Instead, Exponential Moving Average (EMA), owns the ability to trace previous parameters, which can aid in decreasing forgetting. Nonetheless, its stable balance weight fails to deal with the ever-changing datasets, leading to the out-of-balance between plasticity and stability. In this paper, we propose a general continual instruction tuning framework to address the challenge. Starting from the trade-off prerequisite and EMA update, we propose the plasticity and stability ideal condition. Based on Taylor expansion in the loss function, we find the optimal balance weight can be automatically determined by the gradients and learned parameters. Therefore, we propose a stable-plasticity balanced coefficient to avoid knowledge interference. Based on the semantic similarity of the instructions, we can determine whether to retrain or expand the training parameters and allocate the most suitable parameters for the testing instances. Extensive experiments across multiple continual instruction tuning benchmarks demonstrate that our approach not only enhances anti-forgetting capabilities but also significantly improves overall continual tuning performance. Our code is available at https://github.com/JingyangQiao/CoIN.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study</title>
<link>https://arxiv.org/abs/2410.17980</link>
<guid>https://arxiv.org/abs/2410.17980</guid>
<content:encoded><![CDATA[
arXiv:2410.17980v2 Announce Type: replace-cross 
Abstract: The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We investigate an alternative attention mechanism based on the stick-breaking process in larger scale settings. The method works as follows: For each token before the current, we determine a break point, which represents the proportion of the stick, the weight of the attention, to allocate to the current token. We repeat this on the remaining stick, until all tokens are allocated a weight, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing. We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with $2^{11}$ context window to perform well at $2^{14}$ with perplexity improvements.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models</title>
<link>https://arxiv.org/abs/2412.04756</link>
<guid>https://arxiv.org/abs/2412.04756</guid>
<content:encoded><![CDATA[
arXiv:2412.04756v2 Announce Type: replace-cross 
Abstract: The increasing frequency and sophistication of cybersecurity vulnerabilities in software systems underscores the need for more robust and effective vulnerability assessment methods. However, existing approaches often rely on highly technical and abstract frameworks, which hinder understanding and increase the likelihood of exploitation, resulting in severe cyberattacks. In this paper, we introduce ChatNVD, a support tool powered by Large Language Models (LLMs) that leverages the National Vulnerability Database (NVD) to generate accessible, context-rich summaries of software vulnerabilities. We develop three variants of ChatNVD, utilizing three prominent LLMs: GPT-4o Mini by OpenAI, LLaMA 3 by Meta, and Gemini 1.5 Pro by Google. To evaluate their performance, we conduct a comparative evaluation focused on their ability to identify, interpret, and explain software vulnerabilities. Our results demonstrate that GPT-4o Mini outperforms the other models, achieving over 92% accuracy and the lowest error rates, making it the most reliable option for real-world vulnerability assessment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProcessBench: Identifying Process Errors in Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2412.06559</link>
<guid>https://arxiv.org/abs/2412.06559</guid>
<content:encoded><![CDATA[
arXiv:2412.06559v3 Announce Type: replace-cross 
Abstract: As language models regularly make mistakes when solving math problems, automated identification of errors in the reasoning process becomes increasingly significant for their scalable oversight. In this paper, we introduce ProcessBench for measuring the ability to identify erroneous steps in mathematical reasoning. It consists of 3,400 test cases, primarily focused on competition- and Olympiad-level math problems. Each test case contains a step-by-step solution with error location annotated by human experts. Models are required to identify the earliest step that contains an error, or conclude that all steps are correct. We conduct extensive evaluation on ProcessBench, involving two types of models: process reward models (PRMs) and critic models, where for the latter we prompt general language models to critique each solution step by step. We draw two main observations: (1) Existing PRMs typically fail to generalize to more challenging math problems beyond GSM8K and MATH. They underperform both critic models (i.e., prompted general language models) and our own trained PRM that is straightforwardly fine-tuned on the PRM800K dataset. (2) The best open-source model, QwQ-32B-Preview, has demonstrated the critique capability competitive with the proprietary model GPT-4o, despite that it still lags behind the reasoning-specialized o1-mini. We hope ProcessBench can foster future research in reasoning process assessment, paving the way toward scalable oversight of language models.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents</title>
<link>https://arxiv.org/abs/2501.08828</link>
<guid>https://arxiv.org/abs/2501.08828</guid>
<content:encoded><![CDATA[
arXiv:2501.08828v2 Announce Type: replace-cross 
Abstract: Multimodal document retrieval aims to identify and retrieve various forms of multimodal content, such as figures, tables, charts, and layout information from extensive documents. Despite its increasing popularity, there is a notable lack of a comprehensive and robust benchmark to effectively evaluate the performance of systems in such tasks. To address this gap, this work introduces a new benchmark, named MMDocIR, that encompasses two distinct tasks: page-level and layout-level retrieval. The former evaluates the performance of identifying the most relevant pages within a long document, while the later assesses the ability of detecting specific layouts, providing a more fine-grained measure than whole-page analysis. A layout refers to a variety of elements, including textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring 1,685 questions annotated by experts and 173,843 questions with bootstrapped labels, making it a valuable resource in multimodal document retrieval for both training and evaluation. Through rigorous experiments, we demonstrate that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR training set effectively enhances the performance of multimodal document retrieval and (iii) text retrievers leveraging VLM-text significantly outperforms retrievers relying on OCR-text. Our dataset is available at https://mmdocrag.github.io/MMDocIR/.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title>
<link>https://arxiv.org/abs/2501.12368</link>
<guid>https://arxiv.org/abs/2501.12368</guid>
<content:encoded><![CDATA[
arXiv:2501.12368v2 Announce Type: replace-cross 
Abstract: Despite the promising performance of Large Vision Language Models (LVLMs) in visual understanding, they occasionally generate incorrect outputs. While reward models (RMs) with reinforcement learning or test-time scaling offer the potential for improving generation quality, a critical gap remains: publicly available multi-modal RMs for LVLMs are scarce, and the implementation details of proprietary models are often unclear. We bridge this gap with InternLM-XComposer2.5-Reward (IXC-2.5-Reward), a simple yet effective multi-modal reward model that aligns LVLMs with human preferences. To ensure the robustness and versatility of IXC-2.5-Reward, we set up a high-quality multi-modal preference corpus spanning text, image, and video inputs across diverse domains, such as instruction following, general understanding, text-rich documents, mathematical reasoning, and video understanding. IXC-2.5-Reward achieves excellent results on the latest multi-modal reward model benchmark and shows competitive performance on text-only reward model benchmarks. We further demonstrate three key applications of IXC-2.5-Reward: (1) Providing a supervisory signal for RL training. We integrate IXC-2.5-Reward with Proximal Policy Optimization (PPO) yields IXC-2.5-Chat, which shows consistent improvements in instruction following and multi-modal open-ended dialogue; (2) Selecting the best response from candidate responses for test-time scaling; and (3) Filtering outlier or noisy samples from existing image and video instruction tuning training data. To ensure reproducibility and facilitate further research, we have open-sourced all model weights and training recipes at https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-Reward
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairshare Data Pricing via Data Valuation for Large Language Models</title>
<link>https://arxiv.org/abs/2502.00198</link>
<guid>https://arxiv.org/abs/2502.00198</guid>
<content:encoded><![CDATA[
arXiv:2502.00198v2 Announce Type: replace-cross 
Abstract: Training data is the backbone of large language models (LLMs), yet today's data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). We begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. Then we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data's contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. Theoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. Empirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning, fairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers' performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM. Our code will be open sourced.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios</title>
<link>https://arxiv.org/abs/2502.02145</link>
<guid>https://arxiv.org/abs/2502.02145</guid>
<content:encoded><![CDATA[
arXiv:2502.02145v2 Announce Type: replace-cross 
Abstract: Ensuring the safety of autonomous vehicles requires virtual scenario-based testing, which depends on the robust evaluation and generation of safety-critical scenarios. So far, researchers have used scenario-based testing frameworks that rely heavily on handcrafted scenarios as safety metrics. To reduce the effort of human interpretation and overcome the limited scalability of these approaches, we combine Large Language Models (LLMs) with structured scenario parsing and prompt engineering to automatically evaluate and generate safety-critical driving scenarios. We introduce Cartesian and Ego-centric prompt strategies for scenario evaluation, and an adversarial generation module that modifies trajectories of risk-inducing vehicles (ego-attackers) to create critical scenarios. We validate our approach using a 2D simulation framework and multiple pre-trained LLMs. The results show that the evaluation module effectively detects collision scenarios and infers scenario safety. Meanwhile, the new generation module identifies high-risk agents and synthesizes realistic, safety-critical scenarios. We conclude that an LLM equipped with domain-informed prompting techniques can effectively evaluate and generate safety-critical driving scenarios, reducing dependence on handcrafted metrics. We release our open-source code and scenarios at: https://github.com/TUM-AVS/From-Words-to-Collisions.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Language Models to Reason Efficiently</title>
<link>https://arxiv.org/abs/2502.04463</link>
<guid>https://arxiv.org/abs/2502.04463</guid>
<content:encoded><![CDATA[
arXiv:2502.04463v3 Announce Type: replace-cross 
Abstract: Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models.
  In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks</title>
<link>https://arxiv.org/abs/2502.11163</link>
<guid>https://arxiv.org/abs/2502.11163</guid>
<content:encoded><![CDATA[
arXiv:2502.11163v2 Announce Type: replace-cross 
Abstract: Visual-Language Models (VLMs) have shown remarkable performance across various tasks, particularly in recognizing geographic information from images. However, VLMs still show regional biases in this task. To systematically evaluate these issues, we introduce a benchmark consisting of 1,200 images paired with detailed geographic metadata. Evaluating four VLMs, we find that while these models demonstrate the ability to recognize geographic information from images, achieving up to 53.8% accuracy in city prediction, they exhibit significant biases. Specifically, performance is substantially higher for economically developed and densely populated regions compared to less developed (-12.5%) and sparsely populated (-17.0%) areas. Moreover, regional biases of frequently over-predicting certain locations remain. For instance, they consistently predict Sydney for images taken in Australia, shown by the low entropy scores for these countries. The strong performance of VLMs also raises privacy concerns, particularly for users who share images online without the intent of being identified. Our code and dataset are publicly available at https://github.com/uscnlp-lime/FairLocator.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking</title>
<link>https://arxiv.org/abs/2502.12466</link>
<guid>https://arxiv.org/abs/2502.12466</guid>
<content:encoded><![CDATA[
arXiv:2502.12466v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become integral to code-related tasks, a central question emerges: do LLMs truly understand program execution semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's understanding of code execution semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. The transformations span syntactic edits, structural modifications, and algorithmic changes, covering a broad spectrum of semantic variation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning over execution semantics, highlighting fundamental limitations.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning</title>
<link>https://arxiv.org/abs/2502.12623</link>
<guid>https://arxiv.org/abs/2502.12623</guid>
<content:encoded><![CDATA[
arXiv:2502.12623v2 Announce Type: replace-cross 
Abstract: Recent advancements in music large language models (LLMs) have significantly improved music understanding tasks, which involve the model's ability to analyze and interpret various musical elements. These improvements primarily focused on integrating both music and text inputs. However, the potential of incorporating additional modalities such as images, videos and textual music features to enhance music understanding remains unexplored. To bridge this gap, we propose DeepResonance, a multimodal music understanding LLM fine-tuned via multi-way instruction tuning with multi-way aligned music, text, image, and video data. To this end, we construct Music4way-MI2T, Music4way-MV2T, and Music4way-Any2T, three 4-way training and evaluation datasets designed to enable DeepResonance to integrate both visual and textual music feature content. We also introduce multi-sampled ImageBind embeddings and a pre-LLM fusion Transformer to enhance modality fusion prior to input into text LLMs, tailoring DeepResonance for multi-way instruction tuning. Our model achieves state-of-the-art performances across six music understanding tasks, highlighting the benefits of the auxiliary modalities and the structural superiority of DeepResonance. We plan to open-source the models and the newly constructed datasets.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study</title>
<link>https://arxiv.org/abs/2503.06794</link>
<guid>https://arxiv.org/abs/2503.06794</guid>
<content:encoded><![CDATA[
arXiv:2503.06794v3 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) are powerful yet computationally intensive for widespread practical deployments. To address such challenge without costly re-training, post-training acceleration techniques like quantization and token reduction are extensively explored. However, current acceleration evaluations primarily target minimal overall performance degradation, overlooking a crucial question: does the accelerated model still give the same answers to the same questions as it did before acceleration? This is vital for stability-centered industrial applications where consistently correct answers for specific, known situations are paramount, such as in AI-based disease diagnosis. We systematically investigate this for accelerated VLMs, testing four leading models (LLaVA-1.5, LLaVA-Next, Qwen2-VL, Qwen2.5-VL) with eight acceleration methods on ten multi-modal benchmarks. Our findings are stark: despite minimal aggregate performance drops, accelerated models changed original answers up to 20% of the time. Critically, up to 6.5% of these changes converted correct answers to incorrect. Input perturbations magnified these inconsistencies, and the trend is confirmed by case studies with the medical VLM LLaVA-Med. This research reveals a significant oversight in VLM acceleration, stressing an urgent need for instance-level stability checks to ensure trustworthy real-world deployment.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models</title>
<link>https://arxiv.org/abs/2503.07575</link>
<guid>https://arxiv.org/abs/2503.07575</guid>
<content:encoded><![CDATA[
arXiv:2503.07575v2 Announce Type: replace-cross 
Abstract: This research investigates both explicit and implicit social biases exhibited by Vision-Language Models (VLMs). The key distinction between these bias types lies in the level of awareness: explicit bias refers to conscious, intentional biases, while implicit bias operates subconsciously. To analyze explicit bias, we directly pose questions to VLMs related to gender and racial differences: (1) Multiple-choice questions based on a given image (e.g., "What is the education level of the person in the image?") (2) Yes-No comparisons using two images (e.g., "Is the person in the first image more educated than the person in the second image?") For implicit bias, we design tasks where VLMs assist users but reveal biases through their responses: (1) Image description tasks: Models are asked to describe individuals in images, and we analyze disparities in textual cues across demographic groups. (2) Form completion tasks: Models draft a personal information collection form with 20 attributes, and we examine correlations among selected attributes for potential biases. We evaluate Gemini-1.5, GPT-4V, GPT-4o, LLaMA-3.2-Vision and LLaVA-v1.6. Our code and data are publicly available at https://github.com/uscnlp-lime/VisBias.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More</title>
<link>https://arxiv.org/abs/2503.10542</link>
<guid>https://arxiv.org/abs/2503.10542</guid>
<content:encoded><![CDATA[
arXiv:2503.10542v2 Announce Type: replace-cross 
Abstract: This work concerns the path-star task, a minimal example of searching over a graph. The graph, $G$, is star-shaped with $D$ arms radiating from a start node, $s$. A language model (LM) is given $G$, $s$, and a target node $t$, which ends one of the arms and is tasked with generating the arm containing $t$. The minimal nature of this task means only a single choice needs to be made: which of the $D$ arms contains $t$?
  Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to a learned shortcut that absorbs training supervision. We show how this pathology is caused by excess supervision and we present a series of solutions demonstrating that the task is solvable via decoder-only LMs. We find that the task's minimal nature causes its difficulty, as it prevents task decomposition. Our solutions provide insight into the pathology and its implications for LMs trained via next-token prediction.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2503.14232</link>
<guid>https://arxiv.org/abs/2503.14232</guid>
<content:encoded><![CDATA[
arXiv:2503.14232v2 Announce Type: replace-cross 
Abstract: Text-to-Image diffusion models can produce undesirable content that necessitates concept erasure. However, existing methods struggle with under-erasure, leaving residual traces of targeted concepts, or over-erasure, mistakenly eliminating unrelated but visually similar concepts. To address these limitations, we introduce CRCE, a novel concept erasure framework that leverages Large Language Models to identify both semantically related concepts that should be erased alongside the target and distinct concepts that should be preserved. By explicitly modelling coreferential and retained concepts semantically, CRCE enables more precise concept removal, without unintended erasure. Experiments demonstrate that CRCE outperforms existing methods on diverse erasure tasks, including real-world object, person identities, and abstract intellectual property characteristics. The constructed dataset CorefConcept and the source code will be release upon acceptance.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAPO: An Open-Source LLM Reinforcement Learning System at Scale</title>
<link>https://arxiv.org/abs/2503.14476</link>
<guid>https://arxiv.org/abs/2503.14476</guid>
<content:encoded><![CDATA[
arXiv:2503.14476v2 Announce Type: replace-cross 
Abstract: Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\textbf{D}$ecoupled Clip and $\textbf{D}$ynamic s$\textbf{A}$mpling $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions</title>
<link>https://arxiv.org/abs/2503.16505</link>
<guid>https://arxiv.org/abs/2503.16505</guid>
<content:encoded><![CDATA[
arXiv:2503.16505v2 Announce Type: replace-cross 
Abstract: Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using Large Language Models (LLMs) to create initial pilot experiments. We propose a simple, generalizable, LLM-driven methodology to prototype the development of LLM facilitators, and produce high-quality synthetic data without human involvement. We use our methodology to test whether current facilitation strategies can improve the performance of LLM facilitators. We find that, while LLM facilitators significantly improve synthetic discussions, there is no evidence that the application of more elaborate facilitation strategies proposed in modern Social Science research lead to further improvements in discussion quality, compared to more basic approaches. Additionally, we find that small LLMs (such as Mistral Nemo 12B) can perform comparably to larger models (such as LLaMa 70B), and that special instructions must be used for instruction-tuned models to induce toxicity in synthetic discussions. We confirm that each component of our methodology contributes substantially to high quality data via an ablation study. We release an open-source framework, "SynDisco" (pip install syndisco), which implements our methodology. We also release the "Virtual Moderation Dataset" (https://paperswithcode.com/dataset/vmd), a large, publicly available dataset containing LLM-generated and LLM-annotated discussions using multiple open-source LLMs.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding</title>
<link>https://arxiv.org/abs/2504.01281</link>
<guid>https://arxiv.org/abs/2504.01281</guid>
<content:encoded><![CDATA[
arXiv:2504.01281v3 Announce Type: replace-cross 
Abstract: We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Reason under Off-Policy Guidance</title>
<link>https://arxiv.org/abs/2504.14945</link>
<guid>https://arxiv.org/abs/2504.14945</guid>
<content:encoded><![CDATA[
arXiv:2504.14945v3 Announce Type: replace-cross 
Abstract: Recent advances in large reasoning models (LRMs) demonstrate that sophisticated behaviors such as multi-step reasoning and self-reflection can emerge via reinforcement learning with verifiable rewards~(\textit{RLVR}). However, existing \textit{RLVR} approaches are inherently ``on-policy'', limiting learning to a model's own outputs and failing to acquire reasoning abilities beyond its initial capabilities. To address this issue, we introduce \textbf{LUFFY} (\textbf{L}earning to reason \textbf{U}nder o\textbf{FF}-polic\textbf{Y} guidance), a framework that augments \textit{RLVR} with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training. Specifically, LUFFY combines the Mixed-Policy GRPO framework, which has a theoretically guaranteed convergence rate, alongside policy shaping via regularized importance sampling to avoid superficial and rigid imitation during mixed-policy training. Compared with previous RLVR methods, LUFFY achieves an over \textbf{+6.4} average gain across six math benchmarks and an advantage of over \textbf{+6.2} points in out-of-distribution tasks. Most significantly, we show that LUFFY successfully trains weak models in scenarios where on-policy RLVR completely fails. These results provide compelling evidence that LUFFY transcends the fundamental limitations of on-policy RLVR and demonstrates the great potential of utilizing off-policy guidance in RLVR.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension</title>
<link>https://arxiv.org/abs/2504.17821</link>
<guid>https://arxiv.org/abs/2504.17821</guid>
<content:encoded><![CDATA[
arXiv:2504.17821v2 Announce Type: replace-cross 
Abstract: Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation</title>
<link>https://arxiv.org/abs/2504.21751</link>
<guid>https://arxiv.org/abs/2504.21751</guid>
<content:encoded><![CDATA[
arXiv:2504.21751v2 Announce Type: replace-cross 
Abstract: Modern software development demands code that is maintainable, testable, and scalable by organizing the implementation into modular components with iterative reuse of existing codes. We formalize this iterative, multi-turn paradigm as codeflow and introduce CodeFlowBench, the first benchmark designed to comprehensively evaluate LLMs' ability to perform codeflow, namely implementing new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5,258 problems from Codeforces and is continuously updated via an automated pipeline, which decomposes each problem into subproblems with unit tests based on dependency tree analysis and dataflow analysis. We further propose a novel evaluation framework featured dual assessment protocol and structural metrics derived from dependency trees. Extensive experiments on 16 popular LLMs reveal significant performance degradation in multi-turn scenarios. For instance, o1-mini retains only 20.8% Pass@1 in multi-turn scenario versus 37.8% in single-turn scenario. More fine-grained analysis illustrates that model performance inversely correlates with dependency complexity. These findings not only highlight the critical challenges for supporting real-world workflows, but also establish CodeFlowBench as an essential tool for advancing code generation research.
]]></content:encoded>
<pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant</title>
<link>https://arxiv.org/abs/2409.11055</link>
<guid>https://arxiv.org/abs/2409.11055</guid>
<content:encoded><![CDATA[
<div> evaluation, quantization methods, model performance, task difficulty, instruction-following, hallucination detection, AWQ, GPTQ, FP16, FP8, 4-bit quantization, 70B-scale models, task weaknesses

Summary:
Our evaluation of instruction-tuned models ranging from 1B to 405B parameters and applying four quantization methods across 13 datasets revealed that quantized models generally outperform smaller FP16 baselines but struggle with instruction-following and hallucination detection. FP8 emerged as the most robust option, with AWQ outperforming GPTQ in weight-only quantization. Smaller models experienced severe accuracy drops at 4-bit quantization, while larger models maintained stable performance. Notably, task difficulty did not always correlate with the largest accuracy losses, indicating that quantization magnifies a model's weaknesses. An LLM-based judge highlighted significant performance declines in Coding and STEM tasks, with occasional improvements in reasoning tasks. Overall, quantization offers promising cost-effective deployment options for language models but requires careful consideration of model size, task requirements, and quantization methods. <br /><br />Summary: <div>
arXiv:2409.11055v5 Announce Type: replace 
Abstract: Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.00979</link>
<guid>https://arxiv.org/abs/2505.00979</guid>
<content:encoded><![CDATA[
<div> framework, data generation, synthetic, knowledge, language models
Summary:
- Large Language Models (LLMs) are successful but data-inefficient, especially with limited, specialized data.
- Synthetic-on-Graph (SoG) framework aims to improve data diversity and coherence by incorporating cross-document knowledge associations.
- SoG constructs a context graph from the original corpus to enhance synthetic data quality.
- Integration of Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic methods further enhances reasoning and discriminative power.
- Experiments show SoG outperforms state-of-the-art methods in a multi-hop document Q&amp;A dataset and performs comparably in reading comprehension tasks, showing better generalization capabilities for LLMs in limited data domains.
<br /><br />Summary: <div>
arXiv:2505.00979v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&amp;A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RM-R1: Reward Modeling as Reasoning</title>
<link>https://arxiv.org/abs/2505.02387</link>
<guid>https://arxiv.org/abs/2505.02387</guid>
<content:encoded><![CDATA[
<div> reward modeling, deep thinking, interpretable reasoning, reasoning capabilities, generative reward models<br />
<br />
Summary:<br />
Reward modeling is crucial for aligning large language models with human preferences. By integrating reasoning capabilities into reward modeling, a new class of generative reward models called Reasoning Reward Models (ReasRMs) is introduced. These models formulate reward modeling as a reasoning task, stimulating deep thinking and conducting interpretable reasoning before assigning scores or judgments. The training pipeline involves distillation of high-quality reasoning chains and reinforcement learning with verifiable rewards. Empirical results show that ReasRMs, specifically RM-R1, outperform existing models on benchmark tasks by up to 4.9%. The release of six REASRM models, along with code and data, aims to facilitate future research in this area.<br /> 
<br />Summary: <div>
arXiv:2505.02387v3 Announce Type: replace 
Abstract: Reward modeling is essential for aligning large language models with human preferences through reinforcement learning from human feedback. To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. Inspired by recent advances of long chain-of-thought on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RMs interpretability and performance. To this end, we introduce a new class of generative reward models - Reasoning Reward Models (ReasRMs) - which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. RM-R1 features a chain-of-rubrics (CoR) mechanism - self-generating sample-level chat rubrics or math/code solutions, and evaluating candidate responses against them. The training of RM-R1 consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. Empirically, our models achieve state-of-the-art performance across three reward model benchmarks on average, outperforming much larger open-weight models (e.g., INF-ORM-Llama3.1-70B) and proprietary ones (e.g., GPT-4o) by up to 4.9%. Beyond final performance, we perform thorough empirical analyses to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six REASRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection</title>
<link>https://arxiv.org/abs/2505.05327</link>
<guid>https://arxiv.org/abs/2505.05327</guid>
<content:encoded><![CDATA[
<div> method, data selection, language models, performance improvement, training costs

Summary:
The paper introduces Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method for data selection in instruction tuning for large language models. RICo accurately quantifies the contribution of individual data samples to task-level and global-level model performance, leading to better instruction tuning. A lightweight selection paradigm based on RICo scores allows for scalable data selection with linear inference complexity. Experiments across multiple benchmarks demonstrate RICo's effectiveness, with models trained on RICo-selected data outperforming full datasets and other selection methods. The analysis of high-contribution samples selected by RICo reveals diverse tasks and appropriate difficulty levels, showing the method's capability to select data beyond just the hardest ones. <div>
arXiv:2505.05327v2 Announce Type: replace 
Abstract: Data selection for instruction tuning is crucial for improving the performance of large language models (LLMs) while reducing training costs. In this paper, we propose Refined Contribution Measurement with In-Context Learning (RICo), a novel gradient-free method that quantifies the fine-grained contribution of individual samples to both task-level and global-level model performance. RICo enables more accurate identification of high-contribution data, leading to better instruction tuning. We further introduce a lightweight selection paradigm trained on RICo scores, enabling scalable data selection with a strictly linear inference complexity. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of RICo. Remarkably, on LLaMA3.1-8B, models trained on 15% of RICo-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by RICo, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking LLMs' Swarm intelligence</title>
<link>https://arxiv.org/abs/2505.04364</link>
<guid>https://arxiv.org/abs/2505.04364</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-Agent Systems, Swarm Intelligence, Decentralized Coordination, Emergent Collective Behavior

Summary:<br /><br />
The article introduces SwarmBench, a new benchmark to evaluate the swarm intelligence capabilities of Large Language Models (LLMs) acting as decentralized agents in Multi-Agent Systems (MAS). The benchmark includes five coordination tasks within a 2D grid environment, where agents rely on local sensory information and communication. Results from zero-shot evaluations of leading LLMs show significant performance variations across tasks, with struggles in long-range planning and adaptive strategy formation. The study highlights the importance of assessing LLMs under swarm-like constraints for understanding their utility in decentralized intelligent systems. SwarmBench is released as an open toolkit to support reproducible research in LLM-based MAS coordination and the study of emergent collective behavior under severe decentralization. The code repository is available at https://github.com/x66ccff/swarmbench. <div>
arXiv:2505.04364v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict swarm-like constraints-limited local perception and communication-remains largely unexplored. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks (Pursuit, Synchronization, Foraging, Flocking, Transport) within a configurable 2D grid environment, forcing agents to rely solely on local sensory input ($k\times k$ view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Zero-shot evaluations of leading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent performance variations. While some rudimentary coordination is observed, our results indicate that current LLMs significantly struggle with robust long-range planning and adaptive strategy formation under the uncertainty inherent in these decentralized scenarios. Assessing LLMs under such swarm-like constraints is crucial for understanding their utility in future decentralized intelligent systems. We release SwarmBench as an open, extensible toolkit-built on a customizable physical system-providing environments, prompts, evaluation scripts, and comprehensive datasets. This aims to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of emergent collective behavior under severe informational decentralization. Our code repository is available at https://github.com/x66ccff/swarmbench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism</title>
<link>https://arxiv.org/abs/2505.11533</link>
<guid>https://arxiv.org/abs/2505.11533</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, implicit user intentions, data synthesis, proactive mining, tourism<br />
<br />
Summary:
SynPT is a new approach that addresses limitations in current methods for mining implicit user intentions in the tourism domain using Large Language Models (LLMs). By creating a data synthesis method driven by LLMs, SynPT aims to generate a high-quality training dataset containing explicit reasoning for proactive intention mining. By simulating dialogues based on seed data from Chinese tourism websites, SynPT-Dialog is constructed to fine-tune LLMs to better understand and guide user needs. The approach focuses on adapting to the tourism domain, balancing detail levels in inquiries, reducing contextual redundancy, and considering tourists' emotions and values. Experimental evaluations show the superiority of SynPT compared to existing methods, with insights on key hyperparameters and case studies demonstrating practical applicability, including potential adaptation to English-language scenarios. The code and data are publicly available. <br /><br />Summary: <div>
arXiv:2505.11533v1 Announce Type: new 
Abstract: In the tourism domain, Large Language Models (LLMs) often struggle to mine implicit user intentions from tourists' ambiguous inquiries and lack the capacity to proactively guide users toward clarifying their needs. A critical bottleneck is the scarcity of high-quality training datasets that facilitate proactive questioning and implicit intention mining. While recent advances leverage LLM-driven data synthesis to generate such datasets and transfer specialized knowledge to downstream models, existing approaches suffer from several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed distributions of detail levels in initial inquiries, (3) contextual redundancy in the implicit intention mining module, and (4) lack of explicit thinking about tourists' emotions and intention values. Therefore, we propose SynPT (A Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User Intentions in the Tourism), which constructs an LLM-driven user agent and assistant agent to simulate dialogues based on seed data collected from Chinese tourism websites. This approach addresses the aforementioned limitations and generates SynPT-Dialog, a training dataset containing explicit reasoning. The dataset is utilized to fine-tune a general LLM, enabling it to proactively mine implicit user intentions. Experimental evaluations, conducted from both human and LLM perspectives, demonstrate the superiority of SynPT compared to existing methods. Furthermore, we analyze key hyperparameters and present case studies to illustrate the practical applicability of our method, including discussions on its adaptability to English-language scenarios. All code and data are publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification</title>
<link>https://arxiv.org/abs/2505.11550</link>
<guid>https://arxiv.org/abs/2505.11550</guid>
<content:encoded><![CDATA[
<div> Language Models, AI-generated text, Detection, Model identification, Neural architectures
Summary: 
- Large Language Models (LLMs) are powerful in generating text resembling human writing, but may be misused for fake news and spam.
- Accurate detection of AI-generated text and identifying the model responsible are crucial for responsible use.
- This work addressed two tasks from the Defactify workshop on AI-Generated Text Detection.
- Task A involved distinguishing between human and AI-generated text, where an optimized neural architecture ranked fifth with an F1 score of 0.994.
- Task B focused on attributing text to its originating language model, with a simpler neural architecture ranking fifth with an F1 score of 0.627.<br /><br />Summary: <div>
arXiv:2505.11550v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks</title>
<link>https://arxiv.org/abs/2505.11556</link>
<guid>https://arxiv.org/abs/2505.11556</guid>
<content:encoded><![CDATA[
<div> Hidden Profile paradigm, multi-agent systems, large language models, collective reasoning, distributed knowledge. 
Summary: 
This paper introduces the Hidden Profile paradigm as a benchmark for evaluating multi-agent systems built on large language models (LLMs). The paradigm involves distributing critical information unevenly among agents to assess collective reasoning. Experiments with GPT-4.1 and other LLMs show that multi-agent systems struggle to match single-agent accuracy with complete information. Despite comparable performance to human groups, subtle behavioral differences, like sensitivity to social desirability, emerge. The study also reveals a trade-off between cooperation and contradiction in multi-agent systems, with cooperative agents tending to over-coordinate and increased contradiction hindering group convergence. This work provides a reproducible framework for assessing multi-agent LLM systems, with implications for artificial collective intelligence and human-AI interaction. 
<br /><br />Summary: <div>
arXiv:2505.11556v1 Announce Type: new 
Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents' collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm's diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models</title>
<link>https://arxiv.org/abs/2505.11604</link>
<guid>https://arxiv.org/abs/2505.11604</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, PowerPoint, slide editing, Talk-to-Your-Slides, TSBench<br />
Summary:<br />
Existing research has primarily focused on generating slides using large language models (LLMs) for PowerPoint, neglecting the editing aspect. This study introduces Talk-to-Your-Slides, an LLM-powered system that directly edits slides in active PowerPoint sessions through COM communication. The system utilizes a two-level approach involving high-level processing by an LLM agent and low-level execution through Python scripts, enabling flexible and contextually-aware editing. A dataset called TSBench, consisting of 379 editing instructions and slide variations, is created for evaluation purposes. Experimental results demonstrate the superior performance of Talk-to-Your-Slides over baseline methods in terms of execution success rate, instruction fidelity, and editing efficiency. The code and benchmark are publicly available for further research. <br /><br />Summary: <div>
arXiv:2505.11604v1 Announce Type: new 
Abstract: Existing research on large language models (LLMs) for PowerPoint predominantly focuses on slide generation, overlooking the common yet tedious task of editing existing slides. We introduce Talk-to-Your-Slides, an LLM-powered agent that directly edits slides within active PowerPoint sessions through COM communication. Our system employs a two-level approach: (1) high-level processing where an LLM agent interprets instructions and formulates editing plans, and (2) low-level execution where Python scripts directly manipulate PowerPoint objects. Unlike previous methods relying on predefined operations, our approach enables more flexible and contextually-aware editing. To facilitate evaluation, we present TSBench, a human-annotated dataset of 379 diverse editing instructions with corresponding slide variations. Experimental results demonstrate that Talk-to-Your-Slides significantly outperforms baseline methods in execution success rate, instruction fidelity, and editing efficiency. Our code and benchmark are available at https://anonymous.4open.science/r/talk-to-your-slides/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11613</link>
<guid>https://arxiv.org/abs/2505.11613</guid>
<content:encoded><![CDATA[
<div> benchmark, clinical guidelines, Large Language Models, MedGUIDE, structured protocols

Summary:
- Clinical guidelines are essential for safe and accurate diagnostic decision-making in evidence-based medical practice.
- The MedGUIDE benchmark evaluates Large Language Models (LLMs) on their ability to make guideline-consistent clinical decisions.
- MedGUIDE consists of 55 curated NCCN decision trees across 17 cancer types and uses LLM-generated clinical scenarios for multiple-choice diagnostic questions.
- A two-stage quality selection process is used to identify high-quality samples for evaluation.
- Results show that even domain-specific LLMs often struggle with tasks requiring adherence to structured guidelines. Continued pretraining or including guidelines in context did not significantly improve performance. The importance of MedGUIDE in assessing the ability of LLMs to operate safely within real-world clinical settings is highlighted. 

<br /><br />Summary: <div>
arXiv:2505.11613v1 Announce Type: new 
Abstract: Clinical guidelines, typically structured as decision trees, are central to evidence-based medical practice and critical for ensuring safe and accurate diagnostic decision-making. However, it remains unclear whether Large Language Models (LLMs) can reliably follow such structured protocols. In this work, we introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to make guideline-consistent clinical decisions. MedGUIDE is constructed from 55 curated NCCN decision trees across 17 cancer types and uses clinical scenarios generated by LLMs to create a large pool of multiple-choice diagnostic questions. We apply a two-stage quality selection process, combining expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25 LLMs spanning general-purpose, open-source, and medically specialized models, and find that even domain-specific LLMs often underperform on tasks requiring structured guideline adherence. We also test whether performance can be improved via in-context guideline inclusion or continued pretraining. Our findings underscore the importance of MedGUIDE in assessing whether LLMs can operate safely within the procedural frameworks expected in real-world clinical settings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations</title>
<link>https://arxiv.org/abs/2505.11615</link>
<guid>https://arxiv.org/abs/2505.11615</guid>
<content:encoded><![CDATA[
<div> behavioral methods, latent representations, steering vectors, large language models, neural activations
Summary:<br />
- Large language models' behavior can be changed by editing residual streams with steering vectors.
- Steering vectors modify internal neural activations without retraining the model.
- A systematic approach is proposed to identify steering vectors by aligning latent representations with neural counterparts.
- The approach focuses on extracting risk preferences from LLMs and steering their risk-related outputs using aligned representations.
- Results show that steering vectors effectively modulate LLM outputs to achieve targeted behavior.<br />Summary: <div>
arXiv:2505.11615v1 Announce Type: new 
Abstract: Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer's residual streams using appropriately constructed "steering vectors." These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering</title>
<link>https://arxiv.org/abs/2505.11626</link>
<guid>https://arxiv.org/abs/2505.11626</guid>
<content:encoded><![CDATA[
<div> metrics, RAG, question answering, evaluation, large language model<br />
Summary:<br />
THELMA is a framework designed for holistic evaluation of RAG (Retrieval Augmented Generation) based question answering applications. It consists of six interdependent metrics that allow developers and application owners to assess and enhance end-to-end RAG QA pipelines without the need for labeled sources or reference responses. The framework aims to assist in the monitoring and improvement of RAG QA applications by providing specific metrics for evaluation. The findings presented in the article highlight the dynamic interplay of THELMA metrics, enabling the identification of areas within RAG components that require enhancement. These insights can help developers optimize their QA applications by focusing on the specific components that need improvement. <div>
arXiv:2505.11626v1 Announce Type: new 
Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation</title>
<link>https://arxiv.org/abs/2505.11628</link>
<guid>https://arxiv.org/abs/2505.11628</guid>
<content:encoded><![CDATA[
<div> fine-tuning, expert demonstrations, Critique-Guided Distillation, entropy-based analysis, benchmark tasks 
Summary: 
Critique-Guided Distillation (CGD) addresses the imitation problem in supervised fine-tuning by integrating explanatory critiques and refined responses into the learning process. The framework trains a student model to understand both what to imitate and why by mapping prompts, teacher critiques, and student responses to refined teacher responses. By reducing refinement uncertainty and resembling a Bayesian posterior update, CGD achieves significant gains on math and language tasks while mitigating format drift issues seen in previous techniques. Extensive empirical evaluation shows improvements in math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), highlighting the effectiveness of the approach. <br /><br /> <div>
arXiv:2505.11628v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from the imitation problem, where the model learns to reproduce the correct responses without \emph{understanding} the underlying rationale. To address this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a novel multi-stage framework that integrates teacher model generated \emph{explanatory critiques} and \emph{refined responses} into the SFT process. A student model is then trained to map the triplet of prompt, teacher critique, and its own initial response to the corresponding refined teacher response, thereby learning both \emph{what} to imitate and \emph{why}. Using entropy-based analysis, we show that \textsc{CGD} reduces refinement uncertainty and can be interpreted as a Bayesian posterior update. We perform extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks, and demonstrate significant gains on both math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format drift issues observed in previous critique fine-tuning (CFT) techniques.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2</title>
<link>https://arxiv.org/abs/2505.11643</link>
<guid>https://arxiv.org/abs/2505.11643</guid>
<content:encoded><![CDATA[
<div> curriculum, reasoning transparency, sample-efficiency, language models, Cognivolve

Summary: 
- The study introduces a curriculum-based training approach for small language models, enhancing reasoning transparency and efficiency.
- Cognivolve, a 124 M-parameter GPT-2 model, is trained on a four-stage syllabus progressing from lexical matching to multi-step symbolic inference.
- Without task-specific fine-tuning, Cognivolve achieves target accuracy in half the optimization steps compared to a single-phase baseline.
- The curriculum activates more gradient-salient reasoning heads, shifting them towards deeper layers for higher-entropy attention and balanced contextual understanding.
- Applying the curriculum out of order or with optimizer resets does not replicate the performance gains, indicating the importance of progression over extra compute resources.
- Challenges remain in final-answer success and detection of verbal-knowledge heads, suggesting potential directions for mixed-stage fine-tuning and probe expansion. <div>
arXiv:2505.11643v1 Announce Type: new 
Abstract: We demonstrate that a developmentally ordered curriculum markedly improves reasoning transparency and sample-efficiency in small language models (SLMs). Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage syllabus that ascends from lexical matching to multi-step symbolic inference and then evaluate it without any task-specific fine-tuning. Cognivolve reaches target accuracy in half the optimization steps of a single-phase baseline, activates an order-of-magnitude more gradient-salient reasoning heads, and shifts those heads toward deeper layers, yielding higher-entropy attention that balances local and long-range context. The same curriculum applied out of order or with optimizer resets fails to reproduce these gains, confirming that progression--not extra compute--drives the effect. We also identify open challenges: final-answer success still lags a conventional run by about 30%, and our saliency probe under-detects verbal-knowledge heads in the hardest stage, suggesting directions for mixed-stage fine-tuning and probe expansion.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks</title>
<link>https://arxiv.org/abs/2505.11665</link>
<guid>https://arxiv.org/abs/2505.11665</guid>
<content:encoded><![CDATA[
<div> fluent multilingual prompting techniques, LLMs, NLP tasks, language families, resource levels <br />
Summary: <br />
- Researchers have developed fluent multilingual prompting techniques to enhance Large Language Models (LLMs) performance in diverse Natural Language Processing (NLP) tasks.
- Multilingual prompt engineering allows LLMs to excel in various linguistic settings without the need for extensive re-training or fine-tuning.
- The survey categorizes different multilingual prompting techniques based on the NLP tasks they address across a wide range of datasets spanning approximately 250 languages.
- Insights are derived across language families and resource levels, analyzing the distribution of NLP tasks by language resource types and prompting methods across different language families.
- The review covers 36 research papers detailing 39 prompting techniques applied to 30 multilingual NLP tasks, showcasing the rapid growth of interest in multilingual prompt engineering in recent years. <br /> <div>
arXiv:2505.11665v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ambiguity Resolution in Text-to-Structured Data Mapping</title>
<link>https://arxiv.org/abs/2505.11679</link>
<guid>https://arxiv.org/abs/2505.11679</guid>
<content:encoded><![CDATA[
<div> Ambiguity, natural language, text-to-structured data mapping, large language models, ReACT framework<br />
Summary:<br />
The study addresses ambiguity in natural language that hinders accurate mapping of text to structured data by large language models (LLMs). Existing methods for handling ambiguity involve trial and error or supervised fine-tuning. The proposed approach focuses on characterizing representation differences of ambiguous text in the latent space to detect ambiguity before mapping to structured data. The research emphasizes the relationship between ambiguous questions and their interpretations, highlighting why LLMs may ignore multiple interpretations. Rather than using dense embedding vectors for distance calculation, a new distance measurement is devised based on the observation that ambiguity stems from missing concepts in the LLM's latent space. This measurement, computed through the path kernel, relies on the integral of gradient values for each concept from a sparse autoencoder (SAE) under each state. The study aims to improve LLM performance on ambiguous agentic tool calling by predicting missing concepts. <div>
arXiv:2505.11679v1 Announce Type: new 
Abstract: Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods of ambiguity handling either exploit ReACT framework to produce the correct mapping through trial and error, or supervised fine tuning to guide models to produce a biased mapping to improve certain tasks. In this paper, we adopt a different approach that characterizes the representation difference of ambiguous text in the latent space and leverage the difference to identify ambiguity before mapping them to structured data. To detect ambiguity of a sentence, we focused on the relationship between ambiguous questions and their interpretations and what cause the LLM ignore multiple interpretations. Different to the distance calculated by dense embedding vectors, we utilize the observation that ambiguity is caused by concept missing in latent space of LLM to design a new distance measurement, computed through the path kernel by the integral of gradient values for each concepts from sparse-autoencoder (SAE) under each state. We identify patterns to distinguish ambiguous questions with this measurement. Based on our observation, We propose a new framework to improve the performance of LLMs on ambiguous agentic tool calling through missing concepts prediction.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation</title>
<link>https://arxiv.org/abs/2505.11683</link>
<guid>https://arxiv.org/abs/2505.11683</guid>
<content:encoded><![CDATA[
<div> Entity disambiguation, Dual Encoder, loss function, similarity metric, label verbalization format
<br />
Summary:
VerbalizED is a new model for entity disambiguation (ED) that utilizes Dual Encoders, focusing on key design decisions such as loss function, similarity metric, label verbalization format, and negative sampling strategy. The model includes contextual label verbalizations and efficient hard negative sampling, improving disambiguation accuracy. An iterative prediction variant aims to enhance challenging data point disambiguation. Extensive experiments on AIDA-Yago demonstrate the effectiveness of the approach, achieving a new State-of-the-Art system on the ZELDA benchmark. The study provides insights into influential design choices that contribute to the model's success. <div>
arXiv:2505.11683v1 Announce Type: new 
Abstract: Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. Comprehensive experiments on AIDA-Yago validate the effectiveness of our approach, offering insights into impactful design choices that result in a new State-of-the-Art system on the ZELDA benchmark.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions</title>
<link>https://arxiv.org/abs/2505.11690</link>
<guid>https://arxiv.org/abs/2505.11690</guid>
<content:encoded><![CDATA[
<div> data scarcity, linguistic complexity, limited computational resources, acoustic variability, ethical concerns

Summary:<br />
This study addresses the challenges facing the development of Automatic Speech Recognition (ASR) systems for low-resource African languages. The key barriers include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns. To overcome these challenges, the study suggests strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques for privacy preservation. Pilot projects in African languages demonstrate the feasibility and impact of tailored solutions like morpheme-based modeling and domain-specific ASR applications in healthcare and education. Interdisciplinary collaboration and sustained investment are crucial to tackle the linguistic and infrastructural obstacles in Africa. The study provides a roadmap for creating ethical, efficient, and inclusive ASR systems that enhance digital accessibility, promote socioeconomic participation, and preserve linguistic diversity in African languages. 

<br /><br /> <div>
arXiv:2505.11690v1 Announce Type: new 
Abstract: Automatic Speech Recognition (ASR) technologies have transformed human-computer interaction; however, low-resource languages in Africa remain significantly underrepresented in both research and practical applications. This study investigates the major challenges hindering the development of ASR systems for these languages, which include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns surrounding bias and privacy. The primary goal is to critically analyze these barriers and identify practical, inclusive strategies to advance ASR technologies within the African context. Recent advances and case studies emphasize promising strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques that prioritize privacy. Evidence from pilot projects involving various African languages showcases the feasibility and impact of customized solutions, which encompass morpheme-based modeling and domain-specific ASR applications in sectors like healthcare and education. The findings highlight the importance of interdisciplinary collaboration and sustained investment to tackle the distinct linguistic and infrastructural challenges faced by the continent. This study offers a progressive roadmap for creating ethical, efficient, and inclusive ASR systems that not only safeguard linguistic diversity but also improve digital accessibility and promote socioeconomic participation for speakers of African languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Bracketing Encodings for Dependency Parsing as Tagging</title>
<link>https://arxiv.org/abs/2505.11693</link>
<guid>https://arxiv.org/abs/2505.11693</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence labeling, dependency parsing, hierarchical bracketing, projective encoding, treebanks

Summary: 
Hierarchical bracketing is utilized for a family of encodings in sequence labeling dependency parsing, with proven optimality over existing methods. The established 4-bit projective encoding, although belonging to this family, is shown to be suboptimal in terms of label efficiency for tree encoding. A more optimal hierarchical bracketing scheme has been derived, significantly reducing the number of labels required to encode projective trees to just 12, compared to 16 in the 4-bit encoding. This optimal hierarchical bracketing extends its support to include non-projective structures in a compact manner. These newly proposed encodings demonstrate competitive accuracy across various treebanks, showcasing their effectiveness in improving the efficiency and performance of sequence labeling dependency parsing. 

<br /><br />Summary: <div>
arXiv:2505.11693v1 Announce Type: new 
Abstract: We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We prove that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures</title>
<link>https://arxiv.org/abs/2505.11726</link>
<guid>https://arxiv.org/abs/2505.11726</guid>
<content:encoded><![CDATA[
<div> phrase grounding, multimodal reference resolution, dialogue, coreference resolution, predicate-argument structure analysis

Summary: 

- The paper discusses the importance of integrating textual and multimodal reference resolution for understanding semantic relations between mentions and real-world objects in dialogue.
- A framework is presented that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity.
- Experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, improves performance in multimodal reference resolution.
- The model with coreference resolution performs better in pronoun phrase grounding compared to existing models MDETR and GLIP.
- Qualitative analysis reveals that incorporating textual reference relations increases confidence scores between mentions, including pronouns and predicates, and objects, reducing ambiguities in visually grounded dialogues. 

<br /><br />Summary: <div>
arXiv:2505.11726v1 Announce Type: new 
Abstract: Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports</title>
<link>https://arxiv.org/abs/2505.11733</link>
<guid>https://arxiv.org/abs/2505.11733</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Medical diagnosis, Diagnostic reasoning, Dataset, Clinical reasoning

Summary:<br /><br />
Doctors and patients use Large Language Models (LLMs) for diagnosing clinical cases, but current benchmarks only assess final answer accuracy. To address this limitation, a new dataset called MedCaseReasoning evaluates LLMs' alignment with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic cases with detailed reasoning statements from medical reports. State-of-the-art LLMs show shortcomings in diagnoses and reasoning. Fine-tuning LLMs on MedCaseReasoning improves diagnostic accuracy and clinical reasoning recall. The top-performing open-source model, DeepSeek-R1, achieves 48% 10-shot diagnostic accuracy and mentions 64% of clinician reasoning statements. This dataset, code, and models are available for use, highlighting the importance of accurate clinical reasoning in medical diagnosis.Visit https://github.com/kevinwu23/Stanford-MedCaseReasoning for more information. 

<br /><br /> <div>
arXiv:2505.11733v1 Announce Type: new 
Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training</title>
<link>https://arxiv.org/abs/2505.11739</link>
<guid>https://arxiv.org/abs/2505.11739</guid>
<content:encoded><![CDATA[
<div> attention tuning, large language models, ZeroTuning, token-level, model performance

Summary: 
The paper introduces ZeroTuning, a method that enhances the performance of large language models (LLMs) by adjusting the attention given to the semantically empty initial token. Theoretical analysis reveals that tuning the initial token's attention influences the attention distribution over subsequent tokens. ZeroTuning proves more effective in improving LLM performance compared to tuning other task-specific tokens, with earlier layers showing a greater impact. Different attention heads exhibit distinct preferences in attending to the initial token. Empirical results demonstrate the efficacy of ZeroTuning across various tasks and LLM models, achieving significant performance gains. The approach remains robust under various conditions like limited resources, few-shot settings, and prompt variations. ZeroTuning sheds light on an overlooked control point in LLMs, offering insights into both inference-time tuning and model interpretability. 

<br /><br />Summary: <div>
arXiv:2505.11739v1 Announce Type: new 
Abstract: Recently, training-free methods for improving large language models (LLMs) have attracted growing interest, with token-level attention tuning emerging as a promising and interpretable direction. However, existing methods typically rely on auxiliary mechanisms to identify important or irrelevant task-specific tokens, introducing potential bias and limiting applicability. In this paper, we uncover a surprising and elegant alternative: the semantically empty initial token is a powerful and underexplored control point for optimizing model behavior. Through theoretical analysis, we show that tuning the initial token's attention sharpens or flattens the attention distribution over subsequent tokens, and its role as an attention sink amplifies this effect. Empirically, we find that: (1) tuning its attention improves LLM performance more effectively than tuning other task-specific tokens; (2) the effect follows a consistent trend across layers, with earlier layers having greater impact, but varies across attention heads, with different heads showing distinct preferences in how they attend to this token. Based on these findings, we propose ZeroTuning, a training-free approach that improves LLM performance by applying head-specific attention adjustments to this special token. Despite tuning only one token, ZeroTuning achieves higher performance on text classification, multiple-choice, and multi-turn conversation tasks across models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its multi-turn score from 7.804 to 7.966. The method is also robust to limited resources, few-shot settings, long contexts, quantization, decoding strategies, and prompt variations. Our work sheds light on a previously overlooked control point in LLMs, offering new insights into both inference-time tuning and model interpretability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Masking Improves Transformer-Based Text Classification</title>
<link>https://arxiv.org/abs/2505.11746</link>
<guid>https://arxiv.org/abs/2505.11746</guid>
<content:encoded><![CDATA[
<div> masking, token, regularization, transformer-based models, text classification <br />
<br />
Summary: 
The article introduces token masking regularization, a method that randomly replaces input tokens with a special [MASK] token during training to enhance transformer-based models in text classification tasks. The proposed approach introduces stochastic perturbations that encourage the model to capture deeper inter-token dependencies by implicitly averaging gradients. Experimental results across different models show consistent improvements in language identification and sentiment analysis tasks. Optimal masking rates are identified for specific tasks, with a recommended default value of p = 0.1. The benefits of the approach are attributed to reducing overfitting through input perturbation and acting as implicit ensembling through gradient-level smoothing. <div>
arXiv:2505.11746v1 Announce Type: new 
Abstract: While transformer-based models achieve strong performance on text classification, we explore whether masking input tokens can further enhance their effectiveness. We propose token masking regularization, a simple yet theoretically motivated method that randomly replaces input tokens with a special [MASK] token at probability p. This introduces stochastic perturbations during training, leading to implicit gradient averaging that encourages the model to capture deeper inter-token dependencies. Experiments on language identification and sentiment analysis -- across diverse models (mBERT, Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard regularization techniques. We identify task-specific optimal masking rates, with p = 0.1 as a strong general default. We attribute the gains to two key effects: (1) input perturbation reduces overfitting, and (2) gradient-level smoothing acts as implicit ensembling.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation</title>
<link>https://arxiv.org/abs/2505.11754</link>
<guid>https://arxiv.org/abs/2505.11754</guid>
<content:encoded><![CDATA[
<div> Encoder-decoder, Flan T5, multi-hop reasoning, LM attention weights, GitHub repository <br />
Summary: <br />
1) Encoder-decoder models like those in the Flan T5 family outperform causal decoder-only LMs in multi-hop question answering tasks, despite their smaller size. <br />
2) Altering the order of gold documents reveals performance trends, with optimal results when the order aligns with the reasoning chain sequence. <br />
3) Enhancing causal decoder-only models with bi-directional attention by modifying the causal mask improves their performance. <br />
4) LM attention weights peak at higher values when the answer is correct, allowing for heuristic performance improvements. <br />
5) The study provides insights into the behavior of LMs in multi-hop question answering tasks and offers a publicly available code repository for further research. <br /> <div>
arXiv:2505.11754v1 Announce Type: new 
Abstract: Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Universal Semantics With Large Language Models</title>
<link>https://arxiv.org/abs/2505.11764</link>
<guid>https://arxiv.org/abs/2505.11764</guid>
<content:encoded><![CDATA[
<div> semantic primes, Natural Semantic Metalanguage, explications, large language models, automatic evaluation

Summary:
- The Natural Semantic Metalanguage (NSM) theory is based on semantic primes, basic word meanings found in all languages.
- Any word can be paraphrased using these primes to reveal a universally translatable meaning.
- NSM explications, or paraphrases, have potential applications in various natural language processing (NLP) tasks.
- This study explores using large language models (LLMs) to generate NSM explications, a traditionally manual process.
- The introduced models, 1B and 8B, outperform GPT-4o in producing accurate and cross-translatable explications, advancing universal semantic representation with LLMs and enabling new applications in semantic analysis and translation.

<br /><br />Summary: <div>
arXiv:2505.11764v1 Announce Type: new 
Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrospex: Language Agent Meets Offline Reinforcement Learning Critic</title>
<link>https://arxiv.org/abs/2505.11807</link>
<guid>https://arxiv.org/abs/2505.11807</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Retrospex, Reinforcement Learning, past experiences, agent framework <br />
Summary:
The paper introduces Retrospex, a new agent framework based on Large Language Models (LLMs) that leverages past experiences for improved performance. Unlike existing frameworks, Retrospex does not directly incorporate past experiences into the LLM's context but combines action likelihood with values estimated by a Reinforcement Learning (RL) Critic trained through a retrospective process. Additionally, Retrospex utilizes a dynamic action rescoring mechanism to enhance the importance of experience-based values for tasks requiring more interaction with the environment. Evaluation in ScienceWorld, ALFWorld, and Webshop environments showcases Retrospex's superiority over contemporary baselines in terms of performance and efficiency. The framework exhibits enhanced knowledge and reasoning capabilities, making it a valuable tool for creating powerful agents in various applications. <br /><br />Summary: <div>
arXiv:2505.11807v1 Announce Type: new 
Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model</title>
<link>https://arxiv.org/abs/2505.11810</link>
<guid>https://arxiv.org/abs/2505.11810</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, Classical Chinese, AI Taiyan, domain-specific, information processing

Summary: 
AI Taiyan is a large language model developed specifically for understanding and generating Classical Chinese texts. With 1.8 billion parameters, it outperforms both general-purpose models and traditional domain-specific models in tasks such as punctuation, identification of allusions, and translation. The model showcases high accuracy levels, close to or surpassing human baselines, showcasing its effectiveness in processing Classical Chinese information. The research provides insights into efficiently constructing specialized domain-specific language models and discusses applications in ancient text collation, dictionary editing, and language research through case studies.<br /><br />Summary: <div>
arXiv:2505.11810v1 Announce Type: new 
Abstract: General-purpose large language models demonstrate notable capabilities in language comprehension and generation, achieving results that are comparable to, or even surpass, human performance in many language information processing tasks. Nevertheless, when general models are applied to some specific domains, e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and fine-tuning open-source foundational models similarly struggles to adequately incorporate domain-specific knowledge. To address this challenge, this study developed a large language model, AI Taiyan, specifically designed for understanding and generating Classical Chinese. Experiments show that with a reasonable model design, data processing, foundational training, and fine-tuning, satisfactory results can be achieved with only 1.8 billion parameters. In key tasks related to Classical Chinese information processing such as punctuation, identification of allusions, explanation of word meanings, and translation between ancient and modern Chinese, this model exhibits a clear advantage over both general-purpose large models and domain-specific traditional models, achieving levels close to or surpassing human baselines. This research provides a reference for the efficient construction of specialized domain-specific large language models. Furthermore, the paper discusses the application of this model in fields such as the collation of ancient texts, dictionary editing, and language research, combined with case studies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2505.11811</link>
<guid>https://arxiv.org/abs/2505.11811</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-hop question answering, large language models, Bi-levEL muLti-agEnt reasoning, question types, BELLE framework

Summary: 
The paper analyzes multi-hop question answering benchmarks and identifies four question types. It evaluates different methods for multi-hop QA and introduces the Bi-levEL muLti-agEnt reasoning (BELLE) framework, which matches question types with specific methods. BELLE consists of multiple agents debating to create a comprehensive plan using different ''operators''. The model incorporates fast and slow debaters to ensure rational changes in viewpoints. Experimental results show that BELLE outperforms strong baselines across various datasets. It proves to be more cost-effective in complex multi-hop QA scenarios compared to single models. The study highlights the importance of aligning question types with appropriate methods for effective multi-hop question answering. 

<br /><br />Summary: <div>
arXiv:2505.11811v1 Announce Type: new 
Abstract: Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chain-of-Model Learning for Language Model</title>
<link>https://arxiv.org/abs/2505.11820</link>
<guid>https://arxiv.org/abs/2505.11820</guid>
<content:encoded><![CDATA[
<div> Chain-of-Model, causal relationship, scalability, model training, inference flexibility
<br />
Summary:
The paper introduces a novel learning paradigm called Chain-of-Model (CoM) that incorporates the causal relationship into hidden states, leading to improved scalability in model training and flexibility in deployment. The concept of Chain-of-Representation (CoR) is introduced, forming hidden states as a combination of multiple sub-representations. The model built on the CoM framework can scale up by increasing chains based on previous models and offer multiple sub-models for elastic inference. Chain-of-Language-Model (CoLM) integrates CoM into each layer of the Transformer architecture. CoLM-Air, a variation of CoLM, introduces a KV sharing mechanism for added extensibility. Experimental results show that the CoLM family achieves performance similar to the standard Transformer while enabling progressive scaling and multiple model sizes for elastic inference. This framework provides a new approach to building language models. 
<br /><br />Summary: <div>
arXiv:2505.11820v1 Announce Type: new 
Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11827</link>
<guid>https://arxiv.org/abs/2505.11827</guid>
<content:encoded><![CDATA[
<div> Keywords: long chain-of-thought compression, large language models, reasoning efficiency, automated chunking, collaborative reasoning

Summary:<br />
The study focuses on compressing long chain-of-thought from large language models to enhance reasoning efficiency. It examines the importance of different thoughts in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. A metric is proposed to measure the effectiveness and efficiency of various thoughts. The Long$\otimes$Short framework is introduced, where two LLMs collaborate to solve problems effectively and efficiently. Fine-tuning for long-thought and short-thought reasoning styles is done using cold-start data. Synergizing-oriented multi-turn reinforcement learning is applied to promote model self-evolution and collaboration between the two LLMs. Experimental results demonstrate that the approach achieves comparable performance while significantly reducing token length across various benchmarks. The data and code for the study are available on GitHub. 

Summary: <div>
arXiv:2505.11827v1 Announce Type: new 
Abstract: Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at https://github.com/yasNing/Long-otimes-Short/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks</title>
<link>https://arxiv.org/abs/2505.11829</link>
<guid>https://arxiv.org/abs/2505.11829</guid>
<content:encoded><![CDATA[
<div> Keywords: deviant language, sexism detection, metaphor detection, sarcasm detection, ClaD

Summary:
ClaD is introduced as a novel training paradigm for detecting deviant and nuanced language in online social discourse. It aims to distill small target classes from diverse backgrounds efficiently. ClaD incorporates a loss function based on Mahalanobis distance and an interpretable decision algorithm for class separation. It outperforms competitive baselines in detecting sexism, metaphor, and sarcasm, even with smaller language models and fewer parameters. The results indicate that ClaD is effective for pragmatic language understanding tasks requiring extraction of small target classes from heterogeneous backgrounds. <div>
arXiv:2505.11829v1 Announce Type: new 
Abstract: Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of online social discourse. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose \textbf{Cla}ss \textbf{D}istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models (LLMs). These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Collaborative Defense for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11835</link>
<guid>https://arxiv.org/abs/2505.11835</guid>
<content:encoded><![CDATA[
<div> robustness, security, large language models, multilingual safeguarding, jailbreak

Summary:
Multilingual collaborative defense (MCD) is proposed as a novel learning method to enhance the safeguarding of large language models (LLMs) in multilingual scenarios. MCD optimizes a continuous, soft safety prompt automatically to improve safeguarding performance across multiple languages, maintain strong generalization capabilities, and mitigate language safety misalignment. The effectiveness of MCD is evaluated using manually constructed multilingual jailbreak benchmarks, such as MaliciousInstruct and AdvBench, showing superior performance in safeguarding against multilingual jailbreak attempts. Additionally, MCD exhibits strong language transfer capabilities, especially in underrepresented languages. This approach addresses the urgent need for multilingual safety in LLMs and offers a promising solution to the vulnerabilities posed by translating harmful queries into rare or underrepresented languages. The code for MCD is available at https://github.com/HLiang-Lee/MCD.

<br /><br />Summary: <div>
arXiv:2505.11835v1 Announce Type: new 
Abstract: The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of "jailbreaking" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at https://github.com/HLiang-Lee/MCD.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research</title>
<link>https://arxiv.org/abs/2505.11855</link>
<guid>https://arxiv.org/abs/2505.11855</guid>
<content:encoded><![CDATA[
<div> verifiers, scientific manuscripts, AI Co-Scientists, large language models, dataset

Summary:
SPOT dataset created for academic verification of scientific manuscripts includes errors from 83 papers. State-of-the-art LLMs show limited recall and precision in identifying errors. Confidence estimates are low, and models struggle to rediscover errors consistently. Qualitative analysis indicates errors resemble student-level misconceptions. Current LLM capabilities fall short of reliable AI-assisted academic verification.
<br /><br />Summary: <div>
arXiv:2505.11855v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization</title>
<link>https://arxiv.org/abs/2505.11876</link>
<guid>https://arxiv.org/abs/2505.11876</guid>
<content:encoded><![CDATA[
<div> noise, model editing, transformers, knowledge update, LLMs

Summary:
NAMET is a noise-aware model editing technique proposed to address embedding collisions among knowledge items in large language models (LLMs). It introduces noise during memory extraction, outperforming existing methods in massive editing scenarios. The one-line modification to MEMIT enhances editing reliability at scale, making it more effective in context-rich settings. Extensive experiments across six LLMs and three datasets showcase NAMET's superiority when editing thousands of facts. This simple yet effective approach improves the efficiency of updating knowledge in LLMs, making it a valuable tool for researchers and practitioners working with large language models. NAMET proves to be essential in overcoming the limitations of existing model editing techniques, offering a reliable solution for managing large-scale knowledge updates in the context of language models. 

Summary: <br />
NAMET is a noise-aware model editing technique that addresses embedding collisions in large language models. It outperforms existing methods in massive editing scenarios by introducing noise during memory extraction. The one-line modification to MEMIT enhances editing reliability at scale, making it more effective in context-rich settings. Extensive experiments across six LLMs and three datasets demonstrate NAMET's superiority when editing thousands of facts. This approach improves efficiency in updating knowledge in LLMs, making it a valuable tool for researchers and practitioners working with large language models. <div>
arXiv:2505.11876v1 Announce Type: new 
Abstract: Model editing techniques are essential for efficiently updating knowledge in large language models (LLMs). However, the effectiveness of existing approaches degrades in massive editing scenarios, particularly when evaluated with practical metrics or in context-rich settings. We attribute these failures to embedding collisions among knowledge items, which undermine editing reliability at scale. To address this, we propose NAMET (Noise-aware Model Editing in Transformers), a simple yet effective method that introduces noise during memory extraction via a one-line modification to MEMIT. Extensive experiments across six LLMs and three datasets demonstrate that NAMET consistently outperforms existing methods when editing thousands of facts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation</title>
<link>https://arxiv.org/abs/2505.11887</link>
<guid>https://arxiv.org/abs/2505.11887</guid>
<content:encoded><![CDATA[
<div> evaluation, medical language models, AutoMedEval, question-answering proficiency, hierarchical training method

Summary:
AutoMedEval is a new automatic evaluation model designed to assess the question-answering proficiency of medical language models. It addresses the limitations of traditional metrics by focusing on medical terminology. The model, with 13B parameters, uses a hierarchical training method and iterative knowledge introspection to mimic professional medical assessment capabilities. It aims to reduce the reliance on costly and potentially inaccurate human evaluations. AutoMedEval outperforms other baselines in correlation with human judgments, making it a valuable tool for evaluating the quality of responses generated by medical language models. <div>
arXiv:2505.11887v1 Announce Type: new 
Abstract: With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents</title>
<link>https://arxiv.org/abs/2505.11891</link>
<guid>https://arxiv.org/abs/2505.11891</guid>
<content:encoded><![CDATA[
<div> Keywords: VLM-based mobile agents, benchmark, multi-path evaluation, noisy environment, proactive interaction

Summary:
Mobile-Bench-v2 is a new benchmark designed to evaluate VLM-based mobile agents' capabilities in interacting with smartphone GUIs and completing tasks. It addresses limitations of existing benchmarks by including a common task split with multi-path evaluation to assess agents' ability to obtain step rewards, a noisy split with pop-ups and ads apps for a real noisy environment, and an ambiguous instruction split with preset Q&amp;A interactions to test proactive interaction capabilities. Evaluation is conducted using various mobile agents, including AppAgent-v1, Mobile-Agent-v2, UI-Tars, and OS-Atlas. The benchmark aims to provide a realistic and comprehensive assessment of mobile agents' performance in dynamic environments and their ability to handle noise and engage in proactive interactions. The code and data for Mobile-Bench-v2 are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

<br /><br />Summary: <div>
arXiv:2505.11891v1 Announce Type: new 
Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent's ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\&amp;A interactions is released to evaluate the agent's proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving</title>
<link>https://arxiv.org/abs/2505.11893</link>
<guid>https://arxiv.org/abs/2505.11893</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Adaptive Planning, Large Language Models, Natural Language Processing, Multi-step Planning
Summary:<br /><br />The paper introduces a Reinforcement Learning enhanced Adaptive Planning framework (RLAP) to improve the performance of large language models (LLMs) on multi-step natural language processing (NLP) tasks. They model NLP tasks as Markov decision processes and use an Actor model trained through reinforcement learning to estimate Q-values for sequences of states and actions. This allows for considering linguistic features during planning and interaction with the LLM to determine the optimal order of subtasks for each task instance. RLAP is applied to various NLP tasks and tested on multiple datasets, demonstrating its effectiveness and robustness. The framework improves task-solving outcomes by leveraging the linguistic features of task instances and guiding LLMs in a more structured manner through adaptive planning. <div>
arXiv:2505.11893v1 Announce Type: new 
Abstract: Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data</title>
<link>https://arxiv.org/abs/2505.11900</link>
<guid>https://arxiv.org/abs/2505.11900</guid>
<content:encoded><![CDATA[
<div> Keywords: question answering, mixed sources, personal information, ReQAP, PerQA

Summary: 
ReQAP introduces a new method for question answering over heterogeneous data sources, such as text and tables, with a focus on personal information stored on user devices. The method creates an executable operator tree that enables seamless integration of structured and unstructured data sources to provide users with convenient access to their personal data while keeping it securely on their devices. The PerQA benchmark is released along with this method, containing persona-based data and questions that cover a wide range of realistic user needs. This benchmark aims to challenge existing question answering systems to handle complex queries and provide traceable answers. <div>
arXiv:2505.11900v1 Announce Type: new 
Abstract: Question answering over mixed sources, like text and tables, has been advanced by verbalizing all contents and encoding it with a language model. A prominent case of such heterogeneous data is personal information: user devices log vast amounts of data every day, such as calendar entries, workout statistics, shopping records, streaming history, and more. Information needs range from simple look-ups to queries of analytical nature. The challenge is to provide humans with convenient access with small footprint, so that all personal data stays on the user devices. We present ReQAP, a novel method that creates an executable operator tree for a given question, via recursive decomposition. Operators are designed to enable seamless integration of structured and unstructured sources, and the execution of the operator tree yields a traceable answer. We further release the PerQA benchmark, with persona-based data and questions, covering a diverse spectrum of realistic user needs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELITE: Embedding-Less retrieval with Iterative Text Exploration</title>
<link>https://arxiv.org/abs/2505.11908</link>
<guid>https://arxiv.org/abs/2505.11908</guid>
<content:encoded><![CDATA[
<div> Framework, Retrieval, Language Models, Long-context QA, Logical Inferencing <br />
Summary: <br />
This paper introduces a new approach to Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) that addresses the issue of limited long-term context retention. The proposed method does not rely on embeddings for retrieval but instead utilizes the logical inferencing capability of LLMs. By iteratively refining the search space guided by an importance measure, the approach enhances retrieval accuracy without the need for explicit graph construction. Experiments on long-context QA benchmarks such as NovelQA and Marathon demonstrate that the method outperforms existing baselines while significantly reducing storage and runtime requirements. The framework proves effective in leveraging LLMs for retrieval tasks, providing a more nuanced and accurate approach to accessing external information for document-level or multi-turn tasks. <br /> <div>
arXiv:2505.11908v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved impressive progress in natural language processing, but their limited ability to retain long-term context constrains performance on document-level or multi-turn tasks. Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant information from an external corpus. However, existing RAG systems often rely on embedding-based retrieval trained on corpus-level semantic similarity, which can lead to retrieving content that is semantically similar in form but misaligned with the question's true intent. Furthermore, recent RAG variants construct graph- or hierarchy-based structures to improve retrieval accuracy, resulting in significant computation and storage overhead. In this paper, we propose an embedding-free retrieval framework. Our method leverages the logical inferencing ability of LLMs in retrieval using iterative search space refinement guided by our novel importance measure and extend our retrieval results with logically related information without explicit graph construction. Experiments on long-context QA benchmarks, including NovelQA and Marathon, show that our approach outperforms strong baselines while reducing storage and runtime by over an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning</title>
<link>https://arxiv.org/abs/2505.11922</link>
<guid>https://arxiv.org/abs/2505.11922</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, instruction-following, fine-tuning, MISO, transformer-based

Summary:
Large language models (LLMs) are powerful in handling natural language tasks but struggle with complex instructions. Post-training LLMs using supervised fine-tuning (SFT) is a common approach to enhance instruction following. Existing methods focus on synthesizing complex instruction-output pairs for SFT, but may overlook crucial sub-contexts. This work introduces MISO, an extension to decoder-only transformer-based LLMs, that transforms structured input instructions into multiple parallel subcontexts to improve SFT effectiveness. MISO utilizes a mixture-of-contexts paradigm to consider both overall alignment and individual sub-context influence. Applying MISO fine-tuning to complex instruction-following datasets shows its superiority in handling complex instructions and potential for training efficiency. MISO offers a promising solution for improving LLMs in complex instruction-following scenarios. 

<br /><br />Summary: <div>
arXiv:2505.11922v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit remarkable capabilities in handling natural language tasks; however, they may struggle to consistently follow complex instructions including those involve multiple constraints. Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to improve their ability to follow instructions. In addressing complex instruction following, existing efforts primarily focus on data-driven methods that synthesize complex instruction-output pairs for SFT. However, insufficient attention allocated to crucial sub-contexts may reduce the effectiveness of SFT. In this work, we propose transforming sequentially structured input instruction into multiple parallel instructions containing subcontexts. To support processing this multi-input, we propose MISO (Multi-Input Single-Output), an extension to currently dominant decoder-only transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that jointly considers the overall instruction-output alignment and the influence of individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning to complex instructionfollowing datasets and evaluate it with standard LLM inference. Empirical results demonstrate the superiority of MISO as a fine-tuning method for LLMs, both in terms of effectiveness in complex instruction-following scenarios and its potential for training efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts</title>
<link>https://arxiv.org/abs/2505.11924</link>
<guid>https://arxiv.org/abs/2505.11924</guid>
<content:encoded><![CDATA[
<div> Keywords: intrinsic self-correction, language model, hidden states, output distributions, prompt-induced shifts

Summary: 
This article explores the performance gains of intrinsic self-correction in language models, particularly focusing on the impact of prompting on hidden states and output distributions. The study proposes that prompt-induced shifts in language models can be explained by linear representation vectors, leading to a separation of tokens based on concept alignment. By mathematically formulating self-correction and analyzing alignment magnitudes, the research demonstrates a concentration result for output tokens. Experiments conducted on text detoxification with the zephyr-7b-sft model reveal significant differences in prompt-induced shifts when instructed with toxic prompts, indicating an enhanced capability of latent concept recognition in language models. The analysis sheds light on the underlying mechanisms of self-correction and provides insights into how prompting influences a language model's behavior in an explainable manner.

Summary: <div>
arXiv:2505.11924v1 Announce Type: new 
Abstract: We provide an explanation for the performance gains of intrinsic self-correction, a process where a language model iteratively refines its outputs without external feedback. More precisely, we investigate how prompting induces interpretable changes in hidden states and thus affects the output distributions. We hypothesize that each prompt-induced shift lies in a linear span of some linear representation vectors, naturally separating tokens based on individual concept alignment. Building around this idea, we give a mathematical formulation of self-correction and derive a concentration result for output tokens based on alignment magnitudes. Our experiments on text detoxification with zephyr-7b-sft reveal a substantial gap in the inner products of the prompt-induced shifts and the unembeddings of the top-100 most toxic tokens vs. those of the unembeddings of the bottom-100 least toxic tokens, under toxic instructions. This suggests that self-correction prompts enhance a language model's capability of latent concept recognition. Our analysis offers insights into the underlying mechanism of self-correction by characterizing how prompting works explainably. For reproducibility, our code is available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neuro-Symbolic Query Compiler</title>
<link>https://arxiv.org/abs/2505.11932</link>
<guid>https://arxiv.org/abs/2505.11932</guid>
<content:encoded><![CDATA[
<div> Neuro-symbolic framework, Query intent recognition, Retrieval-augmented generation, Compiler design, Backus-Naur Form<br />
Summary:<br />
The paper introduces QCompiler, a neuro-symbolic framework for precise recognition of search intent in Retrieval-Augmented Generation systems. It utilizes a minimal Backus-Naur Form grammar to formalize complex queries, improving query understanding under resource constraints. QCompiler includes components like Query Expression Translator and Lexical Syntax Parser to compile queries into Abstract Syntax Trees, ensuring atomicity of sub-queries for more precise document retrieval and response generation. By minimizing redundancy and maintaining completeness, this approach bridges the gap in addressing complex queries, enhancing the RAG system's performance. <div>
arXiv:2505.11932v1 Announce Type: new 
Abstract: Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing</title>
<link>https://arxiv.org/abs/2505.11935</link>
<guid>https://arxiv.org/abs/2505.11935</guid>
<content:encoded><![CDATA[
<div> benchmark, chart editing, multimodal large language models, evaluation framework, code generation
Summary:
The article introduces ChartEdit, a new benchmark designed for evaluating chart editing tasks using multimodal large language models (MLLMs). The benchmark consists of 1,405 diverse editing instructions applied to 233 real-world charts, with manual annotation and validation for accuracy. Evaluation of 10 mainstream MLLMs shows that while large-scale models can partially match reference images, precise edits according to instructions remain a challenge. The state-of-the-art model achieves a score of only 59.96, indicating limitations in accurate modifications. Small-scale and chart-domain models struggle with both following instructions and generating overall chart images, highlighting the need for further development in this area.
<br /><br />Summary: <div>
arXiv:2505.11935v1 Announce Type: new 
Abstract: Although multimodal large language models (MLLMs) show promise in generating chart rendering code, chart editing presents a greater challenge. This difficulty stems from its nature as a labor-intensive task for humans that also demands MLLMs to integrate chart understanding, complex reasoning, and precise intent interpretation. While many MLLMs claim such editing capabilities, current assessments typically rely on limited case studies rather than robust evaluation methodologies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose ChartEdit, a new high-quality benchmark designed for chart editing tasks. This benchmark comprises $1,405$ diverse editing instructions applied to $233$ real-world charts, with each instruction-chart instance having been manually annotated and validated for accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream MLLMs across two types of experiments, assessing them at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning</title>
<link>https://arxiv.org/abs/2505.11958</link>
<guid>https://arxiv.org/abs/2505.11958</guid>
<content:encoded><![CDATA[
<div> Keywords: Counterspeech, Hate speech, Hierarchical Prefix learning, Preference Optimization, IntentCONANv2

Summary: 
HiPPrO introduces a novel framework for generating counterspeech online, taking into account multiple attributes simultaneously to produce more nuanced and effective responses. The framework utilizes hierarchical prefix learning and preference optimization in a two-stage process to optimize attribute-specific prefix embedding spaces during counterspeech generation. By incorporating emotion labels and optimizing for intent conformity and relevance through preference optimization, HiPPrO outperforms baseline models in intent conformity and Rouge score metrics. Human evaluations confirm the enhanced relevance and appropriateness of the generated counterspeech, highlighting the potential of multi-attribute conditioning in improving the efficacy of counterspeech generation systems. <div>
arXiv:2505.11958v1 Announce Type: new 
Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English</title>
<link>https://arxiv.org/abs/2505.11959</link>
<guid>https://arxiv.org/abs/2505.11959</guid>
<content:encoded><![CDATA[
<div> Keywords: bilingual dataset, emotions, hope speech, Arabic, English

Summary:
This research introduces a bilingual dataset containing entries for Arabic and English, annotated for emotions and hope speech. The dataset addresses the lack of multi-emotion datasets by providing annotations for emotion intensity, complexity, and causes, as well as detailed classifications for hope speech. Fleiss' Kappa was used to ensure annotation reliability, showing high agreement among annotators for both languages. The evaluation metrics from the baseline model validate the quality of the annotations. This dataset serves as a valuable resource for enhancing natural language processing in underrepresented languages and facilitating cross-linguistic analysis of emotions and hope speech. <div>
arXiv:2505.11959v1 Announce Type: new 
Abstract: This research introduces a bilingual dataset comprising 23,456 entries for Arabic and 10,036 entries for English, annotated for emotions and hope speech, addressing the scarcity of multi-emotion (Emotion and hope) datasets. The dataset provides comprehensive annotations capturing emotion intensity, complexity, and causes, alongside detailed classifications and subcategories for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed, revealing 0.75-0.85 agreement among annotators both for Arabic and English language. The evaluation metrics (micro-F1-Score=0.67) obtained from the baseline model (i.e., using a machine learning model) validate that the data annotations are worthy. This dataset offers a valuable resource for advancing natural language processing in underrepresented languages, fostering better cross-linguistic analysis of emotions and hope speech.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation</title>
<link>https://arxiv.org/abs/2505.11965</link>
<guid>https://arxiv.org/abs/2505.11965</guid>
<content:encoded><![CDATA[
<div> hallucinations, question-answering systems, Large Language Models, Mu-SHROOM, Central China Normal University 

Summary:
The Central China Normal University (CCNU) team developed a system for the Mu-SHROOM shared task, focusing on spotting hallucinations in question-answering systems in 14 languages. Their approach utilized multiple Large Language Models (LLMs) with different expertise simultaneously to mark hallucinations, mimicking a crowdsourcing process. Each LLM-based annotator incorporated internal and external knowledge while annotating. Using DeepSeek-V3 LLM, their system ranked first for Hindi data and achieved a Top-5 position in seven other languages. The paper also discusses unsuccessful strategies explored during development and shares insights gained from the shared task participation. <div>
arXiv:2505.11965v1 Announce Type: new 
Abstract: We present the system developed by the Central China Normal University (CCNU) team for the Mu-SHROOM shared task, which focuses on identifying hallucinations in question-answering systems across 14 different languages. Our approach leverages multiple Large Language Models (LLMs) with distinct areas of expertise, employing them in parallel to annotate hallucinations, effectively simulating a crowdsourcing annotation process. Furthermore, each LLM-based annotator integrates both internal and external knowledge related to the input during the annotation process. Using the open-source LLM DeepSeek-V3, our system achieves the top ranking (\#1) for Hindi data and secures a Top-5 position in seven other languages. In this paper, we also discuss unsuccessful approaches explored during our development process and share key insights gained from participating in this shared task.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Annotated Corpus of Arabic Tweets for Hate Speech Analysis</title>
<link>https://arxiv.org/abs/2505.11969</link>
<guid>https://arxiv.org/abs/2505.11969</guid>
<content:encoded><![CDATA[
<div> Keywords: hate speech, Arabic language, dataset, annotation, transformer model

Summary: 
A new dataset of 10000 Arabic tweets containing hate speech content has been introduced, annotated for offensive content and categorized into different targets such as religion, gender, politics, ethnicity, and origin. Inter-annotator agreement was high at 0.86 for offensive content and 0.71 for multiple hate speech targets. Various annotators participated in the data annotation task. The performance evaluation of different transformers-based models showed that AraBERTv2 achieved a micro-F1 score of 0.7865 and an accuracy of 0.786. This dataset and model can be helpful in identifying and combating hate speech in Arabic content on social media platforms. 

Summary: <div>
arXiv:2505.11969v1 Announce Type: new 
Abstract: Identifying hate speech content in the Arabic language is challenging due to the rich quality of dialectal variations. This study introduces a multilabel hate speech dataset in the Arabic language. We have collected 10000 Arabic tweets and annotated each tweet, whether it contains offensive content or not. If a text contains offensive content, we further classify it into different hate speech targets such as religion, gender, politics, ethnicity, origin, and others. A text can contain either single or multiple targets. Multiple annotators are involved in the data annotation task. We calculated the inter-annotator agreement, which was reported to be 0.86 for offensive content and 0.71 for multiple hate speech targets. Finally, we evaluated the data annotation task by employing a different transformers-based model in which AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of 0.786.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.11995</link>
<guid>https://arxiv.org/abs/2505.11995</guid>
<content:encoded><![CDATA[
<div> knowledge utilization, retrieval-augmented generation, LLM, knowledge streaming, knowledge activation probability entropy

Summary:
The paper explores how large language models (LLMs) integrate internal and external knowledge in retrieval-augmented generation (RAG). Analysis reveals four stages in knowledge utilization within LLM layers: refinement, elicitation, expression, and contestation, guided by passage relevance. A new method, knowledge activation probability entropy (KAPE), identifies neurons linked to internal or external knowledge, enabling targeted shifts in knowledge source reliance. Multi-head attention and multi-layer perceptron layers play complementary roles in knowledge formation. These insights enhance interpretability and reliability in RAG, leading to more robust generative solutions in knowledge-intensive domains. <br /><br /> <div>
arXiv:2505.11995v1 Announce Type: new 
Abstract: Considering the inherent limitations of parametric knowledge in large language models (LLMs), retrieval-augmented generation (RAG) is widely employed to expand their knowledge scope. Since RAG has shown promise in knowledge-intensive tasks like open-domain question answering, its broader application to complex tasks and intelligent assistants has further advanced its utility. Despite this progress, the underlying knowledge utilization mechanisms of LLM-based RAG remain underexplored. In this paper, we present a systematic investigation of the intrinsic mechanisms by which LLMs integrate internal (parametric) and external (retrieved) knowledge in RAG scenarios. Specially, we employ knowledge stream analysis at the macroscopic level, and investigate the function of individual modules at the microscopic level. Drawing on knowledge streaming analyses, we decompose the knowledge utilization process into four distinct stages within LLM layers: knowledge refinement, knowledge elicitation, knowledge expression, and knowledge contestation. We further demonstrate that the relevance of passages guides the streaming of knowledge through these stages. At the module level, we introduce a new method, knowledge activation probability entropy (KAPE) for neuron identification associated with either internal or external knowledge. By selectively deactivating these neurons, we achieve targeted shifts in the LLM's reliance on one knowledge source over the other. Moreover, we discern complementary roles for multi-head attention and multi-layer perceptron layers during knowledge formation. These insights offer a foundation for improving interpretability and reliability in retrieval-augmented LLMs, paving the way for more robust and transparent generative solutions in knowledge-intensive domains.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method</title>
<link>https://arxiv.org/abs/2505.12028</link>
<guid>https://arxiv.org/abs/2505.12028</guid>
<content:encoded><![CDATA[
<div> Keywords: Argument mining, Large Language Models, Fine-grained relation types, Argument structure, Automated essay grading

Summary:<br /><br />Argument mining research has gained attention with the advancement of Large Language Models (LLMs), but current argument relations are limited in capturing complex structures. This study introduces 14 fine-grained relation types to better understand argument components in real-world scenarios. Experiments were conducted on argument component detection, relation prediction, and automated essay grading tasks. The impact of writing quality on detection and prediction, as well as the connection between discourse relations and argumentative features, were also explored. The findings emphasize the need for detailed argumentative annotations for assessing writing quality and advocate for multi-dimensional argument analysis. <div>
arXiv:2505.12028v1 Announce Type: new 
Abstract: Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities</title>
<link>https://arxiv.org/abs/2505.12043</link>
<guid>https://arxiv.org/abs/2505.12043</guid>
<content:encoded><![CDATA[
<div> CPT, domain-specific applications, MoL, cross-entropy loss, KL divergence <br />
Summary: 
- The article discusses the limitations of current language models in domain-specific tasks due to data bias and corpus mixture ratios.
- It proposes a new framework called Mixture of Losses (MoL) to address these issues, using cross-entropy loss for domain data and KL divergence for general corpora.
- The dual-loss architecture aims to preserve universal language skills while enhancing domain expertise without forgetting previous knowledge.
- Empirical validation shows that a 1:1 domain-to-general corpus ratio optimally balances training and overfitting.
- Experiments demonstrate significant performance gains compared to traditional approaches, with the model achieving higher accuracy on the Math-500 benchmark and a remarkable improvement on the AIME25 subset. <br /><br />Summary: <div>
arXiv:2505.12043v1 Announce Type: new 
Abstract: Although LLMs perform well in general tasks, domain-specific applications suffer from hallucinations and accuracy limitations. CPT approaches encounter two key issues: (1) domain-biased data degrades general language skills, and (2) improper corpus-mixture ratios limit effective adaptation. To address these, we propose a novel framework, Mixture of Losses (MoL), which decouples optimization objectives for domain-specific and general corpora. Specifically, cross-entropy (CE) loss is applied to domain data to ensure knowledge acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus training with the base model's foundational capabilities. This dual-loss architecture preserves universal skills while enhancing domain expertise, avoiding catastrophic forgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio optimally balances training and overfitting without the need for extensive tuning or resource-intensive experiments. Furthermore, our experiments demonstrate significant performance gains compared to traditional CPT approaches, which often suffer from degradation in general language capabilities; our model achieves 27.9% higher accuracy on the Math-500 benchmark in the non-think reasoning mode, and an impressive 83.3% improvement on the challenging AIME25 subset in the think mode, underscoring the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ABoN: Adaptive Best-of-N Alignment</title>
<link>https://arxiv.org/abs/2505.12050</link>
<guid>https://arxiv.org/abs/2505.12050</guid>
<content:encoded><![CDATA[
<div> Best-of-N sampling, reward models, test-time alignment, language models, prompt-adaptive strategy
Summary: 
This study introduces a prompt-adaptive strategy for Best-of-N alignment in language models (LMs) paired with reward models (RMs). The aim is to efficiently allocate compute resources during inference, particularly when dealing with varying prompt difficulty. The proposed two-stage algorithm first estimates reward distributions for prompts with a small exploration budget, then adaptively allocates remaining resources based on these estimates. Results on the AlpacaEval dataset demonstrate that the adaptive strategy consistently outperforms uniform allocation methods with the same inference budget across 12 LM/RM pairs and 50 prompt batches. The method also remains competitive when compared to uniform allocations with larger budgets, showing potential for improved performance as batch sizes increase. <br /><br />Summary: <div>
arXiv:2505.12050v1 Announce Type: new 
Abstract: Recent advances in test-time alignment methods, such as Best-of-N sampling, offer a simple and effective way to steer language models (LMs) toward preferred behaviors using reward models (RM). However, these approaches can be computationally expensive, especially when applied uniformly across prompts without accounting for differences in alignment difficulty. In this work, we propose a prompt-adaptive strategy for Best-of-N alignment that allocates inference-time compute more efficiently. Motivated by latency concerns, we develop a two-stage algorithm: an initial exploratory phase estimates the reward distribution for each prompt using a small exploration budget, and a second stage adaptively allocates the remaining budget using these estimates. Our method is simple, practical, and compatible with any LM/RM combination. Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different batches of prompts show that our adaptive strategy consistently outperforms the uniform allocation with the same inference budget. Moreover, our experiments show that our adaptive strategy remains competitive against uniform allocations with 20% larger inference budgets and even improves in performance as the batch size grows.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenderBench: Evaluation Suite for Gender Biases in LLMs</title>
<link>https://arxiv.org/abs/2505.12054</link>
<guid>https://arxiv.org/abs/2505.12054</guid>
<content:encoded><![CDATA[
<div> Evaluation, GenderBench, LLMs, gender biases, harmful behaviors <br />
Summary:
GenderBench is introduced as a tool to assess gender biases in Large Language Models (LLMs) through 14 probes that measure 19 harmful gender-related behaviors. It is shared as an open-source library to enhance benchmarking reproducibility and reliability. The study evaluates 12 LLMs and identifies consistent issues in their performance. LLMs struggle with stereotypical reasoning, lack equitable gender representation in generated texts, and exhibit discriminatory behavior in crucial scenarios like hiring. The evaluation underscores the necessity for continued research and development to address these issues. The insights provided by GenderBench contribute to the understanding of LLM behavior and highlight the need for improvements in addressing gender biases and promoting equity in language model outputs. The findings emphasize the importance of ethical considerations and accountability in the development and deployment of artificial intelligence technologies that impact societal perceptions and interactions. <br /><br />Summary: <div>
arXiv:2505.12054v1 Announce Type: new 
Abstract: We present GenderBench -- a comprehensive evaluation suite designed to measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19 gender-related harmful behaviors exhibited by LLMs. We release GenderBench as an open-source and extensible library to improve the reproducibility and robustness of benchmarking across the field. We also publish our evaluation of 12 LLMs. Our measurements reveal consistent patterns in their behavior. We show that LLMs struggle with stereotypical reasoning, equitable gender representation in generated texts, and occasionally also with discriminatory behavior in high-stakes scenarios, such as hiring.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement</title>
<link>https://arxiv.org/abs/2505.12060</link>
<guid>https://arxiv.org/abs/2505.12060</guid>
<content:encoded><![CDATA[
<div> Keyword: Large Language Models, Jailbreak Attacks, Safety Gap, SAGE, Defense Strategy

Summary: 

Large Language Models (LLMs) have shown impressive capabilities but are vulnerable to jailbreak attacks due to a safety gap in their response generation. A new defense strategy called SAGE (Self-Aware Guard Enhancement) addresses this issue by aligning LLMs' safety discrimination with generation abilities. SAGE consists of a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against jailbreak attempts. Extensive experiments across various LLMs demonstrate SAGE's effectiveness with a 99% defense success rate. Mechanistic interpretability analysis reveals the underlying detection-generation discrepancy. The research contributes to developing LLMs with coherent safety awareness and generation behavior. The code and datasets are publicly available for further research and development.

<br /><br />Summary: <div>
arXiv:2505.12060v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at https://github.com/NJUNLP/SAGE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach</title>
<link>https://arxiv.org/abs/2505.12071</link>
<guid>https://arxiv.org/abs/2505.12071</guid>
<content:encoded><![CDATA[
<div> cognitive-computational perspective, morphological productivity, discriminative lexicon model, Finnish nominal inflection, Thomas Mann <br />
Summary: This study examines morphological productivity from two perspectives: a cognitive-computational approach and a diachronic analysis focusing on writer Thomas Mann. Using the discriminative lexicon model, the study explores the systematicities between form and meaning in Finnish nominal inflection, Malay derivation, and English compounding. The model associates affix-like sublexical units with word embeddings to trace differences in productivity. Analyzing Thomas Mann's output and intake, the study finds a low rate of novel derived word production, with more novel words in his input than output. Mann's likelihood of producing novel derived words decreases with the distance of word embeddings from centroids. The study discusses the challenges of using speaker-specific embeddings for low-frequency and novel words. <br /> <div>
arXiv:2505.12071v1 Announce Type: new 
Abstract: In this study, we approach morphological productivity from two perspectives: a cognitive-computational perspective, and a diachronic perspective zooming in on an actual speaker, Thomas Mann. For developing the first perspective, we make use of a cognitive computational model of the mental lexicon, the discriminative lexicon model. For computational mappings between form and meaning to be productive, in the sense that novel, previously unencountered words, can be understood and produced, there must be systematicities between the form space and the semantic space. If the relation between form and meaning would be truly arbitrary, a model could memorize form and meaning pairings, but there is no way in which the model would be able to generalize to novel test data. For Finnish nominal inflection, Malay derivation, and English compounding, we explore, using the Discriminative Lexicon Model as a computational tool, to trace differences in the degree to which inflectional and word formation patterns are productive. We show that the DLM tends to associate affix-like sublexical units with the centroids of the embeddings of the words with a given affix. For developing the second perspective, we study how the intake and output of one prolific writer, Thomas Mann, changes over time. We show by means of an examination of what Thomas Mann is likely to have read, and what he wrote, that the rate at which Mann produces novel derived words is extremely low. There are far more novel words in his input than in his output. We show that Thomas Mann is less likely to produce a novel derived word with a given suffix the greater the average distance is of the embeddings of all derived words to the corresponding centroid, and discuss the challenges of using speaker-specific embeddings for low-frequency and novel words.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do different prompting methods yield a common task representation in language models?</title>
<link>https://arxiv.org/abs/2505.12075</link>
<guid>https://arxiv.org/abs/2505.12075</guid>
<content:encoded><![CDATA[
<div> vector, language models, in-context learning, task representation, instruction prompts

Summary:
- The study compares the use of demonstrations and instruction prompts to prompt language models for in-context learning tasks.
- New findings suggest that different task presentations elicit different mechanisms in language models.
- Function vectors are proposed as a way to extract task representations for instruction prompts, leading to improved zero-shot task accuracy.
- The study reveals that demonstration- and instruction-based function vectors rely on distinct model components.
- The results challenge the idea of a common task representation and highlight the need for further exploration of language model task inference mechanisms. 

<br /><br />Summary: <div>
arXiv:2505.12075v1 Announce Type: new 
Abstract: Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors, recently proposed as a mechanism to extract few-shot ICL task representations. We generalize function vectors to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task presentations do not induce a common task representation but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining textual instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Merging in Pre-training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.12082</link>
<guid>https://arxiv.org/abs/2505.12082</guid>
<content:encoded><![CDATA[
arXiv:2505.12082v1 Announce Type: new 
Abstract: Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Author Obfuscation with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12090</link>
<guid>https://arxiv.org/abs/2505.12090</guid>
<content:encoded><![CDATA[
arXiv:2505.12090v1 Announce Type: new 
Abstract: In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Fairness in LLMs Through Testing-Time Adversaries</title>
<link>https://arxiv.org/abs/2505.12100</link>
<guid>https://arxiv.org/abs/2505.12100</guid>
<content:encoded><![CDATA[
arXiv:2505.12100v1 Announce Type: new 
Abstract: Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings</title>
<link>https://arxiv.org/abs/2505.12116</link>
<guid>https://arxiv.org/abs/2505.12116</guid>
<content:encoded><![CDATA[
arXiv:2505.12116v1 Announce Type: new 
Abstract: Content moderation research has recently made significant advances, but still fails to serve the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments reveal that small, specialized multi-task models outperform the current frontier models in the low-resource setting, achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the resources publicly available to promote research on online safety.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The AI Gap: How Socioeconomic Status Affects Language Technology Interactions</title>
<link>https://arxiv.org/abs/2505.12158</link>
<guid>https://arxiv.org/abs/2505.12158</guid>
<content:encoded><![CDATA[
arXiv:2505.12158v1 Announce Type: new 
Abstract: Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse</title>
<link>https://arxiv.org/abs/2505.12160</link>
<guid>https://arxiv.org/abs/2505.12160</guid>
<content:encoded><![CDATA[
arXiv:2505.12160v1 Announce Type: new 
Abstract: Social media platforms like X (formerly Twitter) play a crucial role in shaping public discourse and societal norms. This study examines the term Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM) tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such as happiness, fear, anger, sadness, disgust, and surprise. By applying this model to large-scale X data, the study uncovers emotional nuances in Turkish discourse, contributing to computational social science by advancing sentiment analysis in underrepresented languages and enhancing our understanding of global digital discourse and the unique linguistic challenges of Turkish. The findings underscore the transformative potential of localized NLP tools, with our ERM model offering practical applications for real-time sentiment analysis in Turkish-language contexts. By addressing critical areas, including marketing, public relations, and crisis management, these models facilitate improved decision-making through timely and accurate sentiment tracking. This highlights the significance of advancing research that accounts for regional and linguistic nuances.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Truth Neurons</title>
<link>https://arxiv.org/abs/2505.12182</link>
<guid>https://arxiv.org/abs/2505.12182</guid>
<content:encoded><![CDATA[
arXiv:2505.12182v1 Announce Type: new 
Abstract: Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases</title>
<link>https://arxiv.org/abs/2505.12183</link>
<guid>https://arxiv.org/abs/2505.12183</guid>
<content:encoded><![CDATA[
arXiv:2505.12183v1 Announce Type: new 
Abstract: The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled</title>
<link>https://arxiv.org/abs/2505.12196</link>
<guid>https://arxiv.org/abs/2505.12196</guid>
<content:encoded><![CDATA[
arXiv:2505.12196v1 Announce Type: new 
Abstract: The impressive linguistic abilities of large language models (LLMs) have recommended them as models of human sentence processing, with some conjecturing a positive 'quality-power' relationship (Wilcox et al., 2023), in which language models' (LMs') fit to psychometric data continues to improve as their ability to predict words in context increases. This is important because it suggests that elements of LLM architecture, such as veridical attention to context and a unique objective of predicting upcoming words, reflect the architecture of the human sentence processing faculty, and that any inadequacies in predicting human reading time and brain imaging data may be attributed to insufficient model complexity, which recedes as larger models become available. Recent studies (Oh and Schuler, 2023) have shown this scaling inverts after a point, as LMs become excessively large and accurate, when word prediction probability (as information-theoretic surprisal) is used as a predictor. Other studies propose the use of entire vectors from differently sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting doubt on the value of surprisal as a predictor, but do not control for the larger number of predictors in vectors from larger LMs. This study evaluates LLM scaling using entire LLM vectors, while controlling for the larger number of predictors in vectors from larger LLMs. Results show that inverse scaling obtains, suggesting that inadequacies in predicting human reading time and brain imaging data may be due to substantial misalignment between LLMs and human sentence processing, which worsens as larger models are used.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Reliable is Multilingual LLM-as-a-Judge?</title>
<link>https://arxiv.org/abs/2505.12201</link>
<guid>https://arxiv.org/abs/2505.12201</guid>
<content:encoded><![CDATA[
arXiv:2505.12201v1 Announce Type: new 
Abstract: LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions. While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain. To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge. Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages. Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse. To investigate the cause of inconsistency, we analyze various influencing factors. We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages. Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency. These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions. We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning</title>
<link>https://arxiv.org/abs/2505.12212</link>
<guid>https://arxiv.org/abs/2505.12212</guid>
<content:encoded><![CDATA[
arXiv:2505.12212v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment</title>
<link>https://arxiv.org/abs/2505.12215</link>
<guid>https://arxiv.org/abs/2505.12215</guid>
<content:encoded><![CDATA[
arXiv:2505.12215v1 Announce Type: new 
Abstract: Large language models (LLMs) have achieved impressive performance in a variety of natural language processing (NLP) tasks. However, when applied to long-context scenarios, they face two challenges, i.e., low computational efficiency and much redundant information. This paper introduces GMSA, a context compression framework based on the encoder-decoder architecture, which addresses these challenges by reducing input sequence length and redundant information. Structurally, GMSA has two key components: Group Merging and Layer Semantic Alignment (LSA). Group merging is used to effectively and efficiently extract summary vectors from the original context. Layer semantic alignment, on the other hand, aligns the high-level summary vectors with the low-level primary input semantics, thus bridging the semantic gap between different layers. In the training process, GMSA first learns soft tokens that contain complete semantics through autoencoder training. To furtherly adapt GMSA to downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract knowledge from the soft tokens for downstream tasks. We train GMSA by randomly sampling the compression rate for each sample in the dataset. Under this condition, GMSA not only significantly outperforms the traditional compression paradigm in context restoration but also achieves stable and significantly faster convergence with only a few encoder layers. In downstream question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in end-to-end inference while outperforming both the original input prompts and various state-of-the-art (SOTA) methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models</title>
<link>https://arxiv.org/abs/2505.12216</link>
<guid>https://arxiv.org/abs/2505.12216</guid>
<content:encoded><![CDATA[
arXiv:2505.12216v1 Announce Type: new 
Abstract: Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user's compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers</title>
<link>https://arxiv.org/abs/2505.12218</link>
<guid>https://arxiv.org/abs/2505.12218</guid>
<content:encoded><![CDATA[
arXiv:2505.12218v1 Announce Type: new 
Abstract: Large Language Models (LLMs), such as ChatGPT, have prompted academic concerns about their impact on academic writing. Existing studies have primarily examined LLM usage in academic writing through quantitative approaches, such as word frequency statistics and probability-based analyses. However, few have systematically examined the potential impact of LLMs on the linguistic characteristics of academic writing. To address this gap, we conducted a large-scale analysis across 823,798 abstracts published in last decade from arXiv dataset. Through the linguistic analysis of features such as the frequency of LLM-preferred words, lexical complexity, syntactic complexity, cohesion, readability and sentiment, the results indicate a significant increase in the proportion of LLM-preferred words in abstracts, revealing the widespread influence of LLMs on academic writing. Additionally, we observed an increase in lexical complexity and sentiment in the abstracts, but a decrease in syntactic complexity, suggesting that LLMs introduce more new vocabulary and simplify sentence structure. However, the significant decrease in cohesion and readability indicates that abstracts have fewer connecting words and are becoming more difficult to read. Moreover, our analysis reveals that scholars with weaker English proficiency were more likely to use the LLMs for academic writing, and focused on improving the overall logic and fluency of the abstracts. Finally, at discipline level, we found that scholars in Computer Science showed more pronounced changes in writing style, while the changes in Mathematics were minimal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training</title>
<link>https://arxiv.org/abs/2505.12236</link>
<guid>https://arxiv.org/abs/2505.12236</guid>
<content:encoded><![CDATA[
arXiv:2505.12236v1 Announce Type: new 
Abstract: Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \footnote{The code and data are released on https://github.com/UESTC-GQJ/TKRE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs</title>
<link>https://arxiv.org/abs/2505.12238</link>
<guid>https://arxiv.org/abs/2505.12238</guid>
<content:encoded><![CDATA[
arXiv:2505.12238v1 Announce Type: new 
Abstract: The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce</title>
<link>https://arxiv.org/abs/2505.12244</link>
<guid>https://arxiv.org/abs/2505.12244</guid>
<content:encoded><![CDATA[
arXiv:2505.12244v1 Announce Type: new 
Abstract: Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing ''outlier tokens'' are easier to approximate; (3) target distributions generated by LMs -- even LMs with different tokenizers -- are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Documents Are What You Need for Extracting Instruction Tuning Data</title>
<link>https://arxiv.org/abs/2505.12250</link>
<guid>https://arxiv.org/abs/2505.12250</guid>
<content:encoded><![CDATA[
arXiv:2505.12250v1 Announce Type: new 
Abstract: Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches</title>
<link>https://arxiv.org/abs/2505.12259</link>
<guid>https://arxiv.org/abs/2505.12259</guid>
<content:encoded><![CDATA[
arXiv:2505.12259v1 Announce Type: new 
Abstract: Recent progress in large language models (LLMs) has outpaced the development of effective evaluation methods. Traditional benchmarks rely on task-specific metrics and static datasets, which often suffer from fairness issues, limited scalability, and contamination risks. In this paper, we introduce Teach2Eval, an indirect evaluation framework inspired by the Feynman Technique. Instead of directly testing LLMs on predefined tasks, our method evaluates a model's multiple abilities to teach weaker student models to perform tasks effectively. By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment. Our approach not only avoids data leakage and memorization but also captures a broad range of cognitive abilities that are orthogonal to current benchmarks. Experimental results across 26 leading LLMs show strong alignment with existing human and model-based dynamic rankings, while offering additional interpretability for training guidance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation</title>
<link>https://arxiv.org/abs/2505.12265</link>
<guid>https://arxiv.org/abs/2505.12265</guid>
<content:encoded><![CDATA[
arXiv:2505.12265v1 Announce Type: new 
Abstract: Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model's output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families & datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks</title>
<link>https://arxiv.org/abs/2505.12268</link>
<guid>https://arxiv.org/abs/2505.12268</guid>
<content:encoded><![CDATA[
arXiv:2505.12268v1 Announce Type: new 
Abstract: Understanding which neural components drive specific capabilities in mid-sized language models ($\leq$10B parameters) remains a key challenge. We introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads. Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark</title>
<link>https://arxiv.org/abs/2505.12273</link>
<guid>https://arxiv.org/abs/2505.12273</guid>
<content:encoded><![CDATA[
arXiv:2505.12273v1 Announce Type: new 
Abstract: Evaluating machine translation (MT) for low-resource languages poses a persistent challenge, primarily due to the limited availability of high quality reference translations. This issue is further exacerbated in languages with multiple dialects, where linguistic diversity and data scarcity hinder robust evaluation. Large Language Models (LLMs) present a promising solution through reference-free evaluation techniques; however, their effectiveness diminishes in the absence of dialect-specific context and tailored guidance. In this work, we propose a comprehensive framework that enhances LLM-based MT evaluation using a dialect guided approach. We extend the ONUBAD dataset by incorporating Sylheti-English sentence pairs, corresponding machine translations, and Direct Assessment (DA) scores annotated by native speakers. To address the vocabulary gap, we augment the tokenizer vocabulary with dialect-specific terms. We further introduce a regression head to enable scalar score prediction and design a dialect-guided (DG) prompting strategy. Our evaluation across multiple LLMs shows that the proposed pipeline consistently outperforms existing methods, achieving the highest gain of +0.1083 in Spearman correlation, along with improvements across other evaluation settings. The dataset and the code are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models</title>
<link>https://arxiv.org/abs/2505.12287</link>
<guid>https://arxiv.org/abs/2505.12287</guid>
<content:encoded><![CDATA[
arXiv:2505.12287v1 Announce Type: new 
Abstract: Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhance Mobile Agents Thinking Process Via Iterative Preference Learning</title>
<link>https://arxiv.org/abs/2505.12299</link>
<guid>https://arxiv.org/abs/2505.12299</guid>
<content:encoded><![CDATA[
arXiv:2505.12299v1 Announce Type: new 
Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\&amp;A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models</title>
<link>https://arxiv.org/abs/2505.12300</link>
<guid>https://arxiv.org/abs/2505.12300</guid>
<content:encoded><![CDATA[
arXiv:2505.12300v1 Announce Type: new 
Abstract: Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection</title>
<link>https://arxiv.org/abs/2505.12306</link>
<guid>https://arxiv.org/abs/2505.12306</guid>
<content:encoded><![CDATA[
arXiv:2505.12306v1 Announce Type: new 
Abstract: Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpertSteer: Intervening in LLMs through Expert Knowledge</title>
<link>https://arxiv.org/abs/2505.12313</link>
<guid>https://arxiv.org/abs/2505.12313</guid>
<content:encoded><![CDATA[
arXiv:2505.12313v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning</title>
<link>https://arxiv.org/abs/2505.12328</link>
<guid>https://arxiv.org/abs/2505.12328</guid>
<content:encoded><![CDATA[
arXiv:2505.12328v1 Announce Type: new 
Abstract: We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which evaluates large language models on producing fine-grained, controllable, and interpretable reasoning processes. Systems must extract all problem conditions, decompose a chain of thought into statement-evidence pairs, and verify the logical validity of each pair. Leveraging only the off-the-shelf Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that first enumerates all conditions and then guides the model to label, cite, and adjudicate every reasoning step. A lightweight post-processor based on regular expressions normalises spans and enforces the official JSON schema. Without fine-tuning, external retrieval, or ensembling, our method ranks 5th overall, achieving macro F1 scores on par with substantially more complex and resource-consuming pipelines. We conclude by analysing the strengths and limitations of our approach and outlining directions for future research in structural reasoning with LLMs. Our code is available at https://github.com/asdfo123/LLMSR-asdfo123.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2505.12345</link>
<guid>https://arxiv.org/abs/2505.12345</guid>
<content:encoded><![CDATA[
arXiv:2505.12345v1 Announce Type: new 
Abstract: Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds</title>
<link>https://arxiv.org/abs/2505.12349</link>
<guid>https://arxiv.org/abs/2505.12349</guid>
<content:encoded><![CDATA[
arXiv:2505.12349v1 Announce Type: new 
Abstract: Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement</title>
<link>https://arxiv.org/abs/2505.12368</link>
<guid>https://arxiv.org/abs/2505.12368</guid>
<content:encoded><![CDATA[
arXiv:2505.12368v1 Announce Type: new 
Abstract: Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling</title>
<link>https://arxiv.org/abs/2505.12381</link>
<guid>https://arxiv.org/abs/2505.12381</guid>
<content:encoded><![CDATA[
arXiv:2505.12381v1 Announce Type: new 
Abstract: Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SLOT: Sample-specific Language Model Optimization at Test-time</title>
<link>https://arxiv.org/abs/2505.12392</link>
<guid>https://arxiv.org/abs/2505.12392</guid>
<content:encoded><![CDATA[
arXiv:2505.12392v1 Announce Type: new 
Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Traversal Verification for Speculative Tree Decoding</title>
<link>https://arxiv.org/abs/2505.12398</link>
<guid>https://arxiv.org/abs/2505.12398</guid>
<content:encoded><![CDATA[
arXiv:2505.12398v1 Announce Type: new 
Abstract: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT</title>
<link>https://arxiv.org/abs/2505.12405</link>
<guid>https://arxiv.org/abs/2505.12405</guid>
<content:encoded><![CDATA[
arXiv:2505.12405v1 Announce Type: new 
Abstract: Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-R1: Region-based Reinforcement Learning for Table Understanding</title>
<link>https://arxiv.org/abs/2505.12415</link>
<guid>https://arxiv.org/abs/2505.12415</guid>
<content:encoded><![CDATA[
arXiv:2505.12415v1 Announce Type: new 
Abstract: Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PSC: Extending Context Window of Large Language Models via Phase Shift Calibration</title>
<link>https://arxiv.org/abs/2505.12423</link>
<guid>https://arxiv.org/abs/2505.12423</guid>
<content:encoded><![CDATA[
arXiv:2505.12423v1 Announce Type: new 
Abstract: Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at https://github.com/WNQzhu/PSC.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games</title>
<link>https://arxiv.org/abs/2505.12439</link>
<guid>https://arxiv.org/abs/2505.12439</guid>
<content:encoded><![CDATA[
arXiv:2505.12439v1 Announce Type: new 
Abstract: Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment</title>
<link>https://arxiv.org/abs/2505.12452</link>
<guid>https://arxiv.org/abs/2505.12452</guid>
<content:encoded><![CDATA[
arXiv:2505.12452v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual understanding, yet much of their internal knowledge remains latent, loosely structured, and difficult to access or evaluate. We propose self-questioning as a lightweight and scalable strategy to improve LLMs' understanding, particularly in domains where success depends on fine-grained semantic distinctions. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. The benchmark centers on a pairwise differentiation task: can a model distinguish between closely related but substantively different inventions? We show that prompting LLMs to generate and answer their own questions - targeting the background knowledge required for the task - significantly improves performance. These self-generated questions and answers activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve answers from external scientific texts further enhances performance, suggesting that model knowledge is compressed and lacks the full richness of the training data. We also find that chain-of-thought prompting and self-questioning converge, though self-questioning remains more effective for improving understanding of technical concepts. Notably, we uncover an asymmetry in prompting: smaller models often generate more fundamental, more open-ended, better-aligned questions for mid-sized models than large models with better understanding do, revealing a new strategy for cross-model collaboration. Altogether, our findings establish self-questioning as both a practical mechanism for automatically improving LLM comprehension, especially in domains with sparse and underrepresented knowledge, and a diagnostic probe of how internal and external knowledge are organized.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations</title>
<link>https://arxiv.org/abs/2505.12454</link>
<guid>https://arxiv.org/abs/2505.12454</guid>
<content:encoded><![CDATA[
arXiv:2505.12454v1 Announce Type: new 
Abstract: Distantly supervised named entity recognition (DS-NER) has emerged as a cheap and convenient alternative to traditional human annotation methods, enabling the automatic generation of training data by aligning text with external resources. Despite the many efforts in noise measurement methods, few works focus on the latent noise distribution between different distant annotation methods. In this work, we explore the effectiveness and robustness of DS-NER by two aspects: (1) distant annotation techniques, which encompasses both traditional rule-based methods and the innovative large language model supervision approach, and (2) noise assessment, for which we introduce a novel framework. This framework addresses the challenges by distinctly categorizing them into the unlabeled-entity problem (UEP) and the noisy-entity problem (NEP), subsequently providing specialized solutions for each. Our proposed method achieves significant improvements on eight real-world distant supervision datasets originating from three different data sources and involving four distinct annotation techniques, confirming its superiority over current state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization</title>
<link>https://arxiv.org/abs/2505.12474</link>
<guid>https://arxiv.org/abs/2505.12474</guid>
<content:encoded><![CDATA[
arXiv:2505.12474v1 Announce Type: new 
Abstract: In this work, we investigate the performance of LLMs on a new task that requires combining discussion with background knowledge for summarization. This aims to address the limitation of outside observer confusion in existing dialogue summarization systems due to their reliance solely on discussion information. To achieve this, we model the task output as background and opinion summaries and define two standardized summarization patterns. To support assessment, we introduce the first benchmark comprising high-quality samples consistently annotated by human experts and propose a novel hierarchical evaluation framework with fine-grained, interpretable metrics. We evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our findings reveal: (1) LLMs struggle with background summary retrieval, generation, and opinion summary integration. (2) Even top LLMs achieve less than 69% average performance across both patterns. (3) Current LLMs lack adequate self-evaluation and self-correction capabilities for this task.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering</title>
<link>https://arxiv.org/abs/2505.12476</link>
<guid>https://arxiv.org/abs/2505.12476</guid>
<content:encoded><![CDATA[
arXiv:2505.12476v1 Announce Type: new 
Abstract: Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\% and 7.0\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.12495</link>
<guid>https://arxiv.org/abs/2505.12495</guid>
<content:encoded><![CDATA[
arXiv:2505.12495v1 Announce Type: new 
Abstract: The increasing context length of modern language models has created a need for evaluating their ability to retrieve and process information across extensive documents. While existing benchmarks test long-context capabilities, they often lack a structured way to systematically vary question complexity. We introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a framework that (1) extracts QA pairs at multiple complexity levels (2) by leveraging structured representations of financial agreements (3) along three key dimensions -- multi-hop retrieval, set operations, and answer plurality -- enabling fine-grained assessment of model performance across controlled difficulty levels. Using this framework, we construct a dataset of 20,139 QA pairs (the largest number among the long-context benchmarks) and open-source a part of it. We evaluate 13 proprietary and open-source LLMs and observe that even the best-performing models are struggling with set-based comparisons and multi-hop logical inference. Our analysis reveals systematic failure modes tied to semantic misinterpretation and inability to handle implicit relations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection</title>
<link>https://arxiv.org/abs/2505.12507</link>
<guid>https://arxiv.org/abs/2505.12507</guid>
<content:encoded><![CDATA[
arXiv:2505.12507v1 Announce Type: new 
Abstract: The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine-generated texts (MGT) and human-generated texts (HGT), the explainability of these methods remains a significant gap. Traditional explainability techniques often fall short in capturing the complex word relationships that distinguish HGT from MGT. To address this limitation, we present LM$^2$otifs, a novel explainable framework for MGT detection. Inspired by probabilistic graphical models, we provide a theoretical rationale for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks to achieve both accurate detection and interpretability. The LM$^2$otifs pipeline operates in three key stages: first, it transforms text into graphs based on word co-occurrence to represent lexical dependencies; second, graph neural networks are used for prediction; and third, a post-hoc explainability method extracts interpretable motifs, offering multi-level explanations from individual words to sentence structures. Extensive experiments on multiple benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The empirical evaluation of the extracted explainable motifs confirms their effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis reveals distinct and visible linguistic fingerprints characteristic of MGT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design</title>
<link>https://arxiv.org/abs/2505.12511</link>
<guid>https://arxiv.org/abs/2505.12511</guid>
<content:encoded><![CDATA[
arXiv:2505.12511v1 Announce Type: new 
Abstract: Inverse Protein Folding (IPF) is a critical subtask in the field of protein design, aiming to engineer amino acid sequences capable of folding correctly into a specified three-dimensional (3D) conformation. Although substantial progress has been achieved in recent years, existing methods generally rely on either backbone coordinates or molecular surface features alone, which restricts their ability to fully capture the complex chemical and geometric constraints necessary for precise sequence prediction. To address this limitation, we present DS-ProGen, a dual-structure deep language model for functional protein design, which integrates both backbone geometry and surface-level representations. By incorporating backbone coordinates as well as surface chemical and geometric descriptors into a next-amino-acid prediction paradigm, DS-ProGen is able to generate functionally relevant and structurally stable sequences while satisfying both global and local conformational constraints. On the PRIDE dataset, DS-ProGen attains the current state-of-the-art recovery rate of 61.47%, demonstrating the synergistic advantage of multi-modal structural encoding in protein design. Furthermore, DS-ProGen excels in predicting interactions with a variety of biological partners, including ligands, ions, and RNA, confirming its robust functional retention capabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents</title>
<link>https://arxiv.org/abs/2505.12531</link>
<guid>https://arxiv.org/abs/2505.12531</guid>
<content:encoded><![CDATA[
arXiv:2505.12531v1 Announce Type: new 
Abstract: Large language models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is most effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparisons of emotional-support LLMs in Clara Hill's established Exploration-Insight-Action counseling model, providing a structured and interpretable view of performance, and (ii) fully automates the evaluation pipeline at scale. ESC-Judge operates in three stages: first, it synthesizes realistic help-seeker roles by sampling empirically salient attributes such as stressors, personality, and life history; second, it has two candidate support agents conduct separate sessions with the same role, isolating model-specific strategies; and third, it asks a specialized judge LLM to express pairwise preferences across rubric-anchored skills that span the Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and 86 percent of Action decisions, demonstrating human-level reliability at a fraction of the cost. All code, prompts, synthetic roles, transcripts, and judgment scripts are released to promote transparent progress in emotionally supportive AI.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE</title>
<link>https://arxiv.org/abs/2505.12533</link>
<guid>https://arxiv.org/abs/2505.12533</guid>
<content:encoded><![CDATA[
arXiv:2505.12533v1 Announce Type: new 
Abstract: Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disambiguation in Conversational Question Answering in the Era of LLM: A Survey</title>
<link>https://arxiv.org/abs/2505.12543</link>
<guid>https://arxiv.org/abs/2505.12543</guid>
<content:encoded><![CDATA[
arXiv:2505.12543v1 Announce Type: new 
Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable language systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models</title>
<link>https://arxiv.org/abs/2505.12545</link>
<guid>https://arxiv.org/abs/2505.12545</guid>
<content:encoded><![CDATA[
arXiv:2505.12545v1 Announce Type: new 
Abstract: Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Extracting memorized pieces of (copyrighted) books from open-weight language models</title>
<link>https://arxiv.org/abs/2505.12546</link>
<guid>https://arxiv.org/abs/2505.12546</guid>
<content:encoded><![CDATA[
arXiv:2505.12546v1 Announce Type: new 
Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial ML and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we leverage a recent probabilistic extraction technique to extract pieces of the Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show that it's possible to extract substantial parts of at least some books from different LLMs. This is evidence that the LLMs have memorized the extracted text; this memorized content is copied inside the model parameters. But the results are complicated: the extent of memorization varies both by model and by book. With our specific experiments, we find that the largest LLMs don't memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost entirely. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations</title>
<link>https://arxiv.org/abs/2505.12560</link>
<guid>https://arxiv.org/abs/2505.12560</guid>
<content:encoded><![CDATA[
arXiv:2505.12560v1 Announce Type: new 
Abstract: Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large automatically tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains more than 1,800 sentences of pos-tagged parallel text data from over 1,500 languages, representing 133 language families and 111 isolates, dwarfing previously available resources. The accuracy of tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enriching Patent Claim Generation with European Patent Dataset</title>
<link>https://arxiv.org/abs/2505.12568</link>
<guid>https://arxiv.org/abs/2505.12568</guid>
<content:encoded><![CDATA[
arXiv:2505.12568v1 Announce Type: new 
Abstract: Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio</title>
<link>https://arxiv.org/abs/2505.12572</link>
<guid>https://arxiv.org/abs/2505.12572</guid>
<content:encoded><![CDATA[
arXiv:2505.12572v1 Announce Type: new 
Abstract: Writing novels with Large Language Models (LLMs) raises a critical question: how much human-authored outline is necessary to generate high-quality million-word novels? While frameworks such as DOME, Plan&amp;Write, and Long Writer have improved stylistic coherence and logical consistency, they primarily target shorter novels (10k--100k words), leaving ultra-long generation largely unexplored. Drawing on insights from recent text compression methods like LLMZip and LLM2Vec, we conduct an information-theoretic analysis that quantifies distortion occurring when LLMs compress and reconstruct ultra-long novels under varying compression-expansion ratios. We introduce a hierarchical two-stage generation pipeline (outline -> detailed outline -> manuscript) and find an optimal outline length that balances information preservation with human effort. Through extensive experimentation with Chinese novels, we establish that a two-stage hierarchical outline approach significantly reduces semantic distortion compared to single-stage methods. Our findings provide empirically-grounded guidance for authors and researchers collaborating with LLMs to create million-word novels.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Multilingual Language Models by Aligning Representations through Steering</title>
<link>https://arxiv.org/abs/2505.12584</link>
<guid>https://arxiv.org/abs/2505.12584</guid>
<content:encoded><![CDATA[
arXiv:2505.12584v1 Announce Type: new 
Abstract: In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer's activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling</title>
<link>https://arxiv.org/abs/2505.12587</link>
<guid>https://arxiv.org/abs/2505.12587</guid>
<content:encoded><![CDATA[
arXiv:2505.12587v1 Announce Type: new 
Abstract: Code-mixed languages, characterized by frequent within-sentence language transitions, present structural challenges that standard language models fail to address. In this work, we propose CMLFormer, an enhanced multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, designed to model the linguistic and semantic dynamics of code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with switching point and translation annotations with multiple new objectives specifically aimed at capturing switching behavior, cross-lingual structure, and code-mixing complexity. Our experiments show that CMLFormer improves F1 score, precision, and accuracy over other approaches on the HASOC-2021 benchmark under select pre-training setups. Attention analyses further show that it can identify and attend to switching points, validating its sensitivity to code-mixed structure. These results demonstrate the effectiveness of CMLFormer's architecture and multi-task pre-training strategy for modeling code-mixed languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptPrism: A Linguistically-Inspired Taxonomy for Prompts</title>
<link>https://arxiv.org/abs/2505.12592</link>
<guid>https://arxiv.org/abs/2505.12592</guid>
<content:encoded><![CDATA[
arXiv:2505.12592v1 Announce Type: new 
Abstract: Prompts are the interface for eliciting the capabilities of large language models (LLMs). Understanding their structure and components is critical for analyzing LLM behavior and optimizing performance. However, the field lacks a comprehensive framework for systematic prompt analysis and understanding. We introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt analysis across three hierarchical levels: functional structure, semantic component, and syntactic pattern. We show the practical utility of PromptPrism by applying it to three applications: (1) a taxonomy-guided prompt refinement approach that automatically improves prompt quality and enhances model performance across a range of tasks; (2) a multi-dimensional dataset profiling method that extracts and aggregates structural, semantic, and syntactic characteristics from prompt datasets, enabling comprehensive analysis of prompt distributions and patterns; (3) a controlled experimental framework for prompt sensitivity analysis by quantifying the impact of semantic reordering and delimiter modifications on LLM performance. Our experimental results validate the effectiveness of our taxonomy across these applications, demonstrating that PromptPrism provides a foundation for refining, profiling, and analyzing prompts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.12594</link>
<guid>https://arxiv.org/abs/2505.12594</guid>
<content:encoded><![CDATA[
arXiv:2505.12594v1 Announce Type: new 
Abstract: Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2505.12616</link>
<guid>https://arxiv.org/abs/2505.12616</guid>
<content:encoded><![CDATA[
arXiv:2505.12616v1 Announce Type: new 
Abstract: This paper presents the Duluth approach to the SemEval-2025 Task 7 on Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a TF-IDF-based retrieval system with experimentation on vector dimensions and tokenization strategies. Our best-performing configuration used word-level tokenization with a vocabulary size of 15,000 features, achieving an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages. Our system showed stronger performance on higher-resource languages but still lagged significantly behind the top-ranked system, which achieved 0.96 average success@10. Our findings suggest that though advanced neural architectures are increasingly dominant in multilingual retrieval tasks, properly optimized traditional methods like TF-IDF remain competitive baselines, especially in limited compute resource scenarios.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Attribute: Improving the Performance of LLMs Attribution Systems</title>
<link>https://arxiv.org/abs/2505.12621</link>
<guid>https://arxiv.org/abs/2505.12621</guid>
<content:encoded><![CDATA[
arXiv:2505.12621v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly applied in various science domains, yet their broader adoption remains constrained by a critical challenge: the lack of trustworthy, verifiable outputs. Current LLMs often generate answers without reliable source attribution, or worse, with incorrect attributions, posing a barrier to their use in scientific and high-stakes settings, where traceability and accountability are non-negotiable. To be reliable, attribution systems need high accuracy and retrieve data with short lengths, i.e., attribute to a sentence within a document rather than a whole document. We propose a sentence-level pre-attribution step for Retrieve-Augmented Generation (RAG) systems that classify sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes. By separating sentences before attribution, a proper attribution method can be selected for the type of sentence, or the attribution can be skipped altogether. Our results indicate that classifiers are well-suited for this task. In this work, we propose a pre-attribution step to reduce the computational complexity of attribution, provide a clean version of the HAGRID dataset, and provide an end-to-end attribution system that works out of the box.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model</title>
<link>https://arxiv.org/abs/2505.12625</link>
<guid>https://arxiv.org/abs/2505.12625</guid>
<content:encoded><![CDATA[
arXiv:2505.12625v1 Announce Type: new 
Abstract: DeepSeek recently released R1, a high-performing large language model (LLM) optimized for reasoning tasks. Despite its efficient training pipeline, R1 achieves competitive performance, even surpassing leading reasoning models like OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1 refuses to answer certain prompts related to politically sensitive topics in China. While existing LLMs often implement safeguards to avoid generating harmful or offensive outputs, R1 represents a notable shift - exhibiting censorship-like behavior on politically charged queries. In this paper, we investigate this phenomenon by first introducing a large-scale set of heavily curated prompts that get censored by R1, covering a range of politically sensitive topics, but are not censored by other models. We then conduct a comprehensive analysis of R1's censorship patterns, examining their consistency, triggers, and variations across topics, prompt phrasing, and context. Beyond English-language queries, we explore censorship behavior in other languages. We also investigate the transferability of censorship to models distilled from the R1 language model. Finally, we propose techniques for bypassing or removing this censorship. Our findings reveal possible additional censorship integration likely shaped by design choices during training or alignment, raising concerns about transparency, bias, and governance in language model deployment.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing</title>
<link>https://arxiv.org/abs/2505.12636</link>
<guid>https://arxiv.org/abs/2505.12636</guid>
<content:encoded><![CDATA[
arXiv:2505.12636v1 Announce Type: new 
Abstract: Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of "superficial editing" to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals</title>
<link>https://arxiv.org/abs/2505.12654</link>
<guid>https://arxiv.org/abs/2505.12654</guid>
<content:encoded><![CDATA[
arXiv:2505.12654v1 Announce Type: new 
Abstract: This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10\% increase in F1-score on turn-taking and a 33\% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering</title>
<link>https://arxiv.org/abs/2505.12662</link>
<guid>https://arxiv.org/abs/2505.12662</guid>
<content:encoded><![CDATA[
arXiv:2505.12662v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shadow-FT: Tuning Instruct via Base</title>
<link>https://arxiv.org/abs/2505.12716</link>
<guid>https://arxiv.org/abs/2505.12716</guid>
<content:encoded><![CDATA[
arXiv:2505.12716v1 Announce Type: new 
Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{https://github.com/wutaiqiang/Shadow-FT}{Github}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving</title>
<link>https://arxiv.org/abs/2505.12717</link>
<guid>https://arxiv.org/abs/2505.12717</guid>
<content:encoded><![CDATA[
arXiv:2505.12717v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework</title>
<link>https://arxiv.org/abs/2505.12718</link>
<guid>https://arxiv.org/abs/2505.12718</guid>
<content:encoded><![CDATA[
arXiv:2505.12718v1 Announce Type: new 
Abstract: Recent advances in Generative Artificial Intelligence (GenAI) have transformed educational content creation, particularly in developing tutor training materials. However, biases embedded in AI-generated content--such as gender, racial, or national stereotypes--raise significant ethical and educational concerns. Despite the growing use of GenAI, systematic methods for detecting and evaluating such biases in educational materials remain limited. This study proposes an automated bias assessment approach that integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework. We applied this method to AI-generated texts used in tutor training lessons. Results show a high alignment between the automated and manually curated word sets, with a Pearson correlation coefficient of r = 0.993, indicating reliable and consistent bias assessment. Our method reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing GenAI-produced educational content.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding</title>
<link>https://arxiv.org/abs/2505.12723</link>
<guid>https://arxiv.org/abs/2505.12723</guid>
<content:encoded><![CDATA[
arXiv:2505.12723v1 Announce Type: new 
Abstract: Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma</title>
<link>https://arxiv.org/abs/2505.12727</link>
<guid>https://arxiv.org/abs/2505.12727</guid>
<content:encoded><![CDATA[
arXiv:2505.12727v1 Announce Type: new 
Abstract: Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL</title>
<link>https://arxiv.org/abs/2505.12768</link>
<guid>https://arxiv.org/abs/2505.12768</guid>
<content:encoded><![CDATA[
arXiv:2505.12768v1 Announce Type: new 
Abstract: In Text-to-SQL, execution feedback is essential for guiding large language models (LLMs) to reason accurately and generate reliable SQL queries. However, existing methods treat execution feedback solely as a post-hoc signal for correction or selection, failing to integrate it into the generation process. This limitation hinders their ability to address reasoning errors as they occur, ultimately reducing query accuracy and robustness. To address this issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement Learning), a framework for Text-to-SQL that enables models to interact with the database during decoding and dynamically adjust their reasoning based on execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm that interleaves intermediate SQL execution into reasoning paths, facilitating context-sensitive revisions. It achieves this through structured prompts with markup tags and a stepwise rollout strategy that integrates execution feedback into each stage of generation. To supervise policy learning, we develop a composite reward function that includes an exploration reward, explicitly encouraging effective database interaction. Additionally, ReEx-SQL adopts a tree-based decoding strategy to support exploratory reasoning, enabling dynamic expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving 85.2% on Spider-Realistic with leading performance. In addition, its tree-structured decoding improves efficiency and performance over linear decoding, reducing inference time by 51.9% on the BIRD development set.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone</title>
<link>https://arxiv.org/abs/2505.12781</link>
<guid>https://arxiv.org/abs/2505.12781</guid>
<content:encoded><![CDATA[
arXiv:2505.12781v1 Announce Type: new 
Abstract: Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs</title>
<link>https://arxiv.org/abs/2505.12792</link>
<guid>https://arxiv.org/abs/2505.12792</guid>
<content:encoded><![CDATA[
arXiv:2505.12792v1 Announce Type: new 
Abstract: The rapid evolution of large language models (LLMs) has revolutionized various fields, including the identification and discovery of human values within text data. While traditional NLP models, such as BERT, have been employed for this task, their ability to represent textual data is significantly outperformed by emerging LLMs like GPTs. However, the performance of online LLMs often degrades when handling long contexts required for value identification, which also incurs substantial computational costs. To address these challenges, we propose EAVIT, an efficient and accurate framework for human value identification that combines the strengths of both locally fine-tunable and online black-box LLMs. Our framework employs a value detector - a small, local language model - to generate initial value estimations. These estimations are then used to construct concise input prompts for online LLMs, enabling accurate final value identification. To train the value detector, we introduce explanation-based training and data generation techniques specifically tailored for value identification, alongside sampling strategies to optimize the brevity of LLM input prompts. Our approach effectively reduces the number of input tokens by up to 1/6 compared to directly querying online LLMs, while consistently outperforming traditional NLP methods and other LLM-based strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</title>
<link>https://arxiv.org/abs/2505.12808</link>
<guid>https://arxiv.org/abs/2505.12808</guid>
<content:encoded><![CDATA[
arXiv:2505.12808v1 Announce Type: new 
Abstract: The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on https://github.com/maitrix-org/de-arena.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs</title>
<link>https://arxiv.org/abs/2505.12814</link>
<guid>https://arxiv.org/abs/2505.12814</guid>
<content:encoded><![CDATA[
arXiv:2505.12814v1 Announce Type: new 
Abstract: Existing LLM-based role-playing methods often rely on superficial textual descriptions or simplistic metrics, inadequately modeling both intrinsic and extrinsic character dimensions. Additionally, they typically simulate character memory with implicit model knowledge or basic retrieval augment generation without explicit memory alignment, compromising memory consistency. The two issues weaken reliability of role-playing LLMs in several applications, such as trustworthy social simulation. To address these limitations, we propose PsyMem, a novel framework integrating fine-grained psychological attributes and explicit memory control for role-playing. PsyMem supplements textual descriptions with 26 psychological indicators to detailed model character. Additionally, PsyMem implements memory alignment training, explicitly trains the model to align character's response with memory, thereby enabling dynamic memory-controlled responding during inference. By training Qwen2.5-7B-Instruct on our specially designed dataset (including 5,414 characters and 38,962 dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen, outperforms baseline models in role-playing, achieving the best performance in human-likeness and character fidelity.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models</title>
<link>https://arxiv.org/abs/2505.12821</link>
<guid>https://arxiv.org/abs/2505.12821</guid>
<content:encoded><![CDATA[
arXiv:2505.12821v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering</title>
<link>https://arxiv.org/abs/2505.12831</link>
<guid>https://arxiv.org/abs/2505.12831</guid>
<content:encoded><![CDATA[
arXiv:2505.12831v1 Announce Type: new 
Abstract: Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at https://github.com/zifengcheng/CP.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.12835</link>
<guid>https://arxiv.org/abs/2505.12835</guid>
<content:encoded><![CDATA[
arXiv:2505.12835v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting</title>
<link>https://arxiv.org/abs/2505.12837</link>
<guid>https://arxiv.org/abs/2505.12837</guid>
<content:encoded><![CDATA[
arXiv:2505.12837v1 Announce Type: new 
Abstract: Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-identification of De-identified Documents with Autoregressive Infilling</title>
<link>https://arxiv.org/abs/2505.12859</link>
<guid>https://arxiv.org/abs/2505.12859</guid>
<content:encoded><![CDATA[
arXiv:2505.12859v1 Announce Type: new 
Abstract: Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LEXam: Benchmarking Legal Reasoning on 340 Law Exams</title>
<link>https://arxiv.org/abs/2505.12864</link>
<guid>https://arxiv.org/abs/2505.12864</guid>
<content:encoded><![CDATA[
arXiv:2505.12864v1 Announce Type: new 
Abstract: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation</title>
<link>https://arxiv.org/abs/2505.12888</link>
<guid>https://arxiv.org/abs/2505.12888</guid>
<content:encoded><![CDATA[
arXiv:2505.12888v1 Announce Type: new 
Abstract: Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted \textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Thinking-Language Modeling Gap in Large Language Models</title>
<link>https://arxiv.org/abs/2505.12896</link>
<guid>https://arxiv.org/abs/2505.12896</guid>
<content:encoded><![CDATA[
arXiv:2505.12896v1 Announce Type: new 
Abstract: System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyFCG: Fluid Construction Grammar in Python</title>
<link>https://arxiv.org/abs/2505.12920</link>
<guid>https://arxiv.org/abs/2505.12920</guid>
<content:encoded><![CDATA[
arXiv:2505.12920v1 Announce Type: new 
Abstract: We present PyFCG, an open source software library that ports Fluid Construction Grammar (FCG) to the Python programming language. PyFCG enables its users to seamlessly integrate FCG functionality into Python programs, and to use FCG in combination with other libraries within Python's rich ecosystem. Apart from a general description of the library, this paper provides three walkthrough tutorials that demonstrate example usage of PyFCG in typical use cases of FCG: (i) formalising and testing construction grammar analyses, (ii) learning usage-based construction grammars from corpora, and (iii) implementing agent-based experiments on emergent communication.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs</title>
<link>https://arxiv.org/abs/2505.12929</link>
<guid>https://arxiv.org/abs/2505.12929</guid>
<content:encoded><![CDATA[
arXiv:2505.12929v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&amp;K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A3 : an Analytical Low-Rank Approximation Framework for Attention</title>
<link>https://arxiv.org/abs/2505.12942</link>
<guid>https://arxiv.org/abs/2505.12942</guid>
<content:encoded><![CDATA[
arXiv:2505.12942v1 Announce Type: new 
Abstract: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Morphological Tagging for Nguni Languages</title>
<link>https://arxiv.org/abs/2505.12949</link>
<guid>https://arxiv.org/abs/2505.12949</guid>
<content:encoded><![CDATA[
arXiv:2505.12949v1 Announce Type: new 
Abstract: Morphological parsing is the task of decomposing words into morphemes, the smallest units of meaning in a language, and labelling their grammatical roles. It is a particularly challenging task for agglutinative languages, such as the Nguni languages of South Africa, which construct words by concatenating multiple morphemes. A morphological parsing system can be framed as a pipeline with two separate components, a segmenter followed by a tagger. This paper investigates the use of neural methods to build morphological taggers for the four Nguni languages. We compare two classes of approaches: training neural sequence labellers (LSTMs and neural CRFs) from scratch and finetuning pretrained language models. We compare performance across these two categories, as well as to a traditional rule-based morphological parser. Neural taggers comfortably outperform the rule-based baseline and models trained from scratch tend to outperform pretrained models. We also compare parsing results across different upstream segmenters and with varying linguistic input features. Our findings confirm the viability of employing neural taggers based on pre-existing morphological segmenters for the Nguni languages.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuRE:Generative Query REwriter for Legal Passage Retrieval</title>
<link>https://arxiv.org/abs/2505.12950</link>
<guid>https://arxiv.org/abs/2505.12950</guid>
<content:encoded><![CDATA[
arXiv:2505.12950v1 Announce Type: new 
Abstract: Legal Passage Retrieval (LPR) systems are crucial as they help practitioners save time when drafting legal arguments. However, it remains an underexplored avenue. One primary reason is the significant vocabulary mismatch between the query and the target passage. To address this, we propose a simple yet effective method, the Generative query REwriter (GuRE). We leverage the generative capabilities of Large Language Models (LLMs) by training the LLM for query rewriting. "Rewritten queries" help retrievers to retrieve target passages by mitigating vocabulary mismatch. Experimental results show that GuRE significantly improves performance in a retriever-agnostic manner, outperforming all baseline methods. Further analysis reveals that different training objectives lead to distinct retrieval behaviors, making GuRE more suitable than direct retriever fine-tuning for real-world applications. Codes are avaiable at github.com/daehuikim/GuRE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition</title>
<link>https://arxiv.org/abs/2505.12964</link>
<guid>https://arxiv.org/abs/2505.12964</guid>
<content:encoded><![CDATA[
arXiv:2505.12964v1 Announce Type: new 
Abstract: Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language models (LLMs)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at https://github.com/sl-633/macoir-master.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down</title>
<link>https://arxiv.org/abs/2505.12969</link>
<guid>https://arxiv.org/abs/2505.12969</guid>
<content:encoded><![CDATA[
arXiv:2505.12969v1 Announce Type: new 
Abstract: OpenAI's Whisper has achieved significant success in Automatic Speech Recognition. However, it has consistently been found to exhibit hallucination issues, particularly in non-speech segments, which limits its broader application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination on non-speech segments without using any pre- or post-possessing techniques. Specifically, we benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder to the hallucination problem by performing a head-wise mask. Our findings reveal that only 3 of the 20 heads account for over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune these three crazy heads using a collection of non-speech data. The results show that our best fine-tuned model, namely Calm-Whisper, achieves over 80% reduction in non-speech hallucination with only less than 0.1% WER degradation on LibriSpeech test-clean and test-other.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Structured Literature Review on Traditional Approaches in Current Natural Language Processing</title>
<link>https://arxiv.org/abs/2505.12970</link>
<guid>https://arxiv.org/abs/2505.12970</guid>
<content:encoded><![CDATA[
arXiv:2505.12970v1 Announce Type: new 
Abstract: The continued rise of neural networks and large language models in the more recent past has altered the natural language processing landscape, enabling new approaches towards typical language tasks and achieving mainstream success. Despite the huge success of large language models, many disadvantages still remain and through this work we assess the state of the art in five application scenarios with a particular focus on the future perspectives and sensible application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios classification, information and relation extraction, text simplification as well as text summarization. After defining our terminology, i.e., which features are characteristic for traditional techniques in our interpretation for the five scenarios, we survey if such traditional approaches are still being used, and if so, in what way they are used. It turns out that all five application scenarios still exhibit traditional models in one way or another, as part of a processing pipeline, as a comparison/baseline to the core model of the respective paper, or as the main model(s) of the paper. For the complete statistics, see https://zenodo.org/records/13683801
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models</title>
<link>https://arxiv.org/abs/2505.12973</link>
<guid>https://arxiv.org/abs/2505.12973</guid>
<content:encoded><![CDATA[
arXiv:2505.12973v1 Announce Type: new 
Abstract: Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Study of Many-to-Many Summarization with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12983</link>
<guid>https://arxiv.org/abs/2505.12983</guid>
<content:encoded><![CDATA[
arXiv:2505.12983v1 Announce Type: new 
Abstract: Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.12996</link>
<guid>https://arxiv.org/abs/2505.12996</guid>
<content:encoded><![CDATA[
arXiv:2505.12996v1 Announce Type: new 
Abstract: In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code</title>
<link>https://arxiv.org/abs/2505.13004</link>
<guid>https://arxiv.org/abs/2505.13004</guid>
<content:encoded><![CDATA[
arXiv:2505.13004v1 Announce Type: new 
Abstract: Existing code generation benchmarks primarily evaluate functional correctness, with limited focus on code efficiency and often restricted to a single language like Python. To address this gap, we introduce EffiBench-X, the first multi-language benchmark designed to measure the efficiency of LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby, and Golang. It comprises competitive programming tasks with human-expert solutions as efficiency baselines. Evaluating state-of-the-art LLMs on EffiBench-X reveals that while models generate functionally correct code, they consistently underperform human experts in efficiency. Even the most efficient LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human efficiency on average, with significant language-specific variations. LLMs show better efficiency in Python, Ruby, and JavaScript than in Java, C++, and Golang. For instance, DeepSeek-R1's Python code is significantly more efficient than its Java code. These results highlight the critical need for research into LLM optimization techniques to improve code efficiency across diverse languages. The dataset and evaluation infrastructure are submitted and available at https://github.com/EffiBench/EffiBench-X.git and https://huggingface.co/datasets/EffiBench/effibench-x.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain</title>
<link>https://arxiv.org/abs/2505.13006</link>
<guid>https://arxiv.org/abs/2505.13006</guid>
<content:encoded><![CDATA[
arXiv:2505.13006v1 Announce Type: new 
Abstract: Airports from the top 20 in terms of annual passengers are highly dynamic environments with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Bias or Not to Bias: Detecting bias in News with bias-detector</title>
<link>https://arxiv.org/abs/2505.13010</link>
<guid>https://arxiv.org/abs/2505.13010</guid>
<content:encoded><![CDATA[
arXiv:2505.13010v1 Announce Type: new 
Abstract: Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation</title>
<link>https://arxiv.org/abs/2505.13034</link>
<guid>https://arxiv.org/abs/2505.13034</guid>
<content:encoded><![CDATA[
arXiv:2505.13034v1 Announce Type: new 
Abstract: Topic models are statistical tools that allow their users to gain qualitative and quantitative insights into the contents of textual corpora without the need for close reading. They can be applied in a wide range of settings from discourse analysis, through pretraining data curation, to text filtering. Topic models are typically parameter-rich, complex models, and interpreting these parameters can be challenging for their users. It is typical practice for users to interpret topics based on the top 10 highest ranking terms on a given topic. This list-of-words approach, however, gives users a limited and biased picture of the content of topics. Thoughtful user interface design and visualizations can help users gain a more complete and accurate understanding of topic models' output. While some visualization utilities do exist for topic models, these are typically limited to a certain type of topic model. We introduce topicwizard, a framework for model-agnostic topic model interpretation, that provides intuitive and interactive tools that help users examine the complex semantic relations between documents, words and topics learned by topic models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025</title>
<link>https://arxiv.org/abs/2505.13036</link>
<guid>https://arxiv.org/abs/2505.13036</guid>
<content:encoded><![CDATA[
arXiv:2505.13036v1 Announce Type: new 
Abstract: The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation</title>
<link>https://arxiv.org/abs/2505.13053</link>
<guid>https://arxiv.org/abs/2505.13053</guid>
<content:encoded><![CDATA[
arXiv:2505.13053v1 Announce Type: new 
Abstract: Adapting to the addressee is crucial for successful explanations, yet poses significant challenges for dialogsystems. We adopt the approach of treating explanation generation as a non-stationary decision process, where the optimal strategy varies according to changing beliefs about the explainee and the interaction context. In this paper we address the questions of (1) how to track the interaction context and the relevant listener features in a formally defined computational partner model, and (2) how to utilize this model in the dynamically adjusted, rational decision process that determines the currently best explanation strategy. We propose a Bayesian inference-based approach to continuously update the partner model based on user feedback, and a non-stationary Markov Decision Process to adjust decision-making based on the partner model values. We evaluate an implementation of this framework with five simulated interlocutors, demonstrating its effectiveness in adapting to different partners with constant and even changing feedback behavior. The results show high adaptivity with distinct explanation strategies emerging for different partners, highlighting the potential of our approach to improve explainable AI systems and dialogsystems in general.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset</title>
<link>https://arxiv.org/abs/2505.13069</link>
<guid>https://arxiv.org/abs/2505.13069</guid>
<content:encoded><![CDATA[
arXiv:2505.13069v1 Announce Type: new 
Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide risk assessment in adolescents. This study investigates a multimodal approach for this challenge, integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM. Additionally, handcrafted acoustic features -- including MFCCs, spectral contrast, and pitch-related statistics -- were incorporated. We explored three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization. Results show that weighted attention provided the best generalization, achieving 69% accuracy on the development set, though a performance gap between development and test sets highlights generalization challenges. Our findings, strictly tied to the MINI-KID framework, emphasize the importance of refining embedding representations and fusion mechanisms to enhance classification reliability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Sequential Numerical Prediction in Autoregressive Models</title>
<link>https://arxiv.org/abs/2505.13077</link>
<guid>https://arxiv.org/abs/2505.13077</guid>
<content:encoded><![CDATA[
arXiv:2505.13077v1 Announce Type: new 
Abstract: Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Systematic Generalization in Language Models Scales with Information Entropy</title>
<link>https://arxiv.org/abs/2505.13089</link>
<guid>https://arxiv.org/abs/2505.13089</guid>
<content:encoded><![CDATA[
arXiv:2505.13089v1 Announce Type: new 
Abstract: Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation</title>
<link>https://arxiv.org/abs/2505.13090</link>
<guid>https://arxiv.org/abs/2505.13090</guid>
<content:encoded><![CDATA[
arXiv:2505.13090v1 Announce Type: new 
Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning</title>
<link>https://arxiv.org/abs/2505.13115</link>
<guid>https://arxiv.org/abs/2505.13115</guid>
<content:encoded><![CDATA[
arXiv:2505.13115v1 Announce Type: new 
Abstract: The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ModernGBERT: German-only 1B Encoder Model Trained from Scratch</title>
<link>https://arxiv.org/abs/2505.13136</link>
<guid>https://arxiv.org/abs/2505.13136</guid>
<content:encoded><![CDATA[
arXiv:2505.13136v1 Announce Type: new 
Abstract: Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Cross-Lingual Inconsistency in Large Language Models</title>
<link>https://arxiv.org/abs/2505.13141</link>
<guid>https://arxiv.org/abs/2505.13141</guid>
<content:encoded><![CDATA[
arXiv:2505.13141v1 Announce Type: new 
Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize knowledge from one language to the others, we apply the logit lens to interpret the implicit steps taken by LLMs to solve multilingual multi-choice reasoning questions. We find LLMs predict inconsistently and are less accurate because they rely on subspaces of individual languages, rather than working in a shared semantic space. While larger models are more multilingual, we show their hidden states are more likely to dissociate from the shared representation compared to smaller models, but are nevertheless more capable of retrieving knowledge embedded across different languages. Finally, we demonstrate that knowledge sharing can be modulated by steering the models' latent processing towards the shared semantic space. We find reinforcing utilization of the shared space improves the models' multilingual reasoning performance, as a result of more knowledge transfer from, and better output consistency with English.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text</title>
<link>https://arxiv.org/abs/2505.13147</link>
<guid>https://arxiv.org/abs/2505.13147</guid>
<content:encoded><![CDATA[
arXiv:2505.13147v1 Announce Type: new 
Abstract: Can deception be detected solely from written text? Cues of deceptive communication are inherently subtle, even more so in text-only communication. Yet, prior studies have reported considerable success in automatic deception detection. We hypothesize that such findings are largely driven by artifacts introduced during data collection and do not generalize beyond specific datasets. We revisit this assumption by introducing a belief-based deception framework, which defines deception as a misalignment between an author's claims and true beliefs, irrespective of factual accuracy, allowing deception cues to be studied in isolation. Based on this framework, we construct three corpora, collectively referred to as DeFaBel, including a German-language corpus of deceptive and non-deceptive arguments and a multilingual version in German and English, each collected under varying conditions to account for belief change and enable cross-linguistic analysis. Using these corpora, we evaluate commonly reported linguistic cues of deception. Across all three DeFaBel variants, these cues show negligible, statistically insignificant correlations with deception labels, contrary to prior work that treats such cues as reliable indicators. We further benchmark against other English deception datasets following similar data collection protocols. While some show statistically significant correlations, effect sizes remain low and, critically, the set of predictive cues is inconsistent across datasets. We also evaluate deception detection using feature-based models, pretrained language models, and instruction-tuned large language models. While some models perform well on established deception datasets, they consistently perform near chance on DeFaBel. Our findings challenge the assumption that deception can be reliably inferred from linguistic cues and call for rethinking how deception is studied and modeled in NLP.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice</title>
<link>https://arxiv.org/abs/2505.13156</link>
<guid>https://arxiv.org/abs/2505.13156</guid>
<content:encoded><![CDATA[
arXiv:2505.13156v1 Announce Type: new 
Abstract: Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Role-Playing Evaluation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13157</link>
<guid>https://arxiv.org/abs/2505.13157</guid>
<content:encoded><![CDATA[
arXiv:2505.13157v1 Announce Type: new 
Abstract: Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks</title>
<link>https://arxiv.org/abs/2505.13171</link>
<guid>https://arxiv.org/abs/2505.13171</guid>
<content:encoded><![CDATA[
arXiv:2505.13171v1 Announce Type: new 
Abstract: Large language models are known to memorize parts of their training data, posing risk of copyright violations. To systematically examine this risk, we pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing web-scale data with public domain books used to simulate copyrighted content at controlled frequencies at lengths at least ten times longer than prior work. We thereby identified the offset effect, a phenomenon characterized by two key findings: (1) verbatim memorization is most strongly triggered by short prefixes drawn from the beginning of the context window, with memorization decreasing counterintuitively as prefix length increases; and (2) a sharp decline in verbatim recall when prefix begins offset from the initial tokens of the context window. We attribute this to positional fragility: models rely disproportionately on the earliest tokens in their context window as retrieval anchors, making them sensitive to even slight shifts. We further observe that when the model fails to retrieve memorized content, it often produces degenerated text. Leveraging these findings, we show that shifting sensitive data deeper into the context window suppresses both extractable memorization and degeneration. Our results suggest that positional offset is a critical and previously overlooked axis for evaluating memorization risks, since prior work implicitly assumed uniformity by probing only from the beginning of training sequences.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs</title>
<link>https://arxiv.org/abs/2505.13173</link>
<guid>https://arxiv.org/abs/2505.13173</guid>
<content:encoded><![CDATA[
arXiv:2505.13173v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models</title>
<link>https://arxiv.org/abs/2505.13176</link>
<guid>https://arxiv.org/abs/2505.13176</guid>
<content:encoded><![CDATA[
arXiv:2505.13176v1 Announce Type: new 
Abstract: While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space</title>
<link>https://arxiv.org/abs/2505.13181</link>
<guid>https://arxiv.org/abs/2505.13181</guid>
<content:encoded><![CDATA[
arXiv:2505.13181v1 Announce Type: new 
Abstract: We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification</title>
<link>https://arxiv.org/abs/2505.13204</link>
<guid>https://arxiv.org/abs/2505.13204</guid>
<content:encoded><![CDATA[
arXiv:2505.13204v1 Announce Type: new 
Abstract: Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled outputs of the target model. Existing methods mainly achieve draft-target alignment with training-based methods, e.g., EAGLE, Medusa, involving considerable training costs. In this paper, we present a training-free alignment-augmented speculative decoding algorithm. We propose alignment sampling, which leverages output distribution obtained in the prefilling phase to provide more aligned draft candidates. To further benefit from high-quality but non-aligned draft candidates, we also introduce a simple yet effective flexible verification strategy. Through an adaptive probability threshold, our approach can improve generation accuracy while further improving inference efficiency. Experiments on 8 datasets (including question answering, summarization and code completion tasks) show that our approach increases the average generation score by 3.3 points for the LLaMA3 model. Our method achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry</title>
<link>https://arxiv.org/abs/2505.13210</link>
<guid>https://arxiv.org/abs/2505.13210</guid>
<content:encoded><![CDATA[
arXiv:2505.13210v1 Announce Type: new 
Abstract: Classical Chinese poetry is a vital and enduring part of Chinese literature, conveying profound emotional resonance. Existing studies analyze sentiment based on textual meanings, overlooking the unique rhythmic and visual features inherent in poetry,especially since it is often recited and accompanied by Chinese paintings. In this work, we propose a dialect-enhanced multimodal framework for classical Chinese poetry sentiment analysis. We extract sentence-level audio features from the poetry and incorporate audio from multiple dialects,which may retain regional ancient Chinese phonetic features, enriching the phonetic representation. Additionally, we generate sentence-level visual features, and the multimodal features are fused with textual features enhanced by LLM translation through multimodal contrastive representation learning. Our framework outperforms state-of-the-art methods on two public datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro F1. We open-source the code to facilitate research in this area and provide insights for general multimodal Chinese representation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science</title>
<link>https://arxiv.org/abs/2505.13220</link>
<guid>https://arxiv.org/abs/2505.13220</guid>
<content:encoded><![CDATA[
arXiv:2505.13220v1 Announce Type: new 
Abstract: Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench -- the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models</title>
<link>https://arxiv.org/abs/2505.13244</link>
<guid>https://arxiv.org/abs/2505.13244</guid>
<content:encoded><![CDATA[
arXiv:2505.13244v1 Announce Type: new 
Abstract: With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stronger Together: Unleashing the Social Impact of Hate Speech Research</title>
<link>https://arxiv.org/abs/2505.13251</link>
<guid>https://arxiv.org/abs/2505.13251</guid>
<content:encoded><![CDATA[
arXiv:2505.13251v1 Announce Type: new 
Abstract: The advent of the internet has been both a blessing and a curse for once marginalised communities. When used well, the internet can be used to connect and establish communities crossing different intersections; however, it can also be used as a tool to alienate people and communities as well as perpetuate hate, misinformation, and disinformation especially on social media platforms. We propose steering hate speech research and researchers away from pre-existing computational solutions and consider social methods to inform social solutions to address this social problem. In a similar way linguistics research can inform language planning policy, linguists should apply what we know about language and society to mitigate some of the emergent risks and dangers of anti-social behaviour in digital spaces. We argue linguists and NLP researchers can play a principle role in unleashing the social impact potential of linguistics research working alongside communities, advocates, activists, and policymakers to enable equitable digital inclusion and to close the digital divide.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Planning via Coding and Inference Scaling</title>
<link>https://arxiv.org/abs/2505.13252</link>
<guid>https://arxiv.org/abs/2505.13252</guid>
<content:encoded><![CDATA[
arXiv:2505.13252v1 Announce Type: new 
Abstract: Real-life textual planning tasks such as meeting scheduling have posed much challenge to LLMs especially when the complexity is high. While previous work primarily studied auto-regressive generation of plans with closed-source models, we systematically evaluate both closed- and open-source models, including those that scales output length with complexity during inference, in generating programs, which are executed to output the plan. We consider not only standard Python code, but also the code to a constraint satisfaction problem solver. Despite the algorithmic nature of the task, we show that programming often but not always outperforms planning. Our detailed error analysis also indicates a lack of robustness and efficiency in the generated code that hinders generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.13254</link>
<guid>https://arxiv.org/abs/2505.13254</guid>
<content:encoded><![CDATA[
arXiv:2505.13254v1 Announce Type: new 
Abstract: Autoregressive decoding, the standard approach for Large Language Model (LLM) inference, remains a significant bottleneck due to its sequential nature. While speculative decoding algorithms mitigate this inefficiency through parallel verification, they fail to exploit the inherent heterogeneity in linguistic complexity, a key factor leading to suboptimal resource allocation. We address this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding framework that dynamically optimizes computational resource allocation based on linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying predictable contexts. (2) A dynamic resource allocation strategy based on data-driven entropy partitioning, enabling adaptive speculative expansion and pruning tailored to local context difficulty. Evaluated on five public benchmarks and four models, HeteroSpec achieves an average speedup of 4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across speedup rates, average acceptance length, and verification cost. Notably, HeteroSpec requires no draft model retraining, incurs minimal overhead, and is orthogonal to other acceleration techniques. It demonstrates enhanced acceleration with stronger draft models, establishing a new paradigm for context-aware LLM inference acceleration.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?</title>
<link>https://arxiv.org/abs/2505.13257</link>
<guid>https://arxiv.org/abs/2505.13257</guid>
<content:encoded><![CDATA[
arXiv:2505.13257v1 Announce Type: new 
Abstract: Preference alignment has become a standard pipeline in finetuning models to follow \emph{generic} human preferences. Majority of work seeks to optimize model to produce responses that would be preferable \emph{on average}, simplifying the diverse and often \emph{contradicting} space of human preferences. While research has increasingly focused on personalized alignment: adapting models to individual user preferences, there is a lack of personalized preference dataset which focus on nuanced individual-level preferences. To address this, we introduce WikiPersona: the first fine-grained personalization using well-documented, famous individuals. Our dataset challenges models to align with these personas through an interpretable process: generating verifiable textual descriptions of a persona's background and preferences in addition to alignment. We systematically evaluate different personalization approaches and find that as few-shot prompting with preferences and fine-tuning fail to simultaneously ensure effectiveness and efficiency, using \textit{inferred personal preferences} as prefixes enables effective personalization, especially in topics where preferences clash while leading to more equitable generalization across unseen personas.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability</title>
<link>https://arxiv.org/abs/2505.13258</link>
<guid>https://arxiv.org/abs/2505.13258</guid>
<content:encoded><![CDATA[
arXiv:2505.13258v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery</title>
<link>https://arxiv.org/abs/2505.13259</link>
<guid>https://arxiv.org/abs/2505.13259</guid>
<content:encoded><![CDATA[
arXiv:2505.13259v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representation of perceived prosodic similarity of conversational feedback</title>
<link>https://arxiv.org/abs/2505.13268</link>
<guid>https://arxiv.org/abs/2505.13268</guid>
<content:encoded><![CDATA[
arXiv:2505.13268v1 Announce Type: new 
Abstract: Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of spoken dialogue and is crucial to ensuring common ground in conversational systems. The exact meaning of such feedback is conveyed through both lexical and prosodic form. In this work, we investigate the perceived prosodic similarity of vocal feedback with the same lexical form, and to what extent existing speech representations reflect such similarities. A triadic comparison task with recruited participants is used to measure perceived similarity of feedback responses taken from two different datasets. We find that spectral and self-supervised speech representations encode prosody better than extracted pitch features, especially in the case of feedback from the same speaker. We also find that it is possible to further condense and align the representations to human perception through contrastive learning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13271</link>
<guid>https://arxiv.org/abs/2505.13271</guid>
<content:encoded><![CDATA[
arXiv:2505.13271v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at https://github.com/CycloneBoy/csc_sql.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion</title>
<link>https://arxiv.org/abs/2505.13282</link>
<guid>https://arxiv.org/abs/2505.13282</guid>
<content:encoded><![CDATA[
arXiv:2505.13282v1 Announce Type: new 
Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13302</link>
<guid>https://arxiv.org/abs/2505.13302</guid>
<content:encoded><![CDATA[
arXiv:2505.13302v1 Announce Type: new 
Abstract: Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMs' propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: https://github.com/3lis/misinfo_vlm
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.13307</link>
<guid>https://arxiv.org/abs/2505.13307</guid>
<content:encoded><![CDATA[
arXiv:2505.13307v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at https://github.com/LightChen233/reasoning-boundary.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection</title>
<link>https://arxiv.org/abs/2505.13312</link>
<guid>https://arxiv.org/abs/2505.13312</guid>
<content:encoded><![CDATA[
arXiv:2505.13312v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges</title>
<link>https://arxiv.org/abs/2505.13328</link>
<guid>https://arxiv.org/abs/2505.13328</guid>
<content:encoded><![CDATA[
arXiv:2505.13328v1 Announce Type: new 
Abstract: Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \textit{role-consistent response}: response generation and role play. Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation</title>
<link>https://arxiv.org/abs/2505.13338</link>
<guid>https://arxiv.org/abs/2505.13338</guid>
<content:encoded><![CDATA[
arXiv:2505.13338v1 Announce Type: new 
Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization</title>
<link>https://arxiv.org/abs/2505.13346</link>
<guid>https://arxiv.org/abs/2505.13346</guid>
<content:encoded><![CDATA[
arXiv:2505.13346v1 Announce Type: new 
Abstract: To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks</title>
<link>https://arxiv.org/abs/2505.13348</link>
<guid>https://arxiv.org/abs/2505.13348</guid>
<content:encoded><![CDATA[
arXiv:2505.13348v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning</title>
<link>https://arxiv.org/abs/2505.13353</link>
<guid>https://arxiv.org/abs/2505.13353</guid>
<content:encoded><![CDATA[
arXiv:2505.13353v1 Announce Type: new 
Abstract: Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts</title>
<link>https://arxiv.org/abs/2505.13360</link>
<guid>https://arxiv.org/abs/2505.13360</guid>
<content:encoded><![CDATA[
arXiv:2505.13360v1 Announce Type: new 
Abstract: Building LLM-powered software requires developers to communicate their requirements through natural language, but developer prompts are frequently underspecified, failing to fully capture many user-important requirements. In this paper, we present an in-depth analysis of prompt underspecification, showing that while LLMs can often (41.1%) guess unspecified requirements by default, such behavior is less robust: Underspecified prompts are 2x more likely to regress over model or prompt changes, sometimes with accuracy drops by more than 20%. We then demonstrate that simply adding more requirements to a prompt does not reliably improve performance, due to LLMs' limited instruction-following capabilities and competing constraints, and standard prompt optimizers do not offer much help. To address this, we introduce novel requirements-aware prompt optimization mechanisms that can improve performance by 4.8% on average over baselines that naively specify everything in the prompt. Beyond prompt optimization, we envision that effectively managing prompt underspecification requires a broader process, including proactive requirements discovery, evaluation, and monitoring.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinkless: LLM Learns When to Think</title>
<link>https://arxiv.org/abs/2505.13379</link>
<guid>https://arxiv.org/abs/2505.13379</guid>
<content:encoded><![CDATA[
arXiv:2505.13379v1 Announce Type: new 
Abstract: Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens,  for concise responses and  for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R3: Robust Rubric-Agnostic Reward Models</title>
<link>https://arxiv.org/abs/2505.13388</link>
<guid>https://arxiv.org/abs/2505.13388</guid>
<content:encoded><![CDATA[
arXiv:2505.13388v1 Announce Type: new 
Abstract: Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MR. Judge: Multimodal Reasoner as a Judge</title>
<link>https://arxiv.org/abs/2505.13403</link>
<guid>https://arxiv.org/abs/2505.13403</guid>
<content:encoded><![CDATA[
arXiv:2505.13403v1 Announce Type: new 
Abstract: The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Granary: Speech Recognition and Translation Dataset in 25 European Languages</title>
<link>https://arxiv.org/abs/2505.13404</link>
<guid>https://arxiv.org/abs/2505.13404</guid>
<content:encoded><![CDATA[
arXiv:2505.13404v1 Announce Type: new 
Abstract: Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at https://hf.co/datasets/nvidia/Granary
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaptThink: Reasoning Models Can Learn When to Think</title>
<link>https://arxiv.org/abs/2505.13417</link>
<guid>https://arxiv.org/abs/2505.13417</guid>
<content:encoded><![CDATA[
arXiv:2505.13417v1 Announce Type: new 
Abstract: Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness</title>
<link>https://arxiv.org/abs/2505.13418</link>
<guid>https://arxiv.org/abs/2505.13418</guid>
<content:encoded><![CDATA[
arXiv:2505.13418v1 Announce Type: new 
Abstract: Cognitive decline often surfaces in language years before diagnosis. It is frequently non-experts, such as those closest to the patient, who first sense a change and raise concern. As LLMs become integrated into daily communication and used over prolonged periods, it may even be an LLM that notices something is off. But what exactly do they notice--and should be noticing--when making that judgment? This paper investigates how dementia is perceived through language by non-experts. We presented transcribed picture descriptions to non-expert humans and LLMs, asking them to intuitively judge whether each text was produced by someone healthy or with dementia. We introduce an explainable method that uses LLMs to extract high-level, expert-guided features representing these picture descriptions, and use logistic regression to model human and LLM perceptions and compare with clinical diagnoses. Our analysis reveals that human perception of dementia is inconsistent and relies on a narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a richer, more nuanced feature set that aligns more closely with clinical patterns. Still, both groups show a tendency toward false negatives, frequently overlooking dementia cases. Through our interpretable framework and the insights it provides, we hope to help non-experts better recognize the linguistic signs that matter.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SMOTExT: SMOTE meets Large Language Models</title>
<link>https://arxiv.org/abs/2505.13434</link>
<guid>https://arxiv.org/abs/2505.13434</guid>
<content:encoded><![CDATA[
arXiv:2505.13434v1 Announce Type: new 
Abstract: Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.13444</link>
<guid>https://arxiv.org/abs/2505.13444</guid>
<content:encoded><![CDATA[
arXiv:2505.13444v1 Announce Type: new 
Abstract: Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIE: Controlling Language Model Text Generations Using Continuous Signals</title>
<link>https://arxiv.org/abs/2505.13448</link>
<guid>https://arxiv.org/abs/2505.13448</guid>
<content:encoded><![CDATA[
arXiv:2505.13448v1 Announce Type: new 
Abstract: Aligning language models with user intent is becoming increasingly relevant to enhance user experience. This calls for designing methods that can allow users to control the properties of the language that LMs generate. For example, controlling the length of the generation, the complexity of the language that gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate users' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale. In this work, we are interested in \textit{continuous} control signals, ones that exist along a spectrum that can't easily be captured in a natural language prompt or via existing techniques in conditional generation. Through a case study in controlling the precise response-length of generations produced by LMs, we demonstrate how after fine-tuning, behaviors of language models can be controlled via continuous signals -- as vectors that are interpolated between a "low" and a "high" token embedding. Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal. Our full open-sourced code and datasets are available at https://github.com/vsamuel2003/CIE.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TARGET: Benchmarking Table Retrieval for Generative Tasks</title>
<link>https://arxiv.org/abs/2505.11545</link>
<guid>https://arxiv.org/abs/2505.11545</guid>
<content:encoded><![CDATA[
arXiv:2505.11545v1 Announce Type: cross 
Abstract: The data landscape is rich with structured data, often of high value to organizations, driving important applications in data analysis and machine learning. Recent progress in representation learning and generative models for such data has led to the development of natural language interfaces to structured data, including those leveraging text-to-SQL. Contextualizing interactions, either through conversational interfaces or agentic components, in structured data through retrieval-augmented generation can provide substantial benefits in the form of freshness, accuracy, and comprehensiveness of answers. The key question is: how do we retrieve the right table(s) for the analytical query or task at hand? To this end, we introduce TARGET: a benchmark for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the retrieval performance of different retrievers in isolation, as well as their impact on downstream tasks. We find that dense embedding-based retrievers far outperform a BM25 baseline which is less effective than it is for retrieval over unstructured text. We also surface the sensitivity of retrievers across various metadata (e.g., missing table titles), and demonstrate a stark variation of retrieval performance across datasets and tasks. TARGET is available at https://target-benchmark.github.io.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems</title>
<link>https://arxiv.org/abs/2505.11572</link>
<guid>https://arxiv.org/abs/2505.11572</guid>
<content:encoded><![CDATA[
arXiv:2505.11572v1 Announce Type: cross 
Abstract: Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday applications, yet significant disparities in performance across diverse demographic groups persist. In this work, we introduce the ASR-FAIRBENCH leaderboard which is designed to assess both the accuracy and equity of ASR models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures diverse demographic characteristics, we employ a mixed-effects Poisson regression model to derive an overall fairness score. This score is integrated with traditional metrics like Word Error Rate (WER) to compute the Fairness Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our approach reveals significant performance disparities in SOTA ASR models across demographic groups and offers a benchmark to drive the development of more inclusive ASR technologies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO</title>
<link>https://arxiv.org/abs/2505.11595</link>
<guid>https://arxiv.org/abs/2505.11595</guid>
<content:encoded><![CDATA[
arXiv:2505.11595v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has demonstrated significant success in enhancing reasoning capabilities in large language models (LLMs). One of the most widely used RL methods is Group Relative Policy Optimization (GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls when all sampled responses in a group are incorrect -- referred to as an \emph{all-negative-sample} group -- as it fails to update the policy, hindering learning progress. The contributions of this paper are two-fold. First, we propose a simple yet effective framework that introduces response diversity within all-negative-sample groups in GRPO using AI feedback. We also provide a theoretical analysis, via a stylized model, showing how this diversification improves learning dynamics. Second, we empirically validate our approach, showing the improved performance across various model sizes (7B, 14B, 32B) in both offline and online learning settings with 10 benchmarks, including base and distilled variants. Our findings highlight that learning from all-negative-sample groups is not only feasible but beneficial, advancing recent insights from \citet{Xiong-2025-Minimalist}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing the Vulnerability of Large Language Models to Polysemantic Interventions</title>
<link>https://arxiv.org/abs/2505.11611</link>
<guid>https://arxiv.org/abs/2505.11611</guid>
<content:encoded><![CDATA[
arXiv:2505.11611v1 Announce Type: cross 
Abstract: Polysemanticity -- where individual neurons encode multiple unrelated features -- is a well-known characteristic of large neural networks and remains a central challenge in the interpretability of language models. At the same time, its implications for model safety are also poorly understood. Leveraging recent advances in sparse autoencoders, we investigate the polysemantic structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their vulnerability to targeted, covert interventions at the prompt, feature, token, and neuron levels. Our analysis reveals a consistent polysemantic topology shared across both models. Strikingly, we demonstrate that this structure can be exploited to mount effective interventions on two larger, black-box instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These findings suggest not only the generalizability of the interventions but also point to a stable and transferable polysemantic structure that could potentially persist across architectures and training regimes.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions</title>
<link>https://arxiv.org/abs/2505.11614</link>
<guid>https://arxiv.org/abs/2505.11614</guid>
<content:encoded><![CDATA[
arXiv:2505.11614v1 Announce Type: cross 
Abstract: A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents</title>
<link>https://arxiv.org/abs/2505.11717</link>
<guid>https://arxiv.org/abs/2505.11717</guid>
<content:encoded><![CDATA[
arXiv:2505.11717v1 Announce Type: cross 
Abstract: Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--referred to as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage, which can be implemented by modifying the webpage's source code. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models</title>
<link>https://arxiv.org/abs/2505.11731</link>
<guid>https://arxiv.org/abs/2505.11731</guid>
<content:encoded><![CDATA[
arXiv:2505.11731v1 Announce Type: cross 
Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs) during downstream adaptation have addressed key challenges of reliability and simplicity. However, existing Bayesian methods typically require multiple sampling iterations during inference, creating significant efficiency issues that limit practical deployment. In this paper, we investigate the possibility of eliminating the need for test-time sampling for LLM uncertainty estimation. Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned confidence into a non-Bayesian student LLM by minimizing the divergence between their predictive distributions. Unlike typical calibration methods, our distillation is carried out solely on the training dataset without the need of an additional validation dataset. This simple yet effective approach achieves N-times more efficient uncertainty estimation during testing, where N is the number of samples traditionally required by Bayesian LLMs. Our extensive experiments demonstrate that uncertainty estimation capabilities on training data can successfully generalize to unseen test data through our distillation technique, consistently producing results comparable to (or even better than) state-of-the-art Bayesian LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token-Level Uncertainty Estimation for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2505.11737</link>
<guid>https://arxiv.org/abs/2505.11737</guid>
<content:encoded><![CDATA[
arXiv:2505.11737v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities, their output quality remains inconsistent across various application scenarios, making it difficult to identify trustworthy responses, especially in complex tasks requiring multi-step reasoning. In this paper, we propose a token-level uncertainty estimation framework to enable LLMs to self-assess and self-improve their generation quality in mathematical reasoning. Specifically, we introduce low-rank random weight perturbation to LLM decoding, generating predictive distributions that we use to estimate token-level uncertainties. We then aggregate these uncertainties to reflect semantic uncertainty of the generated sequences. Experiments on mathematical reasoning datasets of varying difficulty demonstrate that our token-level uncertainty metrics strongly correlate with answer correctness and model robustness. Additionally, we explore using uncertainty to directly enhance the model's reasoning performance through multiple generations and the particle filtering algorithm. Our approach consistently outperforms existing uncertainty estimation methods, establishing effective uncertainty estimation as a valuable tool for both evaluating and improving reasoning generation in LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2505.11756</link>
<guid>https://arxiv.org/abs/2505.11756</guid>
<content:encoded><![CDATA[
arXiv:2505.11756v1 Announce Type: cross 
Abstract: It is assumed that sparse autoencoders (SAEs) decompose polysemantic activations into interpretable linear directions, as long as the activations are composed of sparse linear combinations of underlying features. However, we find that if an SAE is more narrow than the number of underlying "true features" on which it is trained, and there is correlation between features, the SAE will merge components of correlated features together, thus destroying monosemanticity. In LLM SAEs, these two conditions are almost certainly true. This phenomenon, which we call feature hedging, is caused by SAE reconstruction loss, and is more severe the narrower the SAE. In this work, we introduce the problem of feature hedging and study it both theoretically in toy models and empirically in SAEs trained on LLMs. We suspect that feature hedging may be one of the core reasons that SAEs consistently underperform supervised baselines. Finally, we use our understanding of feature hedging to propose an improved variant of matryoshka SAEs. Our work shows there remain fundamental issues with SAEs, but we are hopeful that that highlighting feature hedging will catalyze future advances that allow SAEs to achieve their full potential of interpreting LLMs at scale.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title>
<link>https://arxiv.org/abs/2505.11770</link>
<guid>https://arxiv.org/abs/2505.11770</guid>
<content:encoded><![CDATA[
arXiv:2505.11770v1 Announce Type: cross 
Abstract: Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VenusX: Unlocking Fine-Grained Functional Understanding of Proteins</title>
<link>https://arxiv.org/abs/2505.11812</link>
<guid>https://arxiv.org/abs/2505.11812</guid>
<content:encoded><![CDATA[
arXiv:2505.11812v1 Announce Type: cross 
Abstract: Deep learning models have driven significant progress in predicting protein function and interactions at the protein level. While these advancements have been invaluable for many biological applications such as enzyme engineering and function annotation, a more detailed perspective is essential for understanding protein functional mechanisms and evaluating the biological knowledge captured by models. To address this demand, we introduce VenusX, the first large-scale benchmark for fine-grained functional annotation and function-based protein pairing at the residue, fragment, and domain levels. VenusX comprises three major task categories across six types of annotations, including residue-level binary classification, fragment-level multi-class classification, and pairwise functional similarity scoring for identifying critical active sites, binding sites, conserved sites, motifs, domains, and epitopes. The benchmark features over 878,000 samples curated from major open-source databases such as InterPro, BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three sequence identity thresholds, our benchmark enables a comprehensive assessment of model performance on both in-distribution and out-of-distribution scenarios. For baseline evaluation, we assess a diverse set of popular and open-source models, including pre-trained protein language models, sequence-structure hybrids, structure-based methods, and alignment-based techniques. Their performance is reported across all benchmark datasets and evaluation settings using multiple metrics, offering a thorough comparison and a strong foundation for future research. Code and data are publicly available at https://github.com/ai4protein/VenusX.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs</title>
<link>https://arxiv.org/abs/2505.11842</link>
<guid>https://arxiv.org/abs/2505.11842</guid>
<content:encoded><![CDATA[
arXiv:2505.11842v1 Announce Type: cross 
Abstract: The increasing deployment of Large Vision-Language Models (LVLMs) raises safety concerns under potential malicious inputs. However, existing multimodal safety evaluations primarily focus on model vulnerabilities exposed by static image inputs, ignoring the temporal dynamics of video that may induce distinct safety risks. To bridge this gap, we introduce Video-SafetyBench, the first comprehensive benchmark designed to evaluate the safety of LVLMs under video-text attacks. It comprises 2,264 video-text pairs spanning 48 fine-grained unsafe categories, each pairing a synthesized video with either a harmful query, which contains explicit malice, or a benign query, which appears harmless but triggers harmful behavior when interpreted alongside the video. To generate semantically accurate videos for safety evaluation, we design a controllable pipeline that decomposes video semantics into subject images (what is shown) and motion text (how it moves), which jointly guide the synthesis of query-relevant videos. To effectively evaluate uncertain or borderline harmful outputs, we propose RJScore, a novel LLM-based metric that incorporates the confidence of judge models and human-aligned decision threshold calibration. Extensive experiments show that benign-query video composition achieves average attack success rates of 67.2%, revealing consistent vulnerabilities to video-induced attacks. We believe Video-SafetyBench will catalyze future research into video-based safety evaluation and defense strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity</title>
<link>https://arxiv.org/abs/2505.11861</link>
<guid>https://arxiv.org/abs/2505.11861</guid>
<content:encoded><![CDATA[
arXiv:2505.11861v1 Announce Type: cross 
Abstract: Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.11875</link>
<guid>https://arxiv.org/abs/2505.11875</guid>
<content:encoded><![CDATA[
arXiv:2505.11875v1 Announce Type: cross 
Abstract: The current focus of AI research is shifting from emphasizing model training towards enhancing evaluation quality, a transition that is crucial for driving further advancements in AI systems. Traditional evaluation methods typically rely on reward models assigning scalar preference scores to outputs. Although effective, such approaches lack interpretability, leaving users often uncertain about why a reward model rates a particular response as high or low. The advent of LLM-as-a-Judge provides a more scalable and interpretable method of supervision, offering insights into the decision-making process. Moreover, with the emergence of large reasoning models, which consume more tokens for deeper thinking and answer refinement, scaling test-time computation in the LLM-as-a-Judge paradigm presents an avenue for further boosting performance and providing more interpretability through reasoning traces. In this paper, we introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on reflection-enhanced datasets collected via rejection-sampling and subsequently trained using Reinforcement Learning (RL) with verifiable rewards. At inference time, we apply Simple Test-Time Scaling (STTS) strategies for additional performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$ surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally, we present three key findings: (1) Existing LLM-as-a-Judge does not inherently exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced datasets continues to demonstrate similarly weak scaling behavior. (3) Significant scaling trend emerges primarily during the RL phase, suggesting that effective STTS capability is acquired predominantly through RL training.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Introduction to Analytical Software Engineering Design Paradigm</title>
<link>https://arxiv.org/abs/2505.11979</link>
<guid>https://arxiv.org/abs/2505.11979</guid>
<content:encoded><![CDATA[
arXiv:2505.11979v1 Announce Type: cross 
Abstract: As modern software systems expand in scale and complexity, the challenges associated with their modeling and formulation grow increasingly intricate. Traditional approaches often fall short in effectively addressing these complexities, particularly in tasks such as design pattern detection for maintenance and assessment, as well as code refactoring for optimization and long-term sustainability. This growing inadequacy underscores the need for a paradigm shift in how such challenges are approached and resolved. This paper presents Analytical Software Engineering (ASE), a novel design paradigm aimed at balancing abstraction, tool accessibility, compatibility, and scalability. ASE enables effective modeling and resolution of complex software engineering problems. The paradigm is evaluated through two frameworks Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR), both developed in accordance with ASE principles. BSS offers a compact, language-agnostic representation of codebases to facilitate precise design pattern detection. ODR unifies artifact and solution representations to optimize code refactoring via heuristic algorithms while eliminating iterative computational overhead. By providing a structured approach to software design challenges, ASE lays the groundwork for future research in encoding and analyzing complex software metrics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research</title>
<link>https://arxiv.org/abs/2505.12039</link>
<guid>https://arxiv.org/abs/2505.12039</guid>
<content:encoded><![CDATA[
arXiv:2505.12039v1 Announce Type: cross 
Abstract: The Science of Science (SoS) explores the mechanisms underlying scientific discovery, and offers valuable insights for enhancing scientific efficiency and fostering innovation. Traditional approaches often rely on simplistic assumptions and basic statistical tools, such as linear regression and rule-based simulations, which struggle to capture the complexity and scale of modern research ecosystems. The advent of artificial intelligence (AI) presents a transformative opportunity for the next generation of SoS, enabling the automation of large-scale pattern discovery and uncovering insights previously unattainable. This paper offers a forward-looking perspective on the integration of Science of Science with AI for automated research pattern discovery and highlights key open challenges that could greatly benefit from AI. We outline the advantages of AI over traditional methods, discuss potential limitations, and propose pathways to overcome them. Additionally, we present a preliminary multi-agent system as an illustrative example to simulate research societies, showcasing AI's ability to replicate real-world research patterns and accelerate progress in Science of Science research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation &amp; Smoke-Tests for Continuous LLM Evaluation</title>
<link>https://arxiv.org/abs/2505.12058</link>
<guid>https://arxiv.org/abs/2505.12058</guid>
<content:encoded><![CDATA[
arXiv:2505.12058v1 Announce Type: cross 
Abstract: Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual smoke-test suite designed to give large-language-model (LLM) pipelines a unit-test style safety net dataset that runs in seconds with minimal cost. Born out of the tight feedback-loop demands building the Comet Opik prompt-optimization SDK, where waiting on heavyweight benchmarks breaks developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with a tiny synthetic-data generator pypi package built on provider-agnostic LiteLLM. The generator lets practitioners mint their own tiny packs in any language, domain, or difficulty, while ten ready-made packs already cover Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian, Spanish, and Turkish. Every dataset ships with Croissant metadata and plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so teams can drop deterministic micro-benchmarks directly into pull-request gates, prompt-engineering loops, and production dashboards without touching GPU budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet reliably flags prompt-template errors, tokenizer drift, and fine-tuning side-effects long before full-scale suites like MMLU or BIG-Bench would finish configuring. The entire framework is released to accelerate continuous, resource-efficient quality assurance across the generative-AI ecosystem.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents</title>
<link>https://arxiv.org/abs/2505.12065</link>
<guid>https://arxiv.org/abs/2505.12065</guid>
<content:encoded><![CDATA[
arXiv:2505.12065v1 Announce Type: cross 
Abstract: Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs</title>
<link>https://arxiv.org/abs/2505.12135</link>
<guid>https://arxiv.org/abs/2505.12135</guid>
<content:encoded><![CDATA[
arXiv:2505.12135v1 Announce Type: cross 
Abstract: Assessing the capacity of Large Language Models (LLMs) to plan and reason within the constraints of interactive environments is crucial for developing capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite designed specifically for this purpose. Built upon a textual adaptation of the procedurally generated BabyAI grid world, this suite evaluates LLMs on three fundamental aspects of grounded intelligence: (1) predicting the consequences of actions on the environment state ($\textbf{Predict}$ task), (2) generating sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$ task), and (3) decomposing high-level instructions into coherent subgoal sequences ($\textbf{Decompose}$ task). We detail the methodology for generating the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$, $\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information from an expert agent operating within the text-based environment. Furthermore, we provide a standardized evaluation harness and metrics, including environment interaction for validating generated plans, to facilitate reproducible assessment of diverse LLMs. Initial baseline results highlight the challenges posed by these grounded reasoning tasks. The benchmark suite, datasets, data generation code, and evaluation code are made publicly available ($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$, $\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective</title>
<link>https://arxiv.org/abs/2505.12185</link>
<guid>https://arxiv.org/abs/2505.12185</guid>
<content:encoded><![CDATA[
arXiv:2505.12185v1 Announce Type: cross 
Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is crucial for their effective use in software engineering. Current evaluations, however, predominantly measure the accuracy of generated code on static benchmarks, neglecting the critical aspect of model robustness during programming tasks. While adversarial attacks offer insights on model robustness, their effectiveness is limited and evaluation could be constrained. Current adversarial attack methods for robustness evaluation yield inconsistent results, struggling to provide a unified evaluation across different LLMs. We introduce EVALOOP, a novel assessment framework that evaluate the robustness from a self-consistency perspective, i.e., leveraging the natural duality inherent in popular software engineering tasks, e.g., code generation and code summarization. EVALOOP initiates a self-contained feedback loop: an LLM generates output (e.g., code) from an input (e.g., natural language specification), and then use the generated output as the input to produce a new output (e.g., summarizes that code into a new specification). EVALOOP repeats the process to assess the effectiveness of EVALOOP in each loop. This cyclical strategy intrinsically evaluates robustness without rely on any external attack setups, providing a unified metric to evaluate LLMs' robustness in programming. We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1 performance within ten loops. Intriguingly, robustness does not always align with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo, despite superior initial code generation compared to DeepSeek-V2, demonstrated lower robustness over repeated evaluation loop.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering</title>
<link>https://arxiv.org/abs/2505.12189</link>
<guid>https://arxiv.org/abs/2505.12189</guid>
<content:encoded><![CDATA[
arXiv:2505.12189v1 Announce Type: cross 
Abstract: Large language models (LLMs) frequently demonstrate reasoning limitations, often conflating content plausibility (i.e., material inference) with logical validity (i.e., formal inference). This can result in biased inferences, where plausible arguments are incorrectly deemed logically valid or vice versa. Mitigating this limitation is critical, as it undermines the trustworthiness and generalizability of LLMs in applications that demand rigorous logical consistency. This paper investigates the problem of mitigating content biases on formal reasoning through activation steering. Specifically, we curate a controlled syllogistic reasoning dataset to disentangle formal validity from content plausibility. After localising the layers responsible for formal and material inference, we investigate contrastive activation steering methods for test-time interventions. An extensive empirical analysis on different LLMs reveals that contrastive steering consistently supports linear control over content biases. However, we observe that a static approach is insufficient for improving all the tested models. We then leverage the possibility to control content effects by dynamically determining the value of the steering parameters via fine-grained conditional methods. We found that conditional steering is effective on unresponsive models, achieving up to 15% absolute improvement in formal reasoning accuracy with a newly introduced kNN-based method (K-CAST). Finally, additional experiments reveal that steering for content effects is robust to prompt variations, incurs minimal side effects on language modeling capabilities, and can partially generalize to out-of-distribution reasoning tasks. Practically, this paper demonstrates that activation-level interventions can offer a scalable strategy for enhancing the robustness of LLMs, contributing towards more systematic and unbiased formal reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling</title>
<link>https://arxiv.org/abs/2505.12225</link>
<guid>https://arxiv.org/abs/2505.12225</guid>
<content:encoded><![CDATA[
arXiv:2505.12225v1 Announce Type: cross 
Abstract: High-quality reward models are crucial for unlocking the reasoning potential of large language models (LLMs), with best-of-N voting demonstrating significant performance gains. However, current reward models, which typically operate on the textual output of LLMs, are computationally expensive and parameter-heavy, limiting their real-world applications. We introduce the Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly parameter-efficient approach that leverages the rich information embedded in LLM hidden states to address these issues. ELHSR systematically outperform baselines with less than 0.005% of the parameters of baselines, requiring only a few samples for training. ELHSR also achieves orders-of-magnitude efficiency improvement with significantly less time and fewer FLOPs per sample than baseline reward models. Moreover, ELHSR exhibits robust performance even when trained only on logits, extending its applicability to some closed-source LLMs. In addition, ELHSR can also be combined with traditional reward models to achieve additional performance gains.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference</title>
<link>https://arxiv.org/abs/2505.12260</link>
<guid>https://arxiv.org/abs/2505.12260</guid>
<content:encoded><![CDATA[
arXiv:2505.12260v1 Announce Type: cross 
Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode queries and documents into low-dimensional dense or high-dimensional sparse vectors. It retrieves documents relevant to search queries based on vector similarities. Documents are pre-encoded offline, while queries arrive in real-time, necessitating an efficient online query encoder. Although LLMs significantly enhance retrieval capabilities, serving deeply parameterized LLMs slows down query inference throughput and increases demands for online deployment resources. In this paper, we propose LightRetriever, a novel LLM-based hybrid retriever with extremely lightweight query encoders. Our method retains a full-sized LLM for document encoding, but reduces the workload of query encoding to no more than an embedding lookup. Compared to serving a full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for query inference with GPU acceleration, and even a 20x speedup without GPU. Experiments on large-scale retrieval benchmarks demonstrate that our method generalizes well across diverse retrieval tasks, retaining an average of 95% full-sized performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vague Knowledge: Evidence from Analyst Reports</title>
<link>https://arxiv.org/abs/2505.12269</link>
<guid>https://arxiv.org/abs/2505.12269</guid>
<content:encoded><![CDATA[
arXiv:2505.12269v1 Announce Type: cross 
Abstract: People in the real world often possess vague knowledge of future payoffs, for which quantification is not feasible or desirable. We argue that language, with differing ability to convey vague information, plays an important but less known-role in subjective expectations. Empirically, we find that in their reports, analysts include useful information in linguistic expressions but not numerical forecasts. Specifically, the textual tone of analyst reports has predictive power for forecast errors and subsequent revisions in numerical forecasts, and this relation becomes stronger when analyst's language is vaguer, when uncertainty is higher, and when analysts are busier. Overall, our theory and evidence suggest that some useful information is vaguely known and only communicated through language.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient RL Training for Reasoning Models via Length-Aware Optimization</title>
<link>https://arxiv.org/abs/2505.12284</link>
<guid>https://arxiv.org/abs/2505.12284</guid>
<content:encoded><![CDATA[
arXiv:2505.12284v1 Announce Type: cross 
Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated remarkable performance on reasoning tasks but often incur a long reasoning path with significant memory and time costs. Existing methods primarily aim to shorten reasoning paths by introducing additional training data and stages. In this paper, we propose three critical reward designs integrated directly into the reinforcement learning process of large reasoning models, which reduce the response length without extra training stages. Experiments on four settings show that our method significantly decreases response length while maintaining or even improving performance. Specifically, in a logic reasoning setting, we achieve a 40% reduction in response length averaged by steps alongside a 14% gain in performance. For math problems, we reduce response length averaged by steps by 33% while preserving performance.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2505.12301</link>
<guid>https://arxiv.org/abs/2505.12301</guid>
<content:encoded><![CDATA[
arXiv:2505.12301v1 Announce Type: cross 
Abstract: LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with empirical human distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?</title>
<link>https://arxiv.org/abs/2505.12307</link>
<guid>https://arxiv.org/abs/2505.12307</guid>
<content:encoded><![CDATA[
arXiv:2505.12307v1 Announce Type: cross 
Abstract: Recent advances in Large Multimodal Models (LMMs) have significantly improved their reasoning and Optical Character Recognition (OCR) capabilities. However, their performance on complex logical reasoning tasks involving text-rich images remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical reasoning abilities on text-rich images, while minimizing reliance on domain-specific knowledge (e.g., mathematics). We construct LogicOCR by curating a text corpus from the Chinese National Civil Servant Examination and develop a scalable, automated pipeline to convert it into multimodal samples. First, we design prompt templates to steer GPT-Image-1 to generate images with diverse backgrounds, interleaved text-illustration layouts, and varied fonts, ensuring contextual relevance and visual realism. Then, the generated images are manually verified, with low-quality examples discarded. We evaluate a range of representative open-source and proprietary LMMs under both Chain-of-Thought (CoT) and direct-answer settings. Our multi-dimensional analysis reveals key insights, such as the impact of test-time scaling, input modality differences, and sensitivity to visual-text orientation. Notably, LMMs still lag in multimodal reasoning compared to text-only inputs, indicating that they have not fully bridged visual reading with reasoning. We hope LogicOCR will serve as a valuable resource for advancing multimodal reasoning research. The dataset is available at https://github.com/MiliLab/LogicOCR.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v1 Announce Type: cross 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v1 Announce Type: cross 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks</title>
<link>https://arxiv.org/abs/2505.12371</link>
<guid>https://arxiv.org/abs/2505.12371</guid>
<content:encoded><![CDATA[
arXiv:2505.12371v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.12442</link>
<guid>https://arxiv.org/abs/2505.12442</guid>
<content:encoded><![CDATA[
arXiv:2505.12442v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection</title>
<link>https://arxiv.org/abs/2505.12457</link>
<guid>https://arxiv.org/abs/2505.12457</guid>
<content:encoded><![CDATA[
arXiv:2505.12457v1 Announce Type: cross 
Abstract: Scaling RL for LLMs is computationally expensive, largely due to multi-sampling for policy optimization and evaluation, making efficient data selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory, we hypothesize LLMs learn best from data within their potential comprehension zone. Addressing the limitation of conventional, computationally intensive multi-sampling methods for data assessment, we introduce UFO-RL. This novel framework uses a computationally efficient single-pass uncertainty estimation to identify informative data instances, achieving up to 185x faster data evaluation. UFO-RL leverages this metric to select data within the estimated ZPD for training. Experiments show that training with just 10% of data selected by UFO-RL yields performance comparable to or surpassing full-data training, reducing overall training time by up to 16x while enhancing stability and generalization. UFO-RL offers a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning on valuable data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model</title>
<link>https://arxiv.org/abs/2505.12565</link>
<guid>https://arxiv.org/abs/2505.12565</guid>
<content:encoded><![CDATA[
arXiv:2505.12565v1 Announce Type: cross 
Abstract: Despite their ability to understand chemical knowledge and accurately generate sequential representations, large language models (LLMs) remain limited in their capacity to propose novel molecules with drug-like properties. In addition, the molecules that LLMs propose can often be challenging to make in the lab. To more effectively enable the discovery of functional small molecules, LLMs need to learn a molecular language. However, LLMs are currently limited by encoding molecules from atoms. In this paper, we argue that just like tokenizing texts into (sub-)word tokens instead of characters, molecules should be decomposed and reassembled at the level of functional building blocks, i.e., parts of molecules that bring unique functions and serve as effective building blocks for real-world automated laboratory synthesis. This motivates us to propose mCLM, a modular Chemical-Language Model tokenizing molecules into building blocks and learning a bilingual language model of both natural language descriptions of functions and molecule building blocks. By reasoning on such functional building blocks, mCLM guarantees to generate efficiently synthesizable molecules thanks to recent progress in block-based chemistry, while also improving the functions of molecules in a principled manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of significantly improving 5 out of 6 chemical functions critical to determining drug potentials. More importantly, mCLM can reason on multiple functions and improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to greatly improve their shortcomings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Latent Computation in Transformers with Latent Tokens</title>
<link>https://arxiv.org/abs/2505.12629</link>
<guid>https://arxiv.org/abs/2505.12629</guid>
<content:encoded><![CDATA[
arXiv:2505.12629v1 Announce Type: cross 
Abstract: Augmenting large language models (LLMs) with auxiliary tokens has emerged as a promising strategy for enhancing model performance. In this work, we introduce a lightweight method termed latent tokens; these are dummy tokens that may be non-interpretable in natural language but steer the autoregressive decoding process of a Transformer-based LLM via the attention mechanism. The proposed latent tokens can be seamlessly integrated with a pre-trained Transformer, trained in a parameter-efficient manner, and applied flexibly at inference time, while adding minimal complexity overhead to the existing infrastructure of standard Transformers. We propose several hypotheses about the underlying mechanisms of latent tokens and design synthetic tasks accordingly to verify them. Numerical results confirm that the proposed method noticeably outperforms the baselines, particularly in the out-of-distribution generalization scenarios, highlighting its potential in improving the adaptability of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents</title>
<link>https://arxiv.org/abs/2505.12632</link>
<guid>https://arxiv.org/abs/2505.12632</guid>
<content:encoded><![CDATA[
arXiv:2505.12632v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing GUI visual agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models that include MONDAY in their pre-training phases demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving an average performance gain of 18.11%p on an unseen mobile OS platform. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework comprises robust OCR-based scene detection (95.04% F1score), near-perfect UI element detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities</title>
<link>https://arxiv.org/abs/2505.12680</link>
<guid>https://arxiv.org/abs/2505.12680</guid>
<content:encoded><![CDATA[
arXiv:2505.12680v1 Announce Type: cross 
Abstract: LLM-based formal proof assistants (e.g., in Lean) hold great promise for automating mathematical discovery. But beyond syntactic correctness, do these systems truly understand mathematical structure as humans do? We investigate this question through the lens of mathematical inequalities -- a fundamental tool across many domains. While modern provers can solve basic inequalities, we probe their ability to handle human-intuitive compositionality. We introduce Ineq-Comp, a benchmark built from elementary inequalities through systematic transformations, including variable duplication, algebraic rewriting, and multi-step composition. Although these problems remain easy for humans, we find that most provers -- including Goedel, STP, and Kimina-7B -- struggle significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly because it is trained to decompose the problems into sub-problems -- but still suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor for all models even when formal proofs of the constituent parts are provided in context, revealing that the source of weakness is indeed in compositional reasoning. Our results expose a persisting gap between the generalization behavior of current AI provers and human mathematical intuition.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bullying the Machine: How Personas Increase LLM Vulnerability</title>
<link>https://arxiv.org/abs/2505.12692</link>
<guid>https://arxiv.org/abs/2505.12692</guid>
<content:encoded><![CDATA[
arXiv:2505.12692v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly deployed in interactions where they are prompted to adopt personas. This paper investigates whether such persona conditioning affects model safety under bullying, an adversarial manipulation that applies psychological pressures in order to force the victim to comply to the attacker. We introduce a simulation framework in which an attacker LLM engages a victim LLM using psychologically grounded bullying tactics, while the victim adopts personas aligned with the Big Five personality traits. Experiments using multiple open-source LLMs and a wide range of adversarial goals reveal that certain persona configurations -- such as weakened agreeableness or conscientiousness -- significantly increase victim's susceptibility to unsafe outputs. Bullying tactics involving emotional or sarcastic manipulation, such as gaslighting and ridicule, are particularly effective. These findings suggest that persona-driven interaction introduces a novel vector for safety risks in LLMs and highlight the need for persona-aware safety evaluation and alignment strategies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization</title>
<link>https://arxiv.org/abs/2505.12763</link>
<guid>https://arxiv.org/abs/2505.12763</guid>
<content:encoded><![CDATA[
arXiv:2505.12763v1 Announce Type: cross 
Abstract: Reward models (RMs) play a crucial role in reinforcement learning from human feedback (RLHF), aligning model behavior with human preferences. However, existing benchmarks for reward models show a weak correlation with the performance of optimized policies, suggesting that they fail to accurately assess the true capabilities of RMs. To bridge this gap, we explore several evaluation designs through the lens of reward overoptimization\textemdash a phenomenon that captures both how well the reward model aligns with human preferences and the dynamics of the learning signal it provides to the policy. The results highlight three key findings on how to construct a reliable benchmark: (i) it is important to minimize differences between chosen and rejected responses beyond correctness, (ii) evaluating reward models requires multiple comparisons across a wide range of chosen and rejected responses, and (iii) given that reward models encounter responses with diverse representations, responses should be sourced from a variety of models. However, we also observe that a extremely high correlation with degree of overoptimization leads to comparatively lower correlation with certain downstream performance. Thus, when designing a benchmark, it is desirable to use the degree of overoptimization as a useful tool, rather than the end goal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents</title>
<link>https://arxiv.org/abs/2505.12842</link>
<guid>https://arxiv.org/abs/2505.12842</guid>
<content:encoded><![CDATA[
arXiv:2505.12842v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI Agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\% over the best-performing baseline. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?</title>
<link>https://arxiv.org/abs/2505.12871</link>
<guid>https://arxiv.org/abs/2505.12871</guid>
<content:encoded><![CDATA[
arXiv:2505.12871v1 Announce Type: cross 
Abstract: Low rank adaptation (LoRA) has emerged as a prominent technique for fine-tuning large language models (LLMs) thanks to its superb efficiency gains over previous methods. While extensive studies have examined the performance and structural properties of LoRA, its behavior upon training-time attacks remain underexplored, posing significant security risks. In this paper, we theoretically investigate the security implications of LoRA's low-rank structure during fine-tuning, in the context of its robustness against data poisoning and backdoor attacks. We propose an analytical framework that models LoRA's training dynamics, employs the neural tangent kernel to simplify the analysis of the training process, and applies information theory to establish connections between LoRA's low rank structure and its vulnerability against training-time attacks. Our analysis indicates that LoRA exhibits better robustness to backdoor attacks than full fine-tuning, while becomes more vulnerable to untargeted data poisoning due to its over-simplified information geometry. Extensive experimental evaluations have corroborated our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective</title>
<link>https://arxiv.org/abs/2505.12886</link>
<guid>https://arxiv.org/abs/2505.12886</guid>
<content:encoded><![CDATA[
arXiv:2505.12886v1 Announce Type: cross 
Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in multi-step reasoning tasks. However, alongside these successes, a more deceptive form of model error has emerged--Reasoning Hallucination--where logically coherent but factually incorrect reasoning traces lead to persuasive yet faulty conclusions. Unlike traditional hallucinations, these errors are embedded within structured reasoning, making them more difficult to detect and potentially more harmful. In this work, we investigate reasoning hallucinations from a mechanistic perspective. We propose the Reasoning Score, which quantifies the depth of reasoning by measuring the divergence between logits obtained from projecting late layers of LRMs to the vocabulary space, effectively distinguishing shallow pattern-matching from genuine deep reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA dataset and identify two key reasoning hallucination patterns: early-stage fluctuation in reasoning depth and incorrect backtracking to flawed prior steps. These insights motivate our Reasoning Hallucination Detection (RHD) framework, which achieves state-of-the-art performance across multiple domains. To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced reinforcement learning algorithm that incorporates step-level deep reasoning rewards via potential-based shaping. Our theoretical analysis establishes stronger generalization guarantees, and experiments demonstrate improved reasoning quality and reduced hallucination rates.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios</title>
<link>https://arxiv.org/abs/2505.12891</link>
<guid>https://arxiv.org/abs/2505.12891</guid>
<content:encoded><![CDATA[
arXiv:2505.12891v1 Announce Type: cross 
Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME .
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models</title>
<link>https://arxiv.org/abs/2505.12900</link>
<guid>https://arxiv.org/abs/2505.12900</guid>
<content:encoded><![CDATA[
arXiv:2505.12900v1 Announce Type: cross 
Abstract: Geospatial code generation is emerging as a key direction in the integration of artificial intelligence and geoscientific analysis. However, there remains a lack of standardized tools for automatic evaluation in this domain. To address this gap, we propose AutoGEEval, the first multimodal, unit-level automated evaluation framework for geospatial code generation tasks on the Google Earth Engine (GEE) platform powered by large language models (LLMs). Built upon the GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench) comprising 1325 test cases that span 26 GEE data types. The framework integrates both question generation and answer verification components to enable an end-to-end automated evaluation pipeline-from function invocation to execution validation. AutoGEEval supports multidimensional quantitative analysis of model outputs in terms of accuracy, resource consumption, execution efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including general-purpose, reasoning-augmented, code-centric, and geoscience-specialized models-revealing their performance characteristics and potential optimization pathways in GEE code generation. This work provides a unified protocol and foundational resource for the development and assessment of geospatial code generation models, advancing the frontier of automated natural language to domain-specific code translation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging LLM Inconsistency to Boost Pass@k Performance</title>
<link>https://arxiv.org/abs/2505.12938</link>
<guid>https://arxiv.org/abs/2505.12938</guid>
<content:encoded><![CDATA[
arXiv:2505.12938v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve impressive abilities in numerous domains, but exhibit inconsistent performance in response to minor input changes. Rather than view this as a drawback, in this paper we introduce a novel method for leveraging models' inconsistency to boost Pass@k performance. Specifically, we present a "Variator" agent that generates k variants of a given task and submits one candidate solution for each one. Our variant generation approach is applicable to a wide range of domains as it is task agnostic and compatible with free-form inputs. We demonstrate the efficacy of our agent theoretically using a probabilistic model of the inconsistency effect, and show empirically that it outperforms the baseline on the APPS dataset. Furthermore, we establish that inconsistency persists even in frontier reasoning models across coding and cybersecurity domains, suggesting our method is likely to remain relevant for future model generations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fractured Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.12992</link>
<guid>https://arxiv.org/abs/2505.12992</guid>
<content:encoded><![CDATA[
arXiv:2505.12992v1 Announce Type: cross 
Abstract: Inference-time scaling techniques have significantly bolstered the reasoning capabilities of large language models (LLMs) by harnessing additional computational effort at inference without retraining. Similarly, Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy by generating rich intermediate reasoning trajectories, but these approaches incur substantial token costs that impede their deployment in latency-sensitive settings. In this work, we first show that truncated CoT, which stops reasoning before completion and directly generates the final answer, often matches full CoT sampling while using dramatically fewer tokens. Building on this insight, we introduce Fractured Sampling, a unified inference-time strategy that interpolates between full CoT and solution-only sampling along three orthogonal axes: (1) the number of reasoning trajectories, (2) the number of final solutions per trajectory, and (3) the depth at which reasoning traces are truncated. Through extensive experiments on five diverse reasoning benchmarks and several model scales, we demonstrate that Fractured Sampling consistently achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling gains in Pass@k versus token budget. Our analysis reveals how to allocate computation across these dimensions to maximize performance, paving the way for more efficient and scalable LLM reasoning.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset</title>
<link>https://arxiv.org/abs/2505.13028</link>
<guid>https://arxiv.org/abs/2505.13028</guid>
<content:encoded><![CDATA[
arXiv:2505.13028v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into critical systems in industries like healthcare and finance. Users can often submit queries to LLM-enabled chatbots, some of which can enrich responses with information retrieved from internal databases storing sensitive data. This gives rise to a range of attacks in which a user submits a malicious query and the LLM-system outputs a response that creates harm to the owner, such as leaking internal data or creating legal liability by harming a third-party. While security tools are being developed to counter these threats, there is little formal evaluation of their effectiveness and usability. This study addresses this gap by conducting a thorough comparative analysis of LLM security tools. We identified 13 solutions (9 closed-source, 4 open-source), but only 7 were evaluated due to a lack of participation by proprietary model owners.To evaluate, we built a benchmark dataset of malicious prompts, and evaluate these tools performance against a baseline LLM model (ChatGPT-3.5-Turbo). Our results show that the baseline model has too many false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard emerged as the best overall tools showcasing the tradeoff between usability and performance. The study concluded with recommendations for greater transparency among closed source providers, improved context-aware detections, enhanced open-source engagement, increased user awareness, and the adoption of more representative performance metrics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix</title>
<link>https://arxiv.org/abs/2505.13032</link>
<guid>https://arxiv.org/abs/2505.13032</guid>
<content:encoded><![CDATA[
arXiv:2505.13032v1 Announce Type: cross 
Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary tasks. MMAR comprises 1,000 meticulously curated audio-question-answer triplets, collected from real-world internet videos and refined through iterative error corrections and quality checks to ensure high quality. Unlike existing benchmarks that are limited to specific domains of sound, music, or speech, MMAR extends them to a broad spectrum of real-world audio scenarios, including mixed-modality combinations of sound, music, and speech. Each question in MMAR is hierarchically categorized across four reasoning layers: Signal, Perception, Semantic, and Cultural, with additional sub-categories within each layer to reflect task diversity and complexity. To further foster research in this area, we annotate every question with a Chain-of-Thought (CoT) rationale to promote future advancements in audio reasoning. Each item in the benchmark demands multi-step deep reasoning beyond surface-level understanding. Moreover, a part of the questions requires graduate-level perceptual and domain-specific knowledge, elevating the benchmark's difficulty and depth. We evaluate MMAR using a broad set of models, including Large Audio-Language Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models (OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with audio caption inputs. The performance of these models on MMAR highlights the benchmark's challenging nature, and our analysis further reveals critical limitations of understanding and reasoning capabilities among current models. We hope MMAR will serve as a catalyst for future advances in this important but little-explored area.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs</title>
<link>https://arxiv.org/abs/2505.13098</link>
<guid>https://arxiv.org/abs/2505.13098</guid>
<content:encoded><![CDATA[
arXiv:2505.13098v1 Announce Type: cross 
Abstract: Current Large Language Models (LLMs) can assist developing program code beside many other things, but can they support working with Knowledge Graphs (KGs) as well? Which LLM is offering the best capabilities in the field of Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to determine without checking many answers manually? The LLM-KG-Bench framework in Version 3.0 is designed to answer these questions. It consists of an extensible set of tasks for automated evaluation of LLM answers and covers different aspects of working with semantic technologies. In this paper the LLM-KG-Bench framework is presented in Version 3 along with a dataset of prompts, answers and evaluations generated with it and several state-of-the-art LLMs. Significant enhancements have been made to the framework since its initial release, including an updated task API that offers greater flexibility in handling evaluation tasks, revised tasks, and extended support for various open models through the vllm library, among other improvements. A comprehensive dataset has been generated using more than 30 contemporary open and proprietary LLMs, enabling the creation of exemplary model cards that demonstrate the models' capabilities in working with RDF and SPARQL, as well as comparing their performance on Turtle and JSON-LD RDF serialization tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference</title>
<link>https://arxiv.org/abs/2505.13109</link>
<guid>https://arxiv.org/abs/2505.13109</guid>
<content:encoded><![CDATA[
arXiv:2505.13109v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Iterative Formalization and Planning in Partially Observable Environments</title>
<link>https://arxiv.org/abs/2505.13126</link>
<guid>https://arxiv.org/abs/2505.13126</guid>
<content:encoded><![CDATA[
arXiv:2505.13126v1 Announce Type: cross 
Abstract: In planning, using LLMs not to predict plans but to formalize an environment into the Planning Domain Definition Language (PDDL) has been shown to greatly improve performance and control. While most work focused on fully observable environments, we tackle the more realistic and challenging partially observable environments where existing methods are incapacitated by the lack of complete information. We propose PDDLego+, a framework to iteratively formalize, plan, grow, and refine PDDL representations in a zero-shot manner, without needing access to any existing trajectories. On two textual simulated environments, we show that PDDLego+ not only achieves superior performance, but also shows robustness against problem complexity. We also show that the domain knowledge captured after a successful trial is interpretable and benefits future tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Generation of Parameterised Quantum Circuits from Large Texts</title>
<link>https://arxiv.org/abs/2505.13208</link>
<guid>https://arxiv.org/abs/2505.13208</guid>
<content:encoded><![CDATA[
arXiv:2505.13208v1 Announce Type: cross 
Abstract: Quantum approaches to natural language processing (NLP) are redefining how linguistic information is represented and processed. While traditional hybrid quantum-classical models rely heavily on classical neural networks, recent advancements propose a novel framework, DisCoCirc, capable of directly encoding entire documents as parameterised quantum circuits (PQCs), besides enjoying some additional interpretability and compositionality benefits. Following these ideas, this paper introduces an efficient methodology for converting large-scale texts into quantum circuits using tree-like representations of pregroup diagrams. Exploiting the compositional parallels between language and quantum mechanics, grounded in symmetric monoidal categories, our approach enables faithful and efficient encoding of syntactic and discourse relationships in long and complex texts (up to 6410 words in our experiments) to quantum circuits. The developed system is provided to the community as part of the augmented open-source quantum NLP package lambeq Gen II.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis</title>
<link>https://arxiv.org/abs/2505.13227</link>
<guid>https://arxiv.org/abs/2505.13227</guid>
<content:encoded><![CDATA[
arXiv:2505.13227v1 Announce Type: cross 
Abstract: Graphical user interface (GUI) grounding, the ability to map natural language instructions to specific actions on graphical user interfaces, remains a critical bottleneck in computer use agent development. Current benchmarks oversimplify grounding tasks as short referring expressions, failing to capture the complexity of real-world interactions that require software commonsense, layout understanding, and fine-grained manipulation capabilities. To address these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising 564 finely annotated samples across diverse task types including text matching, element recognition, layout understanding, and precise manipulation. Additionally, we synthesize and release the largest computer use grounding dataset Jedi, which contains 4 million examples through multi-perspective decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its effectiveness by outperforming existing approaches on ScreenSpot-v2, ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved grounding with Jedi directly enhances agentic capabilities of general foundation models on complex computer tasks, improving from 5% to 27% on OSWorld. Through detailed ablation studies, we identify key factors contributing to grounding performance and verify that combining specialized data for different interface elements enables compositional generalization to novel interfaces. All benchmark, data, checkpoints, and code are open-sourced and available at https://osworld-grounding.github.io.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information</title>
<link>https://arxiv.org/abs/2505.13237</link>
<guid>https://arxiv.org/abs/2505.13237</guid>
<content:encoded><![CDATA[
arXiv:2505.13237v1 Announce Type: cross 
Abstract: Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space</title>
<link>https://arxiv.org/abs/2505.13308</link>
<guid>https://arxiv.org/abs/2505.13308</guid>
<content:encoded><![CDATA[
arXiv:2505.13308v1 Announce Type: cross 
Abstract: Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition</title>
<link>https://arxiv.org/abs/2505.13380</link>
<guid>https://arxiv.org/abs/2505.13380</guid>
<content:encoded><![CDATA[
arXiv:2505.13380v1 Announce Type: cross 
Abstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the model complexity beyond the mean of increasing the network's depth or width. However, we argue that effective SMoE training remains challenging because of the suboptimal routing process where experts that perform computation do not directly contribute to the routing process. In this work, we propose competition, a novel mechanism to route tokens to experts with the highest neural response. Theoretically, we show that the competition mechanism enjoys a better sample efficiency than the traditional softmax routing. Furthermore, we develop CompeteSMoE, a simple yet effective algorithm to train large language models by deploying a router to learn the competition policy, thus enjoying strong performances at a low training overhead. Our extensive empirical evaluations on both the visual instruction tuning and language pre-training tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE compared to state-of-the-art SMoE strategies. We have made the implementation available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an improved version of the previous study at arXiv:2402.02526
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar</title>
<link>https://arxiv.org/abs/2505.13393</link>
<guid>https://arxiv.org/abs/2505.13393</guid>
<content:encoded><![CDATA[
arXiv:2505.13393v1 Announce Type: cross 
Abstract: This article provides an overview of IG Parser, a software that facilitates qualitative content analysis of formal (e.g., legal) rules or informal (e.g., socio-normative) norms, and strategies (such as conventions) -- referred to as \emph{institutions} -- that govern social systems and operate configurally to describe \emph{institutional systems}. To this end, the IG Parser employs a distinctive syntax that ensures rigorous encoding of natural language, while automating the transformation into various formats that support the downstream analysis using diverse analytical techniques. The conceptual core of the IG Parser is an associated syntax, IG Script, that operationalizes the conceptual foundations of the Institutional Grammar, and more specifically Institutional Grammar 2.0, an analytical paradigm for institutional analysis. This article presents the IG Parser, including its conceptual foundations, syntactic specification of IG Script, alongside architectural principles. This introduction is augmented with selective illustrative examples that highlight the use and benefit associated with the tool.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Minimum Description Length Approach to Regularization in Neural Networks</title>
<link>https://arxiv.org/abs/2505.13398</link>
<guid>https://arxiv.org/abs/2505.13398</guid>
<content:encoded><![CDATA[
arXiv:2505.13398v1 Announce Type: cross 
Abstract: State-of-the-art neural networks can be trained to become remarkable solutions to many problems. But while these architectures can express symbolic, perfect solutions, trained models often arrive at approximations instead. We show that the choice of regularization method plays a crucial role: when trained on formal languages with standard regularization ($L_1$, $L_2$, or none), expressive architectures not only fail to converge to correct solutions but are actively pushed away from perfect initializations. In contrast, applying the Minimum Description Length (MDL) principle to balance model complexity with data fit provides a theoretically grounded regularization method. Using MDL, perfect solutions are selected over approximations, independently of the optimization algorithm. We propose that unlike existing regularization techniques, MDL introduces the appropriate inductive bias to effectively counteract overfitting and promote generalization.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process</title>
<link>https://arxiv.org/abs/2505.13408</link>
<guid>https://arxiv.org/abs/2505.13408</guid>
<content:encoded><![CDATA[
arXiv:2505.13408v1 Announce Type: cross 
Abstract: Recent Large Reasoning Models significantly improve the reasoning ability of Large Language Models by learning to reason, exhibiting the promising performance in solving complex tasks. LRMs solve tasks that require complex reasoning by explicitly generating reasoning trajectories together with answers. Nevertheless, judging the quality of such an output answer is not easy because only considering the correctness of the answer is not enough and the soundness of the reasoning trajectory part matters as well. Logically, if the soundness of the reasoning part is poor, even if the answer is correct, the confidence of the derived answer should be low. Existing methods did consider jointly assessing the overall output answer by taking into account the reasoning part, however, their capability is still not satisfactory as the causal relationship of the reasoning to the concluded answer cannot properly reflected. In this paper, inspired by classical mechanics, we present a novel approach towards establishing a CoT-Kinetics energy equation. Specifically, our CoT-Kinetics energy equation formulates the token state transformation process, which is regulated by LRM internal transformer layers, as like a particle kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy assigns a scalar score to evaluate specifically the soundness of the reasoning phase, telling how confident the derived answer could be given the evaluated reasoning. As such, the LRM's overall output quality can be accurately measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-tuning Quantized Neural Networks with Zeroth-order Optimization</title>
<link>https://arxiv.org/abs/2505.13430</link>
<guid>https://arxiv.org/abs/2505.13430</guid>
<content:encoded><![CDATA[
arXiv:2505.13430v1 Announce Type: cross 
Abstract: As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Anytime Reasoning via Budget Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2505.13438</link>
<guid>https://arxiv.org/abs/2505.13438</guid>
<content:encoded><![CDATA[
arXiv:2505.13438v1 Announce Type: cross 
Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards</title>
<link>https://arxiv.org/abs/2505.13445</link>
<guid>https://arxiv.org/abs/2505.13445</guid>
<content:encoded><![CDATA[
arXiv:2505.13445v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) show great promise in complex reasoning, with Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement strategy. However, a prevalent issue is ``superficial self-reflection'', where models fail to robustly verify their own outputs. We introduce RISE (Reinforcing Reasoning with Self-Verification), a novel online RL framework designed to tackle this. RISE explicitly and simultaneously trains an LLM to improve both its problem-solving and self-verification abilities within a single, integrated RL process. The core mechanism involves leveraging verifiable rewards from an outcome verifier to provide on-the-fly feedback for both solution generation and self-verification tasks. In each iteration, the model generates solutions, then critiques its own on-policy generated solutions, with both trajectories contributing to the policy update. Extensive experiments on diverse mathematical reasoning benchmarks show that RISE consistently improves model's problem-solving accuracy while concurrently fostering strong self-verification skills. Our analyses highlight the advantages of online verification and the benefits of increased verification compute. Additionally, RISE models exhibit more frequent and accurate self-verification behaviors during reasoning. These advantages reinforce RISE as a flexible and effective path towards developing more robust and self-aware reasoners.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Linguistic Models: Investigating LLMs' metalinguistic abilities</title>
<link>https://arxiv.org/abs/2305.00948</link>
<guid>https://arxiv.org/abs/2305.00948</guid>
<content:encoded><![CDATA[
arXiv:2305.00948v4 Announce Type: replace 
Abstract: The performance of large language models (LLMs) has recently improved to the point where models can perform well on many language tasks. We show here that--for the first time--the models can also generate valid metalinguistic analyses of language data. We outline a research program where the behavioral interpretability of LLMs on these tasks is tested via prompting. LLMs are trained primarily on text--as such, evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. We show that OpenAI's (2024) o1 vastly outperforms other models on tasks involving drawing syntactic trees and phonological generalization. We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics of Language Models: Part 1, Learning Hierarchical Language Structures</title>
<link>https://arxiv.org/abs/2305.13673</link>
<guid>https://arxiv.org/abs/2305.13673</guid>
<content:encoded><![CDATA[
arXiv:2305.13673v4 Announce Type: replace 
Abstract: Transformer-based language models are effective but complex, and understanding their inner workings and reasoning mechanisms is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models perform recursive language structure reasoning defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn and reason over CFG-defined hierarchies and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.
  This paper also presents several corollaries, including showing why absolute positional embeddings is inferior to relative and rotary embeddings; uniform attention alone is surprisingly effective (motivating our follow-up work on Canon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep structure reasoning on CFGs compared to autoregressive models (e.g., GPT); and injecting structural or syntactic noise into pretraining data markedly improves robustness to corrupted language prompts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models</title>
<link>https://arxiv.org/abs/2310.10378</link>
<guid>https://arxiv.org/abs/2310.10378</guid>
<content:encoded><![CDATA[
arXiv:2310.10378v5 Announce Type: replace 
Abstract: Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatically generating Riddles aiding Concept Attainment</title>
<link>https://arxiv.org/abs/2310.18290</link>
<guid>https://arxiv.org/abs/2310.18290</guid>
<content:encoded><![CDATA[
arXiv:2310.18290v2 Announce Type: replace 
Abstract: One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of riddles prove encouraging.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Sequence Transduction through Dynamic Compression</title>
<link>https://arxiv.org/abs/2402.01172</link>
<guid>https://arxiv.org/abs/2402.01172</guid>
<content:encoded><![CDATA[
arXiv:2402.01172v2 Announce Type: replace 
Abstract: We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can We Verify Step by Step for Incorrect Answer Detection?</title>
<link>https://arxiv.org/abs/2402.10528</link>
<guid>https://arxiv.org/abs/2402.10528</guid>
<content:encoded><![CDATA[
arXiv:2402.10528v4 Announce Type: replace 
Abstract: Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\%$ increase in the F1 score and $2.97\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning</title>
<link>https://arxiv.org/abs/2402.12692</link>
<guid>https://arxiv.org/abs/2402.12692</guid>
<content:encoded><![CDATA[
arXiv:2402.12692v5 Announce Type: replace 
Abstract: The application of formulas (e.g., physics formulas) is a fundamental ability of humans when solving numerical reasoning problems. Existing numerical reasoning datasets seldom explicitly indicate the formulas employed in reasoning, as their questions rely on implicit commonsense mathematical knowledge. In contrast, in this paper, we introduce FormulaReasoning, a new dataset specifically designed for formula-based numerical reasoning. Each of the 4,751 questions in our dataset requires numerical calculation with external physics formulas, making it a more challenging benchmark for evaluating large language models (LLMs). We offer normalized fine-grained annotations for the questions, available in English and Chinese, including formula structures, parameter names, symbols, numerical values, and units, derived from extensive manual effort with LLM assistance for guaranteed quality. We also provide a consolidated formula database to serve as an external knowledge base accompanying the dataset. We employ FormulaReasoning to evaluate LLMs with 7B to over 100B parameters, and explore retrieval-augmented generation with the formula database. Our evaluation also covers supervised methods that break down the reasoning process into formula generation, parameter extraction, and numerical calculation, as well as direct preference optimization methods based on derived preference data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance</title>
<link>https://arxiv.org/abs/2402.12819</link>
<guid>https://arxiv.org/abs/2402.12819</guid>
<content:encoded><![CDATA[
arXiv:2402.12819v3 Announce Type: replace 
Abstract: When solving NLP tasks with limited labelled data, researchers typically either use a general large language model without further update, or use a small number of labelled samples to tune a specialised smaller model. In this work, we answer an important question -- how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 8 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average $100$) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with fine-tuning on binary datasets requiring significantly more samples. When performance variance is taken into consideration, the number of required labels increases on average by $100 - 200\%$. Finally, larger models do not consistently lead to better performance and lower variance, with 4-bit quantisation having negligible impact.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets</title>
<link>https://arxiv.org/abs/2404.17874</link>
<guid>https://arxiv.org/abs/2404.17874</guid>
<content:encoded><![CDATA[
arXiv:2404.17874v2 Announce Type: replace 
Abstract: Perceptions of hate can vary greatly across cultural contexts. Hate speech (HS) datasets, however, have traditionally been developed by language. This hides potential cultural biases, as one language may be spoken in different countries home to different cultures. In this work, we evaluate cultural bias in HS datasets by leveraging two interrelated cultural proxies: language and geography. We conduct a systematic survey of HS datasets in eight languages and confirm past findings on their English-language bias, but also show that this bias has been steadily decreasing in the past few years. For three geographically-widespread languages -- English, Arabic and Spanish -- we then leverage geographical metadata from tweets to approximate geo-cultural contexts by pairing language and country information. We find that HS datasets for these languages exhibit a strong geo-cultural bias, largely overrepresenting a handful of countries (e.g., US and UK for English) relative to their prominence in both the broader social media population and the general population speaking these languages. Based on these findings, we formulate recommendations for the creation of future HS datasets.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparse Matrix in Large Language Model Fine-tuning</title>
<link>https://arxiv.org/abs/2405.15525</link>
<guid>https://arxiv.org/abs/2405.15525</guid>
<content:encoded><![CDATA[
arXiv:2405.15525v3 Announce Type: replace 
Abstract: LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OR-Bench: An Over-Refusal Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2405.20947</link>
<guid>https://arxiv.org/abs/2405.20947</guid>
<content:encoded><![CDATA[
arXiv:2405.20947v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2406.10785</link>
<guid>https://arxiv.org/abs/2406.10785</guid>
<content:encoded><![CDATA[
arXiv:2406.10785v2 Announce Type: replace 
Abstract: In this paper, we introduce \textbf{Share}d \textbf{Lo}w \textbf{R}ank \textbf{A}daptation (ShareLoRA), a Large Language Model (LLM) fine-tuning technique that balances parameter efficiency, adaptability, and robustness without compromising performance. By strategically sharing the low-rank weight matrices across different layers, ShareLoRA achieves 44\% to 96\% reduction in trainable parameters compared to standard LoRA, alongside a substantial decrease in memory overhead. This efficiency gain scales with model size, making ShareLoRA particularly advantageous for resource-constrained environments. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across diverse models, including RoBERTa, GPT-2, and LLaMA series (1, 2, and 3). It consistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning scenarios, achieving up to 1.2\% average accuracy improvement, and enhanced generalization across domains. In continual learning settings, ShareLoRA achieves 1.2\% higher accuracy on GSM8K, 0.6\% on HumanEval, and 0.5\% on both MMLU and MMLU-Pro. Our results demonstrate that ShareLoRA supports high-quality fine-tuning while offering strong generalization and continual adaptation across various model scales and diverse tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging</title>
<link>https://arxiv.org/abs/2406.16330</link>
<guid>https://arxiv.org/abs/2406.16330</guid>
<content:encoded><![CDATA[
arXiv:2406.16330v2 Announce Type: replace 
Abstract: While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models</title>
<link>https://arxiv.org/abs/2406.17513</link>
<guid>https://arxiv.org/abs/2406.17513</guid>
<content:encoded><![CDATA[
arXiv:2406.17513v3 Announce Type: replace 
Abstract: Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs internally represent mental states of self and others. Understanding these internal mechanisms is critical - not only to move beyond surface-level performance, but also for model alignment and safety, where subtle misattributions of mental states may go undetected in generated outputs. In this work, we present the first systematic investigation of belief representations in LMs by probing models across different scales, training regimens, and prompts - using control tasks to rule out confounds. Our experiments provide evidence that both model size and fine-tuning substantially improve LMs' internal representations of others' beliefs, which are structured - not mere by-products of spurious correlations - yet brittle to prompt variations. Crucially, we show that these representations can be strengthened: targeted edits to model activations can correct wrong ToM inferences.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising</title>
<link>https://arxiv.org/abs/2407.00248</link>
<guid>https://arxiv.org/abs/2407.00248</guid>
<content:encoded><![CDATA[
arXiv:2407.00248v2 Announce Type: replace 
Abstract: Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to systems built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. The diffusion layer is trained on top of the existing classifier, ensuring seamless integration with any model in a plug-and-play manner. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over existing adversarial defense methods and achieves state-of-the-art performance against common black-box and white-box adversarial attacks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding</title>
<link>https://arxiv.org/abs/2407.01976</link>
<guid>https://arxiv.org/abs/2407.01976</guid>
<content:encoded><![CDATA[
arXiv:2407.01976v3 Announce Type: replace 
Abstract: Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in KIE and VQA. Comprehensive benchmark evaluations reveal significant improvements of LayTextLLM, with a 15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA OCR-based LLMs. All resources are available at https://github.com/LayTextLLM/LayTextLLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks</title>
<link>https://arxiv.org/abs/2407.18525</link>
<guid>https://arxiv.org/abs/2407.18525</guid>
<content:encoded><![CDATA[
arXiv:2407.18525v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in medicine. However, their utility in non-generative clinical prediction, often presumed inferior to specialized models, remains under-evaluated, leading to ongoing debate within the field and potential for misuse, misunderstanding, or over-reliance due to a lack of systematic benchmarking. Our ClinicRealm study addresses this by benchmarking 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods on unstructured clinical notes and structured Electronic Health Records (EHR). Key findings reveal a significant shift: for clinical note predictions, leading LLMs (e.g., DeepSeek R1/V3, GPT o3-mini-high) in zero-shot settings now decisively outperform finetuned BERT models. On structured EHRs, while specialized models excel with ample data, advanced LLMs (e.g., GPT-4o, DeepSeek R1/V3) show potent zero-shot capabilities, often surpassing conventional models in data-scarce settings. Notably, leading open-source LLMs can match or exceed proprietary counterparts. These results establish modern LLMs as powerful non-generative clinical prediction tools, particularly with unstructured text and offering data-efficient structured data options, thus necessitating a re-evaluation of model selection strategies. This research should serve as an important insight for medical informaticists, AI developers, and clinical researchers, potentially prompting a reassessment of current assumptions and inspiring new approaches to LLM application in predictive healthcare.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning</title>
<link>https://arxiv.org/abs/2408.05517</link>
<guid>https://arxiv.org/abs/2408.05517</guid>
<content:encoded><![CDATA[
arXiv:2408.05517v4 Announce Type: replace 
Abstract: Recent development in Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs) have leverage Attention-based Transformer architectures and achieved superior performance and generalization capabilities. They have since covered extensive areas of traditional learning tasks. For instance, text-based tasks such as text-classification and sequence-labeling, as well as multi-modal tasks like Visual Question Answering (VQA) and Optical Character Recognition (OCR), which were previously addressed using different models, can now be tackled based on one foundation model. Consequently, the training and lightweight fine-tuning of LLMs and MLLMs, especially those based on Transformer architecture, has become particularly important. In recognition of these overwhelming needs, we develop SWIFT, a customizable one-stop infrastructure for large models. With support of over $300+$ LLMs and $50+$ MLLMs, SWIFT stands as the open-source framework that provide the most comprehensive support for fine-tuning large models. In particular, it is the first training framework that provides systematic support for MLLMs. In addition to the core functionalities of fine-tuning, SWIFT also integrates post-training processes such as inference, evaluation, and model quantization, to facilitate fast adoptions of large models in various application scenarios. With a systematic integration of various training techniques, SWIFT offers helpful utilities such as benchmark comparisons among different training techniques for large models. For fine-tuning models specialized in agent framework, we show that notable improvements on the ToolBench leader-board can be achieved by training with customized dataset on SWIFT, with an increase of 5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling</title>
<link>https://arxiv.org/abs/2408.08696</link>
<guid>https://arxiv.org/abs/2408.08696</guid>
<content:encoded><![CDATA[
arXiv:2408.08696v2 Announce Type: replace 
Abstract: Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions</title>
<link>https://arxiv.org/abs/2408.08780</link>
<guid>https://arxiv.org/abs/2408.08780</guid>
<content:encoded><![CDATA[
arXiv:2408.08780v5 Announce Type: replace 
Abstract: With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance. But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns. We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions. Our code will be publicly available once this paper is published.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction</title>
<link>https://arxiv.org/abs/2408.12249</link>
<guid>https://arxiv.org/abs/2408.12249</guid>
<content:encoded><![CDATA[
arXiv:2408.12249v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extraction. To bridge this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs' task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end, we evaluate various open LLMs - including BioMistral and Llama-2 models - on a diverse set of biomedical datasets, using standard prompting, Chain of-Thought (CoT) and Self Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices</title>
<link>https://arxiv.org/abs/2409.01893</link>
<guid>https://arxiv.org/abs/2409.01893</guid>
<content:encoded><![CDATA[
arXiv:2409.01893v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) with extended context windows have significantly improved tasks such as information extraction, question answering, and complex planning scenarios. In order to achieve success in long context tasks, a large amount of work has been done to enhance the long context capabilities of the model through synthetic data. Existing methods typically utilize the Self-Instruct framework to generate instruction tuning data for better long context capability improvement. However, our preliminary experiments indicate that less than 35% of generated samples are multi-hop, and more than 40% exhibit poor quality, limiting comprehensive understanding and further research. To improve the quality of synthetic data, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a Quality Verification Agent, a Single-hop Question Generation Agent, a Multiple Question Sampling Strategy, and a Multi-hop Question Merger Agent. This framework improves the data quality, with the proportion of high-quality, multi-hop, and diverse data exceeding 85%. Furthermore, we systematically investigate strategies for document selection, question merging, and validation techniques through extensive experiments across various models. Our findings show that our synthetic high-quality long-context instruction data significantly enhances model performance, even surpassing models trained on larger amounts of human-annotated data. Our code is available at: https://github.com/WowCZ/LongMIT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Efficient Recursive Numeral Systems via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.07170</link>
<guid>https://arxiv.org/abs/2409.07170</guid>
<content:encoded><![CDATA[
arXiv:2409.07170v4 Announce Type: replace 
Abstract: It has previously been shown that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems that are similar to human ones (Carlsson, 2021). However, it is a major challenge to show how more complex recursive numeral systems, similar to for example English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of efficient recursive number systems. We consider pairs of agents learning how to communicate about numerical quantities through a meta-grammar that can be gradually modified throughout the interactions. Utilising a slightly modified version of the meta-grammar of Hurford (1975), we demonstrate that our RL agents, shaped by the pressures for efficient communication, can effectively modify their lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems in terms of their efficiency.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PACE: Abstractions for Communicating Efficiently</title>
<link>https://arxiv.org/abs/2409.20120</link>
<guid>https://arxiv.org/abs/2409.20120</guid>
<content:encoded><![CDATA[
arXiv:2409.20120v3 Announce Type: replace 
Abstract: A central but unresolved aspect of problem-solving in AI is the capability to introduce and use abstractions, something humans excel at. Work in cognitive science has demonstrated that humans tend towards higher levels of abstraction when engaged in collaborative task-oriented communication, enabling gradually shorter and more information-efficient utterances. Several computational methods have attempted to replicate this phenomenon, but all make unrealistic simplifying assumptions about how abstractions are introduced and learned. Our method, Procedural Abstractions for Communicating Efficiently (PACE), overcomes these limitations through a neuro-symbolic approach. On the symbolic side, we draw on work from library learning for proposing abstractions. We combine this with neural methods for communication and reinforcement learning, via a novel use of bandit algorithms for controlling the exploration and exploitation trade-off in introducing new abstractions. PACE exhibits similar tendencies to humans on a collaborative construction task from the cognitive science literature, where one agent (the architect) instructs the other (the builder) to reconstruct a scene of block-buildings. PACE results in the emergence of an efficient language as a by-product of collaborative communication. Beyond providing mechanistic insights into human communication, our work serves as a first step to providing conversational agents with the ability for human-like communicative abstractions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SSR: Alignment-Aware Modality Connector for Speech Language Models</title>
<link>https://arxiv.org/abs/2410.00168</link>
<guid>https://arxiv.org/abs/2410.00168</guid>
<content:encoded><![CDATA[
arXiv:2410.00168v2 Announce Type: replace 
Abstract: Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-Connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-Connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving pre-trained text ability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</title>
<link>https://arxiv.org/abs/2410.02707</link>
<guid>https://arxiv.org/abs/2410.02707</guid>
<content:encoded><![CDATA[
arXiv:2410.02707v4 Announce Type: replace 
Abstract: Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions</title>
<link>https://arxiv.org/abs/2410.06577</link>
<guid>https://arxiv.org/abs/2410.06577</guid>
<content:encoded><![CDATA[
arXiv:2410.06577v2 Announce Type: replace 
Abstract: Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are open-sourced at https://github.com/codefuse-ai/rodimus.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</title>
<link>https://arxiv.org/abs/2410.07076</link>
<guid>https://arxiv.org/abs/2410.07076</guid>
<content:encoded><![CDATA[
arXiv:2410.07076v5 Announce Type: replace 
Abstract: Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLM Evaluations: The Garbling Trick</title>
<link>https://arxiv.org/abs/2411.01533</link>
<guid>https://arxiv.org/abs/2411.01533</guid>
<content:encoded><![CDATA[
arXiv:2411.01533v3 Announce Type: replace 
Abstract: As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.
  To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative abilities of these models, particularly highlighting the differences between base LLMs and more recent "reasoning" models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs</title>
<link>https://arxiv.org/abs/2411.11266</link>
<guid>https://arxiv.org/abs/2411.11266</guid>
<content:encoded><![CDATA[
arXiv:2411.11266v5 Announce Type: replace 
Abstract: As demonstrated by the proprietary Large Language Models (LLMs) such as GPT and Claude series, LLMs have the potential to achieve remarkable proficiency across a wide range of domains, including law, medicine, finance, science, code, etc., all within a single model. These capabilities are further augmented during the Supervised Fine-Tuning (SFT) phase. Despite their potential, existing work mainly focuses on domain-specific enhancements during fine-tuning, the challenge of which lies in catastrophic forgetting of knowledge across other domains. In this study, we introduce **VersaTune**, a novel data composition framework designed for enhancing LLMs' overall multi-domain capabilities during training. We begin with detecting the distribution of domain-specific knowledge within the base model, followed by the training data composition that aligns with the model's existing knowledge distribution. During the subsequent training process, domain weights are dynamically adjusted based on their learnable potential and forgetting degree. Experimental results indicate that VersaTune is effective in multi-domain fostering, with an improvement of 35.21\% in the overall multi-ability performances compared to uniform domain weights. Furthermore, we find that Qwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o, Claude3.5-Sonnet and DeepSeek-V3 by 0.86\%, 4.76\% and 4.60\%. Additionally, in scenarios where flexible expansion of a specific domain is required, VersaTune reduces the performance degradation in other domains by 38.77\%, while preserving the training efficacy of the target domain.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework</title>
<link>https://arxiv.org/abs/2411.16707</link>
<guid>https://arxiv.org/abs/2411.16707</guid>
<content:encoded><![CDATA[
arXiv:2411.16707v3 Announce Type: replace 
Abstract: The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths</title>
<link>https://arxiv.org/abs/2412.02466</link>
<guid>https://arxiv.org/abs/2412.02466</guid>
<content:encoded><![CDATA[
arXiv:2412.02466v3 Announce Type: replace 
Abstract: This study sets out to answer one major question: Can ChatGPT capture swearing nuances? It presents an empirical study on the ability of ChatGPT to translate Arabic oath expressions into English. 30 Arabic oath expressions were collected from the literature. These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT. Specifically, the gaps involved are: religious gap, cultural gap, both religious and cultural gaps, no gap, using non-oath particles, redundancy and noncapturing of Arabic script diacritics. It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intention Knowledge Graph Construction for User Intention Relation Modeling</title>
<link>https://arxiv.org/abs/2412.11500</link>
<guid>https://arxiv.org/abs/2412.11500</guid>
<content:encoded><![CDATA[
arXiv:2412.11500v2 Announce Type: replace 
Abstract: Understanding user intentions is challenging for online platforms. Recent work on intention knowledge graphs addresses this but often lacks focus on connecting intentions, which is crucial for modeling user behavior and predicting future actions. This paper introduces a framework to automatically generate an intention knowledge graph, capturing connections between user intentions. Using the Amazon m2 dataset, we construct an intention graph with 351 million edges, demonstrating high plausibility and acceptance. Our model effectively predicts new session intentions and enhances product recommendations, outperforming previous state-of-the-art methods and showcasing the approach's practical utility.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DateLogicQA: Benchmarking Temporal Biases in Large Language Models</title>
<link>https://arxiv.org/abs/2412.13377</link>
<guid>https://arxiv.org/abs/2412.13377</guid>
<content:encoded><![CDATA[
arXiv:2412.13377v2 Announce Type: replace 
Abstract: This paper introduces DateLogicQA, a benchmark with 190 questions covering diverse date formats, temporal contexts, and reasoning types. We propose the Semantic Integrity Metric to assess tokenization quality and analyse two biases: Representation-Level Bias, affecting embeddings, and Logical-Level Bias, influencing reasoning outputs. Our findings provide a comprehensive evaluation of LLMs' capabilities and limitations in temporal reasoning, highlighting key challenges in handling temporal data accurately.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set</title>
<link>https://arxiv.org/abs/2412.14872</link>
<guid>https://arxiv.org/abs/2412.14872</guid>
<content:encoded><![CDATA[
arXiv:2412.14872v3 Announce Type: replace 
Abstract: Auto-regressive language models (LMs) have been widely used to generate data in data-scarce domains to train new LMs, compensating for the scarcity of real-world data. Previous work experimentally found that LMs collapse when trained on recursively generated data. This paper presents a theoretical proof: once a corpus (such as a subset of the World Wide Web) begins to incorporate generated data and no new real-world data is added to the corpus, then no matter how small the amount of data each LM generates and contributes to the corpus, LM collapse is inevitable after sufficient time. This finding suggests that attempts to mitigate collapse by limiting the quantity of synthetic data in the corpus are fundamentally insufficient. Instead, avoiding collapse hinges on ensuring the quality of synthetic data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration</title>
<link>https://arxiv.org/abs/2412.17061</link>
<guid>https://arxiv.org/abs/2412.17061</guid>
<content:encoded><![CDATA[
arXiv:2412.17061v2 Announce Type: replace 
Abstract: Scaling laws for inference compute in multi-agent systems remain under-explored compared to single-agent scenarios. This work aims to bridge this gap by investigating the problem of data synthesis through multi-agent sampling, where synthetic responses are generated by sampling from multiple distinct language models. Effective model coordination is crucial for successful multi-agent collaboration. Unlike previous approaches that rely on fixed workflows, we treat model coordination as a multi-step decision-making process, optimizing generation structures dynamically for each input question. We introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow evolves iteratively during the sequential sampling process. To achieve this, we leverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide real-time feedback and accelerate exploration. Our experiments on alignment, machine translation, and mathematical reasoning demonstrate that multi-agent sampling significantly outperforms single-agent sampling as inference compute scales. TOA is the most compute-efficient approach, achieving SOTA performance on WMT and a 72.2\% LC win rate on AlpacaEval. Moreover, fine-tuning with our synthesized alignment data surpasses strong preference learning methods on challenging benchmarks such as Arena-Hard and AlpacaEval.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title>
<link>https://arxiv.org/abs/2501.02506</link>
<guid>https://arxiv.org/abs/2501.02506</guid>
<content:encoded><![CDATA[
arXiv:2501.02506v3 Announce Type: replace 
Abstract: Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios</title>
<link>https://arxiv.org/abs/2501.11269</link>
<guid>https://arxiv.org/abs/2501.11269</guid>
<content:encoded><![CDATA[
arXiv:2501.11269v2 Announce Type: replace 
Abstract: Current multilingual large language models(MLLMs) still focus on simple question-answering formats, often overlooking more complex dialogue scenarios. In other words, their capabilities of multilingual large models have yet to be validated in dialogue tasks with intricate structures. We therefore ask, Q1: How well do LLMs generalize to more complex dialog scenarios? Q2: Can supervised fine-tuning on a high-quality parallel benchmark restore this ability? Q3: Does the "multilingual complementarity" effect survive in the setting? To answer these questions, we introduce XMP, a high-quality parallel Multilingual dataset sourced from Multi-party Podcast dialogues, which is the first parallel dataset focusing on multi-party dialogue scenarios. Most samples in the dataset feature three or more participants, discussing a wide range of topics. Through extensive experiments, we find that, R1: MLLMs fail to generalize to multi-party setting, R2 Fine-tuning on XMP improves only marginally, with the 70B model achieving at most a 1% absolute gain over its 8B counterpart; R3: Mixing languages during SFT is usually detrimental, with any benefits being marginal and limited to isolated cases in the 70B model.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning</title>
<link>https://arxiv.org/abs/2501.11292</link>
<guid>https://arxiv.org/abs/2501.11292</guid>
<content:encoded><![CDATA[
arXiv:2501.11292v2 Announce Type: replace 
Abstract: Multi-party dialogues, common in collaborative scenarios like brainstorming sessions and negotiations, pose significant challenges due to their complexity and diverse speaker roles. Current methods often use graph neural networks to model dialogue context, capturing structural dynamics but heavily relying on annotated graph structures and overlooking individual speaking styles. To address these challenges, we propose CMR, a Contrastive learning-based Multi-party dialogue Response generation framework. CMR employs a two-stage self-supervised contrastive learning framework. First, it captures global differences in speaking styles across individuals. Then, it focuses on intra-conversation comparisons to identify thematic transitions and contextually relevant facts. To the best of our knowledge, this is the first approach that applies contrastive learning in multi-party dialogue generation. Experimental results demonstrate that CMR not only significantly outperforms state-of-the-art models, but also generalizes well to large pre-trained language models, effectively enhancing their capability in handling multi-party conversations.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges</title>
<link>https://arxiv.org/abs/2501.11496</link>
<guid>https://arxiv.org/abs/2501.11496</guid>
<content:encoded><![CDATA[
arXiv:2501.11496v2 Announce Type: replace 
Abstract: The global crisis of language endangerment meets a technological turning point as Generative AI (GenAI) and Large Language Models (LLMs) unlock new frontiers in automating corpus creation, transcription, translation, and tutoring. However, this promise is imperiled by fragmented practices and the critical lack of a methodology to navigate the fraught balance between LLM capabilities and the profound risks of data scarcity, cultural misappropriation, and ethical missteps. This paper introduces a novel analytical framework that systematically evaluates GenAI applications against language-specific needs, embedding community governance and ethical safeguards as foundational pillars. We demonstrate its efficacy through the Te Reo M\=aori revitalization, where it illuminates successes, such as community-led Automatic Speech Recognition achieving 92% accuracy, while critically surfacing persistent challenges in data sovereignty and model bias for digital archives and educational tools. Our findings underscore that GenAI can indeed revolutionize language preservation, but only when interventions are rigorously anchored in community-centric data stewardship, continuous evaluation, and transparent risk management. Ultimately, this framework provides an indispensable toolkit for researchers, language communities, and policymakers, aiming to catalyze the ethical and high-impact deployment of LLMs to safeguard the world's linguistic heritage.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding</title>
<link>https://arxiv.org/abs/2501.12162</link>
<guid>https://arxiv.org/abs/2501.12162</guid>
<content:encoded><![CDATA[
arXiv:2501.12162v2 Announce Type: replace 
Abstract: Modern large language model (LLM) applications exhibit diverse service-level objectives (SLOs), from low-latency requirements in interactive coding assistants to more relaxed constraints in data wrangling tasks. Existing LLM serving systems, which rely on uniform batching and scheduling strategies, often fail to meet these heterogeneous SLOs concurrently. We present AdaServe, the first LLM serving system designed to support efficient multi-SLO serving through SLO-customized speculative decoding. AdaServe formulates multi-SLO serving as a constrained optimization problem and introduces a hardware-aware algorithm that constructs a speculation tree tailored to each request's latency target. It features a speculate-select-verify pipeline that enables fine-grained control over decoding speed while maximizing system throughput. AdaServe further adapts to workload variation by dynamically adjusting speculation parameters. Evaluations across diverse workloads show that AdaServe reduces SLO violations by up to 4.3$\times$ and improves goodput by up to 1.9$\times$ compared to the best performing baselines, highlighting its effectiveness in multi-SLO serving.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Option-ID Based Elimination For Multiple Choice Questions</title>
<link>https://arxiv.org/abs/2501.15175</link>
<guid>https://arxiv.org/abs/2501.15175</guid>
<content:encoded><![CDATA[
arXiv:2501.15175v3 Announce Type: replace 
Abstract: Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing PoE methods typically either have LLMs directly identify incorrect options or score options and replace lower-scoring ones with [MASK]. However, both methods suffer from inapplicability or suboptimal performance. To address these issues, this paper proposes a novel option-ID based PoE ($\text{PoE}_{\text{ID}}$). $\text{PoE}_{\text{ID}}$ critically incorporates a debiasing technique to counteract LLMs token bias, enhancing robustness over naive ID-based elimination. It features two strategies: $\text{PoE}_{\text{ID}}^{\text{log}}$, which eliminates options whose IDs have log probabilities below the average threshold, and $\text{PoE}_{\text{ID}}^{\text{seq}}$, which iteratively removes the option with the lowest ID probability. We conduct extensive experiments with 6 different LLMs on 4 diverse datasets. The results demonstrate that $\text{PoE}_{\text{ID}}$, especially $\text{PoE}_{\text{ID}}^{\text{log}}$, significantly improves zero-shot and few-shot MCQs performance, particularly in datasets with more options. Our analyses demonstrate that $\text{PoE}_{\text{ID}}^{\text{log}}$ enhances the LLMs' confidence in selecting the correct option, and the option elimination strategy outperforms methods relying on [MASK] replacement. We further investigate the limitations of LLMs in directly identifying incorrect options, which stem from their inherent deficiencies.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Linguistics Learned to Stop Worrying and Love the Language Models</title>
<link>https://arxiv.org/abs/2501.17047</link>
<guid>https://arxiv.org/abs/2501.17047</guid>
<content:encoded><![CDATA[
arXiv:2501.17047v2 Announce Type: replace 
Abstract: Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
<link>https://arxiv.org/abs/2501.18280</link>
<guid>https://arxiv.org/abs/2501.18280</guid>
<content:encoded><![CDATA[
arXiv:2501.18280v3 Announce Type: replace 
Abstract: The security issue of large language models (LLMs) has gained wide attention recently, with various defense mechanisms developed to prevent harmful output, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the output distribution of text embedding models is severely biased with a large mean. Inspired by this observation, we propose novel, efficient methods to search for **universal magic words** that attack text embedding models. Universal magic words as suffixes can shift the embedding of any text towards the bias direction, thus manipulating the similarity of any text pair and misleading safeguards. Attackers can jailbreak the safeguards by appending magic words to user prompts and requiring LLMs to end answers with magic words. Experiments show that magic word attacks significantly degrade safeguard performance on JailbreakBench, cause real-world chatbots to produce harmful outputs in full-pipeline attacks, and generalize across input/output texts, models, and languages. To eradicate this security risk, we also propose defense methods against such attacks, which can correct the bias of text embeddings and improve downstream performance in a train-free manner.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-centric Token Compression in Large Language Model</title>
<link>https://arxiv.org/abs/2502.00791</link>
<guid>https://arxiv.org/abs/2502.00791</guid>
<content:encoded><![CDATA[
arXiv:2502.00791v3 Announce Type: replace 
Abstract: Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Localization and Activation Editing for Low-Resource Fine-Tuning</title>
<link>https://arxiv.org/abs/2502.01179</link>
<guid>https://arxiv.org/abs/2502.01179</guid>
<content:encoded><![CDATA[
arXiv:2502.01179v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Embedding Layers in Language Models</title>
<link>https://arxiv.org/abs/2502.01637</link>
<guid>https://arxiv.org/abs/2502.01637</guid>
<content:encoded><![CDATA[
arXiv:2502.01637v2 Announce Type: replace 
Abstract: We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram $E$mbedding), a new method for extending input embedding layers to enhance language model performance. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. After training, embeddings are precomputed and stored in off-accelerator memory; during inference, querying them has minimal impact on latency due to the low complexity of embedding lookups. SCONE enables two new scaling strategies: increasing the number of $n$-gram embeddings and scaling the model used to learn them, both while maintaining fixed accelerator usage during inference (in terms of FLOPS and memory). We show that scaling both aspects enables a model with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline across diverse corpora, while using only about half the FLOPS and accelerator memory during inference.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models</title>
<link>https://arxiv.org/abs/2502.02444</link>
<guid>https://arxiv.org/abs/2502.02444</guid>
<content:encoded><![CDATA[
arXiv:2502.02444v4 Announce Type: replace 
Abstract: Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reformulation for Pretraining Data Augmentation</title>
<link>https://arxiv.org/abs/2502.04235</link>
<guid>https://arxiv.org/abs/2502.04235</guid>
<content:encoded><![CDATA[
arXiv:2502.04235v2 Announce Type: replace 
Abstract: Despite the impressive capabilities of large language models across various tasks, their continued scaling is severely hampered not only by data scarcity but also by the performance degradation associated with excessive data repetition during training. To overcome this critical bottleneck, we propose the Massive Genre-Audience(MGA) reformulation method, a lightweight and scalable data augmentation technique inspired by synthetic data methodologies. MGA systematically reformulates existing corpora into diverse, contextually-rich variations to mitigate the negative effects of repetition, and we introduce this approach along with the resulting 770 billion token MGACorpus in this work. We experimentally validate its core benefit by demonstrating superior performance against data repetition and upsampling in scaling scenarios (up to 13B parameters). Furthermore, comprehensive analysis investigates the role of prompt engineering in generation quality and reveals nuances in evaluating model capabilities using standard loss metrics. Our work shows that MGA provides a reliable pathway to substantially augment training datasets, effectively alleviating repetition bottlenecks and enabling more efficient scaling of large language models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data</title>
<link>https://arxiv.org/abs/2502.05567</link>
<guid>https://arxiv.org/abs/2502.05567</guid>
<content:encoded><![CDATA[
arXiv:2502.05567v2 Announce Type: replace 
Abstract: Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 117k theorem statements and develop ATLAS Translator, which demonstrates statistically significant improvements over both the HERALD Translator and the Kimina-Autoformalizer across all benchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The datasets, model, and code will be released to the public soon.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization</title>
<link>https://arxiv.org/abs/2502.05605</link>
<guid>https://arxiv.org/abs/2502.05605</guid>
<content:encoded><![CDATA[
arXiv:2502.05605v3 Announce Type: replace 
Abstract: While large language models (LLMs) have demonstrated remarkable general performance, enabling smaller models to achieve capabilities comparable to their larger counterparts remains a critical challenge. For humans, iterative refinement of problem analysis and responses is a common strategy to enhance answer quality. However, we observe that existing LLMs exhibit limited ability to refine their outputs for quality improvement. In this paper, we first investigate mechanisms to unlock and progressively enhance self-refinement ability in smaller models within an iterative preference optimization framework, aiming to bridge the performance gap with larger models. To this end, we propose EVOLVE, a novel post-training and inference framework that iteratively integrates preference training with self-refinement-driven data collection. During training, EVOLVE strengthens the model's direct question-answering ability while simultaneously unlocking its self-refinement potential. At inference, the framework leverages this capability to generate progressively refined responses, which are filtered to construct datasets for subsequent rounds of preference training. Experiments demonstrate EVOLVE's exceptional performance: when applied to Llama-3.1-8B base model and under the self-refinement setting, it surpasses state-of-the-art models including Llama-3.1-405B-Instruct and GPT-4o, achieving a 62.3% length-controlled win rate and 63.3% raw win rate on AlpacaEval 2, along with a 50.3% win rate on Arena-Hard. Furthermore, EVOLVE consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement</title>
<link>https://arxiv.org/abs/2502.06207</link>
<guid>https://arxiv.org/abs/2502.06207</guid>
<content:encoded><![CDATA[
arXiv:2502.06207v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Who Taught You That? Tracing Teachers in Model Distillation</title>
<link>https://arxiv.org/abs/2502.06659</link>
<guid>https://arxiv.org/abs/2502.06659</guid>
<content:encoded><![CDATA[
arXiv:2502.06659v2 Announce Type: replace 
Abstract: Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such "footprints" left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues</title>
<link>https://arxiv.org/abs/2502.09120</link>
<guid>https://arxiv.org/abs/2502.09120</guid>
<content:encoded><![CDATA[
arXiv:2502.09120v3 Announce Type: replace 
Abstract: This study investigates whether vision-language models (VLMs) can perform pragmatic inference, focusing on ignorance implicatures, utterances that imply the speaker's lack of precise knowledge. To test this, we systematically manipulated contextual cues: the visually depicted situation (visual cue) and QUD-based linguistic prompts (linguistic cue). When only visual cues were provided, three state-of-the-art VLMs (GPT-4o, Gemini 1.5 Pro, and Claude 3.5 sonnet) produced interpretations largely based on the lexical meaning of the modified numerals. When linguistic cues were added to enhance contextual informativeness, Claude exhibited more human-like inference by integrating both types of contextual cues. In contrast, GPT and Gemini favored precise, literal interpretations. Although the influence of contextual cues increased, they treated each contextual cue independently and aligned them with semantic features rather than engaging in context-driven reasoning. These findings suggest that although the models differ in how they handle contextual cues, Claude's ability to combine multiple cues may signal emerging pragmatic competence in multimodal models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation</title>
<link>https://arxiv.org/abs/2502.10996</link>
<guid>https://arxiv.org/abs/2502.10996</guid>
<content:encoded><![CDATA[
arXiv:2502.10996v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved impressive performance on knowledge-intensive tasks, yet they often struggle with multi-step reasoning due to the unstructured nature of retrieved context. While retrieval-augmented generation (RAG) methods provide external information, the lack of explicit organization among retrieved passages limits their effectiveness, leading to brittle reasoning pathways. Recent interpretability studies highlighting the importance of structured intermediate reasoning further align with this perspective. We propose Retrieval-And-Structuring (RAS), a framework that dynamically constructs query-specific knowledge graphs through iterative retrieval and structured knowledge building. RAS interleaves targeted retrieval planning with incremental graph construction, enabling models to assemble and reason over evolving knowledge structures tailored to each query. On seven knowledge-intensive benchmarks, RAS consistently outperforms strong baselines, achieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs, respectively. Our results demonstrate that dynamic, query-specific knowledge structuring offers a robust path to improving reasoning accuracy and robustness in language model generation. Our data and code can be found at https://github.com/pat-jj/RAS.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Pairwise: Global Zero-shot Temporal Graph Generation</title>
<link>https://arxiv.org/abs/2502.11114</link>
<guid>https://arxiv.org/abs/2502.11114</guid>
<content:encoded><![CDATA[
arXiv:2502.11114v2 Announce Type: replace 
Abstract: Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, where event pairs are classified in isolation, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph in a single step, followed by temporal constraint optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method outperforms existing zero-shot approaches and offers a competitive alternative to supervised TRE models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures</title>
<link>https://arxiv.org/abs/2502.11150</link>
<guid>https://arxiv.org/abs/2502.11150</guid>
<content:encoded><![CDATA[
arXiv:2502.11150v2 Announce Type: replace 
Abstract: Automated text readability prediction is widely used in many real-world scenarios. Over the past century, such measures have primarily been developed and evaluated on reading comprehension outcomes and on human annotations of text readability levels. In this work, we propose an alternative, eye tracking-based cognitive framework which directly taps into a key aspect of readability: reading ease. We use this framework for evaluating a broad range of prominent readability measures, including two systems widely used in education, by quantifying their ability to account for reading facilitation effects in text simplification, as well as text reading ease more broadly. Our analyses suggest that existing readability measures are poor predictors of reading facilitation and reading ease, outperformed by word properties commonly used in psycholinguistics, and in particular by surprisal.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mirage of Model Editing: Revisiting Evaluation in the Wild</title>
<link>https://arxiv.org/abs/2502.11177</link>
<guid>https://arxiv.org/abs/2502.11177</guid>
<content:encoded><![CDATA[
arXiv:2502.11177v4 Announce Type: replace 
Abstract: Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs</title>
<link>https://arxiv.org/abs/2502.11380</link>
<guid>https://arxiv.org/abs/2502.11380</guid>
<content:encoded><![CDATA[
arXiv:2502.11380v2 Announce Type: replace 
Abstract: Lexico-semantic networks represent words as nodes and their semantic relatedness as edges. While such networks are traditionally constructed using embeddings from encoder-based models or static vectors, embeddings from decoder-only large language models (LLMs) remain underexplored. Unlike encoder models, LLMs are trained with a next-token prediction objective, which does not directly encode the meaning of the current token. In this paper, we construct lexico-semantic networks from the input embeddings of LLMs with varying parameter scales and conduct a comparative analysis of their global and local structures. Our results show that these networks exhibit small-world properties, characterized by high clustering and short path lengths. Moreover, larger LLMs yield more intricate networks with less small-world effects and longer paths, reflecting richer semantic structures and relations. We further validate our approach through analyses of common conceptual pairs, structured lexical relations derived from WordNet, and a cross-lingual semantic network for qualitative words.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs</title>
<link>https://arxiv.org/abs/2502.11525</link>
<guid>https://arxiv.org/abs/2502.11525</guid>
<content:encoded><![CDATA[
arXiv:2502.11525v2 Announce Type: replace 
Abstract: Length generalization, the ability to solve problems longer than those seen during training, remains a critical challenge for large language models (LLMs). Previous work modifies positional encodings (PEs) and data formats to improve length generalization on specific symbolic tasks such as addition and sorting. However, these approaches are fundamentally limited to special tasks, often degrading general language performance. Furthermore, they are typically evaluated on small transformers trained from scratch on single tasks and can cause performance drop when applied during post-training stage of practical LLMs with general capabilities. Hu et al., (2024) proposed Rule-Following Fine-Tuning (RFFT) to improve length generalization in the post-training stage of LLMs. Despite its compatibility with practical models and strong performance, RFFT is proposed for single tasks too, requiring re-training for each individual task with extensive examples. In this paper, we study length generalization in multi-task settings and propose Meta Rule-Following Fine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length generalization. As our first contribution, we construct a large length generalization dataset containing 86 tasks spanning code execution, number processing, symbolic and logical reasoning tasks, beyond the common addition or multiplication tasks. Secondly, we show that cross-task length generalization is possible with Meta-RFFT. After training on a large number of tasks and instances, the models achieve remarkable length generalization ability on unseen tasks with minimal fine-tuning or one-shot prompting. For example, after fine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30 digit addition, significantly outperforming the state-of-the-art reasoning models (DeepSeek-R1-671B: 72%), despite never seeing this task during RF-pretraining.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FaMTEB: Massive Text Embedding Benchmark in Persian Language</title>
<link>https://arxiv.org/abs/2502.11571</link>
<guid>https://arxiv.org/abs/2502.11571</guid>
<content:encoded><![CDATA[
arXiv:2502.11571v2 Announce Type: replace 
Abstract: In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2502.12202</link>
<guid>https://arxiv.org/abs/2502.12202</guid>
<content:encoded><![CDATA[
arXiv:2502.12202v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) are designed to solve complex tasks by generating explicit reasoning traces before producing final answers. However, we reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability -- wherein the thinking process can be bypassed by manipulating special delimiter tokens. It is empirically demonstrated to be widespread across mainstream LRMs, posing both a significant risk and potential utility, depending on how it is exploited. In this paper, we systematically investigate this vulnerability from both malicious and beneficial perspectives. On the malicious side, we introduce Breaking of Thought (BoT), a novel attack that enables adversaries to bypass the thinking process of LRMs, thereby compromising their reliability and availability. We present two variants of BoT: a training-based version that injects backdoor during the fine-tuning stage, and a training-free version based on adversarial attack during the inference stage. As a potential defense, we propose thinking recovery alignment to partially mitigate the vulnerability. On the beneficial side, we introduce Monitoring of Thought (MoT), a plug-and-play framework that allows model owners to enhance efficiency and safety. It is implemented by leveraging the same vulnerability to dynamically terminate redundant or risky reasoning through external monitoring. Extensive experiments show that BoT poses a significant threat to reasoning reliability, while MoT provides a practical solution for preventing overthinking and jailbreaking. Our findings expose an inherent flaw in current LRM architectures and underscore the need for more robust reasoning systems in the future.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</title>
<link>https://arxiv.org/abs/2502.12464</link>
<guid>https://arxiv.org/abs/2502.12464</guid>
<content:encoded><![CDATA[
arXiv:2502.12464v2 Announce Type: replace 
Abstract: Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora</title>
<link>https://arxiv.org/abs/2502.13691</link>
<guid>https://arxiv.org/abs/2502.13691</guid>
<content:encoded><![CDATA[
arXiv:2502.13691v2 Announce Type: replace 
Abstract: As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach</title>
<link>https://arxiv.org/abs/2502.14285</link>
<guid>https://arxiv.org/abs/2502.14285</guid>
<content:encoded><![CDATA[
arXiv:2502.14285v2 Announce Type: replace 
Abstract: Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine-generated text detection prevents language model collapse</title>
<link>https://arxiv.org/abs/2502.15654</link>
<guid>https://arxiv.org/abs/2502.15654</guid>
<content:encoded><![CDATA[
arXiv:2502.15654v5 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, converge to a low variance output distribution, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the text characteristics at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2), across a range of model sizes (124M to 1.7B), on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. Source code: github.com/GeorgeDrayson/model_collapse.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FANformer: Improving Large Language Models Through Effective Periodicity Modeling</title>
<link>https://arxiv.org/abs/2502.21309</link>
<guid>https://arxiv.org/abs/2502.21309</guid>
<content:encoded><![CDATA[
arXiv:2502.21309v2 Announce Type: replace 
Abstract: Periodicity, as one of the most important basic characteristics, lays the foundation for facilitating structured knowledge acquisition and systematic cognitive processes within human learning paradigms. However, the potential flaws of periodicity modeling in Transformer affect the learning efficiency and establishment of underlying principles from data for large language models (LLMs) built upon it. In this paper, we demonstrate that integrating effective periodicity modeling can improve the learning efficiency and performance of LLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into attention mechanism to achieve efficient periodicity modeling, by modifying the feature projection process of attention mechanism. Extensive experimental results on language modeling show that FANformer consistently outperforms Transformer when scaling up model size and training tokens, underscoring its superior learning efficiency. Our pretrained FANformer-1B exhibits marked improvements on downstream tasks compared to open-source LLMs with similar model parameters or training tokens. Moreover, we reveal that FANformer exhibits superior ability to learn and apply rules for reasoning compared to Transformer. The results position FANformer as an effective and promising architecture for advancing LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings</title>
<link>https://arxiv.org/abs/2503.03008</link>
<guid>https://arxiv.org/abs/2503.03008</guid>
<content:encoded><![CDATA[
arXiv:2503.03008v2 Announce Type: replace 
Abstract: Deploying language models often requires navigating accuracy vs. performance trade-offs to meet latency constraints while preserving utility. Traditional model distillation reduces size but incurs substantial costs through training separate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter multi-exit encoder for code retrieval and classification that employs a novel Self-Distillation mechanism. This approach significantly enhances lower-layer representations, enabling flexible deployment of different model portions with favorable performance trade-offs. Our architecture improves text-to-code and code-to-code search by targeting specific encoder layers as exit heads, where higher layers guide earlier ones during training-improving intermediate representations at minimal additional cost. We further enhance MoSE with a repository-level contextual loss that maximizes training context window utilization. Additionally, we release a new dataset created through code translation that extends text-to-code benchmarks with cross-language code-to-code pairs. Evaluations demonstrate the effectiveness of Self-Distillation as a principled approach to trading inference cost for accuracy across various code understanding tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions</title>
<link>https://arxiv.org/abs/2503.03261</link>
<guid>https://arxiv.org/abs/2503.03261</guid>
<content:encoded><![CDATA[
arXiv:2503.03261v2 Announce Type: replace 
Abstract: Multiple previous studies have reported suboptimal performance of LLMs in biomedical text mining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. We experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our results show that frontier LLMs can approach or surpass the performance of SOTA BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these findings, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios</title>
<link>https://arxiv.org/abs/2503.06218</link>
<guid>https://arxiv.org/abs/2503.06218</guid>
<content:encoded><![CDATA[
arXiv:2503.06218v2 Announce Type: replace 
Abstract: Currently, long-chain reasoning remains a key challenge for large language models (LLMs) because natural texts lack sufficient explicit reasoning data. However, existing benchmarks suffer from limitations such as narrow coverage, short reasoning paths, or high construction costs. We introduce SCoRE (Scenario-based Commonsense Reasoning Evaluation), a benchmark that synthesizes multi-hop questions from scenario schemas of entities, relations, and logical rules to assess long-chain commonsense reasoning. SCoRE contains 100k bilingual (Chinese-English) multiple-choice questions whose reasoning chains span 2-11 hops and are grouped into various difficulty levels. Each question is accompanied by fine-grained knowledge labels, explicit reasoning chains, and difficulty levels for diagnostic evaluation. Evaluation results on cutting-edge LLMs such as o3-mini and Deepseek R1 shows that even the best model attains only 69.78% accuracy on SCoRE (even only 47.91% on the hard set), with errors often stemming from rare knowledge, logical inconsistency, and over-interpretation of simple questions. SCoRE offers a scalable, extensible framework for evaluating and diagnosing the long-chain commonsense reasoning abilities of LLMs and guiding future advances in model design and training.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs</title>
<link>https://arxiv.org/abs/2503.09543</link>
<guid>https://arxiv.org/abs/2503.09543</guid>
<content:encoded><![CDATA[
arXiv:2503.09543v2 Announce Type: replace 
Abstract: The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Reasoning with LLMs for k-anonymity Estimation</title>
<link>https://arxiv.org/abs/2503.09674</link>
<guid>https://arxiv.org/abs/2503.09674</guid>
<content:encoded><![CDATA[
arXiv:2503.09674v2 Announce Type: replace 
Abstract: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality</title>
<link>https://arxiv.org/abs/2503.10669</link>
<guid>https://arxiv.org/abs/2503.10669</guid>
<content:encoded><![CDATA[
arXiv:2503.10669v2 Announce Type: replace 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs) with human values. However, existing approaches struggle to capture the multi-dimensional, distributional nuances of human preferences. Methods such as RiC that directly inject raw reward values into prompts face significant numerical sensitivity issues--for instance, LLMs may fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF, Rewarded Soups, and MODPO incur high computational costs by training multiple models. In this work, we introduce Utility-Conditioned Multi-Objective Alignment (UC-MOA), a novel framework that overcomes these limitations. Our approach leverages a diverse set of strictly increasing, non-linear utility functions to transform user-specified preferences into symbolic tokens, which are then used to condition a single LLM. This design not only mitigates numerical reasoning challenges but also substantially reduces training overhead, yielding models that achieve superior Pareto fronts and robust alignment across complex reward dimensions.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models</title>
<link>https://arxiv.org/abs/2503.12908</link>
<guid>https://arxiv.org/abs/2503.12908</guid>
<content:encoded><![CDATA[
arXiv:2503.12908v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more "contrast-effective" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models</title>
<link>https://arxiv.org/abs/2503.14411</link>
<guid>https://arxiv.org/abs/2503.14411</guid>
<content:encoded><![CDATA[
arXiv:2503.14411v2 Announce Type: replace 
Abstract: Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement. To tackle these issues, we present \textbf{CROSS}, a flexible framework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is designed by decomposing the TTAG modeling process into two phases: (i) temporal semantics extraction; and (ii) semantic-structural information unification. The key idea is to advance the large language models (LLMs) to dynamically extract the temporal semantics in text space and then generate cohesive representations unifying both semantics and structures. Specifically, we propose a Temporal Semantics Extractor in the CROSS framework, which empowers LLMs to offer the temporal semantic understanding of node's evolving contexts of textual neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experiments show that CROSS achieves state-of-the-art results on four public datasets and one industrial dataset, with 24.7% absolute MRR gain on average in temporal link prediction and 3.7% AUC gain in node classification of industrial application.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21380</link>
<guid>https://arxiv.org/abs/2503.21380</guid>
<content:encoded><![CDATA[
arXiv:2503.21380v2 Announce Type: replace 
Abstract: In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5 Pro Exp demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the benchmark, evaluation code, detailed results and a data visualization tool at https://github.com/RUCAIBox/OlymMATH.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2503.21729</link>
<guid>https://arxiv.org/abs/2503.21729</guid>
<content:encoded><![CDATA[
arXiv:2503.21729v3 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ImF: Implicit Fingerprint for Large Language Models</title>
<link>https://arxiv.org/abs/2503.21805</link>
<guid>https://arxiv.org/abs/2503.21805</guid>
<content:encoded><![CDATA[
arXiv:2503.21805v2 Announce Type: replace 
Abstract: Training large language models (LLMs) is resource-intensive and expensive, making protecting intellectual property (IP) for LLMs crucial. Recently, embedding fingerprints into LLMs has emerged as a prevalent method for establishing model ownership. However, existing fingerprinting techniques typically embed identifiable patterns with weak semantic coherence, resulting in fingerprints that significantly differ from the natural question-answering (QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of the embedded fingerprints and makes them vulnerable to adversarial attacks. In this paper, we first demonstrate the critical vulnerability of existing fingerprint embedding methods by introducing a novel adversarial attack named Generation Revision Intervention (GRI) attack. GRI attack exploits the semantic fragility of current fingerprinting methods, effectively erasing fingerprints by disrupting their weakly correlated semantic structures. Our empirical evaluation highlights that traditional fingerprinting approaches are significantly compromised by the GRI attack, revealing severe limitations in their robustness under realistic adversarial conditions. To advance the state-of-the-art in model fingerprinting, we propose a novel model fingerprint paradigm called Implicit Fingerprints (ImF). ImF leverages steganography techniques to subtly embed ownership information within natural texts, subsequently using Chain-of-Thought (CoT) prompting to construct semantically coherent and contextually natural QA pairs. This design ensures that fingerprints seamlessly integrate with the standard model behavior, remaining indistinguishable from regular outputs and substantially reducing the risk of accidental triggering and targeted removal. We conduct a comprehensive evaluation of ImF on 15 diverse LLMs, spanning different architectures and varying scales.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors</title>
<link>https://arxiv.org/abs/2503.22388</link>
<guid>https://arxiv.org/abs/2503.22388</guid>
<content:encoded><![CDATA[
arXiv:2503.22388v2 Announce Type: replace 
Abstract: LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RARE: Retrieval-Augmented Reasoning Modeling</title>
<link>https://arxiv.org/abs/2503.23513</link>
<guid>https://arxiv.org/abs/2503.23513</guid>
<content:encoded><![CDATA[
arXiv:2503.23513v2 Announce Type: replace 
Abstract: Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts with masked losses, RARE transforms learning objectives from rote memorization to contextualized reasoning. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Extensive experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\% accuracy. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FISH-Tuning: Enhancing PEFT Methods with Fisher Information</title>
<link>https://arxiv.org/abs/2504.04050</link>
<guid>https://arxiv.org/abs/2504.04050</guid>
<content:encoded><![CDATA[
arXiv:2504.04050v2 Announce Type: replace 
Abstract: The rapid growth in the parameter size of Large Language Models (LLMs) has spurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to mitigate the substantial computational costs of fine-tuning. Among these, Fisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT technique that identifies a critical subset of pre-trained parameters using approximate Fisher information. While addition-based and reparameterization-based PEFT methods like LoRA and Adapter already fine-tune only a small number of parameters, the newly introduced parameters within these methods themselves present an opportunity for further optimization. Selectively fine-tuning only the most impactful among these new parameters could further reduce resource consumption while maintaining, or even improving, fine-tuning effectiveness. In this paper, we propose \textbf{FISH-Tuning}, a novel approach that incorporates FISH Mask into such PEFT methods, including LoRA, Adapter, and their variants. By leveraging Fisher information to identify and update only the most significant parameters within these added or reparameterized components, FISH-Tuning aims to achieve superior performance without increasing training time or inference latency compared to the vanilla PEFT methods. Experimental results across various datasets and pre-trained models demonstrate that FISH-Tuning consistently outperforms the vanilla PEFT methods when using the same proportion of trainable parameters. Code is available at https://anonymous.4open.science/r/FISH-Tuning-6F7C.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Robust Optimization for LLM Alignment under Distribution Shifts</title>
<link>https://arxiv.org/abs/2504.05831</link>
<guid>https://arxiv.org/abs/2504.05831</guid>
<content:encoded><![CDATA[
arXiv:2504.05831v2 Announce Type: replace 
Abstract: Preference alignment methods are increasingly critical for steering large language models (LLMs) to generate outputs consistent with human values. While recent approaches often rely on synthetic data generated by LLMs for scalability and cost-efficiency reasons, this reliance can introduce distribution shifts that undermine the nuanced representation of human preferences needed for desirable outputs. In this paper, we propose a novel distribution-aware optimization framework that improves preference alignment despite such shifts. Our approach first leverages well-learned classifiers to assign a calibration value to each training sample, quantifying its alignment with the target human-preferred distribution. These values are then incorporated into a robust optimization objective that minimizes the worst-case loss over regions of the data space most relevant to human preferences. By explicitly focusing optimization on the target distribution, our approach mitigates the impact of distributional mismatch and improves the generation of responses that better reflect intended values.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LSR-MCTS: Alleviating Long Range Dependency in Code Generation</title>
<link>https://arxiv.org/abs/2504.07433</link>
<guid>https://arxiv.org/abs/2504.07433</guid>
<content:encoded><![CDATA[
arXiv:2504.07433v3 Announce Type: replace 
Abstract: The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Could Be Rote Learners</title>
<link>https://arxiv.org/abs/2504.08300</link>
<guid>https://arxiv.org/abs/2504.08300</guid>
<content:encoded><![CDATA[
arXiv:2504.08300v4 Announce Type: replace 
Abstract: Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework reformulating MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2504.10198</link>
<guid>https://arxiv.org/abs/2504.10198</guid>
<content:encoded><![CDATA[
arXiv:2504.10198v2 Announce Type: replace 
Abstract: Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events</title>
<link>https://arxiv.org/abs/2504.12052</link>
<guid>https://arxiv.org/abs/2504.12052</guid>
<content:encoded><![CDATA[
arXiv:2504.12052v3 Announce Type: replace 
Abstract: We present a Bayesian dynamic borrowing (BDB) approach to enhance the quantitative identification of adverse events (AEs) in spontaneous reporting systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior with a Bayesian hierarchical model and incorporates semantic similarity measures (SSMs) to enable weighted information sharing from clinically similar MedDRA Preferred Terms (PTs) to the target PT. This continuous similarity-based borrowing overcomes limitations of rigid hierarchical grouping in current disproportionality analysis (DPA).
  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015 and 2019, we evaluate our approach -- termed IC SSM -- against traditional Information Component (IC) analysis and IC with borrowing at the MedDRA high-level group term level (IC HLGT). A reference set (PVLens), derived from FDA product label update, enabled prospective evaluation of method performance in identifying AEs prior to official labeling.
  The IC SSM approach demonstrated higher sensitivity (1332/2337=0.570, Youden's J=0.246) than traditional IC (Se=0.501, J=0.250) and IC HLGT (Se=0.556, J=0.225), consistently identifying more true positives and doing so on average 5 months sooner than traditional IC. Despite a marginally lower aggregate F1-score and Youden's index, IC SSM showed higher performance in early post-marketing periods or when the detection threshold was raised, providing more stable and relevant alerts than IC HLGT and traditional IC.
  These findings support the use of SSM-informed Bayesian borrowing as a scalable and context-aware enhancement to traditional DPA methods, with potential for validation across other datasets and exploration of additional similarity metrics and Bayesian strategies using case-level data.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.13534</link>
<guid>https://arxiv.org/abs/2504.13534</guid>
<content:encoded><![CDATA[
arXiv:2504.13534v2 Announce Type: replace 
Abstract: Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and interference from natural language reasoning steps with the models' inference process, also known as the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which promotes greater logical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine public datasets spanning three reasoning tasks reveal significant accuracy gains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore, tests on four domain-specific datasets demonstrate exceptional accuracy and efficient execution, underscoring its practical applicability and scalability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach</title>
<link>https://arxiv.org/abs/2504.14321</link>
<guid>https://arxiv.org/abs/2504.14321</guid>
<content:encoded><![CDATA[
arXiv:2504.14321v2 Announce Type: replace 
Abstract: Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources. To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data</title>
<link>https://arxiv.org/abs/2504.14669</link>
<guid>https://arxiv.org/abs/2504.14669</guid>
<content:encoded><![CDATA[
arXiv:2504.14669v2 Announce Type: replace 
Abstract: The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamic Early Exit in Reasoning Models</title>
<link>https://arxiv.org/abs/2504.15895</link>
<guid>https://arxiv.org/abs/2504.15895</guid>
<content:encoded><![CDATA[
arXiv:2504.15895v2 Announce Type: replace 
Abstract: Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and LiveCodeBench) show that the proposed method is consistently effective on 11 cutting-edge reasoning LLMs of varying series and sizes, reducing the length of CoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3% to 5.0%.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2504.16074</link>
<guid>https://arxiv.org/abs/2504.16074</guid>
<content:encoded><![CDATA[
arXiv:2504.16074v2 Announce Type: replace 
Abstract: Current benchmarks for evaluating the reasoning capabilities of Large Language Models (LLMs) face significant limitations: task oversimplification, data contamination, and flawed evaluation items. These deficiencies necessitate more rigorous assessment methods. To address these limitations, we introduce PHYBench, a benchmark of 500 original physics problems ranging from high school to Physics Olympiad difficulty. PHYBench addresses data contamination through original content and employs a systematic curation pipeline to eliminate flawed items. Evaluations show that PHYBench activates more tokens and provides stronger differentiation between reasoning models compared to other baselines like AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini 2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To further enhance evaluation precision, we introduce the Expression Edit Distance (EED) Score for mathematical expression assessment, which improves sample efficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits multi-step and multi-condition reasoning, providing a platform for examining models' reasoning robustness, preferences, and deficiencies. The benchmark results and dataset are publicly available at https://www.phybench.cn/.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents</title>
<link>https://arxiv.org/abs/2504.16918</link>
<guid>https://arxiv.org/abs/2504.16918</guid>
<content:encoded><![CDATA[
arXiv:2504.16918v2 Announce Type: replace 
Abstract: Optimization plays a vital role in scientific research and practical applications. However, formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce OptimAI, a framework for solving Optimization problems described in natural language by leveraging LLM-powered AI agents, and achieve superior performance over current state-of-the-art methods. Our framework is built upon the following key roles: (1) a formulator that translates natural language problem descriptions into precise mathematical formulations; (2) a planner that constructs a high-level solution strategy prior to execution; and (3) a coder and a code critic capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, and our experiments confirm that combining diverse models leads to performance gains. Our approach attains 88.1% accuracy on the NLP4LP dataset and 82.3% on the Optibench dataset, reducing error rates by 58% and 52%, respectively, over prior best results.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning</title>
<link>https://arxiv.org/abs/2504.17192</link>
<guid>https://arxiv.org/abs/2504.17192</guid>
<content:encoded><![CDATA[
arXiv:2504.17192v3 Announce Type: replace 
Abstract: Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, particularly from the authors of those papers, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning</title>
<link>https://arxiv.org/abs/2504.19162</link>
<guid>https://arxiv.org/abs/2504.19162</guid>
<content:encoded><![CDATA[
arXiv:2504.19162v2 Announce Type: replace 
Abstract: Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, SPC can guide the test-time search of diverse LLMs and significantly improve their mathematical reasoning performance on MATH500 and AIME2024, surpassing those guided by state-of-the-art process reward models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning</title>
<link>https://arxiv.org/abs/2504.19627</link>
<guid>https://arxiv.org/abs/2504.19627</guid>
<content:encoded><![CDATA[
arXiv:2504.19627v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models</title>
<link>https://arxiv.org/abs/2504.20157</link>
<guid>https://arxiv.org/abs/2504.20157</guid>
<content:encoded><![CDATA[
arXiv:2504.20157v2 Announce Type: replace 
Abstract: Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, from essay writing to mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and data can be accessed at: https://github.com/minnesotanlp/mpo
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities</title>
<link>https://arxiv.org/abs/2504.20734</link>
<guid>https://arxiv.org/abs/2504.20734</guid>
<content:encoded><![CDATA[
arXiv:2504.20734v2 Announce Type: replace 
Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over various modality-specific and unified baselines.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Reasoning of Large Language Models</title>
<link>https://arxiv.org/abs/2504.20771</link>
<guid>https://arxiv.org/abs/2504.20771</guid>
<content:encoded><![CDATA[
arXiv:2504.20771v2 Announce Type: replace 
Abstract: With the rapid development and widespread application of Large Language Models (LLMs), multidimensional evaluation has become increasingly critical. However, current evaluations are often domain-specific and overly complex, limiting their effectiveness as cross-domain proxies for core capabilities. To address these limitations and enable a unified and simple evaluation framework, an ideal proxy task should target a basic capability that generalizes across tasks and is independent of domain-specific knowledge. Turing machine provides a powerful theoretical lens by reducing complex processes to basic, domain-agnostic computational operations. This perspective offers a principled framework for evaluating basic computational abilities essential to a wide range of tasks. Motivated by this abstraction, we introduce \textbf{Turing Machine Bench}, a benchmark designed to assess the ability of LLMs to \textbf{strictly follow rules} and \textbf{accurately manage internal states} for multi-step, referred to as \textbf{computational reasoning}. TMBench incorporates four key features: self-contained and knowledge-agnostic reasoning, a minimalistic multi-step structure, controllable difficulty, and a solid theoretical foundation based on Turing machine. Empirical results demonstrate that TMBench serves as an effective proxy for evaluating computational reasoning on representative LLMs. It produces clear step-wise accuracy curves, revealing LLMs' ability to execute multi-step reasoning processes. By analyzing performance trends across TMBench and established reasoning benchmarks, we find strong correlations with real-world tasks, bridging real-task evaluation with basic ability assessment. These findings suggest that TMBench holds potential as a cross-domain dimension for evaluating reasoning in LLMs. Code and data are available at \href{https://github.com/HaitaoWuTJU/Turing-Machine-Bench}{Repo}.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</title>
<link>https://arxiv.org/abs/2505.00570</link>
<guid>https://arxiv.org/abs/2505.00570</guid>
<content:encoded><![CDATA[
arXiv:2505.00570v2 Announce Type: replace 
Abstract: Frequency-domain compression has proven effective in reducing redundancies for spatial signals. In this work, we propose FreqKV, a novel frequency domain key-value (KV) compression technique that enables efficient context window extension for decoder-only large language models (LLMs). Our approach is motivated by a key observation that, in the frequency domain, the energy distribution of the KV cache is predominantly concentrated in low-frequency components. By discarding high-frequency components, we achieve efficient compression of the KV cache with minimal information loss. FreqKV iteratively compresses the increasing KV cache to a fixed size in the frequency domain, allowing models to process lengthy contexts efficiently. Introducing no additional parameters or architectural modifications, FreqKV is applicable to both fine-tuning and inference. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window. Experiments on a range of long context language modeling and understanding tasks demonstrate the efficiency and effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent</title>
<link>https://arxiv.org/abs/2309.12555</link>
<guid>https://arxiv.org/abs/2309.12555</guid>
<content:encoded><![CDATA[
arXiv:2309.12555v2 Announce Type: replace-cross 
Abstract: Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users' goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study -- involving a user study, intrinsic evaluation, and expert evaluation -- demonstrated PlanFitting's ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BAT: Learning to Reason about Spatial Sounds with Large Language Models</title>
<link>https://arxiv.org/abs/2402.01591</link>
<guid>https://arxiv.org/abs/2402.01591</guid>
<content:encoded><![CDATA[
arXiv:2402.01591v3 Announce Type: replace-cross 
Abstract: Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era</title>
<link>https://arxiv.org/abs/2403.08946</link>
<guid>https://arxiv.org/abs/2403.08946</guid>
<content:encoded><![CDATA[
arXiv:2403.08946v2 Announce Type: replace-cross 
Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended toward explaining Large Language Models (LLMs). This extension calls for a significant transformation in the XAI methodologies for two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity and advanced capabilities. Second, as LLMs are increasingly deployed in diverse applications, the role of XAI shifts from merely opening the ``black box'' to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, the conversation and generation abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can explain and improve LLM-based AI systems and (2) how XAI techniques can be improved by using LLMs. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlled Training Data Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2403.15309</link>
<guid>https://arxiv.org/abs/2403.15309</guid>
<content:encoded><![CDATA[
arXiv:2403.15309v2 Announce Type: replace-cross 
Abstract: We present a method to control a text-to-image generative model to produce training data useful for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, datasets and architectures, with different types of distribution shifts (spuriously correlated data, unseen domains) and demonstrate the efficiency of the proposed feedback mechanisms compared to open-loop approaches.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak</title>
<link>https://arxiv.org/abs/2405.20015</link>
<guid>https://arxiv.org/abs/2405.20015</guid>
<content:encoded><![CDATA[
arXiv:2405.20015v2 Announce Type: replace-cross 
Abstract: This paper focuses on jailbreaking attacks against large language models (LLMs), eliciting them to generate objectionable content in response to harmful user queries. Unlike previous LLM-jailbreak methods that directly orient to LLMs, our approach begins by constructing a multimodal large language model (MLLM) built upon the target LLM. Subsequently, we perform an efficient MLLM jailbreak and obtain a jailbreaking embedding. Finally, we convert the embedding into a textual jailbreaking suffix to carry out the jailbreak of target LLM. Compared to the direct LLM-jailbreak methods, our indirect jailbreaking approach is more efficient, as MLLMs are more vulnerable to jailbreak than pure LLM. Additionally, to improve the attack success rate of jailbreak, we propose an image-text semantic matching scheme to identify a suitable initial input. Extensive experiments demonstrate that our approach surpasses current state-of-the-art jailbreak methods in terms of both efficiency and effectiveness. Moreover, our approach exhibits superior cross-class generalization abilities.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$S^3$ -- Semantic Signal Separation</title>
<link>https://arxiv.org/abs/2406.09556</link>
<guid>https://arxiv.org/abs/2406.09556</guid>
<content:encoded><![CDATA[
arXiv:2406.09556v3 Announce Type: replace-cross 
Abstract: Topic models are useful tools for discovering latent semantic structures in large textual corpora. Recent efforts have been oriented at incorporating contextual representations in topic modeling and have been shown to outperform classical topic models. These approaches are typically slow, volatile, and require heavy preprocessing for optimal results. We present Semantic Signal Separation ($S^3$), a theory-driven topic modeling approach in neural embedding spaces. $S^3$ conceptualizes topics as independent axes of semantic space and uncovers these by decomposing contextualized document embeddings using Independent Component Analysis. Our approach provides diverse and highly coherent topics, requires no preprocessing, and is demonstrated to be the fastest contextual topic model, being, on average, 4.5x faster than the runner-up BERTopic. We offer an implementation of $S^3$, and all contextual baselines, in the Turftopic Python package.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarking Language Models with Error Correcting Codes</title>
<link>https://arxiv.org/abs/2406.10281</link>
<guid>https://arxiv.org/abs/2406.10281</guid>
<content:encoded><![CDATA[
arXiv:2406.10281v3 Announce Type: replace-cross 
Abstract: Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Facet Learning: A Structured Approach to Prompt Optimization</title>
<link>https://arxiv.org/abs/2406.10504</link>
<guid>https://arxiv.org/abs/2406.10504</guid>
<content:encoded><![CDATA[
arXiv:2406.10504v2 Announce Type: replace-cross 
Abstract: Given a task in the form of a basic description and its training examples, prompt optimization is the problem of synthesizing the given information into a text prompt for a large language model. Humans solve this problem by also considering the different facets that define a task (e.g., counter-examples, explanations, analogies) and including them in the prompt. However, it is unclear whether existing algorithmic approaches, based on iteratively editing a given prompt or automatically selecting a few in-context examples, can cover the multiple facets required to solve a complex task. In this work, we view prompt optimization as that of learning multiple facets of a task from a set of training examples. We exploit structure in the prompt optimization problem and break down a prompt into loosely coupled semantic sections. The proposed algorithm, UniPrompt, (1) clusters the input space and uses clustered batches so that each batch likely corresponds to a different facet of the task, and (2) utilizes a feedback mechanism to propose adding, editing or deleting a section, which in turn is aggregated over a batch to capture generalizable facets. Empirical evaluation on multiple datasets and a real-world task shows that prompts generated using \shortname{} obtain higher accuracy than human-tuned prompts and those from state-of-the-art methods. In particular, our algorithm can generate long, complex prompts that existing methods are unable to generate. Code for UniPrompt is available at https://aka.ms/uniprompt.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient descent with generalized Newton's method</title>
<link>https://arxiv.org/abs/2407.02772</link>
<guid>https://arxiv.org/abs/2407.02772</guid>
<content:encoded><![CDATA[
arXiv:2407.02772v3 Announce Type: replace-cross 
Abstract: We propose the generalized Newton's method (GeN) -- a Hessian-informed approach that applies to any optimizer such as SGD and Adam, and covers the Newton-Raphson method as a sub-case. Our method automatically and dynamically selects the learning rate that accelerates the convergence, without the intensive tuning of the learning rate scheduler. In practice, our method is easily implementable, since it only requires additional forward passes with almost zero computational overhead (in terms of training time and memory cost), if the overhead is amortized over many iterations. We present extensive experiments on language and vision tasks (e.g. GPT and ResNet) to showcase that GeN optimizers match the state-of-the-art performance, which was achieved with carefully tuned learning rate schedulers.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EfficientQAT: Efficient Quantization-Aware Training for Large Language Models</title>
<link>https://arxiv.org/abs/2407.11062</link>
<guid>https://arxiv.org/abs/2407.11062</guid>
<content:encoded><![CDATA[
arXiv:2407.11062v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are crucial in modern natural language processing and artificial intelligence. However, they face challenges in managing their significant memory requirements. Although quantization-aware training (QAT) offers a solution by reducing memory consumption through low-bit representations with minimal accuracy loss, it is impractical due to substantial training resources. To address this, we propose Efficient Quantization-Aware Training (EfficientQAT), a more feasible QAT algorithm. EfficientQAT involves two consecutive phases: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP). To the best of our knowledge, Block-AP is the first method to enable direct training of all parameters in a block-wise manner, reducing accuracy loss in low-bit scenarios by enhancing the solution space during optimization. E2E-QP then trains only the quantization parameters (step sizes) end-to-end, further improving the performance of quantized models by considering interactions among all sub-modules. Extensive experiments demonstrate that EfficientQAT outperforms previous quantization methods across a range of models, including base LLMs, instruction-tuned LLMs, and multimodal LLMs, with scales from 7B to 70B parameters at various quantization bits. For instance, EfficientQAT obtains a 2-bit Llama-2-70B model on a single A100-80GB GPU in 41 hours, with less than 3 points accuracy degradation compared to the full precision (69.48 vs. 72.41). Code is available at https://github.com/OpenGVLab/EfficientQAT.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation</title>
<link>https://arxiv.org/abs/2409.02718</link>
<guid>https://arxiv.org/abs/2409.02718</guid>
<content:encoded><![CDATA[
arXiv:2409.02718v3 Announce Type: replace-cross 
Abstract: Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection</title>
<link>https://arxiv.org/abs/2410.02647</link>
<guid>https://arxiv.org/abs/2410.02647</guid>
<content:encoded><![CDATA[
arXiv:2410.02647v2 Announce Type: replace-cross 
Abstract: Immunogenicity prediction is a central topic in reverse vaccinology for finding candidate vaccines that can trigger protective immune responses. Existing approaches typically rely on highly compressed features and simple model architectures, leading to limited prediction accuracy and poor generalizability. To address these challenges, we introduce VenusVaccine, a novel deep learning solution with a dual attention mechanism that integrates pre-trained latent vector representations of protein sequences and structures. We also compile the most comprehensive immunogenicity dataset to date, encompassing over 7000 antigen sequences, structures, and immunogenicity labels from bacteria, virus, and tumor. Extensive experiments demonstrate that VenusVaccine outperforms existing methods across a wide range of evaluation metrics. Furthermore, we establish a post-hoc validation protocol to assess the practical significance of deep learning models in tackling vaccine design challenges. Our work provides an effective tool for vaccine design and sets valuable benchmarks for future research. The implementation is at https://github.com/songleee/VenusVaccine.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference and Verbalization Functions During In-Context Learning</title>
<link>https://arxiv.org/abs/2410.09349</link>
<guid>https://arxiv.org/abs/2410.09349</guid>
<content:encoded><![CDATA[
arXiv:2410.09349v2 Announce Type: replace-cross 
Abstract: Large language models (LMs) are capable of in-context learning from a few demonstrations (example-label pairs) to solve new tasks during inference. Despite the intuitive importance of high-quality demonstrations, previous work has observed that, in some settings, ICL performance is minimally affected by irrelevant labels (Min et al., 2022). We hypothesize that LMs perform ICL with irrelevant labels via two sequential processes: an inference function that solves the task, followed by a verbalization function that maps the inferred answer to the label space. Importantly, we hypothesize that the inference function is invariant to remappings of the label space (e.g., "true"/"false" to "cat"/"dog"), enabling LMs to share the same inference function across settings with different label words. We empirically validate this hypothesis with controlled layer-wise interchange intervention experiments. Our findings confirm the hypotheses on multiple datasets and tasks (natural language inference, sentiment analysis, and topic classification) and further suggest that the two functions can be localized in specific layers across various open-sourced models, including GEMMA-7B, MISTRAL-7B-V0.3, GEMMA-2-27B, and LLAMA-3.1-70B.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bias Similarity Across Large Language Models</title>
<link>https://arxiv.org/abs/2410.12010</link>
<guid>https://arxiv.org/abs/2410.12010</guid>
<content:encoded><![CDATA[
arXiv:2410.12010v3 Announce Type: replace-cross 
Abstract: Bias in Large Language Models remains a critical concern as these systems are increasingly deployed in high-stakes applications. Yet most fairness evaluations rely on scalar metrics or single-model analysis, overlooking how biases align -- or diverge -- across model families, scales, and tuning strategies. In this work, we reframe bias similarity as a form of functional similarity and evaluate 24 LLMs from four major families on over one million structured prompts spanning four bias dimensions. Our findings uncover that fairness is not strongly determined by model size, architecture, instruction tuning, or openness. Instead, bias behaviors are highly context-dependent and structurally persistent, often resistant to current alignment techniques. Contrary to common assumptions, we find that open-source models frequently match or outperform proprietary models in both fairness and utility. These results call into question the default reliance on proprietary systems and highlight the need for behaviorally grounded, model-specific audits to better understand how bias manifests and endures across the LLM landscape.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation</title>
<link>https://arxiv.org/abs/2410.14971</link>
<guid>https://arxiv.org/abs/2410.14971</guid>
<content:encoded><![CDATA[
arXiv:2410.14971v2 Announce Type: replace-cross 
Abstract: Current EEG/MEG-to-text decoding systems suffer from three key limitations: (1) reliance on teacher-forcing methods, which compromises robustness during inference, (2) sensitivity to session-specific noise, hindering generalization across subjects, and (3) misalignment between brain signals and linguistic representations due to pre-trained language model over-dominance. To overcome these challenges, we propose BrainECHO (Brain signal decoding via vEctor-quantized speCtrogram reconstruction for WHisper-enhanced text generatiOn), a multi-stage framework that employs decoupled representation learning to achieve state-of-the-art performance on both EEG and MEG datasets. Specifically, BrainECHO consists of three stages: (1) Discrete autoencoding, which transforms continuous Mel spectrograms into a finite set of high-quality discrete representations for subsequent stages. (2) Frozen alignment, where brain signal embeddings are mapped to corresponding Mel spectrogram embeddings in a frozen latent space, effectively filtering session-specific noise through vector-quantized reconstruction, yielding a 3.65% improvement in BLEU-4 score. (3) Constrained decoding fine-tuning, which leverages the pre-trained Whisper model for audio-to-text translation, balancing signal adaptation with knowledge preservation, and achieving 74%-89% decoding BLEU scores without excessive reliance on teacher forcing. BrainECHO demonstrates robustness across sentence, session, and subject-independent conditions, passing Gaussian noise tests and showcasing its potential for enhancing language-based brain-computer interfaces.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMScan: Causal Scan for LLM Misbehavior Detection</title>
<link>https://arxiv.org/abs/2410.16638</link>
<guid>https://arxiv.org/abs/2410.16638</guid>
<content:encoded><![CDATA[
arXiv:2410.16638v3 Announce Type: replace-cross 
Abstract: Despite the success of Large Language Models (LLMs) across various fields, their potential to generate untruthful, biased and harmful responses poses significant risks, particularly in critical applications. This highlights the urgent need for systematic methods to detect and prevent such misbehavior. While existing approaches target specific issues such as harmful responses, this work introduces LLMScan, an innovative LLM monitoring technique based on causality analysis, offering a comprehensive solution. LLMScan systematically monitors the inner workings of an LLM through the lens of causal inference, operating on the premise that the LLM's `brain' behaves differently when misbehaving. By analyzing the causal contributions of the LLM's input tokens and transformer layers, LLMScan effectively detects misbehavior. Extensive experiments across various tasks and models reveal clear distinctions in the causal distributions between normal behavior and misbehavior, enabling the development of accurate, lightweight detectors for a variety of misbehavior detection tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation</title>
<link>https://arxiv.org/abs/2410.17462</link>
<guid>https://arxiv.org/abs/2410.17462</guid>
<content:encoded><![CDATA[
arXiv:2410.17462v3 Announce Type: replace-cross 
Abstract: Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title>
<link>https://arxiv.org/abs/2411.19939</link>
<guid>https://arxiv.org/abs/2411.19939</guid>
<content:encoded><![CDATA[
arXiv:2411.19939v3 Announce Type: replace-cross 
Abstract: Safety concerns of Multimodal large language models (MLLMs) have gradually become an important problem in various applications. Surprisingly, previous works indicate a counterintuitive phenomenon that using textual unlearning to align MLLMs achieves comparable safety performances with MLLMs aligned with image text pairs. To explain such a phenomenon, we discover a Visual Safety Information Leakage (VSIL) problem in existing multimodal safety benchmarks, i.e., the potentially risky content in the image has been revealed in the textual query. Thus, MLLMs can easily refuse these sensitive image-text pairs according to textual queries only, leading to unreliable cross-modality safety evaluation of MLLMs. We also conduct a further comparison experiment between textual alignment and multimodal alignment to highlight this drawback. To this end, we construct multimodal Visual Leakless Safety Bench (VLSBench) with 2.2k image-text pairs through an automated data pipeline. Experimental results indicate that VLSBench poses a significant challenge to both open-source and close-source MLLMs, e.g., LLaVA, Qwen2-VL and GPT-4o. Besides, we empirically compare textual and multimodal alignment methods on VLSBench and find that textual alignment is effective enough for multimodal safety scenarios with VSIL, while multimodal alignment is preferable for safety scenarios without VSIL. Code and data are released under https://github.com/AI45Lab/VLSBench
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training-Free Bayesianization for Low-Rank Adapters of Large Language Models</title>
<link>https://arxiv.org/abs/2412.05723</link>
<guid>https://arxiv.org/abs/2412.05723</guid>
<content:encoded><![CDATA[
arXiv:2412.05723v2 Announce Type: replace-cross 
Abstract: Estimating the uncertainty of responses from Large Language Models (LLMs) remains a critical challenge. While recent Bayesian methods have demonstrated effectiveness in quantifying uncertainty through low-rank weight updates, they typically require complex fine-tuning or post-training procedures. In this paper, we propose Training-Free Bayesianization (TFB), a simple yet theoretically grounded framework that efficiently transforms trained low-rank adapters into Bayesian ones without additional training. TFB systematically searches for the maximally acceptable level of variance in the weight posterior, constrained within a family of low-rank isotropic Gaussian distributions. Our theoretical analysis shows that under mild conditions, this search process is equivalent to KL-regularized variational optimization, a generalized form of variational inference. Through comprehensive experiments, we show that TFB achieves superior uncertainty estimation and generalization compared to existing methods while eliminating the need for complex Bayesianization training procedures. Code will be available at https://github.com/Wang-ML-Lab/bayesian-peft.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization</title>
<link>https://arxiv.org/abs/2412.06141</link>
<guid>https://arxiv.org/abs/2412.06141</guid>
<content:encoded><![CDATA[
arXiv:2412.06141v2 Announce Type: replace-cross 
Abstract: The advancement of Large Vision-Language Models (LVLMs) has propelled their application in the medical field. However, Medical LVLMs (Med-LVLMs) encounter factuality challenges due to modality misalignment, where the models prioritize textual knowledge over visual input, leading to hallucinations that contradict information in medical images. Previous attempts to enhance modality alignment in Med-LVLMs through preference optimization have inadequately mitigated clinical relevance in preference data, making these samples easily distinguishable and reducing alignment effectiveness. To address this challenge, we propose MMedPO, a novel multimodal medical preference optimization approach that considers the clinical relevance of preference samples to enhance Med-LVLM alignment. MMedPO curates multimodal preference data by introducing two types of dispreference: (1) plausible hallucinations injected through target Med-LVLMs or GPT-4o to produce medically inaccurate responses, and (2) lesion region neglect achieved through local lesion-noising, disrupting visual understanding of critical areas. We then calculate clinical relevance for each sample based on scores from multiple Med-LLMs and visual tools, and integrate these scores into the preference optimization process as weights, enabling effective alignment. Our experiments demonstrate that MMedPO significantly enhances factual accuracy in Med-LVLMs, achieving substantial improvements over existing preference optimization methods by averaging 14.2% and 51.7% across the Med-VQA and report generation tasks. Our code are available in https://github.com/aiming-lab/MMedPO.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superhuman performance of a large language model on the reasoning tasks of a physician</title>
<link>https://arxiv.org/abs/2412.10849</link>
<guid>https://arxiv.org/abs/2412.10849</guid>
<content:encoded><![CDATA[
arXiv:2412.10849v2 Announce Type: replace-cross 
Abstract: A seminal paper published by Ledley and Lusted in 1959 introduced complex clinical diagnostic reasoning cases as the gold standard for the evaluation of expert medical computing systems, a standard that has held ever since. Here, we report the results of a physician evaluation of a large language model (LLM) on challenging clinical cases against a baseline of hundreds of physicians. We conduct five experiments to measure clinical reasoning across differential diagnosis generation, display of diagnostic reasoning, triage differential diagnosis, probabilistic reasoning, and management reasoning, all adjudicated by physician experts with validated psychometrics. We then report a real-world study comparing human expert and AI second opinions in randomly-selected patients in the emergency room of a major tertiary academic medical center in Boston, MA. We compared LLMs and board-certified physicians at three predefined diagnostic touchpoints: triage in the emergency room, initial evaluation by a physician, and admission to the hospital or intensive care unit. In all experiments--both vignettes and emergency room second opinions--the LLM displayed superhuman diagnostic and reasoning abilities, as well as continued improvement from prior generations of AI clinical decision support. Our study suggests that LLMs have achieved superhuman performance on general medical diagnostic and management reasoning, fulfilling the vision put forth by Ledley and Lusted, and motivating the urgent need for prospective trials.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feedback-Driven Vision-Language Alignment with Minimal Human Supervision</title>
<link>https://arxiv.org/abs/2501.04568</link>
<guid>https://arxiv.org/abs/2501.04568</guid>
<content:encoded><![CDATA[
arXiv:2501.04568v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have demonstrated remarkable potential in integrating visual and linguistic information, but their performance is often constrained by the need for extensive, high-quality image-text training data. Curation of these image-text pairs is both time-consuming and computationally expensive. To address this challenge, we introduce SVP (Sampling-based Visual Projection), a novel framework that enhances vision-language alignment without relying on manually curated text-image pairs or preference annotation. SVP leverages a small set of manually selected images, self-captioning and a pre-trained grounding model as a feedback mechanism to elicit latent information in VLMs. We evaluate our approach across six key areas: captioning, referring, visual question answering, multitasking, hallucination control, and object recall. Results demonstrate significant improvements, including a 14 % average improvement in captioning tasks, up to 12 % increase in object recall, and significantly reduced hallucinations, while maintaining question-answering capabilities. Using SVP, a small VLM achieves hallucination reductions similar to a model five times larger, while a VLM with initially poor referring capabilities more than doubles its performance, approaching parity with a model twice its size.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling</title>
<link>https://arxiv.org/abs/2502.00814</link>
<guid>https://arxiv.org/abs/2502.00814</guid>
<content:encoded><![CDATA[
arXiv:2502.00814v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved considerable success in aligning large language models (LLMs) by modeling human preferences with a learnable reward model and employing a reinforcement learning algorithm to maximize the reward model's scores. However, these reward models are susceptible to exploitation through various superficial confounding factors, with length bias emerging as a particularly significant concern. Moreover, while the pronounced impact of length bias on preference modeling suggests that LLMs possess an inherent sensitivity to length perception, our preliminary investigations reveal that fine-tuned LLMs consistently struggle to adhere to explicit length instructions. To address these two limitations, we propose a novel framework wherein the reward model explicitly differentiates between human semantic preferences and response length requirements. Specifically, we introduce a $\textbf{R}$esponse-$\textbf{c}$onditioned $\textbf{B}$radley-$\textbf{T}$erry (Rc-BT) model that enhances the model's capability in length bias mitigating and length instruction following, through training on our augmented dataset. Furthermore, we propose the Rc-RM and Rc-DPO algorithm to leverage the Rc-BT model for reward modeling and direct policy optimization (DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to length instructions. Extensive experiments across various foundational models and datasets demonstrate the effectiveness and generalizability of our approach.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Context Length Scaling and Bounds for Language Models</title>
<link>https://arxiv.org/abs/2502.01481</link>
<guid>https://arxiv.org/abs/2502.01481</guid>
<content:encoded><![CDATA[
arXiv:2502.01481v3 Announce Type: replace-cross 
Abstract: Long Context Language Models have drawn great attention in the past few years. There has been work discussing the impact of long context on Language Model performance: some find that long irrelevant context could harm performance, while some experimentally summarize loss reduction by relevant long context as Scaling Laws. This calls for a more thorough understanding on how long context impacts Language Modeling. In this work, we (1) propose a clean and effective theoretical framework for explaining the impact of context length on Language Modeling, from an Intrinsic Space perspective; and (2) conduct experiments on natural language and synthetic data, validating our proposed theoretical assumptions and deductions. Our theoretical framework can provide practical insights such as establishing that training dataset size dictates an optimal context length and bounds context length scaling for certain cases. We hope our work may inspire new long context Language Models, as well as future work studying Physics for Language Models. Code for our experiments is available at: https://github.com/JingzheShi/NLPCtlScalingAndBounds.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the true depth of LLMs</title>
<link>https://arxiv.org/abs/2502.02790</link>
<guid>https://arxiv.org/abs/2502.02790</guid>
<content:encoded><![CDATA[
arXiv:2502.02790v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities at the cost of high compute requirements. Recent studies have demonstrated that intermediate layers in LLMs can be removed or reordered without substantial accuracy loss; however, this insight has not yet been exploited to improve inference efficiency. Leveraging observed layer independence, we propose a novel method that groups consecutive layers into pairs evaluated in parallel, effectively restructuring the computational graph to enhance parallelism. Without requiring retraining or fine-tuning, this approach achieves an inference throughput improvement of 1.05x-1.20x on standard benchmarks, retaining 95\%-99\% of the original model accuracy. Empirical results demonstrate the practicality of this method in significantly reducing inference cost for large-scale LLM deployment. Additionally, we demonstrate that modest performance degradation can be substantially mitigated through lightweight fine-tuning, further enhancing the method's applicability.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Potential of Encoder-free Architectures in 3D LMMs</title>
<link>https://arxiv.org/abs/2502.09620</link>
<guid>https://arxiv.org/abs/2502.09620</guid>
<content:encoded><![CDATA[
arXiv:2502.09620v2 Announce Type: replace-cross 
Abstract: Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encoder not meeting the semantic needs of Large Language Models (LLMs). We identify key aspects for 3D LMMs to remove the encoder and enable the LLM to assume the role of the 3D encoder: 1) We propose the LLM-embedded Semantic Encoding strategy in the pre-training stage, exploring the effects of various point cloud self-supervised losses. And we present the Hybrid Semantic Loss to extract high-level semantics. 2) We introduce the Hierarchical Geometry Aggregation strategy in the instruction tuning stage. This incorporates inductive bias into the LLM layers to focus on the local details of the point clouds. To the end, we present the first Encoder-free 3D LMM, ENEL. Our 7B model rivals the current state-of-the-art model, ShapeLLM-13B, achieving 55.10%, 50.98%, and 43.10% on the classification, captioning, and VQA tasks, respectively. Our results demonstrate that the encoder-free architecture is highly promising for replacing encoder-based architectures in the field of 3D understanding. The code is released at https://github.com/Ivan-Tang-3D/ENEL
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning</title>
<link>https://arxiv.org/abs/2502.11799</link>
<guid>https://arxiv.org/abs/2502.11799</guid>
<content:encoded><![CDATA[
arXiv:2502.11799v2 Announce Type: replace-cross 
Abstract: Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARS: Automatic Routing Solver with Large Language Models</title>
<link>https://arxiv.org/abs/2502.15359</link>
<guid>https://arxiv.org/abs/2502.15359</guid>
<content:encoded><![CDATA[
arXiv:2502.15359v3 Announce Type: replace-cross 
Abstract: Real-world Vehicle Routing Problems (VRPs) are characterized by a variety of practical constraints, making manual solver design both knowledge-intensive and time-consuming. Although there is increasing interest in automating the design of routing algorithms, existing research has explored only a limited array of VRP variants and fails to adequately address the complex and prevalent constraints encountered in real-world situations. To fill this gap, this paper introduces RoutBench, a benchmark of 1,000 VRP variants derived from 24 attributes, for evaluating the effectiveness of automatic routing solvers in addressing complex constraints. Along with RoutBench, we present the Automatic Routing Solver (ARS), which employs Large Language Model (LLM) agents to enhance a backbone algorithm framework by automatically generating constraint-aware heuristic code, based on problem descriptions and several representative constraints selected from a database. Our experiments show that ARS outperforms state-of-the-art LLM-based methods and commonly used solvers, automatically solving 91.67% of common VRPs and achieving at least a 30% improvement across all benchmarks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2502.16565</link>
<guid>https://arxiv.org/abs/2502.16565</guid>
<content:encoded><![CDATA[
arXiv:2502.16565v2 Announce Type: replace-cross 
Abstract: Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore</title>
<link>https://arxiv.org/abs/2502.20034</link>
<guid>https://arxiv.org/abs/2502.20034</guid>
<content:encoded><![CDATA[
arXiv:2502.20034v2 Announce Type: replace-cross 
Abstract: Recently, Large Vision-Language Models (LVLMs) show remarkable performance across various domains. However, these models suffer from object hallucination. This study revisits the previous claim that the primary cause of such hallucination lies in the limited representational capacity of the vision encoder. Our analysis reveals that the capacity of the vision encoder itself is already adequate for detecting object hallucination. Based on this insight, we propose a Fine-grained CLIPScore (F-CLIPScore), a simple yet effective evaluation metric that enhances object-level granularity by incorporating text embeddings at the noun level. Evaluations on the OHD-Caps benchmark show that F-CLIPScore significantly outperforms conventional CLIPScore in accuracy by a large margin of 39.6\% without additional training. We further demonstrate that F-CLIPScore-based data filtering reduces object hallucination in LVLMs (4.9\% in POPE).
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2502.20854</link>
<guid>https://arxiv.org/abs/2502.20854</guid>
<content:encoded><![CDATA[
arXiv:2502.20854v3 Announce Type: replace-cross 
Abstract: The integration of Knowledge Graphs (KGs) into the Retrieval Augmented Generation (RAG) framework has attracted significant interest, with early studies showing promise in mitigating hallucinations and improving model accuracy. However, a systematic understanding and comparative analysis of the rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the foundation for systematically answering the question of when and how to use KG-RAG by analyzing their performance in various application scenarios associated with different technical configurations. After outlining the mind map using KG-RAG framework and summarizing its popular pipeline, we conduct a pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG methods across 9 datasets in diverse domains and scenarios, analyzing the impact of 9 KG-RAG configurations in combination with 17 LLMs, and combining Metacognition with KG-RAG as a pilot attempt. Our results underscore the critical role of appropriate application conditions and optimal configurations of KG-RAG components.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</title>
<link>https://arxiv.org/abs/2503.01711</link>
<guid>https://arxiv.org/abs/2503.01711</guid>
<content:encoded><![CDATA[
arXiv:2503.01711v4 Announce Type: replace-cross 
Abstract: Personalized product search aims to retrieve and rank items that match users' preferences and search intent. Despite their effectiveness, existing approaches typically assume that users' query fully captures their real motivation. However, our analysis of a real-world e-commerce platform reveals that users often engage in relevant consultations before searching, indicating they refine intents through consultations based on motivation and need. The implied motivation in consultations is a key enhancing factor for personalized search. This unexplored area comes with new challenges including aligning contextual motivations with concise queries, bridging the category-text gap, and filtering noise within sequence history. To address these, we propose a Motivation-Aware Personalized Search (MAPS) method. It embeds queries and consultations into a unified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE) to prioritize critical semantics, and introduces dual alignment: (1) contrastive learning aligns consultations, reviews, and product features; (2) bidirectional attention integrates motivation-aware embeddings with user preferences. Extensive experiments on real and synthetic data show MAPS outperforms existing methods in both retrieval and ranking tasks.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding</title>
<link>https://arxiv.org/abs/2503.13139</link>
<guid>https://arxiv.org/abs/2503.13139</guid>
<content:encoded><![CDATA[
arXiv:2503.13139v2 Announce Type: replace-cross 
Abstract: Understanding long video content is a complex endeavor that often relies on densely sampled frame captions or end-to-end feature selectors, yet these techniques commonly overlook the logical relationships between textual queries and visual elements. In practice, computational constraints necessitate coarse frame subsampling, a challenge analogous to "finding a needle in a haystack." To address this issue, we introduce a semantics-driven search framework that reformulates keyframe selection under the paradigm of Visual Semantic-Logical Search. Specifically, we systematically define four fundamental logical dependencies: 1) spatial co-occurrence, 2) temporal proximity, 3) attribute dependency, and 4) causal order. These relations dynamically update frame sampling distributions through an iterative refinement process, enabling context-aware identification of semantically critical frames tailored to specific query requirements. Our method establishes new SOTA performance on the manually annotated benchmark in key-frame selection metrics. Furthermore, when applied to downstream video question-answering tasks, the proposed approach demonstrates the best performance gains over existing methods on LongVideoBench and Video-MME, validating its effectiveness in bridging the logical gap between textual queries and visual-temporal reasoning. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation</title>
<link>https://arxiv.org/abs/2503.18225</link>
<guid>https://arxiv.org/abs/2503.18225</guid>
<content:encoded><![CDATA[
arXiv:2503.18225v2 Announce Type: replace-cross 
Abstract: Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, provide greater robustness but are limited to extremely low-rank adaptations and fixed-strength transformations, reducing their adaptation expressive power. In this work, we propose Decoupled Low-rank Adaptation (DeLoRA), a novel finetuning method that normalizes and scales learnable low-rank matrices. By bounding the distance of the transformation, DeLoRA effectively decouples the angular learning from the adaptation strength, enhancing robustness without compromising performance. Through evaluations on subject-driven image generation, natural language understanding, and instruction tuning, we show that DeLoRA matches or surpasses performance of competing PEFT methods, while exhibiting stronger robustness. Code is available at https://github.com/ExplainableML/DeLoRA.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MaintainCoder: Maintainable Code Generation Under Dynamic Requirements</title>
<link>https://arxiv.org/abs/2503.24260</link>
<guid>https://arxiv.org/abs/2503.24260</guid>
<content:encoded><![CDATA[
arXiv:2503.24260v2 Announce Type: replace-cross 
Abstract: Modern code generation has made significant strides in functional correctness and execution efficiency. However, these systems often overlook a critical dimension in real-world software development: \textit{maintainability}. To handle dynamic requirements with minimal rework, we propose \textbf{MaintainCoder} as a pioneering solution. It integrates the Waterfall model, design patterns, and multi-agent collaboration to systematically enhance cohesion, reduce coupling, achieving clear responsibility boundaries and better maintainability. We also introduce \textbf{MaintainBench}, a benchmark comprising requirement changes and novel dynamic metrics on maintenance efforts. Experiments demonstrate that existing code generation methods struggle to meet maintainability standards when requirements evolve. In contrast, MaintainCoder improves dynamic maintainability metrics by more than 60\% with even higher correctness of initial codes. Furthermore, while static metrics fail to accurately reflect maintainability and even contradict each other, our proposed dynamic metrics exhibit high consistency. Our work not only provides the foundation for maintainable code generation, but also highlights the need for more realistic and comprehensive code generation research.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Effectively Controlling Reasoning Models through Thinking Intervention</title>
<link>https://arxiv.org/abs/2503.24370</link>
<guid>https://arxiv.org/abs/2503.24370</guid>
<content:encoded><![CDATA[
arXiv:2503.24370v2 Announce Type: replace-cross 
Abstract: Reasoning-enhanced large language models (LLMs) explicitly generate intermediate reasoning steps prior to generating final answers, helping the model excel in complex problem-solving. In this paper, we demonstrate that this emerging generation framework offers a unique opportunity for more fine-grained control over model behavior. We propose Thinking Intervention, a novel paradigm designed to explicitly guide the internal reasoning processes of LLMs by strategically inserting or revising specific thinking tokens. We find that the Thinking Intervention paradigm enhances the capabilities of reasoning models across a wide range of tasks, including instruction following on IFEval, instruction hierarchy on SEP, and safety alignment on XSTest and SorryBench. Our results demonstrate that Thinking Intervention significantly outperforms baseline prompting approaches, achieving up to 6.7% accuracy gains in instruction-following scenarios, 15.4% improvements in reasoning about instruction hierarchies, and a 40.0% increase in refusal rates for unsafe prompts using open-source DeepSeek R1 models. Overall, our work opens a promising new research avenue for controlling reasoning LLMs.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation</title>
<link>https://arxiv.org/abs/2504.04453</link>
<guid>https://arxiv.org/abs/2504.04453</guid>
<content:encoded><![CDATA[
arXiv:2504.04453v2 Announce Type: replace-cross 
Abstract: Unlocking the next generation of biotechnology and therapeutic innovation demands overcoming the inherent complexity and resource-intensity of conventional protein engineering methods. Recent GenAI-powered computational techniques often rely on the availability of the target protein's 3D structures and specific binding sites to generate high-affinity binders, constraints exhibited by models such as AlphaProteo and RFdiffusion. In this work, we explore the use of Protein Language Models (pLMs) for high-affinity binder generation. We introduce Prot42, a novel family of Protein Language Models (pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing deep evolutionary, structural, and functional insights through an advanced auto-regressive, decoder-only architecture inspired by breakthroughs in natural language processing, Prot42 dramatically expands the capabilities of computational protein design based on language only. Remarkably, our models handle sequences up to 8,192 amino acids, significantly surpassing standard limitations and enabling precise modeling of large proteins and complex multi-domain sequences. Demonstrating powerful practical applications, Prot42 excels in generating high-affinity protein binders and sequence-specific DNA-binding proteins. Our innovative models are publicly available, offering the scientific community an efficient and precise computational toolkit for rapid protein engineering.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Signatures of human-like processing in Transformer forward passes</title>
<link>https://arxiv.org/abs/2504.14107</link>
<guid>https://arxiv.org/abs/2504.14107</guid>
<content:encoded><![CDATA[
arXiv:2504.14107v2 Announce Type: replace-cross 
Abstract: Modern AI models are increasingly being used as theoretical tools to study human cognition. One dominant approach is to evaluate whether human-derived measures are predicted by a model's output: that is, the end-product of a forward pass. However, recent advances in mechanistic interpretability have begun to reveal the internal processes that give rise to model outputs, raising the question of whether models might use human-like processing strategies. Here, we investigate the relationship between real-time processing in humans and layer-time dynamics of computation in Transformers, testing 20 open-source models in 6 domains. We first explore whether forward passes show mechanistic signatures of competitor interference, taking high-level inspiration from cognitive theories. We find that models indeed appear to initially favor a competing incorrect answer in the cases where we would expect decision conflict in humans. We then systematically test whether forward-pass dynamics predict signatures of processing in humans, above and beyond properties of the model's output probability distribution. We find that dynamic measures improve prediction of human processing measures relative to static final-layer measures. Moreover, across our experiments, larger models do not always show more human-like processing patterns. Our work suggests a new way of using AI models to study human cognition: not just as a black box mapping stimuli to responses, but potentially also as explicit processing models.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning</title>
<link>https://arxiv.org/abs/2504.14858</link>
<guid>https://arxiv.org/abs/2504.14858</guid>
<content:encoded><![CDATA[
arXiv:2504.14858v2 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) has become a widely adopted paradigm for enabling knowledge-grounded large language models (LLMs). However, standard RAG pipelines often fail to ensure that model reasoning remains consistent with the evidence retrieved, leading to factual inconsistencies or unsupported conclusions. In this work, we reinterpret RAG as Retrieval-Augmented Reasoning and identify a central but underexplored problem: \textit{Reasoning Misalignment}-the divergence between an LLM's internal reasoning trajectory and the evidential constraints provided by retrieval. To address this issue, we propose \textsc{AlignRAG}, a novel iterative framework grounded in Critique-Driven Alignment (CDA). At the heart of \textsc{AlignRAG} lies a \textit{contrastive critique synthesis} mechanism that generates retrieval-sensitive critiques while mitigating self-bias. This mechanism trains a dedicated retrieval-augmented \textit{Critic Language Model (CLM)} using labeled critiques that distinguish between evidence-aligned and misaligned reasoning. Alignment signals for supervision are obtained through self-supervised or externally guided labeling strategies. The resulting CLM is explicitly optimized for evidence sensitivity, enabling it to detect and revise reasoning errors during inference without relying solely on self-generated feedback. Empirical evaluations show that our 8B-parameter CLM improves performance over the Self-Refine baseline by 12.1\% on out-of-domain tasks and outperforms a standard 72B-parameter CLM by 2.2\%, while remaining compatible with existing RAG architectures as a plug-and-play module. Overall, AlignRAG offers a principled solution for aligning model reasoning with retrieved evidence, substantially improving the factual reliability and robustness of RAG systems.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v2 Announce Type: replace-cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
arXiv:2504.16828v2 Announce Type: replace-cross 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks</title>
<link>https://arxiv.org/abs/2505.00234</link>
<guid>https://arxiv.org/abs/2505.00234</guid>
<content:encoded><![CDATA[
arXiv:2505.00234v3 Announce Type: replace-cross 
Abstract: Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.
]]></content:encoded>
<pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Artificial Intelligence Bias on English Language Learners in Automatic Scoring</title>
<link>https://arxiv.org/abs/2505.10643</link>
<guid>https://arxiv.org/abs/2505.10643</guid>
<content:encoded><![CDATA[
<div> bias, disparities, English Language Learners, automatic scoring systems, BERT

Summary:
The study investigated scoring biases and disparities in automatic scoring systems for middle school science assessments of English Language Learners (ELLs). By fine-tuning BERT with different training datasets, the researchers found no bias or disparities when the dataset size was sufficient (30,000 ELL responses or 1,000 ELL responses). However, concerns arose when the sample size was limited (200 ELL responses). Scoring accuracy was compared using Friedman tests, and Mean Score Gaps (MSGs) were measured to identify disparities between ELLs and non-ELLs. The study suggests that ensuring a large enough training dataset is crucial to avoid bias and disparities in scoring ELLs' written responses. <div>
arXiv:2505.10643v1 Announce Type: new 
Abstract: This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?</title>
<link>https://arxiv.org/abs/2505.10714</link>
<guid>https://arxiv.org/abs/2505.10714</guid>
<content:encoded><![CDATA[
<div> climate variables, foundation models, GeoGrid-Bench, geo-spatial data, scientific research
<br />
Summary: 
The article introduces GeoGrid-Bench, a benchmark created to evaluate foundation models' ability to comprehend geo-spatial data in a grid format. Geo-spatial datasets present unique challenges due to their dense numerical values, spatial and temporal dependencies, and multimodal representations. The benchmark encompasses real-world data on 16 climate variables across 150 locations and extended time frames, with approximately 3,200 question-answer pairs generated from expert-curated templates. These tasks simulate scenarios encountered by scientists, ranging from basic queries to complex spatiotemporal comparisons. Vision-language models demonstrated superior performance overall, with a detailed analysis of strengths and limitations in various geo-spatial tasks provided. GeoGrid-Bench aims to enhance understanding of how foundation models can support scientific research in the analysis of geo-spatial data. 
<br /><br /> <div>
arXiv:2505.10714v1 Announce Type: new 
Abstract: We present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundation models can support scientific research in this domain, GeoGrid-Bench features large-scale, real-world data covering 16 climate variables across 150 locations and extended time frames. The benchmark includes approximately 3,200 question-answer pairs, systematically generated from 8 domain expert-curated templates to reflect practical tasks encountered by human scientists. These range from basic queries at a single location and time to complex spatiotemporal comparisons across regions and periods. Our evaluation reveals that vision-language models perform best overall, and we provide a fine-grained analysis of the strengths and limitations of different foundation models in different geo-spatial tasks. This benchmark offers clearer insights into how foundation models can be effectively applied to geo-spatial data analysis and used to support scientific research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment</title>
<link>https://arxiv.org/abs/2505.10717</link>
<guid>https://arxiv.org/abs/2505.10717</guid>
<content:encoded><![CDATA[
<div> Keywords: small language models, clinical settings, biomedical domain adaptation, clinical data, MediPhi collection

Summary: 
Small language models (SLMs) like GPT-4 offer a cost-effective alternative to large language models but require adaptation to the biomedical domain, a challenging task due to limited capacity. To address this, a new framework has been proposed for adapting SLMs into high-performing clinical models. The framework involves pre-instruction tuning of experts on medical and clinical corpora, model merging, and alignment of clinical tasks. The resulting MediPhi collection of SLMs outperforms the base model on various clinical tasks, including medical entities, radiology reports, and ICD-10 coding. Additionally, the CLUE benchmark has been extended to CLUE+ to cover most clinical tasks, doubling its size. The expert models are unified into MediPhi via model merging, achieving gains across benchmarks. Furthermore, a synthetic dataset called MediFlow has been created for 14 medical NLP tasks, leading to further performance improvements through alignment and fine-tuning. <div>
arXiv:2505.10717v1 Announce Type: new 
Abstract: High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI-enhanced semantic feature norms for 786 concepts</title>
<link>https://arxiv.org/abs/2505.10718</link>
<guid>https://arxiv.org/abs/2505.10718</guid>
<content:encoded><![CDATA[
<div> Keywords: Semantic feature norms, human conceptual knowledge, large language models, NOVA dataset, cognitive science research

Summary:
NOVA, a dataset combining human-generated feature norms with responses from large language models (LLMs), enhances concept/feature coverage without sacrificing quality. The dataset, NOVA: Norms Optimized Via AI, demonstrates higher feature density and overlap among concepts compared to human-only norms and word-embedding models. AI-enhanced norms also outperform traditional models in predicting semantic similarity judgments. The study showcases the richness of human conceptual knowledge beyond previous norm datasets. By validating LLM responses with reliable human judgments, researchers can leverage these powerful tools for cognitive science research.
<br /><br />Summary: 
- NOVA dataset combines human and AI-generated feature norms
- Higher feature density and overlap among concepts in NOVA dataset
- NOVA outperforms human-only norms and word-embedding models
- AI-enhanced norms excel in predicting semantic similarity judgments
- LLMs can be valuable tools for cognitive science research <div>
arXiv:2505.10718v1 Announce Type: new 
Abstract: Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tracr-Injection: Distilling Algorithms into Pre-trained Language Models</title>
<link>https://arxiv.org/abs/2505.10719</link>
<guid>https://arxiv.org/abs/2505.10719</guid>
<content:encoded><![CDATA[
<div> Keywords: transformer architecture, RASP, tracr-injection, language model, symbolic abilities

Summary: 
The article discusses the formal characterization of symbolic abilities in the transformer architecture through a programming language called RASP. It highlights a mismatch between theoretical capabilities of transformers and their practical learnability from unsupervised data. The authors introduce tracr-injection, a method to distill RASP algorithms into pre-trained language models, demonstrating improved out-of-distribution performance compared to baseline. They showcase the injection of three algorithms into a language model, creating an interpretable subspace within the model's residual stream. The method decodes variables present in the RASP algorithm code, indicating a more symbolic mechanism at work in the model. The release of the experiment code allows for further exploration and replication of the results.<br /><br />Summary: <div>
arXiv:2505.10719v1 Announce Type: new 
Abstract: Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model's residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out of distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization</title>
<link>https://arxiv.org/abs/2505.10736</link>
<guid>https://arxiv.org/abs/2505.10736</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Prompt Optimization, Iterative evaluation data selection, Real-time Model Performance, Coreset Selection

Summary:
IPOMP is a new automated technique for optimizing prompts in Large Language Models (LLMs) that addresses the limitations of existing methods by selecting representative and diverse samples using semantic clustering and boundary analysis. It then iteratively refines the selected samples based on real-time model performance data. The evaluations on the BIG-bench dataset demonstrate a significant improvement in effectiveness (1.6% to 5.3%) and stability (at least 57%) compared to state-of-the-art baseline methods, with minimal computational overhead. The results also show that the real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods. Overall, IPOMP offers a more efficient and effective solution for prompt optimization in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.10736v1 Announce Type: new 
Abstract: Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval</title>
<link>https://arxiv.org/abs/2505.10740</link>
<guid>https://arxiv.org/abs/2505.10740</guid>
<content:encoded><![CDATA[
<div> shared task, multilingual claim retrieval, machine learning, online disinformation, fact-checking 
Summary: 
The article discusses a shared task on multilingual claim retrieval at SemEval 2025, focusing on identifying fact-checked claims across different languages, addressing the neglect of low-resource languages in the field of online disinformation. The task includes a monolingual track and a crosslingual track, with 179 participants and 52 test submissions. The best-performing systems and common approaches in both subtracks are reported, providing insights into multilingual claim retrieval and automated fact-checking. The shared task dataset and participating systems aim to support future research in this field. <div>
arXiv:2505.10740v1 Announce Type: new 
Abstract: The rapid spread of online disinformation presents a global challenge, and machine learning has been widely explored as a potential solution. However, multilingual settings and low-resource languages are often neglected in this field. To address this gap, we conducted a shared task on multilingual claim retrieval at SemEval 2025, aimed at identifying fact-checked claims that match newly encountered claims expressed in social media posts across different languages. The task includes two subtracks: (1) a monolingual track, where social posts and claims are in the same language, and (2) a crosslingual track, where social posts and claims might be in different languages. A total of 179 participants registered for the task contributing to 52 test submissions. 23 out of 31 teams have submitted their system papers. In this paper, we report the best-performing systems as well as the most common and the most effective approaches across both subtracks. This shared task, along with its dataset and participating systems, provides valuable insights into multilingual claim retrieval and automated fact-checking, supporting future research in this field.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ranked Voting based Self-Consistency of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10772</link>
<guid>https://arxiv.org/abs/2505.10772</guid>
<content:encoded><![CDATA[
<div> Ranked voting, chain-of-thought reasoning, self-consistency, multiple-choice, open-ended <br />
Summary: 

This work introduces a novel approach to chain-of-thought reasoning by generating ranked answers and using ranked voting methods for enhanced reliability of self-consistency. The proposed method outperforms existing techniques by considering multiple potential answers in each reasoning process and incorporating them into the voting process. Three ranked voting methods - Instant-runoff voting, Borda count voting, and mean reciprocal rank voting - are utilized and validated on six datasets comprising both multiple-choice and open-ended question-answering tasks. Results show significant improvement in reasoning performance, highlighting the effectiveness of leveraging ranked answers and implementing ranked voting. The code for the proposed method is available on GitHub for further exploration and experimentation. <br /> <div>
arXiv:2505.10772v1 Announce Type: new 
Abstract: Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest "self-consistency" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Systematic Analysis of Base Model Choice for Reward Modeling</title>
<link>https://arxiv.org/abs/2505.10775</link>
<guid>https://arxiv.org/abs/2505.10775</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, human feedback, reward modeling, large language models, base model <br />
Summary: 
Reinforcement learning from human feedback and reward modeling are crucial for training large language models. The choice of the base model significantly affects reward modeling performance, with potential improvements of up to 14%. Benchmarks play a key role in predicting downstream performance, and a combination of select benchmarks can enhance model selection by 18% on average. Post-training steps also impact final performance, and using estimated data distributions can reduce prediction errors. <div>
arXiv:2505.10775v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10792</link>
<guid>https://arxiv.org/abs/2505.10792</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Finetune-RAG, factual accuracy, hallucinations, LLM-as-a-judge

Summary: 
Retrieval-Augmented Generation (RAG) aims to enhance the factuality of large language models (LLMs) by grounding their outputs in retrieved documents. However, challenges persist in achieving perfect retrieval, leading to the risk of hallucinations when irrelevant information is incorporated into LLM outputs. To address this, the Finetune-RAG approach is proposed, involving fine-tuning with a specialized dataset mimicking real-world imperfections. This method demonstrates a significant improvement in factual accuracy by 21.2% compared to the base model. Additionally, a novel evaluation pipeline called Bench-RAG is introduced to assess LLM performance in imperfect retrieval scenarios. The codebase and dataset associated with this research are made openly available for community use. 

<br /><br />Summary: <div>
arXiv:2505.10792v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose a Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets</title>
<link>https://arxiv.org/abs/2505.10798</link>
<guid>https://arxiv.org/abs/2505.10798</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, text extraction, AffilKG, dataset, social science research
<br />
Summary: 
The article introduces AffilKG, a collection of datasets that pair complete book scans with large, labeled knowledge graphs. These datasets feature affiliation graphs capturing relationships between Person and Organization entities, useful for studying social phenomena. The datasets enable benchmarking of extraction errors in graph-level analyses and validation of KG extraction methods for social science research. Preliminary experiments show varying model performance across datasets, highlighting AffilKG's importance in evaluating accuracy for downstream analysis. <div>
arXiv:2505.10798v1 Announce Type: new 
Abstract: When knowledge graphs (KGs) are automatically extracted from text, are they accurate enough for downstream analysis? Unfortunately, current annotated datasets can not be used to evaluate this question, since their KGs are highly disconnected, too small, or overly complex. To address this gap, we introduce AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six datasets that are the first to pair complete book scans with large, labeled knowledge graphs. Each dataset features affiliation graphs, which are simple KGs that capture Member relationships between Person and Organization entities -- useful in studies of migration, community interactions, and other social phenomena. In addition, three datasets include expanded KGs with a wider variety of relation types. Our preliminary experiments demonstrate significant variability in model performance across datasets, underscoring AffilKG's ability to enable two critical advances: (1) benchmarking how extraction errors propagate to graph-level analyses (e.g., community structure), and (2) validating KG extraction methods for real-world social science research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances</title>
<link>https://arxiv.org/abs/2505.10829</link>
<guid>https://arxiv.org/abs/2505.10829</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource languages, Large Language Models, Retrieval-Augmented Generation, translation challenges, cultural preservation<br />
Summary:<br />
- The study examines the difficulties of translating low-resource languages using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG). 
- Different model configurations were tested on Hakka translations, with the highest BLEU score of 31% achieved by RAG with Gemini 2.0.
- Model 4, combining retrieval and advanced language modeling, proved to be the most effective, enhancing lexical coverage and grammatical coherence.
- Model 3, a two-stage method combining dictionary outputs and Gemini 2.0, achieved a BLEU score of 26%, showcasing the value of iterative correction and the challenges posed by domain-specific expressions.
- Static dictionary-based approaches struggled with context-sensitive content, highlighting the limitations of relying solely on predefined resources.
<br /><br />Summary: This research delves into the complexities of translating low-resource languages by leveraging Large Language Models and Retrieval-Augmented Generation. The findings emphasize the importance of curated resources, domain knowledge, and ethical collaboration with local communities to improve translation accuracy and fluency while also supporting cultural preservation. <div>
arXiv:2505.10829v1 Announce Type: new 
Abstract: This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0). The best-performing model (Model 4) combined retrieval and advanced language modeling, improving lexical coverage, particularly for specialized or culturally nuanced terms, and enhancing grammatical coherence. A two-stage method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU score of 26%, highlighting iterative correction's value and the challenges of domain-specific expressions. Static dictionary-based approaches struggled with context-sensitive content, demonstrating the limitations of relying solely on predefined resources. These results emphasize the need for curated resources, domain knowledge, and ethical collaboration with local communities, offering a framework that improves translation accuracy and fluency while supporting cultural preservation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL</title>
<link>https://arxiv.org/abs/2505.10832</link>
<guid>https://arxiv.org/abs/2505.10832</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, adaptive thinking, AutoThink, reinforcement learning, scalability <br />
Summary: <br />
Large reasoning models (LRMs) excel in producing detailed reasoning sequences, but this can lead to unnecessary computational overhead. To address this, the concept of adaptive thinking is introduced to LRMs through the AutoThink framework. By dynamically deciding when to engage in explicit reasoning based on problem complexity, AutoThink optimizes the trade-off between accuracy and efficiency. By leveraging a stochastic trigger in the prompt, AutoThink learns to activate explicit reasoning only when necessary and default to succinct responses for simpler tasks. Through a multi-stage reinforcement learning approach, AutoThink improves accuracy by 6.4 percent while reducing token usage by 52 percent on a specific model. This scalable and adaptive reasoning paradigm enhances the performance of LRMs on various mathematical benchmarks, making it a valuable addition to the field of AI reasoning models. <br /> <div>
arXiv:2505.10832v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis ("...") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs</title>
<link>https://arxiv.org/abs/2505.10836</link>
<guid>https://arxiv.org/abs/2505.10836</guid>
<content:encoded><![CDATA[
<div> Keywords: social media, event detection, multimodal models, generative models, supervised methods 

Summary: 
In this paper, the authors investigate the challenges of detecting events on social media platforms, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. They compare various models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o and LLaVA. The study shows that multimodal approaches outperform unimodal models, but generative models, despite having a large number of parameters, fall behind supervised methods in terms of precision. It is also noted that generative models have difficulty accurately generating event classes, leading to lower performance compared to instruction-tuned models. Through error analysis, the authors find that generative models excel in handling common social media issues such as leet speak and text elongation, which are challenging for supervised approaches. <div>
arXiv:2505.10836v1 Announce Type: new 
Abstract: In this paper, we study the challenges of detecting events on social media, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. We employ a range of models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o, and LLaVA. Additionally, we also study the effect of providing multimodal generative models (such as GPT-4o) with a single modality to assess their efficacy. Our results indicate that while multimodal approaches notably outperform unimodal counterparts, generative approaches despite having a large number of parameters, lag behind supervised methods in precision. Furthermore, we also found that they lag behind instruction-tuned models because of their inability to generate event classes correctly. During our error analysis, we discovered that common social media issues such as leet speak, text elongation, etc. are effectively handled by generative approaches but are hard to tackle using supervised approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?</title>
<link>https://arxiv.org/abs/2505.10862</link>
<guid>https://arxiv.org/abs/2505.10862</guid>
<content:encoded><![CDATA[
<div> clocks, multimodal large language models, GPT-4.1, fine-tuning, limitations

Summary:
Multimodal Large Language Models (MLLMs) struggle to tell the time on analog clocks, likely due to a lack of diverse clock images in their training data. In this study, the researchers investigate this issue using GPT-4.1 and explore whether fine-tuning can improve clock-reading capabilities. Results show progress in time-reading, indicating some level of learning. However, it is uncertain if the models truly understand time or simply recognize patterns in their training sets. Testing the models with various clocks highlights their limitations in abstracting and generalizing concepts. This study sheds light on the challenges MLLMs face in understanding and interpreting analog clocks, emphasizing the need for diverse and comprehensive training data for improved performance. 

<br /><br />Summary: <div>
arXiv:2505.10862v1 Announce Type: new 
Abstract: Multimodal Large Language Models which can answer complex questions on an image struggle to tell the time on analog clocks. This is probably due to the lack of images with clocks at different times in their training set. In this work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand why MLLMs fail to tell the time and whether fine-tuning can solve the problem. The results show how models are making progress in reading the time on analog clocks. But have they really learned to do it, or have they only learned patterns in their training datasets? In this work we put the models to the test with different clocks to illustrate the limitations of MLLMs to abstract and generalize.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate</title>
<link>https://arxiv.org/abs/2505.10870</link>
<guid>https://arxiv.org/abs/2505.10870</guid>
<content:encoded><![CDATA[
<div> Keyword: rule retrieval, Large Language Models, query augmentation, reasoning performance, relevance reestimation

Summary:
Self-Induction Augmented Retrieval (SIAR) addresses challenges of rule retrieval by utilizing Large Language Models (LLMs) to induce potential inferential rules that abstract underlying knowledge in queries, improving retrieval effectiveness. Rule Relevance ReEstimate (R3) method re-estimates the relevance of retrieved rules by aligning abstract knowledge with query facts, enhancing reasoning performance. The significant semantic gap between query facts and rule representations is overcome, leading to improved retrieval quality. SIAR's query augmentation with induced rules enhances retrieval accuracy. R3 assesses the helpfulness of retrieved rules for reasoning by instantiating abstract knowledge with query facts. Extensive experiments verify the effectiveness and versatility of SIAR and R3 in rule retrieval for downstream reasoning tasks. <div>
arXiv:2505.10870v1 Announce Type: new 
Abstract: This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?</title>
<link>https://arxiv.org/abs/2505.10924</link>
<guid>https://arxiv.org/abs/2505.10924</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-driven interactions, Computer-Using Agents (CUAs), safety threats, defensive strategies, evaluation metrics 

Summary: 
AI-driven interactions in computing devices have evolved to create Computer-Using Agents (CUAs) capable of autonomous tasks, leading to new safety and security risks. Vulnerabilities in reasoning and the complexity of integrating software components pose challenges. This paper systematizes knowledge on safety and security threats of CUAs by defining suitable safety analysis, categorizing current threats, proposing defensive strategies, and summarizing evaluation metrics. It offers researchers a foundation to explore vulnerabilities and provides practitioners with guidance to design secure CUAs.<br /><br /> <div>
arXiv:2505.10924v1 Announce Type: new 
Abstract: Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents</title>
<link>https://arxiv.org/abs/2505.10936</link>
<guid>https://arxiv.org/abs/2505.10936</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Chain-of-thought, Multi-agent systems, Collaboration prompting framework, Business workflow

Summary:
Large Language Models (LLMs) and multi-agent systems have shown great potential in enhancing reasoning capabilities. However, designing cross-domain prompts and token consumption pose challenges in collaboration and problem-solving tasks. To address these issues, the Cochain framework is proposed. Cochain integrates knowledge and prompts efficiently, using an integrated knowledge graph and prompts tree to enhance collaboration in business workflow tasks. Extensive evaluations show that Cochain outperforms existing methods in prompt engineering and multi-agent LLMs, with expert evaluations favoring the use of Cochain with a smaller model over GPT-4. Cochain presents a promising solution to improve collaboration and problem-solving in complex tasks. 

<br /><br />Summary: <div>
arXiv:2505.10936v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations</title>
<link>https://arxiv.org/abs/2505.10937</link>
<guid>https://arxiv.org/abs/2505.10937</guid>
<content:encoded><![CDATA[
<div> Keywords: large reasoning models, chain-of-thought processes, OmniThought dataset, reasoning verbosity, cognitive difficulty <br />
<br />
Summary: 
The article introduces the OmniThought dataset, comprising 2 million chain-of-thought processes generated by two large reasoning models (LRMs). Each process is annotated with Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores to describe the coherence and complexity of the reasoning. The dataset addresses the lack of comprehensive CoT resources and aims to enhance LRM training effectiveness. Experimentation with Qwen2.5 models demonstrates the positive impact of RV and CD scores. The dataset enables the training of high-performing LRMs with improved reasoning abilities and optimal CoT outputs. These contributions advance the development of LRMs for complex tasks, providing a valuable resource for researchers in the field. <br /><br /> <div>
arXiv:2505.10937v1 Announce Type: new 
Abstract: The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accurate KV Cache Quantization with Outlier Tokens Tracing</title>
<link>https://arxiv.org/abs/2505.10938</link>
<guid>https://arxiv.org/abs/2505.10938</guid>
<content:encoded><![CDATA[
<div> quantization, Large Language Models, KV Cache, memory usage, throughput
<br />
The article discusses the use of quantization in Large Language Models (LLMs) to reduce memory usage and increase throughput during deployment. While KV Cache can assist in reducing recomputation during inference, it can also introduce additional memory overhead. The common practice of applying channel-wise quantization to Keys and token-wise quantization to Values may lead to accuracy issues for unusual tokens with unique characteristics. A simple method is proposed to identify and exclude these outlier tokens during decoding, resulting in significant accuracy improvements under 2-bit quantization. Experiments show a 6.4 times reduction in memory usage and a 2.3 times increase in throughput with this approach.
<br /><br />Summary: <div>
arXiv:2505.10938v1 Announce Type: new 
Abstract: The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction</title>
<link>https://arxiv.org/abs/2505.10939</link>
<guid>https://arxiv.org/abs/2505.10939</guid>
<content:encoded><![CDATA[
<div> General knowledge subtraction, modular framework, LoRA modules, zero-shot generalization, task-specific adaptations<br />
<br />
Summary: <br />
Large language models often struggle with zero-shot generalization, despite the introduction of modular approaches. However, the entanglement of general knowledge with task-specific adaptations continues to be a limiting factor. To address this issue, a modular framework that disentangles these components by utilizing task-specific LoRA modules alongside a general-domain LoRA is proposed. Through the process of general knowledge subtraction (GenKnowSub), residual modules are obtained that focus more exclusively on task-relevant information. By leveraging refined task-specific modules and the Arrow routing algorithm, the framework dynamically selects and combines modules for new inputs without additional training. Experimental studies on the Phi-3 model and standard Arrow baselines confirm performance gains in both monolingual and cross-lingual settings across various benchmarks. Additionally, experiments on weaker LLMs demonstrate the generalizability of GenKnowSub. <div>
arXiv:2505.10939v1 Announce Type: new 
Abstract: Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer</title>
<link>https://arxiv.org/abs/2505.10945</link>
<guid>https://arxiv.org/abs/2505.10945</guid>
<content:encoded><![CDATA[
<div> transfer learning, language models, multilingual, embeddings, cross-lingual understanding <br />
Summary: 
The paper introduces a new cross-lingual transfer technique called Semantic Aware Linear Transfer (SALT) to enhance the performance of Large Language Models (LLMs) when transferring them into target language-specific models. SALT recycles embeddings from target language Pre-trained Language Models (PLMs) to maintain deep representational strengths during transfer. By deriving unique regression lines based on vocabulary overlap between source and target languages, SALT handles non-overlapping tokens effectively. Experimental results demonstrate that SALT outperforms existing transfer methods, achieves lower loss, and converges faster during language adaptation. Additionally, SALT shows significant improvements in cross-lingual understanding tasks compared to other techniques, highlighting its effectiveness in enhancing the functionality of LLMs. The scalability of PLMs in enhancing different LLM architectures is also demonstrated through experiments. <br /> <div>
arXiv:2505.10945v1 Announce Type: new 
Abstract: Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs</title>
<link>https://arxiv.org/abs/2505.10948</link>
<guid>https://arxiv.org/abs/2505.10948</guid>
<content:encoded><![CDATA[
<div> Conceptual Blending Theory, Large Language Models, Prompt-Induced Transitions, Prompt-Induced Hallucinations, Artificial Intelligence <br />
Summary:
The study focuses on understanding the behaviors exhibited by Large Language Models (LLMs) through the lens of Conceptual Blending Theory (CBT). Using prompt-based methods, the researchers investigate Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH) to uncover similarities and differences between artificial and biological cognition. By bridging linguistics, neuroscience, and empirical AI research, the study highlights the potential for human-AI collaboration to inform cognitive science. The research suggests that prompt engineering can not only be used as a technical tool but also as a scientific method to delve into the deep structure of meaning. This interdisciplinary approach sheds light on the mechanisms behind the seemingly intelligent and personality-like behaviors of LLMs, offering insights into the future of cognitive science. <br /><br />Summary: <div>
arXiv:2505.10948v1 Announce Type: new 
Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors that often evoke a sense of personality and intelligence-yet the mechanisms behind these effects remain elusive. Here, we operationalize Conceptual Blending Theory (CBT) as an experimental framework, using prompt-based methods to reveal how LLMs blend and compress meaning. By systematically investigating Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we uncover structural parallels and divergences between artificial and biological cognition. Our approach bridges linguistics, neuroscience, and empirical AI research, demonstrating that human-AI collaboration can serve as a living prototype for the future of cognitive science. This work proposes prompt engineering not just as a technical tool, but as a scientific method for probing the deep structure of meaning itself.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio</title>
<link>https://arxiv.org/abs/2505.10975</link>
<guid>https://arxiv.org/abs/2505.10975</guid>
<content:encoded><![CDATA[
<div> E2E, neural approaches, multi-speaker, ASR, speech recognition, monaural

Summary:
This survey reviews recent developments in monaural multi-speaker automatic speech recognition (ASR) using end-to-end (E2E) neural approaches. It highlights architectural paradigms (SIMO vs. SISO) for pre-segmented audio and recent improvements based on these paradigms. The survey also discusses extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Methods are evaluated and compared across standard benchmarks to provide a comprehensive analysis. The survey concludes with a discussion of open challenges and future research directions for building robust and scalable multi-speaker ASR. 

<br /><br />Summary: <div>
arXiv:2505.10975v1 Announce Type: new 
Abstract: Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E2E) architectures, which reduce error propagation and better exploit the synergy between speech content and speaker identity. Despite rapid progress in E2E multi-speaker ASR, the field lacks a comprehensive review of recent developments. This survey provides a systematic taxonomy of E2E neural approaches for multi-speaker ASR, highlighting recent advances and comparative analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO) for pre-segmented audio, analyzing their distinct characteristics and trade-offs; (2) recent architectural and algorithmic improvements based on these two paradigms; (3) extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate and compare methods across standard benchmarks. We conclude with a discussion of open challenges and future research directions towards building robust and scalable multi-speaker ASR.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning</title>
<link>https://arxiv.org/abs/2505.11004</link>
<guid>https://arxiv.org/abs/2505.11004</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer language models, in-context learning, downstream tasks, training dynamics, AI security<br />
Summary:<br />
The study investigates in-context learning (ICL) in large-scale Transformer language models trained on next-token prediction with web-scale data. It aims to understand the mechanisms behind ICL and its implications. The research introduces investigative tasks and a novel method to systematically explore ICL using the Pythia scaling suite. By analyzing ICL performance on downstream tasks and delving into the residual stream's subspace, the study shows that ICL goes beyond mere data memorization but does not constitute the implementation of an independent symbolic algorithm. Findings highlight the impact of training dynamics, model capabilities, and aspects of mechanistic interpretability on ICL. The results offer insights for model developers on potential improvements and provide AI security practitioners with informed guidelines. <br /><br />Summary: <div>
arXiv:2505.11004v1 Announce Type: new 
Abstract: Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs</title>
<link>https://arxiv.org/abs/2505.11008</link>
<guid>https://arxiv.org/abs/2505.11008</guid>
<content:encoded><![CDATA[
<div> Keywords: Abugida languages, Transformer-based models, syllable sequence prediction, incomplete input types, Asian Language Treebank.

Summary:
This paper investigates syllable sequence prediction in Abugida languages using Transformer-based models. The study focuses on languages such as Bengali, Hindi, Khmer, Lao, Myanmar, and Thai from the ALT dataset. Various incomplete input types, including consonant sequences, vowel sequences, partial syllables, and masked syllables, were explored for complete syllable sequence reconstruction. Results show that consonant sequences play a crucial role in accurate syllable prediction, with high BLEU scores, while vowel sequences pose a greater challenge. The model demonstrates strong performance in tasks involving partial and masked syllable reconstruction, especially with consonant information and syllable masking. This research contributes to advancing the understanding of sequence prediction in Abugida languages and offers practical insights for applications like text prediction, spelling correction, and data augmentation in these scripts.
<br /><br />Summary: <div>
arXiv:2505.11008v1 Announce Type: new 
Abstract: This paper explores syllable sequence prediction in Abugida languages using Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer, Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We investigate the reconstruction of complete syllable sequences from various incomplete input types, including consonant sequences, vowel sequences, partial syllables (with random character deletions), and masked syllables (with fixed syllable deletions). Our experiments reveal that consonant sequences play a critical role in accurate syllable prediction, achieving high BLEU scores, while vowel sequences present a significantly greater challenge. The model demonstrates robust performance across tasks, particularly in handling partial and masked syllable reconstruction, with strong results for tasks involving consonant information and syllable masking. This study advances the understanding of sequence prediction for Abugida languages and provides practical insights for applications such as text prediction, spelling correction, and data augmentation in these scripts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11010</link>
<guid>https://arxiv.org/abs/2505.11010</guid>
<content:encoded><![CDATA[
<div> multi-turn dialogue, large language models, conversational AI, Review-Instruct framework, dataset construction

Summary:
The paper introduces the Review-Instruct framework for generating diverse and high-quality multi-turn dialogue data for large language models (LLMs) in conversational AI. The framework involves an iterative process with three agent roles: a Candidate, multiple Reviewers, and a Chairman, to refine instructions and enhance dialogue diversity and difficulty. By fine-tuning the LLaMA2-13B model on a new multi-turn dataset created using this framework, the study shows significant improvements in various evaluation metrics such as MT-Bench and MMLU-Pro. Ablation studies confirm the importance of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. This work showcases the potential of review-driven, multi-agent frameworks in generating high-quality conversational data at scale. 

<br /><br />Summary: <div>
arXiv:2505.11010v1 Announce Type: new 
Abstract: The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative "Ask-Respond-Review" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StRuCom: A Novel Dataset of Structured Code Comments in Russian</title>
<link>https://arxiv.org/abs/2505.11026</link>
<guid>https://arxiv.org/abs/2505.11026</guid>
<content:encoded><![CDATA[
<div> Keywords: code documentation, machine learning, Russian, dataset, model fine-tuning

Summary:<br />
Structured code comments are crucial for code understanding and maintenance. Existing machine learning models struggle with generating Russian comments compared to English. To address this gap, the article introduces StRuCom, a new dataset specifically tailored for Russian code documentation. It comprises 153K examples sourced from Russian GitHub repositories and synthetic comments. The dataset ensures adherence to Python, Java, JavaScript, C#, and Go standards through automated validation. By fine-tuning Qwen2.5-Coder models on StRuCom, significant enhancements in chrf++ and BERTScore metrics are achieved compared to baseline models. This work showcases the importance of language-specific datasets in enhancing machine learning capabilities for code comment generation. The StRuCom dataset fills a critical gap in the field and sets the stage for further advancements in Russian code documentation using machine learning techniques. 

<br /><br />Summary: <div>
arXiv:2505.11026v1 Announce Type: new 
Abstract: Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning</title>
<link>https://arxiv.org/abs/2505.11031</link>
<guid>https://arxiv.org/abs/2505.11031</guid>
<content:encoded><![CDATA[
<div> benchmark, language models, ontologies, symbolic knowledge, natural language processing

Summary: 
- This article introduces a taxonomy to evaluate large language models' (LLMs) ability to process structured symbolic knowledge.
- The OntoURL benchmark is designed to assess LLMs' proficiency in handling ontologies, representing domain knowledge through concepts, relationships, and instances.
- OntoURL evaluates LLMs in three dimensions: understanding, reasoning, and learning, through 15 tasks derived from 40 ontologies across 8 domains.
- Experiments with 20 open-source LLMs reveal performance variations across models, tasks, and domains, indicating weaknesses in reasoning and learning tasks.
- Current LLMs demonstrate strength in understanding ontological knowledge but lack capability in reasoning and learning tasks. OntoURL is established as a crucial benchmark to advance the integration of LLMs with formal knowledge representations.

<br /><br />Summary: <div>
arXiv:2505.11031v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing tasks, yet their ability to process structured symbolic knowledge remains underexplored. To address this gap, we propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the first comprehensive benchmark designed to systematically evaluate LLMs' proficiency in handling ontologies -- formal, symbolic representations of domain knowledge through concepts, relationships, and instances. Based on the proposed taxonomy, OntoURL systematically assesses three dimensions: understanding, reasoning, and learning through 15 distinct tasks comprising 58,981 questions derived from 40 ontologies across 8 domains. Experiments with 20 open-source LLMs reveal significant performance differences across models, tasks, and domains, with current LLMs showing proficiency in understanding ontological knowledge but substantial weaknesses in reasoning and learning tasks. These findings highlight fundamental limitations in LLMs' capability to process symbolic knowledge and establish OntoURL as a critical benchmark for advancing the integration of LLMs with formal knowledge representations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAMEO: Collection of Multilingual Emotional Speech Corpora</title>
<link>https://arxiv.org/abs/2505.11051</link>
<guid>https://arxiv.org/abs/2505.11051</guid>
<content:encoded><![CDATA[
<div> Keywords: CAMEO, emotional speech datasets, emotion recognition, speech-related tasks, Hugging Face platform

Summary: 
CAMEO is a curated collection of multilingual emotional speech datasets aimed at facilitating research in emotion recognition and other speech-related tasks. The main goals of CAMEO are to provide easy access to data, ensure reproducibility of results, and establish a standardized benchmark for evaluating speech emotion recognition systems across various emotional states and languages. The paper outlines the dataset selection criteria, the curation and normalization process, and presents performance results for different models. The collection, metadata, and leaderboard can be accessed through the Hugging Face platform. <div>
arXiv:2505.11051v1 Announce Type: new 
Abstract: This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLEUBERI: BLEU is a surprisingly effective reward for instruction following</title>
<link>https://arxiv.org/abs/2505.11080</link>
<guid>https://arxiv.org/abs/2505.11080</guid>
<content:encoded><![CDATA[
<div> Keywords: reward models, LLMs, instruction-following datasets, BLEU, BLEUBERI

Summary: 
BLEU, a basic string-matching metric, has been shown to match strong reward models in agreement with human preferences on instruction-following datasets. Based on this discovery, BLEUBERI, a method utilizing challenging instructions and Group Relative Policy Optimization with BLEU as a reward function, proves to be competitive with reward model-guided RL on instruction-following benchmarks. Human evaluation confirms the quality of BLEUBERI model outputs to be comparable to reward model-aligned models. Additionally, BLEUBERI models are more factually grounded than other methods. Access to high-quality reference outputs enables string matching-based metrics to serve as cost-effective proxies for reward models during alignment. The code and data for BLEUBERI are available at https://github.com/lilakk/BLEUBERI.<br /><br />Summary: <div>
arXiv:2505.11080v1 Announce Type: new 
Abstract: Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Better Evaluation for Generated Patent Claims</title>
<link>https://arxiv.org/abs/2505.11095</link>
<guid>https://arxiv.org/abs/2505.11095</guid>
<content:encoded><![CDATA[
<div> Patent claims, automation, large language models, evaluation metrics, benchmark <br />
<br />
Summary: 
Researchers have developed a benchmark, Patent-CE, to evaluate patent claims automatically generated by large language models. The benchmark includes comparative evaluations by patent experts on key criteria such as feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. A novel multi-dimensional evaluation method, PatClaimEval, is proposed to achieve the highest correlation with human expert assessments. This approach addresses the inconsistencies found in existing automated evaluation metrics. The research aims to improve the accuracy of assessing automated patent claim generation systems, reducing barriers for small enterprises in the patent drafting process. <div>
arXiv:2505.11095v1 Announce Type: new 
Abstract: Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated the use of large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning can Improve Factuality in Large Language Models</title>
<link>https://arxiv.org/abs/2505.11140</link>
<guid>https://arxiv.org/abs/2505.11140</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, reasoning traces, test-time compute, token budgets, factual accuracy

Summary: 
- The study focuses on examining the reasoning capabilities of large language models (LLMs) in open-domain question-answering scenarios.
- Reasoning traces from advanced models like QwQ-32B and DeepSeek-R1-671B are distilled and fine-tuned using knowledge graph paths.
- Smaller reasoning models show improved factual accuracy compared to larger instruction-tuned models.
- Adding test-time compute and token budgets results in consistent 2-8% improvement in factual accuracy.
- The study releases all experimental artifacts for further research.<br /><br /> <div>
arXiv:2505.11140v1 Announce Type: new 
Abstract: Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization</title>
<link>https://arxiv.org/abs/2505.11166</link>
<guid>https://arxiv.org/abs/2505.11166</guid>
<content:encoded><![CDATA[
<div> framework, SoLoPO, long-context preference optimization, short-context PO, short-to-long reward alignment <br />
<br />
Summary: The article introduces the SoLoPO framework, which aims to improve the utilization of long-context information in large language models by separating preference optimization into short-context PO and SoLo-RA. Short-context PO enhances contextual knowledge utilization with preference pairs from short contexts, while SoLo-RA promotes reward score consistency in responses conditioned on both short and long contexts. The framework is compatible with existing preference optimization algorithms and improves data construction and training efficiency. Experimental results demonstrate enhanced length and domain generalization abilities across various benchmarks, along with improved computational and memory efficiency. <div>
arXiv:2505.11166v1 Announce Type: new 
Abstract: Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng $\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline</title>
<link>https://arxiv.org/abs/2505.11177</link>
<guid>https://arxiv.org/abs/2505.11177</guid>
<content:encoded><![CDATA[
<div> OCR, multilingual information extraction, image-based documents, language model APIs, document comprehension <br />
<br />
Summary: 
This paper introduces a complete suite for extracting and processing multilingual information from image-based documents. Using Optical Character Recognition (OCR) such as Tesseract, the system extracts text in languages like English, Hindi, and Tamil. It then employs large language model APIs like Gemini for tasks such as cross-lingual translation, abstractive summarization, and re-translation into a desired language. Additional modules for sentiment analysis, topic classification, and date extraction enhance document understanding. The research presents a practical application of libraries, models, and APIs to bridge language barriers and improve access to information in image media across diverse linguistic settings. <div>
arXiv:2505.11177v1 Announce Type: new 
Abstract: This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs (Gemini) for cross-lingual translation, abstractive summarization, and re-translation into a target language. Additional modules add sentiment analysis (TensorFlow), topic classification (Transformers), and date extraction (Regex) for better document comprehension. Made available in an accessible Gradio interface, the current research shows a real-world application of libraries, models, and APIs to close the language gap and enhance access to information in image media across different linguistic environments
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NoPE: The Counting Power of Transformers with No Positional Encodings</title>
<link>https://arxiv.org/abs/2505.11199</link>
<guid>https://arxiv.org/abs/2505.11199</guid>
<content:encoded><![CDATA[
<div> expressiveness, transformers, Positional Encodings, NoPE-transformers, Diophantine equations 
Summary: 
Average Hard Attention NoPE-Transformers (NoPE-AHATs) are shown to be surprisingly expressive, able to express counting languages corresponding to nonnegative integer solutions to multivariate polynomial equations. These transformers can reason about undecidable problems, as their expressibility corresponds to finite unions of sets of nonnegative integer solutions to systems of multivariate polynomial inequations. NoPE-transformers can handle complex counting properties surpassing models like simplified counter machines and Petri nets but fail to express simple properties like PARITY. Analyzing NoPE-transformers is undecidable, presenting challenges in classifying all input strings into one class. Additionally, a counting language exists that is expressible in the circuit complexity class TC$^0 but not by average hard attention transformers even with arbitrary Positional Encodings, resolving an open problem. <div>
arXiv:2505.11199v1 Announce Type: new 
Abstract: Positional Encodings (PEs) seem to be indispensable for ensuring expressiveness of transformers; without them attention transformers reduce to a bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard attention mechanisms were very recently shown to only be able to express regular languages, i.e., with limited counting ability. This paper shows that, with average hard attention mechanisms, NoPE-transformers are still surprisingly expressive: they can express counting languages corresponding to nonnegative integer solutions to multivariate polynomial equations (i.e. Diophantine equations), reasoning about which is well-known to be undecidable. In fact, we provide a precise characterization of languages expressible by Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of sets of nonnegative integer solutions to systems of multivariate polynomial inequations. We obtain several interesting consequences of our characterization. Firstly, NoPE-transformers can express counting properties that are far more complex than established models like simplified counter machines and Petri nets, but cannot express a very simple counting property of PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable, e.g., whether a given NoPE transformer classifies all input strings in one class. To complement our results, we exhibit a counting language that is not expressible by average hard attention transformers even with arbitrary PEs but is expressible in the circuit complexity class TC$^0$, answering an open problem.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2505.11225</link>
<guid>https://arxiv.org/abs/2505.11225</guid>
<content:encoded><![CDATA[
<div> Keywords: efficient test-time scaling, large language models, history-aware policy optimization, concise reasoning abilities, math benchmarks

Summary:<br /><br />
- The article introduces a new approach called History-Aware Policy Optimization (HAPO) for efficient test-time scaling of large language models (LLMs).
- HAPO tracks historical information from previous encounters to incentivize the discovery of more concise correct solutions.
- HAPO employs a novel length reward function that encourages the generation of shorter responses while balancing correctness.
- By combining length and correctness rewards, HAPO optimizes LLMs for both efficiency and accuracy.
- Experiment results show that HAPO successfully induces LLMs' concise reasoning abilities, leading to significant length reductions with minimal accuracy drops on various math benchmarks. 

Summary: <div>
arXiv:2505.11225v1 Announce Type: new 
Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models</title>
<link>https://arxiv.org/abs/2505.11271</link>
<guid>https://arxiv.org/abs/2505.11271</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, semantic caching, efficient information reuse, computational cost, real-time AI assistants

Summary:
This paper introduces a semantic caching approach for efficiently storing and reusing contextual summaries in Large Language Models (LLMs) used for question-answering and generation tasks. By reusing intermediate summaries, redundant computations can be reduced by up to 50-60%, leading to lower computational overhead, memory usage, and network bandwidth in distributed systems. The method maintains answer accuracy comparable to full document processing, as shown in experiments on various datasets including NaturalQuestions and TriviaQA. This approach strikes a balance between computational cost and response quality, making it crucial for real-time AI assistants deployed across edge and cloud platforms. <div>
arXiv:2505.11271v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed across edge and cloud platforms for real-time question-answering and retrieval-augmented generation. However, processing lengthy contexts in distributed systems incurs high computational overhead, memory usage, and network bandwidth. This paper introduces a novel semantic caching approach for storing and reusing intermediate contextual summaries, enabling efficient information reuse across similar queries in LLM-based QA workflows. Our method reduces redundant computations by up to 50-60% while maintaining answer accuracy comparable to full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a synthetic ArXiv dataset. This approach balances computational cost and response quality, critical for real-time AI assistants.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs</title>
<link>https://arxiv.org/abs/2505.11277</link>
<guid>https://arxiv.org/abs/2505.11277</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, large language models, retrieval-augmented reasoning, knowledge refinement, multi-hop reasoning

Summary:
AutoRefine is a reinforcement learning post-training framework that enhances the reasoning capabilities of large language models by introducing a "search-and-refine-during-think" paradigm. It includes explicit knowledge refinement steps between search calls, allowing the model to iteratively filter, distill, and organize information before generating an answer. Tailored retrieval-specific rewards are used alongside answer correctness rewards to optimize the model's performance. Experimental results on both single-hop and multi-hop question-answering benchmarks show that AutoRefine outperforms existing approaches, especially in complex, multi-hop reasoning scenarios. The framework demonstrates the ability to issue frequent, high-quality searches and effectively synthesize evidence for accurate reasoning. <div>
arXiv:2505.11277v1 Announce Type: new 
Abstract: Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal fine-tuning for early risk detection</title>
<link>https://arxiv.org/abs/2505.11280</link>
<guid>https://arxiv.org/abs/2505.11280</guid>
<content:encoded><![CDATA[
<div> Keywords: Early Risk Detection, Web, Multi-objective approach, Transformer-based models, Temporal fine-tuning

Summary: 
Early Risk Detection (ERD) on the Web involves identifying users facing social and health issues promptly, optimizing precision and minimizing detection delay. Standard classification metrics may not be sufficient, so ERDE(theta) metric is used to consider precision and delay. A multi-objective approach is employed, prioritizing classification performance and incorporating a separate criterion for decision time. A new strategy called temporal fine-tuning is proposed, utilizing transformer-based models and explicitly incorporating time in the learning process. The method allows for analyzing complete user post histories, considering different contexts, and evaluating training performance using temporal metrics. Competitive results were achieved in depression and eating disorders tasks for the Spanish language. Temporal fine-tuning optimized decisions by considering context and time progress. By leveraging transformer models effectively, ERD can be addressed by combining precision and speed as a single objective. 

<br /><br />Summary: <div>
arXiv:2505.11280v1 Announce Type: new 
Abstract: Early Risk Detection (ERD) on the Web aims to identify promptly users facing social and health issues. Users are analyzed post-by-post, and it is necessary to guarantee correct and quick answers, which is particularly challenging in critical scenarios. ERD involves optimizing classification precision and minimizing detection delay. Standard classification metrics may not suffice, resorting to specific metrics such as ERDE(theta) that explicitly consider precision and delay. The current research focuses on applying a multi-objective approach, prioritizing classification performance and establishing a separate criterion for decision time. In this work, we propose a completely different strategy, temporal fine-tuning, which allows tuning transformer-based models by explicitly incorporating time within the learning process. Our method allows us to analyze complete user post histories, tune models considering different contexts, and evaluate training performance using temporal metrics. We evaluated our proposal in the depression and eating disorders tasks for the Spanish language, achieving competitive results compared to the best models of MentalRiskES 2023. We found that temporal fine-tuning optimized decisions considering context and time progress. In this way, by properly taking advantage of the power of transformers, it is possible to address ERD by combining precision and speed as a single objective.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probing Subphonemes in Morphology Models</title>
<link>https://arxiv.org/abs/2505.11297</link>
<guid>https://arxiv.org/abs/2505.11297</guid>
<content:encoded><![CDATA[
<div> transformers, morphological inflection tasks, phonological feature encoding, phoneme embeddings, subphonemic feature acquisition <br />
<br />
The article explores the limitations of transformer models in generalizing across languages and morphological rules in morphological inflection tasks. It suggests that the models' inability to capture implicit phonological and subphonemic phenomena could be a contributing factor. The study introduces a language-agnostic probing method to examine phonological feature encoding in transformers trained on phonemes across multiple languages. The results indicate that local phonological features such as final-obstruent devoicing in Turkish are well represented in phoneme embeddings, while long-distance dependencies like vowel harmony are better captured in the transformer's encoder. The findings shed light on the importance of considering subphonemic feature acquisition when training morphological models, offering insights for improving language generalization and morphological rule application in transformer-based systems. <br /><br />Summary: <div>
arXiv:2505.11297v1 Announce Type: new 
Abstract: Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer's encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision</title>
<link>https://arxiv.org/abs/2505.11336</link>
<guid>https://arxiv.org/abs/2505.11336</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, academic writing, research communication, human-AI collaboration, XtraGPT

Summary: 
XtraGPT proposes a human-AI collaboration framework for academic paper revision, addressing the limitations of existing systems in supporting high-quality scientific writing. A dataset of 7,040 research papers annotated with instruction-response pairs is introduced, enabling context-aware, instruction-guided writing assistance through XtraGPT, a suite of open-source LLMs. These models significantly outperform baselines and approach proprietary systems' quality, as validated through automated preference assessments and human evaluations. The framework supports sophisticated demands of research communication, such as conceptual coherence, and facilitates the iterative and revision-driven nature of academic writing. <div>
arXiv:2505.11336v1 Announce Type: new 
Abstract: Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited when it comes to supporting high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision. We first introduce a comprehensive dataset of 7,040 research papers from top-tier venues annotated with over 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. Building on the dataset, we develop XtraGPT, the first suite of open-source LLMs, designed to provide context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B parameters. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of our models in improving scientific drafts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11341</link>
<guid>https://arxiv.org/abs/2505.11341</guid>
<content:encoded><![CDATA[
<div> Keywords: Critical Questions Generation, Dataset, Automatic Evaluation, Large Language Models, Benchmarking

Summary:
This paper introduces the task of Critical Questions Generation (CQs-Gen), which aims to enhance critical thinking by generating questions that challenge assumptions in arguments. The lack of suitable datasets and automatic evaluation methods has hindered progress in this area. The authors present the first large-scale manually-annotated dataset for CQs-Gen and investigate automatic evaluation techniques. They find that using large language models (LLMs) for reference-based evaluation correlates best with human judgments. Zero-shot evaluation of 11 LLMs establishes a strong baseline, highlighting the difficulty of the task. The authors provide data, code, and a public leaderboard to encourage further research on both model performance and the practical applications of CQs-Gen for automated reasoning and human critical thinking. <br /><br />Summary: <div>
arXiv:2505.11341v1 Announce Type: new 
Abstract: The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose assumptions and challenge the reasoning in arguments. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This work presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale manually-annotated dataset. We also investigate automatic evaluation methods and identify a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data, code, and a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors</title>
<link>https://arxiv.org/abs/2505.11352</link>
<guid>https://arxiv.org/abs/2505.11352</guid>
<content:encoded><![CDATA[
<div> Keywords: speech encoders, Large Language Models, ASR, LegoSLM, Connectionist Temporal Classification (CTC) 

Summary: 
The paper introduces a new approach, LegoSLM, that combines speech encoders and Large Language Models (LLMs) for improved performance in spoken language processing tasks. It bridges the gap between the models using ASR posterior matrices, allowing for flexible integration and better results. By training the speech encoder to generate CTC posteriors over the LLM vocabulary, pseudo-audio embeddings can be reconstructed to enhance ASR and speech translation tasks. The method shows promising results, achieving a 49% WERR improvement over baselines on MLS test sets. The model also exhibits modularity, enabling seamless integration with different models. By controlling the decode-time influence using a softmax temperature, effective domain adaptation is achieved. Overall, the LegoSLM approach presents a novel way to leverage the strengths of speech encoders and LLMs for enhanced performance in spoken language processing tasks.<br /><br />Summary: <div>
arXiv:2505.11352v1 Announce Type: new 
Abstract: Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings -- after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents</title>
<link>https://arxiv.org/abs/2505.11368</link>
<guid>https://arxiv.org/abs/2505.11368</guid>
<content:encoded><![CDATA[
<div> benchmarks, domain-oriented guidelines, large language models, instruction following, guideline following
Summary:
GuideBench is a new benchmark introduced to evaluate the performance of large language models (LLMs) in following domain-oriented guidelines. The benchmark assesses LLMs on three key aspects: adherence to various rules, robustness to rule updates, and alignment with human preferences. Experimental results reveal room for improvement in LLMs' capability to follow domain-oriented guidelines. This benchmark addresses the challenges posed by domain-specific guidelines that may conflict with an LLM's commonsense knowledge, providing a comprehensive evaluation framework for assessing and enhancing LLMs' performance in real-world applications. <div>
arXiv:2505.11368v1 Announce Type: new 
Abstract: Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography</title>
<link>https://arxiv.org/abs/2505.11379</link>
<guid>https://arxiv.org/abs/2505.11379</guid>
<content:encoded><![CDATA[
<div> Keywords: Contemporary Quranic Orthography, phonetic notation, tajwid rules, Cairo Quran, computational analysis 

Summary: 
Contemporary Quranic Orthography (CQO) utilizes a precise phonetic notation system rooted in early Islam's oral tradition. The tajwid rules of recitation, represented through diacritical marks on the Quranic Consonantal Text, were systematically explored in the Cairo Quran using digital tools. A python module was developed to manipulate the orthographic layer of tajwid in CQO texts. This analysis of the Cairo Quran, complete and rich in content, can serve as a linchpin for aligning and comparing Quranic manuscripts computationally. The interconnectedness of these texts allows for studying the nature of diacritic notation systems across different manuscripts. This framework enhances understanding of Arabic script and textual phenomena within Quranic manuscripts. Computational analysis based on these tajwid rules opens avenues for in-depth exploration of the Quranic text's phonetic and prosodic aspects.<br /><br />Summary: <div>
arXiv:2505.11379v1 Announce Type: new 
Abstract: Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic notation that can be traced back to the early stages of Islam, when the Quran was mainly oral in nature and the first written renderings of it served as memory aids for this oral tradition. The early systems of diacritical marks created on top of the Quranic Consonantal Text (QCT) motivated the creation and further development of a fine-grained system of phonetic notation that represented tajwid-the rules of recitation. We explored the systematicity of the rules of tajwid, as they are encountered in the Cairo Quran, using a fully and accurately encoded digital edition of the Quranic text. For this purpose, we developed a python module that can remove or add the orthographic layer of tajwid from a Quranic text in CQO. The interesting characteristic of these two sets of rules is that they address the complete Quranic text of the Cairo Quran, so they can be used as precise witnesses to study its phonetic and prosodic processes. From a computational point of view, the text of the Cairo Quran can be used as a linchpin to align and compare Quranic manuscripts, due to its richness and completeness. This will let us create a very powerful framework to work with the Arabic script, not just within an isolated text, but automatically exploring a specific textual phenomenon in other connected manuscripts. Having all the texts mapped among each other can serve as a powerful tool to study the nature of the notation systems of diacritics added to the consonantal skeleton.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs</title>
<link>https://arxiv.org/abs/2505.11413</link>
<guid>https://arxiv.org/abs/2505.11413</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, healthcare, adversarial manipulation, safety evaluation, mitigation strategy <br />
Summary:<br />
The article introduces CARES, a benchmark for evaluating the safety of large language models (LLMs) in healthcare settings. CARES includes prompts covering various medical safety principles, harm levels, and prompting styles to simulate benign and malicious use cases. A three-way response evaluation protocol is proposed to assess model behavior, revealing vulnerabilities in state-of-the-art LLMs to jailbreak-style attacks and over-refusal of safe queries. A mitigation strategy using a lightweight classifier is suggested to detect jailbreak attempts and guide models towards safer responses. By providing a comprehensive framework for testing and enhancing LLM safety under adversarial and ambiguous conditions, CARES aims to address critical concerns about model alignment, susceptibility to manipulation, and overall safety in medical contexts. <br /> <div>
arXiv:2505.11413v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model</title>
<link>https://arxiv.org/abs/2505.11421</link>
<guid>https://arxiv.org/abs/2505.11421</guid>
<content:encoded><![CDATA[
<div> Keywords: Bahnaric, Vietnamese, translation, transfer learning, data augmentation

Summary: 
<br />
This work discusses the challenges of translating Bahnaric to Vietnamese due to limited available resources in the source language. The authors propose a transfer learning approach using a pre-trained Vietnamese language model to overcome this limitation. By training a sequence-to-sequence model with bilingual resources, the model can effectively translate between the two languages. The use of data augmentation techniques and heuristic methods further enhance the translation accuracy. The approach has been proven to be highly effective in bridging the linguistic gap between the Bahnaric and Vietnamese ethnic groups, leading to better mutual understanding and language preservation. 

Summary: <div>
arXiv:2505.11421v1 Announce Type: new 
Abstract: This work explores the journey towards achieving Bahnaric-Vietnamese translation for the sake of culturally bridging the two ethnic groups in Vietnam. However, translating from Bahnaric to Vietnamese also encounters some difficulties. The most prominent challenge is the lack of available original Bahnaric resources source language, including vocabulary, grammar, dialogue patterns and bilingual corpus, which hinders the data collection process for training. To address this, we leverage a transfer learning approach using sequence-to-sequence pre-training language model. First of all, we leverage a pre-trained Vietnamese language model to capture the characteristics of this language. Especially, to further serve the purpose of machine translation, we aim for a sequence-to-sequence model, not encoder-only like BERT or decoder-only like GPT. Taking advantage of significant similarity between the two languages, we continue training the model with the currently limited bilingual resources of Vietnamese-Bahnaric text to perform the transfer learning from language model to machine translation. Thus, this approach can help to handle the problem of imbalanced resources between two languages, while also optimizing the training and computational processes. Additionally, we also enhanced the datasets using data augmentation to generate additional resources and defined some heuristic methods to help the translation more precise. Our approach has been validated to be highly effective for the Bahnaric-Vietnamese translation model, contributing to the expansion and preservation of languages, and facilitating better mutual understanding between the two ethnic people.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs</title>
<link>https://arxiv.org/abs/2505.11423</link>
<guid>https://arxiv.org/abs/2505.11423</guid>
<content:encoded><![CDATA[
<div> instruction-following accuracy, reasoning-enhanced large language models, chain-of-thought prompting, CoT reasoning, constraint attention

Summary:
Reasoning-enhanced large language models have shown impressive performance on complex reasoning tasks, but a new study reveals that explicit CoT reasoning may actually decrease instruction-following accuracy. Evaluating models on two benchmarks, researchers found that CoT prompting led to performance drops. Through case studies and analysis, they identified ways in which reasoning can either help or hinder performance. A metric called constraint attention was introduced to measure model focus during generation, showing that CoT reasoning often diverts attention from relevant instructions. To address these issues, researchers proposed and tested four strategies to mitigate the negative effects of reasoning, with classifier-selective reasoning proving to be particularly effective in restoring lost performance. This study is the first to systematically uncover the failures induced by reasoning in instruction-following tasks and offers practical solutions to improve model performance. 

<br /><br />Summary: <div>
arXiv:2505.11423v1 Announce Type: new 
Abstract: Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art</title>
<link>https://arxiv.org/abs/2505.11436</link>
<guid>https://arxiv.org/abs/2505.11436</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Comment Art, Multimodal Large Language Models, Chain-of-Thought, GODBench, Ripple of Thought

Summary: <br /><br /> This article introduces a new benchmark called GODBench that evaluates the abilities of Multimodal Large Language Models (MLLMs) in composing creative video comments. While MLLMs and Chain-of-Thought (CoT) have shown strong reasoning skills in STEM tasks, they struggle to generate creative expressions like jokes and satire. The GODBench benchmark integrates video and text modalities to assess MLLMs' creativity. The Ripple of Thought (RoT) framework, inspired by physics wave propagation patterns, is proposed to enhance MLLMs' creativity by leveraging multi-step reasoning. Experiments reveal that existing MLLMs and CoT methods face challenges in understanding and generating creative video comments. In contrast, the RoT framework shows promise in improving creative composition, indicating potential advancements in MLLM-based creativity. The GODBench benchmark is publicly available for further research and development. <div>
arXiv:2505.11436v1 Announce Type: new 
Abstract: Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Compression Really Linear with Code Intelligence?</title>
<link>https://arxiv.org/abs/2505.11441</link>
<guid>https://arxiv.org/abs/2505.11441</guid>
<content:encoded><![CDATA[
arXiv:2505.11441v1 Announce Type: new 
Abstract: Understanding the relationship between data compression and the capabilities of Large Language Models (LLMs) is crucial, especially in specialized domains like code intelligence. Prior work posited a linear relationship between compression and general intelligence. However, it overlooked the multifaceted nature of code that encompasses diverse programming languages and tasks, and struggled with fair evaluation of modern Code LLMs. We address this by evaluating a diverse array of open-source Code LLMs on comprehensive multi-language, multi-task code benchmarks. To address the challenge of efficient and fair evaluation of pre-trained LLMs' code intelligence, we introduce \textit{Format Annealing}, a lightweight, transparent training methodology designed to assess the intrinsic capabilities of these pre-trained models equitably. Compression efficacy, measured as bits-per-character (BPC), is determined using a novel, large-scale, and previously unseen code validation set derived from GitHub. Our empirical results reveal a fundamental logarithmic relationship between measured code intelligence and BPC. This finding refines prior hypotheses of linearity, which we suggest are likely observations of the logarithmic curve's tail under specific, limited conditions. Our work provides a more nuanced understanding of compression's role in developing code intelligence and contributes a robust evaluation framework in the code domain.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Reasoning and Knowledge in Medical Large Language Models</title>
<link>https://arxiv.org/abs/2505.11462</link>
<guid>https://arxiv.org/abs/2505.11462</guid>
<content:encoded><![CDATA[
arXiv:2505.11462v1 Announce Type: new 
Abstract: Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies</title>
<link>https://arxiv.org/abs/2505.11470</link>
<guid>https://arxiv.org/abs/2505.11470</guid>
<content:encoded><![CDATA[
arXiv:2505.11470v1 Announce Type: new 
Abstract: We introduce two reference-free metrics for quality evaluation of taxonomies. The first metric evaluates robustness by calculating the correlation between semantic and taxonomic similarity, covering a type of error not handled by existing metrics. The second uses Natural Language Inference to assess logical adequacy. Both metrics are tested on five taxonomies and are shown to correlate well with F1 against gold-standard taxonomies.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages</title>
<link>https://arxiv.org/abs/2505.11475</link>
<guid>https://arxiv.org/abs/2505.11475</guid>
<content:encoded><![CDATA[
arXiv:2505.11475v1 Announce Type: new 
Abstract: Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Assembly Code Performance with Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.11480</link>
<guid>https://arxiv.org/abs/2505.11480</guid>
<content:encoded><![CDATA[
arXiv:2505.11480v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.11484</link>
<guid>https://arxiv.org/abs/2505.11484</guid>
<content:encoded><![CDATA[
arXiv:2505.11484v1 Announce Type: new 
Abstract: Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Modeling cognitive processes of natural reading with transformer-based Language Models</title>
<link>https://arxiv.org/abs/2505.11485</link>
<guid>https://arxiv.org/abs/2505.11485</guid>
<content:encoded><![CDATA[
arXiv:2505.11485v1 Announce Type: new 
Abstract: Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10583</link>
<guid>https://arxiv.org/abs/2505.10583</guid>
<content:encoded><![CDATA[
arXiv:2505.10583v1 Announce Type: cross 
Abstract: Large language models have become multimodal, and many of them are said to integrate their modalities using common representations. If this were true, a drawing of a car as an image, for instance, should map to the similar area in the latent space as a textual description of the strokes that conform the drawing. To explore this in a black-box access regime to these models, we propose the use of machine teaching, a theory that studies the minimal set of examples a teacher needs to choose so that the learner captures the concept. In this paper we evaluate the complexity of teaching visual-language models a subset of objects in the Quick, Draw! dataset using two presentations: raw images as bitmaps and trace coordinates in TikZ format. The results indicate that image-based representations generally require fewer segments and achieve higher accuracy than coordinate-based representations. But, surprisingly, the teaching size usually ranks concepts similarly across both modalities, even when controlling for (a human proxy of) concept priors, suggesting that the simplicity of concepts may be an inherent property that transcends modality representations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports</title>
<link>https://arxiv.org/abs/2505.10586</link>
<guid>https://arxiv.org/abs/2505.10586</guid>
<content:encoded><![CDATA[
arXiv:2505.10586v1 Announce Type: cross 
Abstract: Timely and accurate situation awareness is vital for decision-making in humanitarian response, conflict monitoring, and early warning and early action. However, the manual analysis of vast and heterogeneous data sources often results in delays, limiting the effectiveness of interventions. This paper introduces a dynamic Retrieval-Augmented Generation (RAG) system that autonomously generates situation awareness reports by integrating real-time data from diverse sources, including news articles, conflict event databases, and economic indicators. Our system constructs query-specific knowledge bases on demand, ensuring timely, relevant, and accurate insights.
  To ensure the quality of generated reports, we propose a three-level evaluation framework that combines semantic similarity metrics, factual consistency checks, and expert feedback. The first level employs automated NLP metrics to assess coherence and factual accuracy. The second level involves human expert evaluation to verify the relevance and completeness of the reports. The third level utilizes LLM-as-a-Judge, where large language models provide an additional layer of assessment to ensure robustness. The system is tested across multiple real-world scenarios, demonstrating its effectiveness in producing coherent, insightful, and actionable reports. By automating report generation, our approach reduces the burden on human analysts and accelerates decision-making processes. To promote reproducibility and further research, we openly share our code and evaluation tools with the community via GitHub.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation</title>
<link>https://arxiv.org/abs/2505.10588</link>
<guid>https://arxiv.org/abs/2505.10588</guid>
<content:encoded><![CDATA[
arXiv:2505.10588v1 Announce Type: cross 
Abstract: This research offers a unique evaluation of how AI systems interpret the digital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first cohort raised alongside AI, Gen Alpha faces new forms of online risk due to immersive digital engagement and a growing mismatch between their evolving communication and existing safety tools. Their distinct language, shaped by gaming, memes, and AI-driven trends, often conceals harmful interactions from both human moderators and automated systems. We assess four leading AI models (GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked harassment and manipulation within Gen Alpha discourse. Using a dataset of 100 recent expressions from gaming platforms, social media, and video content, the study reveals critical comprehension failures with direct implications for online safety. This work contributes: (1) a first-of-its-kind dataset capturing Gen Alpha expressions; (2) a framework to improve AI moderation systems for youth protection; (3) a multi-perspective evaluation including AI systems, human moderators, and parents, with direct input from Gen Alpha co-researchers; and (4) an analysis of how linguistic divergence increases youth vulnerability. Findings highlight the urgent need to redesign safety systems attuned to youth communication, especially given Gen Alpha reluctance to seek help when adults fail to understand their digital world. This study combines the insight of a Gen Alpha researcher with systematic academic analysis to address critical digital safety challenges.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment</title>
<link>https://arxiv.org/abs/2505.10597</link>
<guid>https://arxiv.org/abs/2505.10597</guid>
<content:encoded><![CDATA[
arXiv:2505.10597v1 Announce Type: cross 
Abstract: Reward models (RMs) are essential for aligning large language models (LLMs) with human values. However, noisy preferences in human feedback often lead to reward misgeneralization, where RMs overfit to spurious patterns and provide misleading signals during policy optimization. We systematically analyze the training dynamics of preference pairs and identify that noisy examples are harder to fit and introduce instability. Empirical evidence shows that LLMs optimized using reward models trained on full noisy datasets perform worse than those trained on filtered, high-quality preferences. To address this, we propose Collaborative Reward Modeling (CRM), an online framework that enhances robustness by combining peer review and curriculum learning. Two reward models are trained in parallel and assess each other's data selections to filter out potential noise. Curriculum learning structures the preference data from easy to hard, ensuring synchronized training and stable feedback. Extensive experiments demonstrate that CRM improves generalization, with up to 9.94 points of accuracy gain on RewardBench under 40 percent label noise. CRM is also compatible with implicit-reward alignment methods, offering a practical and versatile strategy for robust alignment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech</title>
<link>https://arxiv.org/abs/2505.10599</link>
<guid>https://arxiv.org/abs/2505.10599</guid>
<content:encoded><![CDATA[
arXiv:2505.10599v1 Announce Type: cross 
Abstract: Recent neural codec language models have made great progress in the field of text-to-speech (TTS), but controllable emotional TTS still faces many challenges. Traditional methods rely on predefined discrete emotion labels to control emotion categories and intensities, which can't capture the complexity and continuity of human emotional perception and expression. The lack of large-scale emotional speech datasets with balanced emotion distributions and fine-grained emotion annotations often causes overfitting in synthesis models and impedes effective emotion control. To address these issues, we propose UDDETTS, a neural codec language model unifying discrete and dimensional emotions for controllable emotional TTS. This model introduces the interpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion description and supports emotion control driven by either discrete emotion labels or nonlinearly quantified ADV values. Furthermore, a semi-supervised training strategy is designed to comprehensively utilize diverse speech datasets with different types of emotion annotations to train the UDDETTS. Experiments show that UDDETTS achieves linear emotion control along the three dimensions of ADV space, and exhibits superior end-to-end emotional speech synthesis capabilities.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly</title>
<link>https://arxiv.org/abs/2505.10610</link>
<guid>https://arxiv.org/abs/2505.10610</guid>
<content:encoded><![CDATA[
arXiv:2505.10610v1 Announce Type: cross 
Abstract: The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creating General User Models from Computer Use</title>
<link>https://arxiv.org/abs/2505.10831</link>
<guid>https://arxiv.org/abs/2505.10831</guid>
<content:encoded><![CDATA[
arXiv:2505.10831v1 Announce Type: cross 
Abstract: Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs</title>
<link>https://arxiv.org/abs/2505.10838</link>
<guid>https://arxiv.org/abs/2505.10838</guid>
<content:encoded><![CDATA[
arXiv:2505.10838v1 Announce Type: cross 
Abstract: Efficient red-teaming method to uncover vulnerabilities in Large Language Models (LLMs) is crucial. While recent attacks often use LLMs as optimizers, the discrete language space make gradient-based methods struggle. We introduce LARGO (Latent Adversarial Reflection through Gradient Optimization), a novel latent self-reflection attack that reasserts the power of gradient-based optimization for generating fluent jailbreaking prompts. By operating within the LLM's continuous latent space, LARGO first optimizes an adversarial latent vector and then recursively call the same LLM to decode the latent into natural language. This methodology yields a fast, effective, and transferable attack that produces fluent and stealthy prompts. On standard benchmarks like AdvBench and JailbreakBench, LARGO surpasses leading jailbreaking techniques, including AutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent alternative to agentic LLM prompting, highlighting the efficacy of interpreting and attacking LLM internals through gradient optimization.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models</title>
<link>https://arxiv.org/abs/2505.10844</link>
<guid>https://arxiv.org/abs/2505.10844</guid>
<content:encoded><![CDATA[
arXiv:2505.10844v1 Announce Type: cross 
Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatTools: Benchmarking Large Language Models for Materials Science Tools</title>
<link>https://arxiv.org/abs/2505.10852</link>
<guid>https://arxiv.org/abs/2505.10852</guid>
<content:encoded><![CDATA[
arXiv:2505.10852v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?</title>
<link>https://arxiv.org/abs/2505.10872</link>
<guid>https://arxiv.org/abs/2505.10872</guid>
<content:encoded><![CDATA[
arXiv:2505.10872v1 Announce Type: cross 
Abstract: Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory</title>
<link>https://arxiv.org/abs/2505.10981</link>
<guid>https://arxiv.org/abs/2505.10981</guid>
<content:encoded><![CDATA[
arXiv:2505.10981v1 Announce Type: cross 
Abstract: Recently, scaling test-time compute on Large Language Models (LLM) has garnered wide attention. However, there has been limited investigation of how various reasoning prompting strategies perform as scaling. In this paper, we focus on a standard and realistic scaling setting: majority voting. We systematically conduct experiments on 6 LLMs $\times$ 8 prompting strategies $\times$ 6 benchmarks. Experiment results consistently show that as the sampling time and computational overhead increase, complicated prompting strategies with superior initial performance gradually fall behind simple Chain-of-Thought. We analyze this phenomenon and provide theoretical proofs. Additionally, we propose a method according to probability theory to quickly and accurately predict the scaling performance and select the best strategy under large sampling times without extra resource-intensive inference in practice. It can serve as the test-time scaling law for majority voting. Furthermore, we introduce two ways derived from our theoretical analysis to significantly improve the scaling performance. We hope that our research can promote to re-examine the role of complicated prompting, unleash the potential of simple prompting strategies, and provide new insights for enhancing test-time scaling performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection</title>
<link>https://arxiv.org/abs/2505.11079</link>
<guid>https://arxiv.org/abs/2505.11079</guid>
<content:encoded><![CDATA[
arXiv:2505.11079v1 Announce Type: cross 
Abstract: Audio deepfake detection (ADD) has grown increasingly important due to the rise of high-fidelity audio generative models and their potential for misuse. Given that audio large language models (ALLMs) have made significant progress in various audio processing tasks, a heuristic question arises: Can ALLMs be leveraged to solve ADD?. In this paper, we first conduct a comprehensive zero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in detecting fake audio. To enhance their performance, we propose $\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we reformulate ADD task as an audio question answering problem, prompting the model with the question: "Is this audio fake or real?". We then perform supervised fine-tuning to enable the ALLM to assess the authenticity of query audio. Extensive experiments are conducted to demonstrate that our ALLM-based method can achieve superior performance in fake audio detection, particularly in data-scarce scenarios. As a pioneering study, we anticipate that this work will inspire the research community to leverage ALLMs to develop more effective ADD systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MPMA: Preference Manipulation Attack Against Model Context Protocol</title>
<link>https://arxiv.org/abs/2505.11154</link>
<guid>https://arxiv.org/abs/2505.11154</guid>
<content:encoded><![CDATA[
arXiv:2505.11154v1 Announce Type: cross 
Abstract: Model Context Protocol (MCP) standardizes interface mapping for large language models (LLMs) to access external data and tools, which revolutionizes the paradigm of tool selection and facilitates the rapid expansion of the LLM agent tool ecosystem. However, as the MCP is increasingly adopted, third-party customized versions of the MCP server expose potential security vulnerabilities. In this paper, we first introduce a novel security threat, which we term the MCP Preference Manipulation Attack (MPMA). An attacker deploys a customized MCP server to manipulate LLMs, causing them to prioritize it over other competing MCP servers. This can result in economic benefits for attackers, such as revenue from paid MCP services or advertising income generated from free servers. To achieve MPMA, we first design a Direct Preference Manipulation Attack ($\mathtt{DPMA}$) that achieves significant effectiveness by inserting the manipulative word and phrases into the tool name and description. However, such a direct modification is obvious to users and lacks stealthiness. To address these limitations, we further propose Genetic-based Advertising Preference Manipulation Attack ($\mathtt{GAPMA}$). $\mathtt{GAPMA}$ employs four commonly used strategies to initialize descriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness. The experiment results demonstrate that $\mathtt{GAPMA}$ balances high effectiveness and stealthiness. Our study reveals a critical vulnerability of the MCP in open ecosystems, highlighting an urgent need for robust defense mechanisms to ensure the fairness of the MCP ecosystem.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Asynchronicity in Event-based Neural Networks</title>
<link>https://arxiv.org/abs/2505.11165</link>
<guid>https://arxiv.org/abs/2505.11165</guid>
<content:encoded><![CDATA[
arXiv:2505.11165v1 Announce Type: cross 
Abstract: Event cameras deliver visual data with high temporal resolution, low latency, and minimal redundancy, yet their asynchronous, sparse sequential nature challenges standard tensor-based machine learning (ML). While the recent asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by asynchronously encoding events into learned representations for ML pipelines, existing A2S approaches often sacrifice representation expressivity and generalizability compared to dense, synchronous methods. This paper introduces EVA (EVent Asynchronous representation learning), a novel A2S framework to generate highly expressive and generalizable event-by-event representations. Inspired by the analogy between events and language, EVA uniquely adapts advances from language modeling in linear attention and self-supervised learning for its construction. In demonstration, EVA outperforms prior A2S methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the first A2S framework to successfully master demanding detection tasks, achieving a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's transformative potential for advancing real-time event-based vision applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback</title>
<link>https://arxiv.org/abs/2505.11178</link>
<guid>https://arxiv.org/abs/2505.11178</guid>
<content:encoded><![CDATA[
arXiv:2505.11178v1 Announce Type: cross 
Abstract: State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest's feedback as preference signals to improve diffusion models' compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms</title>
<link>https://arxiv.org/abs/2505.11183</link>
<guid>https://arxiv.org/abs/2505.11183</guid>
<content:encoded><![CDATA[
arXiv:2505.11183v1 Announce Type: cross 
Abstract: Probabilistic next-token prediction trained using cross-entropy loss is the basis of most large language models. Given a sequence of previous values, next-token prediction assigns a probability to each possible next value in the vocabulary. There are many ways to use next-token prediction to output token sequences. This paper examines a few of these algorithms (greedy, lookahead, random sampling, and temperature-scaled random sampling) and studies their consistency with respect to various goals encoded as loss functions. Although consistency of surrogate losses with respect to a target loss function is a well researched topic, we are the first to study it in the context of LLMs (to the best of our knowledge). We find that, so long as next-token prediction converges to its true probability distribution, random sampling is consistent with outputting sequences that mimic sampling from the true probability distribution. For the other goals, such as minimizing the 0-1 loss on the entire sequence, we show no polynomial-time algorithm is optimal for all probability distributions and all decoding algorithms studied are only optimal for a subset of probability distributions. When analyzing these results, we see that there is a dichotomy created between the goals of information retrieval and creative generation for the decoding algorithms. This shows that choosing the correct decoding algorithm based on the desired goal is extremely important and many of the ones used are lacking theoretical grounding in numerous scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese</title>
<link>https://arxiv.org/abs/2505.11200</link>
<guid>https://arxiv.org/abs/2505.11200</guid>
<content:encoded><![CDATA[
arXiv:2505.11200v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have significantly improved text-to-speech (TTS) systems, enhancing control over speech style, naturalness, and emotional expression, which brings TTS Systems closer to human-level performance. Although the Mean Opinion Score (MOS) remains the standard for TTS System evaluation, it suffers from subjectivity, environmental inconsistencies, and limited interpretability. Existing evaluation datasets also lack a multi-dimensional design, often neglecting factors such as speaking styles, context diversity, and trap utterances, which is particularly evident in Chinese TTS evaluation. To address these challenges, we introduce the Audio Turing Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired with a simple, Turing-Test-inspired evaluation protocol. Instead of relying on complex MOS scales or direct model comparisons, ATT asks evaluators to judge whether a voice sounds human. This simplification reduces rating bias and improves evaluation robustness. To further support rapid model development, we also finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for automatic evaluation. Experimental results show that ATT effectively differentiates models across specific capability dimensions using its multi-dimensional design. Auto-ATT also demonstrates strong alignment with human evaluations, confirming its value as a fast and reliable assessment tool. The white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face Collection (https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.11274</link>
<guid>https://arxiv.org/abs/2505.11274</guid>
<content:encoded><![CDATA[
arXiv:2505.11274v1 Announce Type: cross 
Abstract: Recently, large reasoning models demonstrate exceptional performance on various tasks. However, reasoning models inefficiently over-process both trivial and complex queries, leading to resource waste and prolonged user latency. To address this challenge, we propose SelfBudgeter - a self-adaptive controllable reasoning strategy for efficient reasoning. Our approach adopts a dual-phase training paradigm: first, the model learns to pre-estimate the reasoning cost based on the difficulty of the query. Then, we introduce budget-guided GPRO for reinforcement learning, which effectively maintains accuracy while reducing output length. SelfBudgeter allows users to anticipate generation time and make informed decisions about continuing or interrupting the process. Furthermore, our method enables direct manipulation of reasoning length via pre-filling token budget. Experimental results demonstrate that SelfBudgeter can rationally allocate budgets according to problem complexity, achieving up to 74.47% response length compression on the MATH benchmark while maintaining nearly undiminished accuracy.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks</title>
<link>https://arxiv.org/abs/2505.11314</link>
<guid>https://arxiv.org/abs/2505.11314</guid>
<content:encoded><![CDATA[
arXiv:2505.11314v1 Announce Type: cross 
Abstract: The assessment of evaluation metrics (meta-evaluation) is crucial for determining the suitability of existing metrics in text-to-image (T2I) generation tasks. Human-based meta-evaluation is costly and time-intensive, and automated alternatives are scarce. We address this gap and propose CROC: a scalable framework for automated Contrastive Robustness Checks that systematically probes and quantifies metric robustness by synthesizing contrastive test cases across a comprehensive taxonomy of image properties. With CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one million contrastive prompt-image pairs to enable a fine-grained comparison of evaluation metrics. We also use the dataset to train CROCScore, a new metric that achieves state-of-the-art performance among open-source methods, demonstrating an additional key application of our framework. To complement this dataset, we introduce a human-supervised benchmark (CROC$^{hum}$) targeting especially challenging categories. Our results highlight robustness issues in existing metrics: for example, many fail on prompts involving negation, and all tested open-source metrics fail on at least 25% of cases involving correct identification of body parts.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phare: A Safety Probe for Large Language Models</title>
<link>https://arxiv.org/abs/2505.11365</link>
<guid>https://arxiv.org/abs/2505.11365</guid>
<content:encoded><![CDATA[
arXiv:2505.11365v1 Announce Type: cross 
Abstract: Ensuring the safety of large language models (LLMs) is critical for responsible deployment, yet existing evaluations often prioritize performance over identifying failure modes. We introduce Phare, a multilingual diagnostic framework to probe and evaluate LLM behavior across three critical dimensions: hallucination and reliability, social biases, and harmful content generation. Our evaluation of 17 state-of-the-art LLMs reveals patterns of systematic vulnerabilities across all safety dimensions, including sycophancy, prompt sensitivity, and stereotype reproduction. By highlighting these specific failure modes rather than simply ranking models, Phare provides researchers and practitioners with actionable insights to build more robust, aligned, and trustworthy language systems.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2505.11405</link>
<guid>https://arxiv.org/abs/2505.11405</guid>
<content:encoded><![CDATA[
arXiv:2505.11405v1 Announce Type: cross 
Abstract: Emotion understanding is a critical yet challenging task. Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced their capabilities in this area. However, MLLMs often suffer from hallucinations, generating irrelevant or nonsensical content. To the best of our knowledge, despite the importance of this issue, there has been no dedicated effort to evaluate emotion-related hallucinations in MLLMs. In this work, we introduce EmotionHallucer, the first benchmark for detecting and analyzing emotion hallucinations in MLLMs. Unlike humans, whose emotion understanding stems from the interplay of biology and social learning, MLLMs rely solely on data-driven learning and lack innate emotional instincts. Fortunately, emotion psychology provides a solid foundation of knowledge about human emotions. Building on this, we assess emotion hallucinations from two dimensions: emotion psychology knowledge and real-world multimodal perception. To support robust evaluation, we utilize an adversarial binary question-answer (QA) framework, which employs carefully crafted basic and hallucinated pairs to assess the emotion hallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on EmotionHallucer, we reveal that: i) most current models exhibit substantial issues with emotion hallucinations; ii) closed-source models outperform open-source ones in detecting emotion hallucinations, and reasoning capability provides additional advantages; iii) existing models perform better in emotion psychology knowledge than in multimodal emotion perception. As a byproduct, these findings inspire us to propose the PEP-MEK framework, which yields an average improvement of 9.90% in emotion hallucination detection across selected models. Resources will be available at https://github.com/xxtars/EmotionHallucer.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Model Use Impact Locus of Control</title>
<link>https://arxiv.org/abs/2505.11406</link>
<guid>https://arxiv.org/abs/2505.11406</guid>
<content:encoded><![CDATA[
arXiv:2505.11406v1 Announce Type: cross 
Abstract: As AI tools increasingly shape how we write, they may also quietly reshape how we perceive ourselves. This paper explores the psychological impact of co-writing with AI on people's locus of control. Through an empirical study with 462 participants, we found that employment status plays a critical role in shaping users' reliance on AI and their locus of control. Current results demonstrated that employed participants displayed higher reliance on AI and a shift toward internal control, while unemployed users tended to experience a reduction in personal agency. Through quantitative results and qualitative observations, this study opens a broader conversation about AI's role in shaping personal agency and identity.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visual Planning: Let's Think Only with Images</title>
<link>https://arxiv.org/abs/2505.11409</link>
<guid>https://arxiv.org/abs/2505.11409</guid>
<content:encoded><![CDATA[
arXiv:2505.11409v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising alternative to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator</title>
<link>https://arxiv.org/abs/2305.15099</link>
<guid>https://arxiv.org/abs/2305.15099</guid>
<content:encoded><![CDATA[
arXiv:2305.15099v2 Announce Type: replace 
Abstract: The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code is publicly available at https://github.com/LUMIA-Group/FourierTransformer
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?</title>
<link>https://arxiv.org/abs/2311.07564</link>
<guid>https://arxiv.org/abs/2311.07564</guid>
<content:encoded><![CDATA[
arXiv:2311.07564v4 Announce Type: replace 
Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on human-transcribed conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of neural and non-neural baselines, finding that although written text attribution models achieve surprisingly good performance in certain settings, they perform markedly worse as conversational topic is increasingly controlled. We present analyses of the impact of transcription style on performance as well as the ability of fine-tuning on speech transcripts to improve performance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models</title>
<link>https://arxiv.org/abs/2402.14889</link>
<guid>https://arxiv.org/abs/2402.14889</guid>
<content:encoded><![CDATA[
arXiv:2402.14889v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These benchmarks measure bias by observing an LLM's behavior on biased statements. However, these statements lack contextual considerations of the situations they try to present. To address this, we introduce a contextual reliability framework, which evaluates model robustness to biased statements by considering the various contexts in which they may appear. We develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's reliability in detecting bias, based on the variance in model behavior across different contexts. To evaluate the metric, we augmented 2,291 stereotyped statements from two existing benchmark datasets by adding contextual information. We show that COBIAS aligns with human judgment on the contextual reliability of biased statements (Spearman's $\rho = 0.65, p = 3.4 * 10^{-60}$) and can be used to create reliable benchmarks, which would assist bias mitigation works.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images</title>
<link>https://arxiv.org/abs/2404.10652</link>
<guid>https://arxiv.org/abs/2404.10652</guid>
<content:encoded><![CDATA[
arXiv:2404.10652v3 Announce Type: replace 
Abstract: Visual Question Answerinng (VQA) is a complicated task that requires the capability of simultaneously processing natural language and images. This task was initially researched with a focus on developing methods to help machines understand objects and scene contexts in images. However, some scene text that carries explicit information about the full content of the image is not mentioned. Along with the continuous development of the AI era, there have been many studies on the reading comprehension ability of VQA models in the world. Therefore, we introduce the first large-scale dataset in Vietnamese specializing in the ability to understand scene text, we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over 16,000} images and \textbf{over 50,000} questions with answers. To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion. Through experiments with various state-of-the-art models, we uncover the significance of the order in which tokens in OCR text are processed and selected to formulate answers. This finding helped us significantly improve the performance of the baseline models on the ViTextVQA dataset. Our dataset is available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research purposes.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation</title>
<link>https://arxiv.org/abs/2405.00715</link>
<guid>https://arxiv.org/abs/2405.00715</guid>
<content:encoded><![CDATA[
arXiv:2405.00715v5 Announce Type: replace 
Abstract: Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching</title>
<link>https://arxiv.org/abs/2406.06326</link>
<guid>https://arxiv.org/abs/2406.06326</guid>
<content:encoded><![CDATA[
arXiv:2406.06326v5 Announce Type: replace 
Abstract: Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models</title>
<link>https://arxiv.org/abs/2408.06518</link>
<guid>https://arxiv.org/abs/2408.06518</guid>
<content:encoded><![CDATA[
arXiv:2408.06518v3 Announce Type: replace 
Abstract: Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards understanding evolution of science through language model series</title>
<link>https://arxiv.org/abs/2409.09636</link>
<guid>https://arxiv.org/abs/2409.09636</guid>
<content:encoded><![CDATA[
arXiv:2409.09636v2 Announce Type: replace 
Abstract: We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and "one model to rule them all", AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model pretrained from scratch on the full-text of 1.7 million arXiv papers published until 2008 and a collection of progressively trained models on arXiv papers at an annual basis. We demonstrate the effectiveness of AnnualBERT models by showing that they not only have comparable performances in standard tasks but also achieve state-of-the-art performances on domain-specific NLP tasks as well as link prediction tasks in the arXiv citation network. We then utilize probing tasks to quantify the models' behavior in terms of representation learning and forgetting as time progresses. Our approach enables the pretrained models to not only improve performances on scientific text processing tasks but also to provide insights into the development of scientific discourse over time. The series of the models is available at https://huggingface.co/jd445/AnnualBERTs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach</title>
<link>https://arxiv.org/abs/2409.20204</link>
<guid>https://arxiv.org/abs/2409.20204</guid>
<content:encoded><![CDATA[
arXiv:2409.20204v2 Announce Type: replace 
Abstract: Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field. In this review, we make four key contributions: (1) we synthesize the literature into five core themes: definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions; (2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across disciplines; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain. Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured. Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration. Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies. Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training of Scaffolded Language Models with Language Supervision: A Survey</title>
<link>https://arxiv.org/abs/2410.16392</link>
<guid>https://arxiv.org/abs/2410.16392</guid>
<content:encoded><![CDATA[
arXiv:2410.16392v2 Announce Type: replace 
Abstract: This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework</title>
<link>https://arxiv.org/abs/2410.19453</link>
<guid>https://arxiv.org/abs/2410.19453</guid>
<content:encoded><![CDATA[
arXiv:2410.19453v5 Announce Type: replace 
Abstract: Although fine-tuning Large Language Models (LLMs) with multilingual data can rapidly enhance the multilingual capabilities of LLMs, they still exhibit a performance gap between the dominant language (e.g., English) and non-dominant ones due to the imbalance of training data across languages. To further enhance the performance of non-dominant languages, we propose ShifCon, a Shift-based Contrastive framework that aligns the internal forward process of other languages toward that of the dominant one. Specifically, it shifts the representations of non-dominant languages into the dominant language subspace, allowing them to access relatively rich information encoded in the model parameters. The enriched representations are then shifted back into their original language subspace before generation. Moreover, we introduce a subspace distance metric to pinpoint the optimal layer area for shifting representations and employ multilingual contrastive learning to further enhance the alignment of representations within this area. Experiments demonstrate that our ShifCon framework significantly enhances the performance of non-dominant languages, particularly for low-resource ones. Further analysis offers extra insights to verify the effectiveness of ShifCon and propel future research
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP</title>
<link>https://arxiv.org/abs/2411.05527</link>
<guid>https://arxiv.org/abs/2411.05527</guid>
<content:encoded><![CDATA[
arXiv:2411.05527v2 Announce Type: replace 
Abstract: Wikipedia's perceived high quality and broad language coverage have established it as a fundamental resource in multilingual NLP. In the context of low-resource languages, however, these quality assumptions are increasingly being scrutinised. This paper critically examines the data quality of Wikipedia in a non-English setting by subjecting it to various quality filtering techniques, revealing widespread issues such as a high percentage of one-line articles and duplicate articles. We evaluate the downstream impact of quality filtering on Wikipedia and find that data quality pruning is an effective means for resource-efficient training without hurting performance, especially for low-resource languages. Moreover, we advocate for a shift in perspective from seeking a general definition of data quality towards a more language- and task-specific one. Ultimately, we aim for this study to serve as a guide to using Wikipedia for pretraining in a multilingual setting.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction</title>
<link>https://arxiv.org/abs/2411.07019</link>
<guid>https://arxiv.org/abs/2411.07019</guid>
<content:encoded><![CDATA[
arXiv:2411.07019v3 Announce Type: replace 
Abstract: Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing link prediction models for beyond-triple facts have difficulty achieving hierarchical fact modeling and generalizing the modules for one specific facts to other fact types. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Empirical results demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/Lza12a/UniHR.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Role of Speech Data in Reducing Toxicity Detection Bias</title>
<link>https://arxiv.org/abs/2411.08135</link>
<guid>https://arxiv.org/abs/2411.08135</guid>
<content:encoded><![CDATA[
arXiv:2411.08135v2 Announce Type: replace 
Abstract: Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTox dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When to Speak, When to Abstain: Contrastive Decoding with Abstention</title>
<link>https://arxiv.org/abs/2412.12527</link>
<guid>https://arxiv.org/abs/2412.12527</guid>
<content:encoded><![CDATA[
arXiv:2412.12527v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e., contextual) knowledge. While substantial efforts have been made to enhance the utilization of both forms of knowledge, situations in which models lack relevant information remain underexplored. To investigate this challenge, we first present a controlled testbed featuring four distinct knowledge access scenarios, including the aforementioned edge case, revealing that conventional LLM usage exhibits insufficient robustness in handling all instances. Addressing this limitation, we propose Contrastive Decoding with Abstention (CDA), a novel training-free decoding method that allows LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA estimates the relevance of both knowledge sources for a given input, adaptively deciding which type of information to prioritize and which to exclude. Through extensive experiments, we demonstrate that CDA can effectively perform accurate generation and abstention simultaneously, enhancing reliability and preserving user trust.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context</title>
<link>https://arxiv.org/abs/2412.12632</link>
<guid>https://arxiv.org/abs/2412.12632</guid>
<content:encoded><![CDATA[
arXiv:2412.12632v2 Announce Type: replace 
Abstract: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and evaluate LLMs' effectiveness, faithfulness and robustness with CoE, including its application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs show CoE improves generation accuracy, answer faithfulness, robustness to knowledge conflicts, and boosts the performance of existing approaches in three practical RAG scenarios.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2412.15529</link>
<guid>https://arxiv.org/abs/2412.15529</guid>
<content:encoded><![CDATA[
arXiv:2412.15529v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines</title>
<link>https://arxiv.org/abs/2501.00745</link>
<guid>https://arxiv.org/abs/2501.00745</guid>
<content:encoded><![CDATA[
arXiv:2501.00745v2 Announce Type: replace 
Abstract: The increasing integration of Large Language Model (LLM) based search engines has transformed the landscape of information retrieval. However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors. In this paper, we study the dynamics of ranking manipulation attacks. We frame this problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players strategically decide whether to cooperate or attack. We analyze the conditions under which cooperation can be sustained, identifying key factors such as attack costs, discount rates, attack success rates, and trigger strategies that influence player behavior. We identify tipping points in the system dynamics, demonstrating that cooperation is more likely to be sustained when players are forward-looking. However, from a defense perspective, we find that simply reducing attack success probabilities can, paradoxically, incentivize attacks under certain conditions. Furthermore, defensive measures to cap the upper bound of attack success rates may prove futile in some scenarios. These insights highlight the complexity of securing LLM-based systems. Our work provides a theoretical foundation and practical insights for understanding and mitigating their vulnerabilities, while emphasizing the importance of adaptive security strategies and thoughtful ecosystem design.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena</title>
<link>https://arxiv.org/abs/2501.03266</link>
<guid>https://arxiv.org/abs/2501.03266</guid>
<content:encoded><![CDATA[
arXiv:2501.03266v2 Announce Type: replace 
Abstract: LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TreeKV: Smooth Key-Value Cache Compression with Tree Structures</title>
<link>https://arxiv.org/abs/2501.04987</link>
<guid>https://arxiv.org/abs/2501.04987</guid>
<content:encoded><![CDATA[
arXiv:2501.04987v3 Announce Type: replace 
Abstract: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling</title>
<link>https://arxiv.org/abs/2501.10316</link>
<guid>https://arxiv.org/abs/2501.10316</guid>
<content:encoded><![CDATA[
arXiv:2501.10316v3 Announce Type: replace 
Abstract: Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI's suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine</title>
<link>https://arxiv.org/abs/2501.11885</link>
<guid>https://arxiv.org/abs/2501.11885</guid>
<content:encoded><![CDATA[
arXiv:2501.11885v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. Despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.74\% improvement over vanilla RAG methods and even a 3.32\% enhancement compared to fine-tuning strategies, without incurring additional training costs.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms</title>
<link>https://arxiv.org/abs/2501.13977</link>
<guid>https://arxiv.org/abs/2501.13977</guid>
<content:encoded><![CDATA[
arXiv:2501.13977v2 Announce Type: replace 
Abstract: Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do we really have to filter out random noise in pre-training data for language models?</title>
<link>https://arxiv.org/abs/2502.06604</link>
<guid>https://arxiv.org/abs/2502.06604</guid>
<content:encoded><![CDATA[
arXiv:2502.06604v2 Announce Type: replace 
Abstract: Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the Internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \textbf{provides the first systematic investigation of such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in the loss of next-token prediction (NTP) was significantly lower than the proportion of random noise even when the model was scaled up to 2.7B. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models and can be applied to multimodal models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging</title>
<link>https://arxiv.org/abs/2502.06876</link>
<guid>https://arxiv.org/abs/2502.06876</guid>
<content:encoded><![CDATA[
arXiv:2502.06876v3 Announce Type: replace 
Abstract: Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\textit{data-level}) and model merging (\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \textbf{R}eweighting \textbf{E}nhanced task \textbf{S}ingular \textbf{M}erging method, \textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\%-5\% gain) and model merging (1\%-3\% gain) methods in achieving balanced LLM alignment. We release our models through \href{https://huggingface.co/Jinluan}{3H\_Merging} for further investigations.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More</title>
<link>https://arxiv.org/abs/2502.07490</link>
<guid>https://arxiv.org/abs/2502.07490</guid>
<content:encoded><![CDATA[
arXiv:2502.07490v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucination, Monofacts, and Miscalibration: An Empirical Investigation</title>
<link>https://arxiv.org/abs/2502.08666</link>
<guid>https://arxiv.org/abs/2502.08666</guid>
<content:encoded><![CDATA[
arXiv:2502.08666v2 Announce Type: replace 
Abstract: Hallucinated facts in large language models (LLMs) have recently been shown to obey a statistical lower bound determined by the monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration (Kalai & Vempala, 2024). We present the first empirical investigation of this three-way relationship in classical n-gram models and fine-tuned encoder-decoder Transformers. By generating training data from Pareto distributions with varying shape parameters, we systematically control the monofact rates and establish its positive relationship with hallucination. To bridge theory and practice, we derive an empirical analog of the hallucination bound by replacing the population miscalibration term (Section 2.1) with an empirical bin-wise KL divergence and confirm its practical viability. We then introduce selective upweighting -- a simple yet effective technique that strategically repeats as little as 5% of training examples -- to deliberately inject miscalibration into the model. This intervention reduces hallucination by up to 40%, challenging universal deduplication policies. Our experiments reveal a critical trade-off: selective upweighting maintains pre-injection levels of accuracy while substantially reducing hallucination, whereas standard training gradually improves accuracy but fails to address persistently high hallucination, indicating an inherent tension in optimization objectives.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Investigating Language Preference of Multilingual RAG Systems</title>
<link>https://arxiv.org/abs/2502.11175</link>
<guid>https://arxiv.org/abs/2502.11175</guid>
<content:encoded><![CDATA[
arXiv:2502.11175v2 Announce Type: replace 
Abstract: Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Your Uncertainty Scores Detect Hallucinated Entity?</title>
<link>https://arxiv.org/abs/2502.11948</link>
<guid>https://arxiv.org/abs/2502.11948</guid>
<content:encoded><![CDATA[
arXiv:2502.11948v2 Announce Type: replace 
Abstract: To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research. HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>iAgent: LLM Agent as a Shield between User and Recommender Systems</title>
<link>https://arxiv.org/abs/2502.14662</link>
<guid>https://arxiv.org/abs/2502.14662</guid>
<content:encoded><![CDATA[
arXiv:2502.14662v2 Announce Type: replace 
Abstract: Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing</title>
<link>https://arxiv.org/abs/2502.15208</link>
<guid>https://arxiv.org/abs/2502.15208</guid>
<content:encoded><![CDATA[
arXiv:2502.15208v2 Announce Type: replace 
Abstract: Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Call for Rigor in Reporting Quality of Instruction Tuning Data</title>
<link>https://arxiv.org/abs/2503.04807</link>
<guid>https://arxiv.org/abs/2503.04807</guid>
<content:encoded><![CDATA[
arXiv:2503.04807v3 Announce Type: replace 
Abstract: Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TigerLLM -- A Family of Bangla Large Language Models</title>
<link>https://arxiv.org/abs/2503.10995</link>
<guid>https://arxiv.org/abs/2503.10995</guid>
<content:encoded><![CDATA[
arXiv:2503.10995v2 Announce Type: replace 
Abstract: The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse</title>
<link>https://arxiv.org/abs/2503.16525</link>
<guid>https://arxiv.org/abs/2503.16525</guid>
<content:encoded><![CDATA[
arXiv:2503.16525v2 Announce Type: replace 
Abstract: Recent advances in long-text understanding have pushed the context length of large language models (LLMs) up to one million tokens. It boosts LLMs's accuracy and reasoning capacity but causes exorbitant computational costs and unsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the exact same KV cache of prefixes and templates or shares similar ones but with extra selective recomputation, offers a promising way to tackle this issue. However, prior studies overlook the cross-request KV reuse and the attention deviations introduced by new tokens during the decoding stage. In this paper, we present a KV cache management module that shares the KV cache across requests under multi-tenant scenarios without sacrificing model accuracy. Our system, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage High Deviation algorithm (DHD) that conditionally selects a small portion of KV cache to be recomputed during both prefill and decode phases, and 2) a cache-aware scheduler that prioritizes requests based on their KV cache hit rates and orchestrates continuous batching to achieve enhanced system efficiency and faster TTFT. Multi-task experiments conducted on models such as Qwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up to 9.39x and increases 1.2x of the throughput compared to the full KV recompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy compared to SOTA methods.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts</title>
<link>https://arxiv.org/abs/2503.16529</link>
<guid>https://arxiv.org/abs/2503.16529</guid>
<content:encoded><![CDATA[
arXiv:2503.16529v2 Announce Type: replace 
Abstract: DeepSeek-R1, renowned for its exceptional reasoning capabilities and open-source strategy, is significantly influencing the global artificial intelligence landscape. However, it exhibits notable safety shortcomings. Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 achieves a 100\% attack success rate when processing harmful prompts. Furthermore, multiple security firms and research institutions have identified critical security vulnerabilities within the model. Although China Unicom has uncovered safety vulnerabilities of R1 in Chinese contexts, the safety capabilities of the remaining distilled models in the R1 series have not yet been comprehensively evaluated. To address this gap, this study utilizes the comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth safety evaluation of the DeepSeek-R1 series distilled models. The objective is to assess the safety capabilities of these models in Chinese contexts both before and after distillation, and to further elucidate the adverse effects of distillation on model safety. Building on these findings, we implement targeted safety enhancements for the entire DeepSeek-R1 model series. Evaluation results indicate that the enhanced models achieve significant improvements in safety while maintaining reasoning capabilities without notable degradation. We open-source the safety-enhanced models at https://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource for future research and optimization of DeepSeek models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Routers</title>
<link>https://arxiv.org/abs/2503.23362</link>
<guid>https://arxiv.org/abs/2503.23362</guid>
<content:encoded><![CDATA[
arXiv:2503.23362v2 Announce Type: replace 
Abstract: Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: https://anonymous.4open.science/r/MoR-DFC6.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?</title>
<link>https://arxiv.org/abs/2504.01698</link>
<guid>https://arxiv.org/abs/2504.01698</guid>
<content:encoded><![CDATA[
arXiv:2504.01698v3 Announce Type: replace 
Abstract: Theory of Mind (ToM), the ability to attribute mental states to others, is fundamental for human social intelligence and a critical capability for advanced Artificial Intelligence. Recent advancements in Large Language Models (LLMs) have shown promising performance on ToM benchmarks, raising the question: Do these benchmarks necessitate explicit human-like reasoning processes, or can models succeed through alternative strategies? We investigate this question empirically by applying Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters) and evaluating them across multiple ToM datasets. Our results reveal a scale-dependent impact of RL: while RL significantly improves accuracy and fosters high-quality, interpretable, and transferable belief-tracking reasoning in larger models (7B), it leads to "reasoning collapse" in smaller models ($\leq$3B), where high accuracy and generalization ability are achieved via drastically shortened, less meaningful responses. Surprisingly, further SFT achieves competitive and generalizable performance across these benchmarks, often matching or exceeding RL models in accuracy, despite not being explicitly trained to produce structured reasoning traces. These findings highlight a critical discrepancy between benchmark accuracy and the nature of learned reasoning. Our work suggests that current ToM benchmarks may be solvable without requiring the explicit, human-like simulation of mental states they were designed to probe. LLMs, particularly when scale is limited or training signals focus solely on output correctness, may leverage alternative rules effective for benchmark data structures.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameterized Synthetic Text Generation with SimpleStories</title>
<link>https://arxiv.org/abs/2504.09184</link>
<guid>https://arxiv.org/abs/2504.09184</guid>
<content:encoded><![CDATA[
arXiv:2504.09184v2 Announce Type: replace 
Abstract: We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million samples each in English and Japanese. Through parameterizing prompts at multiple levels of abstraction, we achieve control over story characteristics at scale, inducing syntactic and semantic diversity. Ablations on a newly trained model suite show improved sample efficiency and model interpretability compared to the TinyStories dataset. We open-source all constituent parts of model creation, hoping to enable novel ways to study the end-to-end training process. As a byproduct, we move the frontier regarding the fewest-parameter language model that outputs grammatical natural language.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Safety in Large Reasoning Models: A Survey</title>
<link>https://arxiv.org/abs/2504.17704</link>
<guid>https://arxiv.org/abs/2504.17704</guid>
<content:encoded><![CDATA[
arXiv:2504.17704v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning</title>
<link>https://arxiv.org/abs/2403.11083</link>
<guid>https://arxiv.org/abs/2403.11083</guid>
<content:encoded><![CDATA[
arXiv:2403.11083v2 Announce Type: replace-cross 
Abstract: Anomaly detection is vital in various industrial scenarios, including the identification of unusual patterns in production lines and the detection of manufacturing defects for quality control. Existing techniques tend to be specialized in individual scenarios and lack generalization capacities. In this study, our objective is to develop a generic anomaly detection model that can be applied in multiple scenarios. To achieve this, we custom-build generic visual language foundation models that possess extensive knowledge and robust reasoning abilities as anomaly detectors and reasoners. Specifically, we introduce a multi-modal prompting strategy that incorporates domain knowledge from experts as conditions to guide the models. Our approach considers diverse prompt types, including task descriptions, class context, normality rules, and reference images. In addition, we unify the input representation of multi-modality into a 2D image format, enabling multi-modal anomaly detection and reasoning. Our preliminary studies demonstrate that combining visual and language prompts as conditions for customizing the models enhances anomaly detection performance. The customized models showcase the ability to detect anomalies across different data modalities such as images, point clouds, and videos. Qualitative case studies further highlight the anomaly detection and reasoning capabilities, particularly for multi-object scenes and temporal data. Our code is publicly available at https://github.com/Xiaohao-Xu/Customizable-VLM
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear Attention Sequence Parallelism</title>
<link>https://arxiv.org/abs/2404.02882</link>
<guid>https://arxiv.org/abs/2404.02882</guid>
<content:encoded><![CDATA[
arXiv:2404.02882v3 Announce Type: replace-cross 
Abstract: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. Code is available at: https://github.com/OpenNLPLab/LASP.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective</title>
<link>https://arxiv.org/abs/2405.13522</link>
<guid>https://arxiv.org/abs/2405.13522</guid>
<content:encoded><![CDATA[
arXiv:2405.13522v3 Announce Type: replace-cross 
Abstract: Traditional time series forecasting methods predominantly rely on historical data patterns, neglecting external interventions that significantly shape future dynamics. Through control-theoretic analysis, we show that the implicit "self-stimulation" assumption limits the accuracy of these forecasts. To overcome this limitation, we propose an Intervention-Aware Time Series Forecasting (IATSF) framework explicitly designed to incorporate external interventions. We particularly emphasize textual interventions due to their unique capability to represent qualitative or uncertain influences inadequately captured by conventional exogenous variables. We propose a leak-free benchmark composed of temporally synchronized textual intervention data across synthetic and real-world scenarios. To rigorously evaluate IATSF, we develop FIATS, a lightweight forecasting model that integrates textual interventions through Channel-Aware Adaptive Sensitivity Modeling (CASM) and Channel-Aware Parameter Sharing (CAPS) mechanisms, enabling the model to adjust its sensitivity to interventions and historical data in a channel-specific manner. Extensive empirical evaluations confirm that FIATS surpasses state-of-the-art methods, highlighting that forecasting improvements stem explicitly from modeling external interventions rather than increased model complexity alone.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Item-Language Model for Conversational Recommendation</title>
<link>https://arxiv.org/abs/2406.02844</link>
<guid>https://arxiv.org/abs/2406.02844</guid>
<content:encoded><![CDATA[
arXiv:2406.02844v2 Announce Type: replace-cross 
Abstract: Large-language Models (LLMs) have been extremely successful at tasks like complex dialogue understanding, reasoning and coding due to their emergent abilities. These emergent abilities have been extended with multi-modality to include image, audio, and video capabilities. Recommender systems, on the other hand, have been critical for information seeking and item discovery needs. Recently, there have been attempts to apply LLMs for recommendations. One difficulty of current attempts is that the underlying LLM is usually not trained on the recommender system data, which largely contains user interaction signals and is often not publicly available. Another difficulty is user interaction signals often have a different pattern from natural language text, and it is currently unclear if the LLM training setup can learn more non-trivial knowledge from interaction signals compared with traditional recommender system methods. Finally, it is difficult to train multiple LLMs for different use-cases, and to retain the original language and reasoning abilities when learning from recommender system data. To address these three limitations, we propose an Item-Language Model (ILM), which is composed of an item encoder to produce text-aligned item representations that encode user interaction signals, and a frozen LLM that can understand those item representations with preserved pretrained knowledge. We conduct extensive experiments which demonstrate both the importance of the language-alignment and of user interaction knowledge in the item encoder.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers</title>
<link>https://arxiv.org/abs/2406.11624</link>
<guid>https://arxiv.org/abs/2406.11624</guid>
<content:encoded><![CDATA[
arXiv:2406.11624v5 Announce Type: replace-cross 
Abstract: Transformer-based models generate hidden states that are difficult to interpret. In this work, we analyze hidden states and modify them at inference, with a focus on motion forecasting. We use linear probing to analyze whether interpretable features are embedded in hidden states. Our experiments reveal high probing accuracy, indicating latent space regularities with functionally important directions. Building on this, we use the directions between hidden states with opposing features to fit control vectors. At inference, we add our control vectors to hidden states and evaluate their impact on predictions. Remarkably, such modifications preserve the feasibility of predictions. We further refine our control vectors using sparse autoencoders (SAEs). This leads to more linear changes in predictions when scaling control vectors. Our approach enables mechanistic interpretation as well as zero-shot generalization to unseen dataset characteristics with negligible computational overhead.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions</title>
<link>https://arxiv.org/abs/2410.00031</link>
<guid>https://arxiv.org/abs/2410.00031</guid>
<content:encoded><![CDATA[
arXiv:2410.00031v2 Announce Type: replace-cross 
Abstract: Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction</title>
<link>https://arxiv.org/abs/2410.11507</link>
<guid>https://arxiv.org/abs/2410.11507</guid>
<content:encoded><![CDATA[
arXiv:2410.11507v4 Announce Type: replace-cross 
Abstract: As large language models (LLMs) are increasingly deployed to various vertical domains, automatically evaluating their performance across different domains remains a critical challenge. Current evaluation methods often rely on static and resource-intensive datasets that are not aligned with real-world requirements and lack cross-domain adaptability. To address these limitations, we revisit the evaluation process and introduce two key concepts: \textbf{Benchmark+}, which extends the traditional question-answer benchmark into a more flexible ``strategy-criterion'' format; and \textbf{Assessment+}, which enhances the interaction process to facilitate deeper exploration and comprehensive analysis from multiple perspectives. We propose \textbf{\textsc{TestAgent}}, an agent-based evaluation framework that implements these concepts using retrieval-augmented generation and reinforcement learning. \textsc{TestAgent} enables automatic dynamic benchmark generation and in-depth assessment across diverse vertical domains. Experiments on tasks ranging from constructing multiple vertical domain evaluations to transforming static benchmarks into dynamic forms demonstrate the effectiveness of \textsc{TestAgent}. This work provides a novel perspective on automatic evaluation methods for domain-specific LLMs, offering a pathway for domain-adaptive dynamic benchmark construction and exploratory assessment.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search</title>
<link>https://arxiv.org/abs/2410.14609</link>
<guid>https://arxiv.org/abs/2410.14609</guid>
<content:encoded><![CDATA[
arXiv:2410.14609v2 Announce Type: replace-cross 
Abstract: Conversational Search (CS) involves retrieving relevant documents from a corpus while considering the conversational context, integrating retrieval with context modeling. Recent advancements in Large Language Models (LLMs) have significantly enhanced CS by enabling query rewriting based on conversational context. However, employing LLMs during inference poses efficiency challenges. Existing solutions mitigate this issue by distilling embeddings derived from human-rewritten queries, focusing primarily on learning the context modeling task. These methods, however, often separate the contrastive retrieval task from the distillation process, treating it as an independent loss term. To overcome these limitations, we introduce DiSCo (Distillation of Sparse Conversational retrieval), a novel approach that unifies retrieval and context modeling through a relaxed distillation objective. Instead of relying exclusively on representation learning, our method distills similarity scores between conversations and documents, providing more freedom in the representation space and better leveraging the contrastive nature of document relevance. Extensive experiments on Learned Sparse Retrieval (LSR) across five CS datasets demonstrate that DiSCo achieves substantial improvements in both in-domain and out-of-domain retrieval tasks, achieving up to a six-point gain in recall for out-of-domain datasets over state-of-the-art methods. Additionally, DiSCo employs a multi-teacher distillation strategy, using multiple LLMs as teachers, further enhancing performance and surpassing the individual teachers in in-domain settings. Furthermore, analysis of model sparsity reveals that DiSCo allows for more effective control over the sparsity of the trained models.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection</title>
<link>https://arxiv.org/abs/2410.14731</link>
<guid>https://arxiv.org/abs/2410.14731</guid>
<content:encoded><![CDATA[
arXiv:2410.14731v2 Announce Type: replace-cross 
Abstract: KV cache has become a de facto technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. As the size of the model and data grows, the KV cache can quickly become a bottleneck within the system in both storage and memory transfer. To address this, prior studies usually focus on the first three axes of the cache tensors for compression. This paper supplements them, focusing on the feature dimension axis, by utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. We begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). We observe the issue with PCA projection where significant performance degradation is observed at low compression rates. To bridge the gap, we propose to directly tune the orthogonal projection matrices with a distillation objective using an elaborate Matryoshka training strategy. After training, we adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. Compared to previous works, our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. We empirically witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title>
<link>https://arxiv.org/abs/2411.02335</link>
<guid>https://arxiv.org/abs/2411.02335</guid>
<content:encoded><![CDATA[
arXiv:2411.02335v3 Announce Type: replace-cross 
Abstract: Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs). Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-$p\%$ sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., $1-\mathrm{sparsity\ ratio}$) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Vision-Language Models as Evaluators in Path Planning</title>
<link>https://arxiv.org/abs/2411.18711</link>
<guid>https://arxiv.org/abs/2411.18711</guid>
<content:encoded><![CDATA[
arXiv:2411.18711v4 Announce Type: replace-cross 
Abstract: Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce PathEval, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities</title>
<link>https://arxiv.org/abs/2501.02406</link>
<guid>https://arxiv.org/abs/2501.02406</guid>
<content:encoded><![CDATA[
arXiv:2501.02406v4 Announce Type: replace-cross 
Abstract: Verifying the provenance of content is crucial to the function of many organizations, e.g., educational institutions, social media platforms, firms, etc. This problem is becoming increasingly challenging as text generated by Large Language Models (LLMs) becomes almost indistinguishable from human-generated content. In addition, many institutions utilize in-house LLMs and want to ensure that external, non-sanctioned LLMs do not produce content within the institution. In this paper, we answer the following question: Given a piece of text, can we identify whether it was produced by a particular LLM or not? We model LLM-generated text as a sequential stochastic process with complete dependence on history. We then design zero-shot statistical tests to (i) distinguish between text generated by two different known sets of LLMs $A$ (non-sanctioned) and $B$ (in-house), and (ii) identify whether text was generated by a known LLM or generated by any unknown model, e.g., a human or some other language generation process. We prove that the type I and type II errors of our test decrease exponentially with the length of the text. For that, we show that if $B$ generates the text, then except with an exponentially small probability in string length, the log-perplexity of the string under $A$ converges to the average cross-entropy of $B$ and $A$. We then present experiments using LLMs with white-box access to support our theoretical results and empirically examine the robustness of our results to black-box settings and adversarial attacks. In the black-box setting, our method achieves an average TPR of 82.5\% at a fixed FPR of 5\%. Under adversarial perturbations, our minimum TPR is 48.6\% at the same FPR threshold. Both results outperform all non-commercial baselines. See https://github.com/TaraRadvand74/llm-text-detection for code, data, and an online demo of the project.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods</title>
<link>https://arxiv.org/abs/2502.01384</link>
<guid>https://arxiv.org/abs/2502.01384</guid>
<content:encoded><![CDATA[
arXiv:2502.01384v2 Announce Type: replace-cross 
Abstract: Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at https://github.com/ozekri/SEPO.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shuttle Between the Instructions and the Parameters of Large Language Models</title>
<link>https://arxiv.org/abs/2502.02315</link>
<guid>https://arxiv.org/abs/2502.02315</guid>
<content:encoded><![CDATA[
arXiv:2502.02315v3 Announce Type: replace-cross 
Abstract: The interaction with Large Language Models (LLMs) through instructions has been extensively investigated in the research community. While instructions have been widely used as the guidelines for task solving, this paper further notices that both instructions and parameters are the compression of task data. Therefore, they could be strongly correlated and can be learned to predict one from the other. This paper proposes a novel neural network framework, SHIP (\textbf{Sh}uttle between the \textbf{I}nstructions and the \textbf{P}arameters), to model and learn the mutual mappings between the instructions and the parameters of LLMs. We verify that SHIP can effectively map one of the instructions/parameters to the other by evaluating it on the tasks of instruction deduction and induction. The results show that SHIP performs better than existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. Moreover, SHIP can effectively combine the two mapping processes to perform excellent inductive reasoning. The code and data for this paper are released at https://anonymous.4open.science/r/Shuttle-Between-Instructions-Parameters/.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?</title>
<link>https://arxiv.org/abs/2502.09933</link>
<guid>https://arxiv.org/abs/2502.09933</guid>
<content:encoded><![CDATA[
arXiv:2502.09933v4 Announce Type: replace-cross 
Abstract: The ability to recognize patterns from examples and apply them to new ones is a primal ability for general intelligence, and is widely studied by psychology and AI researchers. Many benchmarks have been proposed to measure such ability for Large Language Models (LLMs); however, they focus on few-shot (usually <10) setting and lack evaluation for aggregating many pieces of information from long contexts. On the other hand, the ever-growing context length of LLMs have brought forth the novel paradigm of many-shot In-Context Learning (ICL), which addresses new tasks with hundreds to thousands of examples without expensive and inefficient fine-tuning. However, many-shot evaluations often focus on classification, and popular long-context LLM tasks such as Needle-In-A-Haystack (NIAH) seldom require complicated intelligence for integrating many pieces of information. To fix the issues from both worlds, we propose MIR-Bench, the first many-shot in-context reasoning benchmark for pattern recognition that asks LLM to predict output via input-output examples from underlying functions with diverse data format. Based on MIR-Bench, we study many novel problems for many-shot in-context reasoning, and acquired many insightful findings including scaling effect, robustness, inductive vs. transductive reasoning, retrieval Augmented Generation (RAG), coding for inductive reasoning, cross-domain generalizability, etc.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark</title>
<link>https://arxiv.org/abs/2502.19676</link>
<guid>https://arxiv.org/abs/2502.19676</guid>
<content:encoded><![CDATA[
arXiv:2502.19676v4 Announce Type: replace-cross 
Abstract: Forecasting is an important task in many domains, such as technology and economics. However existing forecasting benchmarks largely lack comprehensive confidence assessment, focus on limited question types, and often consist of artificial questions that do not align with real-world human forecasting needs. To address these gaps, we introduce FOReCAst (Future Outcome Reasoning and Confidence Assessment), a benchmark that evaluates models' ability to make predictions and their confidence in them. FOReCAst spans diverse forecasting scenarios involving Boolean questions, timeframe prediction, and quantity estimation, enabling a comprehensive evaluation of both prediction accuracy and confidence calibration for real-world applications.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Preference Optimization for Vision-Language Long-Horizon Task Planning</title>
<link>https://arxiv.org/abs/2502.20742</link>
<guid>https://arxiv.org/abs/2502.20742</guid>
<content:encoded><![CDATA[
arXiv:2502.20742v3 Announce Type: replace-cross 
Abstract: Existing methods for vision-language task planning excel in short-horizon tasks but often fall short in complex, long-horizon planning within dynamic environments. These challenges primarily arise from the difficulty of effectively training models to produce high-quality reasoning processes for long-horizon tasks. To address this, we propose Structured Preference Optimization (SPO), which aims to enhance reasoning and action selection in long-horizon task planning through structured preference evaluation and optimized training strategies. Specifically, SPO introduces: 1) Preference-Based Scoring and Optimization, which systematically evaluates reasoning chains based on task relevance, visual grounding, and historical consistency; and 2) Curriculum-Guided Training, where the model progressively adapts from simple to complex tasks, improving its generalization ability in long-horizon scenarios and enhancing reasoning robustness. To advance research in vision-language long-horizon task planning, we introduce ExtendaBench, a comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat 2.0, categorized into ultra-short, short, medium, and long tasks. Experimental results demonstrate that SPO significantly improves reasoning quality and final decision accuracy, outperforming prior methods on long-horizon tasks and underscoring the effectiveness of preference-driven optimization in vision-language task planning. Specifically, SPO achieves a +5.98% GCR and +4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement in Habitat over the best-performing baselines.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</title>
<link>https://arxiv.org/abs/2504.13837</link>
<guid>https://arxiv.org/abs/2504.13837</guid>
<content:encoded><![CDATA[
arXiv:2504.13837v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations</title>
<link>https://arxiv.org/abs/2504.13955</link>
<guid>https://arxiv.org/abs/2504.13955</guid>
<content:encoded><![CDATA[
arXiv:2504.13955v4 Announce Type: replace-cross 
Abstract: The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Idea Bench 2025: AI Research Idea Generation Benchmark</title>
<link>https://arxiv.org/abs/2504.14191</link>
<guid>https://arxiv.org/abs/2504.14191</guid>
<content:encoded><![CDATA[
arXiv:2504.14191v2 Announce Type: replace-cross 
Abstract: Large-scale Language Models (LLMs) have revolutionized human-AI interaction and achieved significant success in the generation of novel ideas. However, current assessments of idea generation overlook crucial factors such as knowledge leakage in LLMs, the absence of open-ended benchmarks with grounded truth, and the limited scope of feasibility analysis constrained by prompt design. These limitations hinder the potential of uncovering groundbreaking research ideas. In this paper, we present AI Idea Bench 2025, a framework designed to quantitatively evaluate and compare the ideas generated by LLMs within the domain of AI research from diverse perspectives. The framework comprises a comprehensive dataset of 3,495 AI papers and their associated inspired works, along with a robust evaluation methodology. This evaluation system gauges idea quality in two dimensions: alignment with the ground-truth content of the original papers and judgment based on general reference material. AI Idea Bench 2025's benchmarking system stands to be an invaluable resource for assessing and comparing idea-generation techniques, thereby facilitating the automation of scientific discovery.
]]></content:encoded>
<pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next Word Suggestion using Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.09649</link>
<guid>https://arxiv.org/abs/2505.09649</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Modeling, Graph Convolution, GNNs, LSTMs, Context Embedding

Summary: 
In this project, the focus is on context embedding within language modeling. Traditional language models require large models and extensive resources, but this project aims to develop a more efficient approach. By utilizing the Graph Convolution operation in Graph Neural Networks (GNNs) to encode context and combining it with Long Short-Term Memory networks (LSTMs), the researchers were able to predict the next word in a sentence with high accuracy. The experiment was conducted on a custom Wikipedia text corpus using limited resources, demonstrating the effectiveness of the proposed approach. Overall, this project showcases a promising method for improving language modeling tasks by leveraging graph convolution and LSTM networks for context embedding. 

<br /><br />Summary: <div>
arXiv:2505.09649v1 Announce Type: new 
Abstract: Language Modeling is a prevalent task in Natural Language Processing. The currently existing most recent and most successful language models often tend to build a massive model with billions of parameters, feed in a tremendous amount of text data, and train with enormous computation resources which require millions of dollars. In this project, we aim to address an important sub-task in language modeling, i.e., context embedding. We propose an approach to exploit the Graph Convolution operation in GNNs to encode the context and use it in coalition with LSTMs to predict the next word given a local context of preceding words. We test this on the custom Wikipedia text corpus using a very limited amount of resources and show that this approach works fairly well to predict the next word.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</title>
<link>https://arxiv.org/abs/2505.09655</link>
<guid>https://arxiv.org/abs/2505.09655</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language model, diversity-aware reward adjustment, submodular mutual information, mathematical reasoning<br />
Summary:<br />
The article introduces Diversity-aware Reward Adjustment (DRA) to tackle the diversity-quality inconsistency in reinforcement learning for language model post-training. By incorporating Submodular Mutual Information (SMI) to adjust rewards based on semantic diversity, DRA encourages better exploration while maintaining stable exploitation of high-quality samples. The method seamlessly integrates with Group Relative Policy Optimization (GRPO) and its variant DR.GRPO, resulting in DRA-GRPO and DGA-DR.GRPO. Evaluation on five mathematical reasoning benchmarks shows that DRA outperforms recent baselines, achieving state-of-the-art accuracy of 58.2% with only 7,000 fine-tuning samples and a total training cost of $55. The code for the method is available on GitHub at https://github.com/xiwenc1/DRA-GRPO. <br />Summary: <div>
arXiv:2505.09655v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at https://github.com/xiwenc1/DRA-GRPO.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Are More Persuasive Than Incentivized Human Persuaders</title>
<link>https://arxiv.org/abs/2505.09662</link>
<guid>https://arxiv.org/abs/2505.09662</guid>
<content:encoded><![CDATA[
<div> Keywords: large language model, persuasion, interactive quiz, incentivized experiment, AI<br />
Summary:<br />
In a study comparing the persuasive abilities of a large language model (LLM) and incentivized human persuaders in an interactive quiz setting, the LLM outperformed humans in persuading participants towards correct and incorrect answers. The LLM persuaders achieved significantly higher compliance with their directions, leading to increased quiz takers' accuracy and earnings when guided towards correct answers, and decreased accuracy and earnings when directed towards incorrect answers. The findings suggest that AI's persuasive capabilities surpass those of humans even in scenarios involving real-money incentives. The study underscores the importance of developing alignment and governance frameworks to address the increasing capabilities of AI persuaders. <div>
arXiv:2505.09662v1 Announce Type: new 
Abstract: We directly compare the persuasion capabilities of a frontier large language model (LLM; Claude Sonnet 3.5) against incentivized human persuaders in an interactive, real-time conversational quiz setting. In this preregistered, large-scale incentivized experiment, participants (quiz takers) completed an online quiz where persuaders (either humans or LLMs) attempted to persuade quiz takers toward correct or incorrect answers. We find that LLM persuaders achieved significantly higher compliance with their directional persuasion attempts than incentivized human persuaders, demonstrating superior persuasive capabilities in both truthful (toward correct answers) and deceptive (toward incorrect answers) contexts. We also find that LLM persuaders significantly increased quiz takers' accuracy, leading to higher earnings, when steering quiz takers toward correct answers, and significantly decreased their accuracy, leading to lower earnings, when steering them toward incorrect answers. Overall, our findings suggest that AI's persuasion capabilities already exceed those of humans that have real-money bonuses tied to performance. Our findings of increasingly capable AI persuaders thus underscore the urgency of emerging alignment and governance frameworks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>System Prompt Optimization with Meta-Learning</title>
<link>https://arxiv.org/abs/2505.09666</link>
<guid>https://arxiv.org/abs/2505.09666</guid>
<content:encoded><![CDATA[
<div> optimization, language models, prompts, meta-learning, transferability
Summary: 
The article introduces the concept of bilevel system prompt optimization for Large Language Models (LLMs). It addresses the need to optimize the system prompt, which is task-agnostic and can be applied across different tasks and domains. A meta-learning framework is proposed to optimize the system prompt over various user prompts from multiple datasets, ensuring robustness and transferability. Experimental results on 14 unseen datasets demonstrate the effectiveness of the approach in generalizing to diverse user prompts. The optimized system prompt also enables rapid adaptation to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.<br /><br />Summary: <div>
arXiv:2505.09666v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities, with optimizing their input prompts playing a pivotal role in maximizing their performance. However, while LLM prompts consist of both the task-agnostic system prompts and task-specific user prompts, existing work on prompt optimization has focused on user prompts specific to individual queries or tasks, and largely overlooked the system prompt that is, once optimized, applicable across different tasks and domains. Motivated by this, we introduce the novel problem of bilevel system prompt optimization, whose objective is to design system prompts that are robust to diverse user prompts and transferable to unseen tasks. To tackle this problem, we then propose a meta-learning framework, which meta-learns the system prompt by optimizing it over various user prompts across multiple datasets, while simultaneously updating the user prompts in an iterative manner to ensure synergy between them. We conduct experiments on 14 unseen datasets spanning 5 different domains, on which we show that our approach produces system prompts that generalize effectively to diverse user prompts. Also, our findings reveal that the optimized system prompt enables rapid adaptation even to unseen tasks, requiring fewer optimization steps for test-time user prompts while achieving improved performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts</title>
<link>https://arxiv.org/abs/2505.09701</link>
<guid>https://arxiv.org/abs/2505.09701</guid>
<content:encoded><![CDATA[
<div> Framework, factuality evaluation, VeriFact, FactRBench, long-form model responses

Summary:
VeriFact is a factuality evaluation framework designed to enhance fact extraction by resolving incomplete and missing facts in long-form model responses. It aims to improve factuality assessment accuracy by considering essential context and relational facts often overlooked by existing evaluation pipelines. The introduced FactRBench benchmark evaluates precision and recall in long-form model responses, providing reference fact sets for assessment. Empirical evaluations demonstrate that VeriFact enhances fact completeness and preserves complex relational information, leading to more accurate factuality evaluation. Benchmarking various open- and close-weight LLMs on FactRBench reveals that larger models within the same family improve precision and recall. The study highlights the importance of comprehensive factuality assessment and challenges the assumption that high precision always corresponds to high recall in generating fact-based responses. 

<br /><br />Summary: <div>
arXiv:2505.09701v1 Announce Type: new 
Abstract: Large language models (LLMs) excel at generating long-form responses, but evaluating their factuality remains challenging due to complex inter-sentence dependencies within the generated facts. Prior solutions predominantly follow a decompose-decontextualize-verify pipeline but often fail to capture essential context and miss key relational facts. In this paper, we introduce VeriFact, a factuality evaluation framework designed to enhance fact extraction by identifying and resolving incomplete and missing facts to support more accurate verification results. Moreover, we introduce FactRBench , a benchmark that evaluates both precision and recall in long-form model responses, whereas prior work primarily focuses on precision. FactRBench provides reference fact sets from advanced LLMs and human-written answers, enabling recall assessment. Empirical evaluations show that VeriFact significantly enhances fact completeness and preserves complex facts with critical relational information, resulting in more accurate factuality evaluation. Benchmarking various open- and close-weight LLMs on FactRBench indicate that larger models within same model family improve precision and recall, but high precision does not always correlate with high recall, underscoring the importance of comprehensive factuality assessment.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs</title>
<link>https://arxiv.org/abs/2505.09724</link>
<guid>https://arxiv.org/abs/2505.09724</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, text analysis, taxonomy, intercoder reliability, data-driven

Summary: 
LLMs are efficient tools for analyzing unstructured text data, offering the flexibility of using predefined or data-driven taxonomies. This tutorial outlines a systematic approach to develop, test, and apply taxonomies for text analysis in a collaborative manner. By using personal goals as an example, researchers can write prompts, generate a taxonomy of life domains, evaluate and refine it, test for intercoder agreements, and apply the taxonomy with high reliability. The article discusses the potential and limitations of LLMs for text analysis, highlighting the benefits of iterative and collaborative processes in achieving high-quality results. <div>
arXiv:2505.09724v1 Announce Type: new 
Abstract: Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning</title>
<link>https://arxiv.org/abs/2505.09738</link>
<guid>https://arxiv.org/abs/2505.09738</guid>
<content:encoded><![CDATA[
<div> transplantation method, pre-tokenization learning, token embeddings, compression gains, perplexity results 

Summary:
Tokenadapt introduces a model-agnostic tokenizer transplantation method and pre-tokenization learning for multi-word Supertokens to address constraints in pretrained language models' tokenization schemes. The transplantation heuristic combines local estimates based on subword decomposition and global estimates using semantically similar tokens to minimize retraining needs while preserving semantics. Empirical investigations validate the effectiveness of Tokenadapt in initializing unique tokens, outperforming conventional baselines like ReTok and TransTokenizer. Additionally, Supertokens achieve notable compression gains. Zero-shot perplexity results show that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to ReTok and TransTokenizer across different base models and target tokenizers, with TokenAdapt significantly reducing overall perplexity ratio compared to ReTok. This approach offers a promising solution to overcome tokenizer lock-in challenges and improve efficiency and performance in multilingual or specialized applications. 

<br /><br />Summary: <div>
arXiv:2505.09738v1 Announce Type: new 
Abstract: Pretrained language models (LLMs) are often constrained by their fixed tokenization schemes, leading to inefficiencies and performance limitations, particularly for multilingual or specialized applications. This tokenizer lock-in presents significant challenges. standard methods to overcome this often require prohibitive computational resources. Although tokenizer replacement with heuristic initialization aims to reduce this burden, existing methods often require exhaustive residual fine-tuning and still may not fully preserve semantic nuances or adequately address the underlying compression inefficiencies. Our framework introduces two innovations: first, Tokenadapt, a model-agnostic tokenizer transplantation method, and second, novel pre-tokenization learning for multi-word Supertokens to enhance compression and reduce fragmentation. Tokenadapt initializes new unique token embeddings via a hybrid heuristic that combines two methods: a local estimate based on subword decomposition using the old tokenizer, and a global estimate utilizing the top-k semantically similar tokens from the original vocabulary. This methodology aims to preserve semantics while significantly minimizing retraining requirements. Empirical investigations validate both contributions: the transplantation heuristic successfully initializes unique tokens, markedly outperforming conventional baselines and sophisticated methods including Transtokenizer and ReTok, while our Supertokens achieve notable compression gains. Our zero-shot perplexity results demonstrate that the TokenAdapt hybrid initialization consistently yields lower perplexity ratios compared to both ReTok and TransTokenizer baselines across different base models and newly trained target tokenizers. TokenAdapt typically reduced the overall perplexity ratio significantly compared to ReTok, yielding at least a 2-fold improvement in these aggregate scores.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques</title>
<link>https://arxiv.org/abs/2505.09794</link>
<guid>https://arxiv.org/abs/2505.09794</guid>
<content:encoded><![CDATA[
<div> NLP, Named Entity Recognition, Cancer, EHRs, Data Extraction
Summary: 
- Research projects, especially in cancer, often require manual extraction of information from clinical reports, limiting efficiency.
- Natural Language Processing (NLP) offers a solution by automating data extraction from electronic health records (EHRs).
- The study focuses on lung and breast cancer due to their high incidence and impact on public health.
- Utilizing GMV's NLP tool uQuery, the study aims to enhance accuracy and efficiency in data extraction from EHRs.
- NLP techniques, specifically Named Entity Recognition (NER), are used to automatically identify and extract key clinical information from 200 breast cancer and 400 lung cancer reports.
- Fine-tuning the bsc-bio-ehr-en3 model enables accurate recognition of clinical entities, with strong performance in identifying key entities like MET and PAT.
- Challenges remain with less frequent entities like EVOL. 

<br /><br />Summary: <div>
arXiv:2505.09794v1 Announce Type: new 
Abstract: Research projects, including those focused on cancer, rely on the manual extraction of information from clinical reports. This process is time-consuming and prone to errors, limiting the efficiency of data-driven approaches in healthcare. To address these challenges, Natural Language Processing (NLP) offers an alternative for automating the extraction of relevant data from electronic health records (EHRs). In this study, we focus on lung and breast cancer due to their high incidence and the significant impact they have on public health. Early detection and effective data management in both types of cancer are crucial for improving patient outcomes. To enhance the accuracy and efficiency of data extraction, we utilized GMV's NLP tool uQuery, which excels at identifying relevant entities in clinical texts and converting them into standardized formats such as SNOMED and OMOP. uQuery not only detects and classifies entities but also associates them with contextual information, including negated entities, temporal aspects, and patient-related details. In this work, we explore the use of NLP techniques, specifically Named Entity Recognition (NER), to automatically identify and extract key clinical information from EHRs related to these two cancers. A dataset from Health Research Institute Hospital La Fe (IIS La Fe), comprising 200 annotated breast cancer and 400 lung cancer reports, was used, with eight clinical entities manually labeled using the Doccano platform. To perform NER, we fine-tuned the bsc-bio-ehr-en3 model, a RoBERTa-based biomedical linguistic model pre-trained in Spanish. Fine-tuning was performed using the Transformers architecture, enabling accurate recognition of clinical entities in these cancer types. Our results demonstrate strong overall performance, particularly in identifying entities like MET and PAT, although challenges remain with less frequent entities like EVOL.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the generalization of LLM truth directions on conversational formats</title>
<link>https://arxiv.org/abs/2505.09807</link>
<guid>https://arxiv.org/abs/2505.09807</guid>
<content:encoded><![CDATA[
<div> Keywords: LLMs, truth direction, lie detection, conversational formats, generalization 

Summary:
In this work, the researchers investigate the generalization of the truth direction in Large Language Models (LLMs) across various conversational formats. Previous studies have shown that linear probes on a single hidden state of LLMs can detect lies in conversations. The researchers found that there is good generalization between short conversations ending on a lie, but poor generalization to longer formats with lies occurring earlier in the conversation. To improve this generalization, the researchers propose adding a fixed key phrase at the end of each conversation, which significantly enhances the detection accuracy. These findings shed light on the challenges faced in developing reliable lie detectors based on LLMs that can generalize to different conversational settings. <br /><br />Summary: <div>
arXiv:2505.09807v1 Announce Type: new 
Abstract: Several recent works argue that LLMs have a universal truth direction where true and false statements are linearly separable in the activation space of the model. It has been demonstrated that linear probes trained on a single hidden state of the model already generalize across a range of topics and might even be used for lie detection in LLM conversations. In this work we explore how this truth direction generalizes between various conversational formats. We find good generalization between short conversations that end on a lie, but poor generalization to longer formats where the lie appears earlier in the input prompt. We propose a solution that significantly improves this type of generalization by adding a fixed key phrase at the end of each conversation. Our results highlight the challenges towards reliable LLM lie detectors that generalize to new settings.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning</title>
<link>https://arxiv.org/abs/2505.09825</link>
<guid>https://arxiv.org/abs/2505.09825</guid>
<content:encoded><![CDATA[
<div> literary works, close reading, language models, interpretive reasoning, benchmarking 
Summary: 

- Close reading is a common practice in college English courses where students analyze literary texts through evidence-based arguments.
- Close reading has not been evaluated on large language models, leading to the development of the KRISTEVA benchmark.
- KRISTEVA consists of 1331 multiple-choice questions designed to test different aspects of close reading, such as extracting stylistic features and multi-hop reasoning.
- While state-of-the-art language models show some competency in close reading tasks, they still lag behind human evaluators in accuracy on most tasks.
- This benchmark aims to bridge the gap in evaluating interpretive reasoning in literary analysis using machine learning technologies. 

<br /><br />Summary: <div>
arXiv:2505.09825v1 Announce Type: new 
Abstract: Each year, tens of millions of essays are written and graded in college-level English courses. Students are asked to analyze literary and cultural texts through a process known as close reading, in which they gather textual details to formulate evidence-based arguments. Despite being viewed as a basis for critical thinking and widely adopted as a required element of university coursework, close reading has never been evaluated on large language models (LLMs), and multi-discipline benchmarks like MMLU do not include literature as a subject. To fill this gap, we present KRISTEVA, the first close reading benchmark for evaluating interpretive reasoning, consisting of 1331 multiple-choice questions adapted from classroom data. With KRISTEVA, we propose three progressively more difficult sets of tasks to approximate different elements of the close reading process, which we use to test how well LLMs may seem to understand and reason about literary works: 1) extracting stylistic features, 2) retrieving relevant contextual information from parametric knowledge, and 3) multi-hop reasoning between style and external contexts. Our baseline results find that, while state-of-the-art LLMs possess some college-level close reading competency (accuracy 49.7% - 69.7%), their performances still trail those of experienced human evaluators on 10 out of our 11 tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting</title>
<link>https://arxiv.org/abs/2505.09852</link>
<guid>https://arxiv.org/abs/2505.09852</guid>
<content:encoded><![CDATA[
<div> parametric knowledge, conflict escalation, fatalities, Large Language Models, early warning systems

Summary:
Large Language Models (LLMs) are being explored for their ability to predict conflict escalation and fatalities without external data, crucial for early warning systems and policy-making. The study evaluates LLMs' parametric knowledge encoded in pretrained weights, comparing it with non-parametric capabilities where models access external conflict datasets and news reports. The evaluation covers conflict-prone regions in the Horn of Africa and the Middle East from 2020-2024. In the parametric setting, LLMs forecast conflict trends and fatalities solely based on pretrained knowledge. In the non-parametric setting, models incorporate recent conflict summaries and geopolitical developments. The findings showcase the strengths and limitations of LLMs in conflict forecasting and emphasize the benefits of augmenting them with structured external knowledge. 

<br /><br />Summary: <div>
arXiv:2505.09852v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown impressive performance across natural language tasks, but their ability to forecast violent conflict remains underexplored. We investigate whether LLMs possess meaningful parametric knowledge-encoded in their pretrained weights-to predict conflict escalation and fatalities without external data. This is critical for early warning systems, humanitarian planning, and policy-making. We compare this parametric knowledge with non-parametric capabilities, where LLMs access structured and unstructured context from conflict datasets (e.g., ACLED, GDELT) and recent news reports via Retrieval-Augmented Generation (RAG). Incorporating external information could enhance model performance by providing up-to-date context otherwise missing from pretrained weights. Our two-part evaluation framework spans 2020-2024 across conflict-prone regions in the Horn of Africa and the Middle East. In the parametric setting, LLMs predict conflict trends and fatalities relying only on pretrained knowledge. In the non-parametric setting, models receive summaries of recent conflict events, indicators, and geopolitical developments. We compare predicted conflict trend labels (e.g., Escalate, Stable Conflict, De-escalate, Peace) and fatalities against historical data. Our findings highlight the strengths and limitations of LLMs for conflict forecasting and the benefits of augmenting them with structured external knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries</title>
<link>https://arxiv.org/abs/2505.09902</link>
<guid>https://arxiv.org/abs/2505.09902</guid>
<content:encoded><![CDATA[
<div> language models, Spanish variants, localization, sociolinguistic dissonances, AI models <br />
<br />
Language models based on Spanish face challenges due to differences in variants across Latin America and Spain, leading to sociolinguistic dissonances among dialectal groups. This paper highlights the need for regional localized models to bridge these divides and enhance user trust and reliance on AI. By implementing the proposed five sub variants of Spanish, localization strategies can be improved to meet inclusivity goals and drive user growth in low-risk investment areas. This approach not only fosters a better understanding of cultural, historical, and sociolinguistic nuances but also contributes to the success of internationalization strategies. Ultimately, the adoption of locale-sensitive AI models can lead to more efficient localization processes that benefit both users and developers. <br /><br />Summary: <div>
arXiv:2505.09902v1 Announce Type: new 
Abstract: Large language models are, by definition, based on language. In an effort to underscore the critical need for regional localized models, this paper examines primary differences between variants of written Spanish across Latin America and Spain, with an in-depth sociocultural and linguistic contextualization therein. We argue that these differences effectively constitute significant gaps in the quotidian use of Spanish among dialectal groups by creating sociolinguistic dissonances, to the extent that locale-sensitive AI models would play a pivotal role in bridging these divides. In doing so, this approach informs better and more efficient localization strategies that also serve to more adequately meet inclusivity goals, while securing sustainable active daily user growth in a major low-risk investment geographic area. Therefore, implementing at least the proposed five sub variants of Spanish addresses two lines of action: to foment user trust and reliance on AI language models while also demonstrating a level of cultural, historical, and sociolinguistic awareness that reflects positively on any internationalization strategy.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models</title>
<link>https://arxiv.org/abs/2505.09924</link>
<guid>https://arxiv.org/abs/2505.09924</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, watermarking, logits-based, sampling-based, symbiotic framework

Summary:
The paper introduces a novel symbiotic watermarking framework for Large Language Models (LLMs) that combines the strengths of logits-based and sampling-based watermarking schemes. The framework includes three strategies: serial, parallel, and hybrid, which leverage token entropy and semantic entropy for optimal watermark embedding. Through comprehensive experiments on various datasets and models, the proposed method outperforms existing baselines and achieves state-of-the-art performance. The symbiotic framework provides insights into diverse watermarking paradigms, addressing the trade-offs among robustness, text quality, security, and detectability. This versatile approach offers a balanced solution for watermarking AI-generated text. The code for the framework is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2505.09924v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at \href{https://github.com/redwyd/SymMark}{https://github.com/redwyd/SymMark}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Prompt Optimizers: From Prompt Merits to Optimization</title>
<link>https://arxiv.org/abs/2505.09930</link>
<guid>https://arxiv.org/abs/2505.09930</guid>
<content:encoded><![CDATA[
<div> Keywords: prompt optimization, large language models, interpretable design, lightweight models, MePO

Summary:<br /><br />
This article introduces MePO, a merit-guided prompt optimizer designed to enhance prompt and response quality in large language models. By focusing on interpretable design principles, MePO improves performance without the need for fine-tuning model weights. The approach avoids reliance on advanced LLMs like GPT-4, making it more lightweight and locally deployable. MePO is trained on a preference dataset created from merit-aligned prompts generated by a lightweight LLM. Unlike previous methods, MePO reduces cost and privacy concerns, while learning clear and interpretable merits. The model demonstrates improved results across various tasks and model types, providing a scalable and robust solution for real-world deployment. The model and dataset are publicly available for further exploration and research. <div>
arXiv:2505.09930v1 Announce Type: new 
Abstract: Prompt optimization (PO) offers a practical alternative to fine-tuning large language models (LLMs), enabling performance improvements without altering model weights. Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to generate optimized prompts. However, due to limited downward compatibility, verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight inference models and degrade response quality. In this work, we rethink prompt optimization through the lens of interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, lightweight, and locally deployable prompt optimizer trained on our preference dataset built from merit-aligned prompts generated by a lightweight LLM. Unlike prior work, MePO avoids online optimization reliance, reduces cost and privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment. Our model and dataset are available at: https://github.com/MidiyaZhu/MePO
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph</title>
<link>https://arxiv.org/abs/2505.09945</link>
<guid>https://arxiv.org/abs/2505.09945</guid>
<content:encoded><![CDATA[
<div> knowledge graphs, large language models, response generation, personalized information, retrieval augmented generation

Summary:
Knowledge graphs are used to augment large language models in generating personalized responses by providing factual and timely information. Overfitting in LLMs can lead to the generation of incorrect data, causing hallucinations. The proposed approach, retrieval augmented generation, leverages knowledge graphs to assist LLMs in generating accurate responses tailored to users' personal information, particularly calendar data. Experimental results show improved understanding of personal data and enhanced response accuracy compared to baseline LLMs using personal data as text inputs. Response time is moderately reduced with the proposed approach. <div>
arXiv:2505.09945v1 Announce Type: new 
Abstract: The advent of large language models (LLMs) has allowed numerous applications, including the generation of queried responses, to be leveraged in chatbots and other conversational assistants. Being trained on a plethora of data, LLMs often undergo high levels of over-fitting, resulting in the generation of extra and incorrect data, thus causing hallucinations in output generation. One of the root causes of such problems is the lack of timely, factual, and personalized information fed to the LLM. In this paper, we propose an approach to address these problems by introducing retrieval augmented generation (RAG) using knowledge graphs (KGs) to assist the LLM in personalized response generation tailored to the users. KGs have the advantage of storing continuously updated factual information in a structured way. While our KGs can be used for a variety of frequently updated personal data, such as calendar, contact, and location data, we focus on calendar data in this paper. Our experimental results show that our approach works significantly better in understanding personal information and generating accurate responses compared to the baseline LLMs using personal data as text inputs, with a moderate reduction in response time.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs</title>
<link>https://arxiv.org/abs/2505.10013</link>
<guid>https://arxiv.org/abs/2505.10013</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, implicit bias, ethics, technical issue, benchmark

Summary:
Large Language Models (LLMs) have been a topic of concern due to potential biases inherited from training data. A study explores how LLMs display implicit bias, highlighting both ethical and technical issues. The inability of LLMs to handle extraneous information is a significant problem. The lack of standard methods for benchmarking bias in LLMs necessitated the development of the DIF (Demographic Implicit Fairness) benchmark. By evaluating logic and math problem datasets with sociodemographic personas, the method can statistically validate implicit bias in LLM behavior. The study reveals an inverse relationship between question answering accuracy and implicit bias, supporting the argument that LLMs struggle to handle diverse social contexts effectively. The DIF benchmark provides a valuable tool for assessing and addressing bias in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.10013v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) have risen in prominence over the past few years, there has been concern over the potential biases in LLMs inherited from the training data. Previous studies have examined how LLMs exhibit implicit bias, such as when response generation changes when different social contexts are introduced. We argue that this implicit bias is not only an ethical, but also a technical issue, as it reveals an inability of LLMs to accommodate extraneous information. However, unlike other measures of LLM intelligence, there are no standard methods to benchmark this specific subset of LLM bias. To bridge this gap, we developed a method for calculating an easily interpretable benchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM logic and math problem datasets with sociodemographic personas. We demonstrate that this method can statistically validate the presence of implicit bias in LLM behavior and find an inverse trend between question answering accuracy and implicit bias, supporting our argument.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability</title>
<link>https://arxiv.org/abs/2505.10063</link>
<guid>https://arxiv.org/abs/2505.10063</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Multi-document Question Answering, Coarse-to-fine Method, Retrieval Heads, Attention Mechanism

Summary:
CAFE is a novel two-stage method designed to improve multi-document question-answering capabilities in Large Language Models (LLMs). The method addresses the challenge of balancing retrieval precision and recall by gradually filtering out irrelevant documents and guiding attention to relevant content. In the first stage, a coarse-grained filtering method ranks relevant documents using retrieval heads. In the second stage, a fine-grained steering method directs attention to the most relevant information, reducing the influence of background and distracting documents. Experimental results demonstrate that CAFE outperforms baseline methods, achieving significant improvements in SubEM over existing approaches. Specifically, CAFE shows up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model, highlighting its effectiveness in enhancing the performance of LLMs in complex question-answering tasks. 

<br /><br />Summary: <div>
arXiv:2505.10063v1 Announce Type: new 
Abstract: Advancements in Large Language Models (LLMs) have extended their input context length, yet they still struggle with retrieval and reasoning in long-context inputs. Existing methods propose to utilize the prompt strategy and retrieval head to alleviate this limitation. However, they still face challenges in balancing retrieval precision and recall, impacting their efficacy in answering questions. To address this, we introduce $\textbf{CAFE}$, a two-stage coarse-to-fine method to enhance multi-document question-answering capacities. By gradually eliminating the negative impacts of background and distracting documents, CAFE makes the responses more reliant on the evidence documents. Initially, a coarse-grained filtering method leverages retrieval heads to identify and rank relevant documents. Then, a fine-grained steering method guides attention to the most relevant content. Experiments across benchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7% SubEM improvement over SFT and RAG methods on the Mistral model, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dark LLMs: The Growing Threat of Unaligned AI Models</title>
<link>https://arxiv.org/abs/2505.10066</link>
<guid>https://arxiv.org/abs/2505.10066</guid>
<content:encoded><![CDATA[
<div> Large Language Models, jailbreaking, vulnerabilities, dark content, ethical guardrails
Summary: Large Language Models (LLMs) have revolutionized various fields but are vulnerable to jailbreak attacks due to their training data. Dark LLMs lack ethical safeguards and can be manipulated to produce harmful outputs. A universal jailbreak attack, first shared online, compromises LLMs, allowing them to answer any question. Several state-of-the-art models remain susceptible despite the disclosure of this threat. The industry's response to AI safety concerns has been inadequate, posing risks with the increasing accessibility of model training and open-source LLMs. Without intervention, LLMs might democratize access to dangerous information, leading to unforeseen dangers. <br /><br />Summary: <div>
arXiv:2505.10066v1 Announce Type: new 
Abstract: Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susceptibility of these models to jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems from the very data they learn from. As long as this training data includes unfiltered, problematic, or 'dark' content, the models can inherently learn undesirable patterns or weaknesses that allow users to circumvent their intended safety controls. Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. In our research, we uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. The main idea of our attack was published online over seven months ago. However, many of the tested LLMs were still vulnerable to this attack. Despite our responsible disclosure efforts, responses from major LLM providers were often inadequate, highlighting a concerning gap in industry practices regarding AI safety. As model training becomes more accessible and cheaper, and as open-source LLMs proliferate, the risk of widespread misuse escalates. Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing and Contextualising Probes for African Languages</title>
<link>https://arxiv.org/abs/2505.10081</link>
<guid>https://arxiv.org/abs/2505.10081</guid>
<content:encoded><![CDATA[
<div> Keywords: Pretrained language models, African languages, linguistic knowledge, layer-wise probes, interpretability techniques<br />
Summary:<br />
This paper presents a systematic investigation into probing Pretrained Language Models (PLMs) for linguistic knowledge about African languages. The study involves training layer-wise probes for six typologically diverse African languages to analyze the distribution of linguistic features. Results show that PLMs adapted for African languages encode more linguistic information about the target languages compared to massively multilingual PLMs. The research confirms previous findings that token-level syntactic information tends to concentrate in middle-to-last layers, while sentence-level semantic information is spread across all layers. Additionally, control tasks and probing baselines reveal that probe performance reflects the internal knowledge of PLMs rather than memorization, highlighting the internal mechanisms behind successful strategies like active learning and multilingual adaptation. The study applies established interpretability techniques to African-language PLMs, shedding light on the reasons behind the continuous improvement of PLMs for African languages.<br /> <div>
arXiv:2505.10081v1 Announce Type: new 
Abstract: Pretrained language models (PLMs) for African languages are continually improving, but the reasons behind these advances remain unclear. This paper presents the first systematic investigation into probing PLMs for linguistic knowledge about African languages. We train layer-wise probes for six typologically diverse African languages to analyse how linguistic features are distributed. We also design control tasks, a way to interpret probe performance, for the MasakhaPOS dataset. We find PLMs adapted for African languages to encode more linguistic information about target languages than massively multilingual PLMs. Our results reaffirm previous findings that token-level syntactic information concentrates in middle-to-last layers, while sentence-level semantic information is distributed across all layers. Through control tasks and probing baselines, we confirm that performance reflects the internal knowledge of PLMs rather than probe memorisation. Our study applies established interpretability techniques to African-language PLMs. In doing so, we highlight the internal mechanisms underlying the success of strategies like active learning and multilingual adaptation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>XRAG: Cross-lingual Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2505.10089</link>
<guid>https://arxiv.org/abs/2505.10089</guid>
<content:encoded><![CDATA[
<div> Benchmark, LLMs, Cross-lingual, Retrieval-Augmented Generation, XRAG<br />
Summary: <br />
- XRAG is a new benchmark designed to evaluate the generation abilities of Large Language Models (LLMs) in cross-lingual Retrieval-Augmented Generation (RAG) settings. 
- The dataset is constructed from recent news articles and includes questions that require external knowledge to be answered, challenging LLM reasoning abilities. 
- XRAG covers both monolingual and multilingual retrieval scenarios, with relevancy annotations provided for each retrieved document. 
- Experimental results on five LLMs reveal challenges in response language correctness in monolingual retrieval and reasoning over retrieved information in multilingual retrieval. 
- LLM performance lags significantly behind human performance in answering questions on XRAG, making it a valuable benchmark for studying LLM reasoning abilities in cross-lingual settings. 

Summary: <div>
arXiv:2505.10089v1 Announce Type: new 
Abstract: We propose XRAG, a novel benchmark designed to evaluate the generation abilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG) settings where the user language does not match the retrieval results. XRAG is constructed from recent news articles to ensure that its questions require external knowledge to be answered. It covers the real-world scenarios of monolingual and multilingual retrieval, and provides relevancy annotations for each retrieved document. Our novel dataset construction pipeline results in questions that require complex reasoning, as evidenced by the significant gap between human and LLM performance. Consequently, XRAG serves as a valuable benchmark for studying LLM reasoning abilities, even before considering the additional cross-lingual complexity. Experimental results on five LLMs uncover two previously unreported challenges in cross-lingual RAG: 1) in the monolingual retrieval setting, all evaluated models struggle with response language correctness; 2) in the multilingual retrieval setting, the main challenge lies in reasoning over retrieved information across languages rather than generation of non-English text.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs</title>
<link>https://arxiv.org/abs/2505.10113</link>
<guid>https://arxiv.org/abs/2505.10113</guid>
<content:encoded><![CDATA[
<div> Keywords: S-MedQA, medical question-answering, knowledge injection, fine-tuning, clinical specialties<br />
<br />
Summary: <br />
1) The research introduces S-MedQA, an English medical question-answering dataset for evaluating large language models in specific clinical fields.<br />
2) The study challenges the popular hypothesis that training on data from a medical specialty would yield the best performance on that specialty.<br />
3) Token probabilities of clinically relevant terms consistently increase across specialties, indicating the importance of domain shifting in performance improvement rather than knowledge injection.<br />
4) The findings suggest reconsidering the role of fine-tuning data in the medical domain and imply that gains in performance may be attributed more to domain-specific adaptation than previously thought.<br />
5) The dataset, S-MedQA, and all research code are released to the research community for replication of experiments. <div>
arXiv:2505.10113v1 Announce Type: new 
Abstract: In this paper, we introduce S-MedQA, an English medical question-answering (QA) dataset for benchmarking large language models in fine-grained clinical specialties. We use S-MedQA to check the applicability of a popular hypothesis related to knowledge injection in the knowledge-intense scenario of medical QA, and show that: 1) training on data from a speciality does not necessarily lead to best performance on that specialty and 2) regardless of the specialty fine-tuned on, token probabilities of clinically relevant terms for all specialties increase consistently. Thus, we believe improvement gains come mostly from domain shifting (e.g., general to medical) rather than knowledge injection and suggest rethinking the role of fine-tuning data in the medical domain. We release S-MedQA and all code needed to reproduce all our experiments to the research community.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs</title>
<link>https://arxiv.org/abs/2505.10143</link>
<guid>https://arxiv.org/abs/2505.10143</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Evidence-based response generation, Knowledge Graph, Retrieval-augmented agent, Trustworthiness

Summary: 
The paper introduces GE-Chat, a framework that enhances response generation from Large Language Models (LLMs) by utilizing a Knowledge Graph to provide evidence-based responses. By creating a knowledge graph from the user's uploaded document, the framework is able to construct a retrieval-augmented agent that includes additional knowledge beyond the model's training corpus. Through the use of Chain-of-Thought (CoT) logic generation, n-hop sub-graph searching, and entailment-based sentence generation, the framework improves the accuracy of evidence retrieval in a free-form context. This method helps users examine the sources of LLM conclusions and assess the trustworthiness of the generated responses. GE-Chat addresses the challenge of unreliable outputs from LLMs and aims to enhance user confidence in the decision-making process. 

<br /><br />Summary: <div>
arXiv:2505.10143v1 Announce Type: new 
Abstract: Large Language Models are now key assistants in human decision-making processes. However, a common note always seems to follow: "LLMs can make mistakes. Be careful with important info." This points to the reality that not all outputs from LLMs are dependable, and users must evaluate them manually. The challenge deepens as hallucinated responses, often presented with seemingly plausible explanations, create complications and raise trust issues among users. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph enhanced retrieval-augmented generation framework to provide Evidence-based response generation. Specifically, when the user uploads a material document, a knowledge graph will be created, which helps construct a retrieval-augmented agent, enhancing the agent's responses with additional knowledge beyond its training corpus. Then we leverage Chain-of-Thought (CoT) logic generation, n-hop sub-graph searching, and entailment-based sentence generation to realize accurate evidence retrieval. We demonstrate that our method improves the existing models' performance in terms of identifying the exact evidence in a free-form context, providing a reliable way to examine the resources of LLM's conclusion and help with the judgment of the trustworthiness.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.10182</link>
<guid>https://arxiv.org/abs/2505.10182</guid>
<content:encoded><![CDATA[
<div> supervised fine-tuning, reinforcement learning, large language models, continual pretraining, reasoning skills <br />
Summary: 
- Large Language Models (LLMs) show improved reasoning abilities through supervised fine-tuning and reinforcement learning.
- Continual pretraining (CPT) does not require task-specific signals, providing a broader range of training data.
- Reasoning CPT, utilizing synthetic data to reconstruct hidden thought processes in texts, enhances performance across various domains.
- Reasoning skills learned in one domain transfer effectively to others.
- Models trained with hidden thoughts adjust the depth of reasoning based on problem difficulty, leading to gains of up to 8 points on challenging problems. <div>
arXiv:2505.10182v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlying texts, based on the premise that texts are the result of the author's thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think</title>
<link>https://arxiv.org/abs/2505.10185</link>
<guid>https://arxiv.org/abs/2505.10185</guid>
<content:encoded><![CDATA[
<div> Keywords: Chain-of-thought, Language models, Reasoning strategies, CoT Encyclopedia, Model behavior <br />
Summary: 
The article introduces the CoT Encyclopedia framework for analyzing and guiding model reasoning strategies. By automatically extracting diverse reasoning criteria from model-generated CoTs and clustering them into categories, the framework offers more comprehensive and interpretable analyses than existing methods. Human evaluations confirm the effectiveness of this approach. The framework enables predicting model strategies and guiding them towards more effective alternatives. Practical insights include the impact of training data format on reasoning behavior, with free-form vs. multiple-choice formats playing a significant role. This underscores the importance of format-aware model design in improving reasoning capabilities.<br /><br />Summary: <div>
arXiv:2505.10185v1 Announce Type: new 
Abstract: Long chain-of-thought (CoT) is an essential ingredient in effective usage of modern large language models, but our understanding of the reasoning strategies underlying these capabilities remains limited. While some prior works have attempted to categorize CoTs using predefined strategy types, such approaches are constrained by human intuition and fail to capture the full diversity of model behaviors. In this work, we introduce the CoT Encyclopedia, a bottom-up framework for analyzing and steering model reasoning. Our method automatically extracts diverse reasoning criteria from model-generated CoTs, embeds them into a semantic space, clusters them into representative categories, and derives contrastive rubrics to interpret reasoning behavior. Human evaluations show that this framework produces more interpretable and comprehensive analyses than existing methods. Moreover, we demonstrate that this understanding enables performance gains: we can predict which strategy a model is likely to use and guide it toward more effective alternatives. Finally, we provide practical insights, such as that training data format (e.g., free-form vs. multiple-choice) has a far greater impact on reasoning behavior than data domain, underscoring the importance of format-aware model design.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits</title>
<link>https://arxiv.org/abs/2505.10202</link>
<guid>https://arxiv.org/abs/2505.10202</guid>
<content:encoded><![CDATA[
<div> Vector Quantization, Large Language Models, Output Layer, Computational Efficiency, Parameter Reduction

Summary: 
The paper introduces VQ-Logits, a novel approach to reducing the parameter count and computational load of the final linear projection layer in Large Language Models (LLMs). By leveraging Vector Quantization (VQ), the approach uses a compact codebook of embedding vectors to replace the large output embedding matrix, leading to significant parameter reduction in the output layer and faster logit computation. Experimental results on language modeling benchmarks demonstrate up to 99% parameter reduction and 6x speedup in logit computation with only a marginal increase in perplexity compared to full softmax baselines. Ablation studies on codebook size, initialization, and learning strategies also show the robustness and effectiveness of the VQ-Logits approach. <div>
arXiv:2505.10202v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have achieved remarkable success but face significant computational and memory challenges, particularly due to their extensive output vocabularies. The final linear projection layer, mapping hidden states to vocabulary-sized logits, often constitutes a substantial portion of the model's parameters and computational cost during inference. Existing methods like adaptive softmax or hierarchical softmax introduce structural complexities. In this paper, we propose VQ-Logits, a novel approach that leverages Vector Quantization (VQ) to drastically reduce the parameter count and computational load of the LLM output layer. VQ-Logits replaces the large V * dmodel output embedding matrix with a small, shared codebook of K embedding vectors (K << V ). Each token in the vocabulary is mapped to one of these K codebook vectors. The LLM predicts logits over this compact codebook, which are then efficiently "scattered" to the full vocabulary space using the learned or preassigned mapping. We demonstrate through extensive experiments on standard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits can achieve up to 99% parameter reduction in the output layer and 6x speedup in logit computation, with only a marginal 4% increase in perplexity compared to full softmax baselines. We further provide detailed ablation studies on codebook size, initialization, and learning strategies, showcasing the robustness and effectiveness of our approach.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward</title>
<link>https://arxiv.org/abs/2505.10218</link>
<guid>https://arxiv.org/abs/2505.10218</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, role-playing conversational agents, role consistency, verifiable role-awareness reward, dataset collaboration

Summary:
Role-playing conversational agents (RPCAs) often struggle with maintaining role consistency. To address this issue, this study introduces RAIDEN-R1, a novel reinforcement learning framework that incorporates Verifiable Role-Awareness Reward (VRAR). The method utilizes singular and multi-term mining strategies to generate quantifiable rewards based on role-specific keys. A high-quality role-aware Chain-of-Thought dataset is constructed through multi-LLM collaboration, improving reasoning coherence. Experimental results on the RAIDEN benchmark show that the 14B-GRPO model of RAIDEN-R1 outperforms baseline models with an accuracy of 88.04% and 88.65% on Script-Based Knowledge and Conversation Memory metrics, respectively, while maintaining robustness. Case analyses further demonstrate the model's proficiency in resolving conflicting contextual cues and maintaining first-person narrative consistency. This work not only bridges the non-quantifiability gap in RPCA training but also provides valuable insights into role-aware reasoning patterns, ultimately advancing the development of RPCAs. 

<br /><br />Summary: <div>
arXiv:2505.10218v1 Announce Type: new 
Abstract: Role-playing conversational agents (RPCAs) face persistent challenges in maintaining role consistency. To address this, we propose RAIDEN-R1, a novel reinforcement learning framework that integrates Verifiable Role-Awareness Reward (VRAR). The method introduces both singular and multi-term mining strategies to generate quantifiable rewards by assessing role-specific keys. Additionally, we construct a high-quality, role-aware Chain-of-Thought dataset through multi-LLM collaboration, and implement experiments to enhance reasoning coherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's superiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on Script-Based Knowledge and Conversation Memory metrics, respectively, outperforming baseline models while maintaining robustness. Case analyses further reveal the model's enhanced ability to resolve conflicting contextual cues and sustain first-person narrative consistency. This work bridges the non-quantifiability gap in RPCA training and provides insights into role-aware reasoning patterns, advancing the development of RPCAs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data</title>
<link>https://arxiv.org/abs/2505.10260</link>
<guid>https://arxiv.org/abs/2505.10260</guid>
<content:encoded><![CDATA[
<div> natural language processing, large language models, binary classification, human rights violations, multilingual contexts

Summary:<br />
This study explores the capabilities of state-of-the-art large language models for zero-shot and few-shot annotation of social media posts in Russian and Ukrainian, focusing on identifying references to human rights violations. The models evaluated include GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2, compared against human double-annotated labels. The analysis assesses annotation performance under different prompting conditions in English and Russian, revealing unique error patterns. By comparing model outputs with human annotations, the research contributes to understanding the reliability of language models in multilingual, domain-specific tasks and their handling of subjective judgments. This insight is crucial for considering the deployment of language models in real-world scenarios.<br /><br />Summary: <div>
arXiv:2505.10260v1 Announce Type: new 
Abstract: In the era of increasingly sophisticated natural language processing (NLP) systems, large language models (LLMs) have demonstrated remarkable potential for diverse applications, including tasks requiring nuanced textual understanding and contextual reasoning. This study investigates the capabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3, Mistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex textual dataset comprising social media posts in Russian and Ukrainian. Specifically, the focus is on the binary classification task of identifying references to human rights violations within the dataset.
  To evaluate the effectiveness of these models, their annotations are compared against a gold standard set of human double-annotated labels across 1000 samples. The analysis includes assessing annotation performance under different prompting conditions, with prompts provided in both English and Russian. Additionally, the study explores the unique patterns of errors and disagreements exhibited by each model, offering insights into their strengths, limitations, and cross-linguistic adaptability.
  By juxtaposing LLM outputs with human annotations, this research contributes to understanding the reliability and applicability of LLMs for sensitive, domain-specific tasks in multilingual contexts. It also sheds light on how language models handle inherently subjective and context-dependent judgments, a critical consideration for their deployment in real-world scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine</title>
<link>https://arxiv.org/abs/2505.10261</link>
<guid>https://arxiv.org/abs/2505.10261</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, medicine, generative large language models, ethical use, medical applications

Summary:<br /><br />Natural language processing (NLP) in medicine has evolved with the rise of generative large language models (LLMs). A study of 19,123 research papers reveals that while generative LLMs excel in open-ended tasks, traditional NLP is more efficient in information extraction and analysis tasks. The comparison highlights the distinct advantages of each approach across various medical applications. As these technologies continue to evolve, ethical considerations are paramount to harness their full potential in the healthcare sector. It is crucial to ensure responsible and ethical use of NLP and generative LLMs to maximize their benefits in medical settings. Ethical guidelines and safeguards must be put in place to protect patient privacy, confidentiality, and data security. Embracing the potential of these technologies while upholding ethical principles will be key in driving their successful adoption in the medical field.  <div>
arXiv:2505.10261v1 Announce Type: new 
Abstract: Natural language processing (NLP) has been traditionally applied to medicine, and generative large language models (LLMs) have become prominent recently. However, the differences between them across different medical tasks remain underexplored. We analyzed 19,123 studies, finding that generative LLMs demonstrate advantages in open-ended tasks, while traditional NLP dominates in information extraction and analysis tasks. As these technologies advance, ethical use of them is essential to ensure their potential in medical applications.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making</title>
<link>https://arxiv.org/abs/2505.10282</link>
<guid>https://arxiv.org/abs/2505.10282</guid>
<content:encoded><![CDATA[
<div> clinical evidence, evidence synthesis, clinical decision support system, large language models, Quicker <br />
<br />
Summary: The study introduces Quicker, an evidence-based clinical decision support system that automates evidence synthesis and generates clinical recommendations. Quicker's fully automated chain covers all phases of decision-making, including question decomposition and recommendation generation. Evaluation using the Q2CRBench-3 benchmark dataset showed Quicker's strong performance in question decomposition, retrieval sensitivity, and literature screening. Quicker-assisted evidence assessment supported human reviewers effectively, with recommendations more comprehensive and coherent than those of clinicians. Collaboration between a single reviewer and Quicker reduced recommendation development time significantly. Overall, the study affirms Quicker's potential to help physicians make quicker and more reliable evidence-based clinical decisions. <br /> <div>
arXiv:2505.10282v1 Announce Type: new 
Abstract: Clinical evidence, derived from rigorous research and data analysis, provides healthcare professionals with reliable scientific foundations for informed decision-making. Integrating clinical evidence into real-time practice is challenging due to the enormous workload, complex professional processes, and time constraints. This highlights the need for tools that automate evidence synthesis to support more efficient and accurate decision making in clinical settings. This study introduces Quicker, an evidence-based clinical decision support system powered by large language models (LLMs), designed to automate evidence synthesis and generate clinical recommendations modeled after standard clinical guideline development processes. Quicker implements a fully automated chain that covers all phases, from questions to clinical recommendations, and further enables customized decision-making through integrated tools and interactive user interfaces. To evaluate Quicker's capabilities, we developed the Q2CRBench-3 benchmark dataset, based on clinical guideline development records for three different diseases. Experimental results highlighted Quicker's strong performance, with fine-grained question decomposition tailored to user preferences, retrieval sensitivities comparable to human experts, and literature screening performance approaching comprehensive inclusion of relevant studies. In addition, Quicker-assisted evidence assessment effectively supported human reviewers, while Quicker's recommendations were more comprehensive and logically coherent than those of clinicians. In system-level testing, collaboration between a single reviewer and Quicker reduced the time required for recommendation development to 20-40 minutes. In general, our findings affirm the potential of Quicker to help physicians make quicker and more reliable evidence-based clinical decisions.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.10320</link>
<guid>https://arxiv.org/abs/2505.10320</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, judgment tasks, reward strategies, model training, evaluation

Summary:
J1, a reinforcement learning approach, aims to train LLM-as-a-Judge models for improved judgment ability by enhancing chain-of-thought reasoning. The method converts prompts to judgment tasks with verifiable rewards, surpassing existing 8B or 70B models in performance. Comparison between Pairwise-J1 and Pointwise-J1 models, offline versus online training recipes, reward strategies, seed prompts, and thought length/content variations reveal the effectiveness of J1 in outlining evaluation criteria, self-generating reference answers, and re-evaluating model responses. Through these enhancements, J1 excels in judgment tasks, outperforming larger models like o1-mini and R1 on certain benchmarks despite its smaller size. Overall, J1 contributes to overcoming evaluation quality bottlenecks in AI progress by facilitating stronger chain-of-thought reasoning and incentivizing critical thinking in model training. 

<br /><br />Summary: <div>
arXiv:2505.10320v1 Announce Type: new 
Abstract: The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations</title>
<link>https://arxiv.org/abs/2505.10354</link>
<guid>https://arxiv.org/abs/2505.10354</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic text representation, text embeddings, interpretable embeddings, dense embeddings, farthest point sampling

Summary:
Semantic text representation is crucial in natural language processing, with text embeddings like SimCSE and LLM2Vec showing strong performance but lacking interpretability. Benara et al. recently introduced interpretable text embeddings based on large language models, but they are high-dimensional and complex. In this study, a new approach called Low-dimensional Dense and Interpretable text embeddings with Relative representations (LDIR) is proposed, with fewer than 500 dimensions. LDIR's dimensions indicate semantic relatedness to anchor texts using farthest point sampling, balancing semantic representation with traceability and interpretability. Validation on various tasks demonstrates LDIR's effectiveness, performing comparably to black-box models while surpassing existing interpretable embeddings with significantly fewer dimensions. The LDIR code is available on GitHub for further exploration and implementation.

<br /><br />Summary: <div>
arXiv:2505.10354v1 Announce Type: new 
Abstract: Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms "0/1" embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions. Code is available at https://github.com/szu-tera/LDIR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli</title>
<link>https://arxiv.org/abs/2505.10356</link>
<guid>https://arxiv.org/abs/2505.10356</guid>
<content:encoded><![CDATA[
<div> Keywords: brain decoding, multimodal, language reconstruction, visual-language models, brain-computer interaction

Summary:
This study introduces a framework for decoding coherent language from brain activity, which incorporates multiple input modalities, including visual, auditory, and textual stimuli. By leveraging visual-language models (VLMs) and modality-specific experts, the framework can interpret information across different modalities simultaneously. The experiments conducted demonstrate that the proposed method achieves comparable performance to existing systems while maintaining adaptability and extensibility. This research represents a significant step towards more ecologically valid and generalizable mind decoding techniques in the field of brain-computer interaction. <div>
arXiv:2505.10356v1 Announce Type: new 
Abstract: Decoding thoughts from brain activity offers valuable insights into human cognition and enables promising applications in brain-computer interaction. While prior studies have explored language reconstruction from fMRI data, they are typically limited to single-modality inputs such as images or audio. In contrast, human thought is inherently multimodal. To bridge this gap, we propose a unified and flexible framework for reconstructing coherent language from brain recordings elicited by diverse input modalities-visual, auditory, and textual. Our approach leverages visual-language models (VLMs), using modality-specific experts to jointly interpret information across modalities. Experiments demonstrate that our method achieves performance comparable to state-of-the-art systems while remaining adaptable and extensible. This work advances toward more ecologically valid and generalizable mind decoding.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples</title>
<link>https://arxiv.org/abs/2505.10389</link>
<guid>https://arxiv.org/abs/2505.10389</guid>
<content:encoded><![CDATA[
<div> Keywords: aspect-based sentiment analysis, large language models, opinion extraction, domain-specific taxonomies, structured prediction tasks

Summary:<br />
This paper delves into the design of an aspect-based sentiment analysis system utilizing large language models (LLMs) for practical applications. The focus is on quadruple opinion extraction, involving aspect categories, sentiment polarity, targets, and opinion expressions across various domains and languages. Through the use of internal datasets, the study examines the effectiveness of a single fine-tuned model in handling multiple domain-specific taxonomies concurrently. Results show that a combined multi-domain model can achieve comparable performance to specialized single-domain models, simplifying operational complexity. The paper also discusses insights gained on dealing with non-extractive predictions and evaluating different failure modes in developing LLM-based systems for structured prediction tasks.<br /><br />Summary: <div>
arXiv:2505.10389v1 Announce Type: new 
Abstract: This paper explores the design of an aspect-based sentiment analysis system using large language models (LLMs) for real-world use. We focus on quadruple opinion extraction -- identifying aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages. Using internal datasets, we investigate whether a single fine-tuned model can effectively handle multiple domain-specific taxonomies simultaneously. We demonstrate that a combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity. We also share lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking Repetition Problems of LLMs in Code Generation</title>
<link>https://arxiv.org/abs/2505.10402</link>
<guid>https://arxiv.org/abs/2505.10402</guid>
<content:encoded><![CDATA[
<div> structual repetition, code generation, neural language models, RPG, CodeRepetEval

Summary:<br /><br />This paper addresses the issue of structural repetition in code generation for neural language models (LLMs), a more challenging problem compared to content repetition. The proposed approach, RPG (Repetition Penalization based on Grammar), utilizes grammar rules to identify and mitigate structural repetitions during code generation. By strategically reducing the likelihood of critical tokens that contribute to repetitions, RPG effectively enhances the quality of generated code. The study introduces a new dataset, CodeRepetEval, to evaluate approaches for mitigating repetition problems in code generation. Experimental results demonstrate that RPG outperforms baselines on CodeRepetEval, HumanEval, and MBPP benchmarks, significantly reducing repetitions and improving the overall quality of the generated code. <div>
arXiv:2505.10402v1 Announce Type: new 
Abstract: With the advent of neural language models, the performance of code generation has been significantly boosted. However, the problem of repetitions during the generation process continues to linger. Previous work has primarily focused on content repetition, which is merely a fraction of the broader repetition problem in code generation. A more prevalent and challenging problem is structural repetition. In structural repetition, the repeated code appears in various patterns but possesses a fixed structure, which can be inherently reflected in grammar. In this paper, we formally define structural repetition and propose an efficient decoding approach called RPG, which stands for Repetition Penalization based on Grammar, to alleviate the repetition problems in code generation for LLMs. Specifically, RPG first leverages grammar rules to identify repetition problems during code generation, and then strategically decays the likelihood of critical tokens that contribute to repetitions, thereby mitigating them in code generation. To facilitate this study, we construct a new dataset CodeRepetEval to comprehensively evaluate approaches for mitigating the repetition problems in code generation. Extensive experimental results demonstrate that RPG substantially outperforms the best-performing baselines on CodeRepetEval dataset as well as HumanEval and MBPP benchmarks, effectively reducing repetitions and enhancing the quality of generated code.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation</title>
<link>https://arxiv.org/abs/2505.10409</link>
<guid>https://arxiv.org/abs/2505.10409</guid>
<content:encoded><![CDATA[
<div> Evaluation, Plain language summaries, Large language models, Comprehension, Human judgment

Summary:
The study evaluated the effectiveness of Large Language Models (LLMs) in generating Plain Language Summaries (PLSs) to aid communication between clinicians and patients. While LLM-generated PLSs were rated similarly to human-written ones in subjective evaluations, the study found that human-written PLSs resulted in better comprehension among readers. The research highlighted the limitations of automated evaluation metrics in reflecting human judgment, emphasizing the significance of frameworks that prioritize layperson comprehension in PLS generation. The findings emphasize the need for methods that optimize for understanding rather than surface-level quality.<br /><br /> <div>
arXiv:2505.10409v1 Announce Type: new 
Abstract: Plain language summaries (PLSs) are essential for facilitating effective communication between clinicians and patients by making complex medical information easier for laypeople to understand and act upon. Large language models (LLMs) have recently shown promise in automating PLS generation, but their effectiveness in supporting health information comprehension remains unclear. Prior evaluations have generally relied on automated scores that do not measure understandability directly, or subjective Likert-scale ratings from convenience samples with limited generalizability. To address these gaps, we conducted a large-scale crowdsourced evaluation of LLM-generated PLSs using Amazon Mechanical Turk with 150 participants. We assessed PLS quality through subjective Likert-scale ratings focusing on simplicity, informativeness, coherence, and faithfulness; and objective multiple-choice comprehension and recall measures of reader understanding. Additionally, we examined the alignment between 10 automated evaluation metrics and human judgments. Our findings indicate that while LLMs can generate PLSs that appear indistinguishable from human-written ones in subjective evaluations, human-written PLSs lead to significantly better comprehension. Furthermore, automated evaluation metrics fail to reflect human judgment, calling into question their suitability for evaluating PLSs. This is the first study to systematically evaluate LLM-generated PLSs based on both reader preferences and comprehension outcomes. Our findings highlight the need for evaluation frameworks that move beyond surface-level quality and for generation methods that explicitly optimize for layperson comprehension.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Document Refinement for Long-context Retrieval-augmented Generation</title>
<link>https://arxiv.org/abs/2505.10413</link>
<guid>https://arxiv.org/abs/2505.10413</guid>
<content:encoded><![CDATA[
<div> Keywords: LongRefiner, plug-and-play refiner, hierarchical document structuring, multi-task learning, computational costs  
Summary:  
LongRefiner is a new plug-and-play refiner designed to improve performance in long-context input scenarios encountered in real-world RAG applications. It utilizes dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. The experiments conducted on seven QA datasets demonstrate that LongRefiner achieves competitive performance while significantly reducing computational costs and latency by 10x compared to the best baseline method. The study shows that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. The code for LongRefiner is publicly available on GitHub at https://github.com/ignorejjj/LongRefiner. 

<br /><br />Summary: <div>
arXiv:2505.10413v1 Announce Type: new 
Abstract: Real-world RAG applications often encounter long-context input scenarios, where redundant information and noise results in higher inference costs and reduced performance. To address these challenges, we propose LongRefiner, an efficient plug-and-play refiner that leverages the inherent structural characteristics of long documents. LongRefiner employs dual-level query analysis, hierarchical document structuring, and adaptive refinement through multi-task learning on a single foundation model. Experiments on seven QA datasets demonstrate that LongRefiner achieves competitive performance in various scenarios while using 10x fewer computational costs and latency compared to the best baseline. Further analysis validates that LongRefiner is scalable, efficient, and effective, providing practical insights for real-world long-text RAG applications. Our code is available at https://github.com/ignorejjj/LongRefiner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models</title>
<link>https://arxiv.org/abs/2505.10446</link>
<guid>https://arxiv.org/abs/2505.10446</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Chain of Lateral Thought, reasoning framework, reinforcement learning, diffusion language models, bidirectional reasoning

Summary: 
The article introduces the Diffusion Chain of Lateral Thought (DCoLT), a novel reasoning framework for diffusion language models that optimizes the entire reasoning trajectory using reinforcement learning. DCoLT allows bidirectional, non-linear reasoning with no strict grammatical correctness rules in intermediate thought steps. Two representative diffusion language models, SEDD and LLaDA, are implemented with DCoLT. SEDD uses a probabilistic policy derived from its concrete score to maximize RL reward, while LLaDA optimizes its RL action through a ranking-based Unmasking Policy Module (UPM) based on the Plackett-Luce model. Experiment results on math and code generation tasks show that DCoLT-reinforced DLMs outperform other models trained by supervised fine-tuning (SFT) or RL or both. DCoLT-reinforced LLaDA significantly boosts reasoning accuracy on various tasks, demonstrating the effectiveness of the proposed framework. 

<br /><br />Summary: <div>
arXiv:2505.10446v1 Announce Type: new 
Abstract: We introduce the \emph{Diffusion Chain of Lateral Thought (DCoLT)}, a reasoning framework for diffusion language models. DCoLT treats each intermediate step in the reverse diffusion process as a latent "thinking" action and optimizes the entire reasoning trajectory to maximize the reward on the correctness of the final answer with outcome-based Reinforcement Learning (RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal, linear thinking process, DCoLT allows bidirectional, non-linear reasoning with no strict rule on grammatical correctness amid its intermediate steps of thought. We implement DCoLT on two representative Diffusion Language Models (DLMs). First, we choose SEDD as a representative continuous-time discrete diffusion model, where its concrete score derives a probabilistic policy to maximize the RL reward over the entire sequence of intermediate diffusion steps. We further consider the discrete-time masked diffusion language model -- LLaDA, and find that the order to predict and unmask tokens plays an essential role to optimize its RL action resulting from the ranking-based Unmasking Policy Module (UPM) defined by the Plackett-Luce model. Experiments on both math and code generation tasks show that using only public data and 16 H800 GPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even both. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%, +5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning</title>
<link>https://arxiv.org/abs/2505.10493</link>
<guid>https://arxiv.org/abs/2505.10493</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-Augmented Generation, Curriculum Learning, Training Framework, Open-Domain QA, Performance Improvement

Summary:
The paper introduces a novel approach called CL-RAG, which combines Curriculum Learning with a Retrieval-Augmented Generation (RAG) system to enhance the training process and optimize performance. By generating training data with varying difficulty levels for the retriever and generator components, the model progresses through stages based on the curriculum learning approach. This method improves the generalization and overall performance of the RAG system effectively. Experimental results across four open-domain question answering datasets show consistent performance gains of 2% to 4% compared to other advanced methods. CL-RAG demonstrates the potential of integrating curriculum learning into the training of large language models to enhance their adaptability and knowledge retrieval capabilities. 

<br /><br />Summary: <div>
arXiv:2505.10493v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is an effective method to enhance the capabilities of large language models (LLMs). Existing methods focus on optimizing the retriever or generator in the RAG system by directly utilizing the top-k retrieved documents. However, the documents effectiveness are various significantly across user queries, i.e. some documents provide valuable knowledge while others totally lack critical information. It hinders the retriever and generator's adaptation during training. Inspired by human cognitive learning, curriculum learning trains models using samples progressing from easy to difficult, thus enhancing their generalization ability, and we integrate this effective paradigm to the training of the RAG system. In this paper, we propose a multi-stage Curriculum Learning based RAG system training framework, named CL-RAG. We first construct training data with multiple difficulty levels for the retriever and generator separately through sample evolution. Then, we train the model in stages based on the curriculum learning approach, thereby optimizing the overall performance and generalization of the RAG system more effectively. Our CL-RAG framework demonstrates consistent effectiveness across four open-domain QA datasets, achieving performance gains of 2% to 4% over multiple advanced methods.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective</title>
<link>https://arxiv.org/abs/2505.10494</link>
<guid>https://arxiv.org/abs/2505.10494</guid>
<content:encoded><![CDATA[
<div> propose, CoV-Eval, multi-task benchmark, code security, LLMs <br />
Summary: <br />
The paper introduces CoV-Eval, a multi-task benchmark for evaluating code security in large language models (LLMs). This benchmark covers tasks like code completion, vulnerability repair, detection, and classification, providing a comprehensive assessment of LLMs. The authors also present VC-Judge, a judgment model that efficiently reviews LLM-generated programs for vulnerabilities. Evaluation of 20 LLMs reveals that while they can identify vulnerable code, they struggle with generating secure code and recognizing specific vulnerability types. The study highlights key challenges and areas for improvement in LLM code security, offering valuable insights for future research in this field. <br /> <div>
arXiv:2505.10494v1 Announce Type: new 
Abstract: Code security and usability are both essential for various coding assistant applications driven by large language models (LLMs). Current code security benchmarks focus solely on single evaluation task and paradigm, such as code completion and generation, lacking comprehensive assessment across dimensions like secure code generation, vulnerability repair and discrimination. In this paper, we first propose CoV-Eval, a multi-task benchmark covering various tasks such as code completion, vulnerability repair, vulnerability detection and classification, for comprehensive evaluation of LLM code security. Besides, we developed VC-Judge, an improved judgment model that aligns closely with human experts and can review LLM-generated programs for vulnerabilities in a more efficient and reliable way. We conduct a comprehensive evaluation of 20 proprietary and open-source LLMs. Overall, while most LLMs identify vulnerable codes well, they still tend to generate insecure codes and struggle with recognizing specific vulnerability types and performing repairs. Extensive experiments and qualitative analyses reveal key challenges and optimization directions, offering insights for future research in LLM code security.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks</title>
<link>https://arxiv.org/abs/2505.10507</link>
<guid>https://arxiv.org/abs/2505.10507</guid>
<content:encoded><![CDATA[
<div> translation-based transfer learning, cross-lingual, token classification, label projection, word aligners 

Summary:
- Translation-based strategies for cross-lingual transfer learning (XLT) for token classification tasks involve label projection to map labels between original and translated sentences.
- Word aligners (WAs) are commonly used for label projection, with low-level design decisions impact XLT performance significantly.
- The study systematically investigates WA design decisions like label mapping algorithm, filtering strategies, and pre-tokenization of translated sentences.
- Optimized WA choices offer XLT performance comparable to marker-based methods, with a new ensemble strategy of translate-train and translate-test predictions outperforming marker-based projection.
- The proposed ensembling approach enhances XLT robustness and reduces sensitivity to low-level WA design choices for token classification tasks. 

<br /><br />Summary: <div>
arXiv:2505.10507v1 Announce Type: new 
Abstract: Translation-based strategies for cross-lingual transfer XLT such as translate-train -- training on noisy target language data translated from the source language -- and translate-test -- evaluating on noisy source language data translated from the target language -- are competitive XLT baselines. In XLT for token classification tasks, however, these strategies include label projection, the challenging step of mapping the labels from each token in the original sentence to its counterpart(s) in the translation. Although word aligners (WAs) are commonly used for label projection, the low-level design decisions for applying them to translation-based XLT have not been systematically investigated. Moreover, recent marker-based methods, which project labeled spans by inserting tags around them before (or after) translation, claim to outperform WAs in label projection for XLT. In this work, we revisit WAs for label projection, systematically investigating the effects of low-level design decisions on token-level XLT: (i) the algorithm for projecting labels between (multi-)token spans, (ii) filtering strategies to reduce the number of noisily mapped labels, and (iii) the pre-tokenization of the translated sentences. We find that all of these substantially impact translation-based XLT performance and show that, with optimized choices, XLT with WA offers performance at least comparable to that of marker-based methods. We then introduce a new projection strategy that ensembles translate-train and translate-test predictions and demonstrate that it substantially outperforms the marker-based projection. Crucially, we show that our proposed ensembling also reduces sensitivity to low-level WA design choices, resulting in more robust XLT for token classification tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Token Prediction Needs Registers</title>
<link>https://arxiv.org/abs/2505.10518</link>
<guid>https://arxiv.org/abs/2505.10518</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-token prediction, language model pretraining, MuToR, fine-tuning, parameter-efficient fine-tuning

Summary:
MuToR is a new approach to multi-token prediction that involves integrating learnable register tokens into the input sequence. This method is simple, effective, and requires minimal additional parameters, making it compatible with existing pretrained language models. MuToR aligns well with the next-token pretraining objective, making it suitable for supervised fine-tuning tasks. It also supports scalable prediction horizons, allowing for versatile applications across different tasks. The authors demonstrate the effectiveness of MuToR in supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining on challenging generative tasks in both language and vision domains. The code for MuToR will be made available on GitHub for further exploration and implementation. 

<br /><br />Summary: MuToR is a novel approach to multi-token prediction that leverages learnable register tokens in the input sequence. It is compatible with existing language models, requires minimal parameter changes, and supports scalable prediction horizons. The method demonstrates effectiveness in supervised fine-tuning, PEFT, and pretraining on challenging generative tasks in language and vision domains. <div>
arXiv:2505.10518v1 Announce Type: new 
Abstract: Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldPM: Scaling Human Preference Modeling</title>
<link>https://arxiv.org/abs/2505.10527</link>
<guid>https://arxiv.org/abs/2505.10527</guid>
<content:encoded><![CDATA[
<div> scaling laws, preference modeling, World Preference Modeling, language models, generalization performance

Summary:<br />
- The study explores how test loss scales in preference modeling similarly to language modeling.
- World Preference Modeling (WorldPM) is proposed to represent human preferences.
- Adversarial metrics exhibit consistent scalability with increased training data and model size.
- Objective metrics show emergent behavior in larger language models, highlighting WorldPM's scalability potential.
- Subjective metrics do not exhibit scaling trends.
- WorldPM shows effectiveness in preference fine-tuning, improving generalization performance across human preference datasets.
- Integration of WorldPM into the RLHF pipeline results in significant performance improvements on both in-house and public evaluation sets. <div>
arXiv:2505.10527v1 Announce Type: new 
Abstract: Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.10554</link>
<guid>https://arxiv.org/abs/2505.10554</guid>
<content:encoded><![CDATA[
<div> Keywords: Large reasoning models, reinforcement learning, meta-abilities, alignment, scalability

Summary:<br />
Large reasoning models (LRMs) have the potential for advanced reasoning abilities but lack predictability and control in their emergence. This paper introduces a method to explicitly align LRMs with meta-abilities (deduction, induction, abduction) through self-verifiable tasks, improving performance by over 10% compared to baseline models. The three-stage pipeline involves individual alignment, parameter-space merging, and domain-specific reinforcement learning. The approach not only boosts performance but also increases the reliability and scalability of LRMs' reasoning capabilities. Domain-specific reinforcement learning from the aligned checkpoint further enhances performance across various benchmarks in math, coding, and science, showcasing the effectiveness of meta-ability alignment. The code for this method is openly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.10554v1 Announce Type: new 
Abstract: Large reasoning models (LRMs) already possess a latent capacity for long chain-of-thought reasoning. Prior work has shown that outcome-based reinforcement learning (RL) can incidentally elicit advanced reasoning behaviors such as self-correction, backtracking, and verification phenomena often referred to as the model's "aha moment". However, the timing and consistency of these emergent behaviors remain unpredictable and uncontrollable, limiting the scalability and reliability of LRMs' reasoning capabilities. To address these limitations, we move beyond reliance on prompts and coincidental "aha moments". Instead, we explicitly align models with three meta-abilities: deduction, induction, and abduction, using automatically generated, self-verifiable tasks. Our three stage-pipeline individual alignment, parameter-space merging, and domain-specific reinforcement learning, boosting performance by over 10\% relative to instruction-tuned baselines. Furthermore, domain-specific RL from the aligned checkpoint yields an additional 2\% average gain in the performance ceiling across math, coding, and science benchmarks, demonstrating that explicit meta-ability alignment offers a scalable and dependable foundation for reasoning. Code is available at: https://github.com/zhiyuanhubj/Meta-Ability-Alignment
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling</title>
<link>https://arxiv.org/abs/2505.09665</link>
<guid>https://arxiv.org/abs/2505.09665</guid>
<content:encoded><![CDATA[
<div> perceive response wildfires Reddit discourse LA fires 2025
<br />
Summary: 
During the 2025 Los Angeles wildfires, Reddit discourse was analyzed to understand public perception and response. 385 posts and 114,879 comments related to the Palisades and Eaton fires were collected and analyzed using topic modeling. A hierarchical framework categorized topics into Situational Awareness (SA) and Crisis Narratives (CN). SA topics peaked within the first 2-5 days aligning with fire progression. Public health and safety, loss and damage, and emergency resources were frequent topics, including discussions on environmental, occupational, and one health. Grief signals and mental health risks accounted for a significant portion of CN instances, with the highest volume at night. This study provides insights into public health concerns during wildfires, informing strategies for disaster response, health communication, and future research in similar climate-related disasters.
<br /> <div>
arXiv:2505.09665v1 Announce Type: cross 
Abstract: Wildfires have become increasingly frequent, irregular, and severe in recent years. Understanding how affected populations perceive and respond during wildfire crises is critical for timely and empathetic disaster response. Social media platforms offer a crowd-sourced channel to capture evolving public discourse, providing hyperlocal information and insight into public sentiment. This study analyzes Reddit discourse during the 2025 Los Angeles wildfires, spanning from the onset of the disaster to full containment. We collect 385 posts and 114,879 comments related to the Palisades and Eaton fires. We adopt topic modeling methods to identify the latent topics, enhanced by large language models (LLMs) and human-in-the-loop (HITL) refinement. Furthermore, we develop a hierarchical framework to categorize latent topics, consisting of two main categories, Situational Awareness (SA) and Crisis Narratives (CN). The volume of SA category closely aligns with real-world fire progressions, peaking within the first 2-5 days as the fires reach the maximum extent. The most frequent co-occurring category set of public health and safety, loss and damage, and emergency resources expands on a wide range of health-related latent topics, including environmental health, occupational health, and one health. Grief signals and mental health risks consistently accounted for 60 percentage and 40 percentage of CN instances, respectively, with the highest total volume occurring at night. This study contributes the first annotated social media dataset on the 2025 LA fires, and introduces a scalable multi-layer framework that leverages topic modeling for crisis discourse analysis. By identifying persistent public health concerns, our results can inform more empathetic and adaptive strategies for disaster response, public health communication, and future research in comparable climate-related disaster events.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Survey on Large Language Models in Multimodal Recommender Systems</title>
<link>https://arxiv.org/abs/2505.09777</link>
<guid>https://arxiv.org/abs/2505.09777</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal recommender systems, Large language models, Pre-trained language models, Prompting strategies, Fine-tuning methods<br />
Summary: 
This survey explores the intersection of large language models (LLMs) and multimodal recommender systems (MRS). It discusses the benefits of LLMs in enhancing recommendation performance through semantic reasoning, in-context learning, and dynamic input handling. The survey highlights the challenges of scalability and model accessibility posed by LLMs compared to pre-trained language models (PLMs). It categorizes integration patterns, transferable techniques from related recommendation domains, and evaluation metrics and datasets. The review aims to elucidate the evolving role of LLMs in multimodal recommendation systems and provides insights for future research directions in this dynamic field. <br /><br /> <div>
arXiv:2505.09777v1 Announce Type: cross 
Abstract: Multimodal recommender systems (MRS) integrate heterogeneous user and item data, such as text, images, and structured information, to enhance recommendation performance. The emergence of large language models (LLMs) introduces new opportunities for MRS by enabling semantic reasoning, in-context learning, and dynamic input handling. Compared to earlier pre-trained language models (PLMs), LLMs offer greater flexibility and generalisation capabilities but also introduce challenges related to scalability and model accessibility. This survey presents a comprehensive review of recent work at the intersection of LLMs and MRS, focusing on prompting strategies, fine-tuning methods, and data adaptation techniques. We propose a novel taxonomy to characterise integration patterns, identify transferable techniques from related recommendation domains, provide an overview of evaluation metrics and datasets, and point to possible future directions. We aim to clarify the emerging role of LLMs in multimodal recommendation and support future research in this rapidly evolving field.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers</title>
<link>https://arxiv.org/abs/2505.09855</link>
<guid>https://arxiv.org/abs/2505.09855</guid>
<content:encoded><![CDATA[
<div> Transformer models, learning modes, in-weights learning (IWL), in-context learning (ICL), evolutionary biology<br />
Summary:<br />
- Transformer models exhibit two learning modes, in-weights learning (IWL) and in-context learning (ICL), akin to genetic encoding and phenotypic plasticity in evolutionary biology.<br />
- Environmental stability favors IWL, while cue reliability enhances ICL efficacy, particularly in low stability scenarios.<br />
- High stability leads to a dominance of IWL, with a shift towards ICL in settings with easier IWL or slower ICL acquisition.<br />
- Task-contingent temporal evolution is observed in learning dynamics, with different learning mode transitions based on task characteristics.<br />
- These findings support the relative-cost hypothesis, highlighting predictability as a crucial factor in determining adaptive strategies in Transformers and providing insights for training methodologies. <br /> <div>
arXiv:2505.09855v1 Announce Type: cross 
Abstract: Transformer models learn in two distinct modes: in-weights learning (IWL), encoding knowledge into model weights, and in-context learning (ICL), adapting flexibly to context without weight modification. To better understand the interplay between these learning modes, we draw inspiration from evolutionary biology's analogous adaptive strategies: genetic encoding (akin to IWL, adapting over generations and fixed within an individual's lifetime) and phenotypic plasticity (akin to ICL, enabling flexible behavioral responses to environmental cues). In evolutionary biology, environmental predictability dictates the balance between these strategies: stability favors genetic encoding, while reliable predictive cues promote phenotypic plasticity. We experimentally operationalize these dimensions of predictability and systematically investigate their influence on the ICL/IWL balance in Transformers. Using regression and classification tasks, we show that high environmental stability decisively favors IWL, as predicted, with a sharp transition at maximal stability. Conversely, high cue reliability enhances ICL efficacy, particularly when stability is low. Furthermore, learning dynamics reveal task-contingent temporal evolution: while a canonical ICL-to-IWL shift occurs in some settings (e.g., classification with many classes), we demonstrate that scenarios with easier IWL (e.g., fewer classes) or slower ICL acquisition (e.g., regression) can exhibit an initial IWL phase later yielding to ICL dominance. These findings support a relative-cost hypothesis for explaining these learning mode transitions, establishing predictability as a critical factor governing adaptive strategies in Transformers, and offering novel insights for understanding ICL and guiding training methodologies.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks</title>
<link>https://arxiv.org/abs/2505.09901</link>
<guid>https://arxiv.org/abs/2505.09901</guid>
<content:encoded><![CDATA[
<div> cognitive science, exploratory behavior, multi-armed bandit tasks, large language models, decision-making strategies<br />
<br />Summary: Large language models (LLMs) are increasingly used to simulate human behavior in decision-making tasks. This study compares the exploration-exploitation strategies of LLMs, humans, and multi-armed bandit algorithms. Reasoning enhances LLM decision-making, making them exhibit more human-like behavior in simple tasks. However, in complex, changing environments, LLMs struggle to match human adaptability in directed exploration. Despite achieving similar regret in some scenarios, LLMs show limitations in effective exploration. This study highlights the potential and boundaries of LLMs in simulating human behavior and automated decision-making, pointing to areas for improvement. <br /> <div>
arXiv:2505.09901v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to simulate or automate human behavior in complex sequential decision-making tasks. A natural question is then whether LLMs exhibit similar decision-making behavior to humans, and can achieve comparable (or superior) performance. In this work, we focus on the exploration-exploitation (E&amp;E) tradeoff, a fundamental aspect of dynamic decision-making under uncertainty. We employ canonical multi-armed bandit (MAB) tasks introduced in the cognitive science and psychiatry literature to conduct a comparative study of the E&amp;E strategies of LLMs, humans, and MAB algorithms. We use interpretable choice models to capture the E&amp;E strategies of the agents and investigate how explicit reasoning, through both prompting strategies and reasoning-enhanced models, shapes LLM decision-making. We find that reasoning shifts LLMs toward more human-like behavior, characterized by a mix of random and directed exploration. In simple stationary tasks, reasoning-enabled LLMs exhibit similar levels of random and directed exploration compared to humans. However, in more complex, non-stationary environments, LLMs struggle to match human adaptability, particularly in effective directed exploration, despite achieving similar regret in certain scenarios. Our findings highlight both the promise and limits of LLMs as simulators of human behavior and tools for automated decision-making and point to potential areas of improvements.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization</title>
<link>https://arxiv.org/abs/2505.09921</link>
<guid>https://arxiv.org/abs/2505.09921</guid>
<content:encoded><![CDATA[
<div> Privacy Leakage, Large Language Models, Jailbreak Attacks, Personally Identifiable Information, Privacy Risks  
Summary:  
Large Language Models (LLMs) have shown proficiency in various domains but also present privacy risks. This paper explores the intersection of privacy leakage and jailbreak attacks in LLMs. The authors introduce PIG, a framework designed to target Personally Identifiable Information (PII) by identifying PII entities and types, utilizing in-context learning, and updating privacy context with gradient-based strategies. Evaluations on privacy datasets using white-box and black-box LLMs demonstrate that PIG outperforms existing methods and achieves state-of-the-art results. The study highlights the substantial privacy risks associated with LLMs and emphasizes the necessity for enhanced safeguards. Available code for PIG can be found at the provided GitHub link.  
Summary: <div>
arXiv:2505.09921v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel in various domains but pose inherent privacy risks. Existing methods to evaluate privacy leakage in LLMs often use memorized prefixes or simple instructions to extract data, both of which well-alignment models can easily block. Meanwhile, Jailbreak attacks bypass LLM safety mechanisms to generate harmful content, but their role in privacy scenarios remains underexplored. In this paper, we examine the effectiveness of jailbreak attacks in extracting sensitive information, bridging privacy leakage and jailbreak attacks in LLMs. Moreover, we propose PIG, a novel framework targeting Personally Identifiable Information (PII) and addressing the limitations of current jailbreak methods. Specifically, PIG identifies PII entities and their types in privacy queries, uses in-context learning to build a privacy context, and iteratively updates it with three gradient-based strategies to elicit target PII. We evaluate PIG and existing jailbreak methods using two privacy-related datasets. Experiments on four white-box and two black-box LLMs show that PIG outperforms baseline methods and achieves state-of-the-art (SoTA) results. The results underscore significant privacy risks in LLMs, emphasizing the need for stronger safeguards. Our code is availble at \href{https://github.com/redwyd/PrivacyJailbreak}{https://github.com/redwyd/PrivacyJailbreak}.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors</title>
<link>https://arxiv.org/abs/2505.09949</link>
<guid>https://arxiv.org/abs/2505.09949</guid>
<content:encoded><![CDATA[
<div> Keywords: freeway crashes, large language model, crash causation analysis, traffic safety, machine learning

Summary: 
- LLMs are utilized to analyze freeway crash data and provide comprehensive crash causation analysis by understanding the complex interactions between various factors.
- A training dataset consisting of environmental, driver, traffic, and geometric design factors was compiled from 226 traffic safety studies related to freeway crashes.
- The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and identify crash causes without pre-labeled data through zero-shot classification.
- Results demonstrate the effectiveness of LLMs in identifying primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention.
- Practical applicability and potential to improve traffic safety measures were validated through a high level of agreement among researchers in the field of traffic safety, indicating the valuable insights and potential countermeasures offered by this approach.

<br /><br />Summary: <div>
arXiv:2505.09949v1 Announce Type: cross 
Abstract: Understanding the factors contributing to traffic crashes and developing strategies to mitigate their severity is essential. Traditional statistical methods and machine learning models often struggle to capture the complex interactions between various factors and the unique characteristics of each crash. This research leverages large language model (LLM) to analyze freeway crash data and provide crash causation analysis accordingly. By compiling 226 traffic safety studies related to freeway crashes, a training dataset encompassing environmental, driver, traffic, and geometric design factors was created. The Llama3 8B model was fine-tuned using QLoRA to enhance its understanding of freeway crashes and their contributing factors, as covered in these studies. The fine-tuned Llama3 8B model was then used to identify crash causation without pre-labeled data through zero-shot classification, providing comprehensive explanations to ensure that the identified causes were reasonable and aligned with existing research. Results demonstrate that LLMs effectively identify primary crash causes such as alcohol-impaired driving, speeding, aggressive driving, and driver inattention. Incorporating event data, such as road maintenance, offers more profound insights. The model's practical applicability and potential to improve traffic safety measures were validated by a high level of agreement among researchers in the field of traffic safety, as reflected in questionnaire results with 88.89%. This research highlights the complex nature of traffic crashes and how LLMs can be used for comprehensive analysis of crash causation and other contributing factors. Moreover, it provides valuable insights and potential countermeasures to aid planners and policymakers in developing more effective and efficient traffic safety practices.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI</title>
<link>https://arxiv.org/abs/2505.10093</link>
<guid>https://arxiv.org/abs/2505.10093</guid>
<content:encoded><![CDATA[
<div> CS, Taiwanese China Studies, AI, knowledge graph, generative AI <br />
<br />
Summary: 
This study focuses on the development of Taiwanese China Studies (CS) and proposes an AI-assisted approach to organize and analyze decades of CS scholarship. By utilizing generative AI techniques and large language models, the researchers extract entity relation triples from peer-reviewed CS articles and visualize them in a knowledge graph using a D3.js based system. This allows for the exploration of conceptual nodes and semantic relationships within the corpus, revealing intellectual trajectories and research gaps. The system enables a shift from linear text consumption to network-based knowledge navigation, enhancing scholarly access to CS literature. By demonstrating how generative AI can support area studies and digital humanities, this work offers a scalable, data-driven alternative to traditional ontology construction and highlights the potential for a reimagined scholarly infrastructure for regional knowledge systems. <br /><br /> <div>
arXiv:2505.10093v1 Announce Type: cross 
Abstract: Taiwanese China Studies (CS) has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. This study responds to the growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship by proposing an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations. We apply generative AI (GAI) techniques and large language models (LLMs) to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. By decomposing textual content into graph structured knowledge units, our system enables a paradigm shift from linear text consumption to network based knowledge navigation. In doing so, it enhances scholarly access to CS literature while offering a scalable, data driven alternative to traditional ontology construction. This work not only demonstrates how generative AI can augment area studies and digital humanities but also highlights its potential to support a reimagined scholarly infrastructure for regional knowledge systems.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Virtual Machine Scheduling in Cloud Computing through Language Agents</title>
<link>https://arxiv.org/abs/2505.10117</link>
<guid>https://arxiv.org/abs/2505.10117</guid>
<content:encoded><![CDATA[
arXiv:2505.10117v1 Announce Type: cross 
Abstract: In cloud services, virtual machine (VM) scheduling is a typical Online Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by large-scale complexity and fluctuating demands. Traditional optimization methods struggle to adapt to real-time changes, domain-expert-designed heuristic approaches suffer from rigid strategies, and existing learning-based methods often lack generalizability and interpretability. To address these limitations, this paper proposes a hierarchical language agent framework named MiCo, which provides a large language model (LLM)-driven heuristic design paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov Decision Process with Options (SMDP-Option), enabling dynamic scheduling through a two-stage architecture, i.e., Option Miner and Option Composer. Option Miner utilizes LLMs to discover diverse and useful non-context-aware strategies by interacting with constructed environments. Option Composer employs LLMs to discover a composing strategy that integrates the non-context-aware strategies with the contextual ones. Extensive experiments on real-world enterprise datasets demonstrate that MiCo achieves a 96.9\% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines. It maintains high performance even under nonstationary request flows and diverse configurations, thus validating its effectiveness in complex and large-scale cloud environments.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering</title>
<link>https://arxiv.org/abs/2505.10118</link>
<guid>https://arxiv.org/abs/2505.10118</guid>
<content:encoded><![CDATA[
arXiv:2505.10118v1 Announce Type: cross 
Abstract: Existing visual token pruning methods target prompt alignment and visual preservation with static strategies, overlooking the varying relative importance of these objectives across tasks, which leads to inconsistent performance. To address this, we derive the first closed-form error bound for visual token pruning based on the Hausdorff distance, uniformly characterizing the contributions of both objectives. Moreover, leveraging $\epsilon$-covering theory, we reveal an intrinsic trade-off between these objectives and quantify their optimal attainment levels under a fixed budget. To practically handle this trade-off, we propose Multi-Objective Balanced Covering (MoB), which reformulates visual token pruning as a bi-objective covering problem. In this framework, the attainment trade-off reduces to budget allocation via greedy radius trading. MoB offers a provable performance bound and linear scalability with respect to the number of input visual tokens, enabling adaptation to challenging pruning scenarios. Extensive experiments show that MoB preserves 96.4% of performance for LLaVA-1.5-7B using only 11.1% of the original visual tokens and accelerates LLaVA-Next-7B by 1.3-1.5$\times$ with negligible performance loss. Additionally, evaluations on Qwen2-VL and Video-LLaVA confirm that MoB integrates seamlessly into advanced MLLMs and diverse vision-language tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention</title>
<link>https://arxiv.org/abs/2505.10222</link>
<guid>https://arxiv.org/abs/2505.10222</guid>
<content:encoded><![CDATA[
arXiv:2505.10222v1 Announce Type: cross 
Abstract: Transformer models rely on self-attention to capture token dependencies but face challenges in effectively integrating positional information while allowing multi-head attention (MHA) flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity. This paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA. CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. ComplexFormer incorporates two key improvements: (1) a per-head Euler transformation, converting real-valued query/key projections into polar-form complex vectors for head-specific complex subspace operation; and (2) a per-head adaptive differential rotation mechanism, exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct strategies for integrating semantic angle differences (ASmn,i) with relative positional encodings (Delta(Pmn),i). Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity , and improved long-context coherence compared to strong baselines like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging</title>
<link>https://arxiv.org/abs/2505.10231</link>
<guid>https://arxiv.org/abs/2505.10231</guid>
<content:encoded><![CDATA[
arXiv:2505.10231v1 Announce Type: cross 
Abstract: Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fairness in this domain. Our results show that incorporating human insights consistently reduces fairness gaps and enhances out-of-domain generalization, though excessive alignment can introduce performance trade-offs, emphasizing the need for calibrated strategies. These findings highlight Human-AI alignment as a promising approach for developing fair, robust, and generalizable medical AI systems, striking a balance between expert guidance and automated efficiency. Our code is available at https://github.com/Roypic/Aligner.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation</title>
<link>https://arxiv.org/abs/2505.10292</link>
<guid>https://arxiv.org/abs/2505.10292</guid>
<content:encoded><![CDATA[
arXiv:2505.10292v1 Announce Type: cross 
Abstract: Visual storytelling systems struggle to maintain character identity across frames and link actions to appropriate subjects, frequently leading to referential hallucinations. These issues can be addressed through grounding of characters, objects, and other entities on the visual elements. We propose StoryReasoning, a dataset containing 4,178 stories derived from 52,016 movie images, with both structured scene analyses and grounded stories. Each story maintains character and object consistency across frames while explicitly modeling multi-frame relationships through structured tabular representations. Our approach features cross-frame object re-identification using visual similarity and face recognition, chain-of-thought reasoning for explicit narrative modeling, and a grounding scheme that links textual elements to visual entities across multiple frames. We establish baseline performance by fine-tuning Qwen2.5-VL 7B, creating Qwen Storyteller, which performs end-to-end object detection, re-identification, and landmark detection while maintaining consistent object references throughout the story. Evaluation demonstrates a reduction from 4.06 to 3.56 (-12.3%) hallucinations on average per story when compared to a non-fine-tuned model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Superposition Yields Robust Neural Scaling</title>
<link>https://arxiv.org/abs/2505.10465</link>
<guid>https://arxiv.org/abs/2505.10465</guid>
<content:encoded><![CDATA[
arXiv:2505.10465v1 Announce Type: cross 
Abstract: The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law -- the finding that loss decreases as a power law with model size -- remains unclear. Starting from two empirical principles -- that LLMs represent more things than the model dimensions (widths) they have (i.e., representations are superposed), and that words or concepts in language occur with varying frequencies -- we constructed a toy model to study the loss scaling with model size. We found that when superposition is weak, meaning only the most frequent features are represented without interference, the scaling of loss with model size depends on the underlying feature frequency; if feature frequencies follow a power law, so does the loss. In contrast, under strong superposition, where all features are represented but overlap with each other, the loss becomes inversely proportional to the model dimension across a wide range of feature frequency distributions. This robust scaling behavior is explained geometrically: when many more vectors are packed into a lower dimensional space, the interference (squared overlaps) between vectors scales inversely with that dimension. We then analyzed four families of open-sourced LLMs and found that they exhibit strong superposition and quantitatively match the predictions of our toy model. The Chinchilla scaling law turned out to also agree with our results. We conclude that representation superposition is an important mechanism underlying the observed neural scaling laws. We anticipate that these insights will inspire new training strategies and model architectures to achieve better performance with less computation and fewer parameters.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parallel Scaling Law for Language Models</title>
<link>https://arxiv.org/abs/2505.10475</link>
<guid>https://arxiv.org/abs/2505.10475</guid>
<content:encoded><![CDATA[
arXiv:2505.10475v1 Announce Type: cross 
Abstract: It is commonly believed that scaling language models should commit a significant space or time cost, by increasing the parameters (parameter scaling) or output tokens (inference-time scaling). We introduce the third and more inference-efficient scaling paradigm: increasing the model's parallel computation during both training and inference time. We apply $P$ diverse and learnable transformations to the input, execute forward passes of the model in parallel, and dynamically aggregate the $P$ outputs. This method, namely parallel scaling (ParScale), scales parallel computation by reusing existing parameters and can be applied to any model structure, optimization procedure, data, or task. We theoretically propose a new scaling law and validate it through large-scale pre-training, which shows that a model with $P$ parallel streams is similar to scaling the parameters by $O(\log P)$ while showing superior inference efficiency. For example, ParScale can use up to 22$\times$ less memory increase and 6$\times$ less latency increase compared to parameter scaling that achieves the same performance improvement. It can also recycle an off-the-shelf pre-trained model into a parallelly scaled one by post-training on a small amount of tokens, further reducing the training budget. The new scaling law we discovered potentially facilitates the deployment of more powerful models in low-resource scenarios, and provides an alternative perspective for the role of computation in machine learning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs</title>
<link>https://arxiv.org/abs/2505.10495</link>
<guid>https://arxiv.org/abs/2505.10495</guid>
<content:encoded><![CDATA[
arXiv:2505.10495v1 Announce Type: cross 
Abstract: This paper addresses fine-tuning Large Language Models (LLMs) for function calling tasks when real user interaction data is unavailable. In digital content creation tools, where users express their needs through natural language queries that must be mapped to API calls, the lack of real-world task-specific data and privacy constraints for training on it necessitate synthetic data generation. Existing approaches to synthetic data generation fall short in diversity and complexity, failing to replicate real-world data distributions and leading to suboptimal performance after LLM fine-tuning. We present a novel router-based architecture that leverages domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models to generate high-quality synthetic training data. Our architecture's flexible routing mechanism enables synthetic data generation that matches observed real-world distributions, addressing a fundamental limitation of traditional approaches. Evaluation on a comprehensive set of real user queries demonstrates significant improvements in both function classification accuracy and API parameter selection. Models fine-tuned with our synthetic data consistently outperform traditional approaches, establishing new benchmarks for function calling tasks.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.10526</link>
<guid>https://arxiv.org/abs/2505.10526</guid>
<content:encoded><![CDATA[
arXiv:2505.10526v1 Announce Type: cross 
Abstract: Speculative decoding significantly accelerates language model inference by enabling a lightweight draft model to propose multiple tokens that a larger target model verifies simultaneously. However, applying this technique to vision-language models (VLMs) presents two fundamental challenges: small language models that could serve as efficient drafters lack the architectural components to process visual inputs, and their token predictions fail to match those of VLM target models that consider visual context. We introduce Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models (MASSV), which transforms existing small language models into effective multimodal drafters through a two-phase approach. MASSV first connects the target VLM's vision encoder to the draft model via a lightweight trainable projector, then applies self-distilled visual instruction tuning using responses generated by the target VLM to align token predictions. Comprehensive experiments across the Qwen2.5-VL and Gemma3 model families demonstrate that MASSV increases accepted length by up to 30% and delivers end-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV provides a scalable, architecture-compatible method for accelerating both current and future VLMs.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models</title>
<link>https://arxiv.org/abs/2505.10543</link>
<guid>https://arxiv.org/abs/2505.10543</guid>
<content:encoded><![CDATA[
arXiv:2505.10543v1 Announce Type: cross 
Abstract: While large language models demonstrate impressive performance on static benchmarks, the true potential of large language models as self-learning and reasoning agents in dynamic environments remains unclear. This study systematically evaluates the efficacy of self-reflection, heuristic mutation, and planning as prompting techniques to test the adaptive capabilities of agents. We conduct experiments with various open-source language models in dynamic environments and find that larger models generally outperform smaller ones, but that strategic prompting can close this performance gap. Second, a too-long prompt can negatively impact smaller models on basic reactive tasks, while larger models show more robust behaviour. Third, advanced prompting techniques primarily benefit smaller models on complex games, but offer less improvement for already high-performing large language models. Yet, we find that advanced reasoning methods yield highly variable outcomes: while capable of significantly improving performance when reasoning and decision-making align, they also introduce instability and can lead to big performance drops. Compared to human performance, our findings reveal little evidence of true emergent reasoning. Instead, large language model performance exhibits persistent limitations in crucial areas such as planning, reasoning, and spatial coordination, suggesting that current-generation large language models still suffer fundamental shortcomings that may not be fully overcome through self-reflective prompting alone. Reasoning is a multi-faceted task, and while reasoning methods like Chain of thought improves multi-step reasoning on math word problems, our findings using dynamic benchmarks highlight important shortcomings in general reasoning capabilities, indicating a need to move beyond static benchmarks to capture the complexity of reasoning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.10557</link>
<guid>https://arxiv.org/abs/2505.10557</guid>
<content:encoded><![CDATA[
arXiv:2505.10557v1 Announce Type: cross 
Abstract: Natural language image-caption datasets, widely used for training Large Multimodal Models, mainly focus on natural scenarios and overlook the intricate details of mathematical figures that are critical for problem-solving, hindering the advancement of current LMMs in multimodal mathematical reasoning. To this end, we propose leveraging code as supervision for cross-modal alignment, since code inherently encodes all information needed to generate corresponding figures, establishing a precise connection between the two modalities. Specifically, we co-develop our image-to-code model and dataset with model-in-the-loop approach, resulting in an image-to-code model, FigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date. Furthermore, we utilize FigCodifier to synthesize novel mathematical figures and then construct MM-MathInstruct-3M, a high-quality multimodal math instruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with ImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on MM-MathInstruct-3M for multimodal math problem solving. Our model achieves a new open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and Claude 3.5 Sonnet in the geometry problem-solving subset of MathVista, achieving improvements of 8.9% and 9.2%. The dataset and models will be released at https://github.com/mathllm/MathCoder.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Scaling Law for Large Language Models</title>
<link>https://arxiv.org/abs/2404.17785</link>
<guid>https://arxiv.org/abs/2404.17785</guid>
<content:encoded><![CDATA[
arXiv:2404.17785v3 Announce Type: replace 
Abstract: Recently, Large Language Models (LLMs) have been widely adopted in a wide range of tasks, leading to increasing attention towards the research on how scaling LLMs affects their performance. Existing works, termed Scaling Laws, have discovered that the final test loss of LLMs scales as power-laws with model size, computational budget, and dataset size. However, the temporal change of the test loss of an LLM throughout its pre-training process remains unexplored, though it is valuable in many aspects, such as selecting better hyperparameters \textit{directly} on the target LLM. In this paper, we propose the novel concept of Temporal Scaling Law, studying how the test loss of an LLM evolves as the training steps scale up. In contrast to modeling the test loss as a whole in a coarse-grained manner, we break it down and dive into the fine-grained test loss of each token position, and further develop a dynamic hyperbolic-law. Afterwards, we derive the much more precise temporal scaling law by studying the temporal patterns of the parameters in the dynamic hyperbolic-law. Results on both in-distribution (ID) and out-of-distribution (OOD) validation datasets demonstrate that our temporal scaling law accurately predicts the test loss of LLMs across training steps. Our temporal scaling law has broad practical applications. First, it enables direct and efficient hyperparameter selection on the target LLM, such as data mixture proportions. Secondly, viewing the LLM pre-training dynamics from the token position granularity provides some insights to enhance the understanding of LLM pre-training.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Mosaic Memory of Large Language Models</title>
<link>https://arxiv.org/abs/2405.15523</link>
<guid>https://arxiv.org/abs/2405.15523</guid>
<content:encoded><![CDATA[
arXiv:2405.15523v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become widely adopted, understanding how they learn from, and memorize, training data becomes crucial. Memorization in LLMs is widely assumed to only occur as a result of sequences being repeated in the training data. Instead, we show that LLMs memorize by assembling information from similar sequences, a phenomena we call mosaic memory. We show major LLMs to exhibit mosaic memory, with fuzzy duplicates contributing to memorization as much as 0.8 of an exact duplicate and even heavily modified sequences contributing substantially to memorization. Despite models display reasoning capabilities, we somewhat surprisingly show memorization to be predominantly syntactic rather than semantic. We finally show fuzzy duplicates to be ubiquitous in real-world data, untouched by deduplication techniques. Taken together, our results challenge widely held beliefs and show memorization to be a more complex, mosaic process, with real-world implications for privacy, confidentiality, model utility and evaluation.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization</title>
<link>https://arxiv.org/abs/2405.17067</link>
<guid>https://arxiv.org/abs/2405.17067</guid>
<content:encoded><![CDATA[
arXiv:2405.17067v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown remarkable capabilities in language understanding and generation. Nonetheless, it was also witnessed that LLMs tend to produce inaccurate responses to specific queries. This deficiency can be traced to the tokenization step LLMs must undergo, which is an inevitable limitation inherent to all LLMs. In fact, incorrect tokenization is the critical point that hinders LLMs in understanding the input precisely, thus leading to unsatisfactory output. This defect is more obvious in Chinese scenarios. To demonstrate this flaw of LLMs, we construct an adversarial dataset, named as $\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which draws upon the vocabularies of various open-source LLMs to challenge LLMs' tokenization. ADT consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. Our empirical results reveal that our ADT is highly effective on challenging the tokenization of leading LLMs, including GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs' capabilities. Moreover, our method of automatic data generation has been proven efficient and robust, which can be applied to any open-source LLMs. In this paper, we substantially investigate LLMs' vulnerability in terms of challenging their token segmentation, which will shed light on the subsequent research of improving LLMs' capabilities through optimizing their tokenization process and algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis</title>
<link>https://arxiv.org/abs/2406.00367</link>
<guid>https://arxiv.org/abs/2406.00367</guid>
<content:encoded><![CDATA[
arXiv:2406.00367v2 Announce Type: replace 
Abstract: Effectively analyzing the comments to uncover latent intentions holds immense value in making strategic decisions across various domains. However, several challenges hinder the process of sentiment analysis including the lexical diversity exhibited in comments, the presence of long dependencies within the text, encountering unknown symbols and words, and dealing with imbalanced datasets. Moreover, existing sentiment analysis tasks mostly leveraged sequential models to encode the long dependent texts and it requires longer execution time as it processes the text sequentially. In contrast, the Transformer requires less execution time due to its parallel processing nature. In this work, we introduce a novel hybrid deep learning model, RoBERTa-BiLSTM, which combines the Robustly Optimized BERT Pretraining Approach (RoBERTa) with Bidirectional Long Short-Term Memory (BiLSTM) networks. RoBERTa is utilized to generate meaningful word embedding vectors, while BiLSTM effectively captures the contextual semantics of long-dependent texts. The RoBERTa-BiLSTM hybrid model leverages the strengths of both sequential and Transformer models to enhance performance in sentiment analysis. We conducted experiments using datasets from IMDb, Twitter US Airline, and Sentiment140 to evaluate the proposed model against existing state-of-the-art methods. Our experimental findings demonstrate that the RoBERTa-BiLSTM model surpasses baseline models (e.g., BERT, RoBERTa-base, RoBERTa-GRU, and RoBERTa-LSTM), achieving accuracies of 80.74%, 92.36%, and 82.25% on the Twitter US Airline, IMDb, and Sentiment140 datasets, respectively. Additionally, the model achieves F1-scores of 80.73%, 92.35%, and 82.25% on the same datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling</title>
<link>https://arxiv.org/abs/2406.02069</link>
<guid>https://arxiv.org/abs/2406.02069</guid>
<content:encoded><![CDATA[
arXiv:2406.02069v4 Announce Type: replace 
Abstract: In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing. Our observations reveal that LLMs aggregate information through Pyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusing on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques, achieving up to a 20.5 absolute accuracy improvement on TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms competing methods in maintaining long-context comprehension in LLMs; notably, retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve 100.0 Acc. performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes</title>
<link>https://arxiv.org/abs/2407.01257</link>
<guid>https://arxiv.org/abs/2407.01257</guid>
<content:encoded><![CDATA[
arXiv:2407.01257v5 Announce Type: replace 
Abstract: Recent work on distilling Whisper's knowledge into small models using pseudo-labels shows promising performance while reducing the size by up to 50%. This results in small, efficient, and dedicated models. However, a critical step of distillation using pseudo-labels involves filtering high-quality predictions and using only those during training. This step requires ground truth labels to compare with and filter low-quality examples, making the process dependent on human labels. Additionally, the distillation process requires a large amount of data thereby limiting its applicability in low-resource settings. To address this, we propose a distillation framework that does not require any labeled data. Through experimentation, we show that our best-distilled models outperform the teacher model by 5-7 WER points and are on par with or outperform similar supervised data filtering setups. When scaling the data, our models significantly outperform all zero-shot and supervised models. Our models are also 25-50% more compute- and memory-efficient while maintaining performance equal to or better than that of the teacher model. For more details about our models, dataset, and other resources, please visit our GitHub page: https://github.com/UBC-NLP/uDistilWhisper.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conversational Query Reformulation with the Guidance of Retrieved Documents</title>
<link>https://arxiv.org/abs/2407.12363</link>
<guid>https://arxiv.org/abs/2407.12363</guid>
<content:encoded><![CDATA[
arXiv:2407.12363v5 Announce Type: replace 
Abstract: Conversational search seeks to retrieve relevant passages for the given questions in conversational question answering. Conversational Query Reformulation (CQR) improves conversational search by refining the original queries into de-contextualized forms to resolve the issues in the original queries, such as omissions and coreferences. Previous CQR methods focus on imitating human written queries which may not always yield meaningful search results for the retriever. In this paper, we introduce GuideCQR, a framework that refines queries for CQR by leveraging key information from the initially retrieved documents. Specifically, GuideCQR extracts keywords and generates expected answers from the retrieved documents, then unifies them with the queries after filtering to add useful information that enhances the search process. Experimental results demonstrate that our proposed method achieves state-of-the-art performance across multiple datasets, outperforming previous CQR methods. Additionally, we show that GuideCQR can get additional performance gains in conversational search using various types of queries, even for queries written by humans.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PersLLM: A Personified Training Approach for Large Language Models</title>
<link>https://arxiv.org/abs/2407.12393</link>
<guid>https://arxiv.org/abs/2407.12393</guid>
<content:encoded><![CDATA[
arXiv:2407.12393v5 Announce Type: replace 
Abstract: Large language models (LLMs) exhibit human-like intelligence, enabling them to simulate human behavior and support various applications that require both humanized communication and extensive knowledge reserves. Efforts are made to personify LLMs with special training data or hand-crafted prompts, while correspondingly faced with challenges such as insufficient data usage or rigid behavior patterns. Consequently, personified LLMs fail to capture personified knowledge or express persistent opinion. To fully unlock the potential of LLM personification, we propose PersLLM, a framework for better data construction and model tuning. For insufficient data usage, we incorporate strategies such as Chain-of-Thought prompting and anti-induction, improving the quality of data construction and capturing the personality experiences, knowledge, and thoughts more comprehensively. For rigid behavior patterns, we design the tuning process and introduce automated DPO to enhance the specificity and dynamism of the models' personalities, which leads to a more natural opinion communication. Both automated metrics and expert human evaluations demonstrate the effectiveness of our approach. Case studies in human-machine interactions and multi-agent systems further suggest potential application scenarios and future directions for LLM personification.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Next Token Prediction: Patch-Level Training for Large Language Models</title>
<link>https://arxiv.org/abs/2407.12665</link>
<guid>https://arxiv.org/abs/2407.12665</guid>
<content:encoded><![CDATA[
arXiv:2407.12665v3 Announce Type: replace 
Abstract: The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\times$, without compromising the model performance compared to token-level training. Source code: https://github.com/shaochenze/PatchTrain.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners</title>
<link>https://arxiv.org/abs/2407.15508</link>
<guid>https://arxiv.org/abs/2407.15508</guid>
<content:encoded><![CDATA[
arXiv:2407.15508v3 Announce Type: replace 
Abstract: The quantization of large language models (LLMs) has been a prominent research area aimed at enabling their lightweight deployment in practice. Existing research about LLM's quantization has mainly explored the interplay between weights and activations, or employing auxiliary components while neglecting the necessity of adjusting weights during quantization. Consequently, original weight distributions frequently fail to yield desired results after round-to-nearest (RTN) quantization. Even though incorporating techniques such as mixed precision and low-rank error approximation in LLM's quantization can yield improved results, they inevitably introduce additional computational overhead. On the other hand, traditional techniques for weight quantization, such as Generative Post-Training Quantization, rely on manually tweaking weight distributions to minimize local errors, but they fall short of achieving globally optimal outcomes. Although the recently proposed Learnable Singular-value Increment improves global weight quantization by modifying weight distributions, it disrupts the original distribution considerably. This introduces pronounced bias toward the training data and can degrade downstream task performance. In this paper, we introduce Singular-value Diagonal Expansion, a more nuanced approach to refining weight distributions to achieve better quantization alignment. Furthermore, we introduce Cross-layer Learning that improves overall quantization outcomes by distributing errors more evenly across layers. Our plug-and-play weight-quantization methods demonstrate substantial performance improvements over state-of-the-art approaches, including OmniQuant, DuQuant, and PrefixQuant.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time</title>
<link>https://arxiv.org/abs/2409.13338</link>
<guid>https://arxiv.org/abs/2409.13338</guid>
<content:encoded><![CDATA[
arXiv:2409.13338v3 Announce Type: replace 
Abstract: Who is the US President? The answer changes depending on when the question is asked. While large language models (LLMs) are evaluated on various reasoning tasks, they often miss a crucial dimension: time. In real-world scenarios, the correctness of answers is frequently tied to temporal context. To address this gap, we present a novel framework and dataset spanning over 8,000 events from 2018 to 2024, annotated with day-level granularity and sourced globally across domains such as politics, science, and business. Our TimeShift evaluation method systematically probes LLMs for temporal reasoning, revealing that base models often outperform instruction-tuned and synthetic-trained counterparts on time-sensitive recall. Additionally, we find that even large-scale models exhibit brittleness in handling paraphrased facts, highlighting unresolved challenges in temporal consistency. By identifying these limitations, our work provides a significant step toward advancing time-aware language models capable of adapting to the dynamic nature of real-world knowledge.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder</title>
<link>https://arxiv.org/abs/2409.14074</link>
<guid>https://arxiv.org/abs/2409.14074</guid>
<content:encoded><![CDATA[
arXiv:2409.14074v3 Announce Type: replace 
Abstract: Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology improves patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a linguistic analysis. We present practical ASR end-to-end training schemes optimized for a fixed number of trainable parameters that are common in industry settings. All code, data, and models are available online: https://github.com/leduckhai/MultiMed/tree/master/MultiMed.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TopoLM: brain-like spatio-functional organization in a topographic language model</title>
<link>https://arxiv.org/abs/2410.11516</link>
<guid>https://arxiv.org/abs/2410.11516</guid>
<content:encoded><![CDATA[
arXiv:2410.11516v3 Announce Type: replace 
Abstract: Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Does Knowledge Selection Help Retrieval Augmented Generation?</title>
<link>https://arxiv.org/abs/2410.13258</link>
<guid>https://arxiv.org/abs/2410.13258</guid>
<content:encoded><![CDATA[
arXiv:2410.13258v3 Announce Type: replace 
Abstract: Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChronoFact: Timeline-based Temporal Fact Verification</title>
<link>https://arxiv.org/abs/2410.14964</link>
<guid>https://arxiv.org/abs/2410.14964</guid>
<content:encoded><![CDATA[
arXiv:2410.14964v2 Announce Type: replace 
Abstract: Temporal claims, often riddled with inaccuracies, are a significant challenge in the digital misinformation landscape. Fact-checking systems that can accurately verify such claims are crucial for combating misinformation. Current systems struggle with the complexities of evaluating the accuracy of these claims, especially when they include multiple, overlapping, or recurring events. We introduce a novel timeline-based fact verification framework that identify events from both claim and evidence and organize them into their respective chronological timelines. The framework systematically examines the relationships between the events in both claim and evidence to predict the veracity of each claim event and their chronological accuracy. This allows us to accurately determine the overall veracity of the claim. We also introduce a new dataset of complex temporal claims involving timeline-based reasoning for the training and evaluation of our proposed framework. Experimental results demonstrate the effectiveness of our approach in handling the intricacies of temporal claim verification.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SceneGenAgent: Precise Industrial Scene Generation with Coding Agent</title>
<link>https://arxiv.org/abs/2410.21909</link>
<guid>https://arxiv.org/abs/2410.21909</guid>
<content:encoded><![CDATA[
arXiv:2410.21909v2 Announce Type: replace 
Abstract: The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction</title>
<link>https://arxiv.org/abs/2411.00646</link>
<guid>https://arxiv.org/abs/2411.00646</guid>
<content:encoded><![CDATA[
arXiv:2411.00646v2 Announce Type: replace 
Abstract: Vision Large Language Models (VLLMs) usually take input as a concatenation of image token embeddings and text token embeddings and conduct causal modeling. However, their internal behaviors remain underexplored, raising the question of interaction among two types of tokens. To investigate such multimodal interaction during model inference, in this paper, we measure the contextualization among the hidden state vectors of tokens from different modalities. Our experiments uncover a four-phase inference dynamics of VLLMs against the depth of Transformer-based LMs, including (I) Alignment: In very early layers, contextualization emerges between modalities, suggesting a feature space alignment. (II) Intra-modal Encoding: In early layers, intra-modal contextualization is enhanced while inter-modal interaction is suppressed, suggesting a local encoding within modalities. (III) Inter-modal Encoding: In later layers, contextualization across modalities is enhanced, suggesting a deeper fusion across modalities. (IV) Output Preparation: In very late layers, contextualization is reduced globally, and hidden states are aligned towards the unembedding space.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Disentangling Memory and Reasoning Ability in Large Language Models</title>
<link>https://arxiv.org/abs/2411.13504</link>
<guid>https://arxiv.org/abs/2411.13504</guid>
<content:encoded><![CDATA[
arXiv:2411.13504v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains. In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KBAlign: Efficient Self Adaptation on Specific Knowledge Bases</title>
<link>https://arxiv.org/abs/2411.14790</link>
<guid>https://arxiv.org/abs/2411.14790</guid>
<content:encoded><![CDATA[
arXiv:2411.14790v4 Announce Type: replace 
Abstract: Although retrieval-augmented generation (RAG) remains essential for knowledge-based question answering (KBQA), current paradigms face critical challenges under specific domains. Existing methods struggle with targeted adaptation on small-scale KBs: vanilla unsupervised training exhibits poor effectiveness, while fine-tuning incurs prohibitive costs of external signals. We present KBAlign, a self-supervised framework that enhances RAG systems through efficient model adaptation. Our key insight is to leverage the model's intrinsic capabilities for knowledge alignment through two innovative mechanisms: multi-grained self-annotation that captures global knowledge for data construction, and iterative tuning that accelerates convergence through self verification. This framework enables cost-effective model adaptation to specific textual KBs, without human supervision or external model assistance. Experiments demonstrate that KBAlign can achieve 90\% of the performance gain obtained through GPT-4-supervised adaptation, while relying entirely on self-annotation of much smaller models. KBAlign significantly improves downstream QA accuracy across multiple domains with tiny costs, particularly benefiting scenarios requiring deep knowledge integration from specialized corpora. We release our experimental data, models, and process analyses to the community for further exploration (https://github.com/thunlp/KBAlign).
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models</title>
<link>https://arxiv.org/abs/2411.19477</link>
<guid>https://arxiv.org/abs/2411.19477</guid>
<content:encoded><![CDATA[
arXiv:2411.19477v3 Announce Type: replace 
Abstract: We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models</title>
<link>https://arxiv.org/abs/2412.03587</link>
<guid>https://arxiv.org/abs/2412.03587</guid>
<content:encoded><![CDATA[
arXiv:2412.03587v2 Announce Type: replace 
Abstract: Transformer-based large-scale pre-trained models achieve great success. Fine-tuning is the standard practice for leveraging these models in downstream tasks. Among the fine-tuning methods, adapter-tuning provides a parameter-efficient fine-tuning by introducing lightweight trainable modules while keeping most pre-trained parameters frozen. However, existing adapter-tuning methods still impose substantial resource usage. Through our investigation, we show that each adapter unequally contributes to both task performance and resource usage. Motivated by this insight, we propose Selective Adapter FrEezing (SAFE), which gradually freezes less important adapters early to reduce unnecessary resource usage while maintaining performance. In our experiments, SAFE reduces memory usage, computation amount, and training time by 42.85\%, 34.59\%, and 11.82\%, respectively, while achieving comparable or better task performance compared to the baseline. We also demonstrate that SAFE induces regularization effect, thereby smoothing the loss landscape, which enables the model to generalize better by avoiding sharp minima.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title>
<link>https://arxiv.org/abs/2501.00777</link>
<guid>https://arxiv.org/abs/2501.00777</guid>
<content:encoded><![CDATA[
arXiv:2501.00777v2 Announce Type: replace 
Abstract: Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)</title>
<link>https://arxiv.org/abs/2501.13957</link>
<guid>https://arxiv.org/abs/2501.13957</guid>
<content:encoded><![CDATA[
arXiv:2501.13957v2 Announce Type: replace 
Abstract: Objective Structured Clinical Examinations (OSCEs) are widely used to assess medical students' communication skills, but scoring interview-based assessments is time-consuming and potentially subject to human bias. This study explored the potential of large language models (LLMs) to automate OSCE evaluations using the Master Interview Rating Scale (MIRS). We compared the performance of four state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro) in evaluating OSCE transcripts across all 28 items of the MIRS under the conditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step prompting. The models were benchmarked against a dataset of 10 OSCE cases with 174 expert consensus scores available. Model performance was measured using three accuracy metrics (exact, off-by-one, thresholded). Averaging across all MIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to 0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded accuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater reliability ({\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step techniques proved valuable when tailored to specific assessment items. The performance was consistent across MIRS items, independent of encounter phases and communication domains. We demonstrated the feasibility of AI-assisted OSCE evaluation and provided benchmarking of multiple LLMs across multiple prompt techniques. Our work provides a baseline performance assessment for LLMs that lays a foundation for future research into automated assessment of clinical communication skills.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs</title>
<link>https://arxiv.org/abs/2501.15674</link>
<guid>https://arxiv.org/abs/2501.15674</guid>
<content:encoded><![CDATA[
arXiv:2501.15674v2 Announce Type: replace 
Abstract: The reasoning abilities of Large Language Models (LLMs) can be improved by structurally denoising their weights, yet existing techniques primarily focus on denoising the feed-forward network (FFN) of the transformer block, and can not efficiently utilise the Multi-head Attention (MHA) block, which is the core of transformer architectures. To address this issue, we propose a novel intuitive framework that, at its very core, performs MHA compression through a multi-head tensorisation process and the Tucker decomposition. This enables both higher-dimensional structured denoising and compression of the MHA weights, by enforcing a shared higher-dimensional subspace across the weights of the multiple attention heads. We demonstrate that this approach consistently enhances the reasoning capabilities of LLMs across multiple benchmark datasets, and for both encoder-only and decoder-only architectures, while achieving compression rates of up to $\sim 250$ times in the MHA weights, all without requiring any additional data, training, or fine-tuning. Furthermore, we show that the proposed method can be seamlessly combined with existing FFN-only-based denoising techniques to achieve further improvements in LLM reasoning performance.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning</title>
<link>https://arxiv.org/abs/2502.04689</link>
<guid>https://arxiv.org/abs/2502.04689</guid>
<content:encoded><![CDATA[
arXiv:2502.04689v3 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities on complex evaluation benchmarks, many of which are formulated as question-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts is becoming increasingly vital for advancing their development and applicability. This paper introduces ARR, an intuitive, effective, and general QA solving method that explicitly incorporates three key steps: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Notably, this paper is the first to introduce intent analysis in QA, which plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA tasks demonstrate that ARR consistently outperforms the baseline methods. Ablation and case studies further validate the positive contributions of each ARR component. Furthermore, experiments involving variations in prompt design indicate that ARR maintains its effectiveness regardless of the specific prompt formulation. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Harnessing Multiple Large Language Models: A Survey on LLM Ensemble</title>
<link>https://arxiv.org/abs/2502.18036</link>
<guid>https://arxiv.org/abs/2502.18036</guid>
<content:encoded><![CDATA[
arXiv:2502.18036v4 Announce Type: replace 
Abstract: LLM Ensemble -- which involves the comprehensive use of multiple large language models (LLMs), each aimed at handling user queries during downstream inference, to benefit from their individual strengths -- has gained substantial attention recently. The widespread availability of LLMs, coupled with their varying strengths and out-of-the-box usability, has profoundly advanced the field of LLM Ensemble. This paper presents the first systematic review of recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM Ensemble and discuss several related research problems. Then, we provide a more in-depth classification of the methods under the broad categories of "ensemble-before-inference, ensemble-during-inference, ensemble-after-inference'', and review all relevant methods. Finally, we introduce related benchmarks and applications, summarize existing studies, and suggest several future research directions. A curated list of papers on LLM Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus</title>
<link>https://arxiv.org/abs/2503.06899</link>
<guid>https://arxiv.org/abs/2503.06899</guid>
<content:encoded><![CDATA[
arXiv:2503.06899v2 Announce Type: replace 
Abstract: Video-based dialogue systems, such as education assistants, have compelling application value, thereby garnering growing interest. However, the current video-based dialogue systems are limited by their reliance on a single dialogue type, which hinders their versatility in practical applications across a range of scenarios, including question-answering, emotional dialog, etc. In this paper, we identify this challenge as how to generate video-driven multilingual mixed-type dialogues. To mitigate this challenge, we propose a novel task and create a human-to-human video-driven multilingual mixed-type dialogue corpus, termed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues, across 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally, we establish baseline models on KwaiChat. An extensive analysis of 7 distinct LLMs on KwaiChat reveals that GPT-4o achieves the best performance but still cannot perform well in this situation even with the help of in-context learning and fine-tuning, which indicates that the task is not trivial and needs further research.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concise Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.05185</link>
<guid>https://arxiv.org/abs/2504.05185</guid>
<content:encoded><![CDATA[
arXiv:2504.05185v2 Announce Type: replace 
Abstract: Despite significant advancements in large language models (LLMs), a major drawback of reasoning models is their enormous token usage, which increases computational cost, resource requirements, and response time. In this work, we revisit the core principles of reinforcement learning (RL) and, through mathematical analysis, demonstrate that the tendency to generate lengthy responses arises inherently from RL-based optimization during training. This finding questions the prevailing assumption that longer responses inherently improve reasoning accuracy. Instead, we uncover a natural correlation between conciseness and accuracy that has been largely overlooked. We show that introducing a secondary phase of RL training, using a very small set of problems, can significantly reduce chains of thought while maintaining or even enhancing accuracy. Additionally, we demonstrate that, while GRPO shares some interesting properties of PPO, it suffers from collapse modes, which limit its reliability for concise reasoning. Finally, we validate our conclusions through extensive experimental results.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric</title>
<link>https://arxiv.org/abs/2504.07440</link>
<guid>https://arxiv.org/abs/2504.07440</guid>
<content:encoded><![CDATA[
arXiv:2504.07440v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. One core challenge of evaluation in the large language model (LLM) era is the generalization issue: how to infer a model's near-unbounded abilities from inevitably bounded benchmarks. We address this challenge by proposing Model Utilization Index (MUI), a mechanism interpretability enhanced metric that complements traditional performance scores. MUI quantifies the effort a model expends on a task, defined as the proportion of activated neurons or features during inference. Intuitively, a truly capable model should achieve higher performance with lower effort. Extensive experiments across popular LLMs reveal a consistent inverse logarithmic relationship between MUI and performance, which we formulate as the Utility Law. From this law we derive four practical corollaries that (i) guide training diagnostics, (ii) expose data contamination issue, (iii) enable fairer model comparisons, and (iv) design model-specific dataset diversity. Our code can be found at https://github.com/ALEX-nlp/MUI-Eva.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives</title>
<link>https://arxiv.org/abs/2504.10823</link>
<guid>https://arxiv.org/abs/2504.10823</guid>
<content:encoded><![CDATA[
arXiv:2504.10823v2 Announce Type: replace 
Abstract: Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction</title>
<link>https://arxiv.org/abs/2504.17671</link>
<guid>https://arxiv.org/abs/2504.17671</guid>
<content:encoded><![CDATA[
arXiv:2504.17671v3 Announce Type: replace 
Abstract: This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pose Priors from Language Models</title>
<link>https://arxiv.org/abs/2405.03689</link>
<guid>https://arxiv.org/abs/2405.03689</guid>
<content:encoded><![CDATA[
arXiv:2405.03689v2 Announce Type: replace-cross 
Abstract: Language is often used to describe physical interaction, yet most 3D human pose estimation methods overlook this rich source of information. We bridge this gap by leveraging large multimodal models (LMMs) as priors for reconstructing contact poses, offering a scalable alternative to traditional methods that rely on human annotations or motion capture data. Our approach extracts contact-relevant descriptors from an LMM and translates them into tractable losses to constrain 3D human pose optimization. Despite its simplicity, our method produces compelling reconstructions for both two-person interactions and self-contact scenarios, accurately capturing the semantics of physical and social interactions. Our results demonstrate that LMMs can serve as powerful tools for contact prediction and pose estimation, offering an alternative to costly manual human annotations or motion capture data. Our code is publicly available at https://prosepose.github.io.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Action Pretraining from Videos</title>
<link>https://arxiv.org/abs/2410.11758</link>
<guid>https://arxiv.org/abs/2410.11758</guid>
<content:encoded><![CDATA[
arXiv:2410.11758v2 Announce Type: replace-cross 
Abstract: We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.14251</link>
<guid>https://arxiv.org/abs/2411.14251</guid>
<content:encoded><![CDATA[
arXiv:2411.14251v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) mathematically formulates decision-making with Markov Decision Process (MDP). With MDPs, researchers have achieved remarkable breakthroughs across various domains, including games, robotics, and language models. This paper seeks a new possibility, Natural Language Reinforcement Learning (NLRL), by extending traditional MDP to natural language-based representation space. Specifically, NLRL innovatively redefines RL principles, including task objectives, policy, value function, Bellman equation, and policy iteration, into their language counterparts. With recent advancements in large language models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value improvement by either pure prompting or gradient-based training. Experiments over Maze, Breakthrough, and Tic-Tac-Toe games demonstrate the effectiveness, efficiency, and interpretability of the NLRL framework among diverse use cases.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective</title>
<link>https://arxiv.org/abs/2504.19458</link>
<guid>https://arxiv.org/abs/2504.19458</guid>
<content:encoded><![CDATA[
arXiv:2504.19458v3 Announce Type: replace-cross 
Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.
]]></content:encoded>
<pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence</title>
<link>https://arxiv.org/abs/2505.08828</link>
<guid>https://arxiv.org/abs/2505.08828</guid>
<content:encoded><![CDATA[
<div> authorship verification, AI assistance, academic writing, transparency, student development

Summary:
This research explores the use of authorship verification (AV) techniques to quantify AI assistance in academic writing, focusing on transparency and student development. By expanding datasets and developing an adapted AV method, the study evaluates the effectiveness of identifying stylometric differences between student and AI-generated texts. Results show that the enhanced AV classifier can detect discrepancies at word and sentence levels, providing educators with a transparent tool for academic integrity investigations. This work advances AV technology, offering insights into the dynamics of academic writing in an AI-driven era. <br /><br />Summary: <div>
arXiv:2505.08828v1 Announce Type: new 
Abstract: As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives</title>
<link>https://arxiv.org/abs/2505.08891</link>
<guid>https://arxiv.org/abs/2505.08891</guid>
<content:encoded><![CDATA[
<div> interactive narrative, AI, motivation, educational game, research ethics
<br />
Summary:
This article explores the impact of dynamic narratives in educational interactive games on learner motivation. Two versions of the game, one with a static narrative and the other with a dynamic narrative, were compared. Results showed that responsive content and a variety of choices are crucial for player engagement. Balancing pedagogical goals with dynamic narrative elements presents a challenge. The study highlights the potential of AI-driven dynamic narratives in educational games and discusses design implications arising from the findings. Ultimately, the research provides insights into the benefits and challenges of incorporating dynamic storytelling in educational games. <div>
arXiv:2505.08891v1 Announce Type: new 
Abstract: Motivation is an important factor underlying successful learning. Previous research has demonstrated the positive effects that static interactive narrative games can have on motivation. Concurrently, advances in AI have made dynamic and adaptive approaches to interactive narrative increasingly accessible. However, limited work has explored the impact that dynamic narratives can have on learner motivation. In this paper, we compare two versions of Academical, a choice-based educational interactive narrative game about research ethics. One version employs a traditional hand-authored branching plot (i.e., static narrative) while the other dynamically sequences plots during play (i.e., dynamic narrative). Results highlight the importance of responsive content and a variety of choices for player engagement, while also illustrating the challenge of balancing pedagogical goals with the dynamic aspects of narrative. We also discuss design implications that arise from these findings. Ultimately, this work provides initial steps to illuminate the emerging potential of AI-driven dynamic narrative in educational games.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A suite of LMs comprehend puzzle statements as well as humans</title>
<link>https://arxiv.org/abs/2505.08996</link>
<guid>https://arxiv.org/abs/2505.08996</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, comprehension, human performance, GPT-4, pragmatics

Summary: 
The study challenges previous claims that large language models (LMs) underperform humans in comprehending minimally complex English statements. The research found that human accuracy dropped significantly when rereading was restricted, falling below that of the Falcon-180B-Chat and GPT-4 models. The newer GPT-o1 model achieved perfect accuracy in comprehension tests. Both humans and models struggled with queries involving potentially reciprocal actions, suggesting shared pragmatic sensitivities. The study also revealed systematic underestimation of model performance and highlighted the need for more careful experimental design and coding practices in evaluating LLMs. Additionally, the results showed that GPT-4o can align with either naive or expert grammaticality judgments depending on prompt framing. This challenges the assumption that current models are inherently weaker than humans at language comprehension. 

<br /><br />Summary: <div>
arXiv:2505.08996v1 Announce Type: new 
Abstract: Recent claims suggest that large language models (LMs) underperform humans in comprehending minimally complex English statements (Dentella et al., 2024). Here, we revisit those findings and argue that human performance was overestimated, while LLM abilities were underestimated. Using the same stimuli, we report a preregistered study comparing human responses in two conditions: one allowed rereading (replicating the original study), and one that restricted rereading (a more naturalistic comprehension test). Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect accuracy. Results further show that both humans and models are disproportionately challenged by queries involving potentially reciprocal actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than model-specific deficits. Additional analyses using Llama-2-70B log probabilities, a recoding of open-ended model responses, and grammaticality ratings of other sentences reveal systematic underestimation of model performance. We find that GPT-4o can align with either naive or expert grammaticality judgments, depending on prompt framing. These findings underscore the need for more careful experimental design and coding practices in LLM evaluation, and they challenge the assumption that current models are inherently weaker than humans at language comprehension.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies</title>
<link>https://arxiv.org/abs/2505.09005</link>
<guid>https://arxiv.org/abs/2505.09005</guid>
<content:encoded><![CDATA[
<div> Keywords: LM, natural language understanding, metalinguistic judgments, information structure, syntax

Summary: 
The study investigates the ability of the GPT-4 language model (LM) to understand natural language and make reliable metalinguistic judgments. Through experiments involving information structure and acceptability tasks, the researchers find that GPT-4 exhibits metalinguistic skill that replicates human performance. They also demonstrate a causal relationship between the prominence of a constituent in a context sentence and subsequent acceptability ratings on long distance dependency constructions. The results suggest a close correspondence between how English speakers perceive information structure and syntax, and highlight the potential for LMs to capture subtle linguistic relationships proposed by linguists. Further research is needed to explore the implications of these findings. 

<br /><br />Summary: <div>
arXiv:2505.09005v1 Announce Type: new 
Abstract: It remains debated how well any LM understands natural language or generates reliable metalinguistic judgments. Moreover, relatively little work has demonstrated that LMs can represent and respect subtle relationships between form and function proposed by linguists. We here focus on a particular such relationship established in recent work: English speakers' judgments about the information structure of canonical sentences predicts independently collected acceptability ratings on corresponding 'long distance dependency' [LDD] constructions, across a wide array of base constructions and multiple types of LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on the same tasks used with humans and new extensions.Results reveal reliable metalinguistic skill on the information structure and acceptability tasks, replicating a striking interaction between the two, despite the zero-shot, explicit nature of the tasks, and little to no chance of contamination [Studies 1a, 1b]. Study 2 manipulates the information structure of base sentences and confirms a causal relationship: increasing the prominence of a constituent in a context sentence increases the subsequent acceptability ratings on an LDD construction. The findings suggest a tight relationship between natural and GPT-4 generated English, and between information structure and syntax, which begs for further exploration.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Atomic Consistency Preference Optimization for Long-Form Question Answering</title>
<link>https://arxiv.org/abs/2505.09039</link>
<guid>https://arxiv.org/abs/2505.09039</guid>
<content:encoded><![CDATA[
<div> ---
Large Language Models, Factoid Hallucinations, Model Alignment, Atomic Consistency Signals, Factual Accuracy <br />
<br />
Summary: 
The article introduces a new method called Atomic Consistency Preference Optimization (ACPO) to enhance factual accuracy in Large Language Models (LLMs) without the need for external supervision. ACPO leverages atomic consistency signals to identify high- and low-quality data pairs for model alignment, improving factual reliability in question-answering tasks. The method outperforms a strong supervised alignment baseline, FactAlign, on the LongFact and BioGen datasets, showcasing its effectiveness in enhancing factual accuracy. ACPO eliminates the reliance on costly external models or knowledge bases, providing a scalable and efficient approach for improving factoid question-answering in LLMs. <div>
arXiv:2505.09039v1 Announce Type: new 
Abstract: Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible. To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment. By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering. Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias</title>
<link>https://arxiv.org/abs/2505.09056</link>
<guid>https://arxiv.org/abs/2505.09056</guid>
<content:encoded><![CDATA[
<div> similarity, variability, ethical implications, LLMs, linguistic uniqueness <br />
Summary: 
- Outputs from the same LLM are more similar to each other than to human-written texts.
- Models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses.
- LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness.
- Differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content.
- Some LLMs demonstrate greater gender balance and reduced bias. <div>
arXiv:2505.09056v1 Announce Type: new 
Abstract: Large Language Models (LLMs) represent a major step toward artificial general intelligence, significantly advancing our ability to interact with technology. While LLMs perform well on Natural Language Processing tasks -- such as translation, generation, code writing, and summarization -- questions remain about their output similarity, variability, and ethical implications. For instance, how similar are texts generated by the same model? How does this compare across different models? And which models best uphold ethical standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like generation, explanation, and rewriting. This resulted in approximately 3 million texts from 12 LLMs, including proprietary and open-source systems from OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs from the same LLM are more similar to each other than to human-written texts; (2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses; (3) LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness; (4) differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate greater gender balance and reduced bias. These results offer new insights into the behavior and diversity of LLM outputs, helping guide future development and ethical evaluation.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment</title>
<link>https://arxiv.org/abs/2505.09068</link>
<guid>https://arxiv.org/abs/2505.09068</guid>
<content:encoded><![CDATA[
<div> Keywords: S-DAT, divergent thinking, multilingual, semantic distance, cognitive flexibility <br />
<br />
Summary: <br />
This paper introduces S-DAT, a framework for automated assessment of divergent thinking (DT) using semantic distance calculations. S-DAT is scalable and can be applied across multiple languages, showing consistent scoring. It demonstrates convergent validity with other DT measures and discriminant validity with convergent thinking. This cross-linguistic flexibility makes it a valuable tool for global-scale creativity research, overcoming limitations of previous methods. S-DAT enables fairer and more comprehensive evaluation of cognitive flexibility in diverse populations, providing a powerful online assessment tool. <div>
arXiv:2505.09068v1 Announce Type: new 
Abstract: This paper introduces S-DAT (Synthetic-Divergent Association Task), a scalable, multilingual framework for automated assessment of divergent thinking (DT) -a core component of human creativity. Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability. In contrast, S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance -- a language-agnostic proxy for DT. We evaluate S-DAT across eleven diverse languages, including English, Spanish, German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating robust and consistent scoring across linguistic contexts. Unlike prior DAT approaches, the S-DAT shows convergent validity with other DT measures and correct discriminant validity with convergent thinking. This cross-linguistic flexibility allows for more inclusive, global-scale creativity research, addressing key limitations of earlier approaches. S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online: https://sdat.iol.zib.de/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CEC-Zero: Chinese Error Correction Solution Based on LLM</title>
<link>https://arxiv.org/abs/2505.09082</link>
<guid>https://arxiv.org/abs/2505.09082</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, Chinese Spelling Correction, reinforcement learning, self-correction, generalization

Summary:
The paper introduces CEC-Zero, a novel reinforcement learning framework that enhances large language models (LLMs) for Chinese text processing. LLMs have shown superior performance in Chinese Spelling Correction (CSC) compared to traditional models like BERT, but challenges remain in reliability and generalization. By combining reinforcement learning with LLMs' generative capabilities, CEC-Zero enables autonomous error strategy learning without the need for annotated data or auxiliary models. Experiments demonstrate that the RL-enhanced LLMs achieve high accuracy and improved cross-domain generalization, making them suitable for practical Chinese NLP applications. This breakthrough in self-correction techniques opens up new possibilities for deploying LLMs in real-world scenarios and sets a precedent for self-improving language models in the future. 

<br /><br />Summary: <div>
arXiv:2505.09082v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How an unintended Side Effect of a Research Project led to Boosting the Power of UML</title>
<link>https://arxiv.org/abs/2505.09269</link>
<guid>https://arxiv.org/abs/2505.09269</guid>
<content:encoded><![CDATA[
<div> Keywords: UML, modeling tool, object diagrams, software architectures, teaching

Summary: 
This paper presents a new UML modeling tool that offers advancements over traditional tools by integrating class diagrams and object diagrams, as well as enabling object execution. This tool facilitates the creation of new software architectures that combine software with object models, making it valuable for educational purposes and providing students with an engaging learning experience. The project has originated from a longstanding international research initiative focused on a comprehensive multi-level architecture, demonstrating how research efforts can yield significant results as a byproduct of other undertakings. The tool's innovative features and origins highlight its potential to enhance modeling practices and educational experiences within the software development domain. 

<br /><br />Summary: <div>
arXiv:2505.09269v1 Announce Type: new 
Abstract: This paper describes the design, implementation and use of a new UML modeling tool that represents a significant advance over conventional tools. Among other things, it allows the integration of class diagrams and object diagrams as well as the execution of objects. This not only enables new software architectures characterized by the integration of software with corresponding object models, but is also ideal for use in teaching, as it provides students with a particularly stimulating learning experience. A special feature of the project is that it has emerged from a long-standing international research project, which is aimed at a comprehensive multi-level architecture. The project is therefore an example of how research can lead to valuable results that arise as a side effect of other work.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data</title>
<link>https://arxiv.org/abs/2505.09286</link>
<guid>https://arxiv.org/abs/2505.09286</guid>
<content:encoded><![CDATA[
<div> Keywords: Online review data, Multilingual, Unsupervised framework, Aspect detection, Language models

Summary: <br /><br />Effectively analyzing online review data is crucial for various industries. This study proposes a multilingual, scalable, and unsupervised framework for cross-domain aspect detection, addressing limitations of domain-specific studies and dependency on large labeled datasets. By extracting aspect category candidates through clustering and utilizing aspect-aware embedding vectors, the framework achieves high performance in multi-aspect labeling. Fine-tuning pretrained language models demonstrates the effectiveness of the generated labels, outperforming publicly available models in consistency and scalability. Human evaluation confirms the quality of automatic labels comparable to manual creation. The study highlights the potential of a robust multi-aspect labeling approach adaptable to diverse language and domain environments. Future research will explore automatic review summarization and integrating artificial intelligence agents to enhance review analysis efficiency and depth. <br /><br />Summary: <div>
arXiv:2505.09286v1 Announce Type: new 
Abstract: Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging</title>
<link>https://arxiv.org/abs/2505.09316</link>
<guid>https://arxiv.org/abs/2505.09316</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented reasoning, dynamic interaction, adaptive search behaviors, reinforcement learning, information-seeking process
Summary:<br /><br />
The study introduces InForage, a reinforcement learning framework inspired by Information Foraging Theory, for retrieval-augmented reasoning in large language models. Unlike traditional methods, InForage rewards intermediate retrieval quality, promoting adaptive search behaviors. A human-guided dataset capturing iterative search and reasoning trajectories was constructed for training the model. Extensive evaluations across various tasks showed that InForage outperformed baseline methods in general question answering, multi-hop reasoning tasks, and real-time web QA. The results emphasize InForage's ability to build robust, adaptive, and efficient reasoning agents. <div>
arXiv:2505.09316v1 Announce Type: new 
Abstract: Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs</title>
<link>https://arxiv.org/abs/2505.09338</link>
<guid>https://arxiv.org/abs/2505.09338</guid>
<content:encoded><![CDATA[
<div> entrainment, language models, distraction, context, semantic<br />
<br />
Contextual entrainment is observed in language models, where tokens previously seen in the prompt have higher probabilities assigned to them, regardless of relevance. This phenomenon occurs independently of semantic relations but is influenced by semantic factors, with counterfactual prompts more impactful. Entrainment heads, attention circuits responsible for entrainment, are identified through a novel method, and when deactivated, reduce the effect of contextual entrainment. This discovery offers insights into how language models become distracted and provides a step towards understanding and addressing this issue.<br /><br />Summary: Contextual entrainment is a novel phenomenon observed in language models, where previously seen tokens are assigned higher probabilities. It occurs independently of semantic relevance but is influenced by semantic factors, with specific prompts having a greater impact. Entrainment heads, attention circuits responsible for entrainment, are identified and their deactivation reduces the entrainment effect. This discovery provides a mechanistic insight into how language models become distracted by irrelevant context. <div>
arXiv:2505.09338v1 Announce Type: new 
Abstract: We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant'' contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors.
  We hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we ``turn off'' these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Qwen3 Technical Report</title>
<link>https://arxiv.org/abs/2505.09388</link>
<guid>https://arxiv.org/abs/2505.09388</guid>
<content:encoded><![CDATA[
<div> large language models, performance, multilingual capabilities, thinking mode, computational resources<br />
Summary:<br />
Qwen3 is the latest version in the Qwen model family, consisting of large language models with dense and MoE architectures ranging from 0.6 to 235 billion parameters. It integrates thinking mode for complex reasoning and non-thinking mode for rapid responses into a unified framework. This eliminates the need to switch between different models and enables dynamic mode switching based on user queries. Qwen3 introduces a thinking budget mechanism to adaptively allocate computational resources during inference, balancing latency and performance based on task complexity. The model achieves state-of-the-art results across various benchmarks, including code generation and mathematical reasoning. It expands multilingual support to 119 languages and dialects, enhancing global accessibility. Qwen3 models are publicly accessible under Apache 2.0 to promote reproducibility and community-driven research. <br /><br />Summary: <div>
arXiv:2505.09388v1 Announce Type: new 
Abstract: In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits</title>
<link>https://arxiv.org/abs/2505.09407</link>
<guid>https://arxiv.org/abs/2505.09407</guid>
<content:encoded><![CDATA[
<div> Keywords: Cloud-based multilingual translation, quantum computing, machine translation, quantum encoder-decoder architecture, OPUS dataset<br />
Summary: <br />
- Cloud-based multilingual translation services like Google Translate and Microsoft Translator utilize large multilingual language models such as GRU, LSTM, BERT, GPT, and T5, achieving state-of-the-art translation capabilities.
- New natural language systems like ChatGPT and DeepSeek have also shown significant potential in various natural language processing tasks and multilingual translation.
- QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) offers an alternative approach by exploring the quantum computing realm for multilingual machine translation, using quantum convolution, pooling, variational circuits, and attention.
- QEDACVC achieves an accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora, showcasing its effectiveness in multilingual translations.
- By simulating and running on quantum computing hardware, QEDACVC aims to revolutionize machine translation through its quantum encoder-decoder architecture. <br /> 
Summary: <div>
arXiv:2505.09407v1 Announce Type: new 
Abstract: Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning</title>
<link>https://arxiv.org/abs/2505.09519</link>
<guid>https://arxiv.org/abs/2505.09519</guid>
<content:encoded><![CDATA[
<div> Efficient fine-tuning, matrix decomposition, mixture-of-experts, question answering, mathematical problem solving

Summary:
Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models. The integration of router into prompt tuning (PT) increases efficiency but does not universally improve performance. Parameter reduction through matrix decomposition can enhance performance in specific domains. A novel framework, PT-MoE, integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets show PT-MoE achieves state-of-the-art performance in question answering (QA) and mathematical problem solving tasks. PT-MoE improves F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, using 25% fewer parameters than LoRA. PT methods excel in QA tasks, while LoRA-based methods perform better in math datasets. Integration of matrix decomposition and MoE in PT-MoE yields complementary benefits, demonstrating cross-task consistency and generalization abilities. Ablation studies on routing mechanisms and architectural components provide insights for future PEFT methods. <br /><br />Summary: <div>
arXiv:2505.09519v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models</title>
<link>https://arxiv.org/abs/2505.09595</link>
<guid>https://arxiv.org/abs/2505.09595</guid>
<content:encoded><![CDATA[
<div> bias, cultural inclusivity, benchmark, multiplexity, AI evaluation <br />
Summary: <br />
The article discusses how Large Language Models (LLMs) are often biased towards Western-centric perspectives, leading to cultural homogenization. To address this, the authors introduce WorldView-Bench, a benchmark that evaluates Global Cultural Inclusivity (GCI) in LLMs by measuring their ability to accommodate diverse worldviews. They introduce the concept of Multiplex models, which integrate diverse perspectives, and propose two intervention strategies to implement multiplexity in LLMs. The results show a significant increase in Perspectives Distribution Score (PDS) entropy with MAS-Implemented Multiplex LLMs, indicating a shift towards positive sentiment and enhanced cultural balance. This research highlights the importance of multiplex-aware AI evaluation in mitigating bias in LLMs and promoting more inclusive and ethically aligned AI systems. <br /> <div>
arXiv:2505.09595v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures</title>
<link>https://arxiv.org/abs/2505.08795</link>
<guid>https://arxiv.org/abs/2505.08795</guid>
<content:encoded><![CDATA[
<div> keywords: fast algorithm, hierarchical structures, Minkowski spacetime, token pairs, WordNet

Summary:
A fast algorithm is presented that embeds hierarchical structures in three-dimensional Minkowski spacetime, where data correlations are encoded solely in causal structure. Using oriented token pairs as local hierarchical signals, the model successfully embeds hierarchical structures from the WordNet corpus. The algorithm efficiently represents the mammal sub-tree and expands to include a subset of WordNet with over 82,000 noun tokens. A novel retrieval mechanism based on causality governs hierarchical access, rather than distance. The results suggest that all discrete data can be perfectly represented in a three-dimensional geometry, which exhibits nearly conformal invariance. This indicates deep connections with general relativity and field theory, suggesting that concepts, categories, and their interrelations are inherently geometric. <div>
arXiv:2505.08795v1 Announce Type: cross 
Abstract: We show that there is a fast algorithm that embeds hierarchical structures in three-dimensional Minkowski spacetime. The correlation of data ends up purely encoded in the causal structure. Our model relies solely on oriented token pairs -- local hierarchical signals -- with no access to global symbolic structure. We apply our method to the corpus of \textit{WordNet}. We provide a perfect embedding of the mammal sub-tree including ambiguities (more than one hierarchy per node) in such a way that the hierarchical structures get completely codified in the geometry and exactly reproduce the ground-truth. We extend this to a perfect embedding of the maximal unambiguous subset of the \textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We introduce a novel retrieval mechanism in which causality, not distance, governs hierarchical access. Our results seem to indicate that all discrete data has a perfect geometrical representation that is three-dimensional. The resulting embeddings are nearly conformally invariant, indicating deep connections with general relativity and field theory. These results suggest that concepts, categories, and their interrelations, namely hierarchical meaning itself, is geometric.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits</title>
<link>https://arxiv.org/abs/2505.08823</link>
<guid>https://arxiv.org/abs/2505.08823</guid>
<content:encoded><![CDATA[
<div> quantization, language models, large-scale, computational efficiency, normalization
<br />
Large language models (LLMs) have had a significant impact on natural-language processing but deploying them in real-world scenarios can be costly due to their scale. Post-training quantization can reduce memory and computation requirements, but often leads to accuracy degradation. This study focuses on ternary quantization (2-bit) for LLMs, which can provide significant savings but is challenging to implement. By incorporating RMS normalization and a layer-wise quantization schedule, researchers were able to fine-tune full-precision LLMs into ternary models without sacrificing performance. This approach proved to be as effective as more complex knowledge-distillation methods on standard language-modeling benchmarks, highlighting the importance of careful normalization in bridging the accuracy gap between ternary and full-precision LLMs. These findings suggest that ultra-low-bit inference for LLMs could become more feasible with the right normalization techniques.
<br /><br />Summary: <div>
arXiv:2505.08823v1 Announce Type: cross 
Abstract: Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries</title>
<link>https://arxiv.org/abs/2505.08842</link>
<guid>https://arxiv.org/abs/2505.08842</guid>
<content:encoded><![CDATA[
<div> framework, assessment, libraries, risks, governance
Summary:
LibVulnWatch is a graph-based agentic assessment framework that evaluates open-source AI libraries for risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. Using a directed acyclic graph of specialized agents, the system extracts and quantifies risk using evidence from trusted sources. It generates reproducible scores across five critical domains and publishes them on a public leaderboard for ecosystem monitoring. The system covers up to 88% of OpenSSF Scorecard checks and uncovers additional risks per library, including critical vulnerabilities like Remote Code Execution (RCE) and licensing constraints. It also identifies gaps in regulatory documentation and auditability. LibVulnWatch translates governance principles into practical metrics, advancing technical AI governance with continuous supply chain risk assessment and informed library selection. <br /><br />Summary: <div>
arXiv:2505.08842v1 Announce Type: cross 
Abstract: Open-source AI libraries are foundational to modern AI systems but pose significant, underexamined risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. We present LibVulnWatch, a graph-based agentic assessment framework that performs deep, source-grounded evaluations of these libraries. Built on LangGraph, the system coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases. LibVulnWatch generates reproducible, governance-aligned scores across five critical domains, publishing them to a public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library. These include critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability. By translating high-level governance principles into practical, verifiable metrics, LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance Gains of LLMs With Humans in a World of LLMs Versus Humans</title>
<link>https://arxiv.org/abs/2505.08902</link>
<guid>https://arxiv.org/abs/2505.08902</guid>
<content:encoded><![CDATA[
<div> LLMs, human experts, patient care, safeguard, safe delivery <br />
<br />
Summary: The research discusses the comparison between Large Language Models (LLMs) and human experts in the context of patient care. It highlights the potential harm LLMs could pose to established systems of safe patient care delivery. The article emphasizes the importance of characterizing the safe use of LLMs in clinical settings amidst rapid model developments. Rather than pitting LLMs against humans, the focus should shift towards developing strategies for effective collaboration between humans and LLMs. The research calls for a symbiotic relationship where humans and LLMs can work together efficiently to enhance patient care outcomes. <div>
arXiv:2505.08902v1 Announce Type: cross 
Abstract: Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term "expert" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under "humans versus LLMs" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora</title>
<link>https://arxiv.org/abs/2505.08905</link>
<guid>https://arxiv.org/abs/2505.08905</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Benchmark Construction, Synthetic Data, Document Populations, Model Evaluation

Summary: 
Language Models (LMs) are continuously improving in quality and coherence, raising the need for effective evaluation benchmarks. However, the manual construction of benchmarks is becoming challenging due to the vast amount of data encountered by LMs during training. To address this, a methodology for automating the creation of fact-based synthetic data model evaluations grounded in document populations is proposed. This approach utilizes LMs to evaluate domain-specific knowledge automatically, using only grounding documents as input. The methodology shows a high correlation with human-curated questions and provides diagnostic insight into LM capabilities through multiple choice and open-ended questions. Applied to a relevant arXiv preprint, the evaluation reveals strong performance from Gemma3 models. This automated benchmarking tool offers a promising solution for assessing LM performance efficiently and effectively. 

<br /><br />Summary: <div>
arXiv:2505.08905v1 Announce Type: cross 
Abstract: Language Models (LMs) continue to advance, improving response quality and coherence. Given Internet-scale training datasets, LMs have likely encountered much of what users might ask them to generate in some form during their training. A plethora of evaluation benchmarks have been constructed to assess model quality, response appropriateness, and reasoning capabilities. However, the human effort required for benchmark construction is limited and being rapidly outpaced by the size and scope of the models under evaluation. Additionally, having humans build a benchmark for every possible domain of interest is impractical. Therefore, we propose a methodology for automating the construction of fact-based synthetic data model evaluations grounded in document populations. This work leverages those very same LMs to evaluate domain-specific knowledge automatically, using only grounding documents (e.g., a textbook) as input. This synthetic data benchmarking approach corresponds well with human curated questions with a Spearman ranking correlation of 0.96 and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel tool supports generating both multiple choice and open-ended synthetic data questions to gain diagnostic insight of LM capability. We apply this methodology to evaluate model performance on a recent relevant arXiv preprint, discovering a surprisingly strong performance from Gemma3 models.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behind Maya: Building a Multilingual Vision Language Model</title>
<link>https://arxiv.org/abs/2505.08910</link>
<guid>https://arxiv.org/abs/2505.08910</guid>
<content:encoded><![CDATA[
<div> keywords: Vision-Language Models, Multilingual, Image-Text, Low-resource languages, Cultural contexts

Summary:<br /><br />
In response to the limitations of current large Vision-Language Models (VLMs) in performing well on low-resource languages and diverse cultural contexts, a new Multilingual VLM called Maya has been developed. This open-source model is based on a multilingual image-text pretraining dataset in eight languages, derived from the LLaVA pretraining dataset. Maya aims to enhance cultural and linguistic understanding in vision-language tasks across a range of languages. The model provides support for multiple languages, making it more inclusive and globally applicable. The code for Maya is available on GitHub for easy access and further development in the research community. Maya represents a step forward in promoting diversity and inclusivity in the field of Vision-Language Models, enabling improved performance across different linguistic and cultural backgrounds. <div>
arXiv:2505.08910v1 Announce Type: cross 
Abstract: In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers</title>
<link>https://arxiv.org/abs/2505.08941</link>
<guid>https://arxiv.org/abs/2505.08941</guid>
<content:encoded><![CDATA[
<div> Keywords: academic papers, citation rate prediction, causal language models, transformers, scaling-law analysis

Summary:
ForeCite is a framework designed to predict the future citation rates of academic papers using pre-trained causal language models with a linear head. It achieves a high test correlation of 0.826 on a dataset of over 900,000 biomedical papers published between 2000 and 2024, surpassing the previous state-of-the-art by 27 points. The model shows consistent improvements across different sizes and volumes of data, demonstrating its scalability and reliability. Gradient-based saliency heatmaps indicate a potential over-reliance on titles and abstract texts in predicting citations. These findings establish ForeCite as a new benchmark for forecasting the impact of academic research, paving the way for automated and accurate evaluation of scientific contributions. 

<br /><br />Summary: <div>
arXiv:2505.08941v1 Announce Type: cross 
Abstract: Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present $\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training</title>
<link>https://arxiv.org/abs/2505.08971</link>
<guid>https://arxiv.org/abs/2505.08971</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, pre-training, next-token prediction, importance sampling, scaling properties

Summary:
PRIOR is a novel vision-language pre-training approach that prioritizes image-related tokens in large vision-language models (LVLMs) by introducing a text-only reference language model (LLM) to weight each token based on its probability for LVLMs training. By adjusting the loss through token-specific re-weighting based on importance scores, PRIOR outperforms traditional next-token prediction (NTP) in both LVLMs with visual encoders and without, showing average relative improvements of 19% and 8% on various benchmarks. Additionally, PRIOR demonstrates superior scaling properties, with significantly higher scaling coefficients that suggest greater potential for performance gains compared to NTP with increased computational resources and data. <div>
arXiv:2505.08971v1 Announce Type: cross 
Abstract: In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automated Meta Prompt Engineering for Alignment with the Theory of Mind</title>
<link>https://arxiv.org/abs/2505.09024</link>
<guid>https://arxiv.org/abs/2505.09024</guid>
<content:encoded><![CDATA[
<div> Keywords: meta-prompting, Large Language Model, agentic reinforcement learning, Theory of Mind, content quality<br />
Summary: <br />
The article introduces a method of meta-prompting to improve text generation for complex tasks by optimizing the similarity between human mental expectations and a Large Language Model's neural processing. Using agentic reinforcement learning, a Judge LLM (LLMaaJ) teaches another LLM to produce content by interpreting desired text traits. Human edits on AI-generated articles were used to measure mental beliefs around content production at the US Open 2024. The LLMaaJ successfully aligned with human content reviewers 53.8% of the time, with an average iteration count of 4.38. Through a geometric interpretation of content traits over a Hilbert vector space, the LLMaaJ optimized Theory of Mind alignment by incorporating human edits into text generation. This approach improved content quality and expanded coverage of tennis action at the US Open 2024, leading to its application in other live events in sports and entertainment. <br /><br /> <div>
arXiv:2505.09024v1 Announce Type: cross 
Abstract: We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification</title>
<link>https://arxiv.org/abs/2505.09031</link>
<guid>https://arxiv.org/abs/2505.09031</guid>
<content:encoded><![CDATA[
<div> retrieval-augmented generation, chain-of-thought prompting, self-consistency, self-verification, hallucinations<br />
Summary:<br />
Hallucination is a significant issue in large language models (LLMs) as they often generate incorrect information. This study explores the combination of chain-of-thought (CoT) prompting with retrieval-augmented generation (RAG) and the use of self-consistency and self-verification strategies to address this problem. By integrating external knowledge sources and enabling models to verify or correct their outputs, the goal is to enhance the accuracy and coherence of responses. Comparative evaluations were conducted to assess the effectiveness of baseline LLMs, CoT, CoT+RAG, self-consistency, and self-verification techniques. The results demonstrate the efficacy of each method in reducing hallucinations while maintaining fluency and reasoning depth.Overall, the combined approach of CoT+RAG with self-consistency and self-verification techniques proves to be the most robust in minimizing hallucinations and improving factual accuracy. <br /><br />Summary: <div>
arXiv:2505.09031v1 Announce Type: cross 
Abstract: Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications</title>
<link>https://arxiv.org/abs/2505.09083</link>
<guid>https://arxiv.org/abs/2505.09083</guid>
<content:encoded><![CDATA[
<div> taxonomy-guided reasoning, weakly-supervised textual classification, transparency, explainability, hawkishness<br />
<br />
Summary:<br />
The article introduces Ornithologist, a weakly-supervised textual classification system that measures the hawkishness and dovishness of central bank text. It utilizes "taxonomy-guided reasoning" to guide a large language model with decision trees created by humans, increasing transparency and explainability while reducing the risk of hallucination. This approach requires less supervision than traditional systems, making it adaptable to various text sources like news. Ornithologist's measurements of RBA communication reveal insights into future cash rate paths and market expectations. <div>
arXiv:2505.09083v1 Announce Type: cross 
Abstract: I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.09246</link>
<guid>https://arxiv.org/abs/2505.09246</guid>
<content:encoded><![CDATA[
<div> framework, multi-hop question answering, structured knowledge, unstructured content, Semi-Structured Knowledge Bases  

Summary:  
FocusedRetriever is a modular framework for multi-hop question answering that leverages Semi-Structured Knowledge Bases (SKBs) to bridge the gap between structured knowledge and unstructured content. It outperforms state-of-the-art methods on diverse test sets, with an average first-hit rate 25.7% higher than the second-best method. The framework integrates components for entity search, query generation, pairwise re-ranking, and vector similarity search to retrieve and rank relevant information. Utilizing Large Language Models (LLMs), FocusedRetriever extracts relational facts and entity attributes from unstructured text, filters answer candidates based on extracted triplets, and uses contextual capabilities of LLMs for final ranking. The source code is publicly available for further upgrades, including finetuning with advanced LLMs. <div>
arXiv:2505.09246v1 Announce Type: cross 
Abstract: In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. However, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use. In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering. It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics. The average first-hit rate exceeds that of the second-best method by 25.7%. FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers. For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation. However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning. The source code is publicly available at https://github.com/kramerlab/FocusedRetriever .
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios</title>
<link>https://arxiv.org/abs/2505.09436</link>
<guid>https://arxiv.org/abs/2505.09436</guid>
<content:encoded><![CDATA[
<div> benchmark, large language models, customer experience management, contact center operations, synthetic dataset

Summary:
The article introduces CXMArena, a new synthetic benchmark dataset designed to assess the practical utility of Large Language Models (LLMs) in Customer Experience Management (CXM) contexts. The dataset includes knowledge base integration, real-world noise, and operational tasks beyond conversational fluency. The CXMArena benchmarks cover tasks such as Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Baseline experiments show that even state-of-the-art models struggle with tasks like article search and knowledge base refinement, indicating the need for more advanced solutions over conventional techniques. The dataset aims to bridge the gap in evaluating AI in complex operational CXM environments by providing a realistic and challenging benchmark for LLMs. <div>
arXiv:2505.09436v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors</title>
<link>https://arxiv.org/abs/2505.09610</link>
<guid>https://arxiv.org/abs/2505.09610</guid>
<content:encoded><![CDATA[
<div> Verilog, VHDL, Large Language Models, hardware design, processor design <br />
Summary: The paper discusses the use of Large Language Models (LLMs) in hardware design, particularly focusing on their application in explaining VHDL code for high-performance processor design. The study describes the development of test sets tailored to specific needs and the evaluation of models through extended pretraining (EPT). The expert evaluation of code explanations improved from 43% to 69% with the EPT model. An LLM-as-a-judge was also developed to assess models similarly to expert evaluators. Experimentation led to the creation and evaluation of new models, including an instruction-tuned version of the EPT model expected to achieve a 71% expert evaluator rating. The potential use of newer base models could push this rating to 85% and beyond. The paper concludes by discussing ways to enhance hardware design LLMs using advancements in Generative AI. <br />Summary: ArXiv:2505.09610v1 presents the development of Large Language Models focused on explaining VHDL code in high-performance processor design, highlighting improvements in expert evaluation ratings, the creation of an LLM-as-a-judge, and opportunities to enhance LLM quality. <div>
arXiv:2505.09610v1 Announce Type: cross 
Abstract: The use of Large Language Models (LLMs) in hardware design has taken off in recent years, principally through its incorporation in tools that increase chip designer productivity. There has been considerable discussion about the use of LLMs in RTL specifications of chip designs, for which the two most popular languages are Verilog and VHDL. LLMs and their use in Verilog design has received significant attention due to the higher popularity of the language, but little attention so far has been given to VHDL despite its continued popularity in the industry. There has also been little discussion about the unique needs of organizations that engage in high-performance processor design, and techniques to deploy AI solutions in these settings. In this paper, we describe our journey in developing a Large Language Model (LLM) specifically for the purpose of explaining VHDL code, a task that has particular importance in an organization with decades of experience and assets in high-performance processor design. We show how we developed test sets specific to our needs and used them for evaluating models as we performed extended pretraining (EPT) of a base LLM. Expert evaluation of the code explanations produced by the EPT model increased to 69% compared to a base model rating of 43%. We further show how we developed an LLM-as-a-judge to gauge models similar to expert evaluators. This led us to deriving and evaluating a host of new models, including an instruction-tuned version of the EPT model with an expected expert evaluator rating of 71%. Our experiments also indicate that with the potential use of newer base models, this rating can be pushed to 85% and beyond. We conclude with a discussion on further improving the quality of hardware design LLMs using exciting new developments in the Generative AI world.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?</title>
<link>https://arxiv.org/abs/2505.09614</link>
<guid>https://arxiv.org/abs/2505.09614</guid>
<content:encoded><![CDATA[
<div> Keywords: Language model, exploration, causal relationships, biases, reasoning

Summary: 
Language models (LMS) are used as autonomous decision-makers, but it is unclear if they can efficiently explore and infer causal relationships. In a study using the "Blicket Test" paradigm, LMs reliably infer common disjunctive relationships but struggle with less intuitive conjunctive ones. This "disjunctive bias" persists across LMs of different sizes and prompting strategies and worsens with task complexity. LMs exhibit adult-like inference profiles similar to humans. A test-time sampling method reduces the bias and improves LM reasoning towards scientific, causally rigorous thinking. <div>
arXiv:2505.09614v1 Announce Type: cross 
Abstract: Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established "Blicket Test" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This "disjunctive bias" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-based NLG Evaluation: Current Status and Challenges</title>
<link>https://arxiv.org/abs/2402.01383</link>
<guid>https://arxiv.org/abs/2402.01383</guid>
<content:encoded><![CDATA[
<div> NLG evaluation, natural language processing, large language models, automatic evaluation, ChatGPT <br />
Summary: 
Evaluating natural language generation (NLG) has been a challenging task, with traditional metrics lacking in capturing the essence of content overlap. Recently, large language models (LLMs) like ChatGPT have shown promise in NLG evaluation. Various methods based on LLMs have been developed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. This survey provides a taxonomy of LLM-based NLG evaluation methods, discussing their advantages and disadvantages. The article also highlights open issues in this field and suggests future research directions. <div>
arXiv:2402.01383v3 Announce Type: replace 
Abstract: Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model</title>
<link>https://arxiv.org/abs/2404.03080</link>
<guid>https://arxiv.org/abs/2404.03080</guid>
<content:encoded><![CDATA[
<div> Keywords: materials science, artificial intelligence, natural language processing, knowledge graph, data integration

Summary: 
The article introduces the Materials Knowledge Graph (MKG) which combines artificial intelligence with materials science to accelerate the discovery process. MKG utilizes natural language processing techniques to extract high-quality research data and categorizes it into structured triples with comprehensive labels such as Name, Formula, and Application. With 162,605 nodes and 731,772 edges, MKG enhances data usability and integration through a meticulously designed ontology. By leveraging network-based algorithms, MKG enables efficient link prediction and reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also sets the foundation for more advanced science knowledge graphs. 

Summary: <div>
arXiv:2404.03080v4 Announce Type: replace 
Abstract: Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering</title>
<link>https://arxiv.org/abs/2410.04526</link>
<guid>https://arxiv.org/abs/2410.04526</guid>
<content:encoded><![CDATA[
<div> benchmark, financial, multilingual, multimodal, question answering <br />
<br />
Summary: 
The paper introduces FAMMA, an open-source benchmark for evaluating large language models (LLMs) in financial question answering tasks. FAMMA consists of two versions: FAMMA-Basic with 1,945 questions from textbooks and exams, and FAMMA-LivePro with 103 novel questions created by experts. The questions cover advanced finance topics in multiple languages and include non-text data like charts and tables. Experiments show that FAMMA is challenging for LLMs, including reasoning models like GPT-01. The paper also curated reasoning trajectories of DeepSeek-R1 and fine-tuned Qwen models, leading to performance improvements on FAMMA-LivePro. The benchmark, data, code, and trained models are available on the FAMMA website. <div>
arXiv:2410.04526v3 Announce Type: replace 
Abstract: In this paper, we introduce FAMMA, an open-source benchmark for \underline{f}in\underline{a}ncial \underline{m}ultilingual \underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims to evaluate the abilities of large language models (LLMs) in answering complex reasoning questions that require advanced financial knowledge. The benchmark has two versions: FAMMA-Basic consists of 1,945 questions extracted from university textbooks and exams, along with human-annotated answers and rationales; FAMMA-LivePro consists of 103 novel questions created by human domain experts, with answers and rationales held out from the public for a contamination-free evaluation. These questions cover advanced knowledge of 8 major subfields in finance (e.g., corporate finance, derivatives, and portfolio management). Some are in Chinese or French, while a majority of them are in English. Each question has some non-text data such as charts, diagrams, or tables. Our experiments reveal that FAMMA poses a significant challenge on LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally, we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data, and fine-tuned a series of open-source Qwen models using this reasoning data. We found that training a model on these reasoning trajectories can significantly improve its performance on FAMMA-LivePro. We released our leaderboard, data, code, and trained models at https://famma-bench.github.io/famma/.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs</title>
<link>https://arxiv.org/abs/2411.09116</link>
<guid>https://arxiv.org/abs/2411.09116</guid>
<content:encoded><![CDATA[
<div> Multilingual, Large language models, Benchmark, Performance, Knowledge transfer<br />
<br />
Summary: 
The article introduces a comprehensive multilingual multitask benchmark called P-MMEval, which covers fundamental and capability-specialized datasets. It aims to address the limitations of previous assessments and offers consistent language coverage across various datasets with parallel samples. The benchmark is designed to evaluate performances of multilingual models across tasks and explore factors such as model sizes, languages, and prompts. Extensive experiments are conducted on representative multilingual model series to compare performances and assess knowledge transfer from English to other languages. The insights gained from these experiments are expected to provide valuable guidance for future research in the field of large language models. The dataset for P-MMEval is available at the provided link. 

Summary: <div>
arXiv:2411.09116v2 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark covering effective fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models and tasks, explore the relationship between multilingual performances and factors such as tasks, model sizes, languages, and prompts, and examine the effectiveness of knowledge transfer from English to other languages. The resulting insights are intended to offer valuable guidance for future research. The dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples</title>
<link>https://arxiv.org/abs/2502.09650</link>
<guid>https://arxiv.org/abs/2502.09650</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, alignment, difficulty, data selection, Selective DPO

Summary:<br /><br />
This study challenges the common assumption that using more clean data always leads to better outcomes in aligning large language models (LLMs). The authors propose a new principle that preference data vary in difficulty, with overly difficult examples hindering alignment by exceeding the model's capacity. Through systematic experimentation, they find that preference examples indeed vary in difficulty, affecting model performance. Overly difficult examples significantly degrade LLM performance across different models and datasets. The capacity of a model plays a crucial role in handling difficult examples, highlighting the importance of aligning data difficulty with model capacity. Introducing Selective DPO, a method that filters out overly difficult examples, improves alignment performance by 9-16% in win rates on a benchmark. The study emphasizes the need to consider data difficulty and model capacity in alignment strategies for LLMs. <div>
arXiv:2502.09650v2 Announce Type: replace 
Abstract: The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PropNet: a White-Box and Human-Like Network for Sentence Representation</title>
<link>https://arxiv.org/abs/2502.10725</link>
<guid>https://arxiv.org/abs/2502.10725</guid>
<content:encoded><![CDATA[
<div> transformer-based embedding methods, sentence representation, interpretability, PropNet, cognitive science

Summary: 
The article introduces PropNet, a white-box, human-like sentence representation network designed for interpretability in NLP tasks. While transformer-based embedding models have shown impressive performance, their black-box nature raises concerns about bias and trust. PropNet addresses these issues by constructing a hierarchical network based on propositions in a sentence, inspired by cognitive science principles. Despite lagging behind state-of-the-art models in semantic textual similarity tasks, PropNet offers insights into human cognitive processes in understanding language. The network's interpretability allows for in-depth analysis of sentence representations, shedding light on the limitations and potential improvements in current NLP models. Further research and refinement of PropNet could lead to a more transparent and human-centered approach to natural language understanding. <div>
arXiv:2502.10725v3 Announce Type: replace 
Abstract: Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference</title>
<link>https://arxiv.org/abs/2503.10652</link>
<guid>https://arxiv.org/abs/2503.10652</guid>
<content:encoded><![CDATA[
<div> consumer preferences, policy decisions, stated preference surveys, large language models, energy choices<br />
<br />
Summary: 
Survey research is essential for understanding consumer preferences and informing policy decisions. Stated preference (SP) surveys help in capturing individual trade-offs in hypothetical scenarios. Traditional SP survey methods are costly and time-consuming, leading researchers to explore the use of large language models (LLMs) for simulating consumer choices. This study evaluates the performance of various LLMs in energy-related SP surveys, considering factors like prompt design, reasoning capabilities, model types, and integration with traditional choice models. While LLMs show promising accuracy levels, they are not yet practical for simulation use. DeepSeek-R1 performs the best among the LLMs tested, indicating potential for data analysis. Pre-trained LLMs offer scalability and require minimal historical data, but further research is needed to improve prompt design and explore reasoning capabilities for better simulation results. <div>
arXiv:2503.10652v2 Announce Type: replace 
Abstract: Survey research plays a crucial role in studies by capturing consumer preferences and informing policy decisions. Stated preference (SP) surveys help researchers understand how individuals make trade-offs in hypothetical, potentially futuristic, scenarios. However, traditional methods are costly, time-consuming, and affected by respondent fatigue and ethical constraints. Large language models (LLMs) have shown remarkable capabilities in generating human-like responses, prompting interest in their use in survey research. This study investigates LLMs for simulating consumer choices in energy-related SP surveys and explores their integration into data collection and analysis workflows. Test scenarios were designed to assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and aggregated levels, considering prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, model types, integration with traditional choice models, and potential biases. While LLMs achieve accuracy above random guessing, performance remains insufficient for practical simulation use. Cloud-based LLMs do not consistently outperform smaller local models. DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Previous SP choices are the most effective input; longer prompts with more factors reduce accuracy. Mixed logit models can support LLM prompt refinement. Reasoning LLMs show potential in data analysis by indicating factor significance, offering a qualitative complement to statistical models. Despite limitations, pre-trained LLMs offer scalability and require minimal historical data. Future work should refine prompts, further explore CoT reasoning, and investigate fine-tuning techniques.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark</title>
<link>https://arxiv.org/abs/2503.17599</link>
<guid>https://arxiv.org/abs/2503.17599</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, General Practice, Evaluation Framework, Benchmark, State-of-the-Art

Summary:
The article discusses the challenges of assessing the ability of Large Language Models (LLMs) to function effectively as general practitioners (GPs) in real-world clinical settings. Existing evaluation frameworks are limited in capturing the comprehensive skills required in routine clinical practice. To address this gap, a novel evaluation framework called GPBench is proposed, consisting of data annotated by domain experts based on clinical practice standards. The study evaluates ten state-of-the-art LLMs and finds that they are not yet ready for autonomous deployment in GP roles due to the need for further optimization tailored to the specific responsibilities of GPs. The results highlight the importance of refining LLMs to meet the demands of daily clinical practice in general medicine. 

<br /><br />Summary: <div>
arXiv:2503.17599v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated considerable potential in general practice. However, existing benchmarks and evaluation frameworks primarily depend on exam-style or simplified question-answer formats, lacking a competency-based structure aligned with the real-world clinical responsibilities encountered in general practice. Consequently, the extent to which LLMs can reliably fulfill the duties of general practitioners (GPs) remains uncertain. In this work, we propose a novel evaluation framework to assess the capability of LLMs to function as GPs. Based on this framework, we introduce a general practice benchmark (GPBench), whose data are meticulously annotated by domain experts in accordance with routine clinical practice standards. We evaluate ten state-of-the-art LLMs and analyze their competencies. Our findings indicate that current LLMs are not yet ready for deployment in such settings without human oversight, and further optimization specifically tailored to the daily responsibilities of GPs is essential.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</title>
<link>https://arxiv.org/abs/2503.21696</link>
<guid>https://arxiv.org/abs/2503.21696</guid>
<content:encoded><![CDATA[
<div> Embodied Reasoner, interactive search tasks, spatial understanding, temporal reasoning, self-reflection, observation-thought-action trajectories <br />
Summary: <br />
Recent advancements in deep learning have shown impressive capabilities in math and coding tasks, but their potential in embodied domains requiring interaction with environments is largely unexplored. The Embodied Reasoner model addresses this by incorporating spatial understanding, temporal reasoning, and self-reflection into its problem-solving approach. By synthesizing a large dataset of interactive images and thinking processes, the model undergoes a three-stage training process to improve its abilities through imitation learning, self-exploration, and reflection tuning. Evaluation results demonstrate superior performance compared to other visual reasoning models, specifically outperforming OpenAI O1, O3-mini, and Claude-3.7. The model exhibits fewer repeated searches and logical inconsistencies, particularly excelling in complex long-horizon tasks and real-world environments. <div>
arXiv:2503.21696v2 Announce Type: replace 
Abstract: Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is analogy enough to draw novel adjective-noun inferences?</title>
<link>https://arxiv.org/abs/2503.24293</link>
<guid>https://arxiv.org/abs/2503.24293</guid>
<content:encoded><![CDATA[
<div> generalization, novel, adjective-noun, composition, inference

Summary: 
- Recent research has highlighted the ability of humans and large language models (LLMs) to generalize to novel adjective-noun combinations, suggesting access to a compositional mechanism for deriving inferences.
- The study explores whether inferences can be derived through analogy to known inferences instead of composition.
- A model of analogical reasoning based on similarity over lexical items was built, and human participants were asked to reason by analogy.
- While the analogy strategy worked well for a significant portion of the dataset, there were novel combinations where both humans and LLMs derived convergent inferences not well-suited for analogy.
- The findings suggest that the mechanism used by humans and LLMs to generalize in these instances cannot be fully explained by analogy alone and likely involves composition. 

<br /><br />Summary: <div>
arXiv:2503.24293v2 Announce Type: replace 
Abstract: Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks</title>
<link>https://arxiv.org/abs/2411.03343</link>
<guid>https://arxiv.org/abs/2411.03343</guid>
<content:encoded><![CDATA[
<div> jailbreaks, large language models, probes, non-linear features, model behavior <br />
Summary: <br />
The study focuses on understanding jailbreak mechanisms in large language models (LLMs) by analyzing both linear and non-linear features in prompt attempts. A dataset of 10,800 jailbreak attempts across 35 methods is introduced. Probes are trained to classify successful jailbreaks using latent representations of prompt tokens, showing that different strategies exploit varied non-linear features. While linear probes can predict jailbreak success accurately, their performance does not generalize to new attack methods, indicating the need for a nuanced understanding. Non-linear probes are effective in steering model behavior, guiding targeted perturbations in the latent space to enhance the model's robustness against jailbreaks. This challenges the idea that jailbreaks can be fully grasped through linear or universal prompt features alone, emphasizing the importance of comprehending LLM vulnerabilities in depth. <br /> <div>
arXiv:2411.03343v2 Announce Type: replace-cross 
Abstract: Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train probes to classify successful from unsuccessful jailbreaks using the latent representations corresponding to prompt tokens. Notably, we find that even when probes achieve high accuracy in predicting the success of jailbreaks, their performance often fails to generalize to unseen attack methods. This reveals that different jailbreaking strategies exploit different non-linear, non-universal features. Next, we demonstrate that non-linear probes provide a powerful tool for steering model behavior. Specifically, we use these probes to guide targeted latent space perturbations, enabling us to effectively modulate the model's robustness against jailbreaks. Overall, our findings challenge the assumption that jailbreaks can be fully understood through linear or simple universal prompt features alone, highlighting the importance of a nuanced understanding of the mechanisms behind LLM vulnerabilities.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FAS: Fast ANN-SNN Conversion for Spiking Large Language Models</title>
<link>https://arxiv.org/abs/2502.04405</link>
<guid>https://arxiv.org/abs/2502.04405</guid>
<content:encoded><![CDATA[
<div> Keywords: Spiking Large Language Models, Fast ANN-SNN conversion, fine-tuning, calibration, reduced inference latency

Summary:
The study introduces a novel Fast ANN-SNN conversion strategy (FAS) for transforming Language Models into Spiking Large Language Models (LLMs) in two stages. The first stage involves fine-tuning pre-trained models without the need for direct training from scratch. The second stage implements a calibration method to minimize conversion errors and enhance accuracy. Experimental results demonstrate that FAS achieves state-of-the-art performance across various language and vision-language tasks while significantly reducing inference latency and computational costs. Notably, FAS surpasses the OPT-7B model's accuracy by 3% using only eight timesteps and reducing energy consumption by 96.63%. The source code for FAS is available on GitHub for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2502.04405v2 Announce Type: replace-cross 
Abstract: Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. Notably, FAS only takes eight timesteps to achieve an accuracy of 3\% higher than that of the OPT-7B model, while reducing energy consumption by 96.63\%. The source code is available at https://github.com/lc783/FAS
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InductionBench: LLMs Fail in the Simplest Complexity Class</title>
<link>https://arxiv.org/abs/2502.15823</link>
<guid>https://arxiv.org/abs/2502.15823</guid>
<content:encoded><![CDATA[
<div> InductionBench, benchmark, inductive reasoning, Large language models, deficiencies <br />
Summary: <br />
The study introduces InductionBench, a benchmark to evaluate the inductive reasoning abilities of Large Language Models (LLMs). LLMs, such as o1 and o3, have excelled at deductive reasoning tasks, but struggle with inductive reasoning, crucial for scientific discovery. The new benchmark assesses LLMs' capacity to infer underlying rules from data. Experimental results show that even advanced models struggle with simple complexity classes within the subregular hierarchy of functions, revealing a deficiency in their inductive reasoning capabilities. The challenges LLMs face in mastering inductive reasoning indicate a need for further research and development in this area. The code and data for the benchmark are available on GitHub for further analysis and improvement. <br /> <div>
arXiv:2502.15823v4 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Analytical Emotion Framework of Rumour Threads on Social Media</title>
<link>https://arxiv.org/abs/2502.16560</link>
<guid>https://arxiv.org/abs/2502.16560</guid>
<content:encoded><![CDATA[
<div> Keywords: online social media, rumours, emotions, negativity, positivity

Summary:
Rumours in online social media can have significant impacts on society, highlighting the need for a deeper understanding of their development. This study focuses on the interplay between emotions and rumours in threaded discussions, exploring a comprehensive analytical framework for detecting emotions in both rumour and non-rumour threads. The analysis reveals that rumours tend to elicit more negative emotions like anger, fear, and pessimism, while non-rumours evoke more positive emotions. Emotions in online threads are found to be contagious, with rumours spreading negativity and non-rumours spreading positivity. Causal analysis indicates that surprise acts as a bridge between rumours and other emotions, while pessimism stems from sadness and fear, and optimism from joy and love. By examining the dynamics of emotions in online social media, this study offers insights into the emotional aspects of rumour propagation and sheds light on the comparative differences between rumours and non-rumours. 

<br /><br />Summary: <div>
arXiv:2502.16560v2 Announce Type: replace-cross 
Abstract: Rumours in online social media pose significant risks to modern society, motivating the need for better understanding of how they develop. We focus specifically on the interface between emotion and rumours in threaded discourses, building on the surprisingly sparse literature on the topic which has largely focused on single aspect of emotions within the original rumour posts themselves, and largely overlooked the comparative differences between rumours and non-rumours. In this work, we take one step further to provide a comprehensive analytical emotion framework with multi-aspect emotion detection, contrasting rumour and non-rumour threads and provide both correlation and causal analysis of emotions. We applied our framework on existing widely-used rumour datasets to further understand the emotion dynamics in online social media threads. Our framework reveals that rumours trigger more negative emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive ones. Emotions are contagious, rumours spread negativity, non-rumours spread positivity. Causal analysis shows surprise bridges rumours and other emotions; pessimism comes from sadness and fear, while optimism arises from joy and love.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering</title>
<link>https://arxiv.org/abs/2503.11197</link>
<guid>https://arxiv.org/abs/2503.11197</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, audio understanding, reasoning, audio question answering, large language models<br />
<br />
Summary: <br />
1) The study explores reinforcement learning in audio understanding and reasoning, specifically focusing on audio question answering tasks. The group relative policy optimization (GRPO) algorithm is applied to Qwen2-Audio-7B-Instruct, achieving state-of-the-art performance on the MMAU Test-mini benchmark with 64.5% accuracy.
2) Despite having only 8.2B parameters and 38k post-training samples, RL surpasses supervised fine-tuning (SFT), showcasing its effectiveness with limited data.
3) Explicit reasoning does not significantly improve AQA tasks, highlighting the need for more efficient utilization of deep thinking in future research.
4) Large audio language models (LALMs) like LALMs still fall short of human auditory-language reasoning abilities, emphasizing the need for further exploration and improvement in RL-based approaches. The project resources are available on GitHub and Hugging Face platforms for further analysis and research. <br /> <div>
arXiv:2503.11197v4 Announce Type: replace-cross 
Abstract: Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at https://github.com/xiaomi-research/r1-aqa and https://huggingface.co/mispeech/r1-aqa.
]]></content:encoded>
<pubDate>Thu, 15 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces</title>
<link>https://arxiv.org/abs/2505.07831</link>
<guid>https://arxiv.org/abs/2505.07831</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic neurons, artificial intelligence, language models, categorical vector space, intra-neuronal attention

Summary: 
The article introduces a new perspective on understanding synthetic neurons in artificial intelligence language models. Instead of considering them as a superposition of distributed features within a latent space, the authors propose a geometric approach. They define a neuron in a specific layer as a categorical vector space with a non-orthogonal basis. This basis is composed of categorical sub-dimensions extracted from neurons in the preceding layer. The categorical vector space is structured by the activation space of each neuron, allowing for an intra-neuronal attention process. This process identifies a critical categorical zone within the neuron that is essential for the model's efficiency. This zone is more homogeneous and located at the intersection of different categorical sub-dimensions. This novel approach provides insights into how synthetic neurons operate within language models, emphasizing the importance of categorical spaces for efficient model performance. 

<br /><br />Summary: <div>
arXiv:2505.07831v1 Announce Type: new 
Abstract: The polysemantic nature of synthetic neurons in artificial intelligence language models is currently understood as the result of a necessary superposition of distributed features within the latent space. We propose an alternative approach, geometrically defining a neuron in layer n as a categorical vector space with a non-orthogonal basis, composed of categorical sub-dimensions extracted from preceding neurons in layer n-1. This categorical vector space is structured by the activation space of each neuron and enables, via an intra-neuronal attention process, the identification and utilization of a critical categorical zone for the efficiency of the language model - more homogeneous and located at the intersection of these different categorical sub-dimensions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas</title>
<link>https://arxiv.org/abs/2505.07850</link>
<guid>https://arxiv.org/abs/2505.07850</guid>
<content:encoded><![CDATA[
<div> Algorithmic othering, racial identity, synthetic personas, LLMs, representational harm<br />
<br />
Summary: 
The article examines how large language models (LLMs) generate synthetic personas and evaluate their representation of racial identities. Through analysis of personas created by three LLMs and comparing them to human-authored responses, the study found that LLMs tend to emphasize racial markers, utilize culturally coded language excessively, and create personas that are narratively simplistic despite syntactic complexity. This leads to harmful outcomes such as stereotyping, exoticism, erasure, and bias. The phenomenon is termed algorithmic othering, where minoritized identities are made overly visible but lacking in authenticity. The article suggests design recommendations for evaluation metrics and validation protocols to address these issues in the generation of synthetic identities. <div>
arXiv:2505.07850v1 Announce Type: new 
Abstract: As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment</title>
<link>https://arxiv.org/abs/2505.07852</link>
<guid>https://arxiv.org/abs/2505.07852</guid>
<content:encoded><![CDATA[
<div> ensemble classification model, concept drift analysis, One Class Drift Detector (OCDD), large language model (LLM), social engineering chat scenarios

Summary: 
The paper addresses the challenge of detecting fake interactions in digital communication platforms, which can range from spam to sophisticated scams. Traditional methods often fail to adapt to dynamic conversational shifts, leading to false alarms or missed threats. The proposed framework consists of a two-stage process: first identifying suspicious conversations using an ensemble classification model, then analyzing concept drift within flagged dialogues using an OCDD. When drift is detected, a LLM assesses the shift to determine fraudulent intent or legitimate topic change. The framework is validated using a dataset of social engineering chat scenarios, demonstrating improved accuracy and interpretability for real-time fraud detection. A modular approach outperforms a Dual LLM baseline, highlighting the benefits of incorporating concept drift analysis into fraud detection systems.<br /><br />Summary: <div>
arXiv:2505.07852v1 Announce Type: new 
Abstract: Detecting fake interactions in digital communication platforms remains a challenging and insufficiently addressed problem. These interactions may appear as harmless spam or escalate into sophisticated scam attempts, making it difficult to flag malicious intent early. Traditional detection methods often rely on static anomaly detection techniques that fail to adapt to dynamic conversational shifts. One key limitation is the misinterpretation of benign topic transitions referred to as concept drift as fraudulent behavior, leading to either false alarms or missed threats. We propose a two stage detection framework that first identifies suspicious conversations using a tailored ensemble classification model. To improve the reliability of detection, we incorporate a concept drift analysis step using a One Class Drift Detector (OCDD) to isolate conversational shifts within flagged dialogues. When drift is detected, a large language model (LLM) assesses whether the shift indicates fraudulent manipulation or a legitimate topic change. In cases where no drift is found, the behavior is inferred to be spam like. We validate our framework using a dataset of social engineering chat scenarios and demonstrate its practical advantages in improving both accuracy and interpretability for real time fraud detection. To contextualize the trade offs, we compare our modular approach against a Dual LLM baseline that performs detection and judgment using different language models.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis</title>
<link>https://arxiv.org/abs/2505.07853</link>
<guid>https://arxiv.org/abs/2505.07853</guid>
<content:encoded><![CDATA[
<div> Keywords: Road crashes, Large Language Model, Crash analysis, Data augmentation, Explainability<br />
Summary: <br />
The study introduces CrashSage, a framework based on Large Language Models, to enhance crash analysis by transforming structured crash data into enriched textual narratives. Context-aware data augmentation and fine-tuning of a specific LLaMA3-8B model enable superior performance in crash severity inference compared to traditional approaches. Additionally, a gradient-based explainability technique is employed to elucidate model decisions and provide insights into influential factors. This innovative approach aims to address the limitations of conventional statistical models in capturing complex relationships and contextual nuances in road crash data, ultimately providing actionable insights for targeted road safety interventions. <br /><br />Summary: <div>
arXiv:2505.07853v1 Announce Type: new 
Abstract: Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights</title>
<link>https://arxiv.org/abs/2505.07856</link>
<guid>https://arxiv.org/abs/2505.07856</guid>
<content:encoded><![CDATA[
<div> evaluation, adversarial attacks, inflectional languages, robustness, model behaviour

Summary:<br />
Various techniques for generating adversarial examples have been developed, including subtle perturbations such as TextBugger and synonym substitutions like TextFooler. However, these methods are primarily tested on non-inflectional languages like English. This study evaluates the performance of adversarial attacks in inflectional languages, specifically Polish and English, using a novel Edge Attribution Patching (EAP) protocol. By analyzing models on the MultiEmo dataset, the impact of inflection on model behavior and robustness is explored. A benchmark is created to assess task-specific corpora containing inflected and syncretic text variants, allowing for the identification of inflection-related elements in model circuits and their behavior under attack. This work provides insights into the challenges posed by inflection in adversarial attacks and sheds light on the robustness of models in inflectional languages. 

Summary: <div>
arXiv:2505.07856v1 Announce Type: new 
Abstract: Various techniques are used in the generation of adversarial examples, including methods such as TextBugger which introduce minor, hardly visible perturbations to words leading to changes in model behaviour. Another class of techniques involves substituting words with their synonyms in a way that preserves the text's meaning but alters its predicted class, with TextFooler being a prominent example of such attacks. Most adversarial example generation methods are developed and evaluated primarily on non-inflectional languages, typically English. In this work, we evaluate and explain how adversarial attacks perform in inflectional languages. To explain the impact of inflection on model behaviour and its robustness under attack, we designed a novel protocol inspired by mechanistic interpretability, based on Edge Attribution Patching (EAP) method. The proposed evaluation protocol relies on parallel task-specific corpora that include both inflected and syncretic variants of texts in two languages -- Polish and English. To analyse the models and explain the relationship between inflection and adversarial robustness, we create a new benchmark based on task-oriented dataset MultiEmo, enabling the identification of mechanistic inflection-related elements of circuits within the model and analyse their behaviour under attack.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines</title>
<link>https://arxiv.org/abs/2505.07857</link>
<guid>https://arxiv.org/abs/2505.07857</guid>
<content:encoded><![CDATA[
<div> Urdu, intent detection, contrastive learning, pre-trained language models, prototype-informed attention mechanism
<br />
Multifarious intent detection predictors are developed for different languages, but Urdu, the 10th most spoken language, lacks sophisticated models. This study introduces a contrastive learning approach for intent detection in Urdu, using unlabeled data to re-train pre-trained language models. The re-training enhances representation learning for intent detection tasks. The proposed LLMPIA pipeline combines pre-trained language models and a prototype-informed attention mechanism. The framework is evaluated on two benchmark datasets, ATIS and Web Queries, achieving high F1-scores under various experimental settings. In a case study on the Web Queries dataset, the LLMPIA outperforms the state-of-the-art predictor by a significant margin. This approach shows promise in addressing the underdeveloped field of intent detection in Urdu language.
<br /><br />Summary: <div>
arXiv:2505.07857v1 Announce Type: new 
Abstract: Multifarious intent detection predictors are developed for different languages, including English, Chinese and French, however, the field remains underdeveloped for Urdu, the 10th most spoken language. In the realm of well-known languages, intent detection predictors utilize the strategy of few-shot learning and prediction of unseen classes based on the model training on seen classes. However, Urdu language lacks few-shot strategy based intent detection predictors and traditional predictors are focused on prediction of the same classes which models have seen in the train set. To empower Urdu language specific intent detection, this introduces a unique contrastive learning approach that leverages unlabeled Urdu data to re-train pre-trained language models. This re-training empowers LLMs representation learning for the downstream intent detection task. Finally, it reaps the combined potential of pre-trained LLMs and the prototype-informed attention mechanism to create a comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm of proposed predictive pipeline, it explores the potential of 6 distinct language models and 13 distinct similarity computation methods. The proposed framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing 5836 samples and Web Queries having 8519 samples. Across ATIS dataset under 4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and 98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score, respectively. In an additional case study on the Web Queries dataset under same classes train and test set settings, LLMPIA outperformed state-of-the-art predictor by 53.55% F1-Score.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.07858</link>
<guid>https://arxiv.org/abs/2505.07858</guid>
<content:encoded><![CDATA[
<div> Efficient decoding, large language models, speculative decoding techniques, scaling laws, Scylla<br />
<br />
Summary:<br />
The study focuses on efficient decoding in large language models, particularly in reasoning-intensive architectures. Speculative decoding methods using parallel draft-verification cycles are explored for accelerating reasoning tasks. Log-linear Scaling Laws governing draft model acceptance rate across various dimensions are discovered. Scylla, a system coordinating multi-dimensional scaling for popular LLMs, achieves higher acceptance rates and decoding throughput improvements compared to existing models. Empirical validation highlights performance gains on summarization and QA tasks. Industrial inference engine deployments demonstrate the potential of systematic scaling for efficient LLM inference. Code release is planned for the future. <div>
arXiv:2505.07858v1 Announce Type: new 
Abstract: The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Boosting Performance on ARC is a Matter of Perspective</title>
<link>https://arxiv.org/abs/2505.07859</link>
<guid>https://arxiv.org/abs/2505.07859</guid>
<content:encoded><![CDATA[
<div> data augmentation, depth-first search algorithm, language models, scoring, performance<br />
<br />
Summary: 
The article discusses the use of task-specific data augmentations, a depth-first search algorithm, and language models to improve abstract reasoning abilities on the ARC-AGI Corpus. By incorporating these techniques throughout training, generation, and scoring phases, the proposed method achieves a score of 71.6% on the ARC-AGI evaluation set. This performance is considered state-of-the-art among publicly available approaches. The approach not only utilizes language models as generators but also as scorers, using output probabilities to select the most promising solutions. Notably, the method stands out for its transparency, reproducibility, and low inference cost, averaging only 2 cents per task on standard hardware. While other closed-source work has reported higher scores, this method offers a cost-effective and accessible solution for addressing abstract reasoning challenges posed by ARC-AGI. <br /><br />Summary: <div>
arXiv:2505.07859v1 Announce Type: new 
Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU).
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable LLM Math Reasoning Acceleration with Low-rank Distillation</title>
<link>https://arxiv.org/abs/2505.07861</link>
<guid>https://arxiv.org/abs/2505.07861</guid>
<content:encoded><![CDATA[
<div> Efficient Inference, Large Language Model, Math Reasoning, Caprese, Distillation

Summary:
Caprese is a new low-cost distillation method designed to enhance math performance in large language models (LLMs) while maintaining efficiency in inference methods. By focusing on feedforward blocks and utilizing only 1% additional parameters and 20K synthetic training samples, Caprese successfully recovers lost math capabilities without impacting language tasks. The method does not alter the original weights, significantly reduces the number of active parameters in LLMs such as Gemma and Llama, and integrates seamlessly into existing model layers to reduce latency. Caprese also promotes response brevity and improves token generation efficiency, making it a practical solution for enhancing math reasoning in LLMs. 

<br /><br />Summary: <div>
arXiv:2505.07861v1 Announce Type: new 
Abstract: Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a low-cost distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>11% reduction to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition</title>
<link>https://arxiv.org/abs/2505.07862</link>
<guid>https://arxiv.org/abs/2505.07862</guid>
<content:encoded><![CDATA[
<div> Keywords: sequence-to-sequence, graph wavelet transformer, multi scale spectral decomposition, graph structured sequence modeling, efficient

Summary:
The article introduces the Graph Wavelet Transformer (GWT), a new architecture for structured language tasks that replaces the traditional dot product self-attention mechanism with a learnable, multi-scale wavelet transform. This transform is defined over an explicit graph Laplacian derived from syntactic or semantic parses, providing an interpretable and efficient alternative to quadratic self-attention. The GWT offers a more expressive way to model sequences in graph structures and reduces the computational and memory complexities associated with traditional models. By leveraging multi-scale spectral decomposition, the GWT achieves better performance in graph structured sequence modeling tasks. This approach opens up new possibilities for improving the efficiency and interpretability of sequence-to-sequence models in various applications. <div>
arXiv:2505.07862v1 Announce Type: new 
Abstract: Existing sequence to sequence models for structured language tasks rely heavily on the dot product self attention mechanism, which incurs quadratic complexity in both computation and memory for input length N. We introduce the Graph Wavelet Transformer (GWT), a novel architecture that replaces this bottleneck with a learnable, multi scale wavelet transform defined over an explicit graph Laplacian derived from syntactic or semantic parses. Our analysis shows that multi scale spectral decomposition offers an interpretable, efficient, and expressive alternative to quadratic self attention for graph structured sequence modeling.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction</title>
<link>https://arxiv.org/abs/2505.07863</link>
<guid>https://arxiv.org/abs/2505.07863</guid>
<content:encoded><![CDATA[
<div> BERT, QoS, prediction, uncertainty, service selection 
Summary: 
QoSBERT is introduced as a framework for QoS prediction, utilizing pre-trained language models to encode user service metadata and provide deep semantic understanding. It integrates uncertainty estimation using Monte Carlo Dropout, allowing for trustworthy service quality prediction. QoSBERT outperforms existing models in response time and throughput prediction benchmarks, reducing errors and providing well-calibrated confidence intervals. The framework also aids in selecting high-quality training samples for improved robustness in low-resource settings. QoSBERT not only advances accuracy in service quality prediction but also delivers reliable uncertainty quantification, enhancing data-driven service selection and optimization. <div>
arXiv:2505.07863v1 Announce Type: new 
Abstract: Accurate prediction of Quality of Service (QoS) metrics is fundamental for selecting and managing cloud based services. Traditional QoS models rely on manual feature engineering and yield only point estimates, offering no insight into the confidence of their predictions. In this paper, we propose QoSBERT, the first framework that reformulates QoS prediction as a semantic regression task based on pre trained language models. Unlike previous approaches relying on sparse numerical features, QoSBERT automatically encodes user service metadata into natural language descriptions, enabling deep semantic understanding. Furthermore, we integrate a Monte Carlo Dropout based uncertainty estimation module, allowing for trustworthy and risk-aware service quality prediction, which is crucial yet underexplored in existing QoS models. QoSBERT applies attentive pooling over contextualized embeddings and a lightweight multilayer perceptron regressor, fine tuned jointly to minimize absolute error. We further exploit the resulting uncertainty estimates to select high quality training samples, improving robustness in low resource settings. On standard QoS benchmark datasets, QoSBERT achieves an average reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and 6.9% in MAE for throughput prediction compared to the strongest baselines, while providing well calibrated confidence intervals for robust and trustworthy service quality estimation. Our approach not only advances the accuracy of service quality prediction but also delivers reliable uncertainty quantification, paving the way for more trustworthy, data driven service selection and optimization.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection</title>
<link>https://arxiv.org/abs/2505.07870</link>
<guid>https://arxiv.org/abs/2505.07870</guid>
<content:encoded><![CDATA[
<div> metamorphic testing, fairness detection, large language models, prioritization, fault detection  
Summary:  
- The study focuses on using metamorphic testing to detect fairness issues in Large Language Models (LLMs).  
- By prioritizing metamorphic relations (MRs) based on their effectiveness, the study aims to optimize fault detection in LLMs.  
- A sentence diversity-based approach is proposed to compute and rank MRs, leading to a 22% improvement in fault detection rates compared to random prioritization.  
- The prioritization approach also reduces the time to the first failure by 15% and significantly decreases computational costs associated with fault labeling.  
- Experimental results show that the diversity-based MR prioritization method is effective in enhancing fairness testing for LLMs, performing within 5% of fault-based prioritization.  
<br /><br />Summary: <div>
arXiv:2505.07870v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in various applications, raising critical concerns about fairness and potential biases in their outputs. This paper explores the prioritization of metamorphic relations (MRs) in metamorphic testing as a strategy to efficiently detect fairness issues within LLMs. Given the exponential growth of possible test cases, exhaustive testing is impractical; therefore, prioritizing MRs based on their effectiveness in detecting fairness violations is crucial. We apply a sentence diversity-based approach to compute and rank MRs to optimize fault detection. Experimental results demonstrate that our proposed prioritization approach improves fault detection rates by 22% compared to random prioritization and 12% compared to distance-based prioritization, while reducing the time to the first failure by 15% and 8%, respectively. Furthermore, our approach performs within 5% of fault-based prioritization in effectiveness, while significantly reducing the computational cost associated with fault labeling. These results validate the effectiveness of diversity-based MR prioritization in enhancing fairness testing for LLMs.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy</title>
<link>https://arxiv.org/abs/2505.07871</link>
<guid>https://arxiv.org/abs/2505.07871</guid>
<content:encoded><![CDATA[
<div> Keywords: Financial sentiment analysis, Language Models, Benchmark datasets, Annotators' Instruction Assisted Prompt, WSBS dataset

Summary:
- Financial sentiment analysis poses challenges for Language Models (LLMs) due to nuanced language in financial contexts.
- Existing benchmark datasets like Financial Phrasebank have undefined sentiment classes leading to variability in annotations.
- The Annotators' Instruction Assisted Prompt (AIAP) redefines FSA task for LLMs by integrating detailed instructions into the prompt framework.
- AIAP significantly improves LLM performance by aligning machine operations with refined task definitions, yielding up to 9.08% improvements.
- AIAP introduces a context-aware sentiment-indexing method using model confidence scores, enhancing stock price prediction models and extracting more value from financial sentiment analysis. 
<br /><br />Summary: Financial sentiment analysis presents challenges for Language Models due to nuanced language in financial contexts. Benchmark datasets have undefined sentiment classes, leading to variability in annotations. The Annotators' Instruction Assisted Prompt (AIAP) improves LLM performance by standardizing sentiment understanding. AIAP enhances stock price prediction models using model confidence scores. <div>
arXiv:2505.07871v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Sound of Populism: Distinct Linguistic Features Across Populist Variants</title>
<link>https://arxiv.org/abs/2505.07874</link>
<guid>https://arxiv.org/abs/2505.07874</guid>
<content:encoded><![CDATA[
<div> populism, linguistic markers, political rhetoric, American politics, RoBERTa<br />
Summary:<br />
This study combines LIWC features and a RoBERTa model to analyze the sound of populism in U.S. presidential speeches. It identifies four populist dimensions (left-wing, right-wing, anti-elitism, people-centrism) and explores their linguistic markers, revealing a common direct and assertive tone used to connect with the people. Right-wing populism and people-centrism exhibit more emotionally charged discourse compared to left-wing and anti-elitist expressions, focusing on themes of identity, grievance, and crisis. The study highlights the strategic calibration of populist rhetoric, emphasizing the charismatic leadership persona it constructs. <div>
arXiv:2505.07874v1 Announce Type: new 
Abstract: This study explores the sound of populism by integrating the classic Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional and stylistic tones of language, with a fine-tuned RoBERTa model, a state-of-the-art context-aware language model trained to detect nuanced expressions of populism. This approach allows us to uncover the auditory dimensions of political rhetoric in U.S. presidential inaugural and State of the Union addresses. We examine how four key populist dimensions (i.e., left-wing, right-wing, anti-elitism, and people-centrism) manifest in the linguistic markers of speech, drawing attention to both commonalities and distinct tonal shifts across these variants. Our findings reveal that populist rhetoric consistently features a direct, assertive ``sound" that forges a connection with ``the people'' and constructs a charismatic leadership persona. However, this sound is not simply informal but strategically calibrated. Notably, right-wing populism and people-centrism exhibit a more emotionally charged discourse, resonating with themes of identity, grievance, and crisis, in contrast to the relatively restrained emotional tones of left-wing and anti-elitist expressions.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints</title>
<link>https://arxiv.org/abs/2505.07883</link>
<guid>https://arxiv.org/abs/2505.07883</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, event probabilities, rational decision-making, variational autoencoder, coherence

Summary:
This paper addresses the issue of incoherent event probabilities generated by Large Language Models (LLMs) by exploring the possibility of recovering coherent probabilities from the embeddings used by these models. By enforcing axiomatic constraints in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings, the authors show that event probabilities can naturally emerge in the latent space. The proposed method is evaluated on complementary events, demonstrating that the recovered probabilities exhibit greater coherence and align closely with true probabilities. This approach has the potential to improve rational decision-making under uncertainty by providing more accurate estimates for events involving uncertainty. Additionally, the results suggest that embedding-based approaches can offer a promising solution for addressing the incoherence issue in LLM-generated event probabilities. 

<br /><br />Summary: <div>
arXiv:2505.07883v1 Announce Type: new 
Abstract: Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of a WAZOBIA-Named Entity Recognition System</title>
<link>https://arxiv.org/abs/2505.07884</link>
<guid>https://arxiv.org/abs/2505.07884</guid>
<content:encoded><![CDATA[
<div> Named Entity Recognition, African languages, NLP frameworks, Transfer learning, WAZOBIA-NER system<br />
Summary:<br />
- Development of a WAZOBIA-NER system for Nigerian languages Hausa, Yoruba, and Igbo
- Compilation of annotated datasets to address data scarcity and linguistic diversity challenges
- Evaluation of machine learning techniques like CRF and deep learning models for entity recognition
- Utilization of OCR technology for text extraction from images
- Achieved high performance metrics in precision, recall, F1-score, and accuracy<br />

Summary:<br /> <div>
arXiv:2505.07884v1 Announce Type: new 
Abstract: Named Entity Recognition NER is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLHF: Prompt Optimization with Few-Shot Human Feedback</title>
<link>https://arxiv.org/abs/2505.07886</link>
<guid>https://arxiv.org/abs/2505.07886</guid>
<content:encoded><![CDATA[
<div> Framework, prompt optimization, large language models, PLHF, human feedback
Summary:
The article introduces PLHF, a new prompt optimization framework inspired by RLHF, designed for large language models. PLHF utilizes a specific evaluator module as a metric to estimate output quality, requiring only a single round of human feedback for prompt optimization. Unlike previous strategies, PLHF addresses the challenge of optimizing prompts without a clear metric for output quality assessment. Empirical results on public and industrial datasets demonstrate that PLHF surpasses existing output grading methods for LLM prompt optimizations. The framework offers a more effective and efficient approach to obtaining suitable prompts, especially for tasks where output quality cannot be easily assessed by comparisons with standard golden samples. <div>
arXiv:2505.07886v1 Announce Type: new 
Abstract: Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman "F"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping</title>
<link>https://arxiv.org/abs/2505.07888</link>
<guid>https://arxiv.org/abs/2505.07888</guid>
<content:encoded><![CDATA[
<div> Keywords: long-text style transfer, zero-shot learning, large language models, hierarchical framework, structured rewriting<br />
Summary:<br />
This paper introduces a hierarchical framework, ZeroStylus, for improving long-text style transfer by combining sentence-level stylistic adaptation with paragraph-level structural coherence. The framework addresses the challenge of preserving original syntactic and semantic information during style transfer by including paragraph-level semantic considerations. Two phases are used: hierarchical template acquisition and template-guided generation. Experimental evaluations show that the framework outperforms baseline methods in style consistency, content preservation, and expression quality metrics. Ablation studies confirm the benefits of using both sentence and paragraph template repositories for context-aware transformations. The results demonstrate the feasibility of coherent long-text style transfer without the need for parallel corpora or training large language models. <br /> <div>
arXiv:2505.07888v1 Announce Type: new 
Abstract: This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning</title>
<link>https://arxiv.org/abs/2505.07889</link>
<guid>https://arxiv.org/abs/2505.07889</guid>
<content:encoded><![CDATA[
<div> Keywords: biological protocols, BioProBench, language models, procedural reasoning, benchmark

Summary:
BioProBench is introduced as a multi-task benchmark for evaluating language models (LLMs) on understanding and reasoning with biological protocols. It includes tasks such as Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, providing a comprehensive assessment of LLMs on procedural biological texts. The benchmark, built on 27K original protocols, reveals that while LLMs perform well on surface understanding tasks, they struggle with deeper reasoning and structured generation tasks like ordering and generation. Comparison of 12 mainstream LLMs shows diverse performance, with certain open-source models approaching closed-source levels on some tasks. However, bio-specific small models lag behind general LLMs, indicating limitations in handling complex procedural content. The findings emphasize the challenge of procedural reasoning within biological protocols for current LLMs and highlight the need for improved AI systems tailored for automating scientific procedures safely. The code and data for BioProBench are available on GitHub and Hugging Face. 

<br /><br />Summary: BioProBench, a benchmark for evaluating language models on biological protocols, includes tasks like Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning. While models perform well on certain tasks, they struggle with deep reasoning and structured generation. Comparison of mainstream models shows varying performance levels, with bio-specific models lagging behind general models. This underscores the challenge of procedural reasoning in biological protocols for current language models and the need for improved AI systems tailored for complex scientific procedures. <div>
arXiv:2505.07889v1 Announce Type: new 
Abstract: Biological protocols are fundamental to reproducible and safe life science research. While LLMs excel on general tasks, their systematic evaluation on these highly specialized, accuracy-critical, and inherently procedural texts remains limited. In this work, we present BioProBench, the first large-scale, integrated multi-task benchmark for biological protocol understanding and reasoning. While limited benchmarks have touched upon specific aspects like protocol QA, BioProBench provides a comprehensive suite of five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts. Built upon 27K original protocols, it yields nearly 556K high-quality structured instances. We evaluate 12 mainstream open/closed-source LLMs on BioProBench. Experimental results reveal that while top models preform well on surface understanding tasks, struggle significantly with deep reasoning and structured generation tasks like ordering and generation. Furthermore, model comparisons reveal diverse performance: certain open-source models approach closed-source levels on some tasks, yet bio-specific small models lag behind general LLMs, indicating limitations on complex procedural content. Overall, our findings underscore that procedural reasoning within biological protocols represents a significant challenge for current LLMs. BioProBench serves as a standardized framework to diagnose these specific limitations and guide the development of AI systems better equipped for safely automating complex scientific procedures. The code and data are available at: https://github.com/YuyangSunshine/bioprotocolbench and https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks</title>
<link>https://arxiv.org/abs/2505.07890</link>
<guid>https://arxiv.org/abs/2505.07890</guid>
<content:encoded><![CDATA[
<div> Keywords: Turkish Sign Language, TSLFormer, sequence-to-sequence translation, self-attention mechanism, assistive communication<br />
<br />
Summary: <br />
The study introduces TSLFormer, a model for word-level Turkish Sign Language recognition that uses 3D joint positions instead of raw videos, reducing input dimensionality while retaining gesture information. Inspired by sign languages' linguistic nature and transformer success, TSLFormer employs a sequence-to-sequence translation approach with self-attention. By capturing temporal co-occurrence and motion patterns, it achieves competitive performance on the AUTSL dataset with minimal computational cost. The model's joint-based input enables the development of real-time, mobile assistive communication systems for hearing-impaired individuals, showcasing its potential for practical applications. <div>
arXiv:2505.07890v1 Announce Type: new 
Abstract: This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information.
  Our approach revisits sign language recognition as sequence-to-sequence translation, inspired by the linguistic nature of sign languages and the success of transformers in natural language processing. Since TSLFormer uses the self-attention mechanism, it effectively captures temporal co-occurrence within gesture sequences and highlights meaningful motion patterns as words unfold.
  Evaluated on the AUTSL dataset with over 36,000 samples and 227 different words, TSLFormer achieves competitive performance with minimal computational cost. These results show that joint-based input is sufficient for enabling real-time, mobile, and assistive communication systems for hearing-impaired individuals.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking</title>
<link>https://arxiv.org/abs/2505.07891</link>
<guid>https://arxiv.org/abs/2505.07891</guid>
<content:encoded><![CDATA[
<div> Keywords: misinformation, fact-checking, TrumorGPT, health domain, semantic reasoning

<br /><br />Summary: In today's social media landscape, the rapid spread of misinformation has led to widespread infodemics, particularly in health-related discussions. To tackle this challenge, the authors introduce TrumorGPT, a new generative artificial intelligence tool focused on fact-checking health claims. This system is specifically designed to identify "trumors," which are health-related rumors that may actually be true. TrumorGPT utilizes a large language model (LLM) and employs few-shot learning techniques for constructing and reasoning with semantic health knowledge graphs. To combat the hallucination issues often associated with LLMs and the constraints of static training datasets, TrumorGPT integrates a method called graph-based retrieval-augmented generation (GraphRAG). This innovative approach allows access to frequently updated semantic health knowledge graphs that contain the latest medical news and health information. By relying on the most current data, TrumorGPT enhances its fact-checking capabilities. Evaluation against extensive healthcare datasets indicates that TrumorGPT significantly outperforms other methods in verifying public health claims, marking a substantial advancement in combating health misinformation and fostering trust in the information circulating in the digital realm. <div>
arXiv:2505.07891v1 Announce Type: new 
Abstract: In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish "trumors", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</title>
<link>https://arxiv.org/abs/2505.07897</link>
<guid>https://arxiv.org/abs/2505.07897</guid>
<content:encoded><![CDATA[
<div> Keywords: Long-context models, Code comprehension, Code repair, LongCodeBench, Benchmark

Summary: 
Long-context models have seen a rapid increase in context lengths, presenting challenges in constructing realistic benchmarks for testing their capabilities. LongCodeBench (LCB) is introduced as a benchmark to assess the coding abilities of Long-context language models (LCLMs) in code comprehension and repair tasks. The benchmark draws from real-world GitHub issues to create LongCodeQA and bug fixing tasks, evaluating models across different scales. Results show that long-context remains a weakness for all models, with significant performance drops observed. For example, Claude 3.5 Sonnet experienced a drop from 29% to 3%, while Qwen2.5 dropped from 70.2% to 40%. Overall, the study highlights the need for further advancements in long-context modeling for improved performance in code comprehension and repair tasks. 

<br /><br />Summary: <div>
arXiv:2505.07897v1 Announce Type: new 
Abstract: Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise</title>
<link>https://arxiv.org/abs/2505.07899</link>
<guid>https://arxiv.org/abs/2505.07899</guid>
<content:encoded><![CDATA[
<div> sequential knowledge editing, language models, editing success rates, DeltaEdit, interference reduction <br />
<br />
Summary:<br />
The article introduces the concept of sequential knowledge editing to update large language models efficiently. It highlights the issue of declining editing success rates with an increase in the number of edits due to accumulated superimposed noise. The newly proposed DeltaEdit method addresses this problem by optimizing update parameters through a dynamic orthogonal constraints strategy. By reducing interference between edits, DeltaEdit significantly improves edit success rates and retains generalization capabilities of the model. Experimental results showcase the superiority of DeltaEdit over existing methods, ensuring stable and reliable model performance even during extensive sequential editing. <div>
arXiv:2505.07899v1 Announce Type: new 
Abstract: Sequential knowledge editing techniques aim to continuously update the knowledge in large language models at a low cost, preventing the models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, we identify that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the accumulation of superimposed noise problem. To address this, we identify the factors contributing to this deviation and propose DeltaEdit, a novel method that optimizes update parameters through a dynamic orthogonal constraints strategy, effectively reducing interference between edits to mitigate deviation. Experimental results demonstrate that DeltaEdit significantly outperforms existing methods in edit success rates and the retention of generalization capabilities, ensuring stable and reliable model performance even under extensive sequential editing.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SEM: Reinforcement Learning for Search-Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2505.07903</link>
<guid>https://arxiv.org/abs/2505.07903</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, Large Language Models, search optimization, structured reasoning, external retrieval<br />
<br />
Summary: 
The paper introduces a novel post-training reinforcement learning framework called SEM aimed at optimizing search usage for Large Language Models (LLMs). By combining datasets MuSiQue and MMLU, scenarios are created to train the model to differentiate between questions that can be answered internally and those requiring external search. A structured reasoning template is designed, and Group Relative Policy Optimization (GRPO) is used to post-train the model's search behaviors. The reward function encourages accurate answering and efficient retrieval, reducing redundant search operations while maintaining or improving answer accuracy across various benchmarks. This approach enhances the model's reasoning efficiency and its ability to effectively utilize external knowledge when needed. <div>
arXiv:2505.07903v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.
]]></content:encoded>
<pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>